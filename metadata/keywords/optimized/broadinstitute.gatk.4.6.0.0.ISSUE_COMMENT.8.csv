quality_attribute,sentence,source,author,repo,version,id,keyword,matched_word,match_idx,wiki,url,total_similar,target_keywords,target_matched_words
Performance,"We have discussed this and I have shown @lbergelson the error of his ways ;). Admittedly I'm still working on improving the presentation of content on the website -- but user feedback suggests they find the current site far superior to the old wiki. Also, I hate wikis. Also also, Louis was mostly complaining about the dev zone and queue docs, which do suck.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1049#issuecomment-151957099:333,queue,queue,333,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1049#issuecomment-151957099,1,['queue'],['queue']
Performance,"We recommend backing up data just because it is the ""cleanest"" way to roll back. If backing up data is really such a pain point, you could skip doing that. Just back up the callset.json file, and don't turn on `--consolidate` when you're doing incremental import. If a failure happens, just roll back the callset.json and re-do the import. The downside is that the failed import will hang around and take up disk space, but hopefully it is a rare enough occurrence that it doesn't matter - and you will have saved yourself backing up the data. In response to 2) - I guess you're implying that the overhead of cluster/job scheduling won't amortize any benefits from parallelism there? I suppose that could be true, but doesn't seem to be worth optimizing towards that. What I'm asking is whether split and merge are purely an instrument to allow you to choose the granularity of parallelism you want to use? Or is there something else? As I said before, we are considering enabling other ways to do distributed import which would work for the former. It might go something like:; - Create a workspace/initialize configuration+intervals to be imported; - Actually do the import by kicking off (multiple) import(s). User can pick the number of intervals each import is responsible for. User must ensure that no interval gets specified in multiple import processes. P.S: regarding 1000s of small contigs - the current GenomicsDBImport doesn't so so well with large number of contigs (unless you do concatenate the contigs into fewer groups). We hope to have some changes coming soon that will help with that by adding an option for the tool to merge multiple contigs into a single folder in the workspace.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6620#issuecomment-641037548:743,optimiz,optimizing,743,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6620#issuecomment-641037548,1,['optimiz'],['optimizing']
Performance,"We should only turn on parallel tests if it's multi-process parallelism, rather than multi-threading parallelism. Dealing with thread safety issues in the test suite would be a nightmare.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/221#issuecomment-75817361:85,multi-thread,multi-threading,85,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/221#issuecomment-75817361,1,['multi-thread'],['multi-threading']
Performance,"We're going to release the new model in 3.7 and have users test-drive that for quite a bit before we move to release 4.0, so we should be able to get some feedback on performance in the wild before we need to make any final decisions.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2255#issuecomment-258412589:167,perform,performance,167,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2255#issuecomment-258412589,1,['perform'],['performance']
Performance,"We've found that, generally speaking, you do pay a penalty on single-core performance when becoming a Spark tool, but gain the ability to easily scale to multiple cores and get the job done quickly. This is why we've been maintaining both Spark and non-Spark versions of important tools. Whether this will be the case for your tools as well can only be determined by profiling. If you extract the logic of your tool into a separate class, it's usually possible to call that shared code from both the Spark and walker frameworks without much or any code duplication. See `BaseRecalibrator` and `BaseRecalibratorSpark` for an example of this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2345#issuecomment-273524313:74,perform,performance,74,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2345#issuecomment-273524313,1,['perform'],['performance']
Performance,"We've now run into a secondary issue with GenomicsDB workspaces. In the thread above we talked about problems with WGS import, and reducing batchSize seemed to help. Separately, we created a GenomicsDB workspace with a few hundred exome samples - this import worked fine with default settings. Next, we are using this workspace as input for GenotypeGVCFs. However, the performance is much, much slower than we saw when using similar sized combined gVCFs as input. Do you expect this? Are there ways to improve this? If one thinks about a genomicsDB workspace more like a database than single file, are there defrag/shrink-like tasks that need to be performed on the workspace for efficiency?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6688#issuecomment-669327164:369,perform,performance,369,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6688#issuecomment-669327164,2,['perform'],"['performance', 'performed']"
Performance,"Well, we're definitely happy to accept contributions to the code base should you be interested in trying to implement it yourself. :) . I estimate that implementing the sliding window walker will be relatively easy to do, but making it perform well may be more difficult since the necessary caching for efficient traversal may not be implemented in the engine yet. . We definitely won't have time to spend on it this week since we're pretty busy with work for our first release. It's a feature we're already interested in though, and since others are also obviously interested I'll try and see if we can bump up it's priority.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1198#issuecomment-160752441:236,perform,perform,236,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1198#issuecomment-160752441,1,['perform'],['perform']
Performance,"Well, you're welcome to use gatk-launch as a launch script if you'd like (and feel free to rename to whatever you like...) A. There are a few reasons we have spark and non-spark versions of the tools. . 1. We wanted to port and validate certain tools as quickly as possible and doing a direct port from gatk3 -> gatk4 was easier than making them sparkified at the same time. 2. There's a tradeoff in using spark where you end up spending more total cpu hours in order to finish a job faster. Ideally this would be 1:1, double the number of cores and you halve the time to finish a job. It never scales perfectly though, there's always some overhead for being parallel. Our production pipelines are extremely sensitive to cost and not very sensitive to runtime, so they prefer we have a version that's optimized to use the least cpu hours even if that means a longer runtime. Other users prefer to be able to finish a job quickly and are willing to pay slightly more to do so, so we also have a spark version. . 3. Some tool are complicated to make work well spark. Spark works best when you can divide the input data into independent shards and then process them separately. This is complicated for things like the AssemblyRegion walker where you need context around each location of interest. We had to do things like add extra overlapping padding and things like that to avoid boundary issues where there are shard divisions. We don't yet fully understand spark performance and it's caveats, we're looking into that actively now. We hope that we'll be able to optimize our tools so that a spark pipeline of several tools in series is faster than running the individual non-spark versions, since it lets us avoid doing things like loading the bam file multiple times from disk. Whether or not we can achieve this is still and open question though.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2345#issuecomment-273318100:801,optimiz,optimized,801,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2345#issuecomment-273318100,4,"['load', 'optimiz', 'perform']","['loading', 'optimize', 'optimized', 'performance']"
Performance,"What @droazen says is correct. The `--numReducers` argument is not used when there is no shuffle. So `writeReadsSharded` should ignore it since it doesn't perform a shuffle (the output is already sharded either from the input partitioning, or from a shuffle in an earlier part of the the pipeline). Slightly counterintutively, `writeReadsSingle` does take the `--numReducers` argument since it does a shuffle to sort the reads in parallel, before stitching the files together. Hope that makes sense.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1455#issuecomment-176241205:155,perform,perform,155,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1455#issuecomment-176241205,1,['perform'],['perform']
Performance,"What I'm suggesting does not actually load the reads into the RDD. The RDD knows about input splits, and when you call mapPartitions it iterates through the reads in each split, which is the point at which the downsampling can be applied. So there's no point at which the million reads are held in memory. Does that make more sense?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1644#issuecomment-205894004:38,load,load,38,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1644#issuecomment-205894004,1,['load'],['load']
Performance,"What [this](https://stackoverflow.com/a/45713118) user describes seems to be what is happening (or supposed to happen): GATK extracts the .so file to a tmp directory and then uses `System.load(""/path/to/lib.so"")` to load the library. Is it possible that something inside the GATK package was not cross-compiled / is misconfigured [like in this case](https://github.com/xerial/snappy-java/issues/168#issuecomment-279928072)?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-688642099:188,load,load,188,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-688642099,2,['load'],['load']
Performance,"What is the current I/O throughput, the amount of data being pushed and the desired response-time? Usually keeping read-only data locally is usually faster, but it depends on how frequently it is used. Otherwise there would be dedicated sharded indexed servers, which would perform part of the repetitive data-processing to increase the response-time back to any workers and returned results.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/490#issuecomment-102111648:24,throughput,throughput,24,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/490#issuecomment-102111648,2,"['perform', 'throughput']","['perform', 'throughput']"
Performance,"What is the timeline for portable WDL-NIO?. This would make things like performing preliminary analyses on subsets of contigs, etc. go a bit faster. I agree that this is not a common use case, but since it's such a small amount of work, I don't see the harm. Would be nice to be consistent with other Featured WDLs, if they're all using NIO as well (is this true?)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4806#issuecomment-391724363:72,perform,performing,72,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4806#issuecomment-391724363,1,['perform'],['performing']
Performance,"When performing an action on an RDD, things will be fast and efficient if the output of each task fits in the available memory, and poky slow if we start spilling to disk. I could easily set the number of partitions on my input RDD to make sure that the output would fit in available memory, if only I knew how much memory was available. That's what I want to know, and why.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1456#issuecomment-177210646:5,perform,performing,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1456#issuecomment-177210646,1,['perform'],['performing']
Performance,"Will you have time to have a look to this in the following weeks, @droazen? Because the sliding-window walker is going to take more time, I would like to explore the `AssemblyRegionWalker` for window-traversal of reads instead of the sliding-window variant to see if it is enough for my purpose. But I require to tune the arguments for the shards, and with argument collections is the best way. Thank you in advance. I rebased to the latest master to skip the tests in the cloud and have the proper status for the checks.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2371#issuecomment-281645901:313,tune,tune,313,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2371#issuecomment-281645901,1,['tune'],['tune']
Performance,"With `--genomicsdb-shared-posixfs-optimizations`, the storage system should only require read access. @droazen, will work towards a fix for this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8233#issuecomment-1468563326:34,optimiz,optimizations,34,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8233#issuecomment-1468563326,1,['optimiz'],['optimizations']
Performance,"With the exception of the HMM package, all of our R dependencies are available through the conda R or bioconda channels; the HMM package is available only through a user's custom channel. However, the HMM package is only used to generate truth for testing the Java HMM code by @vruano (which is currently unused, but we thought was worth keeping around). I'm sure we could easily rewrite the tests to load the truth from a file. I think we should get rid of the install_R_packages.R script altogether and just roll all of these dependencies into the conda environment.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4250#issuecomment-406067920:401,load,load,401,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4250#issuecomment-406067920,1,['load'],['load']
Performance,With this new version I'm able to make it fail again; I opened a million channels to read the same file (across 1k threads) and got the error below. Yes I know a million parallel reads on a single file is more than a normal user would issue. ```; shaded.cloud_nio.com.google.api.client.http.HttpRequest execute; WARNING: exception thrown while executing request; javax.net.ssl.SSLHandshakeException: Remote host closed connection during handshake; 	at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:992); 	at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375); 	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403); 	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387); 	at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559); 	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185); 	at sun.net.www.protocol.http.HttpURLConnection.getOutputStream0(HttpURLConnection.java:1316); 	at sun.net.www.protocol.http.HttpURLConnection.getOutputStream(HttpURLConnection.java:1291); 	at sun.net.www.protocol.https.HttpsURLConnectionImpl.getOutputStream(HttpsURLConnectionImpl.java:250); 	at shaded.cloud_nio.com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:77); 	at shaded.cloud_nio.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972); 	at shaded.cloud_nio.com.google.auth.oauth2.ServiceAccountCredentials.refreshAccessToken(ServiceAccountCredentials.java:365); 	at shaded.cloud_nio.com.google.auth.oauth2.OAuth2Credentials.refresh(OAuth2Credentials.java:149); 	at shaded.cloud_nio.com.google.auth.oauth2.OAuth2Credentials.getRequestMetadata(OAuth2Credentials.java:135); 	at shaded.cloud_nio.com.google.auth.http.HttpCredentialsAdapter.initialize(HttpCredentialsAdapter.java:96); 	at com.google.cloud.http.HttpTransportOptions$1.initialize(HttpTransportOptions.java:156); 	at shaded.cloud_nio.c,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3070#issuecomment-309120156:554,perform,performInitialHandshake,554,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3070#issuecomment-309120156,1,['perform'],['performInitialHandshake']
Performance,Workspace/gatk/src/VectorLoglessPairHMM/headers/headers.h:16:10: fatal error: 'omp.h' file not found; #include <omp.h>; ^; 1 error generated. In file included from /Users/louisb/Workspace/gatk/src/VectorLoglessPairHMM/cpp/utils.cc:1:; /Users/louisb/Workspace/gatk/src/VectorLoglessPairHMM/headers/headers.h:16:10: fatal error: 'omp.h' file not found; #include <omp.h>; ^; 1 error generated. In file included from /Users/louisb/Workspace/gatk/src/VectorLoglessPairHMM/cpp/org_broadinstitute_hellbender_utils_pairhmm_VectorLoglessPairHMM.cc:1:; /Users/louisb/Workspace/gatk/src/VectorLoglessPairHMM/headers/headers.h:16:10: fatal error: 'omp.h' file not found; #include <omp.h>; ^; 1 error generated. In file included from /Users/louisb/Workspace/gatk/src/VectorLoglessPairHMM/cpp/baseline.cc:1:; /Users/louisb/Workspace/gatk/src/VectorLoglessPairHMM/headers/headers.h:16:10: fatal error: 'omp.h' file not found; #include <omp.h>; ^; 1 error generated. In file included from /Users/louisb/Workspace/gatk/src/VectorLoglessPairHMM/cpp/LoadTimeInitializer.cc:1:; In file included from /Users/louisb/Workspace/gatk/src/VectorLoglessPairHMM/headers/utils.h:4:; In file included from /Users/louisb/Workspace/gatk/src/VectorLoglessPairHMM/headers/common_data_structure.h:4:; /Users/louisb/Workspace/gatk/src/VectorLoglessPairHMM/headers/headers.h:16:10: fatal error: 'omp.h' file not found; #include <omp.h>; ^; 1 error generated. :compileVectorLoglessPairHMMSharedLibraryVectorLoglessPairHMMCpp FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':compileVectorLoglessPairHMMSharedLibraryVectorLoglessPairHMMCpp'.; > Multiple build operations failed.; C++ compiler failed while compiling avx_function_instantiations.cc.; C++ compiler failed while compiling utils.cc.; C++ compiler failed while compiling org_broadinstitute_hellbender_utils_pairhmm_VectorLoglessPairHMM.cc.; C++ compiler failed while compiling baseline.cc.; C++ compiler failed while compiling LoadT,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1504#issuecomment-185809068:1392,Load,LoadTimeInitializer,1392,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1504#issuecomment-185809068,1,['Load'],['LoadTimeInitializer']
Performance,"Wow, thanks for the detailed comments so far, @davidbenjamin! But perhaps let's quickly chat before you go any further?. There are a lot of things you commented on---temporary integration tests using local files, lots of code/arguments/etc. intentionally copied verbatim over from VQSR/tranches, and entire tools (the ""monolithic"" GMMVariantTrain and ScikitLearnVariantTrain)---that are rather in flux or will be scrapped/cleaned up shortly. That said, the comments on the code inherited from VQSR will certainly be useful in this process!. But it might save you some time if we could chat so I can give you a rough orientation and perhaps point out where the vestigial VQSR code remains. I think focusing discussion on the high level design of the tools that are likely to stay would also be most useful at this stage. Feel free to throw something on my calendar!. In the end, I think we will probably just retain the BGMM backend + the versions of the tools in the ""scalable"" package. I left the ""monolithic"" GMMVariantTrain and ScikitLearnVariantTrain tools in this branch so I could do one round of tieout. That tieout came out OK, so I think we'll abandon the monolithic tools, along with all the associated code outside of the scalable package. If it helps, I can go ahead and remove that stuff from this draft PR.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7659#issuecomment-1029393942:968,scalab,scalable,968,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7659#issuecomment-1029393942,2,['scalab'],['scalable']
Performance,YWxrZXJzL3ZhcmlhbnR1dGlscy9TZWxlY3RWYXJpYW50cy5qYXZh) | `83.911% <86.765%> (+2.516%)` | :arrow_up: |; | [...rs/variantutils/SelectVariantsIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/8092?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhcmlhbnR1dGlscy9TZWxlY3RWYXJpYW50c0ludGVncmF0aW9uVGVzdC5qYXZh) | `96.928% <94.702%> (-0.783%)` | :arrow_down: |; | [.../scalable/data/LabeledVariantAnnotationsDatum.java](https://codecov.io/gh/broadinstitute/gatk/pull/8092?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvZGF0YS9MYWJlbGVkVmFyaWFudEFubm90YXRpb25zRGF0dW0uamF2YQ==) | `68.421% <0.000%> (-3.801%)` | :arrow_down: |; | [...rs/vqsr/scalable/TrainVariantAnnotationsModel.java](https://codecov.io/gh/broadinstitute/gatk/pull/8092?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvVHJhaW5WYXJpYW50QW5ub3RhdGlvbnNNb2RlbC5qYXZh) | `77.778% <0.000%> (-2.991%)` | :arrow_down: |; | [...bender/utils/runtime/AsynchronousStreamWriter.java](https://codecov.io/gh/broadinstitute/gatk/pull/8092?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9ydW50aW1lL0FzeW5jaHJvbm91c1N0cmVhbVdyaXRlci5qYXZh) | `81.633% <0.000%> (-2.041%)` | :arrow_down: |; | [...ct/CreateSomaticPanelOfNormalsIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/8092?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=p,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8092#issuecomment-1374581874:3268,scalab,scalable,3268,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8092#issuecomment-1374581874,1,['scalab'],['scalable']
Performance,YWxrZXJzL3Zxc3Ivc2NhbGFibGUvRXh0cmFjdFZhcmlhbnRBbm5vdGF0aW9uc0ludGVncmF0aW9uVGVzdC5qYXZh) | `98.214% <ø> (+1.548%)` | :arrow_up: |; | [...rs/vqsr/scalable/TrainVariantAnnotationsModel.java](https://codecov.io/gh/broadinstitute/gatk/pull/8132?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvVHJhaW5WYXJpYW50QW5ub3RhdGlvbnNNb2RlbC5qYXZh) | `73.529% <45.161%> (-4.248%)` | :arrow_down: |; | [...stering/BayesianGaussianMixtureModelPosterior.java](https://codecov.io/gh/broadinstitute/gatk/pull/8132?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9jbHVzdGVyaW5nL0JheWVzaWFuR2F1c3NpYW5NaXh0dXJlTW9kZWxQb3N0ZXJpb3IuamF2YQ==) | `58.333% <58.333%> (ø)` | |; | [...walkers/vqsr/scalable/ScoreVariantAnnotations.java](https://codecov.io/gh/broadinstitute/gatk/pull/8132?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvU2NvcmVWYXJpYW50QW5ub3RhdGlvbnMuamF2YQ==) | `82.667% <66.667%> (+6.417%)` | :arrow_up: |; | [...alable/modeling/PythonVariantAnnotationsModel.java](https://codecov.io/gh/broadinstitute/gatk/pull/8132?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvbW9kZWxpbmcvUHl0aG9uVmFyaWFudEFubm90YXRpb25zTW9kZWwuamF2YQ==) | `68.421% <66.667%> (ø)` | |; | [...calable/modeling/BGMMVariantAnnotationsScorer.java](https://codecov.io/gh/broadinstitute/gatk/pull/8132?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comme,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8132#issuecomment-1370265333:4429,scalab,scalable,4429,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8132#issuecomment-1370265333,1,['scalab'],['scalable']
Performance,"Yea, I believe the header mismatching has to do with the canonical header lines in the gatk being updated in the middle of this work. htsjdk doesn't discriminate between bad header line count fields so there are a number of sloppy header lines like this. I will update my genomicsDB branch to perform a proper check of the allele specific annotations based on the new version now.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4047#issuecomment-355645211:293,perform,perform,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4047#issuecomment-355645211,1,['perform'],['perform']
Performance,"Yeah, I don't like these new interface methods -- they make `GATKRead` significantly worse. We should cache `isUnmapped`, etc. in the adapter to accomplish the same thing, as @lbergelson suggests. Not that hard, and we can just unconditionally invalidate the cached values (using `Boolean` fields set to null) whenever the read is mutated in any way in order to simplify the logic.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235102282:102,cache,cache,102,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235102282,2,['cache'],"['cache', 'cached']"
Performance,"Yeah, the goofy name is just a placeholder, as is the separate tool itself. Unless there are any objections (@fleharty @mwalker174?) or I run into any unforeseen snafus or parameter ambiguities along the way, I am going to roll the multisample-segmentation functionality into ModelSegments. We can toggle this functionality by passing multiple `--denoised-copy-ratios` and `--allelic-counts` arguments, e.g.:. ```; gatk ModelSegments ; --normal-allelic-counts normal.allelicCounts.tsv (this is only used for het genotyping); --denoised-copy-ratios normal.denoisedCR.tsv; --denoised-copy-ratios tumor-1.denoisedCR.tsv; ...; --denoised-copy-ratios tumor-N.denoisedCR.tsv; --allelic-counts normal.allelicCounts.tsv; --allelic-counts tumor-1.allelicCounts.tsv; ...; --allelic-counts tumor-N.allelicCounts.tsv \; -O .; --output-prefix joint-segmentation; ```. This will perform both het genotyping and joint segmentation, but will yield a Picard interval-list `joint-segmentation.interval_list` as its sole output. (Although we could proceed to perform MCMC model inference on each sample in series, we'll stop at segmentation to enforce the scattering of inference across samples, which will be quicker.) We can also allow for copy-ratio-only and allelic-count-only modes. Users can use this joint segmentation in their own downstream tools, but we can also allow ModelSegments to ingest it via in a new `--segments` argument:. ```; gatk ModelSegments; --normal-allelic-counts normal.allelicCounts.tsv (equivalently, we could omit this and adjust minimum-total-allele-count-case, as is done in the WDL); --allelic-counts normal.allelicCounts.tsv; --denoised-copy-ratios normal.denoisedCR.tsv; --segments joint-segmentation.interval_list; -O .; --output-prefix normal. gatk ModelSegments; --normal-allelic-counts normal.allelicCounts.tsv; --allelic-counts tumor-1.allelicCounts.tsv; --denoised-copy-ratios tumor-1.denoisedCR.tsv; --segments joint-segmentation.interval_list; -O .; --output-prefix tumor-1. ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6499#issuecomment-607313549:865,perform,perform,865,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6499#issuecomment-607313549,1,['perform'],['perform']
Performance,"Yes, @magicDGS I have answered a continuous stream of questions on indel realignment in the forum and these questions are ongoing. Here's one I answered just last week: http://gatkforums.broadinstitute.org/gatk/discussion/comment/39359#Comment_39359. So it appears that it is a step that folks continue to use. . I would add that indel realignment is especially important for Mutect1 pipelines, which continues as the standard in somatic calling, pending performance of Mutect2. Even if Mutect2 were to outperform Mutect1 calling, because of the extreme difference in compute between the two, I conjecture folks will continue to use Mutect1. In fact, the only sane way to output a BAM for the entirety of a somatic sample set is via indel realignment. Somatic folks are keen on manual review and indel realignment is the cheapest way to get there. Here's another scenerio for which indel realignment is useful: the case of a low coverage site where multiple haplotypes are present. These cannot be resolved by HaplotypeCaller if the difference in allele depth between reads supporting either haplotype does not pass some threshold. In this case, sites receive `./.` no calls. Indel realignment using a known sites resource can aid in the resolution of haplotypes at sites that correspond to common population variants. One could argue that the cohort-level analysis could aid in resolution of these, but it may be that large cohorts are a luxury that cannot be afforded by many research groups. ### I vote yes please to porting the indel realignment pipeline.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3084#issuecomment-307895094:455,perform,performance,455,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3084#issuecomment-307895094,1,['perform'],['performance']
Performance,"Yes, I have also just tested HaplotypeCaller on a CRAM and it seems to run fine locally.; ```; /humgen/gsa-hpprojects/GATK/gatk4/gatk-4.beta.5/gatk-launch HaplotypeCaller \; -R /humgen/gsa-hpprojects/dev/shlee/ref/GRCh38_1kg/GRCh38_full_analysis_set_plus_decoy_hla.fa \; -I HG00190.alt_bwamem_GRCh38DH.20150826.FIN.exome.cram \; -O HG00190_cram_HC.vcf \; -L chr17; ```; gives; ```; Using GATK jar /humgen/gsa-hpprojects/GATK/gatk4/gatk-4.beta.5/gatk-package-4.beta.5-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true -jar /humgen/gsa-hpprojects/GATK/gatk4/gatk-4.beta.5/gatk-package-4.beta.5-local.jar HaplotypeCaller -R /humgen/gsa-hpprojects/dev/shlee/ref/GRCh38_1kg/GRCh38_full_analysis_set_plus_decoy_hla.fa -I HG00190.alt_bwamem_GRCh38DH.20150826.FIN.exome.cram -O HG00190_cram_HC.vcf -L chr17; 14:50:16.528 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/humgen/gsa-hpprojects/GATK/gatk4/gatk-4.beta.5/gatk-package-4.beta.5-local.jar!/com/intel/gkl/native/libgkl_compression.so; [October 6, 2017 2:50:16 PM EDT] HaplotypeCaller --output HG00190_cram_HC.vcf --intervals chr17 --input HG00190.alt_bwamem_GRCh38DH.20150826.FIN.exome.cram --reference /humgen/gsa-hpprojects/dev/shlee/ref/GRCh38_1kg/GRCh38_full_analysis_set_plus_decoy_hla.fa --group StandardAnnotation --group StandardHCAnnotation --GVCFGQBands 1 --GVCFGQBands 2 --GVCFGQBands 3 --GVCFGQBands 4 --GVCFGQBands 5 --GVCFGQBands 6 --GVCFGQBands 7 --GVCFGQBands 8 --GVCFGQBands 9 --GVCFGQBands 10 --GVCFGQBands 11 --GVCFGQBands 12 --GVCFGQBands 13 --GVCFGQBands 14 --GVCFGQBands 15 --GVCFGQBands 16 --GVCFGQBands 17 --GVCFGQBands 18 --GVCFGQBands 19 --GVCFGQBands 20 --GVCFGQBands 21 --GVCFGQBands 22 --GVCFGQBands 23 --GVCFGQBands 24 --GVCFGQBands 25 --GVCFGQBands 26 --GVCFGQBands 27 --GVCFGQBands 28 --GVCFGQBands 29 --GVCFGQBands 30 --GVCFGQBands 31 --GVCF",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3154#issuecomment-334840678:993,Load,Loading,993,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3154#issuecomment-334840678,1,['Load'],['Loading']
Performance,"Yes, I ran option 3, 5 times (using the older version not latest master). Of those 5, 3 failed. One of the failures was: `java.lang.RuntimeException: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: All reopens failed`. The other two were both the non-negative error message:. ```; Using GATK jar /usr/gitc/gatk4/gatk-package-4.beta.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true -XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10 -XX:+PrintFlagsFinal -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -XX:+PrintGCDetails -Xloggc:gc_log.log -Xms4000m -jar /usr/gitc/gatk4/gatk-package-4.beta.1-local.jar BaseRecalibrator -R /cromwell_root/broad-references/hg38/v0/Homo_sapiens_assembly38.fasta -I gs://broad-gotc-dev-cromwell-execution/PairedEndSingleSampleWorkflow/66442def-ad3f-4c6c-960e-17578f6b382c/call-SortAndFixSampleBam/CHMI_CHMI3_WGS2.aligned.duplicate_marked.sorted.bam --useOriginalQualities -O CHIM.recal_data.csv -knownSites gs://broad-references/hg38/v0/Homo_sapiens_assembly38.dbsnp138.vcf -knownSites /cromwell_root/broad-references/hg38/v0/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz -knownSites /cromwell_root/broad-references/hg38/v0/Homo_sapiens_assembly38.known_indels.vcf.gz -L chr6:1+ --use_jdk_deflater --use_jdk_inflater; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp.4ZS1WV; [July 24, 2017 5:48:10 PM UTC] BaseRecalibrator --useOriginalQualities true --knownSites gs://broad-references/hg38/v0/Homo_sapiens_assembly38.dbsnp138.vcf --knownSites /cromwell_root/broad-references/hg38/v0/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz --knownSites /cromwell_root/broad-references/hg38/v0/Homo_sapiens_assembly38.known_indels.vcf.gz --output CHIM.recal_data.csv --intervals chr6:1+ --input gs://broad-gotc-dev-cromwell-execution/PairedEndSingleSampleWorkflow/66442def-ad3f-4c6c-",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317782472:160,concurren,concurrent,160,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317782472,1,['concurren'],['concurrent']
Performance,"Yes, I'd like to test out the new tools in the workflow. All of these changes seem major and I'm excited to see how the workflow performs. I'll stop by to chat with you.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333530110:129,perform,performs,129,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333530110,1,['perform'],['performs']
Performance,"Yes, I'm not sure I would expect that to be true. My guess is that we continue to run long after suitable convergence with the current inference parameters (which we have not had a chance to optimize over for runtime). I think we can probably afford to be less conservative. @mbabadi do you have a better handle on this?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4397#issuecomment-392154161:191,optimiz,optimize,191,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4397#issuecomment-392154161,1,['optimiz'],['optimize']
Performance,"Yes, that is easy to do. The guidance on how much memory to leave wasnt clear on the docs. Is there a rule of thumb on how much we should leave? We can do whatever makes sense. the command is something like:. ```. /home/exacloud/gscratch/prime-seq/java/java8/bin/java \; -Djava.io.tmpdir=/mnt/scratch/prime-seq/tmp.5E76utMagnGenomicsDB_Append_Merge_2020-11-04_09-17-56-Job1 \; -Xmx104g \; -Xms104g \; -Xss2m \; -jar /home/exacloud/gscratch/prime-seq/bin/GenomeAnalysisTK4.jar GenomicsDBImport \; -V <Repeated 183 times for gVCFs> \ ; --genomicsdb-update-workspace-path /home/exacloud/gscratch/prime-seq/workDir/9a2611e8-0112-1039-8c80-f8f3fc869aa5/Job1.work/WGS_Nov_1300.gdb \; --batch-size 50 \; --consolidate \; --genomicsdb-shared-posixfs-optimizations. ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6910#issuecomment-724293565:742,optimiz,optimizations,742,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6910#issuecomment-724293565,1,['optimiz'],['optimizations']
Performance,"Yes, that seems to be working as intended. You'll see that in your first log, the following lines indicate that convergence has been reached after 2 iterations:. ````; 09:40:44.924 INFO MultidimensionalModeller - Smoothing iteration: 2; 09:40:44.924 INFO MultidimensionalModeller - Number of segments before smoothing iteration: 398; 09:40:44.924 INFO MultidimensionalModeller - Number of segments after smoothing iteration: 398; ````. Therefore, when you set N >= 3, the behavior is the same as when N = 2. In your second log, only one iteration goes by before we refit again. This refit yields posteriors that are sufficiently different from the approximation used when no refitting is performed so that additional smoothing needs to be performed before convergence. You'll see that in the second case, we end up with fewer segments. Whether or not this smoother result (which takes longer to arrive at, since refitting is expensive) is desired will depend on the goals of the analysis.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382807646:688,perform,performed,688,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382807646,2,['perform'],['performed']
Performance,"Yes, the ""no space left on device"" was temporary and has been fixed - I'm creating a new workspace with the --genomicsdb-shared-posixfs-optimizations turned on.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-760345071:136,optimiz,optimizations,136,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-760345071,1,['optimiz'],['optimizations']
Performance,"Yes, this looks very similar to what I'm seeing. The VariantSparkSinkUnitTests run fine on their own. When the whole test suite is run though, an OOM error is thrown when VariantSparkSinkUnitTests run:. java.lang.OutOfMemoryError: GC overhead limit exceeded; at org.gradle.internal.remote.internal.hub.MethodInvocationSerializer$MethodInvocationReader.read(MethodInvocationSerializer.java:120); at org.gradle.internal.remote.internal.hub.MethodInvocationSerializer$MethodInvocationReader.read(MethodInvocationSerializer.java:99); at org.gradle.internal.serialize.kryo.TypeSafeSerializer$1.read(TypeSafeSerializer.java:34); at org.gradle.internal.remote.internal.hub.InterHubMessageSerializer$MessageReader.read(InterHubMessageSerializer.java:66); at org.gradle.internal.remote.internal.hub.InterHubMessageSerializer$MessageReader.read(InterHubMessageSerializer.java:52); at org.gradle.internal.remote.internal.inet.SocketConnection.receive(SocketConnection.java:78); at org.gradle.internal.remote.internal.hub.MessageHub$ConnectionReceive.run(MessageHub.java:250); at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 13:37 DEBUG: [kryo] Read object reference 986: reference=GRCh37.3; Test: Test method testWritingToFileURL[0](/Users/cnorman/projects/gatk/src/test/resources/Homo_sapiens_assembly19.dbsnp135.chr1_1M.exome_intervals.vcf, .vcf)(org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSinkUnitTest) produced standard out/err: 13:37 DEBUG: [kryo] Read object reference 203: contig=<ID=20,length=63025520,assembly=b37>",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2490#issuecomment-287857575:1088,concurren,concurrent,1088,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2490#issuecomment-287857575,4,['concurren'],['concurrent']
Performance,"Yes, your description's accurate. Well, we load all variants into memory,; but not into each worker's memory: each worker only has the variants it; needs. Broadcasting all variants to everyone might be a little bit more work than; this code's approach (just sending some of the variants to the workers that; need it), but probably not enough to matter. The bottom line is that I; agree with you, it's also possible to write a version of this code that's; closer to the existing approach of ""each read for itself"" instead of ""each; read in a shard"". Doing this directly would lose the optimization of; loading the reference bases only once per shard instead of once per read; (since they overlap), but we should be able to get it back via judicious; caching. Or of course we can keep the sharded code since it works. Indeed, the input/output is optimized for modern datacenters with flat; datacenter storage; https://www.usenix.org/conference/osdi12/technical-sessions/presentation/nightingale.; In these environments, the filesystem is striped across multiple disks; linked via a fast, full bisection bandwidth network such that the remote; filesystem is faster than a local disk. I don't know whether; HDFS-on-premise can do this today though there is no fundamental reason why; it couldn't. A happy side effect is that it makes it possible for me to run it locally -; ReadsSparkSink currently only seems to work when run on a cluster (I am; sure this can be fixed though). That said, I have not had the luxury of time to optimize this whole thing,; so someone will get all the fun of comparing the two approaches,; identifying the bottlenecks, and melding the two together in the best way; possible. My goal is merely to make this algorithm available quickly so we; can see if it can be as helpful here as it has been on the Dataflow side. On Mon, Oct 12, 2015 at 7:37 AM, Tom White notifications@github.com wrote:. > At a high-level, the approach here seems to be to create shards (of size 1; > mil",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/987#issuecomment-147476006:43,load,load,43,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/987#issuecomment-147476006,4,"['load', 'optimiz']","['load', 'loading', 'optimization', 'optimized']"
Performance,"You can probably eyeball it from the 4.1.8.1 results above, which provide two points on the respective curves (although note those numbers are not broken down by SNP/indel). I’ll generate nicer plots once things are finalized, this is just what is spit out by rtg rocplot. There are a few regions of parameter space where precision seems to improve at the expense of a bit of sensitivity. Again, this might not be optimal if we are going to filter afterwards. I guess the main takeaway at this stage is that changing the parameters can result in non-negligible performance changes, probably beyond the level of sample-to-sample variation, which wasn't obvious from the first few runs I posted.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-712344348:561,perform,performance,561,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-712344348,1,['perform'],['performance']
Performance,You can start with consolidating all of them at once (with no batch size) as your fragments are more or less the same size and it should perform OK with the new tool. You can even use the default buffer size as it should not be a factor with the new tool. If that does not work you can probably use `number-of-fragments/2` or `number-of-fragments/4` for a batch size.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1081075538:137,perform,perform,137,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1081075538,1,['perform'],['perform']
Performance,"You can use `defaultParallelism()` in the JavaSparkContext for this. In my experimentation, this is always equal to the total number of executor threads. (Except for a brief race condition where it gives the wrong answer for the first few seconds of the lifetime of the JavaSparkContext. Nice, eh?)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1967#issuecomment-229986486:174,race condition,race condition,174,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1967#issuecomment-229986486,1,['race condition'],['race condition']
Performance,Yup. Adding the note here to fill in some cost optimizations for some of the common tasks.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3983#issuecomment-352761918:47,optimiz,optimizations,47,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3983#issuecomment-352761918,1,['optimiz'],['optimizations']
Performance,"[MKL](https://software.intel.com/en-us/intel-mkl) has highly optimized versions of `log10()` and `pow()`, which are much faster than standard native implementations. @mbabadi would you mind sharing your profiling code? I'd like to make an apples-to-apples comparison of MKL to Apache and Nd4j.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2577#issuecomment-292621448:61,optimiz,optimized,61,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2577#issuecomment-292621448,1,['optimiz'],['optimized']
Performance,"_FIELD_FORMAT : DECIMAL; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Defaults.USE_CRAM_REF_DOWNLOAD : false; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Deflater IntelDeflater; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Inflater IntelInflater; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Initializing engine; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4769b07b] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@5ef60048].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4769b07b] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@5ef60048].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2618#issuecomment-296871363:2041,load,loaded,2041,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2618#issuecomment-296871363,3,['load'],['loaded']
Performance,"_Is there any way to get it to use fewer partitions in a case like this where there are lots of intervals ?_; No - see the PR message https://github.com/broadinstitute/gatk/pull/4645#issue-180678162. The multi-interval support in GenomicsDBImport tool is purely for convenience. For scalability with a large number of intervals and samples, you should use multiple processes, each writing to a small (1?) number of intervals. **_Somewhat more concerning is that when with 8000 intervals, I see a different failure mode. First I see lots (thousands) of these messages:_**; The first set of messages are spurious debug messages - no error in reality. I'll provide a jar without these messages. The second set of messages are a result of too many file handles open per process - your system is limiting the number of file handles opened by a single process. Again, this goes back to the previous statement.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407228434:283,scalab,scalability,283,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407228434,1,['scalab'],['scalability']
Performance,"_WGS2.bam --interval_set_rule UNION --interval_exclusion_padding 0 --readValidationStringency SILENT --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation false --createOutputBamIndex true --createOutputBamMD5 false --createOutputVariantIndex true --createOutputVariantMD5 false --lenient false --addOutputSAMProgramRecord true --addOutputVCFCommandLine true --cloudPrefetchBuffer 40 --cloudIndexPrefetchBuffer -1 --disableBamIndexCaching false --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --disableToolDefaultReadFilters false; [July 21, 2017 6:20:54 PM UTC] Executing as root@9b75c21b7620 on Linux 4.9.0-0.bpo.3-amd64 amd64; OpenJDK 64-Bit Server VM 1.8.0_111-8u111-b14-2~bpo8+1-b14; Version: 4.beta.1; [July 21, 2017 6:29:58 PM UTC] org.broadinstitute.hellbender.tools.PrintReads done. Elapsed time: 9.06 minutes.; Runtime.totalMemory()=2088763392; java.lang.RuntimeException: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: java.lang.IllegalArgumentException: Position should be non-negative, is %d [-830472192]; 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:309); 	at htsjdk.samtools.seekablestream.SeekablePathStream.read(SeekablePathStream.java:86); 	at htsjdk.samtools.util.BlockCompressedInputStream.readBytes(BlockCompressedInputStream.java:554); 	at htsjdk.samtools.util.BlockCompressedInputStream.readBytes(BlockCompressedInputStream.java:543); 	at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:512); 	at htsjdk.samtools.util.BlockCompressedInputStream.nextBlock(BlockCompressedInputStream.java:455); 	at htsjdk.samtools.util.BlockCompressedInputStream.readBlock(BlockCompressedInputStream.java:445); 	at htsjdk.samtools.util.BlockCompressedInputStream.available(BlockCompressedInputStream.java:194); 	at htsjdk.samtools.util.Blo",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317442564:2517,concurren,concurrent,2517,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317442564,1,['concurren'],['concurrent']
Performance,_WRITE_FOR_TRIBBLE : false; 01:39:03.364 INFO FilterAlignmentArtifacts - Deflater: IntelDeflater; 01:39:03.364 INFO FilterAlignmentArtifacts - Inflater: IntelInflater; 01:39:03.364 INFO FilterAlignmentArtifacts - GCS max retries/reopens: 20; 01:39:03.364 INFO FilterAlignmentArtifacts - Requester pays: disabled; 01:39:03.364 WARN FilterAlignmentArtifacts - . [1m[31m !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: FilterAlignmentArtifacts is an EXPERIMENTAL tool and should not be used for production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!![0m. 01:39:03.364 INFO FilterAlignmentArtifacts - Initializing engine; 01:39:07.644 INFO FeatureManager - Using codec VCFCodec to read file gs://fc-ac4624cb-a8fc-49a2-b071-d3a0ae799418/cb1feccb-0a69-42bf-ba5f-fde762934a59/Mutect2/fe3623c8-0eaf-4cd4-9f81-d1fda4073f2e/call-Filter/22.hg38-filtered.vcf; 01:39:08.399 INFO FilterAlignmentArtifacts - Done initializing engine; 01:39:09.523 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 01:39:09.565 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 01:39:09.566 INFO IntelPairHmm - Available threads: 4; 01:39:09.566 INFO IntelPairHmm - Requested threads: 4; 01:39:09.566 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 01:39:09.567 INFO ProgressMeter - Starting traversal; 01:39:09.567 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; munmap_chunk(): invalid pointer; Using GATK jar /root/gatk.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx11500m -jar /root/gatk.jar FilterAlignmentArtifacts -R gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fast,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-664539860:4838,Load,Loading,4838,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-664539860,1,['Load'],['Loading']
Performance,_broadinstitute_hellbender_utils_pairhmm_VectorLoglessPairHMM.cc:1:; /Users/louisb/Workspace/gatk/src/VectorLoglessPairHMM/headers/headers.h:16:10: fatal error: 'omp.h' file not found; #include <omp.h>; ^; 1 error generated. In file included from /Users/louisb/Workspace/gatk/src/VectorLoglessPairHMM/cpp/baseline.cc:1:; /Users/louisb/Workspace/gatk/src/VectorLoglessPairHMM/headers/headers.h:16:10: fatal error: 'omp.h' file not found; #include <omp.h>; ^; 1 error generated. In file included from /Users/louisb/Workspace/gatk/src/VectorLoglessPairHMM/cpp/LoadTimeInitializer.cc:1:; In file included from /Users/louisb/Workspace/gatk/src/VectorLoglessPairHMM/headers/utils.h:4:; In file included from /Users/louisb/Workspace/gatk/src/VectorLoglessPairHMM/headers/common_data_structure.h:4:; /Users/louisb/Workspace/gatk/src/VectorLoglessPairHMM/headers/headers.h:16:10: fatal error: 'omp.h' file not found; #include <omp.h>; ^; 1 error generated. :compileVectorLoglessPairHMMSharedLibraryVectorLoglessPairHMMCpp FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':compileVectorLoglessPairHMMSharedLibraryVectorLoglessPairHMMCpp'.; > Multiple build operations failed.; C++ compiler failed while compiling avx_function_instantiations.cc.; C++ compiler failed while compiling utils.cc.; C++ compiler failed while compiling org_broadinstitute_hellbender_utils_pairhmm_VectorLoglessPairHMM.cc.; C++ compiler failed while compiling baseline.cc.; C++ compiler failed while compiling LoadTimeInitializer.cc.; See the complete log at: file:///Users/louisb/Workspace/gatk/build/tmp/compileVectorLoglessPairHMMSharedLibraryVectorLoglessPairHMMCpp/output.txt; ```. I'm using clang 6.0. Would you expect it to build on that?. ```; Configured with: --prefix=/Library/Developer/CommandLineTools/usr --with-gxx-include-dir=/usr/include/c++/4.2.1; Apple LLVM version 6.0 (clang-600.0.57) (based on LLVM 3.5svn); Target: x86_64-apple-darwin13.4.0; Thread model: posix; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1504#issuecomment-185809068:2357,Load,LoadTimeInitializer,2357,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1504#issuecomment-185809068,1,['Load'],['LoadTimeInitializer']
Performance,_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9OYXR1cmFsTG9nVXRpbHMuamF2YQ==) | `77.143% <0.000%> (ø)` | |; | [...ls/clustering/BayesianGaussianMixtureModeller.java](https://codecov.io/gh/broadinstitute/gatk/pull/7954/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9jbHVzdGVyaW5nL0JheWVzaWFuR2F1c3NpYW5NaXh0dXJlTW9kZWxsZXIuamF2YQ==) | `0.000% <0.000%> (ø)` | |; | [.../tools/walkers/vqsr/scalable/data/VariantType.java](https://codecov.io/gh/broadinstitute/gatk/pull/7954/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvZGF0YS9WYXJpYW50VHlwZS5qYXZh) | `60.000% <60.000%> (ø)` | |; | [.../walkers/vqsr/scalable/SystemCommandUtilsTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/7954/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvU3lzdGVtQ29tbWFuZFV0aWxzVGVzdC5qYXZh) | `60.870% <60.870%> (ø)` | |; | [.../scalable/data/LabeledVariantAnnotationsDatum.java](https://codecov.io/gh/broadinstitute/gatk/pull/7954/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvZGF0YS9MYWJlbGVkVmFyaWFudEFubm90YXRpb25zRGF0dW0uamF2YQ==) | `72.222% <72.222%> (ø)` | |; | ... and [20 more](https://codecov.io/gh/broadinstitute/gatk/pull/7954/diff?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7954#issuecomment-1191010834:5127,scalab,scalable,5127,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7954#issuecomment-1191010834,1,['scalab'],['scalable']
Performance,_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true --executor-memory 25G --driver-memory 5G /home/genomics/Projects/TomWhitePatches/gatk/build/libs/gatk-package-4.alpha.2-230-g19db939-SNAPSHOT-spark.jar BaseRecalibratorSpark -I hdfs://n001:54310/GATK4TEST/LargeBroadData/WGS-G94982-NA12878.bam -knownSites hdfs://n001:54310/GATK4TEST/DBSNP/dbsnp_138.hg19.vcf.gz -R hdfs://n001:54310/GATK4TEST/OldData/human_g1k_v37.2bit -O hdfs://n001:54310/GATK4TEST/LargeOutput/WGS_BQSR --sparkMaster spark://n001:7077; Picked up JAVA_TOOL_OPTIONS: -XX:+UseG1GC -XX:ParallelGCThreads=4; Picked up JAVA_TOOL_OPTIONS: -XX:+UseG1GC -XX:ParallelGCThreads=4; java.lang.ClassNotFoundException: org.broadinstitute.hellbender.Main; at java.lang.ClassLoader.findClass(ClassLoader.java:530); at org.apache.spark.util.ParentClassLoader.findClass(ParentClassLoader.scala:26); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at org.apache.spark.util.ParentClassLoader.loadClass(ParentClassLoader.scala:34); at org.apache.spark.util.ChildFirstURLClassLoader.loadClass(MutableURLClassLoader.scala:55); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at java.lang.Class.forName0(Native Method); at java.lang.Class.forName(Class.java:348); at org.apache.spark.util.Utils$.classForName(Utils.scala:229); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:695); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala),MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2620#issuecomment-299259877:1977,load,loadClass,1977,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2620#issuecomment-299259877,4,['load'],['loadClass']
Performance,"`KnownSitesCache` loads a VCF file and turns it into an in-memory `IntervalsSkipList`, which is shared by all tasks in the same VM. The Spark job stage that uses `KnownSitesCache` in BQSR takes 6 minutes to load the file into memory, but then the stage takes another 4 minutes to complete (total 10 minutes, stage name `treeAggregate at BaseRecalibratorSparkFn.java:39`). Another approach is to uses Spark's `--files` option to share the VCF file with all executors, then use the file as a `FeatureDataSource` directly. I did this using a `.vcf.gz` file and the accompanying `.vcf.gz.tbi`. See the code in this branch: https://github.com/broadinstitute/gatk/tree/tw_known_sites_perf_files. This however was slower, the same stage took 39 minutes to complete, compared to 10. This is probably because reading from a file is much slower than using an in-memory lookup to find variants overlapping each read.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5103#issuecomment-412896366:18,load,loads,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5103#issuecomment-412896366,2,['load'],"['load', 'loads']"
Performance,```; tarting a Gradle Daemon (subsequent builds will be faster); :compileJava; :processResources; :classes; :shadowJarex; org.gradle.api.GradleException: Could not add file '/wrkdirs/usr/ports/biology/gatk/work/gatk-4.0.11.0/build/classes/java/main/org/broadinstitute/hellbender/metrics/MultiLevelCollector$Distributor.class' to ZIP '/wrkdirs/usr/ports/biology/gatk/work/gatk-4.0.11.0/build/libs/gatk-package-1.0-SNAPSHOT-local.jar'.; 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423); 	at org.codehaus.groovy.reflection.CachedConstructor.invoke(CachedConstructor.java:83); 	at org.codehaus.groovy.runtime.callsite.ConstructorSite$ConstructorSiteNoUnwrapNoCoerce.callConstructor(ConstructorSite.java:105); 	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallConstructor(CallSiteArray.java:60); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callConstructor(AbstractCallSite.java:235); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callConstructor(AbstractCallSite.java:255); 	at com.github.jengelman.gradle.plugins.shadow.tasks.ShadowCopyAction$StreamAction.visitFile(ShadowCopyAction.groovy:185); 	at sun.reflect.GeneratedMethodAccessor34.invoke(Unknown Source); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.codehaus.groovy.runtime.callsite.PogoMetaMethodSite$PogoCachedMethodSiteNoUnwrapNoCoerce.invoke(PogoMetaMethodSite.java:210); 	at org.codehaus.groovy.runtime.callsite.PogoMetaMethodSite.callCurrent(PogoMetaMethodSite.java:59); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:166); 	at com.github.jengelman.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5499#issuecomment-446253445:818,Cache,CachedConstructor,818,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5499#issuecomment-446253445,2,['Cache'],['CachedConstructor']
Performance,a:206); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.doAdvance(StreamSpliterators.java:169); at java.util.stream.StreamSpliterators$WrappingSpliterator.tryAdvance(StreamSpliterators.java:300); at java.util.Spliterators$1Adapter.hasNext(Spliterators.java:681); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:213); at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:935); at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:926); at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:866); at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:926); at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:670); at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330); at org.apache.spark.rdd.RDD.iterator(RDD.scala:281); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3013#issuecomment-308145149:4244,concurren,concurrent,4244,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3013#issuecomment-308145149,2,['concurren'],['concurrent']
Performance,"a:217); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at org.apache.spark.storage.BlockManager.reportAllBlocks(BlockManager.scala:217); 	at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:236); 	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:522); 	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:547); 	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547); 	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547); 	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953); 	at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:547); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308); 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180); 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); 17/10/18 17:35:58 INFO BlockManagerMaster: BlockManagerMaster stopped; 17/10/18 17:35:58 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker-1,5,main]; java.lang.OutOfMemoryError: Java heap space; 	at htsjdk.samtools.BAMRecordCodec.decode(BAMRecordCodec.java:208); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.getNextRecord(BAMFileReader.java:829); 	at htsjdk.samtools.BAMFileReader$BAMFileIndexIterator.getNextRecord(BAMFileReader.java:981); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.advance(BAMFileReader",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-337554749:5461,concurren,concurrent,5461,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-337554749,1,['concurren'],['concurrent']
Performance,a:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:123); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429:10466,concurren,concurrent,10466,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429,1,['concurren'],['concurrent']
Performance,"ach which did not generate an error:. 1. imported the 10 not-reblocked gvcfs from chr16 into genomicsdb ; 2. GenotypeGVCFs with the same command line as number 3 above. . So the error appears to be related to the reblocking of the gvcfs. ```; gendb:///restricted/projectnb/kageproj/gatk/genomicsdb/genomicsDB.chr16; Using GATK jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx60g -jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar GenotypeGVCFs -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -G StandardAnnotation -G AS_StandardAnnotation -V gendb:///restricted/projectnb/kageproj/gatk/genomicsdb/genomicsDB.chr16 -L chr16:105582-211160 --use-new-qual-calculator --only-output-calls-starting-in-intervals TRUE --genomicsdb-shared-posixfs-optimizations TRUE --tmp-dir tmp -O chr16-105582-211160.vcf.gz; 07:46:18.893 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 07:46:18.944 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Aug 25, 2021 7:46:19 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 07:46:19.128 INFO GenotypeGVCFs - ------------------------------------------------------------; 07:46:19.128 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.0.0; 07:46:19.128 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 07:46:19.129 INFO GenotypeGVCFs - Executing as farrell@scc-hadoop.bu.edu on Linux v",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7437#issuecomment-905431278:1069,optimiz,optimizations,1069,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7437#issuecomment-905431278,1,['optimiz'],['optimizations']
Performance,"ack trace, not so useful... ```; 15/10/16 14:54:47 ERROR org.apache.spark.scheduler.TaskResultGetter: Could not deserialize TaskEndReason: ClassNotFound with classloader org.apache.spark.util.MutableURLClassLoader@587c290d; 15/10/16 14:54:47 ERROR org.apache.spark.scheduler.TaskResultGetter: Could not deserialize TaskEndReason: ClassNotFound with classloader org.apache.spark.util.MutableURLClassLoader@587c290d; 15/10/16 14:54:47 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 1374.2 in stage 0.0 (TID 2218, high-mem-32-4-w-14.c.genomics-pipelines.internal): UnknownReason; 15/10/16 14:54:47 WARN org.apache.spark.ThrowableSerializationWrapper: Task exception could not be deserialized; java.lang.ClassNotFoundException: htsjdk.samtools.util.RuntimeEOFException; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at java.lang.Class.forName0(Native Method); at java.lang.Class.forName(Class.java:348); at org.apache.spark.serializer.JavaDeserializationStream$$anon$1.resolveClass(JavaSerializer.scala:67); at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1613); at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1518); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1774); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:371); at org.apache.spark.ThrowableSerializationWrapper.readObject(TaskEndReason.scala:163); at sun.reflect.GeneratedMethodAccessor25.invoke(Unknown Source); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017); at java.io.ObjectInputStream.readSerialData(Obje",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1005#issuecomment-148739184:991,load,loadClass,991,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1005#issuecomment-148739184,1,['load'],['loadClass']
Performance,"adinstitute/gatk/pull/5099. With that patch, we are now retrying on `UnknownHostException`, but the retries are all failing: . ```; [August 14, 2018 7:09:18 AM UTC] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 896.64 minutes.; Runtime.totalMemory()=3966238720; java.util.concurrent.CompletionException: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: Failure while waiting for FeatureReader to initialize with exception: com.google.cloud.storage.StorageException: All 20 reopens failed. Waited a total of 1918000 ms between attempts; at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273); at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1592); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: Failure while waiting for FeatureReader to initialize with exception: com.google.cloud.storage.StorageException: All 20 reopens failed. Waited a total of 1918000 ms between attempts; at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lambda$getFeatureReadersInParallel$614(GenomicsDBImport.java:605); at java.util.LinkedHashMap.forEach(LinkedHashMap.java:684); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getFeatureReadersInParallel(GenomicsDBImport.java:600); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.createSampleToReaderMap(GenomicsDBImport.java:491); at com.intel.genomicsdb.importer.GenomicsDBImporter.lambda$null$2(GenomicsDBImporter.java:602); at java.util.concurrent.CompletableFuture$AsyncSupply.run(",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-412904420:1103,concurren,concurrent,1103,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-412904420,1,['concurren'],['concurrent']
Performance,"adoc for any class in that package. The integration test runs `com.sun.tools.javadoc.Main.execute` and asserts that the output code is zero, which does not yield a useful error message. In order to produce something more meaningful I hacked the test to output the entire `stdout` and `stderr` as follows:. ```; final StringWriter out = new StringWriter();; final PrintWriter err = new PrintWriter(out);. final int result = com.sun.tools.javadoc.Main.execute(""program"", err, err, err, ""doclet"",docArgList.toArray(new String[] {}));; err.flush(); // probably not needed; String message = out.toString(); // message contains the entire stdout and stderr of the call to execute; Assert.assertEquals(result, 0, message);; ```. The output is about 2000 lines, but a lot of it is clearly innocuous. Removing lines such as; * `2022-08-16T00:09:07.2336106Z [parsing completed 1ms]`; * `2022-08-16T00:09:07.4456202Z [loading ZipFileIndexFileObject[/jars/gatk-package-4.2.6.1-56-gad9a538-SNAPSHOT-test.jar(org/broadinstitute/hellbender/tools/walkers/variantutils/ReblockGVCFIntegrationTest.class)]]`; * `2022-08-16T00:09:07.4459732Z [loading RegularFileObject[src/main/java/org/broadinstitute/hellbender/cmdline/argumentcollections/OptionalIntervalArgumentCollection.java]]`; * `2022-08-16T00:09:07.4462012Z [parsing started RegularFileObject[src/main/java/org/broadinstitute/hellbender/cmdline/argumentcollections/OptionalReferenceInputArgumentCollection.java]]`; * 2022-08-16T00:09:07.2322755Z [loading ZipFileObject[/gatk/gatk-package-unspecified-SNAPSHOT-local.jar(htsjdk/samtools/SAMSequenceDictionary.class)]]. brings it down to 353 lines, the majority of which look like . ```2022-08-16T00:09:07.4435974Z src/main/java/org/broadinstitute/hellbender/cmdline/CommandLineProgram.java:479: error: cannot find symbol; 2022-08-16T00:09:07.4436105Z @VisibleForTesting; ```. Here's that 353-line file:. [log-no-parsing-loading.txt](https://github.com/broadinstitute/gatk/files/9355026/log-no-parsing-loading.txt)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217231488:1306,load,loading,1306,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217231488,4,['load'],['loading']
Performance,adsSpark.lambda$runTool$72eaf22$1(KeyReadsSpark.java:28); at org.broadinstitute.hellbender.tools.spark.pipelines.KeyReadsSpark$$Lambda$11/1228804001.call(Unknown Source); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1002); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1002); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:219); at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41); at org.apache.spark.scheduler.Task.run(Task.scala:64); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```. This start offset is 2176858951 (142662628213169L>>>16) - i.e. around 2GB in. I've managed to reproduce with a local program now. This reveals the following problem:. ```; Caused by: java.lang.IllegalArgumentException: Unrecognized CigarOperator: 11; at htsjdk.samtools.CigarOperator.binaryToEnum(CigarOperator.java:143); at htsjdk.samtools.BinaryCigarCodec.binaryCigarToCigarElement(BinaryCigarCodec.java:87); at htsjdk.samtools.BinaryCigarCodec.decode(BinaryCigarCodec.java:63); at htsjdk.samtools.BAMRecord.getCigar(BAMRecord.java:243); at htsjdk.samtools.SAMRecord.getUnclippedStart(SAMRecord.java:482); at org.seqdoop.hadoop_bam.TestBAMInputFormat.getUnclippedStart(TestBAMInputFormat.java:130); at org.seqdoop.hadoop_bam.TestBAMInputFormat.getStrandedUnclippedStart(TestBAMInputFormat.java:122); at org.seqdoop.hadoop_bam.TestBAMInputFormat.key(TestBAMInputFormat.java:89); ... 27 m,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1098#issuecomment-156150350:1664,concurren,concurrent,1664,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1098#issuecomment-156150350,1,['concurren'],['concurrent']
Performance,"airHMM/common_data_structure.h:94:16: error: 'isinf' was not declared in this scope, and no declarations were found by argument-dependent lookup at the point of instantiation [-fpermissive]; if (isinf(small) == -1 || isinf(big) == -1); ^; In file included from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/headers.h:27:0,; from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/common_data_structure.h:4,; from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/utils.h:4,; from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/LoadTimeInitializer.cc:1:; /usr/local/Cellar/gcc/5.3.0/include/c++/5.3.0/cmath:853:5: note: 'template<class _Tp> typename __gnu_cxx::__enable_if<std::__is_arithmetic<_Tp>::__value, int>::__type std::isinf(_Tp)' declared here, later in the translation unit; isinf(_Tp __f); ^; In file included from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/utils.h:4:0,; from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/LoadTimeInitializer.cc:1:; /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/common_data_structure.h:94:38: error: 'isinf' was not declared in this scope, and no declarations were found by argument-dependent lookup at the point of instantiation [-fpermissive]; if (isinf(small) == -1 || isinf(big) == -1); ^; In file included from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/headers.h:27:0,; from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/common_data_structure.h:4,; from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/utils.h:4,; from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/LoadTimeInitializer.cc:1:; /usr/local/Cellar/gcc/5.3.0/include/c++/5.3.0/cmath:853:5: note: 'template<class _Tp> typename __gnu_cxx::__enable_if<std::__is_arithmetic<_Tp>::__value, int>::__type std::isinf(_Tp)' declared here, later in the translation unit; isinf(_Tp __f); ^. /Users/louisb/Workspace/gatk",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1504#issuecomment-187727343:14112,Load,LoadTimeInitializer,14112,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1504#issuecomment-187727343,1,['Load'],['LoadTimeInitializer']
Performance,"airHMM/common_data_structure.h:94:16: error: 'isinf' was not declared in this scope, and no declarations were found by argument-dependent lookup at the point of instantiation [-fpermissive]; if (isinf(small) == -1 || isinf(big) == -1); ^; In file included from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/headers.h:27:0,; from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/common_data_structure.h:4,; from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/utils.h:4,; from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/LoadTimeInitializer.cc:1:; /usr/local/Cellar/gcc/5.3.0/include/c++/5.3.0/cmath:853:5: note: 'template<class _Tp> typename __gnu_cxx::__enable_if<std::__is_arithmetic<_Tp>::__value, int>::__type std::isinf(_Tp)' declared here, later in the translation unit; isinf(_Tp __f); ^; In file included from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/utils.h:4:0,; from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/LoadTimeInitializer.cc:1:; /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/common_data_structure.h:94:38: error: 'isinf' was not declared in this scope, and no declarations were found by argument-dependent lookup at the point of instantiation [-fpermissive]; if (isinf(small) == -1 || isinf(big) == -1); ^; In file included from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/headers.h:27:0,; from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/common_data_structure.h:4,; from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/utils.h:4,; from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/LoadTimeInitializer.cc:1:; /usr/local/Cellar/gcc/5.3.0/include/c++/5.3.0/cmath:853:5: note: 'template<class _Tp> typename __gnu_cxx::__enable_if<std::__is_arithmetic<_Tp>::__value, int>::__type std::isinf(_Tp)' declared here, later in the translation unit; isinf(_Tp __f); ^; In file included from /Users",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1504#issuecomment-187727343:11177,Load,LoadTimeInitializer,11177,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1504#issuecomment-187727343,1,['Load'],['LoadTimeInitializer']
Performance,"airHMM/common_data_structure.h:94:38: error: 'isinf' was not declared in this scope, and no declarations were found by argument-dependent lookup at the point of instantiation [-fpermissive]; if (isinf(small) == -1 || isinf(big) == -1); ^; In file included from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/headers.h:27:0,; from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/common_data_structure.h:4,; from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/utils.h:4,; from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/LoadTimeInitializer.cc:1:; /usr/local/Cellar/gcc/5.3.0/include/c++/5.3.0/cmath:853:5: note: 'template<class _Tp> typename __gnu_cxx::__enable_if<std::__is_arithmetic<_Tp>::__value, int>::__type std::isinf(_Tp)' declared here, later in the translation unit; isinf(_Tp __f); ^; In file included from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/utils.h:4:0,; from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/LoadTimeInitializer.cc:1:; /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/common_data_structure.h: In instantiation of 'static NUMBER ContextBase<NUMBER>::approximateLog10SumLog10(NUMBER, NUMBER) [with NUMBER = double]':; /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/common_data_structure.h:75:53: required from 'static void ContextBase<NUMBER>::initializeMatchToMatchProb() [with NUMBER = double]'; /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/common_data_structure.h:47:35: required from 'static void ContextBase<NUMBER>::initializeStaticMembers() [with NUMBER = double]'; /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/LoadTimeInitializer.cc:53:20: required from here; /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/common_data_structure.h:94:16: error: 'isinf' was not declared in this scope, and no declarations were found by argument-dependent lookup at the point of instantiation [",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1504#issuecomment-187727343:12288,Load,LoadTimeInitializer,12288,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1504#issuecomment-187727343,1,['Load'],['LoadTimeInitializer']
Performance,"alByDepth' annotations have been disabled; 22:42:22.719 INFO NativeLibraryLoader - Loading libgkl_utils.dylib from jar:file:/Users/nhomer/miniconda3/envs/bfx/share/gatk4-4.1.8.1-0/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.dylib; 22:42:22.720 INFO NativeLibraryLoader - Loading libgkl_smithwaterman.dylib from jar:file:/Users/nhomer/miniconda3/envs/bfx/share/gatk4-4.1.8.1-0/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_smithwaterman.dylib; 22:42:22.722 INFO SmithWatermanAligner - Using AVX accelerated SmithWaterman implementation; 22:42:22.724 INFO HaplotypeCallerEngine - Standard Emitting and Calling confidence set to 0.0 for reference-model confidence output; 22:42:22.724 INFO HaplotypeCallerEngine - All sites annotated with PLs forced to true for reference-model confidence output; 22:42:22.734 WARN NativeLibraryLoader - Unable to find native library: native/libgkl_pairhmm_omp.dylib; 22:42:22.734 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; 22:42:22.734 INFO NativeLibraryLoader - Loading libgkl_pairhmm.dylib from jar:file:/Users/nhomer/miniconda3/envs/bfx/share/gatk4-4.1.8.1-0/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_pairhmm.dylib; 22:42:22.748 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 22:42:22.748 WARN IntelPairHmm - Ignoring request for 4 threads; not using OpenMP implementation; 22:42:22.748 INFO PairHMM - Using the AVX-accelerated native PairHMM implementation; 22:42:22.751 WARN GATKVariantContextUtils - Can't determine output variant file format from output file extension ""bam"". Defaulting to VCF.; 22:42:22.776 INFO ProgressMeter - Starting traversal; 22:42:22.777 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x000000010f47efd3, pid=96919, tid=0x0000000000002303; #; # JRE version: OpenJDK",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6733#issuecomment-667631737:3527,multi-thread,multi-threaded,3527,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6733#issuecomment-667631737,1,['multi-thread'],['multi-threaded']
Performance,amF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2ZpbHRlcnMvVmFyaWFudEZpbHRyYXRpb24uamF2YQ==) | `92.593% <ø> (ø)` | |; | [...der/tools/walkers/variantutils/SelectVariants.java](https://codecov.io/gh/broadinstitute/gatk/pull/8092?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhcmlhbnR1dGlscy9TZWxlY3RWYXJpYW50cy5qYXZh) | `83.911% <86.765%> (+2.516%)` | :arrow_up: |; | [...rs/variantutils/SelectVariantsIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/8092?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhcmlhbnR1dGlscy9TZWxlY3RWYXJpYW50c0ludGVncmF0aW9uVGVzdC5qYXZh) | `96.928% <94.702%> (-0.783%)` | :arrow_down: |; | [.../scalable/data/LabeledVariantAnnotationsDatum.java](https://codecov.io/gh/broadinstitute/gatk/pull/8092?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvZGF0YS9MYWJlbGVkVmFyaWFudEFubm90YXRpb25zRGF0dW0uamF2YQ==) | `68.421% <0.000%> (-3.801%)` | :arrow_down: |; | [...rs/vqsr/scalable/TrainVariantAnnotationsModel.java](https://codecov.io/gh/broadinstitute/gatk/pull/8092?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvVHJhaW5WYXJpYW50QW5ub3RhdGlvbnNNb2RlbC5qYXZh) | `77.778% <0.000%> (-2.991%)` | :arrow_down: |; | [...bender/utils/runtime/AsynchronousStreamWriter.java](https://codecov.io/gh/broadinstitute/gatk/pull/8092?src=pr&el=tree&utm_medium=referral&utm_source=githu,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8092#issuecomment-1374581874:2821,scalab,scalable,2821,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8092#issuecomment-1374581874,1,['scalab'],['scalable']
Performance,"ampleIndex);; final int[] indexesToRemove = evidences.stream().mapToInt(e -> {; final int index = evidenceIndexes.getInt(e);; if (index == MISSING_INDEX) {; throw new IllegalArgumentException(""evidence provided is not in sample"");; }; ```; We get an error when `evidenceIndexBySampleIndex(sampleIndex)` yields a `Map` that for some reason doesn't contain a read that it should. So let's investigate `evidenceIndexBySampleIndex()`. This method returns the `evidenceIndexBySampleIndex.get(sampleIndex)` field if it is not `null` (ie uninitialized); otherwise it fills it and then returns it. The code for filling it seems fine, and it explicitly loops over every sample read, so it's hard to see that the error could come from there. It seems rather that the problem is in returning the cached value whenever it is not `null`. The cached value of `evidenceIndexBySampleIndex.get(sampleIndex)` becomes invalid whenever reads are added or removed. However, you can check all the accesses of `evidenceIndexBySampleIndex` (there are only six) and verify that the class never accounts for this. So, suppose that an `AlleleLikelihoods` object invokes `evidenceIndexBySampleIndex(sampleIndex)` more than once and adds or removes reads between these. The second call returns the cached map from the first call, which is bogus. Even if it doesn't explain this issue, it is a bug. Now let's think about which public methods `evidenceIndexBySampleIndex(sampleIndex)` is called in and where this occurs in HaplotypeCaller:. * `addEvidence` (in HC this happens only in the likelihoods for annotations, downstream of our issue, so this is not the culprit).; * `filterPoorlyModeledEvidence` (this happens after Pair-HMM to the haplotype likelihoods, so not the culprit either); * `contaminationDownsampling`; * `retainEvidence` (hmmm in HC `readAlleleLikelihoods.retainEvidence(variantCallingRelevantOverlap::overlaps);` occurs immediately before `contaminationDownsampling`); * `indexOfEvidence` (nothing stands out)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6586#issuecomment-625021336:1460,cache,cached,1460,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6586#issuecomment-625021336,1,['cache'],['cached']
Performance,"andemRepeat`, `VariantOverlapAnnotator`, `GenotypingEngine`,`AlleleFrequencyCalculator`, `AssemblyRegionTrimmer`, `PartiallyDeterminedHaplotypedComputationEngine`, `RampedHaplotypeCallerEngine`, `ReadThreadingAssembler`, `SomaticGenotypingEngine`, `HaplotypeBasedVariantRecaller`, `PartiallyDeterminedHaplotype`: the only real change in these classes is using `Event` instead of `VariantContext` in places where only a lightweight container for locus/ref/alt is required. Some of the diffs are big but trivial. (In `AssemblyRegionTrimmer` I also deleted the unused `nonVariantTargetRegions` method).; - `AlleleAndContext` and `LocationAndAlleles` were deleted as the new `Event` class makes them obsolete.; - `AlleleFiltering` is a big diff but it's just a bunch of replacing `AlleleAndContext` with `Event`.; - `HaplotypeCallerGenotypingEngine`: some trivial renaming and use of `Event` instead of `VariantContext`.; - `PileupDetectionArgumentCollection`: fixed a typo.; - `LeftAlignAndTrimVariants`: just some optimized imports.; - `Event`: this is the new class at the heart of the PR.; - `EventMap`: lighter than it was. Storing a haplotype and other state were replaced by a static method to extract events from a haplotype. Using `Event` instead of `VariantContext` allowed several things to be written more concisely, but no big-picture changes. I didn't like how `buildEventMapForHaplotypes` both returned the set of event positions and filled in the haplotype's events as a side efffect, so I split it into a void method for filling events and another method for extracting start positions.; - `AlleleFilteringUnitTest`, `AssemblyBasedCallerUtilsUnitTest`, `HaplotypeCallerGenotypingEngineUnitTest.java`, `PartiallyDeterminedHaplotypeComputationEngineUnitTest`, `EventMapUnitTest`: everything that was tested before is still tested now. Lots of replacing `VariantContext` with `Event`. Deleted tests for a few unused methods here and there.; - The exact match expected VCF outputs in `Haplot",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8332#issuecomment-1574146393:1223,optimiz,optimized,1223,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8332#issuecomment-1574146393,1,['optimiz'],['optimized']
Performance,"apis.com/hellbender-test-logs/build_reports/master_32072.5/tests/test/index.html). Example error:. ```; org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException: A nack was received from the Python process (most likely caused by a raised exception caused by): nkm received. : Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/vqsr_cnn/vqsr_cnn/models.py"", line 26, in start_session_get_args_and_model; return args_and_model_from_semantics(semantics_json, weights_hd5, tensor_type); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/vqsr_cnn/vqsr_cnn/models.py"", line 33, in args_and_model_from_semantics; model = set_args_and_get_model_from_semantics(args, semantics_json, weights_hd5); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/vqsr_cnn/vqsr_cnn/models.py"", line 90, in set_args_and_get_model_from_semantics; model = load_model(weights_hd5, custom_objects=get_metric_dict(args.labels)); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/keras/engine/saving.py"", line 419, in load_model; model = _deserialize_model(f, custom_objects, compile); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/keras/engine/saving.py"", line 224, in _deserialize_model; model_config = json.loads(model_config.decode('utf-8')); AttributeError: 'str' object has no attribute 'decode'. 	at org.broadinstitute.hellbender.utils.python.StreamingPythonScriptExecutor.waitForAck(StreamingPythonScriptExecutor.java:222); 	at org.broadinstitute.hellbender.utils.python.StreamingPythonScriptExecutor.sendSynchronousCommand(StreamingPythonScriptExecutor.java:183); 	at org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants.initializePythonArgsAndModel(CNNScoreVariants.java:557); 	at org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants.onTraversalStart(CNNScoreVariants.java:317); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1052); ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6718#issuecomment-724836660:1509,load,loads,1509,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6718#issuecomment-724836660,1,['load'],['loads']
Performance,"ark - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 16:55:20.229 INFO BwaAndMarkDuplicatesPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:55:20.229 INFO BwaAndMarkDuplicatesPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - Deflater: IntelDeflater; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - Inflater: IntelInflater; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - Initializing engine; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.varia.NullAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.varia.NullAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantia",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312229998:4711,load,loaded,4711,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312229998,1,['load'],['loaded']
Performance,"asspaths.; ....... **the spark-shell**; bash-4.2$ spark-shell; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; Failed to created SparkJLineReader: java.io.IOException: Permission denied; Falling back to SimpleReader.; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /___/ .__/\_,_/_/ /_/\_\ version 1.6.0; /_/. Using Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_91); Type in expressions to have them evaluated.; Type :help for more information.; Spark context available as sc (master = yarn-client, app id = application_1507683879816_0007).; Wed Oct 11 14:25:24 CST 2017 Thread[main,5,main] java.io.FileNotFoundException: derby.log (Permission denied); ----------------------------------------------------------------; Wed Oct 11 14:25:24 CST 2017:; Booting Derby version The Apache Software Foundation - Apache Derby - 10.11.1.1 - (1616546): instance a816c00e-015f-0a1b-f1bd-00002ce33928 ; on database directory /tmp/spark-98953d35-8594-4907-b4a5-0870f1d17b3e/metastore with class loader sun.misc.Launcher$AppClassLoader@5c647e05 ; Loaded from file:/opt/cloudera/parcels/CDH-5.12.1-1.cdh5.12.1.p0.3/jars/derby-10.11.1.1.jar; java.vendor=Oracle Corporation; java.runtime.version=1.8.0_91-b14; user.dir=/opt/Software/gatk; os.name=Linux; os.arch=amd64; os.version=3.10.0-514.el7.x86_64; derby.system.home=null; Database Class Loader started - derby.database.classpath=''; 17/10/11 14:25:33 WARN metastore.ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.1.0-cdh5.12.1; 17/10/11 14:25:33 WARN metastore.ObjectStore: Failed to get database default, returning NoSuchObjectException; SQL context available as sqlContext. **./gradlew bundle**; **[root@com1 gatk]# ./gradlew bundle; when I executed the command ”./gradlew bundle”， it appeared the error in the last ，did this matter？**. .......; [loading ZipFileIndexFileObject",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-335696240:1998,load,loader,1998,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-335696240,1,['load'],['loader']
Performance,"async_io_write_tribble=false -Dsamjdk.compression_level=2 /share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-spark.jar CountReadsSpark --input /project/casa/gcad/test/HG04302.alt_bwamem_GRCh38DH.20150718.GBR.low_coverage.cram --reference file:///restricted/projectnb/casa/wgs.hg38/pipelines/hc/cram.test/GRCh38_full_analysis_set_plus_decoy_hla.fa.gz --spark-master yarn; 2019-01-09 13:35:04 WARN SparkConf:66 - The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 2019-01-09 13:35:05 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 13:35:09.640 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 13:35:09.799 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 13:35:11.507 INFO CountReadsSpark - ------------------------------------------------------------; 13:35:11.508 INFO CountReadsSpark - The Genome Analysis Toolkit (GATK) v4.0.12.0; 13:35:11.508 INFO CountReadsSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:35:11.508 INFO CountReadsSpark - Executing as farrell@scc-hadoop.bu.edu on Linux v2.6.32-754.6.3.el6.x86_64 amd64; 13:35:11.508 INFO CountReadsSpark - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 13:35:11.509 INFO CountReadsSpark - Start Date/Time: January 9, 2019 1:35:09 PM EST; 13:35:11.509 INFO CountReadsSpark - ------------------------------------------------------------; 13:35:11.509 INFO CountReadsSpark - ------------------------------------------------------------; 13:35:11.510 INFO CountReadsSpark - HTSJDK Ver",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:2282,Load,Loading,2282,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,1,['Load'],['Loading']
Performance,"atedException; 15 Feb 2022 15:46:19,123 DEBUG: 		at htsjdk.samtools.util.BlockCompressedOutputStream.close(BlockCompressedOutputStream.java:355); 15 Feb 2022 15:46:19,128 DEBUG: 		at htsjdk.samtools.util.BlockCompressedOutputStream.close(BlockCompressedOutputStream.java:333); 15 Feb 2022 15:46:19,150 DEBUG: 		at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.close(IndexingVariantContextWriter.java:172); 15 Feb 2022 15:46:19,160 DEBUG: 		at htsjdk.variant.variantcontext.writer.VCFWriter.close(VCFWriter.java:233); 15 Feb 2022 15:46:19,169 DEBUG: 		at org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs.closeTool(GenotypeGVCFs.java:297); 15 Feb 2022 15:46:19,180 DEBUG: 		at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1091); 15 Feb 2022 15:46:19,185 DEBUG: 		at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); 15 Feb 2022 15:46:19,190 DEBUG: 		at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 15 Feb 2022 15:46:19,212 DEBUG: 		at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 15 Feb 2022 15:46:19,217 DEBUG: 		at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 15 Feb 2022 15:46:19,223 DEBUG: 		at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 15 Feb 2022 15:46:19,229 DEBUG: 		at org.broadinstitute.hellbender.Main.main(Main.java:289); 15 Feb 2022 15:46:19,235 DEBUG: 	Caused by: java.lang.ClassNotFoundException: htsjdk.samtools.FileTruncatedException; 15 Feb 2022 15:46:19,242 DEBUG: 		at java.net.URLClassLoader.findClass(URLClassLoader.java:381); 15 Feb 2022 15:46:19,249 DEBUG: 		at java.lang.ClassLoader.loadClass(ClassLoader.java:424); 15 Feb 2022 15:46:19,258 DEBUG: 		at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); 15 Feb 2022 15:46:19,265 DEBUG: 		at java.lang.ClassLoader.loadClass(ClassLoader.java:357); ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7675#issuecomment-1040915714:2403,load,loadClass,2403,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7675#issuecomment-1040915714,3,['load'],['loadClass']
Performance,"ation true --fixedChunkSize 100000 --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false --disableAllReadFilters false; [September 16, 2016 4:55:32 PM EDT] Executing as kh3@rgcaahauva08091.rgc.aws.com on Linux 3.13.0-91-generic amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_101-b13; Version: Version:4.alpha.2-45-ga30af5a-SNAPSHOT; 16:55:32.335 INFO BwaSpark - Defaults.BUFFER_SIZE : 131072; 16:55:32.335 INFO BwaSpark - Defaults.COMPRESSION_LEVEL : 1; 16:55:32.335 INFO BwaSpark - Defaults.CREATE_INDEX : false; 16:55:32.335 INFO BwaSpark - Defaults.CREATE_MD5 : false; 16:55:32.335 INFO BwaSpark - Defaults.CUSTOM_READER_FACTORY : ; 16:55:32.335 INFO BwaSpark - Defaults.EBI_REFERENCE_SERVICE_URL_MASK : http://www.ebi.ac.uk/ena/cram/md5/%s; 16:55:32.335 INFO BwaSpark - Defaults.NON_ZERO_BUFFER_SIZE : 131072; 16:55:32.335 INFO BwaSpark - Defaults.REFERENCE_FASTA : null; 16:55:32.335 INFO BwaSpark - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 16:55:32.336 INFO BwaSpark - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:55:32.336 INFO BwaSpark - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:55:32.336 INFO BwaSpark - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:55:32.336 INFO BwaSpark - Defaults.USE_CRAM_REF_DOWNLOAD : false; 16:55:32.336 INFO BwaSpark - Deflater IntelDeflater; 16:55:32.336 INFO BwaSpark - Initializing engine; 16:55:32.336 INFO BwaSpark - Done initializing engine; 16:55:32.757 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 16:55:34.473 INFO BwaSpark - Shutting down engine; [September 16, 2016 4:55:34 PM EDT] org.broadinstitute.hellbender.tools.spark.bwa.BwaSpark done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=499646464. ---. null. ---",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2171#issuecomment-247709200:2787,load,load,2787,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2171#issuecomment-247709200,1,['load'],['load']
Performance,"atk/genomicsdb/genomicsDB.chr16; Using GATK jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx60g -jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar GenotypeGVCFs -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -G StandardAnnotation -G AS_StandardAnnotation -V gendb:///restricted/projectnb/kageproj/gatk/genomicsdb/genomicsDB.chr16 -L chr16:105582-211160 --use-new-qual-calculator --only-output-calls-starting-in-intervals TRUE --genomicsdb-shared-posixfs-optimizations TRUE --tmp-dir tmp -O chr16-105582-211160.vcf.gz; 07:46:18.893 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 07:46:18.944 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Aug 25, 2021 7:46:19 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 07:46:19.128 INFO GenotypeGVCFs - ------------------------------------------------------------; 07:46:19.128 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.0.0; 07:46:19.128 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 07:46:19.129 INFO GenotypeGVCFs - Executing as farrell@scc-hadoop.bu.edu on Linux v3.10.0-1160.36.2.el7.x86_64 amd64; 07:46:19.129 INFO GenotypeGVCFs - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 07:46:19.129 INFO GenotypeGVCFs - Start Date/Time: August 25, 2021 7:46:18 AM EDT; 07:46:19.129 INFO GenotypeGVCFs - ---------------------------------",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7437#issuecomment-905431278:1317,Load,Loading,1317,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7437#issuecomment-905431278,1,['Load'],['Loading']
Performance,"ator.scala:1336); 	at org.apache.spark.storage.BlockManager.reportAllBlocks(BlockManager.scala:217); 	at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:236); 	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:522); 	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:547); 	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547); 	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547); 	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953); 	at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:547); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308); 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180); 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); 17/10/18 17:35:58 INFO BlockManagerMaster: BlockManagerMaster stopped; 17/10/18 17:35:58 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker-1,5,main]; java.lang.OutOfMemoryError: Java heap space; 	at htsjdk.samtools.BAMRecordCodec.decode(BAMRecordCodec.java:208); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.getNextRecord(BAMFileReader.java:829); 	at htsjdk.samtools.BAMFileReader$BAMFileIndexIterator.getNextRecord(BAMFileReader.java:981); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.advance(BAMFileReader.java:803); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.java:797); 	at htsjdk.samtools.BAMFileRead",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-337554749:5584,concurren,concurrent,5584,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-337554749,1,['concurren'],['concurrent']
Performance,"aught exception in thread Thread[Executor task launch worker-0,5,main]; java.lang.IncompatibleClassChangeError: Found class org.apache.hadoop.mapreduce.TaskAttemptContext, but interface was expected; at com.cloudera.dataflow.spark.TemplatedTextOutputFormat.getOutputFile(TemplatedTextOutputFormat.java:50); at com.cloudera.dataflow.spark.TemplatedTextOutputFormat.getDefaultWorkFile(TemplatedTextOutputFormat.java:46); at org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:125); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$12.apply(PairRDDFunctions.scala:995); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$12.apply(PairRDDFunctions.scala:979); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61); at org.apache.spark.scheduler.Task.run(Task.scala:64); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 15/07/14 13:14:53 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 1.0 (TID 1, localhost): java.lang.IncompatibleClassChangeError: Found class org.apache.hadoop.mapreduce.TaskAttemptContext, but interface was expected; at com.cloudera.dataflow.spark.TemplatedTextOutputFormat.getOutputFile(TemplatedTextOutputFormat.java:50); at com.cloudera.dataflow.spark.TemplatedTextOutputFormat.getDefaultWorkFile(TemplatedTextOutputFormat.java:46); at org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:125); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$12.apply(PairRDDFunctions.scala:995); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$12.apply(PairRDDFunctions.scala:979); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61); at org.apache.spark.scheduler.Task.run(Task.scala:64); at org.apache.spark.executor.Executor$TaskRun",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/639#issuecomment-121313713:30659,concurren,concurrent,30659,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/639#issuecomment-121313713,1,['concurren'],['concurrent']
Performance,"avadoc.doclet](https://docs.oracle.com/en/java/javase/11/docs/api/jdk.javadoc/jdk/javadoc/doclet/package-summary.html). The javadoc tools in `org.broadinstitute.hellbender.utils.help` may need to be re-written (and it's not clear if it's possible to support Java 8 and Java 11 simultaneously).; * Travis build. Getting this to build and test on Java 11 in addition to the current builds may be fairly involved as the matrix is already quite complicated. (The current PR just changes Java 8 to Java 11 for testing purposes - we'd need a way of getting both to run.). The vast majority of tests are passing on Java 11, the following are failing:; * Missing `TwoBitRecord` (from ADAM); * `ReferenceMultiSparkSourceUnitTest`; * `ImpreciseVariantDetectorUnitTest`; * `SVVCFWriterUnitTest`; * `DiscoverVariantsFromContigAlignmentsSAMSparkIntegrationTest`; * `StructuralVariationDiscoveryPipelineSparkIntegrationTest`; * `SvDiscoverFromLocalAssemblyContigAlignmentsSparkIntegrationTest`; * `java.lang.NoSuchMethodError: java.nio.ByteBuffer.clear()Ljava/nio/ByteBuffer;`; * `SeekableByteChannelPrefetcherTest`; * `GatherVcfsCloudIntegrationTest`; * `Could not serialize lambda`; * `ExampleAssemblyRegionWalkerSparkIntegrationTest`; * `PileupSparkIntegrationTest`; * Native HMM library code caused the tests to crash on my Mac:; ```; Running Test: Test method testLikelihoodsFromHaplotypes[0](org.broadinstitute.hellbender.utils.pairhmm.VectorLoglessPairHMM@6282d367, true)(org.broadinstitute.hellbender.utils.pairhmm.VectorPairHMMUnitTest); dyld: lazy symbol binding failed: can't resolve symbol __ZN13shacc_pairhmm9calculateERNS_5BatchE in /private/var/folders/cj/wyp4zgw17vj4m9qdmddvmcc80000gn/T/libgkl_pairhmm13775554937319419112.dylib because dependent dylib #1 could not be loaded; dyld: can't resolve symbol __ZN13shacc_pairhmm9calculateERNS_5BatchE in /private/var/folders/cj/wyp4zgw17vj4m9qdmddvmcc80000gn/T/libgkl_pairhmm13775554937319419112.dylib because dependent dylib #1 could not be loaded; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6119#issuecomment-527179359:2536,load,loaded,2536,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6119#issuecomment-527179359,2,['load'],['loaded']
Performance,"ax"": ""3.701625"",; ""NIST controlMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/89508d5f-29f1-4534-9fe1-220a80de17c4/call-NISTSampleHeadToHead/BenchmarkComparison/338d644e-3327-471e-9d17-1c103fa5e01e/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""NIST controlindelF1Score"": ""0.9902"",; ""NIST controlindelPrecision"": ""0.9903"",; ""NIST controlsnpF1Score"": ""0.9899"",; ""NIST controlsnpPrecision"": ""0.9887"",; ""NIST controlsnpRecall"": ""0.9911"",; ""NIST controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/89508d5f-29f1-4534-9fe1-220a80de17c4/call-NISTSampleHeadToHead/BenchmarkComparison/338d644e-3327-471e-9d17-1c103fa5e01e/call-BenchmarkVCFControlSample/Benchmark/145d88de-5810-47e1-972a-18ff0169fe27/call-CombineSummaries/summary.csv"",; ""NIST evalHCprocesshours"": ""92.82975"",; ""NIST evalHCsystemhours"": ""0.17177777777777778"",; ""NIST evalHCwallclockhours"": ""66.4404388888889"",; ""NIST evalHCwallclockmax"": ""3.325327777777778"",; ""NIST evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/89508d5f-29f1-4534-9fe1-220a80de17c4/call-NISTSampleHeadToHead/BenchmarkComparison/338d644e-3327-471e-9d17-1c103fa5e01e/call-EVALRuntimeTask/monitoring.pdf"",; ""NIST evalindelF1Score"": ""0.9902"",; ""NIST evalindelPrecision"": ""0.9903"",; ""NIST evalsnpF1Score"": ""0.9899"",; ""NIST evalsnpPrecision"": ""0.9887"",; ""NIST evalsnpRecall"": ""0.9911"",; ""NIST evalsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/89508d5f-29f1-4534-9fe1-220a80de17c4/call-NISTSampleHeadToHead/BenchmarkComparison/338d644e-3327-471e-9d17-1c103fa5e01e/call-BenchmarkVCFTestSample/Benchmark/e37c2b01-a62d-4b8c-9fb3-6f86d8377ca7/call-CombineSummaries/summary.csv"",; ""ROC_Plots_Reported"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/89508d5f-29f1-4534-9fe1-220a80de17c4/call-CreateHTMLReport/cacheCopy/report.html""; },; ""errors"": null; } ; </pre> </details>",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7876#issuecomment-1193038382:21348,cache,cacheCopy,21348,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7876#issuecomment-1193038382,1,['cache'],['cacheCopy']
Performance,"b174:54310/GATK4TEST/BroadData/CEUTrio.HiSeq.WEx.b37.NA12892.bam -O hdfs://arlab174:54310/GATK4TEST/Output/Test_CEU_ReadsCount --sparkMaster yarn; 14:56:20.999 INFO IntelGKLUtils - Trying to load Intel GKL library from:; 	jar:file:/mnt/raid5/frankliu/code/GATK/gatk/build/libs/gatk-package-4.alpha.2-120-g00a40ea-SNAPSHOT-spark.jar!/com/intel/gkl/native/libgkl_compression.so; Exception in thread ""main"" java.lang.UnsatisfiedLinkError: /tmp/frankliu/libgkl_compression8271576113600851848.so: /tmp/frankliu/libgkl_compression8271576113600851848.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64-bit platform); 	at java.lang.ClassLoader$NativeLibrary.load(Native Method); 	at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941); 	at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824); 	at java.lang.Runtime.load0(Runtime.java:809); 	at java.lang.System.load(System.java:1086); 	at com.intel.gkl.IntelGKLUtils.load(IntelGKLUtils.java:133); 	at com.intel.gkl.compression.IntelDeflater.load(IntelDeflater.java:58); 	at com.intel.gkl.compression.IntelDeflater.load(IntelDeflater.java:53); 	at com.intel.gkl.compression.IntelDeflaterFactory.<init>(IntelDeflaterFactory.java:16); 	at com.intel.gkl.compression.IntelDeflaterFactory.<init>(IntelDeflaterFactory.java:20); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:143); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116); 	at org.broadinstitute.hellbender.Main.main(Main.java:158); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingM",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2302#issuecomment-265854885:1921,load,load,1921,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2302#issuecomment-265854885,1,['load'],['load']
Performance,"b6-04bd-4344-b4fc-8a1df66bb5d9/call-CHMSampleHeadToHead/BenchmarkComparison/79d1a2a4-6b5e-424a-8528-9059bda6db1c/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""CHM controlindelF1Score"": ""0.8724"",; ""CHM controlindelPrecision"": ""0.8814"",; ""CHM controlsnpF1Score"": ""0.9784"",; ""CHM controlsnpPrecision"": ""0.9706"",; ""CHM controlsnpRecall"": ""0.9863"",; ""CHM controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/bf86d5b6-04bd-4344-b4fc-8a1df66bb5d9/call-CHMSampleHeadToHead/BenchmarkComparison/79d1a2a4-6b5e-424a-8528-9059bda6db1c/call-BenchmarkVCFControlSample/Benchmark/3046acf7-ded7-40c8-9b7a-3826f480418f/call-CombineSummaries/summary.csv"",; ""CHM evalHCprocesshours"": ""67.35536666666667"",; ""CHM evalHCsystemhours"": ""0.1557166666666667"",; ""CHM evalHCwallclockhours"": ""42.53388888888889"",; ""CHM evalHCwallclockmax"": ""2.7197444444444443"",; ""CHM evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/bf86d5b6-04bd-4344-b4fc-8a1df66bb5d9/call-CHMSampleHeadToHead/BenchmarkComparison/79d1a2a4-6b5e-424a-8528-9059bda6db1c/call-EVALRuntimeTask/cacheCopy/monitoring.pdf"",; ""CHM evalindelF1Score"": ""0.8778"",; ""CHM evalindelPrecision"": ""0.8968"",; ""CHM evalsnpF1Score"": ""0.9813"",; ""CHM evalsnpPrecision"": ""0.9774"",; ""CHM evalsnpRecall"": ""0.9852"",; ""CHM evalsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/bf86d5b6-04bd-4344-b4fc-8a1df66bb5d9/call-CHMSampleHeadToHead/BenchmarkComparison/79d1a2a4-6b5e-424a-8528-9059bda6db1c/call-BenchmarkVCFTestSample/Benchmark/2f376005-bdfb-42bd-8736-1e6df978ab80/call-CombineSummaries/summary.csv"",; ""EXOME1 controlindelF1Score"": ""0.727"",; ""EXOME1 controlindelPrecision"": ""0.632"",; ""EXOME1 controlsnpF1Score"": ""0.9878"",; ""EXOME1 controlsnpPrecision"": ""0.9815"",; ""EXOME1 controlsnpRecall"": ""0.9941"",; ""EXOME1 controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/bf86d5b6-04bd-4344-b4fc-8a1df66bb5d9/call-EXOME1Sampl",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7723#issuecomment-1069765064:11446,cache,cacheCopy,11446,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7723#issuecomment-1069765064,1,['cache'],['cacheCopy']
Performance,back to @lbergelson for second review. log10 cache is now static (and synchronized at expansion only) as in gatk3 - it looks faster (no map lookup that would be needed in the thread local solution),MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1957#issuecomment-230860880:45,cache,cache,45,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1957#issuecomment-230860880,1,['cache'],['cache']
Performance,"be consolidated with---@jamesemery thoughts? Again, let me reiterate that it seems that many of these parameter values were chosen arbitrarily (or, if not, that the procedure for choosing them has been lost). As a start, you can see the results of some optimizations I did on the CHM mix on slide 15 at https://docs.google.com/presentation/d/1zGuquAZWSUQ-wNxp8D6HhGNjIaFcV0_X9WAS4LODbEo/edit?usp=sharing Here, I optimized over haplotype-to-reference + read-to-haplotype SW parameters on various metrics after variant normalization using vcfeval. These optimizations were done using the Bayesian optimization framework I prototyped long ago (see https://github.com/broadinstitute/gatk-evaluation/tree/master/pipeline-optimizer and https://docs.google.com/presentation/d/1t5WOAEOMp0xAzJgpKbP68BUnclNYfIVRrDSL9wl1-3A/edit?usp=sharing); this entailed running parameter scans using a local Cromwell on my desktop. Probably this optimization work could be redone relatively easily using the Neptune framework put together by @dalessioluca, which was still in development at the time I did this work. Happy to share the resources and scripts I used if we go down this route; they are pretty lightweight. See more discussion starting here: https://github.com/broadinstitute/gatk/issues/5564#issuecomment-710107566. Alternatively, we could merge this branch to expose the parameters now and punt on consolidating/optimizing them. I'm not completely convinced we should even do the former unless we are going to follow through on the latter, but happy to defer to others. Finally, note also there is one code optimization that I removed, since it makes assumptions on the SW parameter values that might not be valid for non-default values. I'll highlight this with a comment below. We can restore it if we add code to check whether the assumptions hold, but I'd be curious to see in which cases the optimization makes a big difference. See https://github.com/broadinstitute/gatk/issues/6863#issuecomment-7078703",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6885#issuecomment-891907471:2190,optimiz,optimization,2190,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6885#issuecomment-891907471,1,['optimiz'],['optimization']
Performance,"be written to forkTest/vcfheader.vcf; 16:28:04.156 INFO GenomicsDBImport - Importing to array - forkTest/genomicsdb_array; 16:28:04.158 INFO ProgressMeter - Starting traversal; 16:28:04.158 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 16:28:05.198 INFO GenomicsDBImport - Starting batch input file preload; 16:29:23.571 INFO GenomicsDBImport - Finished batch preload; 16:48:46.140 INFO GenomicsDBImport - Shutting down engine; [May 4, 2018 4:48:46 PM EDT] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 20.96 minutes.; Runtime.totalMemory()=22281715712; java.util.concurrent.CompletionException: java.lang.OutOfMemoryError: Java heap space; at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273); at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1592); at java.util.concurrent.CompletableFuture$AsyncSupply.exec(CompletableFuture.java:1582); at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056); at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692); at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157); Caused by: java.lang.OutOfMemoryError: Java heap space; at com.intel.genomicsdb.importer.SilentByteBufferStream.<init>(SilentByteBufferStream.java:55); at com.intel.genomicsdb.importer.GenomicsDBImporterStreamWrapper.<init>(GenomicsDBImporterStreamWrapper.java:70); at com.intel.genomicsdb.importer.GenomicsDBImporter.addBufferStream(GenomicsDBImporter.java:397); at com.intel.genomicsdb.importer.GenomicsDBImporter.addSortedVariantContextIterator(GenomicsDBImporter.java:358); at com.intel.genomicsdb.importer.GenomicsDBImporter.<init>(GenomicsDBImporter.java:167); at com.intel.genomicsdb.importer.GenomicsDBImporter.lambda$nul",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388079572:3780,concurren,concurrent,3780,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388079572,1,['concurren'],['concurrent']
Performance,"bender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:110); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:28); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 18/12/21 13:48:33 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.; 18/12/21 13:48:33 WARN shortcircuit.DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.; 18/12/21 13:48:37 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.; 806177853; 14:54:41.938 INFO CountReadsSpark - Shutting down engine; [December 21, 2018 2:54:41 PM EST] org.broadinstitute.hellbender.tools.spark.pipelines.CountReadsSpark done. Elapsed time: 66.18 minutes. ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-449510725:11294,load,loaded,11294,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-449510725,1,['load'],['loaded']
Performance,bender.tools.genomicsdb.GenomicsDBImport.lambda$getFeatureReadersInParallel$601(GenomicsDBImport.java:605); 	at java.util.LinkedHashMap.forEach(LinkedHashMap.java:684); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getFeatureReadersInParallel(GenomicsDBImport.java:600); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.createSampleToReaderMap(GenomicsDBImport.java:491); 	at com.intel.genomicsdb.importer.GenomicsDBImporter.lambda$null$2(GenomicsDBImporter.java:602); 	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590); 	... 3 more; Caused by: java.util.concurrent.ExecutionException: org.broadinstitute.hellbender.exceptions.UserException: Failed to create reader from gs://fc-c64a0fcd-fb30-4a7e-bdc6-3c09a9286941/f6aeb0ce-044a-4b36-a5f0-d3fda62252a5/ReblockGVCF/66c24439-cfeb-4b85-bc02-aefe693177b6/call-GenotypeGVCF/NWD197223.vcf.gz; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lambda$getFeatureReadersInParallel$601(GenomicsDBImport.java:602); 	... 8 more; Caused by: org.broadinstitute.hellbender.exceptions.UserException: Failed to create reader from gs://fc-c64a0fcd-fb30-4a7e-bdc6-3c09a9286941/f6aeb0ce-044a-4b36-a5f0-d3fda62252a5/ReblockGVCF/66c24439-cfeb-4b85-bc02-aefe693177b6/call-GenotypeGVCF/NWD197223.vcf.gz; 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getReaderFromPath(GenomicsDBImport.java:640); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lambda$getFeatureReadersInParallel$600(GenomicsDBImport.java:593); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	... 3 more; Caused by: htsjdk.tribble.TribbleException$MalformedFeatureFile: Unable to parse header with error: ComputeEngineCredentials cannot find the metadata server. This is likely because code is not running on Google Compute Engi,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-411503423:2622,concurren,concurrent,2622,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-411503423,1,['concurren'],['concurrent']
Performance,"bgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils347167544598047196.so: /tmp/libgkl_utils347167544598047196.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > 16:17:06.589 **WARN** IntelPairHmm - Intel GKL Utils not loaded; >; > 16:17:06.589 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; >; > 16:17:06.589 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; >; > 16:17:06.590 **WARN** NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils6186849302609329058.so: /tmp/libgkl_utils6186849302609329058.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > 16:17:06.590 **WARN** IntelPairHmm - Intel GKL Utils not loaded; >; > 16:17:06.591 **WARN** PairHMM - ***WARNING: Machine does not have the AVX instruction set support needed for the accelerated AVX PairHmm. Falling back to the MUCH slower LOGLESS_CACHING implementation!; >; > Since the calculation takes quite long, I checked the WARN messages of the; > output above. Especially the last one about the AVX instruction set where; > it says that a *MUCH* slower implementation will be used. From the few; > WARN messages it seems like the root cause is the failure to load libgkl; > and that again seems to be related to my platform. Does anyone know more; > about this issue or how to work around it?; >; > Best regards,; > Robert; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/6794>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AAKOLMAQSKLJ5N7SHDOJQ7DSEESQFANCNFSM4QZASPYQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600:6203,load,loaded,6203,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600,2,['load'],"['load', 'loaded']"
Performance,"bleByteChannelPrefetcher$WorkUnit.getBuf(SeekableByteChannelPrefetcher.java:136); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.fetch(SeekableByteChannelPrefetcher.java:255); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:300); 	... 44 more; Caused by: com.google.cloud.storage.StorageException: java.lang.IllegalArgumentException: Position should be non-negative, is %d [-1056964608]; 	at com.google.cloud.storage.StorageException.translateAndThrow(StorageException.java:71); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:139); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:113); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:131); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:104); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Position should be non-negative, is %d [-1056964608]; 	at shaded.cloud_nio.com.google.common.base.Preconditions.checkArgument(Preconditions.java:146); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:487); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:127); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:124); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:93); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:49); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:124); 	... 7 more; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317782472:9108,concurren,concurrent,9108,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317782472,3,['concurren'],['concurrent']
Performance,bmRlci90b29scy93YWxrZXJzL3N2L0NvbGxlY3RTVkV2aWRlbmNlLmphdmE=) | `74.888% <0.000%> (-4.990%)` | :arrow_down: |; | [.../scalable/data/LabeledVariantAnnotationsDatum.java](https://codecov.io/gh/broadinstitute/gatk/pull/8163?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvZGF0YS9MYWJlbGVkVmFyaWFudEFubm90YXRpb25zRGF0dW0uamF2YQ==) | `68.421% <0.000%> (-3.801%)` | :arrow_down: |; | [...ls/genomicsdb/GenomicsDBImportIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/8163?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9nZW5vbWljc2RiL0dlbm9taWNzREJJbXBvcnRJbnRlZ3JhdGlvblRlc3QuamF2YQ==) | `84.746% <0.000%> (-3.762%)` | :arrow_down: |; | [...vqsr/scalable/LabeledVariantAnnotationsWalker.java](https://codecov.io/gh/broadinstitute/gatk/pull/8163?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvTGFiZWxlZFZhcmlhbnRBbm5vdGF0aW9uc1dhbGtlci5qYXZh) | `86.822% <0.000%> (-3.696%)` | :arrow_down: |; | [.../walkers/haplotypecaller/graphs/InverseAllele.java](https://codecov.io/gh/broadinstitute/gatk/pull/8163?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9ncmFwaHMvSW52ZXJzZUFsbGVsZS5qYXZh) | `59.091% <0.000%> (-3.409%)` | :arrow_down: |; | ... and [104 more](https://codecov.io/gh/broadinstitute/gatk/pull/8163?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&u,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8163#issuecomment-1387550473:4765,scalab,scalable,4765,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8163#issuecomment-1387550473,1,['scalab'],['scalable']
Performance,browse/SPARK-21133. A sample error from my log:; 17/07/17 14:33:17 ERROR org.apache.spark.util.Utils: Exception encountered; java.lang.NullPointerException; 	at org.apache.spark.scheduler.HighlyCompressedMapStatus$$anonfun$writeExternal$2.apply$mcV$sp(MapStatus.scala:171); 	at org.apache.spark.scheduler.HighlyCompressedMapStatus$$anonfun$writeExternal$2.apply(MapStatus.scala:167); 	at org.apache.spark.scheduler.HighlyCompressedMapStatus$$anonfun$writeExternal$2.apply(MapStatus.scala:167); 	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303); 	at org.apache.spark.scheduler.HighlyCompressedMapStatus.writeExternal(MapStatus.scala:167); 	at java.io.ObjectOutputStream.writeExternalData(ObjectOutputStream.java:1459); 	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1430); 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178); 	at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1378); 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174); 	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348); 	at org.apache.spark.MapOutputTracker$$anonfun$serializeMapStatuses$1.apply$mcV$sp(MapOutputTracker.scala:617); 	at org.apache.spark.MapOutputTracker$$anonfun$serializeMapStatuses$1.apply(MapOutputTracker.scala:616); 	at org.apache.spark.MapOutputTracker$$anonfun$serializeMapStatuses$1.apply(MapOutputTracker.scala:616); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337); 	at org.apache.spark.MapOutputTracker$.serializeMapStatuses(MapOutputTracker.scala:619); 	at org.apache.spark.MapOutputTrackerMaster.getSerializedMapOutputStatuses(MapOutputTracker.scala:562); 	at org.apache.spark.MapOutputTrackerMaster$MessageLoop.run(MapOutputTracker.scala:351); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748),MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3290#issuecomment-315846491:1912,concurren,concurrent,1912,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3290#issuecomment-315846491,2,['concurren'],['concurrent']
Performance,btw to lower the JNI cost we may want to batch calls to bwa-mem. Need to measure performance and see,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1517#issuecomment-188412966:81,perform,performance,81,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1517#issuecomment-188412966,1,['perform'],['performance']
Performance,"c VCFCodec to read file file:///home/robert/test/snps.vcf; >; > 16:17:06.539 INFO IntervalArgumentCollection - Processing 61464 bp from intervals; >; > 16:17:06.551 INFO HaplotypeCaller - Done initializing engine; >; > 16:17:06.573 INFO HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output; >; > 16:17:06.588 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; >; > 16:17:06.589 **WARN** NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils347167544598047196.so: /tmp/libgkl_utils347167544598047196.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > 16:17:06.589 **WARN** IntelPairHmm - Intel GKL Utils not loaded; >; > 16:17:06.589 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; >; > 16:17:06.589 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; >; > 16:17:06.590 **WARN** NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils6186849302609329058.so: /tmp/libgkl_utils6186849302609329058.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > 16:17:06.590 **WARN** IntelPairHmm - Intel GKL Utils not loaded; >; > 16:17:06.591 **WARN** PairHMM - ***WARNING: Machine does not have the AVX instruction set support needed for the accelerated AVX PairHmm. Falling back to the MUCH slower LOGLESS_CACHING implementation!; >; > Since the calculation takes quite long, I checked the WARN messages of the; > output above. Especially the last one about the AVX instruction set where; > it says th",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600:5555,multi-thread,multi-threaded,5555,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600,1,['multi-thread'],['multi-threaded']
Performance,"c facing evaluations.; - Some hyperparameter tweaking was necessary to achieve good performance. Hyperparameters changed were contained mostly only to `psi_t` parameter.; - We developed a clustering procedure that is based on coverage profile at the set of targets that are highly variable across different capture kits. ; - We found that filtering on a QS metric on a final callset significantly boosted the specificity while lowering sensitivity insignificantly.; - We developed a hyperparameter optimization framework prototype that could be used in a future for general optimizations of cost/performance parameters for all GATK pipelines.; - We resolved several memory issues that came up during validations. **A few issues were encountered along the way:**; - The sensitivity and specificity on multiallellic (common) sites was significantly lower than on rare events.; - Single target calling sensitivity was lower than 20%.; - Pipeline WDL required optimization in order to handle whole genome data, however these changes were not consolidated in the official WDL. **Currently the ongoing work is focused on the following:**; - Improving sensitivity/specificity of calls on common regions. One solution being tested involves setting a prior for common regions derived from a high quality callset. Second solution is to set a different filtering threshold for common regions.; - Consolidating validation scripts to process gCNV output and outputs of competing tools measure their performances against ground truth.; - Analyzing 1000 Genomes exomes, which could be potentially used for public facing automatic evaluations. **The following items are necessary done for automatic evaluation:** ; - Dataset + truth. We need an access to a high quality public cohort with matched whole genomes. These genomes have to have a corresponding high quality truth set generated from split-read/read-pair methods. From that cohort we need to find 50-200 relatively homogeneous samples.; - An established vali",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4123#issuecomment-532500502:1324,optimiz,optimization,1324,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4123#issuecomment-532500502,1,['optimiz'],['optimization']
Performance,"c void ContextBase<NUMBER>::initializeMatchToMatchProb() [with NUMBER = float]'; /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/common_data_structure.h:47:35: required from 'static void ContextBase<NUMBER>::initializeStaticMembers() [with NUMBER = float]'; /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/LoadTimeInitializer.cc:52:19: required from here; /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/common_data_structure.h:94:16: error: 'isinf' was not declared in this scope, and no declarations were found by argument-dependent lookup at the point of instantiation [-fpermissive]; if (isinf(small) == -1 || isinf(big) == -1); ^; In file included from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/headers.h:27:0,; from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/common_data_structure.h:4,; from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/utils.h:4,; from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/LoadTimeInitializer.cc:1:; /usr/local/Cellar/gcc/5.3.0/include/c++/5.3.0/cmath:853:5: note: 'template<class _Tp> typename __gnu_cxx::__enable_if<std::__is_arithmetic<_Tp>::__value, int>::__type std::isinf(_Tp)' declared here, later in the translation unit; isinf(_Tp __f); ^; In file included from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/utils.h:4:0,; from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/LoadTimeInitializer.cc:1:; /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/common_data_structure.h:94:38: error: 'isinf' was not declared in this scope, and no declarations were found by argument-dependent lookup at the point of instantiation [-fpermissive]; if (isinf(small) == -1 || isinf(big) == -1); ^; In file included from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/headers.h:27:0,; from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/common_data_structure.h:4,; from /User",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1504#issuecomment-187727343:10734,Load,LoadTimeInitializer,10734,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1504#issuecomment-187727343,1,['Load'],['LoadTimeInitializer']
Performance,"c/user/farrell/.sparkStaging/application_1542127286896_0153/__spark_libs__7473738539612638927.zip; 2019-01-07 11:33:38 INFO Client:54 - Uploading resource file:/tmp/spark-1ac79f09-1a36-4668-92d9-0739775f98ed/__spark_conf__4147634812449814799.zip -> hdfs://scc/user/farrell/.sparkStaging/application_1542127286896_0153/__spark_conf__.zip; 2019-01-07 11:33:38 INFO SecurityManager:54 - Changing view acls to: farrell; 2019-01-07 11:33:38 INFO SecurityManager:54 - Changing modify acls to: farrell; 2019-01-07 11:33:38 INFO SecurityManager:54 - Changing view acls groups to:; 2019-01-07 11:33:38 INFO SecurityManager:54 - Changing modify acls groups to:; 2019-01-07 11:33:38 INFO SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(farrell); groups with view permissions: Set(); users with modify permissions: Set(farrell); groups with modify permissions: Set(); 2019-01-07 11:33:38 INFO Client:54 - Submitting application application_1542127286896_0153 to ResourceManager; 2019-01-07 11:33:38 INFO YarnClientImpl:251 - Submitted application application_1542127286896_0153; 2019-01-07 11:33:38 INFO SchedulerExtensionServices:54 - Starting Yarn extension services with app application_1542127286896_0153 and attemptId None; 2019-01-07 11:33:39 INFO Client:54 - Application report for application_1542127286896_0153 (state: ACCEPTED); 2019-01-07 11:33:39 INFO Client:54 -; client token: Token { kind: YARN_CLIENT_TOKEN, service: }; diagnostics: N/A; ApplicationMaster host: N/A; ApplicationMaster RPC port: -1; queue: default; start time: 1546878818531; final status: UNDEFINED; tracking URL: https://scc-hsn1.scc.bu.edu:8090/proxy/application_1542127286896_0153/; user: farrell; 2019-01-07 11:33:40 INFO Client:54 - Application report for application_1542127286896_0153 (state: ACCEPTED); 2019-01-07 11:33:41 INFO Client:54 - Application report for application_1542127286896_0153 (state: ACCEPTED); 2019-01-07 11:33:42 INFO Client:54 - Applic",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:13432,queue,queue,13432,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,1,['queue'],['queue']
Performance,"c/user/farrell/.sparkStaging/application_1542127286896_0166/__spark_libs__7821719163562430010.zip; 2019-01-09 13:35:22 INFO Client:54 - Uploading resource file:/tmp/spark-69cc5c72-eff6-4259-8b3b-12fa6f8c42b0/__spark_conf__4520928824604875683.zip -> hdfs://scc/user/farrell/.sparkStaging/application_1542127286896_0166/__spark_conf__.zip; 2019-01-09 13:35:22 INFO SecurityManager:54 - Changing view acls to: farrell; 2019-01-09 13:35:22 INFO SecurityManager:54 - Changing modify acls to: farrell; 2019-01-09 13:35:22 INFO SecurityManager:54 - Changing view acls groups to:; 2019-01-09 13:35:22 INFO SecurityManager:54 - Changing modify acls groups to:; 2019-01-09 13:35:22 INFO SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(farrell); groups with view permissions: Set(); users with modify permissions: Set(farrell); groups with modify permissions: Set(); 2019-01-09 13:35:22 INFO Client:54 - Submitting application application_1542127286896_0166 to ResourceManager; 2019-01-09 13:35:22 INFO YarnClientImpl:251 - Submitted application application_1542127286896_0166; 2019-01-09 13:35:22 INFO SchedulerExtensionServices:54 - Starting Yarn extension services with app application_1542127286896_0166 and attemptId None; 2019-01-09 13:35:23 INFO Client:54 - Application report for application_1542127286896_0166 (state: ACCEPTED); 2019-01-09 13:35:23 INFO Client:54 -; client token: Token { kind: YARN_CLIENT_TOKEN, service: }; diagnostics: N/A; ApplicationMaster host: N/A; ApplicationMaster RPC port: -1; queue: default; start time: 1547058922320; final status: UNDEFINED; tracking URL: https://scc-hsn1.scc.bu.edu:8090/proxy/application_1542127286896_0166/; user: farrell; 2019-01-09 13:35:24 INFO Client:54 - Application report for application_1542127286896_0166 (state: ACCEPTED); 2019-01-09 13:35:25 INFO Client:54 - Application report for application_1542127286896_0166 (state: ACCEPTED); 2019-01-09 13:35:26 INFO Client:54 - Applic",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:13171,queue,queue,13171,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,1,['queue'],['queue']
Performance,c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvbW9kZWxpbmcvQkdNTVZhcmlhbnRBbm5vdGF0aW9uc01vZGVsLmphdmE=) | `ø` |; | [...sr/scalable/modeling/VariantAnnotationsScorer.java](https://app.codecov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvbW9kZWxpbmcvVmFyaWFudEFubm90YXRpb25zU2NvcmVyLmphdmE=) | `ø` |; | [...able/ExtractVariantAnnotationsIntegrationTest.java](https://app.codecov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvRXh0cmFjdFZhcmlhbnRBbm5vdGF0aW9uc0ludGVncmF0aW9uVGVzdC5qYXZh) | `ø` |; | [...rs/vqsr/scalable/TrainVariantAnnotationsModel.java](https://app.codecov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvVHJhaW5WYXJpYW50QW5ub3RhdGlvbnNNb2RlbC5qYXZh) | `37.037%` |; | [...alable/modeling/PythonVariantAnnotationsModel.java](https://app.codecov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvbW9kZWxpbmcvUHl0aG9uVmFyaWFudEFubm90YXRpb25zTW9kZWwuamF2YQ==) | `66.667%` |; | [...e/TrainVariantAnnotationsModelIntegrationTest.java](https://app.codecov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_t,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8131#issuecomment-1352314153:3954,scalab,scalable,3954,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8131#issuecomment-1352314153,1,['scalab'],['scalable']
Performance,"c_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx8G -Djava.io.tmpdir=./ -jar /data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar BaseRecalibrator -R /data/reference/gatk_resource/Homo_sapiens_assembly38.fasta -I /data/xieduo/Immun_genomics/data/Łuksza_2022_Nature/bam/PAAD11N.bam --known-sites /data/xieduo/WES_pipe/pipeline/gatk_resource/dbsnp_146.hg38.vcf.gz --known-sites /data/reference/gatk_resource/1000G_phase1.snps.high_confidence.hg38.vcf.gz --known-sites /data/reference/gatk_resource/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz -O PAAD11N.recal_data.test.table; 13:46:24.742 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 13:46:24.761 WARN NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (No such file or directory); 13:46:24.764 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 13:46:24.764 WARN NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (No such file or directory); 13:46:24.884 INFO BaseRecalibrator - ------------------------------------------------------------; 13:46:24.884 INFO BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.2.6.1; 13:46:24.885 INFO BaseRecalibrator - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:46:24.885 INFO BaseRecalibrator - Executing as xieduo@pbs-master on Linux v3.10.0-1160.41.1.el7.x86_64 amd64; 13:46:24.885 INFO BaseRecalibrator - Java runtime: Java HotSpot(TM) 64-Bit Server VM v18+36-2087; 13:46:24.885 INFO BaseRecalibrator - Start Date/Time: September 22, 2022 at 1:46:24 PM CST; 13:46:24.885 INFO BaseRecalibrator - -----------------------------------------------------",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8005#issuecomment-1254561081:13146,Load,Loading,13146,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005#issuecomment-1254561081,1,['Load'],['Loading']
Performance,"can you add all 4 to; https://gatk-jenkins.broadinstitute.org/view/Performance/ (i only see 2). also, can you write up instructions on how to add more? (we need to be able; to add more without bothering devops). On Fri, Jun 24, 2016 at 3:37 PM, David Bernick notifications@github.com; wrote:. > All 4 of the above tests are running consistently in Jenkins. Will keep an; > eye on them for avg time.; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/gatk/issues/1609#issuecomment-228441538,; > or mute the thread; > https://github.com/notifications/unsubscribe/AB5rLxVVaq-KvEaU013VPmbhoWwrf1CAks5qPDHmgaJpZM4H2UOY; > .",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1609#issuecomment-228443280:67,Perform,Performance,67,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1609#issuecomment-228443280,1,['Perform'],['Performance']
Performance,case6: same as case5 but on increased cache size to 100_000 (from the previous default value of 10_000); **3m45.728s** on 3.4-46-gbc02625; **3m15.550s** on GATK4 :+1: . cache 20_000; **3m41.981s** on GATK4. cache 50_000; **3m40.185s** on GATK4. cache 200_000; **3m30.461s** on GATK4. increasing the cache to 1_000_000 does not help further:; **3m50.317s** on GATK4,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1092#issuecomment-155499074:38,cache,cache,38,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1092#issuecomment-155499074,5,['cache'],['cache']
Performance,cc @sryza for any pointers. I'll start with Sandy's excellent two blog posts on tuning Spark jobs if you haven't seen them already:; http://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-1/; http://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-2/,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/972#issuecomment-150054581:178,tune,tune-your-apache-spark-jobs-part-,178,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/972#issuecomment-150054581,2,['tune'],['tune-your-apache-spark-jobs-part-']
Performance,"ch can be inspected via the src.zip file...; }; ```. So looking at the code portions of `computeDefaultSUID()` and I notice in our instance `ReadFilter` is a interface, which gets defined later via [ReadFilterLibrary.java](https://github.com/broadinstitute/hellbender/blob/62ef76ba60951c562a0d4c39189aa3f01f27f8d3/src/main/java/org/broadinstitute/hellbender/engine/filters/ReadFilterLibrary.java) or via `new ReadsFilter(readFilter, header)`, in either of these instances the fields would be different, based on this portion of `computeDefaultSUID` when looking at declared fields:. ```; Field[] fields = cl.getDeclaredFields();; MemberSignature[] fieldSigs = new MemberSignature[fields.length];; for (int i = 0; i < fields.length; i++) {; fieldSigs[i] = new MemberSignature(fields[i]);; }; ```. Therefore the `SUID` would be different based on the fields it would have. Please let me know if I misinterpreted something. @jean-philippe-martin I would be happy to help out, but even when I try a small test like this I get an error - I probably am performing something incorrectly. Below are the steps I performed using the codebase from this PR:. ```; $ wget https://github.com/broadinstitute/hellbender/archive/67b380fbed272bb92ef667ec2128d5720718a5e8.zip; $ unzip 67b380fbed272bb92ef667ec2128d5720718a5e8.zip; $ cd hellbender-67b380fbed272bb92ef667ec2128d5720718a5e8; $ gradle fatJar; $ java -jar build/libs/hellbender-67b380fbed272bb92ef667ec2128d5720718a5e8-all-version-unknown-SNAPSHOT.jar BaseRecalibratorDataflow -R ./src/test/resources/human_g1k_v37.chr17_1Mb.fasta --knownSites ./src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf -I ./src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf -RECAL_TABLE_FILE gatk4.pre.cols.table --sort_by_all_columns true; [June 1, 2015 5:51:02 PM EDT] org.broadinstitute.hellbender.dev.tools.walkers.bqsr.BaseRecalibratorData",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499:2222,perform,performing,2222,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499,1,['perform'],['performing']
Performance,closing this ticket until we determine this is a bottleneck,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/233#issuecomment-94342722:49,bottleneck,bottleneck,49,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/233#issuecomment-94342722,1,['bottleneck'],['bottleneck']
Performance,"clude/c++/5.3.0/cmath:853:5: note: 'template<class _Tp> typename __gnu_cxx::__enable_if<std::__is_arithmetic<_Tp>::__value, int>::__type std::isinf(_Tp)' declared here, later in the translation unit; isinf(_Tp __f); ^; In file included from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/utils.h:4:0,; from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/LoadTimeInitializer.cc:1:; /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/common_data_structure.h: In instantiation of 'static NUMBER ContextBase<NUMBER>::approximateLog10SumLog10(NUMBER, NUMBER) [with NUMBER = double]':; /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/common_data_structure.h:75:53: required from 'static void ContextBase<NUMBER>::initializeMatchToMatchProb() [with NUMBER = double]'; /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/common_data_structure.h:47:35: required from 'static void ContextBase<NUMBER>::initializeStaticMembers() [with NUMBER = double]'; /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/LoadTimeInitializer.cc:53:20: required from here; /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/common_data_structure.h:94:16: error: 'isinf' was not declared in this scope, and no declarations were found by argument-dependent lookup at the point of instantiation [-fpermissive]; if (isinf(small) == -1 || isinf(big) == -1); ^; In file included from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/headers.h:27:0,; from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/common_data_structure.h:4,; from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/utils.h:4,; from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/LoadTimeInitializer.cc:1:; /usr/local/Cellar/gcc/5.3.0/include/c++/5.3.0/cmath:853:5: note: 'template<class _Tp> typename __gnu_cxx::__enable_if<std::__is_arithmetic<_Tp>::__value, int>::__type std::isinf(_Tp)' declared here, later in",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1504#issuecomment-187727343:12978,Load,LoadTimeInitializer,12978,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1504#issuecomment-187727343,1,['Load'],['LoadTimeInitializer']
Performance,com.sun.proxy.$Proxy2.stop(Unknown Source); 	at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:377); 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.testng.TestNGException:An error occurred while instantiating class org.broadinstitute.hellbender.engine.spark.ReadsPreprocessingPipelineSparkTestData. Check to make sure it can be instantiated; 	at org.testng.internal.InstanceCreator.createInstanceUsingObjectFactory(InstanceCreator.java:134); 	at org.testng.internal.InstanceCreator.createInstance(InstanceCreator.java:79); 	at org.testng.internal.ClassImpl.getDefaultInstance(ClassImpl.java:110); 	at org.testng.internal.ClassImpl.getInstances(ClassImpl.java:195); 	at org.testng.TestClass.getInstances(TestClass.java:102); 	at org.testng.TestClass.initTestClassesAndInstances(TestClass.java:82); 	at org.testng.TestClass.init(TestClass.java:74); 	at org.testng.TestClass.<init>(TestClass.java:39); 	at org.testng.TestRunner.initMethods(TestRunner.java:466); 	at org.testng.TestRunner.init(TestRunner.java:345); 	at org.testn,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5787#issuecomment-472107858:2216,concurren,concurrent,2216,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5787#issuecomment-472107858,1,['concurren'],['concurrent']
Performance,"compression_level=1 -DGATK_STACKTRACE_ON_USER_EXCEPTION=true --conf spark.executor.extraJavaOptions=-Dsamjdk.compression_level=1 -DGATK_STACKTRACE_ON_USER_EXCEPTION=true --deploy-mode client --num-executors 59 --executor-cores 4 --executor-memory 24180M --driver-memory 10G /mnt/raid5/frankliu/code/GATK/gatk/build/libs/gatk-package-4.alpha.2-120-g00a40ea-SNAPSHOT-spark.jar CountReadsSpark -I hdfs://arlab174:54310/GATK4TEST/BroadData/CEUTrio.HiSeq.WEx.b37.NA12892.bam -O hdfs://arlab174:54310/GATK4TEST/Output/Test_CEU_ReadsCount --sparkMaster yarn; 14:56:20.999 INFO IntelGKLUtils - Trying to load Intel GKL library from:; 	jar:file:/mnt/raid5/frankliu/code/GATK/gatk/build/libs/gatk-package-4.alpha.2-120-g00a40ea-SNAPSHOT-spark.jar!/com/intel/gkl/native/libgkl_compression.so; Exception in thread ""main"" java.lang.UnsatisfiedLinkError: /tmp/frankliu/libgkl_compression8271576113600851848.so: /tmp/frankliu/libgkl_compression8271576113600851848.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64-bit platform); 	at java.lang.ClassLoader$NativeLibrary.load(Native Method); 	at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941); 	at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824); 	at java.lang.Runtime.load0(Runtime.java:809); 	at java.lang.System.load(System.java:1086); 	at com.intel.gkl.IntelGKLUtils.load(IntelGKLUtils.java:133); 	at com.intel.gkl.compression.IntelDeflater.load(IntelDeflater.java:58); 	at com.intel.gkl.compression.IntelDeflater.load(IntelDeflater.java:53); 	at com.intel.gkl.compression.IntelDeflaterFactory.<init>(IntelDeflaterFactory.java:16); 	at com.intel.gkl.compression.IntelDeflaterFactory.<init>(IntelDeflaterFactory.java:20); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:143); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbe",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2302#issuecomment-265854885:1559,load,load,1559,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2302#issuecomment-265854885,1,['load'],['load']
Performance,"cotator_dataSources.v1.6.20190124s/achilles/hg38/achilles_lineage_results.import.txt; > 12:28:28.866 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/gencode_xrefseq_v90_38.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/gencode_xrefseq/hg38/gencode_xrefseq_v90_38.tsv; > 12:28:31.215 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/hgnc_download_Nov302017.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/hgnc/hg38/hgnc_download_Nov302017.tsv; > 12:28:31.563 INFO Funcotator - Initializing Funcotator Engine...; > 12:28:31.593 INFO Funcotator - Creating a VCF file for output: file:/home/pkus/mutect_test/filtered_variants/P1.avcf.gz; > 12:28:31.731 INFO ProgressMeter - Starting traversal; > 12:28:31.731 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; > 12:28:31.969 INFO VcfFuncotationFactory - dbSNP 9606_b150 cache hits/total: 0/0; > 12:28:31.975 INFO Funcotator - Shutting down engine; > [July 21, 2020 12:28:31 PM CEST] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 0.26 minutes.; > Runtime.totalMemory()=2200961024; > org.broadinstitute.hellbender.exceptions.GATKException: Unable to query the database for geneName: NCRNA00115; > at org.broadinstitute.hellbender.tools.funcotator.dataSources.cosmic.CosmicFuncotationFactory.createFuncotationsOnVariant(CosmicFuncotationFactory.java:320); > at org.broadinstitute.hellbender.tools.funcotator.DataSourceFuncotationFactory.determineFuncotations(DataSourceFuncotationFactory.java:245); > at org.broadinstitute.hellbender.tools.funcotator.DataSourceFuncotationFactory.createFuncotations(DataSourceFuncotationFactory.java:211); > at org.broadinstitute.hellbender.tools.funcotator.FuncotatorEngine.createFuncotationMapForVariant(FuncotatorEngine.java:173); > at org.broadinstitute.hellbender.tools.funcotator.Funcotator.enqu",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6708#issuecomment-661776975:16958,cache,cache,16958,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6708#issuecomment-661776975,1,['cache'],['cache']
Performance,cov.io/gh/broadinstitute/gatk/pull/3032?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/84fbda69bf9528059777496a415be8eb6db63e61?src=pr&el=desc) will **increase** coverage by `0.012%`.; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #3032 +/- ##; ===============================================; + Coverage 79.973% 79.985% +0.012% ; - Complexity 16727 16745 +18 ; ===============================================; Files 1139 1139 ; Lines 60902 60938 +36 ; Branches 9437 9439 +2 ; ===============================================; + Hits 48705 48741 +36 ; + Misses 8401 8398 -3 ; - Partials 3796 3799 +3; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/3032?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...itute/hellbender/tools/walkers/SplitIntervals.java](https://codecov.io/gh/broadinstitute/gatk/pull/3032?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL1NwbGl0SW50ZXJ2YWxzLmphdmE=) | `88.235% <ø> (ø)` | `6 <0> (ø)` | :arrow_down: |; | [...itute/hellbender/tools/walkers/mutect/Mutect2.java](https://codecov.io/gh/broadinstitute/gatk/pull/3032?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL211dGVjdC9NdXRlY3QyLmphdmE=) | `92.593% <0%> (ø)` | `32% <0%> (+16%)` | :arrow_up: |; | [...egmentation/PerformAlleleFractionSegmentation.java](https://codecov.io/gh/broadinstitute/gatk/pull/3032?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9zZWdtZW50YXRpb24vUGVyZm9ybUFsbGVsZUZyYWN0aW9uU2VnbWVudGF0aW9uLmphdmE=) | `88.889% <0%> (ø)` | `4% <0%> (+2%)` | :arrow_up: |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/3032?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `73.649% <0%> (+2.027%)` | `34% <0%> (ø)` | :arrow_down: |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3032#issuecomment-306375449:1515,Perform,PerformAlleleFractionSegmentation,1515,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3032#issuecomment-306375449,1,['Perform'],['PerformAlleleFractionSegmentation']
Performance,"created for the TH evaluation:. Here is the 100% tumor run through ModelSegments, which yields 130 segments:; ![T modeled](https://user-images.githubusercontent.com/11076296/76560309-43d2dd80-6477-11ea-87d2-75d430d220a2.png). Here is the 75% tumor + 25% normal mixture run through ModelSegments, which yields 83 segments:; ![N-25-T-75 modeled](https://user-images.githubusercontent.com/11076296/76558609-fd2fb400-6473-11ea-9bd3-293a66eb3e4e.png). Here is the 25% tumor + 75% normal mixture run through ModelSegments, which yields 50 segments:; ![N-75-T-25 modeled](https://user-images.githubusercontent.com/11076296/76560439-85638880-6477-11ea-8d8d-f0f9a11d70a6.png). We can compare against the new workflow, in which we run SegmentJointSamples to jointly segment on the two mixtures. This yields a joint-sample segmentation with 162 segments, which can be passed as an additional input to individual ModelSegments runs on the two mixtures. It is used as the initial segmentation for both runs, after which the usual modelling and smoothing steps are performed. For the 75% tumor + 25% normal mixture, this yields 122 segments (up from 83):; ![N-25-T-75-SJS modeled](https://user-images.githubusercontent.com/11076296/76558618-015bd180-6474-11ea-996a-48d39770149b.png). For the 25% tumor + 75% normal mixture, this yields 105 segments (up from 50):; ![N-75-T-25-SJS modeled](https://user-images.githubusercontent.com/11076296/76560726-34a05f80-6478-11ea-9027-a54726c46b9e.png). One could imagine that smoothing could be disabled (so that all samples retain the common segmentation after modeling) or made more aggressive (so that private events don't get inadvertently introduced into other samples due to noise, perhaps), depending on the use case. It looks like the joint segmentation allows some additional events to be resolved, although I haven't done any rigorous evaluations. We could probably cook up some evaluations using simulated toy data or in silico mixtures, but there's really no reaso",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6499#issuecomment-598386823:1187,perform,performed,1187,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6499#issuecomment-598386823,1,['perform'],['performed']
Performance,ctOriginalAlignmentRecordsByNameSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/3838/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi91dGlscy9FeHRyYWN0T3JpZ2luYWxBbGlnbm1lbnRSZWNvcmRzQnlOYW1lU3BhcmsuamF2YQ==) | `87.5% <0%> (-0.735%)` | `12% <0%> (+6%)` | |; | [.../sv/StructuralVariationDiscoveryPipelineSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/3838/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9TdHJ1Y3R1cmFsVmFyaWF0aW9uRGlzY292ZXJ5UGlwZWxpbmVTcGFyay5qYXZh) | `92.529% <0%> (-0.412%)` | `14% <0%> (+8%)` | |; | [...s/spark/sv/evidence/experimental/CalcMetadata.java](https://codecov.io/gh/broadinstitute/gatk/pull/3838/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9ldmlkZW5jZS9leHBlcmltZW50YWwvQ2FsY01ldGFkYXRhLmphdmE=) | `0% <0%> (ø)` | `0% <0%> (ø)` | :arrow_down: |; | [...te/hellbender/tools/exome/PerformSegmentation.java](https://codecov.io/gh/broadinstitute/gatk/pull/3838/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9QZXJmb3JtU2VnbWVudGF0aW9uLmphdmE=) | `100% <0%> (ø)` | `6% <0%> (+3%)` | :arrow_up: |; | [.../broadinstitute/hellbender/tools/exome/Sample.java](https://codecov.io/gh/broadinstitute/gatk/pull/3838/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9TYW1wbGUuamF2YQ==) | `100% <0%> (ø)` | `10% <0%> (+5%)` | :arrow_up: |; | [...v/evidence/experimental/FindSmallIndelRegions.java](https://codecov.io/gh/broadinstitute/gatk/pull/3838/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9ldmlkZW5jZS9leHBlcmltZW50YWwvRmluZFNtYWxsSW5kZWxSZWdpb25zLmphdmE=) | `0% <0%> (ø)` | `0% <0%> (ø)` | :arrow_down: |; | [...e/hellbender/engine/filters/LibraryReadFilter.java](https://codecov.io/gh/broadinstitute/gatk/pull/3838/diff?src=,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3838#issuecomment-344512890:2844,Perform,PerformSegmentation,2844,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3838#issuecomment-344512890,1,['Perform'],['PerformSegmentation']
Performance,"ctors=True --disable_bias_factors_in_active_class=False --p_alt=1.000000e-06 --cnv_coherence_length=1.000000e+04 --max_copy_number=5 --p_active=0.010000 --class_coherence_length=10000.000000 --learning_rate=1.000000e-02 --adamax_beta1=9.000000e-01 --adamax_beta2=9.900000e-01 --log_emission_samples_per_round=50 --log_emission_sampling_rounds=10 --log_emission_sampling_median_rel_error=5.000000e-03 --max_advi_iter_first_epoch=5000 --max_advi_iter_subsequent_epochs=200 --min_training_epochs=10 --max_training_epochs=50 --initial_temperature=1.500000e+00 --num_thermal_advi_iters=2500 --convergence_snr_averaging_window=500 --convergence_snr_trigger_threshold=1.000000e-01 --convergence_snr_countdown_window=10 --max_calling_iters=10 --caller_update_convergence_threshold=1.000000e-03 --caller_internal_admixing_rate=7.500000e-01 --caller_external_admixing_rate=1.000000e+00 --disable_caller=false --disable_sampler=false --disable_annealing=false; Stdout: 14:13:50.032 INFO cohort_denoising_calling - Loading 24 read counts file(s)...; 14:13:53.719 INFO gcnvkernel.io.io_metadata - Loading germline contig ploidy and global read depth metadata...; 14:13:58.626 INFO gcnvkernel.tasks.task_cohort_denoising_calling - Instantiating the denoising model (warm-up)...; 14:14:04.543 INFO gcnvkernel.models.fancy_model - Global model variables: {'W_tu', 'psi_t_log__', 'ard_u_log__', 'log_mean_bias_t'}; 14:14:04.544 INFO gcnvkernel.models.fancy_model - Sample-specific model variables: {'z_su', 'psi_s_log__', 'read_depth_s_log__'}; 14:14:04.544 WARNING gcnvkernel.tasks.inference_task_base - No log emission sampler given; skipping the sampling step; 14:14:04.544 WARNING gcnvkernel.tasks.inference_task_base - No caller given; skipping the calling step; 14:14:04.544 INFO gcnvkernel.tasks.inference_task_base - Instantiating the convergence tracker...; 14:14:04.544 INFO gcnvkernel.tasks.inference_task_base - Setting up DA-ADVI...; 14:14:10.902 INFO gcnvkernel.tasks.inference_task_base - (denoising (wa",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4825#issuecomment-467406398:3231,Load,Loading,3231,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4825#issuecomment-467406398,1,['Load'],['Loading']
Performance,"d I notice in our instance `ReadFilter` is a interface, which gets defined later via [ReadFilterLibrary.java](https://github.com/broadinstitute/hellbender/blob/62ef76ba60951c562a0d4c39189aa3f01f27f8d3/src/main/java/org/broadinstitute/hellbender/engine/filters/ReadFilterLibrary.java) or via `new ReadsFilter(readFilter, header)`, in either of these instances the fields would be different, based on this portion of `computeDefaultSUID` when looking at declared fields:. ```; Field[] fields = cl.getDeclaredFields();; MemberSignature[] fieldSigs = new MemberSignature[fields.length];; for (int i = 0; i < fields.length; i++) {; fieldSigs[i] = new MemberSignature(fields[i]);; }; ```. Therefore the `SUID` would be different based on the fields it would have. Please let me know if I misinterpreted something. @jean-philippe-martin I would be happy to help out, but even when I try a small test like this I get an error - I probably am performing something incorrectly. Below are the steps I performed using the codebase from this PR:. ```; $ wget https://github.com/broadinstitute/hellbender/archive/67b380fbed272bb92ef667ec2128d5720718a5e8.zip; $ unzip 67b380fbed272bb92ef667ec2128d5720718a5e8.zip; $ cd hellbender-67b380fbed272bb92ef667ec2128d5720718a5e8; $ gradle fatJar; $ java -jar build/libs/hellbender-67b380fbed272bb92ef667ec2128d5720718a5e8-all-version-unknown-SNAPSHOT.jar BaseRecalibratorDataflow -R ./src/test/resources/human_g1k_v37.chr17_1Mb.fasta --knownSites ./src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf -I ./src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf -RECAL_TABLE_FILE gatk4.pre.cols.table --sort_by_all_columns true; [June 1, 2015 5:51:02 PM EDT] org.broadinstitute.hellbender.dev.tools.walkers.bqsr.BaseRecalibratorDataflow --knownSites /home/pgrosu/me/gg_hellbender_broad_institute/test/hellbender-67b380fbed272bb92ef667ec2128d5720",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499:2278,perform,performed,2278,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499,1,['perform'],['performed']
Performance,"d Minutes Batches Processed Batches/Minute; 16:28:05.198 INFO GenomicsDBImport - Starting batch input file preload; 16:29:23.571 INFO GenomicsDBImport - Finished batch preload; 16:48:46.140 INFO GenomicsDBImport - Shutting down engine; [May 4, 2018 4:48:46 PM EDT] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 20.96 minutes.; Runtime.totalMemory()=22281715712; java.util.concurrent.CompletionException: java.lang.OutOfMemoryError: Java heap space; at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273); at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1592); at java.util.concurrent.CompletableFuture$AsyncSupply.exec(CompletableFuture.java:1582); at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056); at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692); at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157); Caused by: java.lang.OutOfMemoryError: Java heap space; at com.intel.genomicsdb.importer.SilentByteBufferStream.<init>(SilentByteBufferStream.java:55); at com.intel.genomicsdb.importer.GenomicsDBImporterStreamWrapper.<init>(GenomicsDBImporterStreamWrapper.java:70); at com.intel.genomicsdb.importer.GenomicsDBImporter.addBufferStream(GenomicsDBImporter.java:397); at com.intel.genomicsdb.importer.GenomicsDBImporter.addSortedVariantContextIterator(GenomicsDBImporter.java:358); at com.intel.genomicsdb.importer.GenomicsDBImporter.<init>(GenomicsDBImporter.java:167); at com.intel.genomicsdb.importer.GenomicsDBImporter.lambda$null$2(GenomicsDBImporter.java:598); at com.intel.genomicsdb.importer.GenomicsDBImporter$$Lambda$58/15335646.get(Unknown Source); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590); ... 5 more; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388079572:4017,concurren,concurrent,4017,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388079572,3,['concurren'],['concurrent']
Performance,"d positives and negatives and use the existing code for extracting labels, but this will require a bit of engineering and be more trouble than it's worth. There are other options---see https://ir.cwi.nl/pub/30479, for example. We might want to experiment with the LL score discussed there (see https://www.aaai.org/Papers/ICML/2003/ICML03-060.pdf for the original paper---although note that despite the paper's high citation count, I'm not sure what the canonical name for this metric actually is, but it doesn't appear to be ""LL score""---perhaps someone else knows or has better Google-fu and can figure it out) before moving on to their methods for estimating F1. Doing a literature search for other discussions of optimizing F1 or other metrics in the context of positive-unlabeled learning might be worthwhile, but I think most methods will probably involve some sort of estimation of the base rate in unlabeled data. I think we may have to add some mechanism for holding out a validation set during training if we want to automatically tune thresholds in a rigorous fashion. Shouldn't be too bad---we can just have the training tool randomly mask out a set of the truth and pass the mask to the scoring tool (or maybe just determine the threshold in the training tool, if we are running in positive/negative mode and have access to unlabeled data)---but does add a couple of parameters to the tool interfaces. This also adds additional dependence on the quality of the truth resources. I think an implicit assumption in any use of the truth---even just thresholding/calibrating by sensitivity---is that it is a random sample; however, I'm not sure how true this is in actual use. For example, in malaria, it looks like we may have to resort to using a callset that has been very conservatively filtered as truth, which will bias us towards high scores and the peaks of the positive distribution. Perhaps we can also experiment with just treating training/truth on an equal footing (I think the d",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1062931241:1316,tune,tune,1316,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1062931241,1,['tune'],['tune']
Performance,"d try to do it, as that is a relatively expensive resource to create. For example, some very naive hard filtering (red) of the histogram yields a peak that is easily fit by a negative binomial (green)---even a Poisson fit does not appear to bias the depth estimates, and certainly does not result in incorrect ploidy estimates:. ![masked_fit](https://user-images.githubusercontent.com/11076296/37863641-827a6e8a-2f37-11e8-83d5-cb4af32a898b.png). (Incidentally, it is helpful to plot on a log scale when checking the fit of these distributions.). This strategy also gives us a way to ignore low-level mosaicism or large germline events, which filtering on mappability may not address:. ![mosaic](https://user-images.githubusercontent.com/11076296/37863649-d0ac378c-2f37-11e8-8e98-45e1fa9a3d7a.png). So let's try to encapsulate changes to the ploidy tool. I agree that the histogram creation can be easily done on the Java side, to save on intermediate file writing. We can probably just cap the maximum bin to `k` and pass a samples x contig TSV where each entry is a vector with `k + 1` elements. I agree that there is still a lot of important work to be done in exploring our best practices for coverage collection, and I know that you have been interested in improving them for a while. Ultimately, we may want to consider incorporating mappability or other informative metadata, as we've discussed. However, this will require some non-trivial investment in method/tool development time. Since our preliminary evaluations show that even with the current, naive strategies the tool is performing reasonably well, I am prioritizing cutting a release and improving/automating the evaluations. As we discussed, this will both allow users to start using the tool (which will hopefully result in useful feedback) and establish a baseline for us. This will ultimately provide the necessary foundation for future exploratory work and method development---which always takes more time than we think it will!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4558#issuecomment-375881040:2153,perform,performing,2153,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4558#issuecomment-375881040,1,['perform'],['performing']
Performance,"d; the smaller of this or the number of samples remaining after filtering is used. The number actually used to denoise can be specified in DenoiseReadCounts. If we are going to spend energy computing K eigensamples, there is no reason we shouldn't expose all of them in the PoN, even if we don't want to use all of them for denoising. (Also, the current SVD utility methods do not allow for specification of K < N when performing SVD on an MxN matrix, even though the backend implementations that are called do allow for this; this is terrible. In any case, randomized SVD should be much faster than the currently available implementations, even when K = N).; - [x] Rename CreatePanelOfNormals to CreateReadCountPanelOfNormals; - [x] Refer to ""targets"" as intervals. See #3246.; - [x] Remove QC.; - [x] Refer to proportional coverage as fractional coverage.; - [x] Perform optional GC-bias correction internally if annotated intervals are passed as input.; - [x] Make standardization process for panel and case samples identical. Currently, a sample mean is taken at one point in the PoN standardization process, while a sample median is taken in the case standardization process.; - [x] HDF5 PoN will store version number, all integer read counts, all/panel intervals, all/panel sample paths/names, all annotated intervals (if GC-bias correction was performed), fractional-coverage medians for all intervals, relevant SVD results (eigenvalues and left-singular vectors) for the specified number of eigensamples, and command line.; - [x] In a future iteration, we could allow an input PoN to be the source of read counts. This would allow iteration on filter parameters without needing output from CombineReadCounts. The code should easily allow for this.; - [x] ReadCountCollection is too memory intensive; minimize use in DenoiseReadCounts when writing results.; - [x] Optimize and clean up HDF5 writing (e.g., transpose skinny matrices, make writing of intervals <s>as a string matrix</s> faster).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-313921687:2072,perform,performed,2072,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-313921687,2,"['Optimiz', 'perform']","['Optimize', 'performed']"
Performance,"da_read_pipeline is the right spot. If it's OK, I suggest holding off for a little bit. That branch is next in queue to get merged. I plan to have it merged by next Friday. Until then, I'll be doing (hopefully small) cleanup. I don't expect any serious refactors, but it hasn't gone for review, so I can't promise anything. (Read: it probably won't change much, but I can't guarantee it, so using it may waste some of your time). Right after that goes in, I want to work with you Tom on a refactor that'll work well for everyone. There many also be other places where we're overly tied to Cloud Dataflow right now.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/567#issuecomment-120037353:111,queue,queue,111,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/567#issuecomment-120037353,1,['queue'],['queue']
Performance,"data 1). These intervals were used for scattering tasks in the Germline short variant calling and Somatic short variant calling workflows outlined below.; > ; > Germline short variant calling; > ; > HaplotypeCaller was run at the 3,200 pre-defined intervals for each sample and interval VCFs were gathered into GVCFs per sample. The following steps were then performed for the OSCC, TCGA and CSCC cohorts separately. Per sample GVCFs were consolidated with GenomicsDBImport and joint-called with GenotypeGVCFs at the 3,200 pre-defined intervals. Multi-sample interval calls were gathered using GatherVcfs. VariantFiltration and MakeSitesOnlyVcf was used to filter records with excess heterozygosity and retain only the site-level annotations. VariantRecalibrator and ApplyRecal were then applied for SNP and indels separately following GATK’s recommendations to obtain a final, filtered cohort VCF.; > ; > Somatic short variant calling; > ; > Somatic short variant calling was performed for tumour-normal pairs, first by preparing a panel of normals (PoN) for the OSCC, TCGA and CSCC cohorts separately. To create a PoN, Mutect2 was run in tumour only mode for the normal samples in the cohort at the 3,200 pre-defined intervals. GatherVcfs was used to gather interval VCFs into per sample VCFs. GenomicsDBImport and CreateSomaticPON was used to consolidate sample VCFs and create the PoN resource at the 3,200 pre-defined intervals. Interval VCFs were gathered into a single, cohort PoN VCF using GatherVcfs.; > ; > ; > ; > For each tumour-normal pair, Mutect2 was used for calling SNP and indels at the 3,200 pre-defined intervals, applying settings –f1r2-tar-gz, --germline-resource with gnomAD v3 as input and –panel-of-normals with PoN for the tumour-normal pair’s respective cohort as input. Interval stats files were merged into a single stats file with MergeMutectStats and interval VCFs were gathered with GatherVcfs into a single VCF for each tumour-normal pair, as preparation for further f",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6674#issuecomment-656499472:3312,perform,performed,3312,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6674#issuecomment-656499472,1,['perform'],['performed']
Performance,db63e61?src=pr&el=desc) will **increase** coverage by `0.169%`.; > The diff coverage is `90%`. ```diff; @@ Coverage Diff @@; ## master #3035 +/- ##; ===============================================; + Coverage 79.973% 80.142% +0.169% ; - Complexity 16727 16787 +60 ; ===============================================; Files 1139 1140 +1 ; Lines 60902 60867 -35 ; Branches 9437 9437 ; ===============================================; + Hits 48705 48780 +75 ; + Misses 8401 8296 -105 ; + Partials 3796 3791 -5; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...coveragemodel/CoverageModelArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL0NvdmVyYWdlTW9kZWxBcmd1bWVudENvbGxlY3Rpb24uamF2YQ==) | `86.592% <ø> (ø)` | `40 <0> (ø)` | :arrow_down: |; | [...gemodel/cachemanager/ComputableGraphStructure.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9Db21wdXRhYmxlR3JhcGhTdHJ1Y3R1cmUuamF2YQ==) | `100% <100%> (+26.994%)` | `63 <62> (+24)` | :arrow_up: |; | [...ragemodel/cachemanager/ComputableNodeFunction.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9Db21wdXRhYmxlTm9kZUZ1bmN0aW9uLmphdmE=) | `100% <100%> (+66.667%)` | `4 <1> (+2)` | :arrow_up: |; | [.../coveragemodel/cachemanager/DuplicableNDArray.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9EdXBsaWNhYmxlTkRBcnJheS5qYXZh) | `81.818% <100%> (+38.068%)` | `6 <2> (+2)` | :arrow_up: |; | [...s/coveragemodel/cachemanager/Du,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3035#issuecomment-306412418:1253,cache,cachemanager,1253,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3035#issuecomment-306412418,1,['cache'],['cachemanager']
Performance,decov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) | Coverage |; |---|---|; | [...lkers/vqsr/scalable/ExtractVariantAnnotations.java](https://app.codecov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvRXh0cmFjdFZhcmlhbnRBbm5vdGF0aW9ucy5qYXZh) | `ø` |; | [...walkers/vqsr/scalable/ScoreVariantAnnotations.java](https://app.codecov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvU2NvcmVWYXJpYW50QW5ub3RhdGlvbnMuamF2YQ==) | `0.000%` |; | [.../scalable/data/LabeledVariantAnnotationsDatum.java](https://app.codecov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvZGF0YS9MYWJlbGVkVmFyaWFudEFubm90YXRpb25zRGF0dW0uamF2YQ==) | `ø` |; | [...scalable/modeling/BGMMVariantAnnotationsModel.java](https://app.codecov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvbW9kZWxpbmcvQkdNTVZhcmlhbnRBbm5vdGF0aW9uc01vZGVsLmphdmE=) | `ø` |; | [...sr/scalable/modeling/VariantAnnotationsScorer.java](https://app.codecov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8131#issuecomment-1352314153:2331,scalab,scalable,2331,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8131#issuecomment-1352314153,1,['scalab'],['scalable']
Performance,diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9NYXRoT2JqZWN0QXNzZXJ0cy5qYXZh) | `31.818% <0%> (ø)` | `5% <0%> (?)` | |; | [...e/hellbender/utils/icg/ComputableNodeFunction.java](https://codecov.io/gh/broadinstitute/gatk/pull/4203/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9pY2cvQ29tcHV0YWJsZU5vZGVGdW5jdGlvbi5qYXZh) | `100% <0%> (ø)` | `4% <0%> (?)` | |; | [...der/utils/linalg/GeneralLinearOperatorNDArray.java](https://codecov.io/gh/broadinstitute/gatk/pull/4203/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9saW5hbGcvR2VuZXJhbExpbmVhck9wZXJhdG9yTkRBcnJheS5qYXZh) | `66.667% <0%> (ø)` | `4% <0%> (?)` | |; | [...hellbender/utils/icg/ComputableGraphStructure.java](https://codecov.io/gh/broadinstitute/gatk/pull/4203/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9pY2cvQ29tcHV0YWJsZUdyYXBoU3RydWN0dXJlLmphdmE=) | `100% <0%> (ø)` | `63% <0%> (?)` | |; | [...ender/utils/icg/ImmutableComputableGraphUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4203/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9pY2cvSW1tdXRhYmxlQ29tcHV0YWJsZUdyYXBoVXRpbHMuamF2YQ==) | `81.081% <0%> (ø)` | `1% <0%> (?)` | |; | [...broadinstitute/hellbender/utils/icg/CacheNode.java](https://codecov.io/gh/broadinstitute/gatk/pull/4203/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9pY2cvQ2FjaGVOb2RlLmphdmE=) | `80.645% <0%> (ø)` | `9% <0%> (?)` | |; | [.../hellbender/utils/nd4j/Nd4jApacheAdapterUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4203/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9uZDRqL05kNGpBcGFjaGVBZGFwdGVyVXRpbHMuamF2YQ==) | `85.714% <0%> (ø)` | `12% <0%> (?)` | |; | ... and [15 more](https://codecov.io/gh/broadinstitute/gatk/pull/4203/diff?src=pr&el=tree-more) | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4203#issuecomment-358765311:3295,Cache,CacheNode,3295,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4203#issuecomment-358765311,1,['Cache'],['CacheNode']
Performance,does this need to be in alpha? seems to be an optimization that can be postponed?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/670#issuecomment-146229817:46,optimiz,optimization,46,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/670#issuecomment-146229817,1,['optimiz'],['optimization']
Performance,"done with my review. small edits, 1 bug, and a few performance questions. I'd like to see a simple perf run of HC with and without those changes. All those streams in math-heavy tight loops make me concerned a bit",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1979#issuecomment-231096260:51,perform,performance,51,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1979#issuecomment-231096260,1,['perform'],['performance']
Performance,"ds.bam; **********. 11:25:52.673 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/fleharty/resources/picard.jar!/com/intel/gkl/native/libgkl_compression.dylib; [Tue Jul 14 11:25:52 EDT 2020] ValidateSamFile INPUT=concatenated_ACC5611A1_XXXXXX_consensusalign_ds.bam MODE=VERBOSE MAX_OUTPUT=100 IGNORE_WARNINGS=false VALIDATE_INDEX=true INDEX_VALIDATION_STRINGENCY=EXHAUSTIVE IS_BISULFITE_SEQUENCED=false MAX_OPEN_TEMP_FILES=8000 SKIP_MATE_VALIDATION=false VERBOSITY=INFO QUIET=false VALIDATION_STRINGENCY=STRICT COMPRESSION_LEVEL=5 MAX_RECORDS_IN_RAM=500000 CREATE_INDEX=false CREATE_MD5_FILE=false GA4GH_CLIENT_SECRETS=client_secrets.json USE_JDK_DEFLATER=false USE_JDK_INFLATER=false; [Tue Jul 14 11:25:52 EDT 2020] Executing as fleharty@wm462-624 on Mac OS X 10.15.5 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_191-b12; Deflater: Intel; Inflater: Intel; Provider GCS is not available; Picard version: 2.20.4-SNAPSHOT; WARNING	2020-07-14 11:25:52	ValidateSamFile	NM validation cannot be performed without the reference. All other validations will still occur.; ERROR: Record 18321, Read name UMI-ATT-GAA-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 26312, Read name UMI-CCT-TTC-1, Zero-length read without FZ, CS or CQ tag; ERROR: Record 70755, Read name UMI-CAG-GGA-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 145082, Read name UMI-AAC-ATG-5, Zero-length read without FZ, CS or CQ tag; ERROR: Record 181500, Read name UMI-ACT-CTT-1, Zero-length read without FZ, CS or CQ tag; ERROR: Record 186837, Read name UMI-CAA-CTC-4, Zero-length read without FZ, CS or CQ tag; ERROR: Record 186862, Read name UMI-CGC-GCC-0, Zero-length read without FZ, CS or CQ tag; ERROR: Record 186904, Read name UMI-AGG-GTC-0, Zero-length read without FZ, CS or CQ tag; ERROR: Record 186919, Read name UMI-CGC-TGC-0, Zero-length read without FZ, CS or CQ tag; ERROR: Record 186947, Read name UMI-TAA-TAG-3, Zero-length read without FZ, CS or CQ tag; ERROR",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6695#issuecomment-658247132:1895,perform,performed,1895,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6695#issuecomment-658247132,1,['perform'],['performed']
Performance,"e (no insight so far, just looking up the line from the stack trace):; ```; final Object2IntMap<EVIDENCE> evidenceIndexes = evidenceIndexBySampleIndex(sampleIndex);; final int[] indexesToRemove = evidences.stream().mapToInt(e -> {; final int index = evidenceIndexes.getInt(e);; if (index == MISSING_INDEX) {; throw new IllegalArgumentException(""evidence provided is not in sample"");; }; ```; We get an error when `evidenceIndexBySampleIndex(sampleIndex)` yields a `Map` that for some reason doesn't contain a read that it should. So let's investigate `evidenceIndexBySampleIndex()`. This method returns the `evidenceIndexBySampleIndex.get(sampleIndex)` field if it is not `null` (ie uninitialized); otherwise it fills it and then returns it. The code for filling it seems fine, and it explicitly loops over every sample read, so it's hard to see that the error could come from there. It seems rather that the problem is in returning the cached value whenever it is not `null`. The cached value of `evidenceIndexBySampleIndex.get(sampleIndex)` becomes invalid whenever reads are added or removed. However, you can check all the accesses of `evidenceIndexBySampleIndex` (there are only six) and verify that the class never accounts for this. So, suppose that an `AlleleLikelihoods` object invokes `evidenceIndexBySampleIndex(sampleIndex)` more than once and adds or removes reads between these. The second call returns the cached map from the first call, which is bogus. Even if it doesn't explain this issue, it is a bug. Now let's think about which public methods `evidenceIndexBySampleIndex(sampleIndex)` is called in and where this occurs in HaplotypeCaller:. * `addEvidence` (in HC this happens only in the likelihoods for annotations, downstream of our issue, so this is not the culprit).; * `filterPoorlyModeledEvidence` (this happens after Pair-HMM to the haplotype likelihoods, so not the culprit either); * `contaminationDownsampling`; * `retainEvidence` (hmmm in HC `readAlleleLikelihoods.ret",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6586#issuecomment-625021336:1020,cache,cached,1020,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6586#issuecomment-625021336,1,['cache'],['cached']
Performance,"e GATK3 traversal. Initially, I did plan on having `AssemblyRegionWalker` extend the former `ReadWindowWalker`, or an adapted version of your `SlidingWindowWalker`, and I did implement it like this at first, but ultimately I collapsed it into a single class for several reasons:; - Inheriting from a more generic traversal type caused usability issues and confusion with respect to the command-line arguments. The `ReadWindow` was the unit of processing for the superclass, but for `AssemblyRegionWalker` it was the unit of I/O and `AssemblyRegion` was the unit of processing, and I couldn't update the docs for `ReadWindowWalker` to clear up the confusion without mentioning `AssemblyRegion`-specific concepts.; - The `ReadShard` / `ReadWindow` was/is **only** there to prove that we can shard the data without introducing calling artifacts, and to provide a unit of parallelism for the upcoming Spark implementation. It's not something we really want to expose to users as a prominent knob, and we may hide it completely in the future once the shard size is tuned for performance.; - Inheriting from a more abstract walker type caused a number of other problems as well: methods that should have been final in the supertype could no longer be made final, with the result that tool implementations could inappropriately override engine initialization/shutdown routines. Also, there were issues with the progress meter, since both the supertype traversal and subtype traversal needed their own progress meter for their different units of processing. Ultimately it was just too awkward and forced, and the read shard is something that we eventually want to make an internal/encapsulated implementation detail anyway. GATK3 made the mistake, I think, of using long, confusing inheritance chains for its walker types, with the result that you got awkward and forced relationships like `RodWalker` inheriting from `LocusWalker`. It's better, I think, to make each traversal as standalone as possible, esp",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210806513:1359,tune,tuned,1359,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210806513,2,"['perform', 'tune']","['performance', 'tuned']"
Performance,"e forceValidOutput=false filter_reads_with_N_cigar=false filter_mismatching_base_and_quals=false filter_bases_not_stored=false"">; ##GATKCommandLine.SelectVariants=<ID=SelectVariants,Version=3.4-228-g2497091,Date=""Tue Jan 05 13:29:45 EST 2016"",Epoch=1452018585661,CommandLineOptions=""analysis_type=SelectVariants input_file=[] showFullBamList=false read_buffer_size=null phone_home=AWS gatk_key=null tag=NA read_filter=[] disable_read_filter=[] intervals=null excludeIntervals=null interval_set_rule=UNION interval_merging=ALL interval_padding=0 reference_sequence=/humgen/1kg/reference/human_g1k_v37.fasta nonDeterministicRandomSeed=false disableDithering=false maxRuntime=-1 maxRuntimeUnits=MINUTES downsampling_type=BY_SAMPLE downsample_to_fraction=null downsample_to_coverage=1000 baq=OFF baqGapOpenPenalty=40.0 refactor_NDN_cigar_string=false fix_misencoded_quality_scores=false allow_potentially_misencoded_quality_scores=false useOriginalQualities=false defaultBaseQualities=-1 performanceLog=null BQSR=null quantize_quals=0 static_quantized_quals=null round_down_quantized=false disable_indel_quals=false emit_original_quals=false preserve_qscores_less_than=6 globalQScorePrior=-1.0 validation_strictness=SILENT remove_program_records=false keep_program_records=false sample_rename_mapping_file=null unsafe=null disable_auto_index_creation_and_locking_when_reading_rods=false no_cmdline_in_header=false sites_only=false never_trim_vcf_format_field=false bcf=false bam_compression=null simplifyBAM=false disable_bam_indexing=false generate_md5=false num_threads=1 num_cpu_threads_per_data_thread=1 num_io_threads=0 monitorThreadEfficiency=false num_bam_file_handles=null read_group_black_list=null pedigree=[] pedigreeString=[] pedigreeValidationType=STRICT allow_intervals_with_unindexed_bam=false generateShadowBCF=false variant_index_type=DYNAMIC_SEEK variant_index_parameter=-1 reference_window_stop=0 logging_level=INFO log_to_file=null help=false version=false variant=(RodBinding name=var",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4252#issuecomment-364237834:4069,perform,performanceLog,4069,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4252#issuecomment-364237834,1,['perform'],['performanceLog']
Performance,"e to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.varia.NullAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.varia.NullAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""NullAppender"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.varia.NullAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""or",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312229998:5462,load,loaded,5462,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312229998,1,['load'],['loaded']
Performance,"e to allow previous PoNs to be provided via -I as an additional source of read counts. Remaining TODO's:. - [x] Allow DenoiseReadCounts to be run without a PoN. This will just perform standardization and optional GC correction. This gives us the ability to run the case sample pipeline without a PoN, which will give users more options and might be good enough if the data is not too noisy.; - [x] Actually, I'm not sure why we take perform SVD on the intervals x samples matrix and take the left-singular vectors. <s>Typically, SVD is performed on the samples x intervals matrix and the right-singular vectors are taken, which saves some extra computation that is necessary to calculate the left-singular vectors.</s> (EDIT: Actually, looks like Spark's SVD is faster on tall and skinny matrices, which might be due to the fact that the underlying implementation calls Fortran code. I still think that representing samples as row vectors has some benefits, so I've changed things to reflect this; I now just take a transpose before performing the SVD, so that we still operate on the same intervals x samples matrix.) This will also save us some transposing, which we do anyway to make HDF5 writes faster.; - [x] Change HDF5 matrix writing to allow matrices with NxM > MAX_INT, which can be done naively by chunking and writing to multiple HDF5 subdirectories. This will allow for smaller bin sizes. (EDIT: I implemented this in a way that allows one to set the maximum number of values allowed per chunk, so that heap usage can be controlled, but the downside is that this translates into a corresponding limit on the number of columns (i.e., intervals). On the other hand, you could theoretically crank this number up to Integer.MAX_VALUE, as long as you set -Xmx high enough... In practice, it's very unlikely that we'll need to go to bins smaller than a read length.); - [ ] <s>Check that CreatePanelOfNormals works correctly on Spark cluster.</s> Implement Randomized SVD, which should give bet",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351:2412,perform,performing,2412,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351,1,['perform'],['performing']
Performance,"e up 20GB as a combined TSV file and a whopping 63GB as individual TSV files---as well as the eigenvectors, filtering results, etc.). The run can be found in /dsde/working/slee/wgs-pon-test/tieout/no-gc. It completed successfully with **-Xmx32G** (in comparison, CreatePanelOfNormals crashed after 40 minutes with -Xmx128G). The runtime breakdown was as follows:. - ~45 minutes simply from reading of the 90 TSV read-count files in serial. Hopefully #3349 should greatly speed this up. (In comparison, CombineReadCounts reading 10 files in parallel at a time took ~100 minutes to create the aforementioned 20GB combined TSV file, creating 25+GB of temp files along the way.). - ~5 minutes from the preprocessing and filtering steps. We could probably further optimize some of this code in terms of speed and heap usage. (I had to throw in a call to System.gc() to avoid an OOM with -Xmx32G, which I encountered in my first attempt at the run...). - ~5 minutes from performing the SVD of the post-filtering 8643028 x 86 matrix, maintaining 30 eigensamples. I could write a quick implementation of randomized SVD, which I think could bring this down a bit (the scikit-learn implementation takes <2 minutes on a 10M x 100 matrix), but this can probably wait. Clearly making I/O faster and more space efficient is the highest priority. Luckily it's also low hanging fruit. The 8643028 x 30 matrix of eigenvectors takes <2 minutes to read from HDF5 when the WGS PoN is used in DenoiseReadCounts, which gives us a rough idea of how long it should take to read in the original ~11.5M x 90 counts from HDF5. So once #3349 is in, then I think that a **~15 minute single-core WGS PoN could easily be viable**. I believe that a PoN on the order of this size will be all that is required for WGS denoising, if it is not already overkill. To go bigger by more than an order of magnitude, we'll have to go out of core, which will require more substantial changes to the code. But since the real culprit responsible ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-317614503:1167,perform,performing,1167,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-317614503,1,['perform'],['performing']
Performance,"e+04 --max_copy_number=5 --p_active=0.010000 --class_coherence_length=10000.000000 --learning_rate=1.000000e-02 --adamax_beta1=9.000000e-01 --adamax_beta2=9.900000e-01 --log_emission_samples_per_round=50 --log_emission_sampling_rounds=10 --log_emission_sampling_median_rel_error=5.000000e-03 --max_advi_iter_first_epoch=5000 --max_advi_iter_subsequent_epochs=200 --min_training_epochs=10 --max_training_epochs=50 --initial_temperature=1.500000e+00 --num_thermal_advi_iters=2500 --convergence_snr_averaging_window=500 --convergence_snr_trigger_threshold=1.000000e-01 --convergence_snr_countdown_window=10 --max_calling_iters=10 --caller_update_convergence_threshold=1.000000e-03 --caller_internal_admixing_rate=7.500000e-01 --caller_external_admixing_rate=1.000000e+00 --disable_caller=false --disable_sampler=false --disable_annealing=false; Stdout: 14:13:50.032 INFO cohort_denoising_calling - Loading 24 read counts file(s)...; 14:13:53.719 INFO gcnvkernel.io.io_metadata - Loading germline contig ploidy and global read depth metadata...; 14:13:58.626 INFO gcnvkernel.tasks.task_cohort_denoising_calling - Instantiating the denoising model (warm-up)...; 14:14:04.543 INFO gcnvkernel.models.fancy_model - Global model variables: {'W_tu', 'psi_t_log__', 'ard_u_log__', 'log_mean_bias_t'}; 14:14:04.544 INFO gcnvkernel.models.fancy_model - Sample-specific model variables: {'z_su', 'psi_s_log__', 'read_depth_s_log__'}; 14:14:04.544 WARNING gcnvkernel.tasks.inference_task_base - No log emission sampler given; skipping the sampling step; 14:14:04.544 WARNING gcnvkernel.tasks.inference_task_base - No caller given; skipping the calling step; 14:14:04.544 INFO gcnvkernel.tasks.inference_task_base - Instantiating the convergence tracker...; 14:14:04.544 INFO gcnvkernel.tasks.inference_task_base - Setting up DA-ADVI...; 14:14:10.902 INFO gcnvkernel.tasks.inference_task_base - (denoising (warm-up)) starting...: 0%| | 0/5000 [00:00<?, ?it/s]; 14:14:12.877 INFO gcnvkernel.tasks.inference_task_base ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4825#issuecomment-467406398:3312,Load,Loading,3312,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4825#issuecomment-467406398,1,['Load'],['Loading']
Performance,e-4.2.6.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx8G -Djava.io.tmpdir=./ -jar /data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar BaseRecalibrator -R /data/reference/gatk_resource/Homo_sapiens_assembly38.fasta -I /data/xieduo/Immun_genomics/data/Łuksza_2022_Nature/bam/PAAD11N.bam --known-sites /data/xieduo/WES_pipe/pipeline/gatk_resource/dbsnp_146.hg38.vcf.gz --known-sites /data/reference/gatk_resource/1000G_phase1.snps.high_confidence.hg38.vcf.gz --known-sites /data/reference/gatk_resource/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz -O PAAD11N.recal_data.test.table; 13:46:24.742 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 13:46:24.761 WARN NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (No such file or directory); 13:46:24.764 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 13:46:24.764 WARN NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (No such file or directory); 13:46:24.884 INFO BaseRecalibrator - ------------------------------------------------------------; 13:46:24.884 INFO BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.2.6.1; 13:46:24.885 INFO BaseRecalibrator - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:46:24.885 INFO BaseRecalibrator - Executing as xieduo@pbs-master on Linux v3.10.0-1160.41.1.el7.x86_64 amd64; 13:46:24.885 INFO BaseRecalibrator - Java runtime: Java HotSpot(TM) 64-Bit Server VM v18+36-2087; 13:46:24.885 INFO BaseRecalibrator - Start Dat,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8005#issuecomment-1254561081:13016,load,load,13016,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005#issuecomment-1254561081,1,['load'],['load']
Performance,e.AbstractCallSite.call(AbstractCallSite.java:125); 	at com.github.jengelman.gradle.plugins.shadow.tasks.ShadowCopyAction$1.execute(ShadowCopyAction.groovy:78); 	at com.github.jengelman.gradle.plugins.shadow.tasks.ShadowCopyAction$1$execute.call(Unknown Source); 	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:48); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:113); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:125); 	at com.github.jengelman.gradle.plugins.shadow.tasks.ShadowCopyAction.withResource(ShadowCopyAction.groovy:109); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:93); 	at org.codehaus.groovy.runtime.callsite.StaticMetaMethodSite$StaticMetaMethodSiteNoUnwrapNoCoerce.invoke(StaticMetaMethodSite.java:151); 	at org.codehaus.groovy.runtime.callsite.StaticMetaMethodSite.callStatic(StaticMetaMethodSite.java:102); 	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallStatic(CallSiteArray.java:56); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callStatic(AbstractCallSite.java:194); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callStatic(AbstractCallSite.java:214); 	at com.github.jengelman.gradle.plugins.shadow.tasks.ShadowCopyAction.execute(ShadowCopyAction.groovy:75); 	at org.gradle.api.internal.file.copy.NormalizingCopyActionDecorator.execute(NormalizingCopyActionDecorator.java:53); 	at org.gradle.api.internal.file.copy.DuplicateHandlingCopyActionDecorator.execute(DuplicateHandlingCopyActionDecorator.java:42); 	at org.gradle.api.internal.file.copy.CopyActionExecuter.execute(CopyActionExecuter.java:40,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5499#issuecomment-446253445:5626,Cache,CachedMethod,5626,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5499#issuecomment-446253445,1,['Cache'],['CachedMethod']
Performance,"e.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:94); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:838); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:115); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:131); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:152); 	at org.broadinstitute.hellbender.Main.main(Main.java:230); Caused by: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: java.lang.IllegalArgumentException: Position should be non-negative, is %d [-1056964608]; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.getBuf(SeekableByteChannelPrefetcher.java:136); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.fetch(SeekableByteChannelPrefetcher.java:255); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:300); 	... 44 more; Caused by: com.google.cloud.storage.StorageException: java.lang.IllegalArgumentException: Position should be non-negative, is %d [-1056964608]; 	at com.google.cloud.storage.StorageException.translateAndThrow(StorageException.java:71); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:139); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:113); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(Se",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317782472:7920,concurren,concurrent,7920,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317782472,1,['concurren'],['concurrent']
Performance,"e40-2025-46fd-9aa0-d591a3799007/call-CHMSampleHeadToHead/BenchmarkComparison/f65a7960-7b66-4a5d-a346-bd188a1b3830/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""CHM controlindelF1Score"": ""0.8724"",; ""CHM controlindelPrecision"": ""0.8814"",; ""CHM controlsnpF1Score"": ""0.9784"",; ""CHM controlsnpPrecision"": ""0.9706"",; ""CHM controlsnpRecall"": ""0.9863"",; ""CHM controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/e6f57e40-2025-46fd-9aa0-d591a3799007/call-CHMSampleHeadToHead/BenchmarkComparison/f65a7960-7b66-4a5d-a346-bd188a1b3830/call-BenchmarkVCFControlSample/Benchmark/8d0e47ca-66f5-42a0-8785-6aa8d2db2663/call-CombineSummaries/summary.csv"",; ""CHM evalHCprocesshours"": ""80.5165222222222"",; ""CHM evalHCsystemhours"": ""0.1713305555555555"",; ""CHM evalHCwallclockhours"": ""53.10978888888891"",; ""CHM evalHCwallclockmax"": ""2.7458416666666667"",; ""CHM evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/e6f57e40-2025-46fd-9aa0-d591a3799007/call-CHMSampleHeadToHead/BenchmarkComparison/f65a7960-7b66-4a5d-a346-bd188a1b3830/call-EVALRuntimeTask/cacheCopy/monitoring.pdf"",; ""CHM evalindelF1Score"": ""0.8724"",; ""CHM evalindelPrecision"": ""0.8814"",; ""CHM evalsnpF1Score"": ""0.9784"",; ""CHM evalsnpPrecision"": ""0.9706"",; ""CHM evalsnpRecall"": ""0.9863"",; ""CHM evalsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/e6f57e40-2025-46fd-9aa0-d591a3799007/call-CHMSampleHeadToHead/BenchmarkComparison/f65a7960-7b66-4a5d-a346-bd188a1b3830/call-BenchmarkVCFTestSample/Benchmark/96c96714-3ac6-4d2b-a79c-57086cda6373/call-CombineSummaries/summary.csv"",; ""EXOME1 controlindelF1Score"": ""0.727"",; ""EXOME1 controlindelPrecision"": ""0.632"",; ""EXOME1 controlsnpF1Score"": ""0.9878"",; ""EXOME1 controlsnpPrecision"": ""0.9815"",; ""EXOME1 controlsnpRecall"": ""0.9941"",; ""EXOME1 controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/e6f57e40-2025-46fd-9aa0-d591a3799007/call-EXOME1Sampl",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7651#issuecomment-1069378815:11445,cache,cacheCopy,11445,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7651#issuecomment-1069378815,1,['cache'],['cacheCopy']
Performance,eExecutorsAction.execute(DefaultTaskExecutionGraph.java:343); at org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$BuildOperationAwareExecutionAction.execute(DefaultTaskExecutionGraph.java:336); at org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$BuildOperationAwareExecutionAction.execute(DefaultTaskExecutionGraph.java:322); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker$1.execute(DefaultPlanExecutor.java:134); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker$1.execute(DefaultPlanExecutor.java:129); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker.execute(DefaultPlanExecutor.java:202); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker.executeNextNode(DefaultPlanExecutor.java:193); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker.run(DefaultPlanExecutor.java:129); at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64); at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48); at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56); Caused by: org.gradle.api.GradleException: Javadoc generation failed. Generated Javadoc options file (useful for troubleshooting): '/home/cb2/gatk/build/tmp/gatkDoc/javadoc.options'; at org.gradle.api.tasks.javadoc.internal.JavadocGenerator.execute(JavadocGenerator.java:58); at org.gradle.api.tasks.javadoc.internal.JavadocGenerator.execute(JavadocGenerator.java:31); at org.gradle.api.tasks.javadoc.Javadoc.executeExternalJavadoc(Javadoc.java:158); at org.gradle.api.tasks.javadoc.Javadoc.generate(Javadoc.java:146); at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at org.gradle.interna,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4155#issuecomment-566796716:5855,concurren,concurrent,5855,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4155#issuecomment-566796716,1,['concurren'],['concurrent']
Performance,eExecutorsAction.execute(DefaultTaskExecutionGraph.java:343); at org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$BuildOperationAwareExecutionAction.execute(DefaultTaskExecutionGraph.java:336); at org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$BuildOperationAwareExecutionAction.execute(DefaultTaskExecutionGraph.java:322); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker$1.execute(DefaultPlanExecutor.java:134); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker$1.execute(DefaultPlanExecutor.java:129); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker.execute(DefaultPlanExecutor.java:202); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker.executeNextNode(DefaultPlanExecutor.java:193); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker.run(DefaultPlanExecutor.java:129); at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64); at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48); at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56); Caused by: org.gradle.api.GradleException: Javadoc generation failed. Generated Javadoc options file (useful for troubleshooting): '/usr/bin/gatk/build/tmp/gatkDoc/javadoc.options'; at org.gradle.api.tasks.javadoc.internal.JavadocGenerator.execute(JavadocGenerator.java:58); at org.gradle.api.tasks.javadoc.internal.JavadocGenerator.execute(JavadocGenerator.java:31); at org.gradle.api.tasks.javadoc.Javadoc.executeExternalJavadoc(Javadoc.java:158); at org.gradle.api.tasks.javadoc.Javadoc.generate(Javadoc.java:146); at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at org.gradle.internal,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6466#issuecomment-590387973:4729,concurren,concurrent,4729,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6466#issuecomment-590387973,1,['concurren'],['concurrent']
Performance,"ed to, since `PreprocessIntervals` will output a Picard interval list, and `AnnotateTargets` outputs a target file).; - [x] Integration tests are still needed for `CreateReadCountPanelOfNormals`. These might not test for correctness, but we could possibly compare to old PoNs. Segmentation/modeling:; - Instead of separate tools for copy-ratio segmentation (`PerformSegmentation`) and allele-fraction segmentation/union/modeling (`AllelicCNV`), there is now just a single segmentation/modeling tool (`ModelSegments`).; - Input is denoised copy ratio and/or allelic counts. If only one input is provided, then we only model only the corresponding quantity.; - There is no separate allele-fraction workflow. Unlike the old approach, we do not perform any genotyping or modeling before doing kernel segmentation.; - [x] Old code and classes are used for segment union. We should port or possibly replace this with a simple method that uses kernel segmentation. EDIT: Actually, just tried running a WGS sample and this is still a major bottleneck. EDIT 2: Hmm...actually doesn't seem to be an issue on my desktop (compared to my laptop, on which the run hangs here). Will try to track down the source of the discrepancy. EDIT 3: Added segment union based on single-changepoint detection using kernel segmentation.; - [x] Segment union should be replaced by a proper joint kernel segmentation. EDIT: I've added this, but there could be some minor improvements. Right now, only use one het per copy-ratio interval and throw away those off-target/bin. There are a few percent of targets/bins that have more than one het, and we could potentially rescue the off-target/bin hets as well with some care.; - Old code and models are used for modeling. Since the old allele-fraction model only models hets, we perform a `GetHetCoverage`-like binomial genotyping step (and output the results) before modeling. However, instead of assuming the null hypothesis of het (f = 0.5) and accepting a site when we cannot re",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:3795,bottleneck,bottleneck,3795,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828,1,['bottleneck'],['bottleneck']
Performance,"ed with single line brackets. I.e. gatk typically uses. ```; if ( something ) {; doThing; } else {; otherThing; }; ```. rather than. ```; if ( something ) ; {; doThing; }; else ; {; otherThing; }; ```. 2) You use a lot of raw iterators, which is fine and is necessary in many cases. In other cases those operations can be written much more succinctly with either a for-each loop, or a stream. i.e. . ```; List<Integer> values;; Iterator itr = iterable.iterator();; while(itr.hasNext()){; Element elem = itr.next();; int value = someFunction(elem); if ( value > SOME_CONSTANT) {; values.append(value); }; }; return values;; ```. can be . ```; return StreamSupport.stream(iterable.spliterator, false); .map( elem -> someFunction(elem)); .filter( value -> value > SOME_CONSTANT ); .collect( Collectors.toList()); ```. We should probably add a utilty function to convert an iterator to a stream directly so we can stream iterators easily even if there is no associated iteratable. . 3) The tools need tests. This is important. 4) It would be good to think about how the tools can be composited into a spark pipline and run without writing intermediate files. . 5) Bitwise operations are a rarity in GATK and many of our users will not be very comfortable with them. Please avoid bit twiddling tricks when possible. When it's not possible (i.e. when you are performing tricks to treat a long as a set of byte pairs) please add detailed explanation to the comments so that readers who are less familiar will be able to follow along. Likewise all magic values should be named constants unless they are extremely obvious. . 6) Avoid state like the plague. Everything that can reasonable be static should be. Some things will require mutable state, but avoid it as much as possible. . Similarly, static mutable objects should be avoided like a plague infected with more plagues. Never expose a static object that could be mutated. (you have a static array, it's ok because it's private and nothing mutates it)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1435#issuecomment-172985394:1915,perform,performing,1915,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1435#issuecomment-172985394,1,['perform'],['performing']
Performance,"eed up the old allele-fraction model, which is now the main bottleneck. An easy (lazy) strategy might simply be to downsample and scale likelihoods when estimating global parameters. Addresses #2884.; - [x] Even though the simple copy-ratio model is much faster, it still takes ~15-20 minutes for 100 iterations on WGS, so we can downsample here too.; - [x] Integration tests are still needed; again, these might not test for correctness.; - I've added the ability to specify a prior for the minor-allele fraction, which alleviates the problem of residual bias in balanced segments.; - I've reduced the verbosity of the modeled-segments file. I only report posterior mode and 10%, 50%, and 90% deciles. Global parameters have the full deciles output in the .param files, but I removed the mode and highest density credible interval (because of the below item).; - [x] Some residual bias remains in the estimate of the minor-allele fraction posterior mode. This is simply because we are performing kernel density estimation of a bounded quantity. One possibility would be to logit transform to an unbounded support, perform the estimation, then transform back. EDIT: Just removed kernel density estimation for now, partly due to #3599 as well.; - Hmm, actually still a tiny bit of residual bias. This is apparent e.g. in WGS normals. I think focusing on a new allele-fraction model rather than trying to figure out where the old one is failing would be best.; - [x] For small bins (250bp), the copy-ratio model is currently a bit memory intensive, since it stores an outlier indicator boolean for every data point (it gets by with -Xmx12g for 100 iterations at 250bp). There is no easy away around storing this at the GibbsSampler level (although we could make some non-trivial changes to that code, as @davidbenjamin suggested long ago at https://github.com/broadinstitute/gatk-protected/issues/195). However, I got rid of these at the CopyRatioModeller level. If we want to go down in memory, we cou",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:6616,perform,performing,6616,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828,1,['perform'],['performing']
Performance,eflater false --use-jdk-inflater false --gcs-max-retries 20 --gcs-project-for-requester-pays --disable-tool-default-read-filters false. METRICS CLASS	org.broadinstitute.hellbender.utils.read.markduplicates.GATKDuplicationMetrics LIBRARY	UNPAIRED_READS_EXAMINED	READ_PAIRS_EXAMINED	SECONDARY_OR_SUPPLEMENTARY_RDS	UNMAPPED_READS	UNPAIRED_READ_DUPLICATES READ_PAIR_DUPLICATES	READ_PAIR_OPTICAL_DUPLICATES	PERCENT_DUPLICATION ESTIMATED_LIBRARY_SIZE; lib1	173613	53799913	0	7610605	81003	11974162	585768	0.222961	05870713. MarkDuplicates --INPUT /home/test/WGS_pipeline/TEST/output/orig_412.bowtie2.bam --OUTPUT /home/test/WGS_pipeline/TEST/output/orig_412.MarkDuplicates.bam --METRICS_FILE /home/test/WGS_pipeline/TEST/output/orig_412.MarkDuplicates-metrics.txt --MAX_SEQUENCES_FOR_DISK_READ_ENDS_MAP 50000 --MAX_FILE_HANDLES_FOR_READ_ENDS_MAP 8000 --SORTING_COLLECTION_SIZE_RATIO 0.25 --TAG_DUPLICATE_SET_MEMBERS false --REMOVE_SEQUENCING_DUPLICATES false --TAGGING_POLICY DontTag --CLEAR_DT true --ADD_PG_TAG_TO_READS true --REMOVE_DUPLICATES false --ASSUME_SORTED false --DUPLICATE_SCORING_STRATEGY SUM_OF_BASE_QUALITIES --PROGRAM_RECORD_ID MarkDup; licates --PROGRAM_GROUP_NAME MarkDuplicates --READ_NAME_REGEX <optimized capture of last three ':' separated fields as numeric values> --OPTICAL_DUPLICATE_PIXEL_DISTANCE 100 --MAX_OPTICAL_DUPLICATE_SET_SIZE 300000 --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX false --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help f; alse --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false. METRICS CLASS	picard.sam.DuplicationMetrics; LIBRARY	UNPAIRED_READS_EXAMINED	READ_PAIRS_EXAMINED	SECONDARY_OR_SUPPLEMENTARY_RDS	UNMAPPED_READS	UNPAIRED_READ_DUPLICATES	READ_PAIR_DUPLICATES	READ_PAIR_OPTICAL_DUPLICATES	PERCENT_DUPLICATION ESTIMATED_LIBRARY_SIZE; lib1	173613	53799913	0	7610605	81003	11933661	585768	0.22221	06317338. ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427229905:2459,optimiz,optimized,2459,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427229905,1,['optimiz'],['optimized']
Performance,"eflater: Intel; Inflater: Intel; Provider GCS is available; Picard version: Version:4.1.7.0-41-g79586b8-SNAPSHOT; [Mon Jun 22 16:00:13 EDT 2020] picard.vcf.MergeVcfs done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=373293056; Tool returned:; 0; ```; The only way I can reproduce it is to delete one of the files so it *really* doesn't exist at the specified location:; ```; (base) /tmp/test a /Users/cnorman/projects/gatk/gatk MergeVcfs -I data/calling/my.vcf.gz -I data/calling/b.vcf.gz -O out.vcf.gz; Using GATK jar /Users/cnorman/projects/gatk/build/libs/gatk-package-4.1.7.0-41-g79586b8-SNAPSHOT-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /Users/cnorman/projects/gatk/build/libs/gatk-package-4.1.7.0-41-g79586b8-SNAPSHOT-local.jar MergeVcfs -I data/calling/my.vcf.gz -I data/calling/b.vcf.gz -O out.vcf.gz; 16:03:19.691 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/cnorman/projects/gatk/build/libs/gatk-package-4.1.7.0-41-g79586b8-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_compression.dylib; [Mon Jun 22 16:03:19 EDT 2020] MergeVcfs --INPUT data/calling/my.vcf.gz --INPUT data/calling/b.vcf.gz --OUTPUT out.vcf.gz --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX true --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false; [Mon Jun 22 16:03:19 EDT 2020] Executing as cnorman@WMCEA-78B on Mac OS X 10.13.6 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_111-b14; Deflater: Intel; Inflater: Intel; Provider GCS is available; Picard version: Version:4.1.7.0-41-g79586b8-SNAPSHOT; [Mon Jun 22 16:03:19 EDT 2020] picard.vcf.MergeVcfs done. Elapsed time: 0.00 minutes.; Runtime.totalMemory()=372244480; To get help, see ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6664#issuecomment-647743321:2563,Load,Loading,2563,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6664#issuecomment-647743321,1,['Load'],['Loading']
Performance,"eflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:182); 	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:164); 	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:412); 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64); 	at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 5.0 failed 1 times, most recent failure: Lost task 1.0 in stage 5.0 (TID 12, localhost, executor driver): java.util.ConcurrentModificationException; 	at java.util.ArrayList.sort(ArrayList.java:1464); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.<init>(ReadThreadingAssembler.java:81); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerReadThreadingAssemblerArgumentCollection.makeReadThreadingAssembler(HaplotypeCallerReadThreadingAssemblerArgumentCollection.java:37); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerArgumentCollection.createReadThreadingAssembler(Assemb",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429:7604,concurren,concurrent,7604,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429,1,['concurren'],['concurrent']
Performance,"ehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac --with-mpfr=/Users/ray/mc-x64-3.5/conda-bld/gcc-4.8_1477649012852/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac --with-mpc=/Users/ray/mc-x64-3.5/conda-bld/gcc-4.8_1477649012852/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac --with-isl=/Users/ray/mc-x64-3.5/conda-bld/gcc-4.8_1477649012852/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac --with-cloog=/Users/ray/mc-x64-3.5/conda-bld/gcc-4.8_1477649012852/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac --with-boot-ldflags='-Wl,-headerpad_max_install_names -Wl,-L/Users/ray/mc-x64-3.5/conda-bld/gcc-4.8_1477649012852/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac/lib -Wl,-L/usr/lib' --with-stage1-ldflags='-Wl,-headerpad_max_install_names -Wl,-L/Users/ray/mc-x64-3.5/conda-bld/gcc-4.8_1477649012852/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac/lib -Wl,-L/usr/lib' --enable-checking=release --with-tune=generic --disable-multilib; Thread model: posix; gcc version 4.8.5 (GCC); `; Perhaps using the Xcode version would fix things?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4742#issuecomment-391065177:3715,tune,tune,3715,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4742#issuecomment-391065177,1,['tune'],['tune']
Performance,el/CoverageModelArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL0NvdmVyYWdlTW9kZWxBcmd1bWVudENvbGxlY3Rpb24uamF2YQ==) | `86.592% <ø> (ø)` | `40 <0> (ø)` | :arrow_down: |; | [...gemodel/cachemanager/ComputableGraphStructure.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9Db21wdXRhYmxlR3JhcGhTdHJ1Y3R1cmUuamF2YQ==) | `100% <100%> (+26.994%)` | `63 <62> (+24)` | :arrow_up: |; | [...ragemodel/cachemanager/ComputableNodeFunction.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9Db21wdXRhYmxlTm9kZUZ1bmN0aW9uLmphdmE=) | `100% <100%> (+66.667%)` | `4 <1> (+2)` | :arrow_up: |; | [.../coveragemodel/cachemanager/DuplicableNDArray.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9EdXBsaWNhYmxlTkRBcnJheS5qYXZh) | `81.818% <100%> (+38.068%)` | `6 <2> (+2)` | :arrow_up: |; | [...s/coveragemodel/cachemanager/DuplicableNumber.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9EdXBsaWNhYmxlTnVtYmVyLmphdmE=) | `80% <100%> (+80%)` | `5 <2> (+5)` | :arrow_up: |; | [...coveragemodel/cachemanager/PrimitiveCacheNode.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9QcmltaXRpdmVDYWNoZU5vZGUuamF2YQ==) | `83.333% <71.429%> (+30.702%)` | `10 <7> (+3)` | :arrow_up: |; | [...er/tools/coveragemodel/cachemanager/CacheNode.ja,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3035#issuecomment-306412418:1927,cache,cachemanager,1927,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3035#issuecomment-306412418,1,['cache'],['cachemanager']
Performance,"ellbender/tools/flag_stat.bam \; --output countreads \; --runner SPARK; [July 14, 2015 1:14:50 PM EDT] org.broadinstitute.hellbender.tools.dataflow.pipelines.CountReadsDataflow --output countreads --input ./src/test/resources/org/broadinstitute/hellbender/tools/flag_stat.bam --runner SPARK --interval_set_rule UNION --interval_padding 0 --client_secret client_secret.json --numWorkers 0 --help false --version false --VERBOSITY INFO --QUIET false; [July 14, 2015 1:14:50 PM EDT] Executing as louisb@wm1b0-8ab on Mac OS X 10.9.5 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_45-b14; Version: Version:GATK.4.alpha-413-g741d007-SNAPSHOT JdkDeflater; 13:14:50.576 [main] INFO org.broadinstitute.hellbender.tools.dataflow.pipelines.CountReadsDataflow - Initializing engine; 13:14:50.577 [main] INFO org.broadinstitute.hellbender.tools.dataflow.pipelines.CountReadsDataflow - Done initializing engine; 13:14:50.971 [main] INFO org.broadinstitute.hellbender.tools.dataflow.pipelines.CountReadsDataflow - Loading client_secret.json; Jul 14, 2015 1:14:51 PM com.google.cloud.genomics.dataflow.utils.DataflowWorkarounds registerGenomicsCoders; INFO: Registering coders for genomics classes; 15/07/14 13:14:51 WARN reflections.Reflections: could not create Vfs.Dir from url. ignoring the exception and continuing; org.reflections.ReflectionsException: could not create Vfs.Dir from url, no matching UrlType was found [file:/Library/KeyAccess/KeyAccess.app/Contents/SharedFrameworks/KeyAccess.framework/KeyAccess]; either use fromURL(final URL url, final List<UrlType> urlTypes) or use the static setDefaultURLTypes(final List<UrlType> urlTypes) or addDefaultURLTypes(UrlType urlType) with your specialized UrlType.; at org.reflections.vfs.Vfs.fromURL(Vfs.java:109); at org.reflections.vfs.Vfs.fromURL(Vfs.java:91); at org.reflections.Reflections.scan(Reflections.java:237); at org.reflections.Reflections.scan(Reflections.java:204); at org.reflections.Reflections.<init>(Reflections.java:129); at com.google.c",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/639#issuecomment-121313713:1264,Load,Loading,1264,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/639#issuecomment-121313713,1,['Load'],['Loading']
Performance,"elow. ### Log from run with value of `--max-alternate-alleles` left at default value of 6:; ```; on-chinookomes-dna-seq-gatk-variant-calling]--% gatk --java-options ""-Xmx4g"" GenotypeGVCFs -R resources/genome.fasta -V gendb://results/genomics_db/chromosomes/CM031199.1 -O results/vcf_parts/CM031199.1.vcf.gz. Using GATK jar /home/eanderson/Documents/projects/yukon-chinookomes-dna-seq-gatk-variant-calling/.snakemake/conda/cd50d464/share/gatk4-4.2.4.1-0/gatk-package-4.2.4.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx4g -jar /home/eanderson/Documents/projects/yukon-chinookomes-dna-seq-gatk-variant-calling/.snakemake/conda/cd50d464/share/gatk4-4.2.4.1-0/gatk-package-4.2.4.1-local.jar GenotypeGVCFs -R resources/genome.fasta -V gendb://results/genomics_db/chromosomes/CM031199.1 -O results/vcf_parts/CM031199.1.vcf.gz; 22:17:18.737 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/eanderson/Documents/projects/yukon-chinookomes-dna-seq-gatk-variant-calling/.snakemake/conda/cd50d464/share/gatk4-4.2.4.1-0/gatk-package-4.2.4.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 16, 2022 10:17:18 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 22:17:18.863 INFO GenotypeGVCFs - ------------------------------------------------------------; 22:17:18.863 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.4.1; 22:17:18.863 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 22:17:18.864 INFO GenotypeGVCFs - Executing as eanderson@node34.cluster on Linux v4.18.0-193.28.1.el8_2.x86_64 amd64; 22:17:18.864 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v11.0.9.1-internal+0-adhoc..src; 22:17:18.864 INFO GenotypeGVCFs - Start Date/Time",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7639#issuecomment-1014180059:1803,Load,Loading,1803,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7639#issuecomment-1014180059,1,['Load'],['Loading']
Performance,enFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more; 18/04/24 17:42:11 INFO ShutdownHookManager: Shutdown hook called; 18/04/24 17:42:11 INFO ShutdownHookManager: Deleting directory /tmp/username/spark-99d4cb79-5c44-425b-8f72-9476e7fd884c; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:46039,concurren,concurrent,46039,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,2,['concurren'],['concurrent']
Performance,"encePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:94); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:838); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:115); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:131); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:152); 	at org.broadinstitute.hellbender.Main.main(Main.java:230); Caused by: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: java.lang.IllegalArgumentException: Position should be non-negative, is %d [-1056964608]; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.getBuf(SeekableByteChannelPrefetcher.java:136); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.fetch(SeekableByteChannelPrefetcher.java:255); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:300); 	... 44 more; Caused by: com.google.cloud.storage.StorageException: java.lang.IllegalArgumentException: Position should be non-negative, is %d [-1056964608]; 	at com.google.cloud.storage.StorageException.translateAndThrow(StorageException.java:71); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:139); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:113); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:131); 	at org.broadinstitute.hel",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317782472:7985,concurren,concurrent,7985,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317782472,1,['concurren'],['concurrent']
Performance,"ent Locus Elapsed Minutes Variants Processed Variants/Minute; 14:23:57.323 WARN ReferenceConfidenceVariantContextMerger - Detected invalid annotations: When trying to merge variant contexts at location chr17:18363145 the annotation AS_RAW_MQ=64800.000|50400.000|0.000 was not a numerical value and was ignored; 14:23:57.346 WARN ReferenceConfidenceVariantContextMerger - Reducible annotation 'AS_RAW_MQ' detected, add -G Standard -G AS_Standard to the command to annotate in the final VC with this annotation.; 14:23:58.180 INFO ProgressMeter - chr17:18363854 32.3 1000 31.0; 14:24:13.258 INFO ProgressMeter - chr17:18376854 32.6 14000 430.0; 14:24:58.358 INFO ProgressMeter - chr17:18382854 33.3 20000 600.5; 14:32:49.287 INFO ProgressMeter - chr17:18393855 41.2 31000 753.2; 14:33:39.240 INFO ProgressMeter - chr17:18405856 42.0 43000 1024.1; 14:33:49.493 INFO ProgressMeter - chr17:18411856 42.2 49000 1162.3; 14:34:17.285 INFO ProgressMeter - chr17:18425856 42.6 63000 1478.1; ```. CPU utilisation does not improve after the variants begin processing after half an hour preparing traversal. ```; %CPU WallTime Time Lim RSS mem memlim cpus; normal-exe = open&run; 105581211 R ds6924 hm82 genotype 4 00:42:34 02:00:00 1200GB 1200GB 3072GB 768; ```. - Excellent CPU efficiency if running serially (but defeats the purpose of a H.P.C. with Lustre). ```; %CPU WallTime Time Lim RSS mem memlim cpus; normal-exe = open&run; 105381052 R ds6924 hm82 genotype 61 00:19:55 10:00:00 1487MB 1487MB 4096MB 1. 09:17:51.114 INFO ProgressMeter - chr10:106687146 1.2 1000 822.3; 09:18:01.308 INFO ProgressMeter - chr10:106710146 1.4 24000 17315.6; 09:18:21.691 INFO ProgressMeter - chr10:106721171 1.7 35000 20281.0; 09:18:31.944 INFO ProgressMeter - chr10:106742172 1.9 56000 29526.0; ```. Intervals take about fifteen minutes each instead of about seven hours if running serially. Outputting results to `$PBS_JOBFS` folder on compute node instead of directly to project folder did not improve performance at all.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8637#issuecomment-1879551089:2932,perform,performance,2932,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8637#issuecomment-1879551089,1,['perform'],['performance']
Performance,ent&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL0NvbGxlY3RSZWFkQ291bnRzLmphdmE=) | `85.484% <ø> (ø)` | |; | [...ools/copynumber/CreateReadCountPanelOfNormals.java](https://codecov.io/gh/broadinstitute/gatk/pull/7954/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL0NyZWF0ZVJlYWRDb3VudFBhbmVsT2ZOb3JtYWxzLmphdmE=) | `89.831% <ø> (ø)` | |; | [...e/hellbender/tools/copynumber/utils/HDF5Utils.java](https://codecov.io/gh/broadinstitute/gatk/pull/7954/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL3V0aWxzL0hERjVVdGlscy5qYXZh) | `79.787% <ø> (ø)` | |; | [...scalable/modeling/BGMMVariantAnnotationsModel.java](https://codecov.io/gh/broadinstitute/gatk/pull/7954/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvbW9kZWxpbmcvQkdNTVZhcmlhbnRBbm5vdGF0aW9uc01vZGVsLmphdmE=) | `0.000% <0.000%> (ø)` | |; | [...calable/modeling/BGMMVariantAnnotationsScorer.java](https://codecov.io/gh/broadinstitute/gatk/pull/7954/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvbW9kZWxpbmcvQkdNTVZhcmlhbnRBbm5vdGF0aW9uc1Njb3Jlci5qYXZh) | `0.000% <0.000%> (ø)` | |; | [...oadinstitute/hellbender/utils/NaturalLogUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/7954/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7954#issuecomment-1191010834:2695,scalab,scalable,2695,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7954#issuecomment-1191010834,1,['scalab'],['scalable']
Performance,ent.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:300); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeMedia(AbstractGoogleClientRequest.java:380); 	at shaded.cloud_nio.com.google.api.services.storage.Storage$Objects$Get.executeMedia(Storage.java:5130); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:494); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:127); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:124); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:93); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:49); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:124); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:113); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:131); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:104); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: java.io.EOFException: SSL peer shut down incorrectly; 	at sun.security.ssl.InputRecord.read(InputRecord.java:505); 	at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973); 	... 34 more; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3070#issuecomment-309120156:3652,concurren,concurrent,3652,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3070#issuecomment-309120156,3,['concurren'],['concurrent']
Performance,ents&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUmVhZHNEYXRhU291cmNlLmphdmE=) | `91.667% <84.615%> (+33.333%)` | :arrow_up: |; | [...ellbender/engine/VariantWalkerIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvVmFyaWFudFdhbGtlckludGVncmF0aW9uVGVzdC5qYXZh) | `87.288% <86.667%> (ø)` | |; | [...engine/cache/DrivingFeatureInputCacheStrategy.java](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvY2FjaGUvRHJpdmluZ0ZlYXR1cmVJbnB1dENhY2hlU3RyYXRlZ3kuamF2YQ==) | `88.000% <88.000%> (ø)` | |; | [...ellbender/engine/cache/LocatableCacheUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvY2FjaGUvTG9jYXRhYmxlQ2FjaGVVbml0VGVzdC5qYXZh) | `96.471% <96.471%> (ø)` | |; | [...gumentcollections/ReadInputArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL2FyZ3VtZW50Y29sbGVjdGlvbnMvUmVhZElucHV0QXJndW1lbnRDb2xsZWN0aW9uLmphdmE=) | `87.500% <100.000%> (+20.833%)` | :arrow_up: |; | [...org/broadinstitute/hellbender/engine/GATKTool.java](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comm,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4902#issuecomment-397744741:3914,cache,cache,3914,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4902#issuecomment-397744741,1,['cache'],['cache']
Performance,eption: com.google.cloud.storage.StorageException: All 20 reopens failed. Waited a total of 1918000 ms between attempts; at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lambda$getFeatureReadersInParallel$614(GenomicsDBImport.java:605); at java.util.LinkedHashMap.forEach(LinkedHashMap.java:684); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getFeatureReadersInParallel(GenomicsDBImport.java:600); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.createSampleToReaderMap(GenomicsDBImport.java:491); at com.intel.genomicsdb.importer.GenomicsDBImporter.lambda$null$2(GenomicsDBImporter.java:602); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590); ... 3 more; Caused by: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: All 20 reopens failed. Waited a total of 1918000 ms between attempts; at java.util.concurrent.FutureTask.report(FutureTask.java:122); at java.util.concurrent.FutureTask.get(FutureTask.java:192); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lambda$getFeatureReadersInParallel$614(GenomicsDBImport.java:602); ... 8 more; Caused by: com.google.cloud.storage.StorageException: All 20 reopens failed. Waited a total of 1918000 ms between attempts; at com.google.cloud.storage.contrib.nio.CloudStorageRetryHandler.handleReopenForStorageException(CloudStorageRetryHandler.java:124); at com.google.cloud.storage.contrib.nio.CloudStorageRetryHandler.handleStorageException(CloudStorageRetryHandler.java:94); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:621); at java.nio.file.Files.exists(Files.java:2385); at htsjdk.tribble.util.ParsingUtils.resourceExists(ParsingUtils.java:419); at htsjdk.tribble.AbstractFeatureReader.isTabix(AbstractFeatureReader.java:222); at htsjdk.tribble.AbstractFeatureReader$ComponentMethods.isTabix(AbstractFeatureReader.java:228); at ht,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-412904420:2393,concurren,concurrent,2393,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-412904420,1,['concurren'],['concurrent']
Performance,"er it's worth consolidating some of the parameter sets at this stage? I think there's an argument for having at least two sets (haplotype-to-reference + read-to-haplotype), but I'm not sure how to justify having a separate set for dangling heads/tails. But also not sure which set the latter should be consolidated with---@jamesemery thoughts? Again, let me reiterate that it seems that many of these parameter values were chosen arbitrarily (or, if not, that the procedure for choosing them has been lost). As a start, you can see the results of some optimizations I did on the CHM mix on slide 15 at https://docs.google.com/presentation/d/1zGuquAZWSUQ-wNxp8D6HhGNjIaFcV0_X9WAS4LODbEo/edit?usp=sharing Here, I optimized over haplotype-to-reference + read-to-haplotype SW parameters on various metrics after variant normalization using vcfeval. These optimizations were done using the Bayesian optimization framework I prototyped long ago (see https://github.com/broadinstitute/gatk-evaluation/tree/master/pipeline-optimizer and https://docs.google.com/presentation/d/1t5WOAEOMp0xAzJgpKbP68BUnclNYfIVRrDSL9wl1-3A/edit?usp=sharing); this entailed running parameter scans using a local Cromwell on my desktop. Probably this optimization work could be redone relatively easily using the Neptune framework put together by @dalessioluca, which was still in development at the time I did this work. Happy to share the resources and scripts I used if we go down this route; they are pretty lightweight. See more discussion starting here: https://github.com/broadinstitute/gatk/issues/5564#issuecomment-710107566. Alternatively, we could merge this branch to expose the parameters now and punt on consolidating/optimizing them. I'm not completely convinced we should even do the former unless we are going to follow through on the latter, but happy to defer to others. Finally, note also there is one code optimization that I removed, since it makes assumptions on the SW parameter values that might not be va",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6885#issuecomment-891907471:1983,optimiz,optimizer,1983,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6885#issuecomment-891907471,1,['optimiz'],['optimizer']
Performance,erationExecutor.java:199); 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:110); 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker.execute(DefaultTaskGraphExecuter.java:249); 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker.execute(DefaultTaskGraphExecuter.java:238); 	at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor$TaskExecutorWorker.processTask(DefaultTaskPlanExecutor.java:123); 	at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor$TaskExecutorWorker.access$200(DefaultTaskPlanExecutor.java:79); 	at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor$TaskExecutorWorker$1.execute(DefaultTaskPlanExecutor.java:104); 	at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor$TaskExecutorWorker$1.execute(DefaultTaskPlanExecutor.java:98); 	at org.gradle.execution.taskgraph.DefaultTaskExecutionPlan.execute(DefaultTaskExecutionPlan.java:663); 	at org.gradle.execution.taskgraph.DefaultTaskExecutionPlan.executeWithTask(DefaultTaskExecutionPlan.java:597); 	at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor$TaskExecutorWorker.run(DefaultTaskPlanExecutor.java:98); 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:63); 	at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:46); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:55); 	at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: /wrkdirs/usr/ports/biology/gatk/work/gatk-4.0.11.0/build/classes/java/main/org/broadinstitute/hellbender/metrics/MultiLevelCollector$Distributor.class (Too many open files); 	at java.io.FileInputStream.open0(Native Method); ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5499#issuecomment-446253445:12075,concurren,concurrent,12075,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5499#issuecomment-446253445,5,['concurren'],['concurrent']
Performance,"erent results by the first filtering step (on intervals by interval median). @LeeTL1220 @davidbenjamin what is the ""official ReCapSeg"" behavior, and do we want to keep the current behavior? In general, I think all of the standardization (i.e., filtering/imputation/truncation/transformation) steps could stand some revisiting. Evaluation:. - [ ] Revisit standardization procedure by checking with simulated data. We should make sure that the centering of the data does not rescale the true copy ratio.; - [x] <s>Investigate the effect of keeping duplicates. I am still not sure why we do this, and it may have a more drastic impact on WGS data.</s> Turns out we don't keep duplicates for WGS; see #3367.; - [ ] Check that GC-bias-correction+PCA and PCA-only perform comparably, even at small bin sizes (~300bp). From what I've seen, this is true for larger bin sizes (~3kbp), so explicit GC-bias correction may not be necessary. (That is, even at these (purportedly) large bin sizes, the effect of the read-based GC-bias correction is obvious for those samples where it is important. However, the end result is not very different from PCA-only denoising with no GC-bias correction performed.); - [x] <s>Check that changing CBS alpha parameter sufficiently reduces hypersegmentation.</s> <s>Looks like the hybrid p-value calculation in DNACopy is not accurate enough to handle WGS-size data. (Also, it's relatively slow, taking ~30 minutes on ~10M intervals.) Even if I set alpha to 0, I still get a ridiculous number of segments! So I think it's finally time to scrap CBS. I'll look into other R segmentation packages that might give us a quick drop-in solution, but we may want to roll our own simple algorithm (which we will scrap anyway once the coverage model is in for somatic).</s> I've implemented a fast kernel-segmentation method that seems very promising, see below.; - [ ] Investigate performance vs. CGA ReCapSeg pipeline on THCA samples.; - [ ] Investigate concordance with Genome STRiP.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351:5096,perform,performed,5096,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351,2,['perform'],"['performance', 'performed']"
Performance,"error message:. ```; 01:15:27.623 INFO GenotypeGVCFs - Shutting down engine; GENOMICSDB_TIMER,GenomicsDB iterator next() timer,Wall-clock time(s),8476.664214527651,Cpu time(s),8391.206707930733; [January 14, 2020 1:15:30 AM BRT] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 279.78 minutes.; Runtime.totalMemory()=16865820672; htsjdk.tribble.TribbleException: Invalid block size -122708061; at htsjdk.variant.bcf2.BCF2Decoder.readNextBlock(BCF2Decoder.java:66); at htsjdk.variant.bcf2.BCF2Codec.decode(BCF2Codec.java:134); at htsjdk.variant.bcf2.BCF2Codec.decode(BCF2Codec.java:58); at org.genomicsdb.reader.GenomicsDBFeatureIterator.next(GenomicsDBFeatureIterator.java:181); at org.genomicsdb.reader.GenomicsDBFeatureIterator.next(GenomicsDBFeatureIterator.java:49); at org.broadinstitute.hellbender.engine.FeatureIntervalIterator.loadNextFeature(FeatureIntervalIterator.java:98); at org.broadinstitute.hellbender.engine.FeatureIntervalIterator.loadNextNovelFeature(FeatureIntervalIterator.java:74); at org.broadinstitute.hellbender.engine.FeatureIntervalIterator.next(FeatureIntervalIterator.java:62); at org.broadinstitute.hellbender.engine.FeatureIntervalIterator.next(FeatureIntervalIterator.java:24); at java.util.Iterator.forEachRemaining(Iterator.java:116); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEachOrdered(ReferencePipeline.java:490); at org.broadinstitute.hellbender.engine.VariantLocusWalker.traverse(VariantLocusWalker.java:132); at org.broadinstitute.hellbender.eng",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6275#issuecomment-574113941:1811,load,loadNextNovelFeature,1811,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6275#issuecomment-574113941,1,['load'],['loadNextNovelFeature']
Performance,"ese environments, the filesystem is striped across multiple disks; linked via a fast, full bisection bandwidth network such that the remote; filesystem is faster than a local disk. I don't know whether; HDFS-on-premise can do this today though there is no fundamental reason why; it couldn't. A happy side effect is that it makes it possible for me to run it locally -; ReadsSparkSink currently only seems to work when run on a cluster (I am; sure this can be fixed though). That said, I have not had the luxury of time to optimize this whole thing,; so someone will get all the fun of comparing the two approaches,; identifying the bottlenecks, and melding the two together in the best way; possible. My goal is merely to make this algorithm available quickly so we; can see if it can be as helpful here as it has been on the Dataflow side. On Mon, Oct 12, 2015 at 7:37 AM, Tom White notifications@github.com wrote:. > At a high-level, the approach here seems to be to create shards (of size 1; > million bps) and load the reads, variants and references for each shard in; > memory. Each shard then has a recalibration table created for it, then the; > tables are merged into one.; > ; > The existing version finds the variants for each read, and has two; > shuffles (this is for the reference broadcast approach, there's an extra; > one for the reference shuffle approach). The first is to join the variants; > and reads together, and a second to aggregate the variants for each read.; > The optimization in this PR has no shuffles, since it loads all variants; > into memory rather than doing distributed joins. Is that a valid; > assumption? If so, it would be possible to load the variants into memory in; > the driver and broadcast to all workers to remove the shuffles (in the; > existing implementation).; > ; > I noticed that AddContextDataToReadSpark#subdivideAndFillReads opens the; > BAM itself, rather than using ReadsSparkSink. This won't work well in the; > Hadoop case since you lose ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/987#issuecomment-147476006:2015,load,load,2015,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/987#issuecomment-147476006,1,['load'],['load']
Performance,"est; 16:28:04.155 INFO GenomicsDBImport - Vid Map JSON file will be written to forkTest/vidmap.json; 16:28:04.155 INFO GenomicsDBImport - Callset Map JSON file will be written to forkTest/callset.json; 16:28:04.156 INFO GenomicsDBImport - Complete VCF Header will be written to forkTest/vcfheader.vcf; 16:28:04.156 INFO GenomicsDBImport - Importing to array - forkTest/genomicsdb_array; 16:28:04.158 INFO ProgressMeter - Starting traversal; 16:28:04.158 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 16:28:05.198 INFO GenomicsDBImport - Starting batch input file preload; 16:29:23.571 INFO GenomicsDBImport - Finished batch preload; 16:48:46.140 INFO GenomicsDBImport - Shutting down engine; [May 4, 2018 4:48:46 PM EDT] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 20.96 minutes.; Runtime.totalMemory()=22281715712; java.util.concurrent.CompletionException: java.lang.OutOfMemoryError: Java heap space; at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273); at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1592); at java.util.concurrent.CompletableFuture$AsyncSupply.exec(CompletableFuture.java:1582); at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056); at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692); at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157); Caused by: java.lang.OutOfMemoryError: Java heap space; at com.intel.genomicsdb.importer.SilentByteBufferStream.<init>(SilentByteBufferStream.java:55); at com.intel.genomicsdb.importer.GenomicsDBImporterStreamWrapper.<init>(GenomicsDBImporterStreamWrapper.java:70); at com.intel.genomicsdb.importer.GenomicsDBImporter.addBufferStream(GenomicsDBImporter.java:397); a",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388079572:3516,concurren,concurrent,3516,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388079572,1,['concurren'],['concurrent']
Performance,ethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); 	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); 	at com.sun.proxy.$Proxy2.stop(Unknown Source); 	at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:132); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:175); 	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:157); 	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:404); 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:63); 	at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:46); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:55); 	at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6086#issuecomment-519578293:4067,concurren,concurrent,4067,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6086#issuecomment-519578293,5,['concurren'],['concurrent']
Performance,etryingFutureImpl.access$500(RetryingFutureImpl.java:59); 	at shaded.cloud_nio.com.google.api.gax.retrying.RetryingFutureImpl$AttemptFutureCallback.onFailure(RetryingFutureImpl.java:177); 	at shaded.cloud_nio.com.google.api.gax.core.ApiFutures$1.onFailure(ApiFutures.java:52); 	at shaded.cloud_nio.com.google.common.util.concurrent.Futures$6.run(Futures.java:1764); 	at shaded.cloud_nio.com.google.common.util.concurrent.MoreExecutors$DirectExecutor.execute(MoreExecutors.java:456); 	at shaded.cloud_nio.com.google.common.util.concurrent.Futures$ImmediateFuture.addListener(Futures.java:153); 	at shaded.cloud_nio.com.google.common.util.concurrent.ForwardingListenableFuture.addListener(ForwardingListenableFuture.java:47); 	at shaded.cloud_nio.com.google.api.gax.core.internal.ApiFutureToListenableFuture.addListener(ApiFutureToListenableFuture.java:53); 	at shaded.cloud_nio.com.google.common.util.concurrent.Futures.addCallback(Futures.java:1776); 	at shaded.cloud_nio.com.google.common.util.concurrent.Futures.addCallback(Futures.java:1713); 	at shaded.cloud_nio.com.google.api.gax.core.ApiFutures.addCallback(ApiFutures.java:47); 	at shaded.cloud_nio.com.google.api.gax.retrying.RetryingFutureImpl.setAttemptFuture(RetryingFutureImpl.java:107); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:100); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:47); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:125); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:109); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:131); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:104); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolEx,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300298180:5038,concurren,concurrent,5038,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300298180,1,['concurren'],['concurrent']
Performance,"events are not mosaic CNLOH, then we should clean up all mention of CNLOH in this code. Either way, can we quantify the level of improvement gained by filtering such events in a reproducible evaluation? If so, let's bring that into gatk-evaluation. Finally, there are many more options available to change the segmentation and/or resolution than the single one you mentioned. If the users you are working with can clearly specify their analysis goals in terms of resolution, then it might be possible to sidestep the problem entirely without adding more unsupported code. This would also buy us more time to put in a principled solution, without the risk of unsupported code getting entrenched in their workflows. > There are definitely events that get missed without the germline tagging, so this is an improvement over blacklisting alone. And while I have seen erroneous germline tagging (i.e. false calling a segment germline), it was only ever due to really noisy data (e.g. a bad PoN) or a poorly tuned segment caller. This is encouraging. This means that a straightforward approach to germline filtering, such as simply identifying overlapping posteriors as mentioned above, should work well. Prototyping this approach shouldn't take long at all, especially when the matched normal is guaranteed to be available, as it is in this workflow (tumor-only would require some work to identify the normal state, as mentioned previously). I'd rather just roll that, evaluate it, and merge it instead. Key here is that we sidestep the deficiencies of the current CR-only caller, which also shares the blame for this ""CNLOH"" issue (since these events aren't called in the normal and don't become candidates for tagging, as currently implemented). > And this would be a possible ""better solution"" Shall I file an issue for this? This could also allow us to obviate the TagGermline tool, which is fine by me. I've already expanded the scope of https://github.com/broadinstitute/gatk/issues/4115 to include t",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5450#issuecomment-461431199:3501,tune,tuned,3501,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5450#issuecomment-461431199,1,['tune'],['tuned']
Performance,executeAttempt(RetryingFutureImpl.java:141); 	at shaded.cloud_nio.com.google.api.gax.retrying.RetryingFutureImpl.access$500(RetryingFutureImpl.java:59); 	at shaded.cloud_nio.com.google.api.gax.retrying.RetryingFutureImpl$AttemptFutureCallback.onFailure(RetryingFutureImpl.java:177); 	at shaded.cloud_nio.com.google.api.gax.core.ApiFutures$1.onFailure(ApiFutures.java:52); 	at shaded.cloud_nio.com.google.common.util.concurrent.Futures$6.run(Futures.java:1764); 	at shaded.cloud_nio.com.google.common.util.concurrent.MoreExecutors$DirectExecutor.execute(MoreExecutors.java:456); 	at shaded.cloud_nio.com.google.common.util.concurrent.Futures$ImmediateFuture.addListener(Futures.java:153); 	at shaded.cloud_nio.com.google.common.util.concurrent.ForwardingListenableFuture.addListener(ForwardingListenableFuture.java:47); 	at shaded.cloud_nio.com.google.api.gax.core.internal.ApiFutureToListenableFuture.addListener(ApiFutureToListenableFuture.java:53); 	at shaded.cloud_nio.com.google.common.util.concurrent.Futures.addCallback(Futures.java:1776); 	at shaded.cloud_nio.com.google.common.util.concurrent.Futures.addCallback(Futures.java:1713); 	at shaded.cloud_nio.com.google.api.gax.core.ApiFutures.addCallback(ApiFutures.java:47); 	at shaded.cloud_nio.com.google.api.gax.retrying.RetryingFutureImpl.setAttemptFuture(RetryingFutureImpl.java:107); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:100); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:47); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:125); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:109); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:131); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:104); 	at java.util.concurrent.FutureTa,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300298180:4943,concurren,concurrent,4943,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300298180,1,['concurren'],['concurrent']
Performance,"explain what the splitting index is a bit better and then it will make more sense I think. Spark works by splitting files up into similar sized chunks and passing those chunks to different worker machines. ; Bam files are hard to split nicely into chunks. The way they're structured makes it hard to identify where safe boundaries are to split on. If you don't have a splitting index, we have an algorithm to start reading at essentially random locations and look for safe splitting points, but we've had some issues in the past where you can misidentify a split (which results in a crash) or miss good splits. The splitting index is a precomputed list of split points, which works around the problem of having to find the splits again next time. It's only used by spark tools that load bams, so it won't benefit Mutect2 because that's not built on spark. . We should add some documentation about this somewhere... #4235",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4219#issuecomment-359826965:782,load,load,782,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4219#issuecomment-359826965,1,['load'],['load']
Performance,"f variants from a single scaffold to the output file but then exits with `java.lang.ArrayIndexOutOfBoundsException` (see below). I have also tried adding the `-L` flag and an interval list, which performs similarly but outputs variants from a different scaffold. Any idea why this is happening or what I can do to overcome this problem? I have run `GenomicsDBImport` and `GenotypeGVCFs` successfully in the past (same version, same computer) on a different dataset, so I'm not sure what about this data is causing the problem. Any guidance is much appreciated!. Thanks,; Jessie. ```; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/jsalt/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar GenotypeGVCFs -R /nfs/data1/jsalt/3RAD/colinus_virginianus_13May2017_V3Fw6_newchrom.fasta -V gendb://odont_cyr_8_snp_db -O odont_cyr_8_snp_db.vcf; 14:59:47.866 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/jsalt/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 03, 2020 2:59:59 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 14:59:59.674 INFO GenotypeGVCFs - ------------------------------------------------------------; 14:59:59.675 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.1.2.0; 14:59:59.675 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 15:00:09.686 INFO GenotypeGVCFs - Executing as jsalt@mustard on Linux v3.10.0-957.1.3.el7.x86_64 amd64; 15:00:09.686 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_192-b01; 15:00:09.687 INFO GenotypeGVCFs - Start Date/Time: February 3, 2020 2:59:47 PM CST; 15:00:09.687 INFO GenotypeGVCFs - ------------------------------------------------------------; 15:00:",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6357#issuecomment-581619640:1137,Load,Loading,1137,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6357#issuecomment-581619640,1,['Load'],['Loading']
Performance,"factor of around four, which gzip often does not reach (because it doesn't know ahead of time that DNA has only four letters).; Use reference genome fasta as proxy for nearly no repetition at all. It doesn't compress much beyond 2bit. Tweaking of the Huffmann coding etc. might have influenced the compression level much in this case, by ""giving the compressor a subtle hint about the four letters"".; Paradoxically, Intel might have optimized for average data and thus brought a disadvantage for the four letter nature of DNA (and also the few letters used in quality data encoding compared to text). 3. BQSR:; When I did interleaving compression experiments, I noticed that the BQSR step decreases compressiblity considerably.; In this example I had the same BAM file in different versions that were aligned to hs38DH, hs38, hs37d5 and could compress them to nearly the size of one, by putting similar pieces of the files after one another.; Adding the same BAM with BQSR increased final file size more than several pre-BQSR versions together.; Note: This piece-meal packing might be useful for different BAMs mostly only with many BAMs where similar regions accumulate. 4. Even faster:; In my experience, level 0 (no compression) (with samtools view -u) increases speed even more, if files are on a lz4 encrypted disk (such as with ZFS).; The speed-up of lz4 over even level 1 of any gzip-like compression is substantial.; With data on SSDs or similarly fast storage, that can make a huge difference. Another factor 6 six faster than level 1 on compression and a factor 9 on decompression. The then possible decompression speed of 3GB/s makes it possible to e.g. load a 180GB bam into a RAM disk in 60 seconds on a sufficiently fast SSD array (e.g. as on an aws ec2 i3.8xlarge instance).; Still 600MB/s if the RAM is also lz4 compressed. See image from https://github.com/lz4/lz4 below.; ![grafik](https://user-images.githubusercontent.com/1612006/35339046-d84b8b78-011f-11e8-99ec-a36cde725bb3.png)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3413#issuecomment-360179673:3932,load,load,3932,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3413#issuecomment-360179673,1,['load'],['load']
Performance,fc221f39ae35a0604d3b3eca?src=pr&el=desc) will **decrease** coverage by `0.57%`.; > The diff coverage is `0%`. ```diff; @@ Coverage Diff @@; ## master #3903 +/- ##; ==============================================; - Coverage 79.487% 78.917% -0.57% ; + Complexity 18094 16760 -1334 ; ==============================================; Files 1187 1103 -84 ; Lines 65403 59950 -5453 ; Branches 9932 9464 -468 ; ==============================================; - Hits 51987 47311 -4676 ; + Misses 9429 8972 -457 ; + Partials 3987 3667 -320; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/3903?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...der/utils/linalg/FourierLinearOperatorNDArray.java](https://codecov.io/gh/broadinstitute/gatk/pull/3903/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9saW5hbGcvRm91cmllckxpbmVhck9wZXJhdG9yTkRBcnJheS5qYXZh) | `47.619% <ø> (ø)` | `7 <0> (?)` | |; | [...te/hellbender/tools/exome/PerformSegmentation.java](https://codecov.io/gh/broadinstitute/gatk/pull/3903/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9QZXJmb3JtU2VnbWVudGF0aW9uLmphdmE=) | `100% <ø> (ø)` | `3 <0> (ø)` | :arrow_down: |; | [...itute/hellbender/utils/GATKProtectedMathUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/3903/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9HQVRLUHJvdGVjdGVkTWF0aFV0aWxzLmphdmE=) | `69.531% <ø> (-13.802%)` | `51 <0> (-8)` | |; | [...stitute/hellbender/utils/icg/DuplicableNumber.java](https://codecov.io/gh/broadinstitute/gatk/pull/3903/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9pY2cvRHVwbGljYWJsZU51bWJlci5qYXZh) | `80% <ø> (ø)` | `5 <0> (?)` | |; | [...bender/tools/exome/NormalizeSomaticReadCounts.java](https://codecov.io/gh/broadinstitute/gatk/pull/3903/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3903#issuecomment-348528911:1254,Perform,PerformSegmentation,1254,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3903#issuecomment-348528911,1,['Perform'],['PerformSegmentation']
Performance,"file path: file:///home/deepak/software_library/gatk-4.1.7.0/Cosmic.db -> file:///media/deepak/EXTRA/FUNCOTATOR_DATA/DATA_SOURCES/data_source_3/hg38/Cosmic.db; .; .; .; .; .; .; .; . 16:01:43.969 INFO DataSourceUtils - Resolved data source file path: file:///home/deepak/software_library/gatk-4.1.7.0/dnaRepairGenes.20180524T145835.csv -> file:///media/deepak/EXTRA/FUNCOTATOR_DATA/DATA_SOURCES/data_source_8/hg38/dnaRepairGenes.20180524T145835.csv; 16:01:43.979 INFO Funcotator - Initializing Funcotator Engine...; 16:01:43.983 INFO Funcotator - Creating a VCF file for output: file:/home/deepak/software_library/gatk-4.1.7.0/variants.funcotated.vcf; 16:01:44.020 INFO ProgressMeter - Starting traversal; 16:01:44.020 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 16:01:44.068 WARN GencodeFuncotationFactory - Cannot create complete funcotation for variant at chr1:1-10454 due to alternate allele: <NON_REF>; 16:01:44.116 INFO VcfFuncotationFactory - dbSNP 9606_b150 cache hits/total: 0/0; 16:01:44.121 INFO Funcotator - Shutting down engine; [12 May, 2020 4:01:44 PM IST] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 0.14 minutes.; Runtime.totalMemory()=2889875456; java.lang.IllegalArgumentException: Invalid interval. Contig:chr1 start:-9 end:10464; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:733); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:59); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:35); at org.broadinstitute.hellbender.tools.funcotator.FuncotatorUtils.createReferenceSnippet(FuncotatorUtils.java:1439); at org.broadinstitute.hellbender.tools.funcotator.FuncotatorUtils.getBasesInWindowAroundReferenceAllele(FuncotatorUtils.java:1468); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createFuncotationForSymbolicAltAllele(GencodeFuncotationFactory.java:25",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6598#issuecomment-664565036:6037,cache,cache,6037,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6598#issuecomment-664565036,1,['cache'],['cache']
Performance,"flow. We can add a single BAM case workflow or expand the matched-pair workflow to handle this, depending on the discussion at https://github.com/broadinstitute/gatk/issues/3657.; - WES/WGS is toggled by providing an optional target-file input.; - For all workflows, we always collect integer read counts; for WGS, these are output as both HDF5 and TSV and the HDF5 is used for subsequent input.; - For the case workflow, we always collect allelic counts at all sites and output as TSV.; - [x] We should output all data files as HDF5 by default and as TSV optionally. EDIT: This is done for `CollectFragmentCounts`.; - [x] We will need to update the workflows when @MartonKN and @asmirnov239 get `PreprocessIntervals` and `CollectReadCounts` merged, respectively. These tools will remove the awkwardness required by `PadTargets` and `CalculateTargetCoverage`/`SparkGenomeReadCounts`. Denoising:; - All parameters are exposed in the PoN creation tool (#3356).; - Without a PoN, standardization and optional GC correction are performed (#3570).; - Other than the minor point about sample mean/median being used inconsistently noted above, the denoising process is essentially exactly the same mathematically as before (""superficial"" differences include the vastly improved memory and I/O optimizations, the ability to adjust number of principal components used, the removal of redundant SVDs, the enforcement of consistent GC-bias correction).; - [ ] That said, I'll carry over this TODO from above: Revisit standardization procedure by checking with simulated data. We should make sure that the centering of the data does not rescale the true copy ratio.; - The only major difference is we no longer make a QC PoN or check for large events. This was performed awkwardly in the old pipeline, so I'd rather not port it over. Eventually we will do all denoising with the gCNV coverage model anyway.; - Pre/tangent-normalization copy ratio are now referred to as standardized/denoised copy ratio.; - [x] O",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:1544,perform,performed,1544,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828,1,['perform'],['performed']
Performance,focusing on performance right now. If someone else wants to investigate they're welcome to.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/679#issuecomment-126076581:12,perform,performance,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/679#issuecomment-126076581,1,['perform'],['performance']
Performance,"fter which the usual modelling and smoothing steps are performed. For the 75% tumor + 25% normal mixture, this yields 122 segments (up from 83):; ![N-25-T-75-SJS modeled](https://user-images.githubusercontent.com/11076296/76558618-015bd180-6474-11ea-996a-48d39770149b.png). For the 25% tumor + 75% normal mixture, this yields 105 segments (up from 50):; ![N-75-T-25-SJS modeled](https://user-images.githubusercontent.com/11076296/76560726-34a05f80-6478-11ea-9027-a54726c46b9e.png). One could imagine that smoothing could be disabled (so that all samples retain the common segmentation after modeling) or made more aggressive (so that private events don't get inadvertently introduced into other samples due to noise, perhaps), depending on the use case. It looks like the joint segmentation allows some additional events to be resolved, although I haven't done any rigorous evaluations. We could probably cook up some evaluations using simulated toy data or in silico mixtures, but there's really no reason why this shouldn't work decently well, especially if the kernel-segmentation method works well on a single sample for your data. It would also be interesting to understand at which point changing segmentation parameters on a single sample can no longer yield the same performance as joint segmentation on a fixed number of samples; however, this is probably a function of various S/N ratios, and it might not be easy to characterize this behavior outside of toy data. The segmentation parameter space is big enough to make this unwieldy even for toy data, too. Perhaps we can get some feedback from test users---not only on performance, but also on the structure of the new workflow. It might also be worth gauging whether a new WDL is warranted. Otherwise, we just need to add some unit tests for correctness of the multisample-segmentation backend class, integration tests for plumbing of the new tool, and perhaps address some of the issues mentioned above. Then I'd say this is good to go.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6499#issuecomment-598386823:2407,perform,performance,2407,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6499#issuecomment-598386823,2,['perform'],['performance']
Performance,"g.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.varia.NullAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.varia.NullAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""NullAppender"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.varia.NullAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.varia.NullAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""NullAppender"".; log4j:ERROR A ""org.apache.log4j.varia.NullAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.varia.NullAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""NullAppender"".; ^C; ####################### Ctrl-C after 16 hours ##############; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312229998:6363,load,loaded,6363,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312229998,4,['load'],['loaded']
Performance,"germline-resource /home/proj/stage/cancer/reference/GRCh37/variants/dbsnp_grch37_b138.vcf.gz -O mutect2/concatenated_ACC5611A5_XXXXXX_mutect2_unfiltered_ss_r2.vcf.gz; Using GATK jar /home/proj/bin/conda/envs/D_UMI_APJ/share/gatk4-4.1.8.0-0/gatk-package-4.1.8.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/proj/bin/conda/envs/D_UMI_APJ/share/gatk4-4.1.8.0-0/gatk-package-4.1.8.0-local.jar Mutect2 -R /home/proj/stage/cancer/reference/GRCh37/genome/human_g1k_v37_decoy.fasta -L /home/proj/stage/cancer/reference/target_capture_bed/production/balsamic/gicfdna_3.1_hg19_design.bed -I consensus/concatenated_ACC5611A5_XXXXXX_consensusalign_ss_r2.bam --germline-resource /home/proj/stage/cancer/reference/GRCh37/variants/dbsnp_grch37_b138.vcf.gz -O mutect2/concatenated_ACC5611A5_XXXXXX_mutect2_unfiltered_ss_r2.vcf.gz; 11:47:50.850 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/proj/bin/conda/envs/D_UMI_APJ/share/gatk4-4.1.8.0-0/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jul 02, 2020 11:47:51 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 11:47:51.054 INFO Mutect2 - ------------------------------------------------------------; 11:47:51.055 INFO Mutect2 - The Genome Analysis Toolkit (GATK) v4.1.8.0; 11:47:51.055 INFO Mutect2 - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:47:51.055 INFO Mutect2 - Executing as ashwini.jeggari@compute-0-0.local on Linux v3.10.0-1062.4.1.el7.x86_64 amd64; 11:47:51.055 INFO Mutect2 - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_152-release-1056-b12; 11:47:51.055 INFO Mutect2 - Start Date/Time: July 2, 2020 11:47:50 AM CEST; 11:47:51.056 INFO Mutect2 - --------------------------------------------",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-652912482:1345,Load,Loading,1345,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-652912482,1,['Load'],['Loading']
Performance,"ges.githubusercontent.com/11076296/29582016-23fbc6a6-8749-11e7-951e-f618e8489a0b.png). ![4](https://user-images.githubusercontent.com/11076296/29582044-3eb20a1e-8749-11e7-84a0-3734bad15e1f.png). ![5](https://user-images.githubusercontent.com/11076296/29582047-410ac490-8749-11e7-8a98-b2098cf1b5ea.png); 4) For each of these cost functions, find (up to) the _C<sub>max</sub>_ most significant local minima. The problem of finding local minima of a noisy function can be solved by using topological persistence (e.g., https://people.mpi-inf.mpg.de/~weinkauf/notes/persistence1d.html and http://www2.iap.fr/users/sousbie/web/html/indexd3dd.html?post/Persistence-and-simplification). A straightforward watershed algorithm can sort all local minima by persistence in linear time after an initial sort of the data.; 5) These sets of local minima from all window sizes together provide the pool of candidate changepoints (some of which may overlap exactly or approximately). We perform backwards selection using the global segmentation cost. That is, we compute the global segmentation cost given all the candidate changepoints, calculate the cost change for removing each of the changepoints individually, remove the changepoint with the minimum cost change, and repeat. This gives the global cost as a function of the number of changepoints _C_.; 6) Add a penalty _a C + b C log(N / C)_ to the global cost and find the minimum to determine the number of changepoints. For the above simulated data, _a = 2_ and _b = 2_ works well, recovering all of the changepoints in the above example with no false positives:; ![6](https://user-images.githubusercontent.com/11076296/29582517-fbd604be-874a-11e7-8ef7-7bd727f65dcb.png). ![7](https://user-images.githubusercontent.com/11076296/29582518-fddf015c-874a-11e7-89e4-87250d2a52ab.png). In contrast, CBS produces two false positives (around the third and seventh of the true changepoints):. ![8](https://user-images.githubusercontent.com/11076296/29582545-18875126-",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-324125586:2312,perform,perform,2312,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-324125586,1,['perform'],['perform']
Performance,"gion walkers it reflects the undownsampled depth subject to things like realignment to the haplotypes (easiest option, but doesn't fix the underlying craziness); 2. Change the ActiveRegion traversal so that it respects the dcov value (could be hard -- the LIBS downsampling process discards reads on-the-fly from previous loci when moving to a new locus, but an active region involves data for multiple loci. The potential performance win for the HC is huge, though, if we could pull this off). [...]. Alright, next step then is to figure out whether it's even feasible to make the ActiveRegion traversal fully respect dcov. I think Mark, in implementing the current scheme, might have been thinking that maintaining the undownsampled reads in memory is actually less expensive in typical (non-extreme) cases than reconstructing the full set of post-downsampling reads in an active region from multiple AlignmentContexts emitted by LIBS without any duplicates. I'll have to do some performance testing to see whether or not this is the case. Will try to get to this within the next few weeks, but the QC project has immediate priority. [...]. Discussed this with Ryan -- we agreed that the right thing to do is to move the enforcement of the hard cap on the total number of reads that can be in an active region from the HC walker to the engine, and have the size of the cap be controlled by a new argument (not dcov). That way you never pay the cost of storing the undownsampled reads for an active region in memory. We'd also have to educate users on exactly what the various downsampling arguments do for active region walkers. [...]. Making the hardcoded per-active-region cap settable from the command line is the easy part -- what seems hard is:; - Determining whether we can avoid storing all undownsampled reads in memory at once without affecting the quality of calls. Currently, as outlined in earlier comments on this ticket, we do a downsampling pass per locus which respects dcov (in Locu",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345:4890,perform,performance,4890,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345,1,['perform'],['performance']
Performance,"gion.java:139); at org.apache.spark.network.protocol.MessageWithHeader.transferTo(MessageWithHeader.java:121); at io.netty.channel.socket.nio.NioSocketChannel.doWriteFileRegion(NioSocketChannel.java:287); at io.netty.channel.nio.AbstractNioByteChannel.doWrite(AbstractNioByteChannel.java:237); at io.netty.channel.socket.nio.NioSocketChannel.doWrite(NioSocketChannel.java:314); at io.netty.channel.AbstractChannel$AbstractUnsafe.flush0(AbstractChannel.java:802); at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.forceFlush(AbstractNioChannel.java:319); at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:637); at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566); at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442); at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144); at java.lang.Thread.run(Thread.java:748); 18/04/24 17:42:11 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 18/04/24 17:42:11 INFO MemoryStore: MemoryStore cleared; 18/04/24 17:42:11 INFO BlockManager: BlockManager stopped; 18/04/24 17:42:11 INFO BlockManagerMaster: BlockManagerMaster stopped; 18/04/24 17:42:11 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/04/24 17:42:11 INFO SparkContext: Successfully stopped SparkContext; 17:42:11.053 INFO PathSeqPipelineSpark - Shutting down engine; [April 24, 2018 5:42:11 PM CEST] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 2.87 minutes.; Runtime.totalMemory()=866648064; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 8, xx.xx.xx",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:38177,concurren,concurrent,38177,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,1,['concurren'],['concurrent']
Performance,"given that we only have easy access to scores for positive truth---and hence, no false positives, which precludes calculation of precision and F1. I *think* we could pass a VCF for a sample with gold-standard positives and negatives and use the existing code for extracting labels, but this will require a bit of engineering and be more trouble than it's worth. There are other options---see https://ir.cwi.nl/pub/30479, for example. We might want to experiment with the LL score discussed there (see https://www.aaai.org/Papers/ICML/2003/ICML03-060.pdf for the original paper---although note that despite the paper's high citation count, I'm not sure what the canonical name for this metric actually is, but it doesn't appear to be ""LL score""---perhaps someone else knows or has better Google-fu and can figure it out) before moving on to their methods for estimating F1. Doing a literature search for other discussions of optimizing F1 or other metrics in the context of positive-unlabeled learning might be worthwhile, but I think most methods will probably involve some sort of estimation of the base rate in unlabeled data. I think we may have to add some mechanism for holding out a validation set during training if we want to automatically tune thresholds in a rigorous fashion. Shouldn't be too bad---we can just have the training tool randomly mask out a set of the truth and pass the mask to the scoring tool (or maybe just determine the threshold in the training tool, if we are running in positive/negative mode and have access to unlabeled data)---but does add a couple of parameters to the tool interfaces. This also adds additional dependence on the quality of the truth resources. I think an implicit assumption in any use of the truth---even just thresholding/calibrating by sensitivity---is that it is a random sample; however, I'm not sure how true this is in actual use. For example, in malaria, it looks like we may have to resort to using a callset that has been very conservat",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1062931241:992,optimiz,optimizing,992,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1062931241,1,['optimiz'],['optimizing']
Performance,"gradle builds the dependency cache archive while online, and then builds a package using this archive while offline.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6395#issuecomment-584455056:29,cache,cache,29,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6395#issuecomment-584455056,1,['cache'],['cache']
Performance,graph.DefaultTaskExecutionGraph$BuildOperationAwareExecutionAction.execute(DefaultTaskExecutionGraph.java:336); at org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$BuildOperationAwareExecutionAction.execute(DefaultTaskExecutionGraph.java:322); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker$1.execute(DefaultPlanExecutor.java:134); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker$1.execute(DefaultPlanExecutor.java:129); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker.execute(DefaultPlanExecutor.java:202); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker.executeNextNode(DefaultPlanExecutor.java:193); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker.run(DefaultPlanExecutor.java:129); at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64); at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48); at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56); Caused by: org.gradle.api.GradleException: Javadoc generation failed. Generated Javadoc options file (useful for troubleshooting): '/home/cb2/gatk/build/tmp/gatkDoc/javadoc.options'; at org.gradle.api.tasks.javadoc.internal.JavadocGenerator.execute(JavadocGenerator.java:58); at org.gradle.api.tasks.javadoc.internal.JavadocGenerator.execute(JavadocGenerator.java:31); at org.gradle.api.tasks.javadoc.Javadoc.executeExternalJavadoc(Javadoc.java:158); at org.gradle.api.tasks.javadoc.Javadoc.generate(Javadoc.java:146); at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at org.gradle.internal.reflect.JavaMethod.invoke(JavaMethod.java:103); at org.gradle.api.internal.project.taskf,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4155#issuecomment-566796716:5945,concurren,concurrent,5945,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4155#issuecomment-566796716,1,['concurren'],['concurrent']
Performance,graph.DefaultTaskExecutionGraph$BuildOperationAwareExecutionAction.execute(DefaultTaskExecutionGraph.java:336); at org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$BuildOperationAwareExecutionAction.execute(DefaultTaskExecutionGraph.java:322); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker$1.execute(DefaultPlanExecutor.java:134); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker$1.execute(DefaultPlanExecutor.java:129); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker.execute(DefaultPlanExecutor.java:202); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker.executeNextNode(DefaultPlanExecutor.java:193); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker.run(DefaultPlanExecutor.java:129); at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64); at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48); at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56); Caused by: org.gradle.api.GradleException: Javadoc generation failed. Generated Javadoc options file (useful for troubleshooting): '/usr/bin/gatk/build/tmp/gatkDoc/javadoc.options'; at org.gradle.api.tasks.javadoc.internal.JavadocGenerator.execute(JavadocGenerator.java:58); at org.gradle.api.tasks.javadoc.internal.JavadocGenerator.execute(JavadocGenerator.java:31); at org.gradle.api.tasks.javadoc.Javadoc.executeExternalJavadoc(Javadoc.java:158); at org.gradle.api.tasks.javadoc.Javadoc.generate(Javadoc.java:146); at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at org.gradle.internal.reflect.JavaMethod.invoke(JavaMethod.java:103); at org.gradle.api.internal.project.taskfa,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6466#issuecomment-590387973:4819,concurren,concurrent,4819,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6466#issuecomment-590387973,1,['concurren'],['concurrent']
Performance,"gy BROADCAST \; --apiKey $API_KEY \; --sparkMaster yarn-client; ```. I recommend running BROADCAST with 4 cores per executor and 10 GB/memory per executor (though this may need to be increased if you see any swapping). 16 executors (ie., 64 cores total) seems like a good size for our cluster as it would allow you to increase memory per core if necessary. All inputs should be in hdfs, as in the example script above. Run the SHARDED implementation on `dataflow01` using the unix `time` command and a script like the one below:. ```; spark-submit \; --master yarn-client \; --driver-memory 8G \; --num-executors 16 \; --executor-cores 4 \; --executor-memory 10G \; --conf spark.driver.maxResultSize=0 \; --conf spark.driver.userClassPathFirst=true \; --conf spark.executor.userClassPathFirst=true \; --conf spark.io.compression.codec=lzf \; --conf spark.yarn.executor.memoryOverhead=600 \; --conf spark.akka.frameSize=1024 \; $JAR BaseRecalibratorSparkSharded \; --input gs://your.bam \; --output bqsr_out_sharded.bam \; -R hdfs:///user/droazen/bqsr/human_g1k_v37.fasta \; --knownSites /path/to/your.vcf \; -L EXACT_INTERVALS_FOR_YOUR_BAM \; --apiKey $API_KEY \; --sparkMaster yarn-client; ```. Notes on running the SHARDED implementation:; -The bam must be in a GCS bucket; -The vcf must be in a local file (not hdfs!); -The reference should be either an indexed fasta (not `.2bit`, ) in hdfs, or you can use the Google API for the reference (eg., `-R gg://reference/EOSsjdnTicvzwAE` for b37) -- not sure which option is better for this implementation, may want to test both; -You MUST specify the exact intervals spanning the reads in the bam using `-L`, otherwise you will get very poor performance.; -You MUST specify `--conf spark.akka.frameSize=1024` otherwise you will get a crash.; -It's unclear what the ideal executor size for this implementation is, but there is some evidence that it does slightly better with more cores per executor (compared to BROADCAST), so you may want to try that.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/995#issuecomment-160722098:3327,perform,performance,3327,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/995#issuecomment-160722098,1,['perform'],['performance']
Performance,"hat's happening. We wouldn't expect gatk4 haplotype caller to be that much slower. . It looks like they're running beta2 which is kind of old as well. Can you ask them what exact version they're using?. Can you ask if they have the log (stdout + stderr) for the gatk4 non-spark run? I can't tell what pairhmm they're actually running with and the logs would help with that. . Can you also find out what sort of hardware they're running on? Specifically, is it an intel machine with support for AVX?. A good setting for` --nativePairHmmThreads` is probably 4-8, you won't see any improvement after that. I also noticed that they're setting -XX:+UseParallelGC -XX:ParallelGCThreads=32 for the gatk3. They would be better off setting it to 2-4 threads. Performance gets worse beyond that typically from what I've seen. They can set the same thing for gatk4 using`--javaOptions ' -XX:+UseParallelGC -XX:ParallelGCThreads=4'`. Their spark configuration looks wrong in a number of ways which is probably a big part of why they're not seeing any improvement. In general you want executors with ~4-8 cores and at least 4g of memory per core. I don't know how much memory their nodes have, and I don't know if they're running with autoscaling turned on, but I suspect they're only allocating 1 executor on 1 node and then it's thrashing memory because it's trying to run 32 threads at once. Spark tuning for haplotype caller is going to be complicated though and I don't know how to do it will yet, we will be revisiting it in the next quarter probably. They're also running withs spark 2.1.0, we currently require spark 2.0.2 which is an unfortunately specific version, we're planning on upgrading to spark 2.2.+ in the next quarter. . You should make it clear to them that the results will not be the same between 3, 4, and 4-spark yet and that 4 is in rapid state of flux and has known performance issues that we're planning on working soon. Even so though, that slowdown they're seeing is bizarrely large.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3631#issuecomment-332879964:1917,perform,performance,1917,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3631#issuecomment-332879964,1,['perform'],['performance']
Performance,"have a different message now, which is also confusing:. ```; ./gatk-launch PrintReadsSpark -I hdfs://local/print_reads.sorted.bam -O output.bam -- --sparkRunner SPARK --sparkMaster yarn-client; ```. ```; java.lang.IllegalArgumentException: java.net.UnknownHostException: local; at org.apache.hadoop.security.SecurityUtil.buildTokenService(SecurityUtil.java:374); at org.apache.hadoop.hdfs.NameNodeProxies.createNonHAProxy(NameNodeProxies.java:310); at org.apache.hadoop.hdfs.NameNodeProxies.createProxy(NameNodeProxies.java:176); at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:707); at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:650); at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:148); at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2643); at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:93); at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2680); at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2662); at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:379); at org.apache.hadoop.fs.Path.getFileSystem(Path.java:296); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:183); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReads(GATKSparkTool.java:337); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:317); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:308); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:146); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); a",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1257#issuecomment-175789890:1032,Cache,Cache,1032,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1257#issuecomment-175789890,1,['Cache'],['Cache']
Performance,"he RDD:. ``` Java; JavaRDD<Variant> rddParallelVariants =; variantsSparkSource.getParallelVariants(output);. System.out.println( rddParallelVariants.first().toString() );; ```. And after re-compiling GATK and running `PrintVCFSpark`, I got the following to print the first element of the RDD:. ``` Bash; $ ./gatk-launch PrintVCFSpark --input test.vcf --output test.vcf. Running:; /home/pgrosu/me/hellbender_broad_institute/gatk/build/install/gatk/bin/gatk PrintVCFSpark --input test.vcf --output test.vcf; [February 14, 2016 7:04:16 PM EST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintVCFSpark --output test.vcf --input test.vcf --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false; [February 14, 2016 7:04:16 PM EST] Executing as pgrosu on Linux 2.6.32-358.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_05-b13; Version: Version:4.alpha-86-g154d0a8-SNAPSHOT JdkDeflater; 19:04:16.098 INFO PrintVCFSpark - Initializing engine; 19:04:16.100 INFO PrintVCFSpark - Done initializing engine; 2016-02-14 19:04:17 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 2016-02-14 19:04:19 WARN MetricsSystem:71 - Using default name DAGScheduler for source because spark.app.id is not set.; MinimalVariant -- interval(1:737406-737411), snp(false), indel(true); 19:04:24.266 INFO PrintVCFSpark - Shutting down engine; [February 14, 2016 7:04:24 PM EST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintVCFSpark done. Elapsed time: 0.14 minutes.; Runtime.totalMemory()=90177536; ```. This seems to have the ability to load a VCF as a JavaRDD. Let me know if this is what you were looking for, and sorry if I am assuming this solves the problem. Thanks and hope you're keeping warm :); Paul",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857:2182,load,load,2182,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857,2,['load'],['load']
Performance,"he per-contig coverage histograms (unfiltered bins in blue, bins retained after filtering in red, and negative-binomial fit in green) and a heatmap of per-contig ploidy probabilities. Both the panel (first 20) and case (remaining) samples are shown:. ![prototype-result](https://user-images.githubusercontent.com/11076296/37938642-e9fbd804-312c-11e8-8a6c-02ea4e4fa704.png). Although the prototype model is clearly a good fit to the filtered data, some care in choosing the optimizer and its learning parameters is required to achieve convergence to the correct solution. This is because the problem is inherently multimodal and thus there are many local minima. I found that using AdaMax with a naive strategy of warm restarts (to help kick us out of local minima) worked decently; we can achieve convergence in <10 minutes for 60 samples x 24 contigs x 250 count bins:. ![elbo](https://user-images.githubusercontent.com/11076296/37938658-fc176f12-312c-11e8-89e2-40c68e0f9953.png). I expect that @mbabadi's annealing implementation in the gcnvkernel package will handle the local minima much better. The course of action needed to implement this model should be as follows:. 1) Alter Java code to emit per-contig histograms. Change python code to consume histograms, perform filtering, and fit using the above model (or some variation).; 2) Choose learning parameters appropriate with annealing and check that results are still good.; 3) Update gCNV model to consume the depth emitted by this model properly, if necessary, and rerun evaluations. Other improvements enabled by mappability filtering (as discussed in #4558) or coverage collection can follow this initial model revision. In the meantime, we will continue the first round of evaluations using the old ploidy model, spot checking genotype calls as necessary. This will allow us to tune gCNV parameters (which will hopefully be largely unaffected by any changes to the ploidy model). How does this sound, @ldgauthier @mbabadi @asmirnov239?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-376278271:3289,perform,perform,3289,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-376278271,2,"['perform', 'tune']","['perform', 'tune']"
Performance,"he; > machine is a ""PowerLinux"" machine and I'm guessing that the most relevant; > info for the following problem is that it is a ppc64le system. When I use; > HaplotypeCaller, I see the following messages on the screen:; >; > Running:; > java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx50G -jar /home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar HaplotypeCaller -R ref.fa -I mybam.bam -O mycalls.vcf.gz -L snps.vcf -ip 100; >; > 16:17:04.377 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; >; > 16:17:04.397 **WARN** NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (/tmp/libgkl_compression3825249225068031371.so: /tmp/libgkl_compression3825249225068031371.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > 16:17:04.402 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; >; > 16:17:04.407 **WARN** NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (/tmp/libgkl_compression7506152962158874866.so: /tmp/libgkl_compression7506152962158874866.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > Sep 04, 2020 4:17:05 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; >; > INFO: Failed to detect whether we are running on Google Compute Engine.; >; > 16:17:05.842 INFO HaplotypeCaller - ------------------------------------------------------------; >; > 16:17:05.843 INFO HaplotypeCaller - The Genome Analysis Toolkit",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600:1869,load,load,1869,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600,1,['load'],['load']
Performance,"his would be fixed by #1433, but it only fixed the inverse of this problem. It's possible now to load HDFS files from the local runner using the full namenode path . i.e. ; `hdfs://dataflow01.broadinstitute.org/user/louisb/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam`, . Loading files with the sparkRunner and yarn-client is still failing. . We're getting a new error now though. ```; java.lang.IllegalArgumentException: unknown SAM format, cannot create RecordReader: file:/local/dev/akiezun/bin/gatk/src/test/resources/org/broadinstitute/hellbender/tools/valid.bam; at org.seqdoop.hadoop_bam.AnySAMInputFormat.createRecordReader(AnySAMInputFormat.java:181); at org.apache.spark.rdd.NewHadoopRDD$$anon$1.<init>(NewHadoopRDD.scala:151); at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:124); at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:65); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:88); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1417#issuecomment-174655123:1821,concurren,concurrent,1821,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1417#issuecomment-174655123,2,['concurren'],['concurrent']
Performance,https://gatk-jenkins.broadinstitute.org/view/Performance/ -- running non-spark tests over and over while I build the spark tests. I'll run about 5 times (take about 2 hours each in parallel) so we can get an average. I've constrained each test to 1 CPU at a time.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1609#issuecomment-227822481:45,Perform,Performance,45,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1609#issuecomment-227822481,1,['Perform'],['Performance']
Performance,"i'm assuming this is a temporary hack and we'll remove the static cache altogether later, right?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/915#issuecomment-142625030:66,cache,cache,66,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/915#issuecomment-142625030,1,['cache'],['cache']
Performance,"ibrary/Java/JavaVirtualMachines/jdk1.8.0_45.jdk/Contents/Home/jre/../include/jni.h:45:20: fatal error: jni_md.h: No such file or directory; compilation terminated. In file included from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/LoadTimeInitializer.h:4:0,; from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/LoadTimeInitializer.cc:2:; /Library/Java/JavaVirtualMachines/jdk1.8.0_45.jdk/Contents/Home/jre/../include/jni.h:45:20: fatal error: jni_md.h: No such file or directory; compilation terminated. In file included from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/jni_common.h:4:0,; from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/org_broadinstitute_hellbender_utils_pairhmm_VectorLoglessPairHMM.cc:2:; /Library/Java/JavaVirtualMachines/jdk1.8.0_45.jdk/Contents/Home/jre/../include/jni.h:45:20: fatal error: jni_md.h: No such file or directory; compilation terminated. In file included from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/LoadTimeInitializer.h:4:0,; from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/utils.cc:3:; /Library/Java/JavaVirtualMachines/jdk1.8.0_45.jdk/Contents/Home/jre/../include/jni.h:45:20: fatal error: jni_md.h: No such file or directory; compilation terminated. In file included from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/template.h:86:0,; from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/avx_function_instantiations.cc:3:; /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/common_data_structure.h: In instantiation of 'static NUMBER ContextBase<NUMBER>::approximateLog10SumLog10(NUMBER, NUMBER) [with NUMBER = double]':; /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/pairhmm-template-kernel.cc:120:9: required from 'void initializeVectorsavxd(int, int, NUMBER*, NUMBER*, NUMBER*, Context<NUMBER>, testcase*, __m256d*, __m256d*, __m256d*, __m256d*, __m256d*, __m256d*, __m256d*) [",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1504#issuecomment-187417081:1421,Load,LoadTimeInitializer,1421,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1504#issuecomment-187417081,1,['Load'],['LoadTimeInitializer']
Performance,ich seems like it's probably related. My favorite part about that exception is `This is likely because code is not running on Google Compute Engine.`. ```; java.util.concurrent.CompletionException: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: Failure while waiting for FeatureReader to initialize with exception: org.broadinstitute.hellbender.exceptions.UserException: Failed to create reader from gs://fc-c64a0fcd-fb30-4a7e-bdc6-3c09a9286941/f6aeb0ce-044a-4b36-a5f0-d3fda62252a5/ReblockGVCF/66c24439-cfeb-4b85-bc02-aefe693177b6/call-GenotypeGVCF/NWD197223.vcf.gz; 	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273); 	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280); 	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1592); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: Failure while waiting for FeatureReader to initialize with exception: org.broadinstitute.hellbender.exceptions.UserException: Failed to create reader from gs://fc-c64a0fcd-fb30-4a7e-bdc6-3c09a9286941/f6aeb0ce-044a-4b36-a5f0-d3fda62252a5/ReblockGVCF/66c24439-cfeb-4b85-bc02-aefe693177b6/call-GenotypeGVCF/NWD197223.vcf.gz; 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lambda$getFeatureReadersInParallel$601(GenomicsDBImport.java:605); 	at java.util.LinkedHashMap.forEach(LinkedHashMap.java:684); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getFeatureReadersInParallel(GenomicsDBImport.java:600); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.createSampleToReaderMap(GenomicsDBImport.java:491); 	at com.in,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-411503423:1044,concurren,concurrent,1044,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-411503423,1,['concurren'],['concurrent']
Performance,icsdb.GenomicsDBImport.getReaderFromVCFUri(GenomicsDBImport.java:437); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.loadHeaderFromVCFUri(GenomicsDBImport.java:252); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.initializeHeaderAndSampleMappings(GenomicsDBImport.java:223); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.onStartup(GenomicsDBImport.java:202); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:114); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:220); Caused by: java.lang.RuntimeException: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: Connection has been shutdown: javax.net.ssl.SSLException: java.net.SocketException: Connection reset; 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:309); 	at htsjdk.samtools.seekablestream.SeekablePathStream.read(SeekablePathStream.java:86); 	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286); 	at java.io.BufferedInputStream.read(BufferedInputStream.java:345); 	at htsjdk.samtools.util.BlockCompressedInputStream.readBytes(BlockCompressedInputStream.java:562); 	at htsjdk.samtools.util.BlockCompressedInputStream.readBytes(BlockCompressedInputStream.java:541); 	at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:493); 	at htsjdk.samtools.util.BlockCompressedInputStream.nextBlock(BlockCompressedInputStream.java:451); 	at htsjdk,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-301610931:1870,concurren,concurrent,1870,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-301610931,1,['concurren'],['concurrent']
Performance,"if this is so, i think we will need to rediscover it in a profiler. It will be super obvious. So i dont worry about it. It will be heavily used in the genotyper - we'll run the genotyper and see what the cache miss rate is and what values it wants to compute logs or factorials of",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1395#issuecomment-166988870:204,cache,cache,204,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1395#issuecomment-166988870,2,"['cache', 'miss rate']","['cache', 'miss rate']"
Performance,ign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvY2FjaGUvRHJpdmluZ0ZlYXR1cmVJbnB1dENhY2hlU3RyYXRlZ3kuamF2YQ==) | `88.000% <88.000%> (ø)` | |; | [...ellbender/engine/cache/LocatableCacheUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvY2FjaGUvTG9jYXRhYmxlQ2FjaGVVbml0VGVzdC5qYXZh) | `96.471% <96.471%> (ø)` | |; | [...gumentcollections/ReadInputArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL2FyZ3VtZW50Y29sbGVjdGlvbnMvUmVhZElucHV0QXJndW1lbnRDb2xsZWN0aW9uLmphdmE=) | `87.500% <100.000%> (+20.833%)` | :arrow_up: |; | [...org/broadinstitute/hellbender/engine/GATKTool.java](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvR0FUS1Rvb2wuamF2YQ==) | `87.719% <100.000%> (+19.135%)` | :arrow_up: |; | [...titute/hellbender/engine/cache/LocatableCache.java](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvY2FjaGUvTG9jYXRhYmxlQ2FjaGUuamF2YQ==) | `100.000% <100.000%> (ø)` | |; | ... and [1863 more](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4902#issuecomment-397744741:5125,cache,cache,5125,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4902#issuecomment-397744741,1,['cache'],['cache']
Performance,"iled to create reader from gs://fc-c64a0fcd-fb30-4a7e-bdc6-3c09a9286941/f6aeb0ce-044a-4b36-a5f0-d3fda62252a5/ReblockGVCF/66c24439-cfeb-4b85-bc02-aefe693177b6/call-GenotypeGVCF/NWD197223.vcf.gz; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lambda$getFeatureReadersInParallel$601(GenomicsDBImport.java:602); 	... 8 more; Caused by: org.broadinstitute.hellbender.exceptions.UserException: Failed to create reader from gs://fc-c64a0fcd-fb30-4a7e-bdc6-3c09a9286941/f6aeb0ce-044a-4b36-a5f0-d3fda62252a5/ReblockGVCF/66c24439-cfeb-4b85-bc02-aefe693177b6/call-GenotypeGVCF/NWD197223.vcf.gz; 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getReaderFromPath(GenomicsDBImport.java:640); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lambda$getFeatureReadersInParallel$600(GenomicsDBImport.java:593); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	... 3 more; Caused by: htsjdk.tribble.TribbleException$MalformedFeatureFile: Unable to parse header with error: ComputeEngineCredentials cannot find the metadata server. This is likely because code is not running on Google Compute Engine., for input source: gs://fc-c64a0fcd-fb30-4a7e-bdc6-3c09a9286941/f6aeb0ce-044a-4b36-a5f0-d3fda62252a5/ReblockGVCF/66c24439-cfeb-4b85-bc02-aefe693177b6/call-GenotypeGVCF/NWD197223.vcf.gz; 	at htsjdk.tribble.TabixFeatureReader.readHeader(TabixFeatureReader.java:97); 	at htsjdk.tribble.TabixFeatureReader.<init>(TabixFeatureReader.java:82); 	at htsjdk.tribble.AbstractFeatureReader.getFeatureReader(AbstractFeatureReader.java:109); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getReaderFromPath(GenomicsDBImport.java:638); 	... 5 more; Caused by: com.google.cloud.storage.StorageException: ComputeEngineCredentials cannot find the metadata server. This is likely because code is not running on Goog",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-411503423:3344,concurren,concurrent,3344,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-411503423,1,['concurren'],['concurrent']
Performance,"imation of a bounded quantity. One possibility would be to logit transform to an unbounded support, perform the estimation, then transform back. EDIT: Just removed kernel density estimation for now, partly due to #3599 as well.; - Hmm, actually still a tiny bit of residual bias. This is apparent e.g. in WGS normals. I think focusing on a new allele-fraction model rather than trying to figure out where the old one is failing would be best.; - [x] For small bins (250bp), the copy-ratio model is currently a bit memory intensive, since it stores an outlier indicator boolean for every data point (it gets by with -Xmx12g for 100 iterations at 250bp). There is no easy away around storing this at the GibbsSampler level (although we could make some non-trivial changes to that code, as @davidbenjamin suggested long ago at https://github.com/broadinstitute/gatk-protected/issues/195). However, I got rid of these at the CopyRatioModeller level. If we want to go down in memory, we could move to a BitSet, but I'm not sure what the performance hit will be. EDIT: It was trivial to switch over to a BitSet, which seems to let us get away with -Xmx8g instead of -Xmx12g. Calling:; - I've ported over the naive `ReCapSegCaller` wholesale. This can take in the output of `ModelSegments`, so we can take advantage of the improved segmentation as before, but we still don't use the modeled minor-allele fractions when making calls. The method for copy-ratio calling is also extremely naive, with hardcoded bounds for identifying the copy-neutral state.; - [ ] @MartonKN is going to work on an improved caller for his next project. This caller should also make simple calls (not full allelic copy number, but just `0`, `+`, `-`), but should also take advantage of the copy-ratio and minor-allele fraction posteriors estimated by `ModelSegments` to generate quality scores. Plotting:; - Other than the allele-fraction model, the limiting factor was the original plotting code (some plotting runs originally to",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:7677,perform,performance,7677,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828,1,['perform'],['performance']
Performance,"imization of; loading the reference bases only once per shard instead of once per read; (since they overlap), but we should be able to get it back via judicious; caching. Or of course we can keep the sharded code since it works. Indeed, the input/output is optimized for modern datacenters with flat; datacenter storage; https://www.usenix.org/conference/osdi12/technical-sessions/presentation/nightingale.; In these environments, the filesystem is striped across multiple disks; linked via a fast, full bisection bandwidth network such that the remote; filesystem is faster than a local disk. I don't know whether; HDFS-on-premise can do this today though there is no fundamental reason why; it couldn't. A happy side effect is that it makes it possible for me to run it locally -; ReadsSparkSink currently only seems to work when run on a cluster (I am; sure this can be fixed though). That said, I have not had the luxury of time to optimize this whole thing,; so someone will get all the fun of comparing the two approaches,; identifying the bottlenecks, and melding the two together in the best way; possible. My goal is merely to make this algorithm available quickly so we; can see if it can be as helpful here as it has been on the Dataflow side. On Mon, Oct 12, 2015 at 7:37 AM, Tom White notifications@github.com wrote:. > At a high-level, the approach here seems to be to create shards (of size 1; > million bps) and load the reads, variants and references for each shard in; > memory. Each shard then has a recalibration table created for it, then the; > tables are merged into one.; > ; > The existing version finds the variants for each read, and has two; > shuffles (this is for the reference broadcast approach, there's an extra; > one for the reference shuffle approach). The first is to join the variants; > and reads together, and a second to aggregate the variants for each read.; > The optimization in this PR has no shuffles, since it loads all variants; > into memory rather tha",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/987#issuecomment-147476006:1523,optimiz,optimize,1523,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/987#issuecomment-147476006,2,"['bottleneck', 'optimiz']","['bottlenecks', 'optimize']"
Performance,"include/c++/5.3.0/cmath:853:5: note: 'template<class _Tp> typename __gnu_cxx::__enable_if<std::__is_arithmetic<_Tp>::__value, int>::__type std::isinf(_Tp)' declared here, later in the translation unit; isinf(_Tp __f); ^. In file included from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/utils.h:4:0,; from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/LoadTimeInitializer.cc:1:; /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/common_data_structure.h: In instantiation of 'static NUMBER ContextBase<NUMBER>::approximateLog10SumLog10(NUMBER, NUMBER) [with NUMBER = float]':; /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/common_data_structure.h:75:53: required from 'static void ContextBase<NUMBER>::initializeMatchToMatchProb() [with NUMBER = float]'; /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/common_data_structure.h:47:35: required from 'static void ContextBase<NUMBER>::initializeStaticMembers() [with NUMBER = float]'; /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/LoadTimeInitializer.cc:52:19: required from here; /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/common_data_structure.h:94:16: error: 'isinf' was not declared in this scope, and no declarations were found by argument-dependent lookup at the point of instantiation [-fpermissive]; if (isinf(small) == -1 || isinf(big) == -1); ^; In file included from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/headers.h:27:0,; from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/common_data_structure.h:4,; from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/utils.h:4,; from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/LoadTimeInitializer.cc:1:; /usr/local/Cellar/gcc/5.3.0/include/c++/5.3.0/cmath:853:5: note: 'template<class _Tp> typename __gnu_cxx::__enable_if<std::__is_arithmetic<_Tp>::__value, int>::__type std::isinf(_Tp)' declared here, later in",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1504#issuecomment-187727343:10043,Load,LoadTimeInitializer,10043,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1504#issuecomment-187727343,1,['Load'],['LoadTimeInitializer']
Performance,ines 60902 60867 -35 ; Branches 9437 9437 ; ===============================================; + Hits 48705 48780 +75 ; + Misses 8401 8296 -105 ; + Partials 3796 3791 -5; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...coveragemodel/CoverageModelArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL0NvdmVyYWdlTW9kZWxBcmd1bWVudENvbGxlY3Rpb24uamF2YQ==) | `86.592% <ø> (ø)` | `40 <0> (ø)` | :arrow_down: |; | [...gemodel/cachemanager/ComputableGraphStructure.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9Db21wdXRhYmxlR3JhcGhTdHJ1Y3R1cmUuamF2YQ==) | `100% <100%> (+26.994%)` | `63 <62> (+24)` | :arrow_up: |; | [...ragemodel/cachemanager/ComputableNodeFunction.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9Db21wdXRhYmxlTm9kZUZ1bmN0aW9uLmphdmE=) | `100% <100%> (+66.667%)` | `4 <1> (+2)` | :arrow_up: |; | [.../coveragemodel/cachemanager/DuplicableNDArray.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9EdXBsaWNhYmxlTkRBcnJheS5qYXZh) | `81.818% <100%> (+38.068%)` | `6 <2> (+2)` | :arrow_up: |; | [...s/coveragemodel/cachemanager/DuplicableNumber.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9EdXBsaWNhYmxlTnVtYmVyLmphdmE=) | `80% <100%> (+80%)` | `5 <2> (+5)` | :arrow_up: |; | [...coveragemodel/cachemanager/PrimitiveCacheNode.java],MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3035#issuecomment-306412418:1592,cache,cachemanager,1592,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3035#issuecomment-306412418,1,['cache'],['cachemanager']
Performance,"ing task set 0.0 with 1 tasks; 15/07/14 13:14:53 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1435 bytes); 15/07/14 13:14:53 INFO executor.Executor: Running task 0.0 in stage 0.0 (TID 0); 15/07/14 13:14:53 INFO executor.Executor: Fetching http://10.1.4.233:65239/jars/hellbender-all-GATK.4.alpha-413-g741d007-SNAPSHOT-spark.jar with timestamp 1436894092643; 15/07/14 13:14:53 INFO util.Utils: Fetching http://10.1.4.233:65239/jars/hellbender-all-GATK.4.alpha-413-g741d007-SNAPSHOT-spark.jar to /var/folders/xt/vq7wz8955r1401mv8w0f4zf9qbfwzl/T/louisb/spark-ccf49262-23e0-45d9-a273-0096f310c64a/userFiles-0909d070-1b0a-4cf7-8d86-e4144a2dd020/fetchFileTemp6888440563594800088.tmp; 15/07/14 13:14:53 INFO executor.Executor: Adding file:/var/folders/xt/vq7wz8955r1401mv8w0f4zf9qbfwzl/T/louisb/spark-ccf49262-23e0-45d9-a273-0096f310c64a/userFiles-0909d070-1b0a-4cf7-8d86-e4144a2dd020/hellbender-all-GATK.4.alpha-413-g741d007-SNAPSHOT-spark.jar to class loader; 13:14:53.486 [Executor task launch worker-0] INFO org.broadinstitute.hellbender.engine.ReadsDataSource - Preparing intervals for traversal; 13:14:53.486 [Executor task launch worker-0] INFO org.broadinstitute.hellbender.engine.ReadsDataSource - Done preparing intervals for traversal; 13:14:53.486 [Executor task launch worker-0] INFO org.broadinstitute.hellbender.engine.ReadsDataSource - Preparing readers for traversal; 13:14:53.506 [Executor task launch worker-0] INFO org.broadinstitute.hellbender.engine.ReadsDataSource - Done preparing readers for traversal; 15/07/14 13:14:53 INFO executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 779 bytes result sent to driver; 15/07/14 13:14:53 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 403 ms on localhost (1/1); 15/07/14 13:14:53 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool; 15/07/14 13:14:53 INFO scheduler.DAGScheduler: Stage 0 (aggregate at TransformT",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/639#issuecomment-121313713:24408,load,loader,24408,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/639#issuecomment-121313713,1,['load'],['loader']
Performance,institute.hellbender.tools.genomicsdb.GenomicsDBImport.getFeatureReaders(GenomicsDBImport.java:419); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.traverse(GenomicsDBImport.java:344); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:740); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:220); Caused by: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: Read timed out; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.getBuf(SeekableByteChannelPrefetcher.java:136); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.fetch(SeekableByteChannelPrefetcher.java:255); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:300); 	... 28 more; Caused by: com.google.cloud.storage.StorageException: Read timed out; 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:186); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:512); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:128); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:125); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:92); 	at shaded.cloud_nio.com.google.api.gax.retrying.RetryingFuture,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300298180:2937,concurren,concurrent,2937,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300298180,1,['concurren'],['concurrent']
Performance,internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:80); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:59); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:483); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); at org.gradle.messaging.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); at org.gradle.messaging.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); at com.sun.proxy.$Proxy2.stop(Unknown Source); at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:116); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:483); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); at org.gradle.messaging.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:360); at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1258#issuecomment-162574174:4813,concurren,concurrent,4813,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1258#issuecomment-162574174,4,['concurren'],['concurrent']
Performance,"ion. We will repeat the het-genotyping step, but this is cheap and it's probably better to repeat it to make sure filtering is applied consistently. It would also require more changes to the command line to specify where to output the hets for each sample during multisample segmentation and to skip genotyping in each scatter, if we were to go that route. There are many possible combinations of inputs that need to be tested, but the same is already true of the current ModelSegments. Furthermore, there are slight wrinkles when running in tumor-only mode (i.e., when `--normal-allelic-counts` are not available). Because each sample is genotyped indiviudally, each may yield a different set of hets (in contrast to genotyping in matched-normal mode, in which the normal determines the set of hets used in all samples). We will thus have to take the intersection of these hets before performing multisample segmentation. Unfortunately, we will not be able to re-perform this intersection in each scatter, since we will no longer have access to the hets from the other samples. However, we *will* ultimately intersect the hets from each sample with the joint segmentation before modeling, which may be a rough proxy for the intersection of hets from all samples. As always, tumor-only mode may yield suboptimal results in certain scenarios, e.g., high purity CNLOH. I think I'm OK with just documenting these wrinkles, rather than working too hard to iron them out. I think this structure sets us up nicely to accommodate germline tagging/filtering in the near future. We can still pass the Picard interval list containing the joint segmentation to the scatter for the normal, but can instead subsequently pass the *.modelBegin.seg result from the normal to the tumors. This modeled-segment file will have breakpoints identical to those from the joint segmentation (as opposed to the *.modelFinal.seg result, since that undergoes segment smoothing/merging), but will also contain the segment-level p",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6499#issuecomment-607313549:3104,perform,perform,3104,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6499#issuecomment-607313549,1,['perform'],['perform']
Performance,"ion.java:182); 	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:164); 	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:412); 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64); 	at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 5.0 failed 1 times, most recent failure: Lost task 1.0 in stage 5.0 (TID 12, localhost, executor driver): java.util.ConcurrentModificationException; 	at java.util.ArrayList.sort(ArrayList.java:1464); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.<init>(ReadThreadingAssembler.java:81); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerReadThreadingAssemblerArgumentCollection.makeReadThreadingAssembler(HaplotypeCallerReadThreadingAssemblerArgumentCollection.java:37); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerArgumentCollection.createReadThreadingAssembler(AssemblyBasedCallerArgumentCollection.java:36); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.initialize(HaplotypeCallerEngine.java:231); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.<init>(HaplotypeCallerEngine.java:166); 	at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.lambda$assemblyFunction$29848511$1(HaplotypeCallerSpark.java:174); 	at org.apache.spark",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429:8035,Concurren,ConcurrentModificationException,8035,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429,1,['Concurren'],['ConcurrentModificationException']
Performance,"ire count file for all samples a determining factor? If we can drastically reduce this cost, then we can dedicate more to increasing resolution, etc. Here is a minimal set of fixes that could enable the querying of intervals for GermlineCNVCaller (and also for DetermineGermlineContigPloidy without too much extra work, since we also subset intervals there) *only in the gCNV WGS pipeline*, without disrupting other interfaces:. 1) Write a Tribble SimpleCountCodec for the `counts.tsv` extension. I've already done this in a branch.; 2) Change GermlineCNVCaller and DetermineGermlineContigPloidy tools to accept paths.; 3) If an index is present for each count path, create a FeatureDataSource, merge the requested -L/-XL intervals, and query to perform the subset. We will also need to stream the SAM header metadata. It should not require much code to extract all this to a temporary IndexedSimpleCountCollection class. (Caveat: for now, this will work with the current gCNV convention of providing bins via -L/-XL. Technically, it will also work with the more conventional use of -L/-XL to denote contiguous regions, but we may have to perform checks that bins are not duplicated in adjacent shards if they overlap both, since querying a FeatureDataSource will return any bins that overlap the interval---rather than only those that are completely contained within it.); 4) Index read-count TSVs in the gCNV WGS pipeline after collection and modify the DetermineGermlineContigPloidy and GermlineCNVCaller tasks to take read-count paths and indices, if necessary. These changes could be confined in the gCNV WGS WDL for now. I think that should do the trick. If this is high priority, I can implement now. In the future, we might be able to promote all Locatable CNV Records to Features and write code to automatically pass the columns/encoders/decoders (currently held in the Collection corresponding to each Record) to a single Tribble codec. This codec should not depend upon the file extension.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5716#issuecomment-468360082:1291,perform,perform,1291,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5716#issuecomment-468360082,1,['perform'],['perform']
Performance,"is also includes an addition of `libblas-dev`.; - [x] Update expected results for integration tests, perhaps add any that might be missing. EDIT: These were generated on WSL Ubuntu 20.04.2, we'll see if things pass on 22.04. Note that changing the ARD priors does change the *names* of the expected files, since the transform is appended to the corresponding variable name. DetermineGermlineContigPloidy and PostprocessGermlineCNVCalls are missing exact-match tests and should probably have some, but I'll leave that to someone else.; - [x] Update other python integration tests.; - [x] Clean up some of the changes to the priors.; - [x] Clean up some TODO comments that I left to track code changes that might result in changed numerics. I'll try to go through and convert these to PR comments in an initial review pass.; - [x] Test over multiple shards on WGS and WES. Probably some scientific tests on ~100 samples in both cohort and case mode would do the trick. We should also double check runtime/memory performance (I noted ~1.5x speedups, but didn't measure carefully; I also want to make sure the changes to posterior sampling didn't introduce any memory issues). @mwalker174 will ping you when a Docker is ready! Might be good to loop in Isaac and/or Jack as well.; - [x] Perhaps add back the fix for 2-interval shards in https://github.com/broadinstitute/gatk/pull/8180, which I removed since the required functionality wasn't immediately available in Pytensor. Not sure if this actually broke things though---need to check. (However, I don't actually think this is a very important use case to support...); - [x] Delete/deprecate/etc. CNN tools/tests as appropriate. Note that this has to be done concurrently, since we remove Tensorflow. @droazen perhaps I can take a first stab at this in a subsequent commit to this PR once more of the gCNV dust settles and/or has undergone a preliminary review? EDIT: Disabled integration/WDL tests. We should add some deprecation messages to the too",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8561#issuecomment-1847549285:2883,perform,performance,2883,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8561#issuecomment-1847549285,1,['perform'],['performance']
Performance,"is run appear to be:. haplotype-to-reference: 2, -8, -19; read-to-haplotype: 1, -4, -3. I wouldn't put much stock in interpreting these parameters or their exact values for now, but it does appear that the match values and the haplotype-to-reference gap-open penalty might be saturating the bounds of the search. Plots of the type suggested by @dalessioluca might be more illuminating. Compare with default performance:. ````; Threshold True-pos-baseline True-pos-call False-pos False-neg Precision Sensitivity F-measure; ----------------------------------------------------------------------------------------------------; 9.000 4003 4019 494 1036 0.8905 0.7944 0.8397; None 4009 4025 511 1030 0.8873 0.7956 0.8390; ````. That the corresponding curve with a precision/sensitivity endpoint of (0.8873, 0.7956) above isn't at the top of the pack means that we could squeeze out some extra calls by varying the SW parameters. Of course, this doesn't account for negative impact elsewhere. One could imagine writing a loss where this sensitivity is optimized while putting minimum constraints on precision, sensitivity, and/or F1 in the high-confidence, high-complexity regions (the assumption being the truth set is complete in those regions), or some weightings/variations thereof. EDIT: Actually, looks like overall performance in the high-confidence region improves:. ````; Threshold True-pos-baseline True-pos-call False-pos False-neg Precision Sensitivity F-measure; ----------------------------------------------------------------------------------------------------; 12.000 49955 49955 1932 2065 0.9628 0.9603 0.9615; None 49988 49988 1994 2032 0.9616 0.9609 0.9613; ````; vs. defaults:; ````; Threshold True-pos-baseline True-pos-call False-pos False-neg Precision Sensitivity F-measure; ----------------------------------------------------------------------------------------------------; 12.000 49837 49865 2012 2183 0.9612 0.9580 0.9596; None 49870 49898 2077 2150 0.9600 0.9587 0.9594; ````",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-715465692:1501,optimiz,optimized,1501,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-715465692,2,"['optimiz', 'perform']","['optimized', 'performance']"
Performance,ispatch(ContextClassLoaderDispatch.java:32); 	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); 	at com.sun.proxy.$Proxy2.stop(Unknown Source); 	at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:377); 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.testng.TestNGException:An error occurred while instantiating class org.broadinstitute.hellbender.engine.spark.ReadsPreprocessingPipelineSparkTestData. Check to make sure it can be instantiated; 	at org.testng.internal.InstanceCreator.createInstanceUsingObjectFactory(InstanceCreator.java:134); 	at org.testng.internal.InstanceCreator.createInstance(InstanceCreator.java:79); 	at org.testng.internal.ClassImpl.getDefaultInstance(ClassImpl.java:110); 	at org.testng.internal.ClassImpl.getInstances(ClassImpl.java:195); 	at org.testng.TestClass.getInstances(TestClass.java:102); 	at org.testng.TestClass.initTestClassesAndInstances(TestClass.java:82); 	at org.testng.TestClass.init(TestClass.java:74); 	at org.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5787#issuecomment-472107858:2046,concurren,concurrent,2046,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5787#issuecomment-472107858,1,['concurren'],['concurrent']
Performance,it/757112ff6cd8c8a81bb43319a0936dc0bf69a9ec?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) (757112f) will **increase** coverage by `0.020%`.; > The diff coverage is `85.791%`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## master #8132 +/- ##; ===============================================; + Coverage 86.642% 86.662% +0.020% ; - Complexity 38963 39097 +134 ; ===============================================; Files 2336 2341 +5 ; Lines 182730 183522 +792 ; Branches 20066 20117 +51 ; ===============================================; + Hits 158321 159043 +722 ; - Misses 17366 17399 +33 ; - Partials 7043 7080 +37 ; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/8132?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) | Coverage Δ | |; |---|---|---|; | [...lkers/vqsr/scalable/ExtractVariantAnnotations.java](https://codecov.io/gh/broadinstitute/gatk/pull/8132?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvRXh0cmFjdFZhcmlhbnRBbm5vdGF0aW9ucy5qYXZh) | `89.062% <ø> (-3.125%)` | :arrow_down: |; | [.../scalable/data/LabeledVariantAnnotationsDatum.java](https://codecov.io/gh/broadinstitute/gatk/pull/8132?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvZGF0YS9MYWJlbGVkVmFyaWFudEFubm90YXRpb25zRGF0dW0uamF2YQ==) | `63.158% <ø> (-5.263%)` | :arrow_down: |; | [...lable/modeling/VariantAnnotationsModelBackend.java](https://codecov.io/gh/broadinstitute/gatk/pull/8132?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8132#issuecomment-1370265333:1444,scalab,scalable,1444,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8132#issuecomment-1370265333,1,['scalab'],['scalable']
Performance,"ites /data/reference/gatk_resource/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz -O PAAD11N.recal_data.test.table; Using GATK jar /data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx8G -Djava.io.tmpdir=/data/xieduo/gatktest -jar /data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar BaseRecalibrator -R /data/reference/gatk_resource/Homo_sapiens_assembly38.fasta -I /data/xieduo/Immun_genomics/data/Łuksza_2022_Nature/bam/PAAD11N.bam --known-sites /data/xieduo/WES_pipe/pipeline/gatk_resource/dbsnp_146.hg38.vcf.gz --known-sites /data/reference/gatk_resource/1000G_phase1.snps.high_confidence.hg38.vcf.gz --known-sites /data/reference/gatk_resource/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz -O PAAD11N.recal_data.test.table; 13:35:32.710 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 13:35:32.890 INFO BaseRecalibrator - ------------------------------------------------------------; 13:35:32.891 INFO BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.2.6.1; 13:35:32.891 INFO BaseRecalibrator - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:35:32.891 INFO BaseRecalibrator - Executing as xieduo@pbs-master on Linux v3.10.0-1160.41.1.el7.x86_64 amd64; 13:35:32.891 INFO BaseRecalibrator - Java runtime: Java HotSpot(TM) 64-Bit Server VM v18+36-2087; 13:35:32.891 INFO BaseRecalibrator - Start Date/Time: September 22, 2022 at 1:35:32 PM CST; 13:35:32.891 INFO BaseRecalibrator - ------------------------------------------------------------; 13:35:32.892 INFO BaseRecalibrator - ------------------------------------------------------------; 13:35:32.892 INFO BaseRecalibrat",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8005#issuecomment-1254561081:1732,Load,Loading,1732,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005#issuecomment-1254561081,1,['Load'],['Loading']
Performance,"ith downstream tools.; - [x] Maximum number of eigensamples K to retain in the PoN is specified; the smaller of this or the number of samples remaining after filtering is used. The number actually used to denoise can be specified in DenoiseReadCounts. If we are going to spend energy computing K eigensamples, there is no reason we shouldn't expose all of them in the PoN, even if we don't want to use all of them for denoising. (Also, the current SVD utility methods do not allow for specification of K < N when performing SVD on an MxN matrix, even though the backend implementations that are called do allow for this; this is terrible. In any case, randomized SVD should be much faster than the currently available implementations, even when K = N).; - [x] Rename CreatePanelOfNormals to CreateReadCountPanelOfNormals; - [x] Refer to ""targets"" as intervals. See #3246.; - [x] Remove QC.; - [x] Refer to proportional coverage as fractional coverage.; - [x] Perform optional GC-bias correction internally if annotated intervals are passed as input.; - [x] Make standardization process for panel and case samples identical. Currently, a sample mean is taken at one point in the PoN standardization process, while a sample median is taken in the case standardization process.; - [x] HDF5 PoN will store version number, all integer read counts, all/panel intervals, all/panel sample paths/names, all annotated intervals (if GC-bias correction was performed), fractional-coverage medians for all intervals, relevant SVD results (eigenvalues and left-singular vectors) for the specified number of eigensamples, and command line.; - [x] In a future iteration, we could allow an input PoN to be the source of read counts. This would allow iteration on filter parameters without needing output from CombineReadCounts. The code should easily allow for this.; - [x] ReadCountCollection is too memory intensive; minimize use in DenoiseReadCounts when writing results.; - [x] Optimize and clean up HDF5 writing ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-313921687:1586,Perform,Perform,1586,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-313921687,1,['Perform'],['Perform']
Performance,"itializing engine; Created workspace /humgen/gsa-hpprojects/dev/gauthier/reblockGVCF/forkTest; 16:28:04.155 INFO GenomicsDBImport - Vid Map JSON file will be written to forkTest/vidmap.json; 16:28:04.155 INFO GenomicsDBImport - Callset Map JSON file will be written to forkTest/callset.json; 16:28:04.156 INFO GenomicsDBImport - Complete VCF Header will be written to forkTest/vcfheader.vcf; 16:28:04.156 INFO GenomicsDBImport - Importing to array - forkTest/genomicsdb_array; 16:28:04.158 INFO ProgressMeter - Starting traversal; 16:28:04.158 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 16:28:05.198 INFO GenomicsDBImport - Starting batch input file preload; 16:29:23.571 INFO GenomicsDBImport - Finished batch preload; 16:48:46.140 INFO GenomicsDBImport - Shutting down engine; [May 4, 2018 4:48:46 PM EDT] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 20.96 minutes.; Runtime.totalMemory()=22281715712; java.util.concurrent.CompletionException: java.lang.OutOfMemoryError: Java heap space; at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273); at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1592); at java.util.concurrent.CompletableFuture$AsyncSupply.exec(CompletableFuture.java:1582); at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056); at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692); at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157); Caused by: java.lang.OutOfMemoryError: Java heap space; at com.intel.genomicsdb.importer.SilentByteBufferStream.<init>(SilentByteBufferStream.java:55); at com.intel.genomicsdb.importer.GenomicsDBImporterStreamWrapper.<init>(GenomicsDBImporterStreamWrapper.java:70); at com.in",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388079572:3426,concurren,concurrent,3426,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388079572,1,['concurren'],['concurrent']
Performance,itute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvU2NvcmVWYXJpYW50QW5ub3RhdGlvbnMuamF2YQ==) | `0.000%` |; | [.../scalable/data/LabeledVariantAnnotationsDatum.java](https://app.codecov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvZGF0YS9MYWJlbGVkVmFyaWFudEFubm90YXRpb25zRGF0dW0uamF2YQ==) | `ø` |; | [...scalable/modeling/BGMMVariantAnnotationsModel.java](https://app.codecov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvbW9kZWxpbmcvQkdNTVZhcmlhbnRBbm5vdGF0aW9uc01vZGVsLmphdmE=) | `ø` |; | [...sr/scalable/modeling/VariantAnnotationsScorer.java](https://app.codecov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvbW9kZWxpbmcvVmFyaWFudEFubm90YXRpb25zU2NvcmVyLmphdmE=) | `ø` |; | [...able/ExtractVariantAnnotationsIntegrationTest.java](https://app.codecov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvRXh0cmFjdFZhcmlhbnRBbm5vdGF0aW9uc0ludGVncmF0aW9uVGVzdC5qYXZh) | `ø` |; | [...rs/vqsr/scalable/TrainVariantAnnotationsModel.java](https://app.codecov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8131#issuecomment-1352314153:3141,scalab,scalable,3141,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8131#issuecomment-1352314153,1,['scalab'],['scalable']
Performance,"itute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:152); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:195); at org.broadinstitute.hellbender.Main.main(Main.java:275). Next, I tried only ""gencode"" in the data-source folder. Using GATK jar /omics/chatchawit/gatk/gatk-package-4.0.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -jar /omics/chatchawit/gatk/gatk-package-4.0.0.0-local.jar Funcotator -R /omics/chatchawit/bundle/hsa38.fasta -V /omics/chatchawit/sm/out/sample21.vcf -O /omics/chatchawit/sm/anno/sample21.vcf --data-sources-path /omics/chatchawit/bundle/test/ --ref-version hg38; 23:01:57.151 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/omics/chatchawit/gatk/gatk-package-4.0.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 23:01:57.341 INFO Funcotator - ------------------------------------------------------------; 23:01:57.341 INFO Funcotator - The Genome Analysis Toolkit (GATK) v4.0.0.0; 23:01:57.341 INFO Funcotator - For support and documentation go to https://software.broadinstitute.org/gatk/; 23:01:57.342 INFO Funcotator - Executing as chatchawit@omics on Linux v3.13.0-133-generic amd64; 23:01:57.342 INFO Funcotator - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_161-b12; 23:01:57.342 INFO Funcotator - Start Date/Time: April 27, 2018 11:01:57 PM ICT; 23:01:57.343 INFO Funcotator - ------------------------------------------------------------; 23:01:57.343 INFO Funcotator - ------------------------------------------------------------; 23:01:57.344 INFO Funcotator - HTSJDK Version: 2.13.2; 23:01:57.344 INFO Funcotator - Picard Version: 2.17.2; 23:01:57",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385021157:4281,Load,Loading,4281,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385021157,1,['Load'],['Loading']
Performance,java.lang.Thread.run(Thread.java:745); Caused by: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: Failure while waiting for FeatureReader to initialize with exception: com.google.cloud.storage.StorageException: All 20 reopens failed. Waited a total of 1918000 ms between attempts; at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lambda$getFeatureReadersInParallel$614(GenomicsDBImport.java:605); at java.util.LinkedHashMap.forEach(LinkedHashMap.java:684); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getFeatureReadersInParallel(GenomicsDBImport.java:600); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.createSampleToReaderMap(GenomicsDBImport.java:491); at com.intel.genomicsdb.importer.GenomicsDBImporter.lambda$null$2(GenomicsDBImporter.java:602); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590); ... 3 more; Caused by: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: All 20 reopens failed. Waited a total of 1918000 ms between attempts; at java.util.concurrent.FutureTask.report(FutureTask.java:122); at java.util.concurrent.FutureTask.get(FutureTask.java:192); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lambda$getFeatureReadersInParallel$614(GenomicsDBImport.java:602); ... 8 more; Caused by: com.google.cloud.storage.StorageException: All 20 reopens failed. Waited a total of 1918000 ms between attempts; at com.google.cloud.storage.contrib.nio.CloudStorageRetryHandler.handleReopenForStorageException(CloudStorageRetryHandler.java:124); at com.google.cloud.storage.contrib.nio.CloudStorageRetryHandler.handleStorageException(CloudStorageRetryHandler.java:94); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:621); at java.nio.file.Files.exists(Files.java:2385); at htsjdk.tribble.util.ParsingUtils.re,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-412904420:2172,concurren,concurrent,2172,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-412904420,1,['concurren'],['concurrent']
Performance,"k-4.0.12.0/gatk-package-4.0.12.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx4g -Xms4g -jar /home/user/gatk-4.0.12.0/gatk-package-4.0.12.0-local.jar GenomicsDBImport -V /mnt/isilon/experiment/18075-01/aligned/variant_calling/18175D-01-03_S44_L006.filt.srt.nodup.recal.bam.g.vcf.gz -V /mnt/isilon/experiment/18075-01/aligned/variant_calling/18175D-01-04_S45_L006.filt.srt.nodup.recal.bam.g.vcf.gz -V /mnt/isilon/experiment/18075-01/aligned/variant_calling/18175D-01-01_S42_L006.filt.srt.nodup.recal.bam.g.vcf.gz -V /mnt/isilon/experiment/18075-01/aligned/variant_calling/18175D-01-02_S43_L006.filt.srt.nodup.recal.bam.g.vcf.gz --genomicsdb-workspace-path /mnt/isilon/experiment/18075-01/aligned/variant_calling/genomics_db -L /mnt/isilon/experiment/resources/final_mm10_exon.bed --interval-padding 500 --reader-threads 5; 11:57:49.202 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/user/gatk-4.0.12.0/gatk-package-4.0.12.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 11:57:50.896 INFO GenomicsDBImport - ------------------------------------------------------------; 11:57:50.896 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.0.12.0; 11:57:50.896 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:57:50.896 INFO GenomicsDBImport - Executing as user@user-machine on Linux v3.13.0-119-generic amd64; 11:57:50.896 INFO GenomicsDBImport - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_191-b12; 11:57:50.896 INFO GenomicsDBImport - Start Date/Time: January 11, 2019 11:57:49 AM EST; 11:57:50.897 INFO GenomicsDBImport - ------------------------------------------------------------; 11:57:50.897 INFO GenomicsDBImport - ------------------------------------------------------------; 11:57:50.897 INFO GenomicsDBImport - HTSJDK Version: 2.18.1; 11",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5342#issuecomment-453590820:1078,Load,Loading,1078,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5342#issuecomment-453590820,1,['Load'],['Loading']
Performance,"k.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 ,spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 ,spark.kryoserializer.buffer.max=512m,spark.yarn.executor.memoryOverhead=600,spark.executor.cores=2,spark.executor.instances=2 --jar /Users/droazen/src/hellbender/build/libs/gatk-package-4.beta.6-54-g0ee99da-SNAPSHOT-spark.jar -- CountReadsSpark -I gs://hellbender/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam --sparkMaster yarn; Job [acdae2af-e0ce-4822-87f5-dcd165d85cf4] submitted.; Waiting for job output...; 20:39:42.869 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 20:39:43.053 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/tmp/acdae2af-e0ce-4822-87f5-dcd165d85cf4/gatk-package-4.beta.6-54-g0ee99da-SNAPSHOT-spark.jar!/com/intel/gkl/native/libgkl_compression.so; [November 27, 2017 8:39:43 PM UTC] CountReadsSpark --input gs://hellbender/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam --sparkMaster yarn --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --interval_merging_rule ALL --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --gcs_max_retries 20 --disableToolDefaultReadFilters false; [November 27, 2017 8:39:43 PM UTC] Executing as root@droazen-test-cluster-m on Linux 3.16.0-4-amd64 amd64; OpenJDK 64-Bit Server VM 1.8.0_131-8u131-b11-1~bpo8+1-b11; Version: 4.beta.6-54-g0ee99da-SNAPSHOT; 20:39:43.245 INFO Cou",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3855#issuecomment-347320994:2824,Load,Loading,2824,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3855#issuecomment-347320994,1,['Load'],['Loading']
Performance,k/commit/c5bccee40e3b645a71f0f3ffb48a6c7b485e99a8?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) (c5bccee) will **increase** coverage by `0.051%`.; > The diff coverage is `69.697%`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## master #8074 +/- ##; ===============================================; + Coverage 86.593% 86.644% +0.051% ; - Complexity 38899 38963 +64 ; ===============================================; Files 2336 2336 ; Lines 182709 182730 +21 ; Branches 20060 20066 +6 ; ===============================================; + Hits 158213 158325 +112 ; + Misses 17441 17365 -76 ; + Partials 7055 7040 -15 ; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/8074?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) | Coverage Δ | |; |---|---|---|; | [.../scalable/data/LabeledVariantAnnotationsDatum.java](https://codecov.io/gh/broadinstitute/gatk/pull/8074/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvZGF0YS9MYWJlbGVkVmFyaWFudEFubm90YXRpb25zRGF0dW0uamF2YQ==) | `68.421% <45.455%> (-3.801%)` | :arrow_down: |; | [...vqsr/scalable/LabeledVariantAnnotationsWalker.java](https://codecov.io/gh/broadinstitute/gatk/pull/8074/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvTGFiZWxlZFZhcmlhbnRBbm5vdGF0aW9uc1dhbGtlci5qYXZh) | `86.822% <46.154%> (+0.208%)` | :arrow_up: |; | [...rs/vqsr/scalable/TrainVariantAnnotationsModel.java](https://codecov.io/gh/broadinstitute/gatk/pull/8074/diff?src=pr&el=tree&utm_medium=referr,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8074#issuecomment-1294055323:1428,scalab,scalable,1428,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8074#issuecomment-1294055323,1,['scalab'],['scalable']
Performance,"kableByteChannelPrefetcher$WorkUnit.getBuf(SeekableByteChannelPrefetcher.java:136); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.fetch(SeekableByteChannelPrefetcher.java:255); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:300); 	... 44 more; Caused by: com.google.cloud.storage.StorageException: java.lang.IllegalArgumentException: Position should be non-negative, is %d [-134217728]; 	at com.google.cloud.storage.StorageException.translateAndThrow(StorageException.java:71); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:139); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:113); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:131); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:104); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Position should be non-negative, is %d [-134217728]; 	at shaded.cloud_nio.com.google.common.base.Preconditions.checkArgument(Preconditions.java:146); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:487); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:127); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:124); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:93); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:49); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:124); 	... 7 more; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317549881:8880,concurren,concurrent,8880,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317549881,3,['concurren'],['concurrent']
Performance,"kableByteChannelPrefetcher$WorkUnit.getBuf(SeekableByteChannelPrefetcher.java:136); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.fetch(SeekableByteChannelPrefetcher.java:255); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:300); 	... 44 more; Caused by: com.google.cloud.storage.StorageException: java.lang.IllegalArgumentException: Position should be non-negative, is %d [-830472192]; 	at com.google.cloud.storage.StorageException.translateAndThrow(StorageException.java:71); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:139); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:113); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:131); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:104); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Position should be non-negative, is %d [-830472192]; 	at shaded.cloud_nio.com.google.common.base.Preconditions.checkArgument(Preconditions.java:146); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:487); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:127); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:124); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:93); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:49); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:124); 	... 7 more; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317442564:8068,concurren,concurrent,8068,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317442564,3,['concurren'],['concurrent']
Performance,ketException: Connection reset; 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:186); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:512); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:128); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:125); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:92); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:47); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:125); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:109); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:131); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:104); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: javax.net.ssl.SSLException: Connection has been shutdown: javax.net.ssl.SSLException: java.net.SocketException: Connection reset; 	at sun.security.ssl.SSLSocketImpl.checkEOF(SSLSocketImpl.java:1541); 	at sun.security.ssl.AppInputStream.available(AppInputStream.java:60); 	at java.io.BufferedInputStream.available(BufferedInputStream.java:410); 	at sun.net.www.MeteredStream.available(MeteredStream.java:170); 	at sun.net.www.http.KeepAliveStream.close(KeepAliveStream.java:85); 	at java.io.FilterInputStream.close(FilterInputStream.java:181); 	at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.close(HttpURLConnection.java:3448); 	at java.io.FilterInputStream.close(FilterInputStream.java:181); 	at shaded.cloud_nio.com.g,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-301610931:6433,concurren,concurrent,6433,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-301610931,1,['concurren'],['concurrent']
Performance,lDeflater; 15:47:37.247 INFO Mutect2 - Inflater: IntelInflater; 15:47:37.247 INFO Mutect2 - GCS max retries/reopens: 20; 15:47:37.247 INFO Mutect2 - Requester pays: disabled; 15:47:37.247 INFO Mutect2 - Initializing engine; 15:47:41.204 INFO Mutect2 - Done initializing engine; 15:47:42.352 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/genouest/uni_limoges_fr/jpollet/.conda/envs/myd88/share/gatk4-4.1.4.0-1/gatk-package-4.1.4.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 15:47:42.423 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/genouest/uni_limoges_fr/jpollet/.conda/envs/myd88/share/gatk4-4.1.4.0-1/gatk-package-4.1.4.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 15:47:42.482 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 15:47:42.483 INFO IntelPairHmm - Available threads: 8; 15:47:42.483 INFO IntelPairHmm - Requested threads: 4; 15:47:42.483 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 15:47:42.936 INFO ProgressMeter - Starting traversal; 15:47:42.936 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 15:47:53.565 INFO ProgressMeter - ENA|LVXK01000001|LVXK01000001.1:19555 0.2 90 508.0; 15:48:05.962 INFO ProgressMeter - ENA|LVXK01000001|LVXK01000001.1:136820 0.4 600 1563.5; 15:48:16.023 INFO ProgressMeter - ENA|LVXK01000001|LVXK01000001.1:360783 0.6 1560 2828.9; 15:48:19.342 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 0.010346494000000001; 15:48:19.342 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 6.453042841; 15:48:19.347 INFO SmithWatermanAligner - Total compute time in java Smith-Waterman : 10.39 sec; 15:48:19.348 INFO Mutect2 - Shutting down engine; [28 novembre 2019 15:48:19 CET] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.72 minutes.; Runtime.totalMemory()=3822583808; java.lang.IllegalArgumentException: Ca,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6271#issuecomment-559553558:2717,multi-thread,multi-threaded,2717,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6271#issuecomment-559553558,1,['multi-thread'],['multi-threaded']
Performance,lbender.relocated.com.google.common.collect.AbstractMapBasedMultimap.put; 0.5% 370 + 1 org.broadinstitute.hellbender.utils.baq.BAQ.calcBAQFromHMM; 0.5% 360 + 0 org.broadinstitute.hellbender.relocated.com.google.common.collect.ImmutableListMultimap.copyOf; 0.4% 348 + 5 scala.collection.IndexedSeqOptimized$class.zip; 0.4% 330 + 3 com.esotericsoftware.kryo.serializers.FieldSerializer$ObjectField.read; 0.4% 325 + 0 htsjdk.samtools.SAMBinaryTagAndValue.find; 0.4% 322 + 0 org.broadinstitute.hellbender.utils.read.SAMRecordToGATKReadAdapter.getReadGroup; 66.2% 45378 + 7026 Total compiled (including elided). Stub + native Method ; 11.8% 0 + 9359 java.lang.String.intern; 7.1% 0 + 5608 java.util.zip.Deflater.deflateBytes; 6.9% 0 + 5492 java.lang.System.identityHashCode; 2.8% 0 + 2196 java.util.zip.Inflater.inflateBytes; 1.8% 0 + 1447 java.net.SocketInputStream.socketRead0; 0.6% 0 + 484 java.io.FileOutputStream.writeBytes; 0.4% 0 + 285 java.util.zip.Inflater.reset; 0.3% 0 + 259 sun.nio.ch.NativeThread.current; 0.3% 0 + 222 sun.nio.ch.EPollArrayWrapper.epollWait; 0.2% 0 + 137 java.util.zip.Deflater.reset; 0.1% 0 + 115 sun.nio.ch.FileDispatcherImpl.read0; 0.1% 0 + 113 org.apache.hadoop.util.NativeCrc32.nativeComputeChunkedSumsByteArray; 0.1% 67 + 4 java.lang.ClassLoader.defineClass1; 0.1% 0 + 51 java.util.zip.ZipFile.getEntry; 0.1% 0 + 41 java.lang.Throwable.fillInStackTrace; 0.0% 0 + 24 org.apache.hadoop.util.NativeCrc32.nativeComputeChunkedSums; 0.0% 0 + 23 java.lang.System.arraycopy; 0.0% 0 + 18 java.lang.Object.clone; 0.0% 1 + 15 java.io.UnixFileSystem.getBooleanAttributes0; 0.0% 0 + 14 sun.reflect.Reflection.getClassAccessFlags; 0.0% 0 + 13 java.lang.Thread.isInterrupted; 0.0% 0 + 11 java.lang.Class.isPrimitive; 0.0% 0 + 11 java.lang.Class.isArray; 0.0% 0 + 10 sun.nio.ch.FileDispatcherImpl.size0; 0.0% 4 + 5 java.security.AccessController.doPrivileged; 33.0% 77 + 26077 Total stub (including elided). Thread-local ticks:; 29.9% 33828 Blocked (of total); 0.0% 3 Class loader. ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1657#issuecomment-208967490:5445,load,loader,5445,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1657#issuecomment-208967490,1,['load'],['loader']
Performance,"ld code is still used for GC-bias correction in `CreateReadCountPanelOfNormals`, and we still use the `AnnotateTargets` tool. We should port this over (possibly as part of `PreprocessIntervals`) at some point (actually, I think we will be forced to, since `PreprocessIntervals` will output a Picard interval list, and `AnnotateTargets` outputs a target file).; - [x] Integration tests are still needed for `CreateReadCountPanelOfNormals`. These might not test for correctness, but we could possibly compare to old PoNs. Segmentation/modeling:; - Instead of separate tools for copy-ratio segmentation (`PerformSegmentation`) and allele-fraction segmentation/union/modeling (`AllelicCNV`), there is now just a single segmentation/modeling tool (`ModelSegments`).; - Input is denoised copy ratio and/or allelic counts. If only one input is provided, then we only model only the corresponding quantity.; - There is no separate allele-fraction workflow. Unlike the old approach, we do not perform any genotyping or modeling before doing kernel segmentation.; - [x] Old code and classes are used for segment union. We should port or possibly replace this with a simple method that uses kernel segmentation. EDIT: Actually, just tried running a WGS sample and this is still a major bottleneck. EDIT 2: Hmm...actually doesn't seem to be an issue on my desktop (compared to my laptop, on which the run hangs here). Will try to track down the source of the discrepancy. EDIT 3: Added segment union based on single-changepoint detection using kernel segmentation.; - [x] Segment union should be replaced by a proper joint kernel segmentation. EDIT: I've added this, but there could be some minor improvements. Right now, only use one het per copy-ratio interval and throw away those off-target/bin. There are a few percent of targets/bins that have more than one het, and we could potentially rescue the off-target/bin hets as well with some care.; - Old code and models are used for modeling. Since the old all",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:3504,perform,perform,3504,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828,1,['perform'],['perform']
Performance,lerArgumentCollection.createReadThreadingAssembler(AssemblyBasedCallerArgumentCollection.java:36); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.initialize(HaplotypeCallerEngine.java:231); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.<init>(HaplotypeCallerEngine.java:166); 	at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.lambda$assemblyFunction$29848511$1(HaplotypeCallerSpark.java:174); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:153); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:153); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:123); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	... 1 more; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429:14621,concurren,concurrent,14621,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429,2,['concurren'],['concurrent']
Performance,"let's measure. 2 reruns of HC should tell us all we need to know. On Thu, Jul 7, 2016 at 12:06 PM, Louis Bergelson notifications@github.com; wrote:. > So there may be some unfortunate performance implications with some of; > these changes. Utils.nonNull(value, message) and it's compatriots will; > always compute the message even if the error condition is not met. Using; > any message which isn't a constant will generate garbage in the form of; > strings. In most cases this isn't a problem, but it is not ideal if it's; > placed in a tight loop.; > ; > We could offset the problem by adding a family of Utils functions that; > take a lambda String producer instead of a string itself, this would allow; > the message to be computed only when the error condition is triggered; > avoiding garbage creation.; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/gatk/pull/1979#issuecomment-231126857,; > or mute the thread; > https://github.com/notifications/unsubscribe/AB5rL-76TnggBn8C75KiUGdpb_0ZgIjdks5qTSQigaJpZM4JGaiQ; > .",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1979#issuecomment-231127666:184,perform,performance,184,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1979#issuecomment-231127666,1,['perform'],['performance']
Performance,"ll overlap the locus post-realignment. The end result is that DP for the HaplotypeCaller represents the undownsampled depth of coverage at the locus in question, subject to the hardcoded cap of 1000 and realignment to the haplotypes. In this particular case, the actual depth at locus 1:14464 is 561 (with no downsampling), and the DP value is 546. The difference is likely due to realignment of reads to the haplotypes by the walker. So, we basically have two options:; 1. Change the documentation for the DP annotation to mention that for ActiveRegion walkers it reflects the undownsampled depth subject to things like realignment to the haplotypes (easiest option, but doesn't fix the underlying craziness); 2. Change the ActiveRegion traversal so that it respects the dcov value (could be hard -- the LIBS downsampling process discards reads on-the-fly from previous loci when moving to a new locus, but an active region involves data for multiple loci. The potential performance win for the HC is huge, though, if we could pull this off). [...]. Alright, next step then is to figure out whether it's even feasible to make the ActiveRegion traversal fully respect dcov. I think Mark, in implementing the current scheme, might have been thinking that maintaining the undownsampled reads in memory is actually less expensive in typical (non-extreme) cases than reconstructing the full set of post-downsampling reads in an active region from multiple AlignmentContexts emitted by LIBS without any duplicates. I'll have to do some performance testing to see whether or not this is the case. Will try to get to this within the next few weeks, but the QC project has immediate priority. [...]. Discussed this with Ryan -- we agreed that the right thing to do is to move the enforcement of the hard cap on the total number of reads that can be in an active region from the HC walker to the engine, and have the size of the cap be controlled by a new argument (not dcov). That way you never pay the cost ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345:4331,perform,performance,4331,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345,1,['perform'],['performance']
Performance,"llbender/build/libs/hellbender-all-GATK.4.alpha-413-g741d007-SNAPSHOT-spark.jar at http://10.1.4.233:65239/jars/hellbender-all-GATK.4.alpha-413-g741d007-SNAPSHOT-spark.jar with timestamp 1436894092643; 15/07/14 13:14:52 INFO executor.Executor: Starting executor ID <driver> on host localhost; 15/07/14 13:14:52 INFO util.AkkaUtils: Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@wm1b0-8ab.broadinstitute.org:65238/user/HeartbeatReceiver; 15/07/14 13:14:52 INFO netty.NettyBlockTransferService: Server created on 65240; 15/07/14 13:14:52 INFO storage.BlockManagerMaster: Trying to register BlockManager; 15/07/14 13:14:52 INFO storage.BlockManagerMasterActor: Registering block manager localhost:65240 with 265.1 MB RAM, BlockManagerId(<driver>, localhost, 65240); 15/07/14 13:14:52 INFO storage.BlockManagerMaster: Registered BlockManager; 15/07/14 13:14:52 INFO spark.SparkPipelineRunner: Evaluating Create [Create]; 15/07/14 13:14:52 INFO spark.SparkPipelineRunner: Evaluating LoadReadsFromFile [ParDo]; 15/07/14 13:14:52 INFO spark.SparkPipelineRunner: Evaluating Init [ParDo]; 15/07/14 13:14:52 INFO spark.SparkPipelineRunner: Entering directly-translatable composite transform: 'CountReadsDataflowTransform/Globally/Combine.Globally'; 15/07/14 13:14:52 INFO spark.SparkPipelineRunner: Skipping 'CountReadsDataflowTransform/Globally/Combine.Globally/WithKeys/AddKeys'; already in composite transform.; 15/07/14 13:14:52 INFO spark.SparkPipelineRunner: Skipping 'CountReadsDataflowTransform/Globally/Combine.Globally/Combine.PerKey/GroupByKey/ReifyTimestampsAndWindows/ReifyTimestampAndWindows'; already in composite transform.; 15/07/14 13:14:52 INFO spark.SparkPipelineRunner: Skipping 'CountReadsDataflowTransform/Globally/Combine.Globally/Combine.PerKey/GroupByKey/GroupByKeyOnly'; already in composite transform.; 15/07/14 13:14:52 INFO spark.SparkPipelineRunner: Skipping 'CountReadsDataflowTransform/Globally/Combine.Globally/Combine.PerKey/GroupByKey/SortValuesByTimestamp/Anonymo",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/639#issuecomment-121313713:19328,Load,LoadReadsFromFile,19328,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/639#issuecomment-121313713,1,['Load'],['LoadReadsFromFile']
Performance,"llclockmax"": ""3.701625"",; ""NIST controlMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/bf86d5b6-04bd-4344-b4fc-8a1df66bb5d9/call-NISTSampleHeadToHead/BenchmarkComparison/8e62c1c2-cf9c-4530-846e-1e0d6c6d8acf/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""NIST controlindelF1Score"": ""0.9902"",; ""NIST controlindelPrecision"": ""0.9903"",; ""NIST controlsnpF1Score"": ""0.9899"",; ""NIST controlsnpPrecision"": ""0.9887"",; ""NIST controlsnpRecall"": ""0.9911"",; ""NIST controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/bf86d5b6-04bd-4344-b4fc-8a1df66bb5d9/call-NISTSampleHeadToHead/BenchmarkComparison/8e62c1c2-cf9c-4530-846e-1e0d6c6d8acf/call-BenchmarkVCFControlSample/Benchmark/e71074a5-27ad-4a8b-a533-cdc111c0374f/call-CombineSummaries/summary.csv"",; ""NIST evalHCprocesshours"": ""73.06777222222223"",; ""NIST evalHCsystemhours"": ""0.1622555555555555"",; ""NIST evalHCwallclockhours"": ""46.65241388888888"",; ""NIST evalHCwallclockmax"": ""2.7461055555555554"",; ""NIST evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/bf86d5b6-04bd-4344-b4fc-8a1df66bb5d9/call-NISTSampleHeadToHead/BenchmarkComparison/8e62c1c2-cf9c-4530-846e-1e0d6c6d8acf/call-EVALRuntimeTask/cacheCopy/monitoring.pdf"",; ""NIST evalindelF1Score"": ""0.9843"",; ""NIST evalindelPrecision"": ""0.9895"",; ""NIST evalsnpF1Score"": ""0.9908"",; ""NIST evalsnpPrecision"": ""0.992"",; ""NIST evalsnpRecall"": ""0.9896"",; ""NIST evalsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/bf86d5b6-04bd-4344-b4fc-8a1df66bb5d9/call-NISTSampleHeadToHead/BenchmarkComparison/8e62c1c2-cf9c-4530-846e-1e0d6c6d8acf/call-BenchmarkVCFTestSample/Benchmark/d39f91bf-295b-4a15-bd0b-2b7c6b43a347/call-CombineSummaries/summary.csv"",; ""ROC_Plots_Reported"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/bf86d5b6-04bd-4344-b4fc-8a1df66bb5d9/call-CreateHTMLReport/report.html""; }; } ; </pre> </details>",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7723#issuecomment-1069765064:14467,cache,cacheCopy,14467,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7723#issuecomment-1069765064,1,['cache'],['cacheCopy']
Performance,loaded for pull request base (`master@d9fd22f`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `12.5%`. ```diff; @@ Coverage Diff @@; ## master #5787 +/- ##; ==========================================; Coverage ? 44.104% ; Complexity ? 19589 ; ==========================================; Files ? 1973 ; Lines ? 147147 ; Branches ? 16215 ; ==========================================; Hits ? 64898 ; Misses ? 77129 ; Partials ? 5120; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/5787?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...utils/activityprofile/ActivityProfileUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5787/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9hY3Rpdml0eXByb2ZpbGUvQWN0aXZpdHlQcm9maWxlVW5pdFRlc3QuamF2YQ==) | `0.442% <0%> (ø)` | `1 <0> (?)` | |; | [...ils/optimization/PersistenceOptimizerUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5787/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL3V0aWxzL29wdGltaXphdGlvbi9QZXJzaXN0ZW5jZU9wdGltaXplclVuaXRUZXN0LmphdmE=) | `2% <0%> (ø)` | `1 <0> (?)` | |; | [...utils/downsampling/DownsamplingMethodUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5787/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9kb3duc2FtcGxpbmcvRG93bnNhbXBsaW5nTWV0aG9kVW5pdFRlc3QuamF2YQ==) | `3.448% <0%> (ø)` | `1 <0> (?)` | |; | [...yper/StandardCallerArgumentCollectionUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5787/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9TdGFuZGFyZENhbGxlckFyZ3VtZW50Q29sbGVjdGlvblVuaXRUZXN0LmphdmE=) | `4.098% <0%> (ø)` | `2 <0> (?)` | |; | [...lbender/utils/mcmc/ParameterizedStateUnitTest.java](https://codecov,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5787#issuecomment-471963750:1097,optimiz,optimization,1097,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5787#issuecomment-471963750,1,['optimiz'],['optimization']
Performance,looks good to me. That's some pretty impressive performance increase you've accomplished.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1190#issuecomment-159377987:48,perform,performance,48,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1190#issuecomment-159377987,1,['perform'],['performance']
Performance,"loped a clustering procedure that is based on coverage profile at the set of targets that are highly variable across different capture kits. ; - We found that filtering on a QS metric on a final callset significantly boosted the specificity while lowering sensitivity insignificantly.; - We developed a hyperparameter optimization framework prototype that could be used in a future for general optimizations of cost/performance parameters for all GATK pipelines.; - We resolved several memory issues that came up during validations. **A few issues were encountered along the way:**; - The sensitivity and specificity on multiallellic (common) sites was significantly lower than on rare events.; - Single target calling sensitivity was lower than 20%.; - Pipeline WDL required optimization in order to handle whole genome data, however these changes were not consolidated in the official WDL. **Currently the ongoing work is focused on the following:**; - Improving sensitivity/specificity of calls on common regions. One solution being tested involves setting a prior for common regions derived from a high quality callset. Second solution is to set a different filtering threshold for common regions.; - Consolidating validation scripts to process gCNV output and outputs of competing tools measure their performances against ground truth.; - Analyzing 1000 Genomes exomes, which could be potentially used for public facing automatic evaluations. **The following items are necessary done for automatic evaluation:** ; - Dataset + truth. We need an access to a high quality public cohort with matched whole genomes. These genomes have to have a corresponding high quality truth set generated from split-read/read-pair methods. From that cohort we need to find 50-200 relatively homogeneous samples.; - An established validation workflow that outputs a set predetermined metrics that are unlikely to change in a future. Such as a sensitivity/specificity stratified by event size and allelic frequency.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4123#issuecomment-532500502:1854,perform,performances,1854,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4123#issuecomment-532500502,1,['perform'],['performances']
Performance,"ltBaseQualities -1 --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --readValidationStringency SILENT --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation false --createOutputBamIndex true --createOutputBamMD5 false --createOutputVariantIndex true --createOutputVariantMD5 false --lenient false --addOutputSAMProgramRecord true --addOutputVCFCommandLine true --cloudPrefetchBuffer 40 --cloudIndexPrefetchBuffer -1 --disableBamIndexCaching false --help false --version false --showHidden false --verbosity INFO --QUIET false --disableToolDefaultReadFilters false; [July 24, 2017 5:48:10 PM UTC] Executing as root@57972df58207 on Linux 4.9.0-0.bpo.3-amd64 amd64; OpenJDK 64-Bit Server VM 1.8.0_111-8u111-b14-2~bpo8+1-b14; Version: 4.beta.1; [July 24, 2017 6:04:42 PM UTC] org.broadinstitute.hellbender.tools.walkers.bqsr.BaseRecalibrator done. Elapsed time: 16.54 minutes.; Runtime.totalMemory()=4191682560; java.lang.RuntimeException: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: java.lang.IllegalArgumentException: Position should be non-negative, is %d [-1056964608]; 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:309); 	at htsjdk.samtools.seekablestream.SeekablePathStream.read(SeekablePathStream.java:86); 	at htsjdk.samtools.util.BlockCompressedInputStream.readBytes(BlockCompressedInputStream.java:554); 	at htsjdk.samtools.util.BlockCompressedInputStream.readBytes(BlockCompressedInputStream.java:543); 	at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:512); 	at htsjdk.samtools.util.BlockCompressedInputStream.nextBlock(BlockCompressedInputStream.java:455); 	at htsjdk.samtools.util.BlockCompressedInputStream.readBlock(BlockCompressedInputStream.java:445); 	at htsjdk.samtools.util.BlockCompressedInputStream.available(BlockCompressedInputStream.java:194); 	at htsjdk.samtools.util.Bl",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317782472:3554,concurren,concurrent,3554,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317782472,1,['concurren'],['concurrent']
Performance,"ltBaseQualities -1 --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --readValidationStringency SILENT --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation false --createOutputBamIndex true --createOutputBamMD5 false --createOutputVariantIndex true --createOutputVariantMD5 false --lenient false --addOutputSAMProgramRecord true --addOutputVCFCommandLine true --cloudPrefetchBuffer 40 --cloudIndexPrefetchBuffer -1 --disableBamIndexCaching false --help false --version false --showHidden false --verbosity INFO --QUIET false --disableToolDefaultReadFilters false; [July 24, 2017 5:49:13 PM UTC] Executing as root@bc900e525fef on Linux 4.9.0-0.bpo.3-amd64 amd64; OpenJDK 64-Bit Server VM 1.8.0_111-8u111-b14-2~bpo8+1-b14; Version: 4.beta.1; [July 24, 2017 6:18:40 PM UTC] org.broadinstitute.hellbender.tools.walkers.bqsr.BaseRecalibrator done. Elapsed time: 29.45 minutes.; Runtime.totalMemory()=4191682560; java.lang.RuntimeException: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: java.lang.IllegalArgumentException: Position should be non-negative, is %d [-134217728]; 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:309); 	at htsjdk.samtools.seekablestream.SeekablePathStream.read(SeekablePathStream.java:86); 	at htsjdk.samtools.util.BlockCompressedInputStream.readBytes(BlockCompressedInputStream.java:554); 	at htsjdk.samtools.util.BlockCompressedInputStream.readBytes(BlockCompressedInputStream.java:543); 	at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:512); 	at htsjdk.samtools.util.BlockCompressedInputStream.nextBlock(BlockCompressedInputStream.java:455); 	at htsjdk.samtools.util.BlockCompressedInputStream.readBlock(BlockCompressedInputStream.java:445); 	at htsjdk.samtools.util.BlockCompressedInputStream.available(BlockCompressedInputStream.java:194); 	at htsjdk.samtools.util.Blo",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317549881:3329,concurren,concurrent,3329,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317549881,1,['concurren'],['concurrent']
Performance,mF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3IvQ05OU2NvcmVWYXJpYW50cy5qYXZh) | `78.696% <100.000%> (+61.304%)` | :arrow_up: |; | [...hellbender/tools/walkers/vqsr/CNNVariantTrain.java](https://codecov.io/gh/broadinstitute/gatk/pull/8128/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3IvQ05OVmFyaWFudFRyYWluLmphdmE=) | `60.563% <100.000%> (+29.577%)` | :arrow_up: |; | [...der/tools/walkers/vqsr/CNNVariantWriteTensors.java](https://codecov.io/gh/broadinstitute/gatk/pull/8128/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3IvQ05OVmFyaWFudFdyaXRlVGVuc29ycy5qYXZh) | `85.714% <100.000%> (+53.571%)` | :arrow_up: |; | [.../scalable/data/LabeledVariantAnnotationsDatum.java](https://codecov.io/gh/broadinstitute/gatk/pull/8128/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvZGF0YS9MYWJlbGVkVmFyaWFudEFubm90YXRpb25zRGF0dW0uamF2YQ==) | `68.421% <0.000%> (-1.023%)` | :arrow_down: |; | [...titute/hellbender/utils/help/GATKGSONWorkUnit.java](https://codecov.io/gh/broadinstitute/gatk/pull/8128/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9oZWxwL0dBVEtHU09OV29ya1VuaXQuamF2YQ==) | `100.000% <0.000%> (ø)` | |; | [...notyper/GenotypeCalculationArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/8128/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+c,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8128#issuecomment-1358686586:2669,scalab,scalable,2669,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8128#issuecomment-1358686586,1,['scalab'],['scalable']
Performance,m_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvRXh0cmFjdFZhcmlhbnRBbm5vdGF0aW9ucy5qYXZh) | `ø` |; | [...walkers/vqsr/scalable/ScoreVariantAnnotations.java](https://app.codecov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvU2NvcmVWYXJpYW50QW5ub3RhdGlvbnMuamF2YQ==) | `0.000%` |; | [.../scalable/data/LabeledVariantAnnotationsDatum.java](https://app.codecov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvZGF0YS9MYWJlbGVkVmFyaWFudEFubm90YXRpb25zRGF0dW0uamF2YQ==) | `ø` |; | [...scalable/modeling/BGMMVariantAnnotationsModel.java](https://app.codecov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvbW9kZWxpbmcvQkdNTVZhcmlhbnRBbm5vdGF0aW9uc01vZGVsLmphdmE=) | `ø` |; | [...sr/scalable/modeling/VariantAnnotationsScorer.java](https://app.codecov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvbW9kZWxpbmcvVmFyaWFudEFubm90YXRpb25zU2NvcmVyLmphdmE=) | `ø` |; | [...able/ExtractVariantAnnotationsIntegrationTest.java](https://app.codecov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broa,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8131#issuecomment-1352314153:2734,scalab,scalable,2734,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8131#issuecomment-1352314153,1,['scalab'],['scalable']
Performance,"mapped.aligned.duplicates_marked.recalibrated.bam -bqsr S3_2.unmapped.recal_data.csv --add-output-sam-program-record --use-original-qualities`; RecalTables in S3_2.unmapped.recal_data.csv are empty. Here is the screen dump of BaseRecalibrator and ApplyBQSR.; BaseRecalibrator; ```; Using GATK jar <XXX>/gatk-4.1.4.1-83-g031c407-SNAPSHOT/gatk-package-4.1.4.1-83-g031c407-SNAPSHOT-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar <XXX>/gatk-4.1.4.1-83-g031c407-SNAPSHOT/gatk-package-4.1.4.1-83-g031c407-SNAPSHOT-local.jar BaseRecalibrator -R Homo_sapiens_assembly38.fasta -I S3_2.unmapped.split.bam --use-original-qualities -O S3_2.unmapped.recal_data.csv -known-sites Homo_sapiens_assembly38.dbsnp138.vcf -known-sites Mills_and_1000G_gold_standard.indels.hg38.vcf.gz --known-sites Homo_sapiens_assembly38.known_indels.vcf.gz; 23:39:34.668 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:<XXX>/gatk-4.1.4.1-83-g031c407-SNAPSHOT/gatk-package-4.1.4.1-83-g031c407-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 26, 2020 11:39:34 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 23:39:34.915 INFO BaseRecalibrator - ------------------------------------------------------------; 23:39:34.915 INFO BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.1.4.1-83-g031c407-SNAPSHOT; 23:39:34.915 INFO BaseRecalibrator - For support and documentation go to https://software.broadinstitute.org/gatk/; 23:39:34.915 INFO BaseRecalibrator - Executing as <XXX@XXX> on Linux v3.10.0-957.12.1.el7.x86_64 amd64; 23:39:34.915 INFO BaseRecalibrator - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-b08; 23:39:34.916 INFO BaseRecalibrator - Start Date/Time: February 26, 2020 11:39:34 PM EST; 23:39:34.916 INFO BaseRecal",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6242#issuecomment-592005237:1694,Load,Loading,1694,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6242#issuecomment-592005237,1,['Load'],['Loading']
Performance,"mjdk.compression_level=2 -Xmx8G -Djava.io.tmpdir=/data/xieduo/Łuksza_2022_Nature -jar /data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar BaseRecalibrator -R /data/reference/gatk_resource/Homo_sapiens_assembly38.fasta -I /data/xieduo/Immun_genomics/data/Łuksza_2022_Nature/bam/PAAD11N.bam --known-sites /data/xieduo/WES_pipe/pipeline/gatk_resource/dbsnp_146.hg38.vcf.gz --known-sites /data/reference/gatk_resource/1000G_phase1.snps.high_confidence.hg38.vcf.gz --known-sites /data/reference/gatk_resource/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz -O PAAD11N.recal_data.test.table; 13:36:33.528 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 13:36:33.547 WARN NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (No such file or directory); 13:36:33.550 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 13:36:33.551 WARN NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (No such file or directory); 13:36:33.669 INFO BaseRecalibrator - ------------------------------------------------------------; 13:36:33.670 INFO BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.2.6.1; 13:36:33.670 INFO BaseRecalibrator - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:36:33.670 INFO BaseRecalibrator - Executing as xieduo@pbs-master on Linux v3.10.0-1160.41.1.el7.x86_64 amd64; 13:36:33.670 INFO BaseRecalibrator - Java runtime: Java HotSpot(TM) 64-Bit Server VM v18+36-2087; 13:36:33.671 INFO BaseRecalibrator - Start Date/Time: September 22, 2022 at 1:36:33 PM CST; 13:36:33.671 INFO BaseRecalibrator - -----------------------------------------------------",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8005#issuecomment-1254561081:6838,Load,Loading,6838,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005#issuecomment-1254561081,1,['Load'],['Loading']
Performance,mment&utm_campaign=pr+comments&utm_term=broadinstitute) | Coverage Δ | |; |---|---|---|; | [.../scalable/data/LabeledVariantAnnotationsDatum.java](https://codecov.io/gh/broadinstitute/gatk/pull/8074/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvZGF0YS9MYWJlbGVkVmFyaWFudEFubm90YXRpb25zRGF0dW0uamF2YQ==) | `68.421% <45.455%> (-3.801%)` | :arrow_down: |; | [...vqsr/scalable/LabeledVariantAnnotationsWalker.java](https://codecov.io/gh/broadinstitute/gatk/pull/8074/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvTGFiZWxlZFZhcmlhbnRBbm5vdGF0aW9uc1dhbGtlci5qYXZh) | `86.822% <46.154%> (+0.208%)` | :arrow_up: |; | [...rs/vqsr/scalable/TrainVariantAnnotationsModel.java](https://codecov.io/gh/broadinstitute/gatk/pull/8074/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvVHJhaW5WYXJpYW50QW5ub3RhdGlvbnNNb2RlbC5qYXZh) | `77.778% <66.667%> (-2.991%)` | :arrow_down: |; | [...lkers/vqsr/scalable/ExtractVariantAnnotations.java](https://codecov.io/gh/broadinstitute/gatk/pull/8074/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvRXh0cmFjdFZhcmlhbnRBbm5vdGF0aW9ucy5qYXZh) | `92.188% <100.000%> (+1.116%)` | :arrow_up: |; | [...walkers/vqsr/scalable/ScoreVariantAnnotations.java](https://codecov.io/gh/broadinstitute/gatk/pull/8074/diff?src=pr&el=tree&utm_medium=referral&utm_source=githu,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8074#issuecomment-1294055323:2317,scalab,scalable,2317,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8074#issuecomment-1294055323,1,['scalab'],['scalable']
Performance,"more concretely the private method getReferenceBases(SAMSeqRecord) should be syncronized or avoid it calling directly to the syncronized getReferenceBases(SSR, boolean) and getReferenceBasesByRegions should not update the cache fields.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8139#issuecomment-1376313615:222,cache,cache,222,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8139#issuecomment-1376313615,1,['cache'],['cache']
Performance,"more recent versions of gatk have a different message now, which is also confusing:. ```; ./gatk-launch PrintReadsSpark -I hdfs://local/print_reads.sorted.bam -O output.bam -- --sparkRunner SPARK --sparkMaster yarn-client; ```. ```; java.lang.IllegalArgumentException: java.net.UnknownHostException: local; at org.apache.hadoop.security.SecurityUtil.buildTokenService(SecurityUtil.java:374); at org.apache.hadoop.hdfs.NameNodeProxies.createNonHAProxy(NameNodeProxies.java:310); at org.apache.hadoop.hdfs.NameNodeProxies.createProxy(NameNodeProxies.java:176); at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:707); at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:650); at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:148); at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2643); at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:93); at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2680); at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2662); at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:379); at org.apache.hadoop.fs.Path.getFileSystem(Path.java:296); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:183); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReads(GATKSparkTool.java:337); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:317); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:308); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:146); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(Co",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1257#issuecomment-175789890:956,Cache,Cache,956,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1257#issuecomment-175789890,1,['Cache'],['Cache']
Performance,mtools.BAMRecordCodec.decode(BAMRecordCodec.java:209); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.getNextRecord(BAMFileReader.java:829); 	at htsjdk.samtools.BAMFileReader$BAMFileIndexIterator.getNextRecord(BAMFileReader.java:981); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.advance(BAMFileReader.java:803); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.java:797); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.java:765); 	at htsjdk.samtools.BAMFileReader$BAMQueryFilteringIterator.advance(BAMFileReader.java:1034); 	at htsjdk.samtools.BAMFileReader$BAMQueryFilteringIterator.next(BAMFileReader.java:1024); 	at htsjdk.samtools.BAMFileReader$BAMQueryFilteringIterator.next(BAMFileReader.java:988); 	at htsjdk.samtools.SamReader$AssertingIterator.next(SamReader.java:576); 	at htsjdk.samtools.SamReader$AssertingIterator.next(SamReader.java:548); 	at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.loadNextRecord(SamReaderQueryingIterator.java:114); 	at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.next(SamReaderQueryingIterator.java:151); 	at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.next(SamReaderQueryingIterator.java:29); 	at org.broadinstitute.hellbender.utils.iterators.SAMRecordToReadIterator.next(SAMRecordToReadIterator.java:29); 	at org.broadinstitute.hellbender.utils.iterators.SAMRecordToReadIterator.next(SAMRecordToReadIterator.java:15); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); 	at java.util.stream.AbstractP,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317028955:5494,load,loadNextRecord,5494,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317028955,5,['load'],['loadNextRecord']
Performance,"mutect2 stopped at chromosome 1 . $ java -jar -Xmx12g gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar Mutect2 -R Homo_sapiens_assembly19.fasta -I Specimen_SNCR.10_1.bam -tumor Specimen_10_1 -O mutect2/10_1.vcf&; [1] 40657; $ 09:49:03.454 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/Tools/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 09:49:05.899 INFO Mutect2 - ------------------------------------------------------------; 09:49:05.899 INFO Mutect2 - The Genome Analysis Toolkit (GATK) v4.1.0.0; 09:49:05.900 INFO Mutect2 - For support and documentation go to https://software.broadinstitute.org/gatk/. 09:49:05.900 INFO Mutect2 - Java runtime: IBM J9 VM v8.0.5.25 - pxa6480sr5fp25-20181030_01(SR5 FP25); 09:49:05.901 INFO Mutect2 - Start Date/Time: March 7, 2019 9:49:03 AM EST; 09:49:05.901 INFO Mutect2 - ------------------------------------------------------------; 09:49:05.901 INFO Mutect2 - ------------------------------------------------------------; 09:49:05.901 INFO Mutect2 - HTSJDK Version: 2.18.2; 09:49:05.901 INFO Mutect2 - Picard Version: 2.18.25; 09:49:05.902 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 09:49:05.902 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 09:49:05.902 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 09:49:05.902 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 09:49:05.902 INFO Mutect2 - Deflater: IntelDeflater; 09:49:05.902 INFO Mutect2 - Inflater: IntelInflater; 09:49:05.902 INFO Mutect2 - GCS max retries/reopens: 20; 09:49:05.902 INFO Mutect2 - Requester pays: disabled; 09:49:05.902 INFO Mutect2 - Initializing engine; 09:49:06.887 INFO Mutect2 - Done initializing engine; 09:49:06.935 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/Tools/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 09:49:06.937 INFO PairHMM - OpenMP multi-threaded A",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5543#issuecomment-470579844:261,Load,Loading,261,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5543#issuecomment-470579844,1,['Load'],['Loading']
Performance,"n ""==""?; if len(args) is 0 or (len(args) is 1 and (args[0] == ""--help"" or args[0] == ""-h"")):; /home/genouest/uni_limoges_fr/jpollet/.conda/envs/myd88/bin/gatk:80: SyntaxWarning: ""is"" with a literal. Did you mean ""==""?; if len(args) is 0 or (len(args) is 1 and (args[0] == ""--help"" or args[0] == ""-h"")):; /home/genouest/uni_limoges_fr/jpollet/.conda/envs/myd88/bin/gatk:117: SyntaxWarning: ""is"" with a literal. Did you mean ""==""?; if len(args) is 1 and args[0] == ""--list"":; /home/genouest/uni_limoges_fr/jpollet/.conda/envs/myd88/bin/gatk:308: SyntaxWarning: ""is"" with a literal. Did you mean ""==""?; if call([""gsutil"", ""-q"", ""stat"", gcsjar]) is 0:; /home/genouest/uni_limoges_fr/jpollet/.conda/envs/myd88/bin/gatk:312: SyntaxWarning: ""is"" with a literal. Did you mean ""==""?; if call([""gsutil"", ""cp"", jar, gcsjar]) is 0:; /home/genouest/uni_limoges_fr/jpollet/.conda/envs/myd88/bin/gatk:467: SyntaxWarning: ""is not"" with a literal. Did you mean ""!=""?; if not len(properties) is 0:; /home/genouest/uni_limoges_fr/jpollet/.conda/envs/myd88/bin/gatk:471: SyntaxWarning: ""is not"" with a literal. Did you mean ""!=""?; if not len(filesToAdd) is 0:; Using GATK jar /home/genouest/uni_limoges_fr/jpollet/.conda/envs/myd88/share/gatk4-4.1.4.0-1/gatk-package-4.1.4.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/genouest/uni_limoges_fr/jpollet/.conda/envs/myd88/share/gatk4-4.1.4.0-1/gatk-package-4.1.4.0-local.jar Mutect2 -R /omaha-beach/jpollet/MYD88/data/ref/BALBcJ.fasta -I /omaha-beach/jpollet/MYD88/result/valide_3060_R1vsBALBcJ.sorted.md.bam -O /omaha-beach/jpollet/MYD88/result/valide_3060_R1vsBALBcJ.sortedunf.vcf; 15:47:36.551 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/genouest/uni_limoges_fr/jpollet/.conda/envs/myd88/share/gatk4-4.1.4.0-1/gatk-package-4.1.4.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6271#issuecomment-559553558:7651,Load,Loading,7651,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6271#issuecomment-559553558,1,['Load'],['Loading']
Performance,"n<? extends A>, PCollection<B>> lift(SerializableFunction<A, B> f){; return ParDo.of(new DoFn<A, B>() {; @Override; public void processElement(ProcessContext c) throws Exception {; c.output(f.apply(c.element()));; }; });; }; ```. example usage:. ```; @Test; public void testApplyToString(){; Pipeline p = GATKTestPipeline.create();; PCollection<Integer> pints = p.apply(Create.of(Arrays.asList(1, 2, 3)));. PCollection<String> presults = DataflowUtils.apply( pints, Object::toString).setCoder(StringUtf8Coder.of());. DataflowAssert.that(presults).containsInAnyOrder(""1"",""2"",""3"");; p.run();; }; ```. note the `setCoder` call, if you don't include that you get. ```; SLF4J: Class path contains multiple SLF4J bindings.; SLF4J: Found binding in [jar:file:/Users/louisb/.gradle/caches/modules-2/files-2.1/org.slf4j/slf4j-jdk14/1.7.7/25d160723ea37a6cb84e87cd70773ff02997e857/slf4j-jdk14-1.7.7.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: Found binding in [jar:file:/Users/louisb/.gradle/caches/modules-2/files-2.1/org.slf4j/slf4j-log4j12/1.7.10/b3eeae7d1765f988a1f45ea81517191315c69c9e/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: Found binding in [jar:file:/Users/louisb/.gradle/caches/modules-2/files-2.1/org.slf4j/slf4j-log4j12/1.7.5/6edffc576ce104ec769d954618764f39f0f0f10d/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.; SLF4J: Actual binding is of type [org.slf4j.impl.JDK14LoggerFactory]; java.lang.IllegalStateException: Unable to infer a default Coder for AnonymousParDo.out [PCollection]; either correct the root cause below or use setCoder() to specify one explicitly. ; at com.google.cloud.dataflow.sdk.values.TypedPValue.getCoder(TypedPValue.java:48); at com.google.cloud.dataflow.sdk.values.PCollection.getCoder(PCollection.java:137); at com.google.cloud.dataflow.sdk.transforms.windowing.Window$Bound.getDefaultOutputCoder(Window.java:286); at com.go",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/658#issuecomment-122314248:1322,cache,caches,1322,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/658#issuecomment-122314248,1,['cache'],['caches']
Performance,"nConcordanceEvaluation.hapMap"": ""gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.haplotype_database.txt"",; ""GatkDragenConcordanceEvaluation.include_in_fe_analysis"": ""test_output:FunctionalEquivalenceTest.out_include_in_fe_analysis"",; ""GatkDragenConcordanceEvaluation.refDict"": ""gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.dict"",; ""GatkDragenConcordanceEvaluation.refIndex"": ""gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.fai"",; ""GatkDragenConcordanceEvaluation.reference"": ""gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta"",; ""GatkDragenConcordanceEvaluation.referenceVersion"": ""hg38"",; ""GatkDragenConcordanceEvaluation.replicate_no"": ""test_output:FunctionalEquivalenceTest.out_replicate_no"",; ""GatkDragenConcordanceEvaluation.sample_id"": ""test_output:FunctionalEquivalenceTest.out_sample_id"",; ""GatkDragenConcordanceEvaluation.stratIntervals"": [; ""gs://broad-dsde-methods-ckachulis/benchmarking/stratifiers/LCR_Hg38.interval_list"",; ""gs://broad-dsde-methods-ckachulis/benchmarking/stratifiers/HCR_hg38.bed""; ],; ""GatkDragenConcordanceEvaluation.stratLabels"": [; ""LCR"",; ""HCR""; ],; ""GatkDragenConcordanceEvaluation.truth_vcf"": ""test_output:FunctionalEquivalenceTest.out_truth_vcf"",; ""GatkDragenConcordanceEvaluation.truth_vcf_index"": ""test_output:FunctionalEquivalenceTest.out_truth_vcf_index""; },; ""test_cromwell_job_id"": ""bc61debd-03be-4a13-a7d6-33479e047d08"",; ""eval_cromwell_job_id"": ""2fd3d6b8-6d82-4664-8217-3872d44f89ca"",; ""created_at"": ""2021-01-14T19:14:37.025719"",; ""created_by"": null,; ""finished_at"": ""2021-01-15T02:47:24.222"",; ""results"": {; ""FE plots"": ""gs://dsde-methods-carrot-prod-cromwell/GatkDragenConcordanceEvaluation/2fd3d6b8-6d82-4664-8217-3872d44f89ca/call-MergeFE/cacheCopy/plots.png"",; ""ROC plots"": ""gs://dsde-methods-carrot-prod-cromwell/GatkDragenConcordanceEvaluation/2fd3d6b8-6d82-4664-8217-3872d44f89ca/call-MergeROC/cacheCopy/plots.png""; }; } ; </pre> </details>",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7039#issuecomment-760609431:7581,cache,cacheCopy,7581,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7039#issuecomment-760609431,2,['cache'],['cacheCopy']
Performance,nJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Fubm90YXRvci9SYXdHdENvdW50LmphdmE=) | `63.415% <0.000%> (-12.195%)` | :arrow_down: |; | [...er/utils/MergeAnnotatedRegionsIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/8163?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL3V0aWxzL01lcmdlQW5ub3RhdGVkUmVnaW9uc0ludGVncmF0aW9uVGVzdC5qYXZh) | `93.333% <0.000%> (-6.667%)` | :arrow_down: |; | [...hellbender/tools/walkers/sv/CollectSVEvidence.java](https://codecov.io/gh/broadinstitute/gatk/pull/8163?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3N2L0NvbGxlY3RTVkV2aWRlbmNlLmphdmE=) | `74.888% <0.000%> (-4.990%)` | :arrow_down: |; | [.../scalable/data/LabeledVariantAnnotationsDatum.java](https://codecov.io/gh/broadinstitute/gatk/pull/8163?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvZGF0YS9MYWJlbGVkVmFyaWFudEFubm90YXRpb25zRGF0dW0uamF2YQ==) | `68.421% <0.000%> (-3.801%)` | :arrow_down: |; | [...ls/genomicsdb/GenomicsDBImportIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/8163?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9nZW5vbWljc2RiL0dlbm9taWNzREJJbXBvcnRJbnRlZ3JhdGlvblRlc3QuamF2YQ==) | `84.746% <0.000%> (-3.762%)` | :arrow_down: |; | [...vqsr/scalable/LabeledVariantAnnotationsWalker.java](https://codecov.io/gh/broadinstitute/gatk/pull/8163?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_co,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8163#issuecomment-1387550473:3901,scalab,scalable,3901,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8163#issuecomment-1387550473,1,['scalab'],['scalable']
Performance,nager/ComputableGraphStructure.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9Db21wdXRhYmxlR3JhcGhTdHJ1Y3R1cmUuamF2YQ==) | `100% <100%> (+26.994%)` | `63 <62> (+24)` | :arrow_up: |; | [...ragemodel/cachemanager/ComputableNodeFunction.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9Db21wdXRhYmxlTm9kZUZ1bmN0aW9uLmphdmE=) | `100% <100%> (+66.667%)` | `4 <1> (+2)` | :arrow_up: |; | [.../coveragemodel/cachemanager/DuplicableNDArray.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9EdXBsaWNhYmxlTkRBcnJheS5qYXZh) | `81.818% <100%> (+38.068%)` | `6 <2> (+2)` | :arrow_up: |; | [...s/coveragemodel/cachemanager/DuplicableNumber.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9EdXBsaWNhYmxlTnVtYmVyLmphdmE=) | `80% <100%> (+80%)` | `5 <2> (+5)` | :arrow_up: |; | [...coveragemodel/cachemanager/PrimitiveCacheNode.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9QcmltaXRpdmVDYWNoZU5vZGUuamF2YQ==) | `83.333% <71.429%> (+30.702%)` | `10 <7> (+3)` | :arrow_up: |; | [...er/tools/coveragemodel/cachemanager/CacheNode.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9DYWNoZU5vZGUuamF2YQ==) | `80.645% <76.923%> (+30.645%)` | `9 <8> (+4)` | :arrow_up: |; | [...overagemodel/cachemanager/ComputableCacheNode.java](h,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3035#issuecomment-306412418:2253,cache,cachemanager,2253,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3035#issuecomment-306412418,1,['cache'],['cachemanager']
Performance,"nc_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 --num-executors 20 --executor-cores 6 --executor-memory 6g /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/gatk-4.1.0.0/gatk-package-4.1.0.0-spark.jar CountReadsSpark --input /project/casa/gcad/adsp.cc/cram/A-ADC-AD010072-BL-NCR-11AD44210.hg38.realign.bqsr.cram --reference ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --spark-master yarn; 23:10:10.737 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 23:10:10.965 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/gatk-4.1.0.0/gatk-package-4.1.0.0-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 23:10:12.679 INFO CountReadsSpark - ------------------------------------------------------------; 23:10:12.680 INFO CountReadsSpark - The Genome Analysis Toolkit (GATK) v4.1.0.0; 23:10:12.680 INFO CountReadsSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 23:10:12.680 INFO CountReadsSpark - Executing as farrell@scc-hadoop.bu.edu on Linux v2.6.32-754.6.3.el6.x86_64 amd64; 23:10:12.681 INFO CountReadsSpark - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 23:10:12.681 INFO CountReadsSpark - Start Date/Time: February 5, 2019 11:10:10 PM EST; 23:10:12.681 INFO CountReadsSpark - ------------------------------------------------------------; 23:10:12.681 INFO CountReadsSpark - ------------------------------------------------------------; 23:10:12.683 INFO Count",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-460895912:3356,Load,Loading,3356,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-460895912,1,['Load'],['Loading']
Performance,"ncher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.varia.NullAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.varia.NullAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""NullAppender"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.varia.NullAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.varia.NullAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instan",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312229998:5607,load,loaded,5607,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312229998,1,['load'],['loaded']
Performance,nds 11 --gvcf-gq-bands 12 --gvcf-gq-bands 13 --gvcf-gq-bands 14 --gvcf-gq-bands 15 --gvcf-gq-bands 16 --gvcf-gq-bands 17 --gvc; f-gq-bands 18 --gvcf-gq-bands 19 --gvcf-gq-bands 20 --gvcf-gq-bands 21 --gvcf-gq-bands 22 --gvcf-gq-bands 23 --gvcf-gq-bands 24 --gvcf-gq-bands 25 --gvcf-gq-bands 26 --gvcf-gq-bands 27 --g; vcf-gq-bands 28 --gvcf-gq-bands 29 --gvcf-gq-bands 30 --gvcf-gq-bands 31 --gvcf-gq-bands 32 --gvcf-gq-bands 33 --gvcf-gq-bands 34 --gvcf-gq-bands 35 --gvcf-gq-bands 36 --gvcf-gq-bands 37 -; -gvcf-gq-bands 38 --gvcf-gq-bands 39 --gvcf-gq-bands 40 --gvcf-gq-bands 41 --gvcf-gq-bands 42 --gvcf-gq-bands 43 --gvcf-gq-bands 44 --gvcf-gq-bands 45 --gvcf-gq-bands 46 --gvcf-gq-bands 47; --gvcf-gq-bands 48 --gvcf-gq-bands 49 --gvcf-gq-bands 50 --gvcf-gq-bands 51 --gvcf-gq-bands 52 --gvcf-gq-bands 53 --gvcf-gq-bands 54 --gvcf-gq-bands 55 --gvcf-gq-bands 56 --gvcf-gq-bands ; 57 --gvcf-gq-bands 58 --gvcf-gq-bands 59 --gvcf-gq-bands 60 --gvcf-gq-bands 70 --gvcf-gq-bands 80 --gvcf-gq-bands 90 --gvcf-gq-bands 99 --floor-blocks false --indel-size-to-eliminate-in-re; f-model 10 --disable-optimizations false --dragen-mode false --flow-mode NONE --apply-bqd false --apply-frd false --disable-spanning-event-genotyping false --transform-dragen-mapping-quali; ty false --mapping-quality-threshold-for-genotyping 20 --max-effective-depth-adjustment-for-frd 0 --just-determine-active-regions false --dont-genotype false --do-not-run-physical-phasing ; false --do-not-correct-overlapping-quality false --use-filtered-reads-for-annotations false --use-flow-aligner-for-stepwise-hc-filtering false --adaptive-pruning false --do-not-recover-dan; gling-branches false --recover-dangling-heads false --kmer-size 10 --kmer-size 25 --dont-increase-kmer-sizes-for-cycles false --allow-non-unique-kmers-in-ref false --num-pruning-samples 1 ; --min-dangling-branch-length 4 --recover-all-dangling-branches false --max-num-haplotypes-in-population 128 --min-pruning 2 --adaptive-pruning-initial-error-rate 0.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8574#issuecomment-1793390789:4341,optimiz,optimizations,4341,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8574#issuecomment-1793390789,1,['optimiz'],['optimizations']
Performance,"ne labelled; ""shadowJar"", ie. gatk-all-4.pre-alpha-175-*-SNAPSHOT-shadowJar.jar. This is a proper fatJar with all of the dependencies packaged up together. The installDist target produces a different jar file that doesn't shadow; all of the dependencies, and so I think it needs to be run with the 'gatk'; wrapper script that sets the classpath in build/scripts. It is a little confusing having all three jar targets, and maybe you guys; don't want to support that, but I was hoping to make a fat jar with all the; dependencies bundled, and the previous 'fatJar' target wasn't working; properly. My PR basically just replaced that old 'fatJar' target with; 'shadowJar'. On Fri, Dec 4, 2015 at 11:32 AM, Tom White notifications@github.com wrote:. > @cwhelan https://github.com/cwhelan I've having problems with the; > non-Spark JAR though:; > ; > $ gradle clean installDist; > $ java -jar build/libs/gatk-4.pre-alpha-*-SNAPSHOT.jar; > Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/logging/log4j/LogManager; > at org.broadinstitute.hellbender.cmdline.ClassFinder.<clinit>(ClassFinder.java:29); > at org.broadinstitute.hellbender.Main.extractCommandLineProgram(Main.java:108); > at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); > at org.broadinstitute.hellbender.Main.main(Main.java:86); > Caused by: java.lang.ClassNotFoundException: org.apache.logging.log4j.LogManager; > at java.net.URLClassLoader$1.run(URLClassLoader.java:372); > at java.net.URLClassLoader$1.run(URLClassLoader.java:361); > at java.security.AccessController.doPrivileged(Native Method); > at java.net.URLClassLoader.findClass(URLClassLoader.java:360); > at java.lang.ClassLoader.loadClass(ClassLoader.java:424); > at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308); > at java.lang.ClassLoader.loadClass(ClassLoader.java:357); > ... 4 more; > ; > —; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/gatk/pull/1213#issuecomment-162013287.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1213#issuecomment-162017907:1749,load,loadClass,1749,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1213#issuecomment-162017907,3,['load'],['loadClass']
Performance,"ne.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:94); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:838); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:115); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:131); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:152); 	at org.broadinstitute.hellbender.Main.main(Main.java:230); Caused by: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: java.lang.IllegalArgumentException: Position should be non-negative, is %d [-134217728]; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.getBuf(SeekableByteChannelPrefetcher.java:136); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.fetch(SeekableByteChannelPrefetcher.java:255); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:300); 	... 44 more; Caused by: com.google.cloud.storage.StorageException: java.lang.IllegalArgumentException: Position should be non-negative, is %d [-134217728]; 	at com.google.cloud.storage.StorageException.translateAndThrow(StorageException.java:71); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:139); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:113); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(See",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317549881:7693,concurren,concurrent,7693,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317549881,1,['concurren'],['concurrent']
Performance,"ne.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:94); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:838); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:115); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:131); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:152); 	at org.broadinstitute.hellbender.Main.main(Main.java:230); Caused by: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: java.lang.IllegalArgumentException: Position should be non-negative, is %d [-830472192]; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.getBuf(SeekableByteChannelPrefetcher.java:136); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.fetch(SeekableByteChannelPrefetcher.java:255); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:300); 	... 44 more; Caused by: com.google.cloud.storage.StorageException: java.lang.IllegalArgumentException: Position should be non-negative, is %d [-830472192]; 	at com.google.cloud.storage.StorageException.translateAndThrow(StorageException.java:71); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:139); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:113); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(See",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317442564:6881,concurren,concurrent,6881,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317442564,1,['concurren'],['concurrent']
Performance,"needs a way to prevent data races. Otherwise, multiple concurrent tools write the same index file and bad things happen.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/365#issuecomment-93459762:55,concurren,concurrent,55,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/365#issuecomment-93459762,1,['concurren'],['concurrent']
Performance,nfigFactory - gatk_stacktrace_on_user_exception = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 17:39:19.245 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 17:39:19.246 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 17:39:19.246 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 17:39:19.246 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 17:39:19.246 INFO PathSeqPipelineSpark - Initializing engine; 17:39:19.246 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 18/04/24 17:39:19 INFO SparkContext: Running Spark version 2.2.0; 18/04/24 17:39:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/04/24 17:39:19 INFO SparkContext: Submitted application: PathSeqPipelineSpark; 18/04/24 17:39:20 INFO SecurityManager: Changing view acls to: username; 18/04/24 17:39:20 INFO SecurityManager: Changing modify acls to: username; 18/04/24 17:39:20 INFO SecurityManager: Changing view acls groups to:; 18/04/24 17:39:20 INFO SecurityManager: Changing modify acls groups to:; 18/04/24 17:39:20 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(username); groups with view permissions: Set(); users with modify permissions: Set(username); groups with modify permissions: Set(); 18/04/24 17:39:20 INFO Utils: Successfully started service 'sparkDriver' on port 46576.; 18/04/24 17:39:20 INFO SparkEnv: Registering MapOutputTracker; 18/04/24 17:39:20 INFO SparkEnv: Registering BlockManagerMaster; 18/04/24 17:39:20 INFO BlockManagerMasterEndpo,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:7078,load,load,7078,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,1,['load'],['load']
Performance,"ng. commit dd2dd503a92e6fbb5a49be6a88d2e813eb8bf85b; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Tue Dec 12 15:14:08 2023 -0500. update gCNV expected results, generated on WSL Ubuntu 20.04.2. commit 27d76e8f22d61df90eeb337e033ae128ce07ab90; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Tue Dec 12 14:53:04 2023 -0500. update python env integration tests. commit 348df9192235f7d1ea941d0b31e5c96acc0d6491; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Tue Dec 12 10:59:23 2023 -0500. disable CNN tests, add deprecation message. commit ed59372b4be226785af1d3fb1b1a39a9ad3b4f6a; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Tue Dec 12 09:55:24 2023 -0500. clean up rebase. commit 18e530db26f803ee46a0006843cb36d4ed4194b4; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Fri Dec 8 11:31:46 2023 -0500. postprocess fixed. commit f510c2e9f10d7066c15f1835669d676964b8a4cb; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Fri Dec 8 10:13:01 2023 -0500. fix deprecated np.int in optimizer. commit 939a032f356f2f8f67b5aae426fc427d1d1ea6c4; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Fri Dec 8 09:50:57 2023 -0500. remove unnecessary seeding in cohort denoising script. commit cf82ea5c99250f1784f8b1a9279e7dbb8841fa89; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Fri Dec 8 09:38:08 2023 -0500. add back setup.py files. commit 8348f546de6b3d32e1f02f6851730226c0dbffc9; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Fri Dec 8 09:37:09 2023 -0500. update pymc version in init. commit 850d60ef95b6126c05af9cd7c2cb528a306e1224; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Fri Dec 8 09:32:42 2023 -0500. added pip editable docs. commit 9c51b311442b0796ab1224213e83290caea0f93f; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Fri Dec 8 09:24:50 2023 -0500. whitespace. commit d9b180385168fdd1ef55cee8a1069fc1f7928f38; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Fri Dec 8 09:24:10 2023 -0500. update setup_gcnvkernel.py and pin pytensor. commit 7ccbd6d",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8561#issuecomment-1854434322:2086,optimiz,optimizer,2086,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8561#issuecomment-1854434322,1,['optimiz'],['optimizer']
Performance,ng.IllegalArgumentException: Unknown Java version: 11; 	at net.bytebuddy.ClassFileVersion.ofJavaVersion(ClassFileVersion.java:135); 	at net.bytebuddy.ClassFileVersion$VersionLocator$ForJava9CapableVm.locate(ClassFileVersion.java:357); 	at net.bytebuddy.ClassFileVersion.ofThisVm(ClassFileVersion.java:147); 	at net.bytebuddy.dynamic.loading.ClassInjector$UsingReflection$Dispatcher$CreationAction.run(ClassInjector.java:301); 	at net.bytebuddy.dynamic.loading.ClassInjector$UsingReflection$Dispatcher$CreationAction.run(ClassInjector.java:290); 	at java.base/java.security.AccessController.doPrivileged(Native Method); 	at net.bytebuddy.dynamic.loading.ClassInjector$UsingReflection.<clinit>(ClassInjector.java:70); 	at net.bytebuddy.dynamic.loading.ClassLoadingStrategy$Default$InjectionDispatcher.load(ClassLoadingStrategy.java:184); 	at net.bytebuddy.dynamic.TypeResolutionStrategy$Passive.initialize(TypeResolutionStrategy.java:79); 	at net.bytebuddy.dynamic.DynamicType$Default$Unloaded.load(DynamicType.java:4456); 	at org.mockito.internal.creation.bytebuddy.SubclassBytecodeGenerator.mockClass(SubclassBytecodeGenerator.java:115); 	at org.mockito.internal.creation.bytebuddy.TypeCachingBytecodeGenerator$1.call(TypeCachingBytecodeGenerator.java:37); 	at org.mockito.internal.creation.bytebuddy.TypeCachingBytecodeGenerator$1.call(TypeCachingBytecodeGenerator.java:34); 	at net.bytebuddy.TypeCache.findOrInsert(TypeCache.java:138); 	at net.bytebuddy.TypeCache$WithInlineExpunction.findOrInsert(TypeCache.java:346); 	at net.bytebuddy.TypeCache.findOrInsert(TypeCache.java:161); 	at net.bytebuddy.TypeCache$WithInlineExpunction.findOrInsert(TypeCache.java:355); 	at org.mockito.internal.creation.bytebuddy.TypeCachingBytecodeGenerator.mockClass(TypeCachingBytecodeGenerator.java:32); 	at org.mockito.internal.creation.bytebuddy.SubclassByteBuddyMockMaker.createMockType(SubclassByteBuddyMockMaker.java:71); 	at org.mockito.internal.creation.bytebuddy.SubclassByteBuddyMockMaker.createMock(Subclass,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6119#issuecomment-532377836:1152,load,load,1152,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6119#issuecomment-532377836,1,['load'],['load']
Performance,"ngine; [January 6, 2021 4:26:39 PM CST] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 0.08 minutes.; Runtime.totalMemory()=2303197184; ***********************************************************************. A USER ERROR has occurred: Couldn't create GenomicsDBFeatureReader. ***********************************************************************; Set the system property GATK_STACKTRACE_ON_USER_EXCEPTION (--java-options '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true') to print the stack trace.; [ccastane9@andersserver-01 GenomicsDB]$ bash *_genotype.3.sh; Using GATK jar /data1/_software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Xmx16g -jar /data1/_software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar GenotypeGVCFs --genomicsdb-shared-posixfs-optimizations --reference /data1/EquCab/_ECA30/Equus_caballus.EquCab3.0.dna_sm.toplevel.fa/ -V gendb://ECA3_GenomicsDB_260/3 -O ECA3_GenomicsDB_260.3.g.vcf.gz; 16:27:53.573 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data1/_software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 06, 2021 4:27:54 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 16:27:54.132 INFO GenotypeGVCFs - ------------------------------------------------------------; 16:27:54.133 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.1.8.1; 16:27:54.133 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 16:27:54.143 INFO GenotypeGVCFs - Executing as ccastane9@andersserver-01.cvm.tamu.edu on Linux v3.10.0-1127.19.1.el7.x86_64 amd64; 16:27:54.143 INFO GenotypeGVCFs - Java runtime: OpenJ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-755760402:4125,optimiz,optimizations,4125,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-755760402,1,['optimiz'],['optimizations']
Performance,"ngs_gatk_cnv/4.1.6.0/lib/python3.6/site-packages/theano/gof/link.py"", line 325, in raise_with_op; reraise(exc_type, exc_value, exc_trace); File ""/ngc/projects/gm/data/resources/envs/conda/ngs_gatk_cnv/4.1.6.0/lib/python3.6/site-packages/six.py"", line 692, in reraise; raise value.with_traceback(tb); File ""/ngc/projects/gm/data/resources/envs/conda/ngs_gatk_cnv/4.1.6.0/lib/python3.6/site-packages/theano/compile/function_module.py"", line 903, in __call__; self.fn() if output_subset is None else\; File ""/ngc/projects/gm/data/resources/envs/conda/ngs_gatk_cnv/4.1.6.0/lib/python3.6/site-packages/theano/scan_module/scan_op.py"", line 963, in rval; r = p(n, [x[0] for x in i], o); File ""/ngc/projects/gm/data/resources/envs/conda/ngs_gatk_cnv/4.1.6.0/lib/python3.6/site-packages/theano/scan_module/scan_op.py"", line 952, in p; self, node); File ""scan_perform.pyx"", line 215, in theano.scan_module.scan_perform.perform; NotImplementedError: We didn't implemented yet the case where scan do 0 iteration; Apply node that caused the error: forall_inplace,cpu,scan_fn}(Elemwise{minimum,no_inplace}.0, InplaceDimShuffle{0,2,1}.0, Subtensor{int64:int64:int64}.0, IncSubtensor{InplaceSet;:int64:}.0, Shape_i{0}.0); Toposort index: 95; Inputs types: [TensorType(int64, scalar), TensorType(float64, 3D), TensorType(float64, matrix), TensorType(float64, matrix), TensorType(int64, scalar)]; Inputs shapes: [(), (0, 6, 6), (0, 6), (2, 6), ()]; Inputs strides: [(), (288, 8, 48), (48, 8), (48, 8), ()]; Inputs values: [array(0), array([], shape=(0, 6, 6), dtype=float64), array([], shape=(0, 6), dtype=float64), 'not shown', array(6)]; Inputs type_num: [7, 12, 12, 12, 7]; Outputs clients: [[Subtensor{int64:int64:int8}(forall_inplace,cpu,scan_fn}.0, ScalarFromTensor.0, ScalarFromTensor.0, Constant{1})]]. at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5852#issuecomment-613371282:3137,perform,perform,3137,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5852#issuecomment-613371282,1,['perform'],['perform']
Performance,"nning, top indicates is using about  240g (after importing the 65 batches).; ```; PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM  TIME+ COMMAND; 21698 farrell   20   0      443.7g 240.3g   1416 S  86.7 95.5   7398:14 java; ```. ```; #!/bin/bash -l; #$ -l mem_total=251; #$ -P casa; #$ -pe omp 32; #$ -l h_rt=240:00:00; module load miniconda/4.9.2; module load gatk/[4.2.6.1](http://4.2.6.1/); conda activate /share/pkg.7/gatk/[4.2.6.1/install/gatk-4.2.6.1](http://4.2.6.1/install/gatk-4.2.6.1). CHR=$1; DB=""genomicsDB.rb.chr$CHR""; rm -rf $DB; # mkdir -p $DB; # mkdir tmp; echo ""Processing chr$CHR""; echo ""NSLOTS: $NSLOTS""; # head sample_map.chr$CHR.reblock.list; head sample_map.chr$CHR; wc   sample_map.chr$CHR; gatk --java-options ""-Xmx150g -Xms16g"" \;        GenomicsDBImport \;        --sample-name-map sample_map.chr$CHR \;        --genomicsdb-workspace-path $DB \;        --genomicsdb-shared-posixfs-optimizations True\;        --tmp-dir tmp \;        --L chr$CHR\;        --batch-size 50 \;        --bypass-feature-reader\;        --reader-threads 5\;        --merge-input-intervals \;        --overwrite-existing-genomicsdb-workspace\;        --consolidate. ```; End of log on chr3. ```; 07:19:44.855 INFO  GenomicsDBImport - Done importing batch 38/65; 08:05:11.651 INFO  GenomicsDBImport - Done importing batch 39/65; 08:49:12.112 INFO  GenomicsDBImport - Done importing batch 40/65; 09:32:39.526 INFO  GenomicsDBImport - Done importing batch 41/65; 10:23:36.849 INFO  GenomicsDBImport - Done importing batch 42/65; 11:24:50.566 INFO  GenomicsDBImport - Done importing batch 43/65; 12:17:11.236 INFO  GenomicsDBImport - Done importing batch 44/65; 13:11:10.869 INFO  GenomicsDBImport - Done importing batch 45/65; 13:56:22.927 INFO  GenomicsDBImport - Done importing batch 46/65; 14:45:02.333 INFO  GenomicsDBImport - Done importing batch 47/65; 15:35:20.713 INFO  GenomicsDBImport - Done importing batch 48/65; 16:32:30.162 INFO  GenomicsDBImport - Done importing batch 49/65; 17:1",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1246785232:1380,optimiz,optimizations,1380,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1246785232,1,['optimiz'],['optimizations']
Performance,"nomicsDB/GenomicsDB/releases/download/v1.4.3/consolidate_genomicsdb_array) for consolidation. This executable will consolidate a given array in a GenomicsDB workspace, it has been instrumented to output memory stats to help tune the segment size. Note that the executable is for Centos 7, if you find any unresolved shared library dependencies during usage, please let me know and I will work on getting another one to you. For usage from a bash shell:; ```; ~/GenomicsDB: ./consolidate_genomicsdb_array; Usage: consolidate_genomicsdb_array [options]; where options include:; 	 --help, -h Print a usage message summarizing options available and exit; 	 --workspace=<GenomicsDB workspace URI>, -w <GenomicsDB workspace URI>; 		 Specify path to GenomicsDB workspace; 	 --array-name=<Array Name>, -a <Array Name>; 		 Specify name of array that requires consolidation; 	 --segment-size=<Segment Size>, -z <Segment Size>; 		 Optional, default is 10M. Specify a buffer size for consolidation; 	 --shared-posixfs-optimizations, -p; 		 Optional, default is false. If specified, the array folder is not locked for read/write and file handles are kept open until a final close for write; 	 --version Print version and exit; ```. ```; ~/GenomicsDB.: ./consolidate_genomicsdb_array -w /Users/xxx/WGS.gdb/ -a ""1\$1\$249250621"" -z 1048576 -p; 21:09:47.100 info consolidate_genomicsdb_array - pid=30881 tid=30881 Starting consolidation of 1$1$249250621 in ws; Using buffer_size=1048576 for consolidation; 21:9:47 Memory stats(pages) beginning consolidation size=45821 resident=18998 share=1824 text=3530 lib=0 data=16810 dt=0; 21:9:47 Memory stats(pages) after alloc for attribute=END size=45821 resident=19009 share=1835 text=3530 lib=0 data=16810 dt=0; 21:9:48 Memory stats(pages) after alloc for attribute=REF size=46788 resident=19743 share=1889 text=3530 lib=0 data=17773 dt=0; 21:9:48 Memory stats(pages) after alloc for attribute=ALT size=47143 resident=20124 share=1889 text=3530 lib=0 data=18129 dt=0; 21:9",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1057680354:1175,optimiz,optimizations,1175,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1057680354,1,['optimiz'],['optimizations']
Performance,nomicsDBImport - Done importing batch 53/65; 20:18:42.274 INFO  GenomicsDBImport - Done importing batch 54/65; 21:01:51.304 INFO  GenomicsDBImport - Done importing batch 55/65; 21:36:00.458 INFO  GenomicsDBImport - Done importing batch 56/65; 22:08:38.587 INFO  GenomicsDBImport - Done importing batch 57/65; 22:40:44.082 INFO  GenomicsDBImport - Done importing batch 58/65; 23:14:11.202 INFO  GenomicsDBImport - Done importing batch 59/65; 23:48:23.805 INFO  GenomicsDBImport - Done importing batch 60/65; 00:20:35.869 INFO  GenomicsDBImport - Done importing batch 61/65; 00:51:47.408 INFO  GenomicsDBImport - Done importing batch 62/65; 01:25:23.587 INFO  GenomicsDBImport - Done importing batch 63/65; 01:59:03.103 INFO  GenomicsDBImport - Done importing batch 64/65; Using GATK jar /share/pkg.7/gatk/[4.2.6.1/install/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar](http://4.2.6.1/install/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar) defined in environment variable GATK_LOCAL_JAR; Running:;     java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx150g -Xms16g -jar /share/pkg.7/gatk/[4.2.6.1/install/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar](http://4.2.6.1/install/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar) GenomicsDBImport --sample-name-map sample_map.chr3 --genomicsdb-workspace-path genomicsDB.rb.chr3 --genomicsdb-shared-posixfs-optimizations True --tmp-dir tmp --L chr3 --batch-size 50 --bypass-feature-reader --reader-threads 5 --merge-input-intervals --overwrite-existing-genomicsdb-workspace --consolidate; [farrell@scc-hadoop genomicsdb]$ ls genomicsDB.rb.chr3; __tiledb_workspace.tdb  chr3$1$198295559  vcfheader.vcf  vidmap.json. ```; It never indicates that it imported batch 65/65. No error and the  callset.json is missing which we found in chr4 to chr22. ;   ; ls genomicsDB.rb.chr4. __tiledb_workspace.tdb  callset.json  chr4$1$190214555  vcfheader.vcf  vidmap.json,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1246785232:4133,optimiz,optimizations,4133,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1246785232,1,['optimiz'],['optimizations']
Performance,"not the issue. Stacktrace in the bottom. The folder permission of the datastore folder is as follows:; `drwx--S---+ 26 vidprijatelj group 4096 Mar 14 15:29 Vid_database`. When changing to 766, the error disappears. ```; Tue Mar 14 15:37:57 CET 2023; Using GATK jar /appl/tools/versions/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Djava.io.tmpdir=zzz_tmpdir -Xmx128G -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -jar /appl/tools/versions/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar GenotypeGVCFs --reference /data/Scratch/References/ucsc.hg38.fa --variant gendb://Vid_database --output Step05_MultiSampleCalling/Vid.vcf.gz --intervals /data/Scratch/References/hg38_exome_v2.0.2_merged_probes_sorted_validated.annotated.bed --genomicsdb-shared-posixfs-optimizations True --merge-input-intervals; 15:37:59.895 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/appl/tools/versions/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 15:38:00.018 INFO GenotypeGVCFs - ------------------------------------------------------------; 15:38:00.018 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.3.0.0; 15:38:00.018 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 15:38:00.018 INFO GenotypeGVCFs - Executing as user@server; 15:38:00.018 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_362-b08; 15:38:00.019 INFO GenotypeGVCFs - Start Date/Time: March 14, 2023 3:37:59 PM CET; 15:38:00.019 INFO GenotypeGVCFs - ------------------------------------------------------------; 15:38:00.019 INFO GenotypeGVCFs - ------------------------------------------------------------; 15:38:00.019 INFO GenotypeGVCFs - HTSJDK Version: 3.0.1; 15:38:00.019 INFO GenotypeGVCFs - Picard Version: 2.27.5; 15:38:00.019 INFO ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8233#issuecomment-1468228918:1094,Load,Loading,1094,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8233#issuecomment-1468228918,1,['Load'],['Loading']
Performance,"nsferTo(FileChannelImpl.java:608); at io.netty.channel.DefaultFileRegion.transferTo(DefaultFileRegion.java:139); at org.apache.spark.network.protocol.MessageWithHeader.transferTo(MessageWithHeader.java:121); at io.netty.channel.socket.nio.NioSocketChannel.doWriteFileRegion(NioSocketChannel.java:287); at io.netty.channel.nio.AbstractNioByteChannel.doWrite(AbstractNioByteChannel.java:237); at io.netty.channel.socket.nio.NioSocketChannel.doWrite(NioSocketChannel.java:314); at io.netty.channel.AbstractChannel$AbstractUnsafe.flush0(AbstractChannel.java:802); at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.forceFlush(AbstractNioChannel.java:319); at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:637); at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566); at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442); at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144); at java.lang.Thread.run(Thread.java:748); 18/04/24 17:42:11 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 18/04/24 17:42:11 INFO MemoryStore: MemoryStore cleared; 18/04/24 17:42:11 INFO BlockManager: BlockManager stopped; 18/04/24 17:42:11 INFO BlockManagerMaster: BlockManagerMaster stopped; 18/04/24 17:42:11 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/04/24 17:42:11 INFO SparkContext: Successfully stopped SparkContext; 17:42:11.053 INFO PathSeqPipelineSpark - Shutting down engine; [April 24, 2018 5:42:11 PM CEST] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 2.87 minutes.; Runtime.totalMemory()=866648064; org.apache.spark.SparkException: Job aborted due to stage failure: Tas",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:38080,concurren,concurrent,38080,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,1,['concurren'],['concurrent']
Performance,"ntutils` package, which is strange because the PR did not modify the javadoc for any class in that package. The integration test runs `com.sun.tools.javadoc.Main.execute` and asserts that the output code is zero, which does not yield a useful error message. In order to produce something more meaningful I hacked the test to output the entire `stdout` and `stderr` as follows:. ```; final StringWriter out = new StringWriter();; final PrintWriter err = new PrintWriter(out);. final int result = com.sun.tools.javadoc.Main.execute(""program"", err, err, err, ""doclet"",docArgList.toArray(new String[] {}));; err.flush(); // probably not needed; String message = out.toString(); // message contains the entire stdout and stderr of the call to execute; Assert.assertEquals(result, 0, message);; ```. The output is about 2000 lines, but a lot of it is clearly innocuous. Removing lines such as; * `2022-08-16T00:09:07.2336106Z [parsing completed 1ms]`; * `2022-08-16T00:09:07.4456202Z [loading ZipFileIndexFileObject[/jars/gatk-package-4.2.6.1-56-gad9a538-SNAPSHOT-test.jar(org/broadinstitute/hellbender/tools/walkers/variantutils/ReblockGVCFIntegrationTest.class)]]`; * `2022-08-16T00:09:07.4459732Z [loading RegularFileObject[src/main/java/org/broadinstitute/hellbender/cmdline/argumentcollections/OptionalIntervalArgumentCollection.java]]`; * `2022-08-16T00:09:07.4462012Z [parsing started RegularFileObject[src/main/java/org/broadinstitute/hellbender/cmdline/argumentcollections/OptionalReferenceInputArgumentCollection.java]]`; * 2022-08-16T00:09:07.2322755Z [loading ZipFileObject[/gatk/gatk-package-unspecified-SNAPSHOT-local.jar(htsjdk/samtools/SAMSequenceDictionary.class)]]. brings it down to 353 lines, the majority of which look like . ```2022-08-16T00:09:07.4435974Z src/main/java/org/broadinstitute/hellbender/cmdline/CommandLineProgram.java:479: error: cannot find symbol; 2022-08-16T00:09:07.4436105Z @VisibleForTesting; ```. Here's that 353-line file:. [log-no-parsing-loading.txt](https://",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217231488:1090,load,loading,1090,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217231488,1,['load'],['loading']
Performance,"o cumbersome for large matrices. Change CreatePanelOfNormals to take in multiple -I instead.; - [x] Rename NormalizeSomaticReadCounts to DenoiseReadCounts and require integer read counts as input. These will still be backed by a ReadCountCollection until @asmirnov239's changes are in.; - [x] Remove optional outputs (factor-normalized and beta-hats) from DenoiseReadCounts. For now, TN and PTN output will remain in the same format (log2) to maintain compatibility with downstream tools.; - [x] Maximum number of eigensamples K to retain in the PoN is specified; the smaller of this or the number of samples remaining after filtering is used. The number actually used to denoise can be specified in DenoiseReadCounts. If we are going to spend energy computing K eigensamples, there is no reason we shouldn't expose all of them in the PoN, even if we don't want to use all of them for denoising. (Also, the current SVD utility methods do not allow for specification of K < N when performing SVD on an MxN matrix, even though the backend implementations that are called do allow for this; this is terrible. In any case, randomized SVD should be much faster than the currently available implementations, even when K = N).; - [x] Rename CreatePanelOfNormals to CreateReadCountPanelOfNormals; - [x] Refer to ""targets"" as intervals. See #3246.; - [x] Remove QC.; - [x] Refer to proportional coverage as fractional coverage.; - [x] Perform optional GC-bias correction internally if annotated intervals are passed as input.; - [x] Make standardization process for panel and case samples identical. Currently, a sample mean is taken at one point in the PoN standardization process, while a sample median is taken in the case standardization process.; - [x] HDF5 PoN will store version number, all integer read counts, all/panel intervals, all/panel sample paths/names, all annotated intervals (if GC-bias correction was performed), fractional-coverage medians for all intervals, relevant SVD results (eigenva",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-313921687:1140,perform,performing,1140,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-313921687,1,['perform'],['performing']
Performance,o.CloudStorageRetryHandler.handleReopenForStorageException(CloudStorageRetryHandler.java:124); at com.google.cloud.storage.contrib.nio.CloudStorageRetryHandler.handleStorageException(CloudStorageRetryHandler.java:94); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:621); at java.nio.file.Files.exists(Files.java:2385); at htsjdk.tribble.util.ParsingUtils.resourceExists(ParsingUtils.java:419); at htsjdk.tribble.AbstractFeatureReader.isTabix(AbstractFeatureReader.java:222); at htsjdk.tribble.AbstractFeatureReader$ComponentMethods.isTabix(AbstractFeatureReader.java:228); at htsjdk.tribble.AbstractFeatureReader.getFeatureReader(AbstractFeatureReader.java:106); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getReaderFromPath(GenomicsDBImport.java:638); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lambda$getFeatureReadersInParallel$613(GenomicsDBImport.java:593); at java.util.concurrent.FutureTask.run(FutureTask.java:266); ... 3 more; Caused by: com.google.cloud.storage.StorageException: ComputeEngineCredentials cannot find the metadata server. This is likely because code is not running on Google Compute Engine.; at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:189); at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:335); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:191); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:188); at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:94); at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:54); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:188); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:614); ... 11 more; Caused by: java.io.IOException: ComputeEngineCredentials cannot find the ,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-412904420:3744,concurren,concurrent,3744,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-412904420,1,['concurren'],['concurrent']
Performance,"oading libgkl_utils.dylib from jar:file:/Users/nhomer/miniconda3/envs/bfx/share/gatk4-4.1.8.1-0/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.dylib; 22:42:22.720 INFO NativeLibraryLoader - Loading libgkl_smithwaterman.dylib from jar:file:/Users/nhomer/miniconda3/envs/bfx/share/gatk4-4.1.8.1-0/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_smithwaterman.dylib; 22:42:22.722 INFO SmithWatermanAligner - Using AVX accelerated SmithWaterman implementation; 22:42:22.724 INFO HaplotypeCallerEngine - Standard Emitting and Calling confidence set to 0.0 for reference-model confidence output; 22:42:22.724 INFO HaplotypeCallerEngine - All sites annotated with PLs forced to true for reference-model confidence output; 22:42:22.734 WARN NativeLibraryLoader - Unable to find native library: native/libgkl_pairhmm_omp.dylib; 22:42:22.734 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; 22:42:22.734 INFO NativeLibraryLoader - Loading libgkl_pairhmm.dylib from jar:file:/Users/nhomer/miniconda3/envs/bfx/share/gatk4-4.1.8.1-0/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_pairhmm.dylib; 22:42:22.748 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 22:42:22.748 WARN IntelPairHmm - Ignoring request for 4 threads; not using OpenMP implementation; 22:42:22.748 INFO PairHMM - Using the AVX-accelerated native PairHMM implementation; 22:42:22.751 WARN GATKVariantContextUtils - Can't determine output variant file format from output file extension ""bam"". Defaulting to VCF.; 22:42:22.776 INFO ProgressMeter - Starting traversal; 22:42:22.777 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x000000010f47efd3, pid=96919, tid=0x0000000000002303; #; # JRE version: OpenJDK Runtime Environment (8.0_192-b01) (build 1.8.0_192-b01); # Java VM: OpenJDK 64-Bit ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6733#issuecomment-667631737:3646,Load,Loading,3646,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6733#issuecomment-667631737,1,['Load'],['Loading']
Performance,"odec.decode(AbstractVCFCodec.java:48); at htsjdk.tribble.AsciiFeatureCodec.decode(AsciiFeatureCodec.java:70); at htsjdk.tribble.AsciiFeatureCodec.decode(AsciiFeatureCodec.java:37); at htsjdk.tribble.AbstractFeatureCodec.decodeLoc(AbstractFeatureCodec.java:43); at org.broadinstitute.hellbender.utils.codecs.ProgressReportingDelegatingCodec.decodeLoc(ProgressReportingDelegatingCodec.java:46); at htsjdk.tribble.index.IndexFactory$FeatureIterator.readNextFeature(IndexFactory.java:689); at htsjdk.tribble.index.IndexFactory$FeatureIterator.<init>(IndexFactory.java:606); at htsjdk.tribble.index.IndexFactory.createDynamicIndex(IndexFactory.java:446); at org.broadinstitute.hellbender.tools.IndexFeatureFile.createAppropriateIndexInMemory(IndexFeatureFile.java:118); at org.broadinstitute.hellbender.tools.IndexFeatureFile.doWork(IndexFeatureFile.java:75); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:163); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:206); at org.broadinstitute.hellbender.Main.main(Main.java:292); Full Traceback (most recent call last):; File ""/home/ychrysostomakis/.local/lib/python3.9/site-packages/snakemake/executors/__init__.py"", line 2578, in run_wrapper; run(; File ""/share/pool/CompGenomVert/phoxy_snp_calling/VC_HIFI/joint_snp_calling.smk"", line 422, in __rule_index_feature_file; File ""/home/ychrysostomakis/.local/lib/python3.9/site-packages/snakemake/shell.py"", line 300, in __new__; raise sp.CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'set -euo pipefail; ; module load gatk/4.1.4.1; gatk IndexFeatureFile -I output/called/final/allsites.filtered.vcf' returned non-zero exit status 3.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8372#issuecomment-1733069316:5581,load,load,5581,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8372#issuecomment-1733069316,1,['load'],['load']
Performance,"of `--max-alternate-alleles` set to 7:; ```; on-chinookomes-dna-seq-gatk-variant-calling]--% gatk --java-options ""-Xmx4g"" GenotypeGVCFs -R resources/genome.fasta -V gendb://results/genomics_db/chromosomes/CM031199.1 --max-alternate-alleles 7 -O results/vcf_parts/CM031199.1.vcf.gz. Using GATK jar /home/eanderson/Documents/projects/yukon-chinookomes-dna-seq-gatk-variant-calling/.snakemake/conda/cd50d464/share/gatk4-4.2.4.1-0/gatk-package-4.2.4.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx4g -jar /home/eanderson/Documents/projects/yukon-chinookomes-dna-seq-gatk-variant-calling/.snakemake/conda/cd50d464/share/gatk4-4.2.4.1-0/gatk-package-4.2.4.1-local.jar GenotypeGVCFs -R resources/genome.fasta -V gendb://results/genomics_db/chromosomes/CM031199.1 --max-alternate-alleles 7 -O results/vcf_parts/CM031199.1.vcf.gz; 21:57:11.346 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/eanderson/Documents/projects/yukon-chinookomes-dna-seq-gatk-variant-calling/.snakemake/conda/cd50d464/share/gatk4-4.2.4.1-0/gatk-package-4.2.4.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 16, 2022 9:57:11 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 21:57:11.476 INFO GenotypeGVCFs - ------------------------------------------------------------; 21:57:11.477 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.4.1; 21:57:11.477 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 21:57:11.477 INFO GenotypeGVCFs - Executing as eanderson@node34.cluster on Linux v4.18.0-193.28.1.el8_2.x86_64 amd64; 21:57:11.477 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v11.0.9.1-internal+0-adhoc..src; 21:57:11.477 INFO GenotypeGVCFs - Start Date/Time:",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7639#issuecomment-1014180059:11121,Load,Loading,11121,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7639#issuecomment-1014180059,1,['Load'],['Loading']
Performance,"of the expected files, since the transform is appended to the corresponding variable name. DetermineGermlineContigPloidy and PostprocessGermlineCNVCalls are missing exact-match tests and should probably have some, but I'll leave that to someone else.; - [x] Update other python integration tests.; - [x] Clean up some of the changes to the priors.; - [x] Clean up some TODO comments that I left to track code changes that might result in changed numerics. I'll try to go through and convert these to PR comments in an initial review pass.; - [x] Test over multiple shards on WGS and WES. Probably some scientific tests on ~100 samples in both cohort and case mode would do the trick. We should also double check runtime/memory performance (I noted ~1.5x speedups, but didn't measure carefully; I also want to make sure the changes to posterior sampling didn't introduce any memory issues). @mwalker174 will ping you when a Docker is ready! Might be good to loop in Isaac and/or Jack as well.; - [x] Perhaps add back the fix for 2-interval shards in https://github.com/broadinstitute/gatk/pull/8180, which I removed since the required functionality wasn't immediately available in Pytensor. Not sure if this actually broke things though---need to check. (However, I don't actually think this is a very important use case to support...); - [x] Delete/deprecate/etc. CNN tools/tests as appropriate. Note that this has to be done concurrently, since we remove Tensorflow. @droazen perhaps I can take a first stab at this in a subsequent commit to this PR once more of the gCNV dust settles and/or has undergone a preliminary review? EDIT: Disabled integration/WDL tests. We should add some deprecation messages to the tools---we can note that they should still work in previous environments but will be untested. I might set up a separate PR for deletion, to be done at the appropriate time (but I call dibs on this, can't have @davidbenjamin overtaking my all-time record for number of lines deleted 😛).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8561#issuecomment-1847549285:3582,concurren,concurrently,3582,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8561#issuecomment-1847549285,1,['concurren'],['concurrently']
Performance,"ofcourse, but you need an annotation that would tell you if you should go; @bySampleAndLocus or @bySampleAndInterval...we could do it like that... On Fri, Mar 20, 2015 at 2:29 PM, droazen notifications@github.com wrote:. > Not allowing the data for each sample to be broken down and sent to; > multiple parallel workers would have serious performance implications, you; > realize...; > ; > —; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/hellbender/issues/320#issuecomment-84095044; > .",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/320#issuecomment-84098247:339,perform,performance,339,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/320#issuecomment-84098247,1,['perform'],['performance']
Performance,oken=7RuX7LsQVf&height=150&src=pr)](https://codecov.io/gh/broadinstitute/gatk/pull/5413?src=pr&el=tree). ```diff; @@ Coverage Diff @@; ## master #5413 +/- ##; ============================================; + Coverage 87.02% 87.08% +0.06% ; - Complexity 30454 30511 +57 ; ============================================; Files 1853 1856 +3 ; Lines 140995 141484 +489 ; Branches 15518 15536 +18 ; ============================================; + Hits 122706 123217 +511 ; + Misses 12648 12617 -31 ; - Partials 5641 5650 +9; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/5413?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...der/tools/walkers/contamination/PileupSummary.java](https://codecov.io/gh/broadinstitute/gatk/pull/5413/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2NvbnRhbWluYXRpb24vUGlsZXVwU3VtbWFyeS5qYXZh) | `90% <ø> (+1.42%)` | `14 <0> (+1)` | :arrow_up: |; | [...dinstitute/hellbender/utils/OptimizationUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/5413/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9PcHRpbWl6YXRpb25VdGlscy5qYXZh) | `42.1% <100%> (+3.21%)` | `4 <1> (+1)` | :arrow_up: |; | [...ination/CalculateContaminationIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5413/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2NvbnRhbWluYXRpb24vQ2FsY3VsYXRlQ29udGFtaW5hdGlvbkludGVncmF0aW9uVGVzdC5qYXZh) | `100% <100%> (ø)` | `14 <1> (+2)` | :arrow_up: |; | [...ols/walkers/contamination/ContaminationRecord.java](https://codecov.io/gh/broadinstitute/gatk/pull/5413/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2NvbnRhbWluYXRpb24vQ29udGFtaW5hdGlvblJlY29yZC5qYXZh) | `88.23% <100%> (-2.88%)` | `5 <1> (-1)` | |; | [.../walkers/contamination/CalculateContamination.java](https://codecov.io/gh/broadi,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5413#issuecomment-438824631:1478,Optimiz,OptimizationUtils,1478,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5413#issuecomment-438824631,1,['Optimiz'],['OptimizationUtils']
Performance,"omicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 2 ; 15:44:02.752 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 15:44:02.752 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 15:44:02.752 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 15:44:02.752 INFO GenomicsDBImport - Deflater: IntelDeflater; 15:44:02.752 INFO GenomicsDBImport - Inflater: IntelInflater; 15:44:02.752 INFO GenomicsDBImport - GCS max retries/reopens: 20; 15:44:02.752 INFO GenomicsDBImport - Requester pays: disabled; 15:44:02.752 INFO GenomicsDBImport - Initializing engine; 15:44:07.262 INFO FeatureManager - Using codec BEDCodec to read file file:///home/akansha/vivekruhela/gatk_bundle/hglift_genome1.bed; 15:44:07.274 INFO IntervalArgumentCollection - Processing 2759468497 bp from intervals; 15:44:07.276 WARN GenomicsDBImport - A large number of intervals were specified. Using more than 100 intervals in a single import is not recommended and can cause performance to suffer. If GVCF data only exists within those intervals, performance can be improved by aggregating intervals with the merge-input-intervals argument.; 15:44:07.307 INFO GenomicsDBImport - Done initializing engine; 15:44:07.591 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.3.2-e18fa63; 15:44:07.592 INFO GenomicsDBImport - Vid Map JSON file will be written to /home/akansha/vivekruhela/pon_db/vidmap.json; 15:44:07.592 INFO GenomicsDBImport - Callset Map JSON file will be written to /home/akansha/vivekruhela/pon_db/callset.json; 15:44:07.592 INFO GenomicsDBImport - Complete VCF Header will be written to /home/akansha/vivekruhela/pon_db/vcfheader.vcf; 15:44:07.592 INFO GenomicsDBImport - Importing to workspace - /home/akansha/vivekruhela/pon_db; 15:44:07.592 WARN GenomicsDBImport - GenomicsDBImport cannot use multiple VCF reader threads for initialization when the number of intervals is greater than 1. Falling back to serial V",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7037#issuecomment-761558811:2538,perform,performance,2538,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7037#issuecomment-761558811,1,['perform'],['performance']
Performance,omment&utm_campaign=pr+comments&utm_term=broadinstitute) (c9bf941) will **increase** coverage by `0.264%`.; > The diff coverage is `56.604%`. > :exclamation: Current head 2268ee6 differs from pull request most recent head d1dbe69. Consider uploading reports for the commit d1dbe69 to get more accurate results. ```diff; @@ Coverage Diff @@; ## master #8131 +/- ##; ===============================================; + Coverage 86.362% 86.626% +0.264% ; + Complexity 39551 38919 -632 ; ===============================================; Files 2362 2336 -26 ; Lines 186121 182603 -3518 ; Branches 20305 20062 -243 ; ===============================================; - Hits 160738 158181 -2557 ; + Misses 18236 17379 -857 ; + Partials 7147 7043 -104 ; ```. | [Files Changed](https://app.codecov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) | Coverage |; |---|---|; | [...lkers/vqsr/scalable/ExtractVariantAnnotations.java](https://app.codecov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvRXh0cmFjdFZhcmlhbnRBbm5vdGF0aW9ucy5qYXZh) | `ø` |; | [...walkers/vqsr/scalable/ScoreVariantAnnotations.java](https://app.codecov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvU2NvcmVWYXJpYW50QW5ub3RhdGlvbnMuamF2YQ==) | `0.000%` |; | [.../scalable/data/LabeledVariantAnnotationsDatum.java](https://app.codecov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8131#issuecomment-1352314153:1560,scalab,scalable,1560,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8131#issuecomment-1352314153,1,['scalab'],['scalable']
Performance,"omparison/1269d993-e13f-4635-a12a-e65fdaa4ed16/call-BenchmarkVCFControlSample/Benchmark/492b823a-1e34-46cd-b842-5f042bb31ee8/call-CombineSummaries/summary.csv"",; ""EXOME1 evalindelF1Score"": ""0.727"",; ""EXOME1 evalindelPrecision"": ""0.632"",; ""EXOME1 evalsnpF1Score"": ""0.9878"",; ""EXOME1 evalsnpPrecision"": ""0.9815"",; ""EXOME1 evalsnpRecall"": ""0.9941"",; ""EXOME1 evalsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/89508d5f-29f1-4534-9fe1-220a80de17c4/call-EXOME1SampleHeadToHead/BenchmarkComparison/1269d993-e13f-4635-a12a-e65fdaa4ed16/call-BenchmarkVCFTestSample/Benchmark/834b6562-65d7-4daf-857a-d9118a6456b7/call-CombineSummaries/summary.csv"",; ""NIST controlHCprocesshours"": ""90.94291388888888"",; ""NIST controlHCsystemhours"": ""0.182125"",; ""NIST controlHCwallclockhours"": ""63.56370277777778"",; ""NIST controlHCwallclockmax"": ""3.701625"",; ""NIST controlMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/89508d5f-29f1-4534-9fe1-220a80de17c4/call-NISTSampleHeadToHead/BenchmarkComparison/338d644e-3327-471e-9d17-1c103fa5e01e/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""NIST controlindelF1Score"": ""0.9902"",; ""NIST controlindelPrecision"": ""0.9903"",; ""NIST controlsnpF1Score"": ""0.9899"",; ""NIST controlsnpPrecision"": ""0.9887"",; ""NIST controlsnpRecall"": ""0.9911"",; ""NIST controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/89508d5f-29f1-4534-9fe1-220a80de17c4/call-NISTSampleHeadToHead/BenchmarkComparison/338d644e-3327-471e-9d17-1c103fa5e01e/call-BenchmarkVCFControlSample/Benchmark/145d88de-5810-47e1-972a-18ff0169fe27/call-CombineSummaries/summary.csv"",; ""NIST evalHCprocesshours"": ""92.82975"",; ""NIST evalHCsystemhours"": ""0.17177777777777778"",; ""NIST evalHCwallclockhours"": ""66.4404388888889"",; ""NIST evalHCwallclockmax"": ""3.325327777777778"",; ""NIST evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/89508d5f-29f1-4534-9fe1-220a80de17c",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7876#issuecomment-1193038382:19681,cache,cacheCopy,19681,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7876#issuecomment-1193038382,1,['cache'],['cacheCopy']
Performance,"omparison/3b586c16-feb0-4cdd-8850-8426205cced2/call-BenchmarkVCFControlSample/Benchmark/31dfb54a-9ecc-4af2-9fcd-ea9af745342e/call-CombineSummaries/summary.csv"",; ""EXOME1 evalindelF1Score"": ""0.727"",; ""EXOME1 evalindelPrecision"": ""0.632"",; ""EXOME1 evalsnpF1Score"": ""0.9878"",; ""EXOME1 evalsnpPrecision"": ""0.9815"",; ""EXOME1 evalsnpRecall"": ""0.9941"",; ""EXOME1 evalsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/f7eac327-c59c-43f7-a850-21bc3e0ccf52/call-EXOME1SampleHeadToHead/BenchmarkComparison/3b586c16-feb0-4cdd-8850-8426205cced2/call-BenchmarkVCFTestSample/Benchmark/7c7e45ee-4fe9-48e6-b8ed-cd4372c9e726/call-CombineSummaries/summary.csv"",; ""NIST controlHCprocesshours"": ""90.94291388888888"",; ""NIST controlHCsystemhours"": ""0.182125"",; ""NIST controlHCwallclockhours"": ""63.56370277777778"",; ""NIST controlHCwallclockmax"": ""3.701625"",; ""NIST controlMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/f7eac327-c59c-43f7-a850-21bc3e0ccf52/call-NISTSampleHeadToHead/BenchmarkComparison/d1a60d2b-8100-459a-9b05-72a22afccb4a/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""NIST controlindelF1Score"": ""0.9902"",; ""NIST controlindelPrecision"": ""0.9903"",; ""NIST controlsnpF1Score"": ""0.9899"",; ""NIST controlsnpPrecision"": ""0.9887"",; ""NIST controlsnpRecall"": ""0.9911"",; ""NIST controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/f7eac327-c59c-43f7-a850-21bc3e0ccf52/call-NISTSampleHeadToHead/BenchmarkComparison/d1a60d2b-8100-459a-9b05-72a22afccb4a/call-BenchmarkVCFControlSample/Benchmark/9f6d4e85-981d-4607-8ff6-97495034807f/call-CombineSummaries/summary.csv"",; ""NIST evalHCprocesshours"": ""96.65376666666666"",; ""NIST evalHCsystemhours"": ""0.17881944444444442"",; ""NIST evalHCwallclockhours"": ""68.38394444444445"",; ""NIST evalHCwallclockmax"": ""3.8226138888888888"",; ""NIST evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/f7eac327-c59c-43f7-a850-",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7876#issuecomment-1182703672:19694,cache,cacheCopy,19694,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7876#issuecomment-1182703672,1,['cache'],['cacheCopy']
Performance,"omparison/3ba68beb-5853-4beb-b31c-cbef12825001/call-BenchmarkVCFControlSample/Benchmark/18840f82-6653-4365-8e02-daf8790ea4f0/call-CombineSummaries/summary.csv"",; ""EXOME1 evalindelF1Score"": ""0.727"",; ""EXOME1 evalindelPrecision"": ""0.632"",; ""EXOME1 evalsnpF1Score"": ""0.9878"",; ""EXOME1 evalsnpPrecision"": ""0.9815"",; ""EXOME1 evalsnpRecall"": ""0.9941"",; ""EXOME1 evalsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/6ea2705f-a3fa-41fc-8d17-a2c55d875eab/call-EXOME1SampleHeadToHead/BenchmarkComparison/3ba68beb-5853-4beb-b31c-cbef12825001/call-BenchmarkVCFTestSample/Benchmark/194337cf-f57b-46fa-812c-e6510f51fd8d/call-CombineSummaries/summary.csv"",; ""NIST controlHCprocesshours"": ""90.94291388888888"",; ""NIST controlHCsystemhours"": ""0.182125"",; ""NIST controlHCwallclockhours"": ""63.56370277777778"",; ""NIST controlHCwallclockmax"": ""3.701625"",; ""NIST controlMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/6ea2705f-a3fa-41fc-8d17-a2c55d875eab/call-NISTSampleHeadToHead/BenchmarkComparison/a39481f5-0969-4891-a843-f3c3fd7437d1/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""NIST controlindelF1Score"": ""0.9902"",; ""NIST controlindelPrecision"": ""0.9903"",; ""NIST controlsnpF1Score"": ""0.9899"",; ""NIST controlsnpPrecision"": ""0.9887"",; ""NIST controlsnpRecall"": ""0.9911"",; ""NIST controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/6ea2705f-a3fa-41fc-8d17-a2c55d875eab/call-NISTSampleHeadToHead/BenchmarkComparison/a39481f5-0969-4891-a843-f3c3fd7437d1/call-BenchmarkVCFControlSample/Benchmark/0c99102a-bca1-4426-97c6-5a311ace93c1/call-CombineSummaries/summary.csv"",; ""NIST evalHCprocesshours"": ""95.62183055555556"",; ""NIST evalHCsystemhours"": ""0.18361111111111117"",; ""NIST evalHCwallclockhours"": ""64.22846111111112"",; ""NIST evalHCwallclockmax"": ""3.3683277777777776"",; ""NIST evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/6ea2705f-a3fa-41fc-8d17-",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7651#issuecomment-1069766207:13467,cache,cacheCopy,13467,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7651#issuecomment-1069766207,1,['cache'],['cacheCopy']
Performance,"omparison/4803682b-a3c6-46d6-924b-dbc96a877e16/call-BenchmarkVCFControlSample/Benchmark/dd059ca4-251d-4793-bbff-10dd76123882/call-CombineSummaries/summary.csv"",; ""EXOME1 evalindelF1Score"": ""0.727"",; ""EXOME1 evalindelPrecision"": ""0.632"",; ""EXOME1 evalsnpF1Score"": ""0.9878"",; ""EXOME1 evalsnpPrecision"": ""0.9815"",; ""EXOME1 evalsnpRecall"": ""0.9941"",; ""EXOME1 evalsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/e6f57e40-2025-46fd-9aa0-d591a3799007/call-EXOME1SampleHeadToHead/BenchmarkComparison/4803682b-a3c6-46d6-924b-dbc96a877e16/call-BenchmarkVCFTestSample/Benchmark/e3563584-017d-476b-bbca-775128c80272/call-CombineSummaries/summary.csv"",; ""NIST controlHCprocesshours"": ""90.94291388888888"",; ""NIST controlHCsystemhours"": ""0.182125"",; ""NIST controlHCwallclockhours"": ""63.56370277777778"",; ""NIST controlHCwallclockmax"": ""3.701625"",; ""NIST controlMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/e6f57e40-2025-46fd-9aa0-d591a3799007/call-NISTSampleHeadToHead/BenchmarkComparison/ccdb901c-fb8f-49e4-b542-cf42e011a623/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""NIST controlindelF1Score"": ""0.9902"",; ""NIST controlindelPrecision"": ""0.9903"",; ""NIST controlsnpF1Score"": ""0.9899"",; ""NIST controlsnpPrecision"": ""0.9887"",; ""NIST controlsnpRecall"": ""0.9911"",; ""NIST controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/e6f57e40-2025-46fd-9aa0-d591a3799007/call-NISTSampleHeadToHead/BenchmarkComparison/ccdb901c-fb8f-49e4-b542-cf42e011a623/call-BenchmarkVCFControlSample/Benchmark/6d64f12a-ca50-4ecd-8608-93dc53d241bb/call-CombineSummaries/summary.csv"",; ""NIST evalHCprocesshours"": ""95.62183055555556"",; ""NIST evalHCsystemhours"": ""0.18361111111111117"",; ""NIST evalHCwallclockhours"": ""64.22846111111112"",; ""NIST evalHCwallclockmax"": ""3.3683277777777776"",; ""NIST evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/e6f57e40-2025-46fd-9aa0-",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7651#issuecomment-1069378815:13467,cache,cacheCopy,13467,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7651#issuecomment-1069378815,1,['cache'],['cacheCopy']
Performance,"omparison/5bf5f11a-64cb-4b50-8d05-b61b7f4c803c/call-BenchmarkVCFControlSample/Benchmark/c64dbce6-4a90-42c0-a84b-59857afb98a5/call-CombineSummaries/summary.csv"",; ""EXOME1 evalindelF1Score"": ""0.727"",; ""EXOME1 evalindelPrecision"": ""0.632"",; ""EXOME1 evalsnpF1Score"": ""0.9878"",; ""EXOME1 evalsnpPrecision"": ""0.9815"",; ""EXOME1 evalsnpRecall"": ""0.9941"",; ""EXOME1 evalsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/9bc521dc-3c4c-4274-972c-9d1e4be850d5/call-EXOME1SampleHeadToHead/BenchmarkComparison/5bf5f11a-64cb-4b50-8d05-b61b7f4c803c/call-BenchmarkVCFTestSample/Benchmark/d501a36a-a881-4e5c-9499-ef7dea22980f/call-CombineSummaries/summary.csv"",; ""NIST controlHCprocesshours"": ""90.94291388888888"",; ""NIST controlHCsystemhours"": ""0.182125"",; ""NIST controlHCwallclockhours"": ""63.56370277777778"",; ""NIST controlHCwallclockmax"": ""3.701625"",; ""NIST controlMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/9bc521dc-3c4c-4274-972c-9d1e4be850d5/call-NISTSampleHeadToHead/BenchmarkComparison/4ffa2353-b1bc-4960-a5a4-96291208a7eb/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""NIST controlindelF1Score"": ""0.9902"",; ""NIST controlindelPrecision"": ""0.9903"",; ""NIST controlsnpF1Score"": ""0.9899"",; ""NIST controlsnpPrecision"": ""0.9887"",; ""NIST controlsnpRecall"": ""0.9911"",; ""NIST controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/9bc521dc-3c4c-4274-972c-9d1e4be850d5/call-NISTSampleHeadToHead/BenchmarkComparison/4ffa2353-b1bc-4960-a5a4-96291208a7eb/call-BenchmarkVCFControlSample/Benchmark/8cf95ec9-48a7-4e20-a8fe-816dc3e652ae/call-CombineSummaries/summary.csv"",; ""NIST evalHCprocesshours"": ""100.56416111111112"",; ""NIST evalHCsystemhours"": ""0.19999166666666665"",; ""NIST evalHCwallclockhours"": ""74.00048055555555"",; ""NIST evalHCwallclockmax"": ""4.007605555555555"",; ""NIST evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/9bc521dc-3c4c-4274-972c-",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1533946590:20354,cache,cacheCopy,20354,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1533946590,1,['cache'],['cacheCopy']
Performance,"omparison/688ca200-89b9-479b-b701-5fa0b0854778/call-BenchmarkVCFControlSample/Benchmark/59d8f8b1-1323-4e56-a1b1-0b1b2c8f2cc0/call-CombineSummaries/summary.csv"",; ""EXOME1 evalindelF1Score"": ""0.727"",; ""EXOME1 evalindelPrecision"": ""0.632"",; ""EXOME1 evalsnpF1Score"": ""0.9878"",; ""EXOME1 evalsnpPrecision"": ""0.9815"",; ""EXOME1 evalsnpRecall"": ""0.9941"",; ""EXOME1 evalsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/2a8ce326-baa5-4052-bff9-bd684393ff6c/call-EXOME1SampleHeadToHead/BenchmarkComparison/688ca200-89b9-479b-b701-5fa0b0854778/call-BenchmarkVCFTestSample/Benchmark/1b8ccc58-1ead-4443-b6a8-64f767abfc70/call-CombineSummaries/summary.csv"",; ""NIST controlHCprocesshours"": ""90.94291388888888"",; ""NIST controlHCsystemhours"": ""0.182125"",; ""NIST controlHCwallclockhours"": ""63.56370277777778"",; ""NIST controlHCwallclockmax"": ""3.701625"",; ""NIST controlMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/2a8ce326-baa5-4052-bff9-bd684393ff6c/call-NISTSampleHeadToHead/BenchmarkComparison/d1047505-b7bc-455d-851f-fbed8d81e895/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""NIST controlindelF1Score"": ""0.9902"",; ""NIST controlindelPrecision"": ""0.9903"",; ""NIST controlsnpF1Score"": ""0.9899"",; ""NIST controlsnpPrecision"": ""0.9887"",; ""NIST controlsnpRecall"": ""0.9911"",; ""NIST controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/2a8ce326-baa5-4052-bff9-bd684393ff6c/call-NISTSampleHeadToHead/BenchmarkComparison/d1047505-b7bc-455d-851f-fbed8d81e895/call-BenchmarkVCFControlSample/Benchmark/5388d7b6-6bcd-451d-9a4e-925b386ecd0c/call-CombineSummaries/summary.csv"",; ""NIST evalHCprocesshours"": ""95.03499722222222"",; ""NIST evalHCsystemhours"": ""0.17304166666666665"",; ""NIST evalHCwallclockhours"": ""67.81165555555557"",; ""NIST evalHCwallclockmax"": ""3.691061111111111"",; ""NIST evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/2a8ce326-baa5-4052-bff9-b",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7876#issuecomment-1194801748:19695,cache,cacheCopy,19695,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7876#issuecomment-1194801748,1,['cache'],['cacheCopy']
Performance,"omparison/efb51584-614a-4702-bc80-17a6a388e888/call-BenchmarkVCFControlSample/Benchmark/ea5e6517-663b-4cfb-b264-0dc933da9ae3/call-CombineSummaries/summary.csv"",; ""EXOME1 evalindelF1Score"": ""0.727"",; ""EXOME1 evalindelPrecision"": ""0.632"",; ""EXOME1 evalsnpF1Score"": ""0.9878"",; ""EXOME1 evalsnpPrecision"": ""0.9815"",; ""EXOME1 evalsnpRecall"": ""0.9941"",; ""EXOME1 evalsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/0e5c32ab-65e6-451f-a04e-6a3f5e7fe5c8/call-EXOME1SampleHeadToHead/BenchmarkComparison/efb51584-614a-4702-bc80-17a6a388e888/call-BenchmarkVCFTestSample/Benchmark/086dd5e8-74c8-4603-b618-a70d77398545/call-CombineSummaries/summary.csv"",; ""NIST controlHCprocesshours"": ""90.94291388888888"",; ""NIST controlHCsystemhours"": ""0.182125"",; ""NIST controlHCwallclockhours"": ""63.56370277777778"",; ""NIST controlHCwallclockmax"": ""3.701625"",; ""NIST controlMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/0e5c32ab-65e6-451f-a04e-6a3f5e7fe5c8/call-NISTSampleHeadToHead/BenchmarkComparison/ed0dc9e1-2d64-47e4-82e0-811971957020/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""NIST controlindelF1Score"": ""0.9902"",; ""NIST controlindelPrecision"": ""0.9903"",; ""NIST controlsnpF1Score"": ""0.9899"",; ""NIST controlsnpPrecision"": ""0.9887"",; ""NIST controlsnpRecall"": ""0.9911"",; ""NIST controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/0e5c32ab-65e6-451f-a04e-6a3f5e7fe5c8/call-NISTSampleHeadToHead/BenchmarkComparison/ed0dc9e1-2d64-47e4-82e0-811971957020/call-BenchmarkVCFControlSample/Benchmark/8c516721-e955-41d1-907e-fcee92f592d3/call-CombineSummaries/summary.csv"",; ""NIST evalHCprocesshours"": ""100.56416111111112"",; ""NIST evalHCsystemhours"": ""0.19999166666666665"",; ""NIST evalHCwallclockhours"": ""74.00048055555555"",; ""NIST evalHCwallclockmax"": ""4.007605555555555"",; ""NIST evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/0e5c32ab-65e6-451f-a04e-",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1535104202:20388,cache,cacheCopy,20388,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1535104202,1,['cache'],['cacheCopy']
Performance,"on **data representation**:. @laserson: yes, I think it makes total sense to eventually move to a better format. The requirements seem to be: (a) efficient serialization/deserialization, and (b) can easily convert to a SAMRecord for compatibility with existing code. We can make things extra efficient by only deserializing things if they are needed (if a phase doesn't need the CIGAR-related structures, no need to deserialize that). We can achieve this by having the deserialization be lazy. The LazyBAMRecord is a step in that direction since it looks up the reference name only if we ask for it, but we could go a lot further in this direction. But before we do that, having an efficient coder for SAMRecords (I vote for @tomwhite's approach of using the BAMEncoder) will get us 80% of the way for 20% of the effort. Then we can introduce our OptimizedSAMRecord incrementally. . on **headers**:. I agree with @tomwhite that adding the header back after a shuffle is the right thing to do. We know where that happens and we control that code.; @davidadamsphd, you worry about newcomers. But we've already decided that we were going to provide our own API for them (one that does the Dataflow copying for them so they don't have to worry about it). This same API will provide them with header-filled reads, so they don't have to worry about this detail. This falls into the general category of ""the 3rd party devs won't have to even know about Dataflow/Spark: they just need to know our nice, simple interface and use that"". If they know more and want to do fancier things then more power to them, but those users will surely be able to fill in headers, too. We have library functions to use the reads without the headers, but the problem is that (at least for the sort of code I'm writing), I'm handing off a SAMRecord to a big black box and I can't force it to use the library functions - it's going to work on the SAMRecord directly. So at least in this case it's important to fill in the header ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141151451:847,Optimiz,OptimizedSAMRecord,847,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141151451,1,['Optimiz'],['OptimizedSAMRecord']
Performance,"on appears to have a memory leak or using just requiring too much memory. The -consolidate option was the culprit. So rerunning chr1-3 with just the --bypass-feature-reader option (test2) ran fine without lots of memory being used. Below is the time output from chr1. The output shows the Maximum resident set size (kbytes): **2630440**. Using GATK jar /share/pkg.7/gatk/4.2.6.1/install/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar defined in environment variable GATK_LOCAL_JAR; ```; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx200g -Xms16g -jar /share/pkg.7/gatk/4.2.6.1/install/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar GenomicsDBImport --sample-name-map sample_map.chr1 --genomicsdb-workspace-path genomicsDB.rb.bypass.time.chr1 --genomicsdb-shared-posixfs-optimizations True --tmp-dir tmp --bypass-feature-reader --L chr1 --batch-size 50 --reader-threads 4 --overwrite-existing-genomicsdb-workspace; Command being timed: ""gatk --java-options -Xmx200g -Xms16g GenomicsDBImport --sample-name-map sample_map.chr1 --genomicsdb-workspace-path genomicsDB.rb.bypass.time.chr1 --genomicsdb-shared-posixfs-optimizations True --tmp-dir tmp --bypass-feature-reader --L chr1 --batch-size 50 --reader-threads 4 --overwrite-existing-genomicsdb-workspace""; User time (seconds): 270716.45; System time (seconds): 1723.34; Percent of CPU this job got: 99%; Elapsed (wall clock) time (h:mm:ss or m:ss): 76:08:24; Average shared text size (kbytes): 0; Average unshared data size (kbytes): 0; Average stack size (kbytes): 0; Average total size (kbytes): 0; Maximum resident set size (kbytes): 2630440; Average resident set size (kbytes): 0; Major (requiring I/O) page faults: 5; Minor (reclaiming a frame) page faults: 206030721; Voluntary context switches: 11129822; Involuntary context switches: 176522; Swaps: 0; File system inputs: 627981312; File system outputs: 466730160; Socke",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1252598687:1912,optimiz,optimizations,1912,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1252598687,1,['optimiz'],['optimizations']
Performance,"on unit; isinf(_Tp __f); ^; In file included from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/baseline.cc:2:0:; /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/common_data_structure.h:94:38: error: 'isinf' was not declared in this scope, and no declarations were found by argument-dependent lookup at the point of instantiation [-fpermissive]; if (isinf(small) == -1 || isinf(big) == -1); ^; In file included from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/headers.h:27:0,; from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/baseline.cc:1:; /usr/local/Cellar/gcc/5.3.0/include/c++/5.3.0/cmath:853:5: note: 'template<class _Tp> typename __gnu_cxx::__enable_if<std::__is_arithmetic<_Tp>::__value, int>::__type std::isinf(_Tp)' declared here, later in the translation unit; isinf(_Tp __f); ^. In file included from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/utils.h:4:0,; from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/LoadTimeInitializer.cc:1:; /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/common_data_structure.h: In instantiation of 'static NUMBER ContextBase<NUMBER>::approximateLog10SumLog10(NUMBER, NUMBER) [with NUMBER = float]':; /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/common_data_structure.h:75:53: required from 'static void ContextBase<NUMBER>::initializeMatchToMatchProb() [with NUMBER = float]'; /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/common_data_structure.h:47:35: required from 'static void ContextBase<NUMBER>::initializeStaticMembers() [with NUMBER = float]'; /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/LoadTimeInitializer.cc:52:19: required from here; /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/common_data_structure.h:94:16: error: 'isinf' was not declared in this scope, and no declarations were found by argument-dependent lookup at the point of instantiation [-fp",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1504#issuecomment-187727343:9356,Load,LoadTimeInitializer,9356,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1504#issuecomment-187727343,1,['Load'],['LoadTimeInitializer']
Performance,on.util.concurrent.Futures.addCallback(Futures.java:1776); 	at shaded.cloud_nio.com.google.common.util.concurrent.Futures.addCallback(Futures.java:1713); 	at shaded.cloud_nio.com.google.api.gax.core.ApiFutures.addCallback(ApiFutures.java:47); 	at shaded.cloud_nio.com.google.api.gax.retrying.RetryingFutureImpl.setAttemptFuture(RetryingFutureImpl.java:107); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:100); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:47); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:125); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:109); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:131); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:104); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: java.net.SocketTimeoutException: Read timed out; 	at java.net.SocketInputStream.socketRead0(Native Method); 	at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); 	at java.net.SocketInputStream.read(SocketInputStream.java:171); 	at java.net.SocketInputStream.read(SocketInputStream.java:141); 	at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); 	at sun.security.ssl.InputRecord.read(InputRecord.java:503); 	at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973); 	at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:930); 	at sun.security.ssl.AppInputStream.read(AppInputStream.java:105); 	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); 	at java.io.Buffe,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300298180:5930,concurren,concurrent,5930,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300298180,1,['concurren'],['concurrent']
Performance,"onf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 /scratch/home/int/eva/username/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-spark.jar PathSeqPipelineSpark --spark-master spark://xx.xx.xx.xx:7077 --input test_sample.bam --filter-bwa-image hg19mini.fasta.img --kmer-file hg19mini.hss --min-clipped-read-length 70 --microbe-fasta e_coli_k12.fasta --microbe-bwa-image e_coli_k12.fasta.img --taxonomy-file e_coli_k12.db --output output.pathseq.bam --verbosity DEBUG --scores-output output.pathseq.txt; 17:39:18.382 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 17:39:18.825 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/scratch/home/int/eva/username/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 17:39:18.857 DEBUG NativeLibraryLoader - Extracting libgkl_compression.so to /tmp/username/libgkl_compression3681606702485397808.so; 17:39:19.218 INFO PathSeqPipelineSpark - ------------------------------------------------------------; 17:39:19.218 INFO PathSeqPipelineSpark - The Genome Analysis Toolkit (GATK) v4.0.3.0; 17:39:19.218 INFO PathSeqPipelineSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:39:19.219 INFO PathSeqPipelineSpark - Executing as username@node016 on Linux v2.6.32-220.4.1.el6.x86_64 amd64; 17:39:19.220 INFO PathSeqPipelineSpark - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_131-b11; 17:39:19.220 INFO PathSeqPipelineSpark - Start Date/Time: April 24, 2018 5:39:18 PM CEST; 17:39:19.220 INFO PathSeqPipelineSpark - --------------------------------------",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:2535,Load,Loading,2535,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,1,['Load'],['Loading']
Performance,"onfun$reportAllBlocks$3.apply(BlockManager.scala:219); 	at org.apache.spark.storage.BlockManager$$anonfun$reportAllBlocks$3.apply(BlockManager.scala:217); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at org.apache.spark.storage.BlockManager.reportAllBlocks(BlockManager.scala:217); 	at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:236); 	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:522); 	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:547); 	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547); 	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547); 	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953); 	at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:547); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308); 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180); 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); 17/10/18 17:35:58 INFO BlockManagerMaster: BlockManagerMaster stopped; 17/10/18 17:35:58 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker-1,5,main]; java.lang.OutOfMemoryError: Java heap space; 	at htsjdk.samtools.BAMRecordCodec.decode(BAMRecordCodec.java:208); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.getNextRecord(BAMFileReader.java:829); 	at htsjdk.samtool",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-337554749:5314,concurren,concurrent,5314,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-337554749,1,['concurren'],['concurrent']
Performance,"ons (@fleharty @mwalker174?) or I run into any unforeseen snafus or parameter ambiguities along the way, I am going to roll the multisample-segmentation functionality into ModelSegments. We can toggle this functionality by passing multiple `--denoised-copy-ratios` and `--allelic-counts` arguments, e.g.:. ```; gatk ModelSegments ; --normal-allelic-counts normal.allelicCounts.tsv (this is only used for het genotyping); --denoised-copy-ratios normal.denoisedCR.tsv; --denoised-copy-ratios tumor-1.denoisedCR.tsv; ...; --denoised-copy-ratios tumor-N.denoisedCR.tsv; --allelic-counts normal.allelicCounts.tsv; --allelic-counts tumor-1.allelicCounts.tsv; ...; --allelic-counts tumor-N.allelicCounts.tsv \; -O .; --output-prefix joint-segmentation; ```. This will perform both het genotyping and joint segmentation, but will yield a Picard interval-list `joint-segmentation.interval_list` as its sole output. (Although we could proceed to perform MCMC model inference on each sample in series, we'll stop at segmentation to enforce the scattering of inference across samples, which will be quicker.) We can also allow for copy-ratio-only and allelic-count-only modes. Users can use this joint segmentation in their own downstream tools, but we can also allow ModelSegments to ingest it via in a new `--segments` argument:. ```; gatk ModelSegments; --normal-allelic-counts normal.allelicCounts.tsv (equivalently, we could omit this and adjust minimum-total-allele-count-case, as is done in the WDL); --allelic-counts normal.allelicCounts.tsv; --denoised-copy-ratios normal.denoisedCR.tsv; --segments joint-segmentation.interval_list; -O .; --output-prefix normal. gatk ModelSegments; --normal-allelic-counts normal.allelicCounts.tsv; --allelic-counts tumor-1.allelicCounts.tsv; --denoised-copy-ratios tumor-1.denoisedCR.tsv; --segments joint-segmentation.interval_list; -O .; --output-prefix tumor-1. ...; ```. Each scatter of ModelSegments will run as before, aside from skipping the segmentation step i",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6499#issuecomment-607313549:1040,perform,perform,1040,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6499#issuecomment-607313549,1,['perform'],['perform']
Performance,"onsolidated with---@jamesemery thoughts? Again, let me reiterate that it seems that many of these parameter values were chosen arbitrarily (or, if not, that the procedure for choosing them has been lost). As a start, you can see the results of some optimizations I did on the CHM mix on slide 15 at https://docs.google.com/presentation/d/1zGuquAZWSUQ-wNxp8D6HhGNjIaFcV0_X9WAS4LODbEo/edit?usp=sharing Here, I optimized over haplotype-to-reference + read-to-haplotype SW parameters on various metrics after variant normalization using vcfeval. These optimizations were done using the Bayesian optimization framework I prototyped long ago (see https://github.com/broadinstitute/gatk-evaluation/tree/master/pipeline-optimizer and https://docs.google.com/presentation/d/1t5WOAEOMp0xAzJgpKbP68BUnclNYfIVRrDSL9wl1-3A/edit?usp=sharing); this entailed running parameter scans using a local Cromwell on my desktop. Probably this optimization work could be redone relatively easily using the Neptune framework put together by @dalessioluca, which was still in development at the time I did this work. Happy to share the resources and scripts I used if we go down this route; they are pretty lightweight. See more discussion starting here: https://github.com/broadinstitute/gatk/issues/5564#issuecomment-710107566. Alternatively, we could merge this branch to expose the parameters now and punt on consolidating/optimizing them. I'm not completely convinced we should even do the former unless we are going to follow through on the latter, but happy to defer to others. Finally, note also there is one code optimization that I removed, since it makes assumptions on the SW parameter values that might not be valid for non-default values. I'll highlight this with a comment below. We can restore it if we add code to check whether the assumptions hold, but I'd be curious to see in which cases the optimization makes a big difference. See https://github.com/broadinstitute/gatk/issues/6863#issuecomment-707870344.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6885#issuecomment-891907471:2671,optimiz,optimizing,2671,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6885#issuecomment-891907471,3,['optimiz'],"['optimization', 'optimizing']"
Performance,"ontrast to genotyping in matched-normal mode, in which the normal determines the set of hets used in all samples). We will thus have to take the intersection of these hets before performing multisample segmentation. Unfortunately, we will not be able to re-perform this intersection in each scatter, since we will no longer have access to the hets from the other samples. However, we *will* ultimately intersect the hets from each sample with the joint segmentation before modeling, which may be a rough proxy for the intersection of hets from all samples. As always, tumor-only mode may yield suboptimal results in certain scenarios, e.g., high purity CNLOH. I think I'm OK with just documenting these wrinkles, rather than working too hard to iron them out. I think this structure sets us up nicely to accommodate germline tagging/filtering in the near future. We can still pass the Picard interval list containing the joint segmentation to the scatter for the normal, but can instead subsequently pass the *.modelBegin.seg result from the normal to the tumors. This modeled-segment file will have breakpoints identical to those from the joint segmentation (as opposed to the *.modelFinal.seg result, since that undergoes segment smoothing/merging), but will also contain the segment-level posteriors necessary for performing germline filtering. We will just need to add code to toggle on the type of `--segments` input (Picard interval list or modeled segments), add filtering arguments and code, perhaps output an additional seg file showing filter status for the *.modelBegin.seg segments, and modify the segment merging/smoothing code to properly account for filter status (so we don't incorrectly impute across filtered segments, which is currently done by the unsupported code, for whatever reason). I think this should clock in at well under ~5k lines of code, which is the count for the current unsupported code (see https://github.com/broadinstitute/gatk/pull/5450#issuecomment-461431199).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6499#issuecomment-607313549:4164,perform,performing,4164,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6499#issuecomment-607313549,1,['perform'],['performing']
Performance,"ook called; 2019-01-07 11:34:12 INFO ShutdownHookManager:54 - Deleting directory /tmp/spark-1ac79f09-1a36-4668-92d9-0739775f98ed; 2019-01-07 11:34:12 INFO ShutdownHookManager:54 - Deleting directory /tmp/spark-ed279998-3783-4f41-8fe5-f44a4fac3ee4; ```. CountReads runs fine..... ```; gatk CountReads --input HG04302.alt_bwamem_GRCh38DH.20150718.GBR.low_coverage.cram --reference file:///restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa; Using GATK jar /share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-local.jar CountReads --input HG04302.alt_bwamem_GRCh38DH.20150718.GBR.low_coverage.cram --reference file:///restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa; 11:36:23.022 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 11:36:25.027 INFO CountReads - ------------------------------------------------------------; 11:36:25.028 INFO CountReads - The Genome Analysis Toolkit (GATK) v4.0.12.0; 11:36:25.028 INFO CountReads - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:36:25.029 INFO CountReads - Executing as farrell@scc-hadoop.bu.edu on Linux v2.6.32-754.6.3.el6.x86_64 amd64; 11:36:25.029 INFO CountReads - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 11:36:25.030 INFO CountReads - Start Date/Time: January 7, 2019 11:36:22 AM EST; 11:36:25.030 INFO CountReads - ------------------------------------------------------------; 11:36:25.031 INFO CountReads - ------------------------------------------------------------; 11:36:25.032 INFO CountReads - HTSJDK Version: 2.18.1; 11:36:25.033 INFO CountReads -",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:44263,Load,Loading,44263,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,1,['Load'],['Loading']
Performance,"ore modeling. However, instead of assuming the null hypothesis of het (f = 0.5) and accepting a site when we cannot reject the null, we assume the null hypothesis of hom (f = error rate or 1 - error rate) and accept a site when we can reject the null. This entire process is very similar to what @davidbenjamin does in https://github.com/broadinstitute/gatk/pull/3638. We should consider combining this code (along with `AllelicCount`/`PileupSummary`) at some point.; - [x] Added option to use matched normal.; - [ ] Rather than port over the old modeling code, I would rather expand the allele-fraction model to allow for the modeling of hom sites. I wrote up such a model in some notes I sent around a few months back. This model allows for an allelic PoN that uses all sites to learn reference bias, not just hets. Depending on how our python development proceeds, I may try to implement this model using the old `GibbsSampler` code instead.; - [x] In the meantime, we can try to speed up the old allele-fraction model, which is now the main bottleneck. An easy (lazy) strategy might simply be to downsample and scale likelihoods when estimating global parameters. Addresses #2884.; - [x] Even though the simple copy-ratio model is much faster, it still takes ~15-20 minutes for 100 iterations on WGS, so we can downsample here too.; - [x] Integration tests are still needed; again, these might not test for correctness.; - I've added the ability to specify a prior for the minor-allele fraction, which alleviates the problem of residual bias in balanced segments.; - I've reduced the verbosity of the modeled-segments file. I only report posterior mode and 10%, 50%, and 90% deciles. Global parameters have the full deciles output in the .param files, but I removed the mode and highest density credible interval (because of the below item).; - [x] Some residual bias remains in the estimate of the minor-allele fraction posterior mode. This is simply because we are performing kernel density est",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:5690,bottleneck,bottleneck,5690,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828,1,['bottleneck'],['bottleneck']
Performance,"org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.varia.NullAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.varia.NullAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""NullAppender"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.varia.NullAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.varia.NullAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""NullAppender"".; log4j:ERROR A ""org.apache.log4j.varia.NullAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312229998:5915,load,loaded,5915,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312229998,1,['load'],['loaded']
Performance,ort.getReaderFromVCFUri(GenomicsDBImport.java:437); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getFeatureReaders(GenomicsDBImport.java:419); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.traverse(GenomicsDBImport.java:344); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:740); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:220); Caused by: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: Read timed out; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.getBuf(SeekableByteChannelPrefetcher.java:136); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.fetch(SeekableByteChannelPrefetcher.java:255); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:300); 	... 28 more; Caused by: com.google.cloud.storage.StorageException: Read timed out; 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:186); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:512); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:128); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:125); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:92),MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300298180:2872,concurren,concurrent,2872,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300298180,1,['concurren'],['concurrent']
Performance,oryError: Java heap space; at java.util.Arrays.copyOf(Arrays.java:3332); at java.lang.AbstractStringBuilder.ensureCapacityInternal(AbstractStringBuilder.java:124); at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:448); at java.lang.StringBuilder.append(StringBuilder.java:136); at htsjdk.tribble.util.ParsingUtils.split(ParsingUtils.java:266); at htsjdk.variant.vcf.AbstractVCFCodec.decodeLine(AbstractVCFCodec.java:375); at htsjdk.variant.vcf.AbstractVCFCodec.decode(AbstractVCFCodec.java:328); at htsjdk.variant.vcf.AbstractVCFCodec.decode(AbstractVCFCodec.java:48); at htsjdk.tribble.AsciiFeatureCodec.decode(AsciiFeatureCodec.java:70); at htsjdk.tribble.AsciiFeatureCodec.decode(AsciiFeatureCodec.java:37); at org.genomicsdb.reader.GenomicsDBFeatureIterator.next(GenomicsDBFeatureIterator.java:181); at org.genomicsdb.reader.GenomicsDBFeatureIterator.next(GenomicsDBFeatureIterator.java:49); at org.broadinstitute.hellbender.engine.FeatureIntervalIterator.loadNextFeature(FeatureIntervalIterator.java:98); at org.broadinstitute.hellbender.engine.FeatureIntervalIterator.loadNextNovelFeature(FeatureIntervalIterator.java:74); at org.broadinstitute.hellbender.engine.FeatureIntervalIterator.next(FeatureIntervalIterator.java:62); at org.broadinstitute.hellbender.engine.FeatureIntervalIterator.next(FeatureIntervalIterator.java:24); at java.util.Iterator.forEachRemaining(Iterator.java:116); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472) ; at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150) ; at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEachOrdered(ReferencePipeline.java:490) ; at org.broadin,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6275#issuecomment-574329688:3114,load,loadNextFeature,3114,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6275#issuecomment-574329688,1,['load'],['loadNextFeature']
Performance,"ot supported, using Java.util.zip.Inflater; >; > 16:17:06.503 INFO FeatureManager - Using codec VCFCodec to read file file:///home/robert/test/snps.vcf; >; > 16:17:06.539 INFO IntervalArgumentCollection - Processing 61464 bp from intervals; >; > 16:17:06.551 INFO HaplotypeCaller - Done initializing engine; >; > 16:17:06.573 INFO HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output; >; > 16:17:06.588 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; >; > 16:17:06.589 **WARN** NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils347167544598047196.so: /tmp/libgkl_utils347167544598047196.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > 16:17:06.589 **WARN** IntelPairHmm - Intel GKL Utils not loaded; >; > 16:17:06.589 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; >; > 16:17:06.589 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; >; > 16:17:06.590 **WARN** NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils6186849302609329058.so: /tmp/libgkl_utils6186849302609329058.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > 16:17:06.590 **WARN** IntelPairHmm - Intel GKL Utils not loaded; >; > 16:17:06.591 **WARN** PairHMM - ***WARNING: Machine does not have the AVX instruction set support needed for the accelerated AVX PairHmm. Falling back to the MUCH slower LOGLESS_CACHING implementation!; >; > Since the calculation takes quite long, I checked the WARN messages of",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600:5507,load,loaded,5507,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600,1,['load'],['loaded']
Performance,"ot sure if that was already covered. @sooheelee - this is going to be an exact port of the indel-realignment pipeline, as it is in the GATK3 code, so that means that I won't modify the interval list format or anything (although I will use the HTSJDK/Picard classes as used on GATK3). Because this will be an experimental/beta feature, I think that I can have a look to the new format after acceptance of the original port. @cmnbroad - I understand that a fully functional tool is a requirement for acceptance, but what I mean is that some specific features might require more work than others. I am only concerned about the `NWaySAMFileWriter`, which is just an specific way of output the data but does not add anything to the real realignment process (actually, I think that I've never heard about anyone around me using it). That is a nice feature, but I don't think that it is a high-priority - I care more about having the algorithm implemented to test if the actual processing of the data works, and add support for some way of output the data in a different PR. In addition, if the people still using indel-realignment does not require the n-way output, then it is pointless to spend time on it. I was also thinking about the mate-fixing algorithm in the tool, because it can be performed afterwards with Picard, which is not constraining by any distance between reads or records in RAM - nevertheless, this is really a drop of functionality that will change results, and that's why I didn't propose that. About the target-creator, known indels are really easy to port because the code is within the tool and is simpler - the only problem might be code coverage if there is no data for known indels. I will propose very soon two PRs with fully functional tools (without the n-way out feature for indel-realignment), and trying to add simple integration tests with the data already available on the repository and running with GATK3.8-1. If that is OK for you, I will proceed with this approach.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-371515115:1619,perform,performed,1619,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-371515115,1,['perform'],['performed']
Performance,otator/filtrationRules/FuncotationFilter.java](https://codecov.io/gh/broadinstitute/gatk/pull/5588/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9mdW5jb3RhdG9yL2ZpbHRyYXRpb25SdWxlcy9GdW5jb3RhdGlvbkZpbHRlci5qYXZh) | `100% <100%> (ø)` | `8 <4> (+4)` | :arrow_up: |; | [...r/filtrationRules/FilterFuncotationsExacUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/5588/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9mdW5jb3RhdG9yL2ZpbHRyYXRpb25SdWxlcy9GaWx0ZXJGdW5jb3RhdGlvbnNFeGFjVXRpbHMuamF2YQ==) | `81.818% <80.952%> (-0.535%)` | `12 <7> (+5)` | |; | [...walkers/genotyper/afcalc/AFCalculatorProvider.java](https://codecov.io/gh/broadinstitute/gatk/pull/5588/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9hZmNhbGMvQUZDYWxjdWxhdG9yUHJvdmlkZXIuamF2YQ==) | `22.222% <0%> (-44.444%)` | `2% <0%> (-2%)` | |; | [...notyper/afcalc/ConcurrentAFCalculatorProvider.java](https://codecov.io/gh/broadinstitute/gatk/pull/5588/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9hZmNhbGMvQ29uY3VycmVudEFGQ2FsY3VsYXRvclByb3ZpZGVyLmphdmE=) | `50% <0%> (-33.333%)` | `1% <0%> (-1%)` | |; | [...nder/utils/downsampling/PositionalDownsampler.java](https://codecov.io/gh/broadinstitute/gatk/pull/5588/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9kb3duc2FtcGxpbmcvUG9zaXRpb25hbERvd25zYW1wbGVyLmphdmE=) | `88.462% <0%> (-11.538%)` | `22% <0%> (+1%)` | |; | [...er/engine/spark/datasources/VariantsSparkSink.java](https://codecov.io/gh/broadinstitute/gatk/pull/5588/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvZGF0YXNvdXJjZXMvVmFyaWFudHNTcGFya1NpbmsuamF2YQ==) | `78.125% <0%> (-11.53%)` | `8% <0%> (-1%)` | |; | ... and [138 more](https://codecov.io/gh/broadinstitute/gatk/pull/5588/diff,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5588#issuecomment-455358539:3191,Concurren,ConcurrentAFCalculatorProvider,3191,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5588#issuecomment-455358539,1,['Concurren'],['ConcurrentAFCalculatorProvider']
Performance,oud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:186); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:512); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:128); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:125); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:92); 	at shaded.cloud_nio.com.google.api.gax.retrying.RetryingFutureImpl.executeAttempt(RetryingFutureImpl.java:141); 	at shaded.cloud_nio.com.google.api.gax.retrying.RetryingFutureImpl.access$500(RetryingFutureImpl.java:59); 	at shaded.cloud_nio.com.google.api.gax.retrying.RetryingFutureImpl$AttemptFutureCallback.onFailure(RetryingFutureImpl.java:177); 	at shaded.cloud_nio.com.google.api.gax.core.ApiFutures$1.onFailure(ApiFutures.java:52); 	at shaded.cloud_nio.com.google.common.util.concurrent.Futures$6.run(Futures.java:1764); 	at shaded.cloud_nio.com.google.common.util.concurrent.MoreExecutors$DirectExecutor.execute(MoreExecutors.java:456); 	at shaded.cloud_nio.com.google.common.util.concurrent.Futures$ImmediateFuture.addListener(Futures.java:153); 	at shaded.cloud_nio.com.google.common.util.concurrent.ForwardingListenableFuture.addListener(ForwardingListenableFuture.java:47); 	at shaded.cloud_nio.com.google.api.gax.core.internal.ApiFutureToListenableFuture.addListener(ApiFutureToListenableFuture.java:53); 	at shaded.cloud_nio.com.google.common.util.concurrent.Futures.addCallback(Futures.java:1776); 	at shaded.cloud_nio.com.google.common.util.concurrent.Futures.addCallback(Futures.java:1713); 	at shaded.cloud_nio.com.google.api.gax.core.ApiFutures.addCallback(ApiFutures.java:47); 	at shaded.cloud_nio.com.google.api.gax.retrying.RetryingFutureImpl.setAttemptFuture(RetryingFutureImpl.java:107); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:100); 	at com.google.cloud.RetryHelper.runWithRetries(Retry,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300298180:4453,concurren,concurrent,4453,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300298180,1,['concurren'],['concurrent']
Performance,"ound first, since these give rise to longer segments:. ![wave-kern-no-local](https://user-images.githubusercontent.com/11076296/29322673-4dd9a1ac-81ac-11e7-94f5-5c5494e44ac5.png). CBS similarly finds many false positive breakpoints:. ![wave-cbs](https://user-images.githubusercontent.com/11076296/29322677-5576e4ba-81ac-11e7-888b-07ed5bff27e3.png). However, when we tune down the sine waves to 1:10, ApproxKernSeg still gets tripped up, but CBS looks better:. ![wave-kern-no-local-small-waves](https://user-images.githubusercontent.com/11076296/29322732-815df58c-81ac-11e7-8305-6e1798616336.png); ![wave-cbs-small-waves](https://user-images.githubusercontent.com/11076296/29322737-836b78fe-81ac-11e7-93be-753a40011203.png). To improve ApproxKernSeg, we can 1) make the cost function intensive, by simply dividing by the number of points in a segment, and 2) add to the cost function a local term, given by the cost of making each point a changepoint within a local window of a determined size. This local term was inspired by methods such as SaRa (http://c2s2.yale.edu/software/sara/). The reasoning is that with events at higher S/N ratio, we typically don't need to perform a global test to see whether any given point is a suitable changepoint; using the data locally surrounding the point typically suffices. With these modifications, ApproxKernSeg can handle both scenarios:; ![wave-kern](https://user-images.githubusercontent.com/11076296/29322762-a679dba6-81ac-11e7-9360-083a4e1da398.png); ![wave-kern-small-waves](https://user-images.githubusercontent.com/11076296/29322801-dad82010-81ac-11e7-8238-e057b0072e1b.png). This local window approach is still linear in time, so runtime is still ~1s for the above (about ~10x faster than CBS). One issue still remains, which is that even this improved approach tends to find directly adjacent possible changepoints around a true changepoint before moving on to another true changepoint. We can probably clean this up with some simple postprocessing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-322502045:1964,perform,perform,1964,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-322502045,1,['perform'],['perform']
Performance,pacityInternal(AbstractStringBuilder.java:124); at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:448); at java.lang.StringBuilder.append(StringBuilder.java:136); at htsjdk.tribble.util.ParsingUtils.split(ParsingUtils.java:266); at htsjdk.variant.vcf.AbstractVCFCodec.decodeLine(AbstractVCFCodec.java:375); at htsjdk.variant.vcf.AbstractVCFCodec.decode(AbstractVCFCodec.java:328); at htsjdk.variant.vcf.AbstractVCFCodec.decode(AbstractVCFCodec.java:48); at htsjdk.tribble.AsciiFeatureCodec.decode(AsciiFeatureCodec.java:70); at htsjdk.tribble.AsciiFeatureCodec.decode(AsciiFeatureCodec.java:37); at org.genomicsdb.reader.GenomicsDBFeatureIterator.next(GenomicsDBFeatureIterator.java:181); at org.genomicsdb.reader.GenomicsDBFeatureIterator.next(GenomicsDBFeatureIterator.java:49); at org.broadinstitute.hellbender.engine.FeatureIntervalIterator.loadNextFeature(FeatureIntervalIterator.java:98); at org.broadinstitute.hellbender.engine.FeatureIntervalIterator.loadNextNovelFeature(FeatureIntervalIterator.java:74); at org.broadinstitute.hellbender.engine.FeatureIntervalIterator.next(FeatureIntervalIterator.java:62); at org.broadinstitute.hellbender.engine.FeatureIntervalIterator.next(FeatureIntervalIterator.java:24); at java.util.Iterator.forEachRemaining(Iterator.java:116); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472) ; at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150) ; at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEachOrdered(ReferencePipeline.java:490) ; at org.broadinstitute.hellbender.engine.VariantLocusWalker.traverse(VariantLocusWalker.java:132); at org.broadinstitute.hellbender.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6275#issuecomment-574329688:3228,load,loadNextNovelFeature,3228,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6275#issuecomment-574329688,1,['load'],['loadNextNovelFeature']
Performance,"parison/113b01be-9124-41dd-acc0-5732ef2c7b38/call-BenchmarkVCFControlSample/Benchmark/7222f3cf-155c-423f-bc1e-8194e87ff05f/call-CombineSummaries/summary.csv"",; ""EXOME1 evalindelF1Score"": ""0.7573"",; ""EXOME1 evalindelPrecision"": ""0.6882"",; ""EXOME1 evalsnpF1Score"": ""0.9896"",; ""EXOME1 evalsnpPrecision"": ""0.9852"",; ""EXOME1 evalsnpRecall"": ""0.9941"",; ""EXOME1 evalsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/e372bd14-cd1f-4563-8d8a-abf6b6ca7883/call-EXOME1SampleHeadToHead/BenchmarkComparison/113b01be-9124-41dd-acc0-5732ef2c7b38/call-BenchmarkVCFTestSample/Benchmark/e929ad45-5026-4630-8b85-19f6205f068c/call-CombineSummaries/summary.csv"",; ""NIST controlHCprocesshours"": ""90.94291388888888"",; ""NIST controlHCsystemhours"": ""0.182125"",; ""NIST controlHCwallclockhours"": ""63.56370277777778"",; ""NIST controlHCwallclockmax"": ""3.701625"",; ""NIST controlMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/e372bd14-cd1f-4563-8d8a-abf6b6ca7883/call-NISTSampleHeadToHead/BenchmarkComparison/103cd89c-b177-4a0b-84fc-9553a1f8161f/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""NIST controlindelF1Score"": ""0.9843"",; ""NIST controlindelPrecision"": ""0.9895"",; ""NIST controlsnpF1Score"": ""0.9908"",; ""NIST controlsnpPrecision"": ""0.992"",; ""NIST controlsnpRecall"": ""0.9896"",; ""NIST controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/e372bd14-cd1f-4563-8d8a-abf6b6ca7883/call-NISTSampleHeadToHead/BenchmarkComparison/103cd89c-b177-4a0b-84fc-9553a1f8161f/call-BenchmarkVCFControlSample/Benchmark/eaf4d582-e197-4e13-8122-5e1ec22591ae/call-CombineSummaries/summary.csv"",; ""NIST evalHCprocesshours"": ""73.06777222222223"",; ""NIST evalHCsystemhours"": ""0.1622555555555555"",; ""NIST evalHCwallclockhours"": ""46.65241388888888"",; ""NIST evalHCwallclockmax"": ""2.7461055555555554"",; ""NIST evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/e372bd14-cd1f-4563-8d8a-ab",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7723#issuecomment-1069381494:13472,cache,cacheCopy,13472,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7723#issuecomment-1069381494,1,['cache'],['cacheCopy']
Performance,"parison/7b11647c-6643-4c47-8e1c-3f07bd97e371/call-BenchmarkVCFControlSample/Benchmark/086348b1-f09c-49b0-b830-587e28eec63d/call-CombineSummaries/summary.csv"",; ""EXOME1 evalindelF1Score"": ""0.7573"",; ""EXOME1 evalindelPrecision"": ""0.6882"",; ""EXOME1 evalsnpF1Score"": ""0.9896"",; ""EXOME1 evalsnpPrecision"": ""0.9852"",; ""EXOME1 evalsnpRecall"": ""0.9941"",; ""EXOME1 evalsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/bf86d5b6-04bd-4344-b4fc-8a1df66bb5d9/call-EXOME1SampleHeadToHead/BenchmarkComparison/7b11647c-6643-4c47-8e1c-3f07bd97e371/call-BenchmarkVCFTestSample/Benchmark/47d80f67-4375-460f-9ce0-8186eec9fe5b/call-CombineSummaries/summary.csv"",; ""NIST controlHCprocesshours"": ""90.94291388888888"",; ""NIST controlHCsystemhours"": ""0.182125"",; ""NIST controlHCwallclockhours"": ""63.56370277777778"",; ""NIST controlHCwallclockmax"": ""3.701625"",; ""NIST controlMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/bf86d5b6-04bd-4344-b4fc-8a1df66bb5d9/call-NISTSampleHeadToHead/BenchmarkComparison/8e62c1c2-cf9c-4530-846e-1e0d6c6d8acf/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""NIST controlindelF1Score"": ""0.9902"",; ""NIST controlindelPrecision"": ""0.9903"",; ""NIST controlsnpF1Score"": ""0.9899"",; ""NIST controlsnpPrecision"": ""0.9887"",; ""NIST controlsnpRecall"": ""0.9911"",; ""NIST controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/bf86d5b6-04bd-4344-b4fc-8a1df66bb5d9/call-NISTSampleHeadToHead/BenchmarkComparison/8e62c1c2-cf9c-4530-846e-1e0d6c6d8acf/call-BenchmarkVCFControlSample/Benchmark/e71074a5-27ad-4a8b-a533-cdc111c0374f/call-CombineSummaries/summary.csv"",; ""NIST evalHCprocesshours"": ""73.06777222222223"",; ""NIST evalHCsystemhours"": ""0.1622555555555555"",; ""NIST evalHCwallclockhours"": ""46.65241388888888"",; ""NIST evalHCwallclockmax"": ""2.7461055555555554"",; ""NIST evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/bf86d5b6-04bd-4344-b4fc-8",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7723#issuecomment-1069765064:13470,cache,cacheCopy,13470,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7723#issuecomment-1069765064,1,['cache'],['cacheCopy']
Performance,"park - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - Deflater: IntelDeflater; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - Inflater: IntelInflater; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - Initializing engine; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.varia.NullAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.varia.NullAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""NullAppender"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""or",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312229998:5014,load,loaded,5014,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312229998,1,['load'],['loaded']
Performance,park.api.java.JavaPairRDD$$anonfun$toScalaFunction2$1.apply(JavaPairRDD.scala:1037); at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157); at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157); at scala.collection.Iterator$class.foreach(Iterator.scala:891); at scala.collection.AbstractIterator.foreach(Iterator.scala:1334); at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1334); at scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214); at scala.collection.AbstractIterator.aggregate(Iterator.scala:1334); at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$26.apply(RDD.scala:1190); at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$26.apply(RDD.scala:1190); at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$27.apply(RDD.scala:1191); at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$27.apply(RDD.scala:1191); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346); at org.apache.spark.rdd.RDD.iterator(RDD.scala:310); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5854#issuecomment-808817724:2923,concurren,concurrent,2923,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5854#issuecomment-808817724,2,['concurren'],['concurrent']
Performance,"park.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. 02:12 DEBUG: [kryo] Write: WrappedArray([NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED, NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED]); 18/04/24 17:41:31 INFO TaskSetManager: Starting task 0.2 in stage 2.0 (TID 7, xx.xx.xx.24, executor 1, partition 0, PROCESS_LOCAL, 6010 bytes); 18/04/24 17:41:31 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.24:44322 (size: 6.4 KB, free: 366.3 MB); 18/04/24 17:41:49 INFO BlockManagerInfo: Added broadcast_0",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:29089,concurren,concurrent,29089,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,1,['concurren'],['concurrent']
Performance,"park.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. 02:34 DEBUG: [kryo] Write: WrappedArray([NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED, NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED]); 18/04/24 17:41:53 INFO TaskSetManager: Starting task 0.3 in stage 2.0 (TID 8, xx.xx.xx.xx, executor 3, partition 0, PROCESS_LOCAL, 6010 bytes); 18/04/24 17:41:53 INFO TaskSetManager: Lost task 1.1 in stage 2.0 (TID 6) on xx.xx.xx.24, executor 1: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn'",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:31788,concurren,concurrent,31788,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,1,['concurren'],['concurrent']
Performance,"park.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. 18/04/24 17:40:52 INFO TaskSetManager: Lost task 0.0 in stage 2.0 (TID 3) on xx.xx.xx.25, executor 2: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory)) [duplicate 1]; 01:33 DEBUG: [kryo] Write: WrappedArray([NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED, NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED]); 18/04/24 17:40:52 INFO TaskSetMana",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:25902,concurren,concurrent,25902,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,1,['concurren'],['concurrent']
Performance,"park.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. Driver stacktrace:; 18/04/24 17:42:02 INFO DAGScheduler: Job 2 failed: count at PathSeqPipelineSpark.java:245, took 117.869179 s; 18/04/24 17:42:02 INFO SparkUI: Stopped Spark web UI at http://xx.xx.xx.xx:4040; 18/04/24 17:42:02 INFO StandaloneSchedulerBackend: Shutting down all executors; 18/04/24 17:42:02 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down; 18/04/24 17:42:02 ERROR TransportRequestHandler: Error sending result StreamResponse{",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:35526,concurren,concurrent,35526,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,1,['concurren'],['concurrent']
Performance,park.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.forea,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:40722,concurren,concurrent,40722,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,1,['concurren'],['concurrent']
Performance,patch(ReflectionDispatch.java:36); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:33); 	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:94); 	at com.sun.proxy.$Proxy5.stop(Unknown Source); 	at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:132); 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.base/java.lang.reflect.Method.invoke(Method.java:566); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:182); 	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:164); 	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:412); 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64); 	at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56); 	at java.base/java.lang.Thread.run(Thread.java:834); ```. Unlikely to be related to this branch.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6652#issuecomment-672024253:5377,concurren,concurrent,5377,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6652#issuecomment-672024253,5,['concurren'],['concurrent']
Performance,performance point: on our cluster this gives ~900MB/second (on 60 spinning disks).,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1486#issuecomment-188412104:0,perform,performance,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1486#issuecomment-188412104,1,['perform'],['performance']
Performance,pi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:186); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:512); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:128); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:125); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:92); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:47); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:125); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:109); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:131); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:104); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: javax.net.ssl.SSLException: Connection has been shutdown: javax.net.ssl.SSLException: java.net.SocketException: Connection reset; 	at sun.security.ssl.SSLSocketImpl.checkEOF(SSLSocketImpl.java:1541); 	at sun.security.ssl.AppInputStream.available(AppInputStream.java:60); 	at java.io.BufferedInputStream.available(BufferedInputStream.java:410); 	at sun.net.www.MeteredStream.available(MeteredStream.java:170); 	at sun.net.www.http.KeepAliveStream.close(KeepAliveStream.java:85); 	at java.io.FilterInputStream.close(FilterInputStream.java:181); 	at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.close(HttpURLConnection.java:3448); 	at java.io.FilterInputStream.close(FilterInputStream.java:181); 	at shaded.cloud_nio.com.google.api.client.util.IOUtils.copy(IOUtils.java:97); 	at shade,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-301610931:6495,concurren,concurrent,6495,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-301610931,1,['concurren'],['concurrent']
Performance,"piens_assembly38.fasta"",; ""BenchmarkVCFsHeadToHeadOrchestrated.referenceVersion"": ""HG38"",; ""BenchmarkVCFsHeadToHeadOrchestrated.stratIntervals"": [; ""gs://dsp-methods-carrot-data/test_data/haplotypecaller_tests/HCR_hg38.bed"",; ""gs://dsp-methods-carrot-data/test_data/haplotypecaller_tests/LCR_Hg38.interval_list""; ],; ""BenchmarkVCFsHeadToHeadOrchestrated.stratLabels"": [; ""HCR"",; ""LCR""; ]; },; ""eval_options"": null,; ""test_cromwell_job_id"": ""050d2d6e-4a50-4145-a9da-8a39731ebdd2"",; ""eval_cromwell_job_id"": ""0e5c32ab-65e6-451f-a04e-6a3f5e7fe5c8"",; ""created_at"": ""2023-05-04T15:40:52.834692"",; ""created_by"": null,; ""finished_at"": ""2023-05-04T17:03:53.525"",; ""results"": {; ""CHM controlHCprocesshours"": ""75.88741944444445"",; ""CHM controlHCsystemhours"": ""0.1663777777777778"",; ""CHM controlHCwallclockhours"": ""52.24009722222222"",; ""CHM controlHCwallclockmax"": ""2.852152777777778"",; ""CHM controlMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/0e5c32ab-65e6-451f-a04e-6a3f5e7fe5c8/call-CHMSampleHeadToHead/BenchmarkComparison/a332776f-175a-4595-bdeb-ab62e7f89921/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""CHM controlindelF1Score"": ""0.8724"",; ""CHM controlindelPrecision"": ""0.8814"",; ""CHM controlsnpF1Score"": ""0.9784"",; ""CHM controlsnpPrecision"": ""0.9706"",; ""CHM controlsnpRecall"": ""0.9863"",; ""CHM controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/0e5c32ab-65e6-451f-a04e-6a3f5e7fe5c8/call-CHMSampleHeadToHead/BenchmarkComparison/a332776f-175a-4595-bdeb-ab62e7f89921/call-BenchmarkVCFControlSample/Benchmark/06cbfab4-17a7-4415-9118-d0ebbe156bfd/call-CombineSummaries/summary.csv"",; ""CHM evalHCprocesshours"": ""84.26158888888888"",; ""CHM evalHCsystemhours"": ""0.19243055555555555"",; ""CHM evalHCwallclockhours"": ""60.242008333333345"",; ""CHM evalHCwallclockmax"": ""3.176513888888889"",; ""CHM evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/0e5c32ab-65e6-451f-a04e-6a3f5e7f",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1535104202:17381,cache,cacheCopy,17381,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1535104202,1,['cache'],['cacheCopy']
Performance,"piens_assembly38.fasta"",; ""BenchmarkVCFsHeadToHeadOrchestrated.referenceVersion"": ""HG38"",; ""BenchmarkVCFsHeadToHeadOrchestrated.stratIntervals"": [; ""gs://dsp-methods-carrot-data/test_data/haplotypecaller_tests/HCR_hg38.bed"",; ""gs://dsp-methods-carrot-data/test_data/haplotypecaller_tests/LCR_Hg38.interval_list""; ],; ""BenchmarkVCFsHeadToHeadOrchestrated.stratLabels"": [; ""HCR"",; ""LCR""; ]; },; ""eval_options"": null,; ""test_cromwell_job_id"": ""07271d7b-729d-4db9-862d-5f992a60a598"",; ""eval_cromwell_job_id"": ""89508d5f-29f1-4534-9fe1-220a80de17c4"",; ""created_at"": ""2022-07-22T17:23:11.546971"",; ""created_by"": null,; ""finished_at"": ""2022-07-23T02:09:23.327"",; ""results"": {; ""CHM controlHCprocesshours"": ""75.88741944444445"",; ""CHM controlHCsystemhours"": ""0.1663777777777778"",; ""CHM controlHCwallclockhours"": ""52.24009722222222"",; ""CHM controlHCwallclockmax"": ""2.852152777777778"",; ""CHM controlMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/89508d5f-29f1-4534-9fe1-220a80de17c4/call-CHMSampleHeadToHead/BenchmarkComparison/a2a2515a-b32a-44a6-a6d1-9a6d0d2199bb/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""CHM controlindelF1Score"": ""0.8724"",; ""CHM controlindelPrecision"": ""0.8814"",; ""CHM controlsnpF1Score"": ""0.9784"",; ""CHM controlsnpPrecision"": ""0.9706"",; ""CHM controlsnpRecall"": ""0.9863"",; ""CHM controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/89508d5f-29f1-4534-9fe1-220a80de17c4/call-CHMSampleHeadToHead/BenchmarkComparison/a2a2515a-b32a-44a6-a6d1-9a6d0d2199bb/call-BenchmarkVCFControlSample/Benchmark/2c4ad666-e885-4e23-bd5c-d54ca521ffbf/call-CombineSummaries/summary.csv"",; ""CHM evalHCprocesshours"": ""78.99195555555558"",; ""CHM evalHCsystemhours"": ""0.16168333333333337"",; ""CHM evalHCwallclockhours"": ""55.43875833333334"",; ""CHM evalHCwallclockmax"": ""2.913311111111111"",; ""CHM evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/89508d5f-29f1-4534-9fe1-220a80de1",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7876#issuecomment-1193038382:16685,cache,cacheCopy,16685,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7876#issuecomment-1193038382,1,['cache'],['cacheCopy']
Performance,"piens_assembly38.fasta"",; ""BenchmarkVCFsHeadToHeadOrchestrated.referenceVersion"": ""HG38"",; ""BenchmarkVCFsHeadToHeadOrchestrated.stratIntervals"": [; ""gs://dsp-methods-carrot-data/test_data/haplotypecaller_tests/HCR_hg38.bed"",; ""gs://dsp-methods-carrot-data/test_data/haplotypecaller_tests/LCR_Hg38.interval_list""; ],; ""BenchmarkVCFsHeadToHeadOrchestrated.stratLabels"": [; ""HCR"",; ""LCR""; ]; },; ""eval_options"": null,; ""test_cromwell_job_id"": ""410a88f6-62ca-4745-89fd-df6e30aac65b"",; ""eval_cromwell_job_id"": ""bf86d5b6-04bd-4344-b4fc-8a1df66bb5d9"",; ""created_at"": ""2022-03-16T19:53:45.833854"",; ""created_by"": null,; ""finished_at"": ""2022-03-17T00:11:38.702"",; ""results"": {; ""CHM controlHCprocesshours"": ""75.88741944444445"",; ""CHM controlHCsystemhours"": ""0.1663777777777778"",; ""CHM controlHCwallclockhours"": ""52.24009722222222"",; ""CHM controlHCwallclockmax"": ""2.852152777777778"",; ""CHM controlMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/bf86d5b6-04bd-4344-b4fc-8a1df66bb5d9/call-CHMSampleHeadToHead/BenchmarkComparison/79d1a2a4-6b5e-424a-8528-9059bda6db1c/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""CHM controlindelF1Score"": ""0.8724"",; ""CHM controlindelPrecision"": ""0.8814"",; ""CHM controlsnpF1Score"": ""0.9784"",; ""CHM controlsnpPrecision"": ""0.9706"",; ""CHM controlsnpRecall"": ""0.9863"",; ""CHM controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/bf86d5b6-04bd-4344-b4fc-8a1df66bb5d9/call-CHMSampleHeadToHead/BenchmarkComparison/79d1a2a4-6b5e-424a-8528-9059bda6db1c/call-BenchmarkVCFControlSample/Benchmark/3046acf7-ded7-40c8-9b7a-3826f480418f/call-CombineSummaries/summary.csv"",; ""CHM evalHCprocesshours"": ""67.35536666666667"",; ""CHM evalHCsystemhours"": ""0.1557166666666667"",; ""CHM evalHCwallclockhours"": ""42.53388888888889"",; ""CHM evalHCwallclockmax"": ""2.7197444444444443"",; ""CHM evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/bf86d5b6-04bd-4344-b4fc-8a1df66bb",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7723#issuecomment-1069765064:10462,cache,cacheCopy,10462,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7723#issuecomment-1069765064,1,['cache'],['cacheCopy']
Performance,"piens_assembly38.fasta"",; ""BenchmarkVCFsHeadToHeadOrchestrated.referenceVersion"": ""HG38"",; ""BenchmarkVCFsHeadToHeadOrchestrated.stratIntervals"": [; ""gs://dsp-methods-carrot-data/test_data/haplotypecaller_tests/HCR_hg38.bed"",; ""gs://dsp-methods-carrot-data/test_data/haplotypecaller_tests/LCR_Hg38.interval_list""; ],; ""BenchmarkVCFsHeadToHeadOrchestrated.stratLabels"": [; ""HCR"",; ""LCR""; ]; },; ""eval_options"": null,; ""test_cromwell_job_id"": ""54997ade-421d-439f-acc9-abf50b3f9cb5"",; ""eval_cromwell_job_id"": ""6ea2705f-a3fa-41fc-8d17-a2c55d875eab"",; ""created_at"": ""2022-03-16T19:52:46.276978"",; ""created_by"": null,; ""finished_at"": ""2022-03-17T00:13:17.198"",; ""results"": {; ""CHM controlHCprocesshours"": ""75.88741944444445"",; ""CHM controlHCsystemhours"": ""0.1663777777777778"",; ""CHM controlHCwallclockhours"": ""52.24009722222222"",; ""CHM controlHCwallclockmax"": ""2.852152777777778"",; ""CHM controlMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/6ea2705f-a3fa-41fc-8d17-a2c55d875eab/call-CHMSampleHeadToHead/BenchmarkComparison/1fb97a8b-caee-4184-8e36-be21e6c43549/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""CHM controlindelF1Score"": ""0.8724"",; ""CHM controlindelPrecision"": ""0.8814"",; ""CHM controlsnpF1Score"": ""0.9784"",; ""CHM controlsnpPrecision"": ""0.9706"",; ""CHM controlsnpRecall"": ""0.9863"",; ""CHM controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/6ea2705f-a3fa-41fc-8d17-a2c55d875eab/call-CHMSampleHeadToHead/BenchmarkComparison/1fb97a8b-caee-4184-8e36-be21e6c43549/call-BenchmarkVCFControlSample/Benchmark/3b068fb2-7140-4c1e-8860-df8df21821ec/call-CombineSummaries/summary.csv"",; ""CHM evalHCprocesshours"": ""80.5165222222222"",; ""CHM evalHCsystemhours"": ""0.1713305555555555"",; ""CHM evalHCwallclockhours"": ""53.10978888888891"",; ""CHM evalHCwallclockmax"": ""2.7458416666666667"",; ""CHM evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/6ea2705f-a3fa-41fc-8d17-a2c55d875e",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7651#issuecomment-1069766207:10462,cache,cacheCopy,10462,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7651#issuecomment-1069766207,1,['cache'],['cacheCopy']
Performance,"piens_assembly38.fasta"",; ""BenchmarkVCFsHeadToHeadOrchestrated.referenceVersion"": ""HG38"",; ""BenchmarkVCFsHeadToHeadOrchestrated.stratIntervals"": [; ""gs://dsp-methods-carrot-data/test_data/haplotypecaller_tests/HCR_hg38.bed"",; ""gs://dsp-methods-carrot-data/test_data/haplotypecaller_tests/LCR_Hg38.interval_list""; ],; ""BenchmarkVCFsHeadToHeadOrchestrated.stratLabels"": [; ""HCR"",; ""LCR""; ]; },; ""eval_options"": null,; ""test_cromwell_job_id"": ""5e9a598e-1e80-4622-b153-78e97491a478"",; ""eval_cromwell_job_id"": ""f7eac327-c59c-43f7-a850-21bc3e0ccf52"",; ""created_at"": ""2022-07-12T17:28:58.385152"",; ""created_by"": null,; ""finished_at"": ""2022-07-13T02:47:47.016"",; ""results"": {; ""CHM controlHCprocesshours"": ""75.88741944444445"",; ""CHM controlHCsystemhours"": ""0.1663777777777778"",; ""CHM controlHCwallclockhours"": ""52.24009722222222"",; ""CHM controlHCwallclockmax"": ""2.852152777777778"",; ""CHM controlMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/f7eac327-c59c-43f7-a850-21bc3e0ccf52/call-CHMSampleHeadToHead/BenchmarkComparison/cd28fe49-1672-4321-a836-47f76419c1c8/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""CHM controlindelF1Score"": ""0.8724"",; ""CHM controlindelPrecision"": ""0.8814"",; ""CHM controlsnpF1Score"": ""0.9784"",; ""CHM controlsnpPrecision"": ""0.9706"",; ""CHM controlsnpRecall"": ""0.9863"",; ""CHM controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/f7eac327-c59c-43f7-a850-21bc3e0ccf52/call-CHMSampleHeadToHead/BenchmarkComparison/cd28fe49-1672-4321-a836-47f76419c1c8/call-BenchmarkVCFControlSample/Benchmark/d5df8455-36cf-4ecb-8dc2-ec35b974c0b7/call-CombineSummaries/summary.csv"",; ""CHM evalHCprocesshours"": ""78.23616944444446"",; ""CHM evalHCsystemhours"": ""0.16188333333333332"",; ""CHM evalHCwallclockhours"": ""55.167422222222214"",; ""CHM evalHCwallclockmax"": ""2.887522222222222"",; ""CHM evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/f7eac327-c59c-43f7-a850-21bc3e0c",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7876#issuecomment-1182703672:16697,cache,cacheCopy,16697,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7876#issuecomment-1182703672,1,['cache'],['cacheCopy']
Performance,"piens_assembly38.fasta"",; ""BenchmarkVCFsHeadToHeadOrchestrated.referenceVersion"": ""HG38"",; ""BenchmarkVCFsHeadToHeadOrchestrated.stratIntervals"": [; ""gs://dsp-methods-carrot-data/test_data/haplotypecaller_tests/HCR_hg38.bed"",; ""gs://dsp-methods-carrot-data/test_data/haplotypecaller_tests/LCR_Hg38.interval_list""; ],; ""BenchmarkVCFsHeadToHeadOrchestrated.stratLabels"": [; ""HCR"",; ""LCR""; ]; },; ""eval_options"": null,; ""test_cromwell_job_id"": ""5f0f8f34-cdc7-46ff-a59d-2368edcdf007"",; ""eval_cromwell_job_id"": ""e6f57e40-2025-46fd-9aa0-d591a3799007"",; ""created_at"": ""2022-03-16T14:20:46.087600"",; ""created_by"": null,; ""finished_at"": ""2022-03-16T17:21:08.639"",; ""results"": {; ""CHM controlHCprocesshours"": ""75.88741944444445"",; ""CHM controlHCsystemhours"": ""0.1663777777777778"",; ""CHM controlHCwallclockhours"": ""52.24009722222222"",; ""CHM controlHCwallclockmax"": ""2.852152777777778"",; ""CHM controlMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/e6f57e40-2025-46fd-9aa0-d591a3799007/call-CHMSampleHeadToHead/BenchmarkComparison/f65a7960-7b66-4a5d-a346-bd188a1b3830/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""CHM controlindelF1Score"": ""0.8724"",; ""CHM controlindelPrecision"": ""0.8814"",; ""CHM controlsnpF1Score"": ""0.9784"",; ""CHM controlsnpPrecision"": ""0.9706"",; ""CHM controlsnpRecall"": ""0.9863"",; ""CHM controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/e6f57e40-2025-46fd-9aa0-d591a3799007/call-CHMSampleHeadToHead/BenchmarkComparison/f65a7960-7b66-4a5d-a346-bd188a1b3830/call-BenchmarkVCFControlSample/Benchmark/8d0e47ca-66f5-42a0-8785-6aa8d2db2663/call-CombineSummaries/summary.csv"",; ""CHM evalHCprocesshours"": ""80.5165222222222"",; ""CHM evalHCsystemhours"": ""0.1713305555555555"",; ""CHM evalHCwallclockhours"": ""53.10978888888891"",; ""CHM evalHCwallclockmax"": ""2.7458416666666667"",; ""CHM evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/e6f57e40-2025-46fd-9aa0-d591a37990",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7651#issuecomment-1069378815:10462,cache,cacheCopy,10462,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7651#issuecomment-1069378815,1,['cache'],['cacheCopy']
Performance,"piens_assembly38.fasta"",; ""BenchmarkVCFsHeadToHeadOrchestrated.referenceVersion"": ""HG38"",; ""BenchmarkVCFsHeadToHeadOrchestrated.stratIntervals"": [; ""gs://dsp-methods-carrot-data/test_data/haplotypecaller_tests/HCR_hg38.bed"",; ""gs://dsp-methods-carrot-data/test_data/haplotypecaller_tests/LCR_Hg38.interval_list""; ],; ""BenchmarkVCFsHeadToHeadOrchestrated.stratLabels"": [; ""HCR"",; ""LCR""; ]; },; ""eval_options"": null,; ""test_cromwell_job_id"": ""9886a710-334a-41eb-a495-6968d322730a"",; ""eval_cromwell_job_id"": ""9bc521dc-3c4c-4274-972c-9d1e4be850d5"",; ""created_at"": ""2023-05-03T15:51:41.295461"",; ""created_by"": null,; ""finished_at"": ""2023-05-04T01:24:02.606"",; ""results"": {; ""CHM controlHCprocesshours"": ""75.88741944444445"",; ""CHM controlHCsystemhours"": ""0.1663777777777778"",; ""CHM controlHCwallclockhours"": ""52.24009722222222"",; ""CHM controlHCwallclockmax"": ""2.852152777777778"",; ""CHM controlMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/9bc521dc-3c4c-4274-972c-9d1e4be850d5/call-CHMSampleHeadToHead/BenchmarkComparison/092bfb4f-d978-4964-a8ae-e5a7f7362f7c/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""CHM controlindelF1Score"": ""0.8724"",; ""CHM controlindelPrecision"": ""0.8814"",; ""CHM controlsnpF1Score"": ""0.9784"",; ""CHM controlsnpPrecision"": ""0.9706"",; ""CHM controlsnpRecall"": ""0.9863"",; ""CHM controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/9bc521dc-3c4c-4274-972c-9d1e4be850d5/call-CHMSampleHeadToHead/BenchmarkComparison/092bfb4f-d978-4964-a8ae-e5a7f7362f7c/call-BenchmarkVCFControlSample/Benchmark/6ab078fb-b668-452c-bbaa-8fb1fd8e25ba/call-CombineSummaries/summary.csv"",; ""CHM evalHCprocesshours"": ""84.26158888888888"",; ""CHM evalHCsystemhours"": ""0.19243055555555555"",; ""CHM evalHCwallclockhours"": ""60.242008333333345"",; ""CHM evalHCwallclockmax"": ""3.176513888888889"",; ""CHM evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/9bc521dc-3c4c-4274-972c-9d1e4be8",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1533946590:17357,cache,cacheCopy,17357,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1533946590,1,['cache'],['cacheCopy']
Performance,"piens_assembly38.fasta"",; ""BenchmarkVCFsHeadToHeadOrchestrated.referenceVersion"": ""HG38"",; ""BenchmarkVCFsHeadToHeadOrchestrated.stratIntervals"": [; ""gs://dsp-methods-carrot-data/test_data/haplotypecaller_tests/HCR_hg38.bed"",; ""gs://dsp-methods-carrot-data/test_data/haplotypecaller_tests/LCR_Hg38.interval_list""; ],; ""BenchmarkVCFsHeadToHeadOrchestrated.stratLabels"": [; ""HCR"",; ""LCR""; ]; },; ""eval_options"": null,; ""test_cromwell_job_id"": ""a8ee297d-9fd6-433f-ac22-14488a09b832"",; ""eval_cromwell_job_id"": ""2a8ce326-baa5-4052-bff9-bd684393ff6c"",; ""created_at"": ""2022-07-25T15:10:00.795227"",; ""created_by"": null,; ""finished_at"": ""2022-07-26T00:11:26.646"",; ""results"": {; ""CHM controlHCprocesshours"": ""75.88741944444445"",; ""CHM controlHCsystemhours"": ""0.1663777777777778"",; ""CHM controlHCwallclockhours"": ""52.24009722222222"",; ""CHM controlHCwallclockmax"": ""2.852152777777778"",; ""CHM controlMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/2a8ce326-baa5-4052-bff9-bd684393ff6c/call-CHMSampleHeadToHead/BenchmarkComparison/a1db35b8-cc7b-4019-bdd0-9f423762542e/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""CHM controlindelF1Score"": ""0.8724"",; ""CHM controlindelPrecision"": ""0.8814"",; ""CHM controlsnpF1Score"": ""0.9784"",; ""CHM controlsnpPrecision"": ""0.9706"",; ""CHM controlsnpRecall"": ""0.9863"",; ""CHM controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/2a8ce326-baa5-4052-bff9-bd684393ff6c/call-CHMSampleHeadToHead/BenchmarkComparison/a1db35b8-cc7b-4019-bdd0-9f423762542e/call-BenchmarkVCFControlSample/Benchmark/7195c554-534f-43ef-80c2-77bdafa1827f/call-CombineSummaries/summary.csv"",; ""CHM evalHCprocesshours"": ""78.10181666666668"",; ""CHM evalHCsystemhours"": ""0.16157500000000005"",; ""CHM evalHCwallclockhours"": ""55.006172222222226"",; ""CHM evalHCwallclockmax"": ""2.8554194444444443"",; ""CHM evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/2a8ce326-baa5-4052-bff9-bd68439",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7876#issuecomment-1194801748:16697,cache,cacheCopy,16697,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7876#issuecomment-1194801748,1,['cache'],['cacheCopy']
Performance,"piens_assembly38.fasta"",; ""BenchmarkVCFsHeadToHeadOrchestrated.referenceVersion"": ""HG38"",; ""BenchmarkVCFsHeadToHeadOrchestrated.stratIntervals"": [; ""gs://dsp-methods-carrot-data/test_data/haplotypecaller_tests/HCR_hg38.bed"",; ""gs://dsp-methods-carrot-data/test_data/haplotypecaller_tests/LCR_Hg38.interval_list""; ],; ""BenchmarkVCFsHeadToHeadOrchestrated.stratLabels"": [; ""HCR"",; ""LCR""; ]; },; ""eval_options"": null,; ""test_cromwell_job_id"": ""d6f96a63-9657-4ff6-9934-fe1ab3cea617"",; ""eval_cromwell_job_id"": ""e372bd14-cd1f-4563-8d8a-abf6b6ca7883"",; ""created_at"": ""2022-03-16T14:19:54.192086"",; ""created_by"": null,; ""finished_at"": ""2022-03-16T17:26:08.529"",; ""results"": {; ""CHM controlHCprocesshours"": ""75.88741944444445"",; ""CHM controlHCsystemhours"": ""0.1663777777777778"",; ""CHM controlHCwallclockhours"": ""52.24009722222222"",; ""CHM controlHCwallclockmax"": ""2.852152777777778"",; ""CHM controlMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/e372bd14-cd1f-4563-8d8a-abf6b6ca7883/call-CHMSampleHeadToHead/BenchmarkComparison/7ff0db7c-0871-4cda-95f3-fa75436cbb21/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""CHM controlindelF1Score"": ""0.8778"",; ""CHM controlindelPrecision"": ""0.8968"",; ""CHM controlsnpF1Score"": ""0.9813"",; ""CHM controlsnpPrecision"": ""0.9774"",; ""CHM controlsnpRecall"": ""0.9852"",; ""CHM controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/e372bd14-cd1f-4563-8d8a-abf6b6ca7883/call-CHMSampleHeadToHead/BenchmarkComparison/7ff0db7c-0871-4cda-95f3-fa75436cbb21/call-BenchmarkVCFControlSample/Benchmark/16cd1efe-5cea-403e-8e85-aec15e71bd1d/call-CombineSummaries/summary.csv"",; ""CHM evalHCprocesshours"": ""67.35536666666667"",; ""CHM evalHCsystemhours"": ""0.1557166666666667"",; ""CHM evalHCwallclockhours"": ""42.53388888888889"",; ""CHM evalHCwallclockmax"": ""2.7197444444444443"",; ""CHM evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/e372bd14-cd1f-4563-8d8a-abf6b6ca7",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7723#issuecomment-1069381494:10462,cache,cacheCopy,10462,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7723#issuecomment-1069381494,1,['cache'],['cacheCopy']
Performance,"pled solution. And this workflow is clearly marked as an unsupported prototype anyway (as are the GATK CLIs). I want to emphasize that this whole workflow is not a long-term solution. In other words, I would like to get this in and then focus on a supported solution. Two comments: ; > If these events were indeed not CNLOH, as we discussed, then I don't think we should merge this. Perhaps we should take a step back and answer definitively whether simply blacklisting common germline regions is enough to replicate/obviate most of the postprocessing. Should be straightforward to run an evaluation with and without blacklisting---and hopefully our truth data accurately reflects whether blacklisting is desirable. There are definitely events that get missed without the germline tagging, so this is an improvement over blacklisting alone. And while I have seen erroneous germline tagging (i.e. false calling a segment germline), it was only ever due to really noisy data (e.g. a bad PoN) or a poorly tuned segment caller. I am pretty sure that most common germline regions are being blacklisted already. The hotspots addressed in this PR (faux-CNLoH) could be added, but I think we will find new areas and a few of these areas were rather big. I have users that are actively using this from the branch, for reasons other than the faux-CNLoH pruning. Results are improving without an appreciable hit to sensitivity, which we got when using parameters like num_changepoints_penalty_factor. As a compromise, I can always default the CNLoH piece to `false`, since there are other useful changes on this branch. (Users did not have as strong an opinion about the faux-CNLoH pruning, since GISTIC does not use MAF and ABSOLUTE requires a manual review). > simple filtering based on CR-AF as described above could be implemented. If the normal is available, we can make IS_NORMAL calls simply based on the overlap of the ModelSegments posteriors (with corresponding qualities). If not, then some heuristic ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5450#issuecomment-461258874:1512,tune,tuned,1512,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5450#issuecomment-461258874,1,['tune'],['tuned']
Performance,ply$23.apply(RDD.scala:801); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:121); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at scala.Opti,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6513#issuecomment-601702690:6843,concurren,concurrent,6843,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6513#issuecomment-601702690,1,['concurren'],['concurrent']
Performance,possibly relate to this warning:. ```; 17/01/09 21:57:53 INFO com.google.cloud.genomics.dataflow.readers.bam.BAMIO: getReadsFromBAMFile - got input resources; 17/01/09 21:57:54 INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat: Total input paths to process : 1; 17/01/09 22:01:44 WARN com.google.cloud.hadoop.gcsio.FileSystemBackedDirectoryListCache: Got null fileList for listBaseFile '/hadoop_gcs_connector_metadata_cache/hellbender/test/output/gatk4-spark/recalibrated.bam' even though exists() was true!; 17/01/09 22:01:45 INFO com.google.cloud.hadoop.gcsio.CacheSupplementedGoogleCloudStorage: Populating missing itemInfo on-demand for entry: gs://hellbender/test/output/gatk4-spark/recalibrated.bam; 17/01/09 22:01:45 WARN com.google.cloud.hadoop.gcsio.CacheSupplementedGoogleCloudStorage: Possible stale CacheEntry; failed to fetch item info for: gs://hellbender/test/output/gatk4-spark/recalibrated.bam - removing from cache; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2306#issuecomment-271421976:571,Cache,CacheSupplementedGoogleCloudStorage,571,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2306#issuecomment-271421976,4,"['Cache', 'cache']","['CacheEntry', 'CacheSupplementedGoogleCloudStorage', 'cache']"
Performance,"r definitively whether simply blacklisting common germline regions is enough to replicate/obviate most of the postprocessing. Should be straightforward to run an evaluation with and without blacklisting---and hopefully our truth data accurately reflects whether blacklisting is desirable. There are definitely events that get missed without the germline tagging, so this is an improvement over blacklisting alone. And while I have seen erroneous germline tagging (i.e. false calling a segment germline), it was only ever due to really noisy data (e.g. a bad PoN) or a poorly tuned segment caller. I am pretty sure that most common germline regions are being blacklisted already. The hotspots addressed in this PR (faux-CNLoH) could be added, but I think we will find new areas and a few of these areas were rather big. I have users that are actively using this from the branch, for reasons other than the faux-CNLoH pruning. Results are improving without an appreciable hit to sensitivity, which we got when using parameters like num_changepoints_penalty_factor. As a compromise, I can always default the CNLoH piece to `false`, since there are other useful changes on this branch. (Users did not have as strong an opinion about the faux-CNLoH pruning, since GISTIC does not use MAF and ABSOLUTE requires a manual review). > simple filtering based on CR-AF as described above could be implemented. If the normal is available, we can make IS_NORMAL calls simply based on the overlap of the ModelSegments posteriors (with corresponding qualities). If not, then some heuristic determination of the normal state from the tumor alone as in Marton's caller could be performed. This would combine the IS_NORMAL calling and filtering steps into one simple tool. The output could be a tagged/filtered ModelSegments .seg file and the corresponding VCF. And this would be a possible ""better solution"" Shall I file an issue for this? This could also allow us to obviate the TagGermline tool, which is fine by me.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5450#issuecomment-461258874:2597,perform,performed,2597,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5450#issuecomment-461258874,1,['perform'],['performed']
Performance,"r$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.varia.NullAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.varia.NullAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""NullAppender"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.varia.NullAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.varia.NullAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""NullAppender"".; log4j:ERROR A ""org.apache.log4j.varia.NullAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.varia.NullAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could no",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312229998:6057,load,loaded,6057,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312229998,1,['load'],['loaded']
Performance,"r,Description=""Phasing set (typically the position of the first variant in the set)"">; ##FORMAT=<ID=SB,Number=4,Type=Integer,Description=""Per-sample component statistics which comprise the Fisher's Exact Test to detect strand bias."">; ##GATKCommandLine=<ID=HaplotypeCaller,CommandLine=""HaplotypeCaller --emit-ref-confidence GVCF --output CMC_C_1.g.vcf --input CMC_C_1.sorted.markdup.addRG.bam --reference kxc_hic_final.fast; a --use-posteriors-to-calculate-qual false --dont-use-dragstr-priors false --use-new-qual-calculator true --annotate-with-num-discovered-alleles false --heterozygosity 0.001 --indel-hetero; zygosity 1.25E-4 --heterozygosity-stdev 0.01 --standard-min-confidence-threshold-for-calling 30.0 --max-alternate-alleles 6 --max-genotype-count 1024 --sample-ploidy 2 --num-reference-samp; les-if-no-call 0 --genotype-assignment-method USE_PLS_TO_ASSIGN --contamination-fraction-to-filter 0.0 --output-mode EMIT_VARIANTS_ONLY --all-site-pls false --flow-likelihood-parallel-thre; ads 0 --flow-likelihood-optimized-comp false --flow-use-t0-tag false --flow-probability-threshold 0.003 --flow-remove-non-single-base-pair-indels false --flow-remove-one-zero-probs false -; -flow-quantization-bins 121 --flow-fill-empty-bins-value 0.001 --flow-symmetric-indel-probs false --flow-report-insertion-or-deletion false --flow-disallow-probs-larger-than-call false --f; low-lump-probs false --flow-retain-max-n-probs-base-format false --flow-probability-scaling-factor 10 --flow-order-cycle-length 4 --flow-number-of-uncertain-flows-to-clip 0 --flow-nucleoti; de-of-first-uncertain-flow T --keep-boundary-flows false --gvcf-gq-bands 1 --gvcf-gq-bands 2 --gvcf-gq-bands 3 --gvcf-gq-bands 4 --gvcf-gq-bands 5 --gvcf-gq-bands 6 --gvcf-gq-bands 7 --gvc; f-gq-bands 8 --gvcf-gq-bands 9 --gvcf-gq-bands 10 --gvcf-gq-bands 11 --gvcf-gq-bands 12 --gvcf-gq-bands 13 --gvcf-gq-bands 14 --gvcf-gq-bands 15 --gvcf-gq-bands 16 --gvcf-gq-bands 17 --gvc; f-gq-bands 18 --gvcf-gq-bands 19 --gvcf-gq-bands 20 --gv",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8574#issuecomment-1793390789:2445,optimiz,optimized-comp,2445,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8574#issuecomment-1793390789,1,['optimiz'],['optimized-comp']
Performance,"r.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 2019-01-07 11:34:07 INFO TaskSetManager:54 - Starting task 1.1 in stage 0.0 (TID 3, scc-q21.scc.bu.edu, executor 1, partition 1, NODE_LOCAL, 7992 bytes); 2019-01-07 11:34:07 WARN TaskSetManager:66 - Lost task 3.0 in stage 0.0 (TID 2, scc-q21.scc.bu.edu, executor 1): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 1, start 4924320, span 190238, expected MD5 8a9ef2f91a78ffdc56561ece832e9f5d; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at sc",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:25072,concurren,concurrent,25072,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,1,['concurren'],['concurrent']
Performance,"r.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 2019-01-07 11:34:08 INFO TaskSetManager:54 - Starting task 3.1 in stage 0.0 (TID 4, scc-q21.scc.bu.edu, executor 1, partition 3, NODE_LOCAL, 7992 bytes); 2019-01-07 11:34:08 INFO TaskSetManager:54 - Lost task 1.1 in stage 0.0 (TID 3) on scc-q21.scc.bu.edu, executor 1: htsjdk.samtools.cram.CRAMException (Reference sequence MD5 mismatch for slice: sequence id 0, start 87545719, span 186383, expected MD5 492a29f6d7d6fcaf8dde06834861e7ae) [duplicate 1]; 2019-01-07 11:34:09 INFO TaskSetManager:54 - Starting task 9.0 in stage 0.0 (TID 5, scc-q12.scc.bu.edu, executor 2, partition 9, NODE_LOCAL, 7992 bytes); 2019-01-07 11:34:09 WARN TaskSetManager:66 - Lost task 2.0 in stage 0.0 (TID 0, scc-q12.scc.bu.edu, executor 2): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 0, start 160972515, span 170618, expected MD5 0cc5e1f5ec5c1b06d5a4bd5fff11b77f; a",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:26807,concurren,concurrent,26807,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,1,['concurren'],['concurrent']
Performance,"r.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 2019-01-07 11:34:09 INFO TaskSetManager:54 - Starting task 1.2 in stage 0.0 (TID 6, scc-q21.scc.bu.edu, executor 1, partition 1, NODE_LOCAL, 7992 bytes); 2019-01-07 11:34:09 INFO TaskSetManager:54 - Lost task 3.1 in stage 0.0 (TID 4) on scc-q21.scc.bu.edu, executor 1: htsjdk.samtools.cram.CRAMException (Reference sequence MD5 mismatch for slice: sequence id 1, start 4924320, span 190238, expected MD5 8a9ef2f91a78ffdc56561ece832e9f5d) [duplicate 1]; 2019-01-07 11:34:10 INFO TaskSetManager:54 - Starting task 2.1 in stage 0.0 (TID 7, scc-q12.scc.bu.edu, executor 2, partition 2, NODE_LOCAL, 7992 bytes); 2019-01-07 11:34:10 WARN TaskSetManager:66 - Lost task 9.0 in stage 0.0 (TID 5, scc-q12.scc.bu.edu, executor 2): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 3, start 97885291, span 192458, expected MD5 ef90368731b6e0be845bc82cd92b0c6a; at ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:28998,concurren,concurrent,28998,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,1,['concurren'],['concurrent']
Performance,"r.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 2019-01-07 11:34:10 INFO TaskSetManager:54 - Starting task 3.2 in stage 0.0 (TID 8, scc-q21.scc.bu.edu, executor 1, partition 3, NODE_LOCAL, 7992 bytes); 2019-01-07 11:34:10 INFO TaskSetManager:54 - Lost task 1.2 in stage 0.0 (TID 6) on scc-q21.scc.bu.edu, executor 1: htsjdk.samtools.cram.CRAMException (Reference sequence MD5 mismatch for slice: sequence id 0, start 87545719, span 186383, expected MD5 492a29f6d7d6fcaf8dde06834861e7ae) [duplicate 2]; 2019-01-07 11:34:11 INFO TaskSetManager:54 - Starting task 1.3 in stage 0.0 (TID 9, scc-q21.scc.bu.edu, executor 1, partition 1, NODE_LOCAL, 7992 bytes); 2019-01-07 11:34:11 INFO TaskSetManager:54 - Lost task 3.2 in stage 0.0 (TID 8) on scc-q21.scc.bu.edu, executor 1: htsjdk.samtools.cram.CRAMException (Reference sequence MD5 mismatch for slice: sequence id 1, start 4924320, span 190238, expected MD5 8a9ef2f91a78ffdc56561ece832e9f5d) [",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:31187,concurren,concurrent,31187,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,1,['concurren'],['concurrent']
Performance,"r.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 2019-01-09 13:35:49 INFO TaskSetManager:54 - Starting task 7.0 in stage 0.0 (TID 3, scc-q01.scc.bu.edu, executor 1, partition 7, NODE_LOCAL, 7992 bytes); 2019-01-09 13:35:49 WARN TaskSetManager:66 - Lost task 2.0 in stage 0.0 (TID 1, scc-q01.scc.bu.edu, executor 1): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 0, start 160972515, span 170618, expected MD5 0cc5e1f5ec5c1b06d5a4bd5fff11b77f; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:24359,concurren,concurrent,24359,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,1,['concurren'],['concurrent']
Performance,"r.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 2019-01-09 13:35:50 INFO TaskSetManager:54 - Starting task 1.1 in stage 0.0 (TID 4, scc-q20.scc.bu.edu, executor 2, partition 1, NODE_LOCAL, 7992 bytes); 2019-01-09 13:35:50 WARN TaskSetManager:66 - Lost task 4.0 in stage 0.0 (TID 2, scc-q20.scc.bu.edu, executor 2): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 1, start 93925364, span 266689, expected MD5 54babf05a23e9e88a8738dfbb20a1683; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at s",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:26096,concurren,concurrent,26096,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,1,['concurren'],['concurrent']
Performance,"r.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 2019-01-09 13:35:51 INFO TaskSetManager:54 - Starting task 4.1 in stage 0.0 (TID 5, scc-q20.scc.bu.edu, executor 2, partition 4, NODE_LOCAL, 7992 bytes); 2019-01-09 13:35:51 INFO TaskSetManager:54 - Lost task 1.1 in stage 0.0 (TID 4) on scc-q20.scc.bu.edu, executor 2: htsjdk.samtools.cram.CRAMException (Reference sequence MD5 mismatch for slice: sequence id 0, start 87545719, span 186383, expected MD5 492a29f6d7d6fcaf8dde06834861e7ae) [duplicate 1]; 2019-01-09 13:35:52 INFO TaskSetManager:54 - Starting task 2.1 in stage 0.0 (TID 6, scc-q01.scc.bu.edu, executor 1, partition 2, NODE_LOCAL, 7992 bytes); 2019-01-09 13:35:52 WARN TaskSetManager:66 - Lost task 7.0 in stage 0.0 (TID 3, scc-q01.scc.bu.edu, executor 1): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 2, start 131325815, span 181534, expected MD5 c240a972d49aa89fb57dae94d1d90d36; a",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:27832,concurren,concurrent,27832,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,1,['concurren'],['concurrent']
Performance,"r.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 2019-01-09 13:35:52 INFO TaskSetManager:54 - Starting task 1.2 in stage 0.0 (TID 7, scc-q20.scc.bu.edu, executor 2, partition 1, NODE_LOCAL, 7992 bytes); 2019-01-09 13:35:52 INFO TaskSetManager:54 - Lost task 4.1 in stage 0.0 (TID 5) on scc-q20.scc.bu.edu, executor 2: htsjdk.samtools.cram.CRAMException (Reference sequence MD5 mismatch for slice: sequence id 1, start 93925364, span 266689, expected MD5 54babf05a23e9e88a8738dfbb20a1683) [duplicate 1]; 2019-01-09 13:35:53 INFO TaskSetManager:54 - Starting task 7.1 in stage 0.0 (TID 8, scc-q01.scc.bu.edu, executor 1, partition 7, NODE_LOCAL, 7992 bytes); 2019-01-09 13:35:53 INFO TaskSetManager:54 - Lost task 2.1 in stage 0.0 (TID 6) on scc-q01.scc.bu.edu, executor 1: htsjdk.samtools.cram.CRAMException (Reference sequence MD5 mismatch for slice: sequence id 0, start 160972515, span 170618, expected MD5 0cc5e1f5ec5c1b06d5a4bd5fff11b77f)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:30023,concurren,concurrent,30023,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,1,['concurren'],['concurrent']
Performance,"r.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 2019-01-07 11:34:12 INFO DAGScheduler:54 - Job 0 failed: count at CountReadsSpark.java:80, took 9.419880 s; 2019-01-07 11:34:12 INFO AbstractConnector:318 - Stopped Spark@f1d88ea{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}; 2019-01-07 11:34:12 INFO SparkUI:54 - Stopped Spark web UI at http://scc-hadoop.bu.edu:4041; 2019-01-07 11:34:12 INFO YarnClientSchedulerBackend:54 - Interrupting monitor thread; 2019-01-07 11:34:12 INFO YarnClientSchedulerBackend:54 - Shutting down all executors; 2019-01-07 11:34:12 INFO YarnSchedulerBackend$YarnDriverEndpoint:54 - Asking each executor to shut down; 2019-01-07 11:34:12 INFO SchedulerExtensionServices:54 - Stopping SchedulerExtensionServices; (serviceOption=None,; services=List(),; started=false); 2019-01-07 11:34:12 INFO YarnClientSchedulerBackend:54 - Stopped; 2019-01-07 11:34:12 INFO MapOutputTrackerMasterEndpoint:54 - MapOutputTr",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:34977,concurren,concurrent,34977,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,1,['concurren'],['concurrent']
Performance,"r.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 2019-01-09 13:35:56 INFO DAGScheduler:54 - Job 0 failed: count at CountReadsSpark.java:80, took 12.691336 s; 2019-01-09 13:35:56 INFO AbstractConnector:318 - Stopped Spark@22fda322{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}; 2019-01-09 13:35:56 INFO SparkUI:54 - Stopped Spark web UI at http://scc-hadoop.bu.edu:4041; 2019-01-09 13:35:56 INFO YarnClientSchedulerBackend:54 - Interrupting monitor thread; 2019-01-09 13:35:56 INFO YarnClientSchedulerBackend:54 - Shutting down all executors; 2019-01-09 13:35:56 INFO YarnSchedulerBackend$YarnDriverEndpoint:54 - Asking each executor to shut down; 2019-01-09 13:35:56 INFO SchedulerExtensionServices:54 - Stopping SchedulerExtensionServices; (serviceOption=None,; services=List(),; started=false); 2019-01-09 13:35:56 INFO YarnClientSchedulerBackend:54 - Stopped; 2019-01-09 13:35:56 INFO MapOutputTrackerMasterEndpoint:54 - MapOutput",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:34727,concurren,concurrent,34727,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,1,['concurren'],['concurrent']
Performance,r.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskS,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:38193,concurren,concurrent,38193,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,2,['concurren'],['concurrent']
Performance,r.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 2019-01-07 11:34:12 INFO ShutdownHookManager:54 - Shutdown hook called; 2019-01-07 11:34:12 INFO ShutdownHookManager:54 - Deleting directory /tmp/spark-1ac79f09-1a36-4668-92d9-0739775f98ed; 2019-01-07 11:34:12 INFO ShutdownHookManager:54 - Deleting directory /tmp/spark-ed279998-3783-4f41-8fe5-f44a4fac3ee4; ```. CountReads runs fine..... ```; gatk CountReads --input HG04302.alt_bwamem_GRCh38DH.20150718.GBR.low_coverage.cram --reference file:///restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa; Using GATK jar /share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-local.jar CountReads --input HG04302.alt_bwamem_GRCh3,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:43088,concurren,concurrent,43088,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,1,['concurren'],['concurrent']
Performance,"r.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 2019-01-09 13:35:56 INFO ShutdownHookManager:54 - Shutdown hook called; 2019-01-09 13:35:56 INFO ShutdownHookManager:54 - Deleting directory /tmp/spark-69cc5c72-eff6-4259-8b3b-12fa6f8c42b0; 2019-01-09 13:35:56 INFO ShutdownHookManager:54 - Deleting directory /tmp/spark-0bd07e00-4f6d-43bd-b9d2-b1999376c72b; ```. Just to verify, the non-spark version still runs fine with the compressed fasta.... ```; gatk CountReads --input HG04302.alt_bwamem_GRCh38DH.20150718.GBR.low_coverage.cram --reference GRCh38_full_analysis_set_plus_decoy_hla.fa.gz; Using GATK jar /share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-local.jar CountReads --input H",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:42840,concurren,concurrent,42840,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,1,['concurren'],['concurrent']
Performance,"r_round=2000 --log_emission_sampling_rounds=100 --log_emission_sampling_median_rel_error=5.000000e-04 --max_advi_iter_first_epoch=1000 --max_advi_iter_subsequent_epochs=1000 --min_training_epochs=20 --max_training_epochs=100 --initial_temperature=2.000000e+00 --num_thermal_advi_iters=5000 --convergence_snr_averaging_window=5000 --convergence_snr_trigger_threshold=1.000000e-01 --convergence_snr_countdown_window=10 --max_calling_iters=1 --caller_update_convergence_threshold=1.000000e-03 --caller_internal_admixing_rate=7.500000e-01 --caller_external_admixing_rate=7.500000e-01 --disable_caller=false --disable_sampler=false --disable_annealing=false --interval_list=/tmp/intervals9016836733228000464.tsv --contig_ploidy_prior_table=/home/n.liorni/snakemake_cnv_gatk/resources/contig_ploidy_priors.tsv --output_model_path=/home/n.liorni/snakemake_cnv_gatk/results/cnv/ploidy/ploidy-model; Stdout: 15:09:46.970 INFO cohort_determine_ploidy_and_depth - THEANO_FLAGS environment variable has been set to: device=cpu,floatX=float64,optimizer=fast_run,compute_test_value=ignore,openmp=true,blas.ldflags=-lmkl_rt,openmp_elemwise_minsize=10; 15:09:47.017 INFO gcnvkernel.structs.metadata - Generating intervals metadata...; 15:09:47.024 INFO gcnvkernel.tasks.task_cohort_ploidy_determination - Instantiating the germline contig ploidy determination model...; 15:09:50.320 INFO gcnvkernel.tasks.task_cohort_ploidy_determination - Instantiating the ploidy emission sampler...; 15:09:50.321 INFO gcnvkernel.tasks.task_cohort_ploidy_determination - Instantiating the ploidy caller...; 15:09:50.957 INFO gcnvkernel.models.fancy_model - Global model variables: {'psi_j_log__', 'mean_bias_j_lowerbound__'}; 15:09:50.957 INFO gcnvkernel.models.fancy_model - Sample-specific model variables: {'psi_s_log__'}; 15:09:50.957 INFO gcnvkernel.tasks.inference_task_base - Instantiating the convergence tracker...; 15:09:50.958 INFO gcnvkernel.tasks.inference_task_base - Setting up DA-ADVI...; 15:10:03.310 INFO gcnvkern",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7444#issuecomment-945753905:6755,optimiz,optimizer,6755,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7444#issuecomment-945753905,1,['optimiz'],['optimizer']
Performance,"rce/Homo_sapiens_assembly38.fasta -I /data/xieduo/Immun_genomics/data/Łuksza_2022_Nature/bam/PAAD11N.bam --known-sites /data/xieduo/WES_pipe/pipeline/gatk_resource/dbsnp_146.hg38.vcf.gz --known-sites /data/reference/gatk_resource/1000G_phase1.snps.high_confidence.hg38.vcf.gz --known-sites /data/reference/gatk_resource/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz -O PAAD11N.recal_data.test.table; 13:36:33.528 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 13:36:33.547 WARN NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (No such file or directory); 13:36:33.550 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 13:36:33.551 WARN NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (No such file or directory); 13:36:33.669 INFO BaseRecalibrator - ------------------------------------------------------------; 13:36:33.670 INFO BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.2.6.1; 13:36:33.670 INFO BaseRecalibrator - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:36:33.670 INFO BaseRecalibrator - Executing as xieduo@pbs-master on Linux v3.10.0-1160.41.1.el7.x86_64 amd64; 13:36:33.670 INFO BaseRecalibrator - Java runtime: Java HotSpot(TM) 64-Bit Server VM v18+36-2087; 13:36:33.671 INFO BaseRecalibrator - Start Date/Time: September 22, 2022 at 1:36:33 PM CST; 13:36:33.671 INFO BaseRecalibrator - ------------------------------------------------------------; 13:36:33.671 INFO BaseRecalibrator - ------------------------------------------------------------; 13:36:33.672 INFO BaseRecalibrator - HTSJDK Version: 2.24.1; 13:36:33.672 INFO BaseRecalibrator - Picard",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8005#issuecomment-1254561081:7056,load,load,7056,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005#issuecomment-1254561081,1,['load'],['load']
Performance,"rce/Homo_sapiens_assembly38.fasta -I /data/xieduo/Immun_genomics/data/Łuksza_2022_Nature/bam/PAAD11N.bam --known-sites /data/xieduo/WES_pipe/pipeline/gatk_resource/dbsnp_146.hg38.vcf.gz --known-sites /data/reference/gatk_resource/1000G_phase1.snps.high_confidence.hg38.vcf.gz --known-sites /data/reference/gatk_resource/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz -O PAAD11N.recal_data.test.table; 13:46:24.742 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 13:46:24.761 WARN NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (No such file or directory); 13:46:24.764 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 13:46:24.764 WARN NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (No such file or directory); 13:46:24.884 INFO BaseRecalibrator - ------------------------------------------------------------; 13:46:24.884 INFO BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.2.6.1; 13:46:24.885 INFO BaseRecalibrator - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:46:24.885 INFO BaseRecalibrator - Executing as xieduo@pbs-master on Linux v3.10.0-1160.41.1.el7.x86_64 amd64; 13:46:24.885 INFO BaseRecalibrator - Java runtime: Java HotSpot(TM) 64-Bit Server VM v18+36-2087; 13:46:24.885 INFO BaseRecalibrator - Start Date/Time: September 22, 2022 at 1:46:24 PM CST; 13:46:24.885 INFO BaseRecalibrator - ------------------------------------------------------------; 13:46:24.885 INFO BaseRecalibrator - ------------------------------------------------------------; 13:46:24.886 INFO BaseRecalibrator - HTSJDK Version: 2.24.1; 13:46:24.886 INFO BaseRecalibrator - Picard",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8005#issuecomment-1254561081:13364,load,load,13364,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005#issuecomment-1254561081,1,['load'],['load']
Performance,rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); 17:43:23.161 INFO FeatureManager - Using codec VCFCodec to read file file:///scratch/tmp/spark-ecd63991-68be-4879-b481-68e6789a2004/userFiles-b72d4821-5e36-4d36-aa79-aa6263768669/1000G_phase1.indels.hg19.sites.vcf; 20/01/05 17:43:23 INFO NewHadoopRDD: Input split: file:/panfs/roc/groups/6/clinicalmdl/shared/wgs_exome_v1.0/projects/BT_WGS_Flex_S1/data/exome_dedup_reads.bam:167436615680+33554432; 20/01/05 17:43:23 ERROR Executor: Exception in task 4990.0 in stage 0.0 (TID 4990); java.io.FileNotFoundException: /panfs/roc/groups/6/clinicalmdl/shared/v1.0/projects/BT_WGS_Flex_S1/data/exome_dedup_reads.bam (Too many open files); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at org.ap,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5316#issuecomment-570992855:4783,concurren,concurrent,4783,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5316#issuecomment-570992855,1,['concurren'],['concurrent']
Performance,"re is the full output:. > (base) [pkus@wn45 mutect_test]$ ~/programs/gatk-4.1.8.0/gatk Funcotator --variant filtered_variants/P1.vcf.gz --reference ~/resources/hg38_for_bwa/hs38DH.fa --ref-version hg38 --data-sources-path ~/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s --output filtered_variants/P1.avcf.gz --output-file-format VCF; > Using GATK jar /home/pkus/programs/gatk-4.1.8.0/gatk-package-4.1.8.0-local.jar; > Running:; > java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/pkus/programs/gatk-4.1.8.0/gatk-package-4.1.8.0-local.jar Funcotator --variant filtered_variants/P1.vcf.gz --reference /home/pkus/resources/hg38_for_bwa/hs38DH.fa --ref-version hg38 --data-sources-path /home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s --output filtered_variants/P1.avcf.gz --output-file-format VCF; > 12:28:16.251 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/pkus/programs/gatk-4.1.8.0/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; > Jul 21, 2020 12:28:16 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; > INFO: Failed to detect whether we are running on Google Compute Engine.; > 12:28:16.537 INFO Funcotator - ------------------------------------------------------------; > 12:28:16.538 INFO Funcotator - The Genome Analysis Toolkit (GATK) v4.1.8.0; > 12:28:16.538 INFO Funcotator - For support and documentation go to https://software.broadinstitute.org/gatk/; > 12:28:16.541 INFO Funcotator - Executing as xxx on Linux v3.10.0-123.20.1.el7.x86_64 amd64; > 12:28:16.541 INFO Funcotator - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_251-b08; > 12:28:16.542 INFO Funcotator - Start Date/Time: July 21, 2020 12:28:16 PM CEST; > 12:28:16.542 INFO Funcotator - ------------------------------------------------------------; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6708#issuecomment-661776975:1087,Load,Loading,1087,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6708#issuecomment-661776975,1,['Load'],['Loading']
Performance,"re: `VariantFiltration`, we did not profile the case of using an auxiliary data. The 100k cache will work for that one too, i'm sure.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1092#issuecomment-155554387:90,cache,cache,90,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1092#issuecomment-155554387,1,['cache'],['cache']
Performance,reamSpliterators$AbstractWrappingSpliterator.doAdvance(StreamSpliterators.java:169); 	at java.util.stream.StreamSpliterators$WrappingSpliterator.tryAdvance(StreamSpliterators.java:300); 	at java.util.Spliterators$1Adapter.hasNext(Spliterators.java:681); 	at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); 	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:215); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1038); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1029); 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:969); 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1029); 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:760); 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4661#issuecomment-408874230:3514,concurren,concurrent,3514,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4661#issuecomment-408874230,2,['concurren'],['concurrent']
Performance,redReader.readLine(LongLineBufferedReader.java:298); 	at htsjdk.tribble.readers.LongLineBufferedReader.readLine(LongLineBufferedReader.java:354); 	at htsjdk.tribble.readers.SynchronousLineReader.readLine(SynchronousLineReader.java:51); 	at htsjdk.tribble.readers.LineIteratorImpl.advance(LineIteratorImpl.java:24); 	at htsjdk.tribble.readers.LineIteratorImpl.advance(LineIteratorImpl.java:11); 	at htsjdk.samtools.util.AbstractIterator.hasNext(AbstractIterator.java:44); 	at htsjdk.variant.vcf.VCFCodec.readActualHeader(VCFCodec.java:89); 	at htsjdk.tribble.AsciiFeatureCodec.readHeader(AsciiFeatureCodec.java:83); 	at htsjdk.tribble.AsciiFeatureCodec.readHeader(AsciiFeatureCodec.java:36); 	at htsjdk.tribble.TabixFeatureReader.readHeader(TabixFeatureReader.java:100); 	... 12 more; Caused by: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: Connection has been shutdown: javax.net.ssl.SSLException: java.net.SocketException: Connection reset; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.getBuf(SeekableByteChannelPrefetcher.java:136); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.fetch(SeekableByteChannelPrefetcher.java:255); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:300); 	... 41 more; Caused by: com.google.cloud.storage.StorageException: Connection has been shutdown: javax.net.ssl.SSLException: java.net.SocketException: Connection reset; 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:186); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:512); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:128); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:125); 	at shaded.cloud_nio.com,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-301610931:4815,concurren,concurrent,4815,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-301610931,1,['concurren'],['concurrent']
Performance,"rencePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:94); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:838); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:115); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:131); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:152); 	at org.broadinstitute.hellbender.Main.main(Main.java:230); Caused by: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: java.lang.IllegalArgumentException: Position should be non-negative, is %d [-134217728]; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.getBuf(SeekableByteChannelPrefetcher.java:136); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.fetch(SeekableByteChannelPrefetcher.java:255); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:300); 	... 44 more; Caused by: com.google.cloud.storage.StorageException: java.lang.IllegalArgumentException: Position should be non-negative, is %d [-134217728]; 	at com.google.cloud.storage.StorageException.translateAndThrow(StorageException.java:71); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:139); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:113); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:131); 	at org.broadinstitute.hell",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317549881:7758,concurren,concurrent,7758,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317549881,1,['concurren'],['concurrent']
Performance,"rencePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:94); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:838); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:115); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:131); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:152); 	at org.broadinstitute.hellbender.Main.main(Main.java:230); Caused by: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: java.lang.IllegalArgumentException: Position should be non-negative, is %d [-830472192]; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.getBuf(SeekableByteChannelPrefetcher.java:136); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.fetch(SeekableByteChannelPrefetcher.java:255); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:300); 	... 44 more; Caused by: com.google.cloud.storage.StorageException: java.lang.IllegalArgumentException: Position should be non-negative, is %d [-830472192]; 	at com.google.cloud.storage.StorageException.translateAndThrow(StorageException.java:71); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:139); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:113); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:131); 	at org.broadinstitute.hell",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317442564:6946,concurren,concurrent,6946,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317442564,1,['concurren'],['concurrent']
Performance,"ression_level=2 ,spark.kryoserializer.buffer.max=512m,spark.yarn.executor.memoryOverhead=600 --jar gs://hellbender-test-logs/staging/gatk-package-4.1.0.0-24-g18a95c7-SNAPSHOT-spark_3e9078b7e67707952fa12a0c5c4d2b71.jar -- PrintVariantsSpark --V gs://hellbender/test/resources/large/gvcfs/gatk3.7_30_ga4f720357.24_sample.21.expected.vcf --output gs://hellbender-test-logs/staging/12dc38b0-0b40-49d5-a98e-fe83ca658003.vcf --spark-master yarn; Job [654b5b8e01de4c60bd87d941d4ec8831] submitted.; Waiting for job output...; 19/02/18 16:58:03 WARN org.apache.spark.SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 16:58:09.526 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 16:58:09.705 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/tmp/654b5b8e01de4c60bd87d941d4ec8831/gatk-package-4.1.0.0-24-g18a95c7-SNAPSHOT-spark_3e9078b7e67707952fa12a0c5c4d2b71.jar!/com/intel/gkl/native/libgkl_compression.so; 16:58:10.112 INFO PrintVariantsSpark - ------------------------------------------------------------; 16:58:10.113 INFO PrintVariantsSpark - The Genome Analysis Toolkit (GATK) v4.1.0.0-24-g18a95c7-SNAPSHOT; 16:58:10.113 INFO PrintVariantsSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 16:58:10.113 INFO PrintVariantsSpark - Executing as root@gatk-test-2495f43b-04fc-49e7-aa0a-7108cc876246-m on Linux v4.9.0-8-amd64 amd64; 16:58:10.114 INFO PrintVariantsSpark - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_181-8u181-b13-2~deb9u1-b13; 16:58:10.114 INFO PrintVariantsSpark - Start Date/Time: February 18, 2019 4:58:09 PM UTC; 16:58:10.114 INFO PrintVariantsSpark - ------------------------------------------------------------; 16:58:10.114 INFO Pri",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3840#issuecomment-464825765:1701,Load,Loading,1701,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3840#issuecomment-464825765,1,['Load'],['Loading']
Performance,rg.gradle.internal.execution.steps.CleanupOutputsStep.execute(CleanupOutputsStep.java:35); at org.gradle.internal.execution.steps.ResolveInputChangesStep.execute(ResolveInputChangesStep.java:48); at org.gradle.internal.execution.steps.ResolveInputChangesStep.execute(ResolveInputChangesStep.java:33); at org.gradle.internal.execution.steps.CancelExecutionStep.execute(CancelExecutionStep.java:39); at org.gradle.internal.execution.steps.TimeoutStep.executeWithoutTimeout(TimeoutStep.java:73); at org.gradle.internal.execution.steps.TimeoutStep.execute(TimeoutStep.java:54); at org.gradle.internal.execution.steps.CatchExceptionStep.execute(CatchExceptionStep.java:35); at org.gradle.internal.execution.steps.CreateOutputsStep.execute(CreateOutputsStep.java:51); at org.gradle.internal.execution.steps.SnapshotOutputsStep.execute(SnapshotOutputsStep.java:45); at org.gradle.internal.execution.steps.SnapshotOutputsStep.execute(SnapshotOutputsStep.java:31); at org.gradle.internal.execution.steps.CacheStep.executeWithoutCache(CacheStep.java:208); at org.gradle.internal.execution.steps.CacheStep.execute(CacheStep.java:70); at org.gradle.internal.execution.steps.CacheStep.execute(CacheStep.java:45); at org.gradle.internal.execution.steps.BroadcastChangingOutputsStep.execute(BroadcastChangingOutputsStep.java:49); at org.gradle.internal.execution.steps.StoreSnapshotsStep.execute(StoreSnapshotsStep.java:43); at org.gradle.internal.execution.steps.StoreSnapshotsStep.execute(StoreSnapshotsStep.java:32); at org.gradle.internal.execution.steps.RecordOutputsStep.execute(RecordOutputsStep.java:38); at org.gradle.internal.execution.steps.RecordOutputsStep.execute(RecordOutputsStep.java:24); at org.gradle.internal.execution.steps.SkipUpToDateStep.executeBecause(SkipUpToDateStep.java:96); at org.gradle.internal.execution.steps.SkipUpToDateStep.lambda$execute$0(SkipUpToDateStep.java:89); at org.gradle.internal.execution.steps.SkipUpToDateStep.execute(SkipUpToDateStep.java:54); at org.gradle.intern,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6466#issuecomment-590387973:9075,Cache,CacheStep,9075,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6466#issuecomment-590387973,2,['Cache'],['CacheStep']
Performance,ribble.readers.LongLineBufferedReader.readLine(LongLineBufferedReader.java:354); 	at htsjdk.tribble.readers.SynchronousLineReader.readLine(SynchronousLineReader.java:51); 	at htsjdk.tribble.readers.LineIteratorImpl.advance(LineIteratorImpl.java:24); 	at htsjdk.tribble.readers.LineIteratorImpl.advance(LineIteratorImpl.java:11); 	at htsjdk.samtools.util.AbstractIterator.hasNext(AbstractIterator.java:44); 	at htsjdk.variant.vcf.VCFCodec.readActualHeader(VCFCodec.java:89); 	at htsjdk.tribble.AsciiFeatureCodec.readHeader(AsciiFeatureCodec.java:83); 	at htsjdk.tribble.AsciiFeatureCodec.readHeader(AsciiFeatureCodec.java:36); 	at htsjdk.tribble.TabixFeatureReader.readHeader(TabixFeatureReader.java:100); 	... 12 more; Caused by: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: Connection has been shutdown: javax.net.ssl.SSLException: java.net.SocketException: Connection reset; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.getBuf(SeekableByteChannelPrefetcher.java:136); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.fetch(SeekableByteChannelPrefetcher.java:255); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:300); 	... 41 more; Caused by: com.google.cloud.storage.StorageException: Connection has been shutdown: javax.net.ssl.SSLException: java.net.SocketException: Connection reset; 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:186); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:512); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:128); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:125); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetr,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-301610931:4880,concurren,concurrent,4880,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-301610931,1,['concurren'],['concurrent']
Performance,"ribute to the project so I'm excited by the prospect!. -Dan. On Fri, Sep 4, 2020, 11:53 AM R-obert <notifications@github.com> wrote:. > Hello,; >; > I'm trying to use GATK4 (4.1.8.1) on an Ubuntu (16.04) machine. The; > machine is a ""PowerLinux"" machine and I'm guessing that the most relevant; > info for the following problem is that it is a ppc64le system. When I use; > HaplotypeCaller, I see the following messages on the screen:; >; > Running:; > java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx50G -jar /home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar HaplotypeCaller -R ref.fa -I mybam.bam -O mycalls.vcf.gz -L snps.vcf -ip 100; >; > 16:17:04.377 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; >; > 16:17:04.397 **WARN** NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (/tmp/libgkl_compression3825249225068031371.so: /tmp/libgkl_compression3825249225068031371.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > 16:17:04.402 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; >; > 16:17:04.407 **WARN** NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (/tmp/libgkl_compression7506152962158874866.so: /tmp/libgkl_compression7506152962158874866.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > Sep 04, 2020 4:17:05 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; >; > INFO: Failed to detect whether ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600:1632,load,load,1632,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600,1,['load'],['load']
Performance,rk.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2114); 	at org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:78); 	... 87 more; Caused by: java.util.ConcurrentModificationException; 	at java.util.ArrayList.sort(ArrayList.java:1464); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.<init>(ReadThreadingAssembler.java:81); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerReadThreadingAssemblerArgumentCollection.makeReadThreadingAssembler(HaplotypeCallerReadThreadingAssemblerArgumentCollection.java:37); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerArgumentCollection.createReadThreadingAssembler(AssemblyBasedCallerArgumentCollection.java:36); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.initialize(HaplotypeCallerEngine.java:231); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.<init>(HaplotypeCallerEngine.java:166); 	at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.lambda$assemblyFunction$29848511$1(HaplotypeCallerSpark.java:174); 	at org.apache.spark,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429:12275,Concurren,ConcurrentModificationException,12275,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429,1,['Concurren'],['ConcurrentModificationException']
Performance,"roblem with the following location: '/home/jeremie/GATK/build/classes/java/main'. Reason: Task ':gatkDoc' uses this output of task ':condaStandardEnvironmentDefinition' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem.; - Gradle detected a problem with the following location: '/home/jeremie/GATK/build/resources/main'. Reason: Task ':gatkDoc' uses this output of task ':condaStandardEnvironmentDefinition' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':gatkDoc'.; > Javadoc generation failed. Generated Javadoc options file (useful for troubleshooting): '/home/jeremie/GATK/build/tmp/gatkDoc/javadoc.options'. * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full insights. * Get more help at https://help.gradle.org. Deprecated Gradle features were used in this build, making it incompatible with Gradle 8.0. You can use '--warning-mode all' to show the individual deprecation warnings and determine if they come from your own scripts or plugins. See https://docs.gradle.org/7.3.2/userguide/command_line_interface.html#sec:command_line_warnings. Execution optimizations have been disabled for 1 invalid unit(s) of work during this build to ensure correctness.; Please consult deprecation warnings for more details. BUILD FAILED in 33s; 5 actionable tasks: 5 executed; ```; which does not seem related to any changes I made.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7936#issuecomment-1202544500:1950,optimiz,optimizations,1950,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7936#issuecomment-1202544500,1,['optimiz'],['optimizations']
Performance,"rogram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:163); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:149); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:190); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:27); at org.broadinstitute.hellbender.testutils.CommandLineProgramTester.runCommandLine(CommandLineProgramTester.java:107); at org.broadinstitute.hellbender.tools.HaplotypeCallerSparkIntegrationTest.testNonStrictVCFModeIsConsistentWithPastResults(HaplotypeCallerSparkIntegrationTest.java:109); Caused by:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 5.0 failed 1 times, most recent failure: Lost task 1.0 in stage 5.0 (TID 12, localhost, executor driver): java.util.ConcurrentModificationException; at java.util.ArrayList.sort(ArrayList.java:1464); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.<init>(ReadThreadingAssembler.java:81); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerReadThreadingAssemblerArgumentCollection.makeReadThreadingAssembler(HaplotypeCallerReadThreadingAssemblerArgumentCollection.java:37); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerArgumentCollection.createReadThreadingAssembler(AssemblyBasedCallerArgumentCollection.java:36); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.initialize(HaplotypeCallerEngine.java:231); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.<init>(HaplotypeCallerEngine.java:166); at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.lambda$assemblyFunction$29848511$1(HaplotypeCallerSpark.java:174); at org.apache.spark.api.jav",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6513#issuecomment-601702690:4523,Concurren,ConcurrentModificationException,4523,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6513#issuecomment-601702690,1,['Concurren'],['ConcurrentModificationException']
Performance,"rote:. > Hi again,; > I tried installing java8 and switching to this version prior to running; > gatk. It runs and looks to be running the right Java, but spits out roughly; > the same error:; >; > Thoughts?; >; > /cold/drichard/gatk/./gatk --java-options ""-Xmx25g"" SplitNCigarReads; > -R /cold/drichard/VARIANTS/Homo_sapiens.GRCh38.dna.primary_assembly.fa -I; > subset_TINY_rehead.bam; > --tmp-dir /thing -O thing.bam; > Using GATK jar; > /cold/drichard/gatk/build/libs/gatk-package-4.3.0.0-44-g227bbca-SNAPSHOT-local.jar; > Running:; > java -Dsamjdk.use_async_io_read_samtools=false; > -Dsamjdk.use_async_io_write_samtools=true; > -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2; > -Xmx25g -jar; > /cold/drichard/gatk/build/libs/gatk-package-4.3.0.0-44-g227bbca-SNAPSHOT-local.jar; > SplitNCigarReads -R; > /cold/drichard/VARIANTS/Homo_sapiens.GRCh38.dna.primary_assembly.fa -I; > subset_TINY_rehead.bam --tmp-dir /thing -O thing.bam; > 15:34:59.974 INFO NativeLibraryLoader - Loading libgkl_compression.so from; > jar:file:/cold/drichard/gatk/build/libs/gatk-package-4.3.0.0-44-g227bbca-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_compression.so; > 15:35:00.220 INFO SplitNCigarReads -; > ------------------------------------------------------------; > 15:35:00.226 INFO SplitNCigarReads - The Genome Analysis Toolkit (GATK); > v4.3.0.0-44-g227bbca-SNAPSHOT; > 15:35:00.226 INFO SplitNCigarReads - For support and documentation go to; > https://software.broadinstitute.org/gatk/; > 15:35:00.226 INFO SplitNCigarReads - Executing as ***@***.*** on; > Linux v5.19.0-32-generic amd64; > 15:35:00.226 INFO SplitNCigarReads - Java runtime: OpenJDK 64-Bit Server; > VM v1.8.0_362-8u362-ga-0ubuntu1~22.04-b09; > 15:35:00.226 INFO SplitNCigarReads - Start Date/Time: March 2, 2023; > 3:34:59 PM EST; > 15:35:00.226 INFO SplitNCigarReads -; > ------------------------------------------------------------; > 15:35:00.226 INFO SplitNCigarReads -; > --------------------------------",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452528344:1196,Load,Loading,1196,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452528344,1,['Load'],['Loading']
Performance,"rray - forkTest/genomicsdb_array; 16:28:04.158 INFO ProgressMeter - Starting traversal; 16:28:04.158 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 16:28:05.198 INFO GenomicsDBImport - Starting batch input file preload; 16:29:23.571 INFO GenomicsDBImport - Finished batch preload; 16:48:46.140 INFO GenomicsDBImport - Shutting down engine; [May 4, 2018 4:48:46 PM EDT] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 20.96 minutes.; Runtime.totalMemory()=22281715712; java.util.concurrent.CompletionException: java.lang.OutOfMemoryError: Java heap space; at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273); at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1592); at java.util.concurrent.CompletableFuture$AsyncSupply.exec(CompletableFuture.java:1582); at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056); at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692); at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157); Caused by: java.lang.OutOfMemoryError: Java heap space; at com.intel.genomicsdb.importer.SilentByteBufferStream.<init>(SilentByteBufferStream.java:55); at com.intel.genomicsdb.importer.GenomicsDBImporterStreamWrapper.<init>(GenomicsDBImporterStreamWrapper.java:70); at com.intel.genomicsdb.importer.GenomicsDBImporter.addBufferStream(GenomicsDBImporter.java:397); at com.intel.genomicsdb.importer.GenomicsDBImporter.addSortedVariantContextIterator(GenomicsDBImporter.java:358); at com.intel.genomicsdb.importer.GenomicsDBImporter.<init>(GenomicsDBImporter.java:167); at com.intel.genomicsdb.importer.GenomicsDBImporter.lambda$null$2(GenomicsDBImporter.java:598); at com.intel.genomicsdb.importer.GenomicsDBImporter$$La",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388079572:3869,concurren,concurrent,3869,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388079572,1,['concurren'],['concurrent']
Performance,"running with `-verbose:class` shows 2 separate class loaders being used for each class. . ```; ./gatk-launch PrintReadsSpark -I src/test/resources/org/broadinstitute/hellbender/tools/flag_stat.bam -O foo.bam -- --sparkRunner SPARK --sparkMaster ""local[*]"" --driver-java-options 2> /dev/null | grep SAMFileHeader; [Loaded htsjdk.samtools.SAMFileHeader from file:/Users/louisb/Workspace/gatk/build/libs/gatk-all-4.alpha-142-g08c27aa-SNAPSHOT-spark.jar]; [Loaded htsjdk.samtools.SAMFileHeader$SortOrder from file:/Users/louisb/Workspace/gatk/build/libs/gatk-all-4.alpha-142-g08c27aa-SNAPSHOT-spark.jar]; [Loaded htsjdk.samtools.SAMFileHeader$GroupOrder from file:/Users/louisb/Workspace/gatk/build/libs/gatk-all-4.alpha-142-g08c27aa-SNAPSHOT-spark.jar]; [Loaded htsjdk.samtools.SAMFileHeader from file:/private/var/folders/xt/vq7wz8955r1401mv8w0f4zf9qbfwzl/T/louisb/spark-6607a950-e256-47fb-9f8c-7a9b60388c0a/userFiles-bd28d98d-f860-43bd-ae54-a3a94dd7eaa7/gatk-all-4.alpha-142-g08c27aa-SNAPSHOT-spark.jar]; [Loaded htsjdk.samtools.SAMFileHeader$SortOrder from file:/private/var/folders/xt/vq7wz8955r1401mv8w0f4zf9qbfwzl/T/louisb/spark-6607a950-e256-47fb-9f8c-7a9b60388c0a/userFiles-bd28d98d-f860-43bd-ae54-a3a94dd7eaa7/gatk-all-4.alpha-142-g08c27aa-SNAPSHOT-spark.jar]; [Loaded htsjdk.samtools.SAMFileHeader$GroupOrder from file:/private/var/folders/xt/vq7wz8955r1401mv8w0f4zf9qbfwzl/T/louisb/spark-6607a950-e256-47fb-9f8c-7a9b60388c0a/userFiles-bd28d98d-f860-43bd-ae54-a3a94dd7eaa7/gatk-all-4.alpha-142-g08c27aa-SNAPSHOT-spark.jar]; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1315#issuecomment-192437383:53,load,loaders,53,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1315#issuecomment-192437383,7,"['Load', 'load']","['Loaded', 'loaders']"
Performance,"rvalArgumentCollection - Processing 61464 bp from intervals; >; > 16:17:06.551 INFO HaplotypeCaller - Done initializing engine; >; > 16:17:06.573 INFO HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output; >; > 16:17:06.588 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; >; > 16:17:06.589 **WARN** NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils347167544598047196.so: /tmp/libgkl_utils347167544598047196.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > 16:17:06.589 **WARN** IntelPairHmm - Intel GKL Utils not loaded; >; > 16:17:06.589 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; >; > 16:17:06.589 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; >; > 16:17:06.590 **WARN** NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils6186849302609329058.so: /tmp/libgkl_utils6186849302609329058.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > 16:17:06.590 **WARN** IntelPairHmm - Intel GKL Utils not loaded; >; > 16:17:06.591 **WARN** PairHMM - ***WARNING: Machine does not have the AVX instruction set support needed for the accelerated AVX PairHmm. Falling back to the MUCH slower LOGLESS_CACHING implementation!; >; > Since the calculation takes quite long, I checked the WARN messages of the; > output above. Especially the last one about the AVX instruction set where; > it says that a *MUCH* slower implementation will be used. From the few; > WARN messages it seem",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600:5679,Load,Loading,5679,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600,1,['Load'],['Loading']
Performance,"s doc. Note that some of the CNV section is out of date and incorrect. In particular, we have been taking in PCOV as input to CreatePanelOfNormals for some time now, but the doc states that we take integer read counts. This already yields different results by the first filtering step (on intervals by interval median). @LeeTL1220 @davidbenjamin what is the ""official ReCapSeg"" behavior, and do we want to keep the current behavior? In general, I think all of the standardization (i.e., filtering/imputation/truncation/transformation) steps could stand some revisiting. Evaluation:. - [ ] Revisit standardization procedure by checking with simulated data. We should make sure that the centering of the data does not rescale the true copy ratio.; - [x] <s>Investigate the effect of keeping duplicates. I am still not sure why we do this, and it may have a more drastic impact on WGS data.</s> Turns out we don't keep duplicates for WGS; see #3367.; - [ ] Check that GC-bias-correction+PCA and PCA-only perform comparably, even at small bin sizes (~300bp). From what I've seen, this is true for larger bin sizes (~3kbp), so explicit GC-bias correction may not be necessary. (That is, even at these (purportedly) large bin sizes, the effect of the read-based GC-bias correction is obvious for those samples where it is important. However, the end result is not very different from PCA-only denoising with no GC-bias correction performed.); - [x] <s>Check that changing CBS alpha parameter sufficiently reduces hypersegmentation.</s> <s>Looks like the hybrid p-value calculation in DNACopy is not accurate enough to handle WGS-size data. (Also, it's relatively slow, taking ~30 minutes on ~10M intervals.) Even if I set alpha to 0, I still get a ridiculous number of segments! So I think it's finally time to scrap CBS. I'll look into other R segmentation packages that might give us a quick drop-in solution, but we may want to roll our own simple algorithm (which we will scrap anyway once the coverage",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351:4673,perform,perform,4673,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351,1,['perform'],['perform']
Performance,"s method than our previous probabilistic approaches. Even SNP segmentation will be much cheaper. > What is the name of this approach? ""KernSeg""?. Not sure...I couldn't find an R package, although an R/C implementation is mentioned in the paper. But the python implementation is straightforward and a pure Java implementation should not be so bad. There are some cythonized numpy methods that my python implementation used, but I think equivalent implementations of these methods should be relatively fast in pure Java as well. > What variant of the algorithm did you implement? the paper lists several. I implemented what they call ApproxKSeg. It's an approximate version that combines binary segmentation with the low-rank approximation to the Gaussian kernel. > I haven't read the paper in detail yet, but is it possible to choose a conservatively large number of possible break points and then filter bad break points, possibly based on the rapid decline of the change point probability? i.e. does the algorithm naturally produce change point probabilities?. Yes, you can oversegment and then choose which breakpoints to retain. However, there are no proper changepoint probabilities, only changepoint costs. Adding a penalty term based on the number of changepoints seems to perform relatively well in simple tests, but one could certainly devise other ways to filter changepoints (some of which could yield probabilities, if you are willing to assume a probabilistic model). I think we should just think of this as a fast, heuristic, non-parametric method for finding breakpoints in multidimensional data. > Is it possible to throw in additional change points incrementally, without doing extra work, until a certain criterion is met? (see above). The version I implemented adds changepoints via binary segmentation. The time complexity required to split a segment is linear in the number of points contained in the segment, although some care must be taken in the implementation to ensure this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-321140715:2775,perform,perform,2775,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-321140715,1,['perform'],['perform']
Performance,"s on 10 of our 2000 samples (only in WES) none of our 600 WGS seems to have the same issue. It is always on some small contig (you can see here range is 544, but all cases are small ranges like this one). Everything is the default mutect2 pipeline and params (e.g. [gs://genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.fasta](https://console.cloud.google.com/storage/browser/genomics-public-data/resources/broad/hg38/v0?prefix=Homo_sapiens_assembly38.fasta&authuser=jkalfon%40broadinstitute.org)) : except the interval file: [gs://ccleparams/region_file_wgs.list](https://console.cloud.google.com/storage/browser/ccleparams?prefix=region_file_wgs.list&authuser=jkalfon%40broadinstitute.org); GATK 4.2.6.1. . Here is the VCF file to annotate `gs://ccleparams/test/CDS-2jucw0.hg38-filtered.vcf.gz`. Here is the stacktrace:. ```; ....; 10:53:39.044 INFO VcfFuncotationFactory - ClinVar_VCF 20180429_hg38 cache hits/total: 0/2145; 10:53:39.249 INFO VcfFuncotationFactory - dbSNP 9606_b151 cache hits/total: 0/1069225; 10:53:39.520 INFO Funcotator - Shutting down engine; [July 12, 2022 10:53:39 AM GMT] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 115.46 minutes.; Runtime.totalMemory()=2050490368; java.lang.StringIndexOutOfBoundsException: String index out of range: 544; at java.lang.String.substring(String.java:1963); at org.broadinstitute.hellbender.tools.funcotator.ProteinChangeInfo.initializeForInsertion(ProteinChangeInfo.java:293); at org.broadinstitute.hellbender.tools.funcotator.ProteinChangeInfo.<init>(ProteinChangeInfo.java:101); at org.broadinstitute.hellbender.tools.funcotator.ProteinChangeInfo.create(ProteinChangeInfo.java:399); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createSequenceComparison(GencodeFuncotationFactory.java:2054); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createCodingRegionFuncotationForProteinCoding",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6651#issuecomment-1182102653:1029,cache,cache,1029,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6651#issuecomment-1182102653,1,['cache'],['cache']
Performance,"s$ForEachOp.evaluateSequential(ForEachOps.java:151); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:94); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:838); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:115); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:131); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:152); 	at org.broadinstitute.hellbender.Main.main(Main.java:230); Caused by: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: java.lang.IllegalArgumentException: Position should be non-negative, is %d [-1056964608]; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.getBuf(SeekableByteChannelPrefetcher.java:136); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.fetch(SeekableByteChannelPrefetcher.java:255); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:300); 	... 44 more; Caused by: com.google.cloud.storage.StorageException: java.lang.IllegalArgumentException: Position should be non-negative, is %d [-1056964608]; 	at com.google.cloud.storage.StorageException.translateAndThrow(StorageException.java:71); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:139); 	at com.google.cl",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317782472:7742,concurren,concurrent,7742,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317782472,1,['concurren'],['concurrent']
Performance,"s$ForEachOp.evaluateSequential(ForEachOps.java:151); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:94); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:838); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:115); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:131); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:152); 	at org.broadinstitute.hellbender.Main.main(Main.java:230); Caused by: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: java.lang.IllegalArgumentException: Position should be non-negative, is %d [-134217728]; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.getBuf(SeekableByteChannelPrefetcher.java:136); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.fetch(SeekableByteChannelPrefetcher.java:255); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:300); 	... 44 more; Caused by: com.google.cloud.storage.StorageException: java.lang.IllegalArgumentException: Position should be non-negative, is %d [-134217728]; 	at com.google.cloud.storage.StorageException.translateAndThrow(StorageException.java:71); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:139); 	at com.google.clou",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317549881:7516,concurren,concurrent,7516,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317549881,1,['concurren'],['concurrent']
Performance,"s$ForEachOp.evaluateSequential(ForEachOps.java:151); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:94); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:838); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:115); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:131); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:152); 	at org.broadinstitute.hellbender.Main.main(Main.java:230); Caused by: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: java.lang.IllegalArgumentException: Position should be non-negative, is %d [-830472192]; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.getBuf(SeekableByteChannelPrefetcher.java:136); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.fetch(SeekableByteChannelPrefetcher.java:255); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:300); 	... 44 more; Caused by: com.google.cloud.storage.StorageException: java.lang.IllegalArgumentException: Position should be non-negative, is %d [-830472192]; 	at com.google.cloud.storage.StorageException.translateAndThrow(StorageException.java:71); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:139); 	at com.google.clou",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317442564:6704,concurren,concurrent,6704,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317442564,1,['concurren'],['concurrent']
Performance,"s.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false 13:39:56.672 INFO HaplotypeCaller - Deflater: IntelDeflater 13:39:56.673 INFO HaplotypeCaller - Inflater: IntelInflater 13:39:56.674 INFO HaplotypeCaller - GCS max retries/reopens: 20 13:39:56.679 INFO HaplotypeCaller - Requester pays: disabled 13:39:56.680 INFO HaplotypeCaller - Initializing engine 13:39:56.968 INFO HaplotypeCaller - Done initializing engine 13:39:56.971 INFO HaplotypeCallerEngine - Tool is in reference confidence mode and the annotation, the following changes will be made to any specified annotations: 'StrandBiasBySample' will be enabled. 'ChromosomeCounts', 'FisherStrand', 'StrandOddsRatio' and 'QualByDepth' annotations have been disabled 13:39:57.000 INFO HaplotypeCallerEngine - Standard Emitting and Calling confidence set to -0.0 for reference-model confidence output 13:39:57.000 INFO HaplotypeCallerEngine - All sites annotated with PLs forced to true for reference-model confidence output 13:39:57.020 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/omics/groups/OE0540/internal/software/jvm/gatk/4.4.0.0/gatk-package-4.4.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so 13:39:57.026 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported 13:39:57.026 WARN PairHMM - ***WARNING: Machine does not have the AVX instruction set support needed for the accelerated AVX PairHmm. Falling back to the MUCH slower LOGLESS_CACHING implementation! 13:39:57.108 INFO ProgressMeter - Starting traversal 13:39:57.110 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute 13:40:07.119 INFO ProgressMeter - chr19:8969701 0.2 29900 179382.1 13:40:17.116 INFO ProgressMeter - chr19:20264701 0.3 67550 202609.5 13:40:27.115 INFO ProgressMeter - chr19:31874701 0.5 106250 212471.7 13:40:37.116 INFO ProgressMeter - chr19:44792701 0.7 149310 223937.0 13:40:49.251 WARN InbreedingCoeff - InbreedingCoeff will not be calculated at position chr19:55910",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5727#issuecomment-1781017195:13505,Load,Loading,13505,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5727#issuecomment-1781017195,1,['Load'],['Loading']
Performance,s.iterators.PushToPullIterator.advanceToNextElement(PushToPullIterator.java:58); 	at org.broadinstitute.hellbender.utils.iterators.PushToPullIterator.<init>(PushToPullIterator.java:37); 	at org.broadinstitute.hellbender.utils.variant.writers.GVCFBlockCombiningIterator.<init>(GVCFBlockCombiningIterator.java:14); 	at org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSink.lambda$writeVariantsSingle$516343c4$1(VariantsSparkSink.java:127); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:153); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:153); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745)`,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2554#issuecomment-530773994:4379,concurren,concurrent,4379,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2554#issuecomment-530773994,2,['concurren'],['concurrent']
Performance,"sal complete. Processed 1 total batches in 2531.4 minutes.; 05:39:42.061 INFO GenomicsDBImport - Import completed!; 05:39:42.061 INFO GenomicsDBImport - Shutting down engine; [January 16, 2021 5:39:42 AM CST] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 2,531.64 minutes.; Runtime.totalMemory()=9711910912; Tool returned:; true; **Calling Variants Attempt**; Using GATK jar /data1/_software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Xmx32g -jar /data1/_software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar GenotypeGVCFs --genomicsdb-shared-posixfs-optimizations --reference /data1/EquCab/_ECA30/Equus_caballus.EquCab3.0.dna_sm.toplevel.fa/ -V gendb://ECA3_GenomicsDB_260/3 -O ECA3_GenomicsDB_260.3.g.vcf.gz; 21:16:35.251 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data1/_software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 17, 2021 9:16:35 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 21:16:35.496 INFO GenotypeGVCFs - ------------------------------------------------------------; 21:16:35.497 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.1.8.1; 21:16:35.497 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 21:16:35.497 INFO GenotypeGVCFs - Executing as ccastane9@andersserver-01.cvm.tamu.edu on Linux v3.10.0-1127.19.1.el7.x86_64 amd64; 21:16:35.497 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_275-b01; 21:16:35.497 INFO GenotypeGVCFs - Start Date/Time: January 17, 2021 9:16:35 PM CST; 21:16:35.497 INFO GenotypeGVCFs - ------------------------------------",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-761953839:2437,Load,Loading,2437,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-761953839,1,['Load'],['Loading']
Performance,"samjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 /opt/Software/gatk/build/libs/gatk-package-4.beta.5-70-gdc3237e-SNAPSHOT-spark.jar PrintReadsSpark -I /gatk4/output.bam -O /gatk4/output_3.bam --sparkMaster yarn-client; Warning: Master yarn-client is deprecated since 2.0. Please use master ""yarn"" with specified deploy mode instead.; 18:11:33.604 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 18:11:33.737 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/opt/Software/gatk/build/libs/gatk-package-4.beta.5-70-gdc3237e-SNAPSHOT-spark.jar!/com/intel/gkl/native/libgkl_compression.so; [October 13, 2017 6:11:33 PM CST] PrintReadsSpark --output /gatk4/output_3.bam --input /gatk4/output.bam --sparkMaster yarn-client --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --interval_merging_rule ALL --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --gcs_max_retries 20 --disableToolDefaultReadFilters false; [October 13, 2017 6:11:33 PM CST] Executing as hdfs@mg on Linux 3.10.0-514.el7.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_91-b14; Version: 4.beta.5-70-gdc3237e-SNAPSHOT; 18:11:33.870 INFO PrintReadsSpark - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 18:11:3",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-336412775:1859,Load,Loading,1859,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-336412775,1,['Load'],['Loading']
Performance,sciiLineReader Creating an indexable source for an AsciiFeatureCodec using a stream that is neither a PositionalBufferedStream nor a BlockCompressedInputStream; > 12:28:18.125 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/simple_uniprot_Dec012014.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/simple_uniprot/hg38/simple_uniprot_Dec012014.tsv; > 12:28:18.424 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/Familial_Cancer_Genes.no_dupes.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/familial/hg38/Familial_Cancer_Genes.no_dupes.tsv; > 12:28:18.442 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/hg38_All_20170710.vcf.gz -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/dbsnp/hg38/hg38_All_20170710.vcf.gz; > 12:28:18.442 INFO DataSourceUtils - Setting lookahead cache for data source: dbSNP : 100000; > 12:28:18.452 INFO FeatureManager - Using codec VCFCodec to read file file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/dbsnp/hg38/hg38_All_20170710.vcf.gz; > 12:28:18.599 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/hg38_All_20170710.vcf.gz -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/dbsnp/hg38/hg38_All_20170710.vcf.gz; > 12:28:19.018 INFO FeatureManager - Using codec VCFCodec to read file file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/dbsnp/hg38/hg38_All_20170710.vcf.gz; > 12:28:19.213 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/CancerGeneCensus_Table_1_full_2012-03-15.txt -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/cancer_gene_census/hg38/CancerGeneCensus_Table_1_full_2012-03-15.txt; > 12:28:19.227 INFO DataSourceUtils -,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6708#issuecomment-661776975:11525,cache,cache,11525,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6708#issuecomment-661776975,1,['cache'],['cache']
Performance,"sh Babadi <mehrtash@broadinstitute.org>; Date: Wed Nov 15 01:50:03 2017 -0500. Polished code, ready for review; ; gCNV computational kernel (initial release); ; renaming gammas_s to psi_s to uniformity (sample-specific unexplained variance); ; renamed determine_ploidy_and_depth.py to cohort_determine_ploidy_and_depth.py; finite-temperature forward-backward algorithm; in the ploidy model, replaced alpha_j (NB over-dispersion) with psi_j (unexplained variance) for uniformity. Also, added the possibility of sample-specific unexplained variance in the germline contig ploidy model; ; updated I/O routines and CLIs according to team discussion; ; updated I/O routines and CLIs according to team discussion; ; changed the output layout of the ploidy determination tool; refactored parts of io.py; upped the version to 0.3 as it is not backwards compatible anymore; ; case ploidy determination tool from a given ploidy model; major code cleanup and refactoring of I/O module; refactoring of common CLI script snippets; ; removed all ""targets""; some code cleanup; ; pad flat class bitmask w/ a given padding value in the hybrid q_c_expectation_mode; option to disable annealing and keep the temperature fixed; ; bugfix in finite-temperature forward-backward; further refactoring of model I/O; ; the option to take a previously trained model as starting point in cohort CLI; the option to take previous calls as a starting point in cohort CLI; ; option to save and load adamax moments; ; import/export adamax bias correction tensor; ; refactoring related to fancy opt I/O; added average ploidy column to read depth; updated docs of hybrid inference; ; modeling intervals can span multiple contigs now; ploidy can change; across contigs with no issue; ; save/load adamax state to .npy instead of .tsv for speed; ; part 1 of doc updates; ; part 2 of doc updates; ; part 3 of doc updates; ; part 4 of doc updates; ; bumped version to 0.5; readme; ; update readme; ; last minute stylistic doc updates.; ````",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598:11775,load,load,11775,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598,2,['load'],['load']
Performance,"simply be to downsample and scale likelihoods when estimating global parameters. Addresses #2884.; - [x] Even though the simple copy-ratio model is much faster, it still takes ~15-20 minutes for 100 iterations on WGS, so we can downsample here too.; - [x] Integration tests are still needed; again, these might not test for correctness.; - I've added the ability to specify a prior for the minor-allele fraction, which alleviates the problem of residual bias in balanced segments.; - I've reduced the verbosity of the modeled-segments file. I only report posterior mode and 10%, 50%, and 90% deciles. Global parameters have the full deciles output in the .param files, but I removed the mode and highest density credible interval (because of the below item).; - [x] Some residual bias remains in the estimate of the minor-allele fraction posterior mode. This is simply because we are performing kernel density estimation of a bounded quantity. One possibility would be to logit transform to an unbounded support, perform the estimation, then transform back. EDIT: Just removed kernel density estimation for now, partly due to #3599 as well.; - Hmm, actually still a tiny bit of residual bias. This is apparent e.g. in WGS normals. I think focusing on a new allele-fraction model rather than trying to figure out where the old one is failing would be best.; - [x] For small bins (250bp), the copy-ratio model is currently a bit memory intensive, since it stores an outlier indicator boolean for every data point (it gets by with -Xmx12g for 100 iterations at 250bp). There is no easy away around storing this at the GibbsSampler level (although we could make some non-trivial changes to that code, as @davidbenjamin suggested long ago at https://github.com/broadinstitute/gatk-protected/issues/195). However, I got rid of these at the CopyRatioModeller level. If we want to go down in memory, we could move to a BitSet, but I'm not sure what the performance hit will be. EDIT: It was trivial to switch",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:6745,perform,perform,6745,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828,1,['perform'],['perform']
Performance,"sion: 2.27.1; 05:39:39.306 INFO CNNScoreVariants - Built for Spark Version: 2.4.5; 05:39:39.307 INFO CNNScoreVariants - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 05:39:39.307 INFO CNNScoreVariants - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 05:39:39.307 INFO CNNScoreVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 05:39:39.307 INFO CNNScoreVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 05:39:39.307 INFO CNNScoreVariants - Deflater: IntelDeflater; 05:39:39.307 INFO CNNScoreVariants - Inflater: IntelInflater; 05:39:39.307 INFO CNNScoreVariants - GCS max retries/reopens: 20; 05:39:39.307 INFO CNNScoreVariants - Requester pays: disabled; 05:39:39.307 INFO CNNScoreVariants - Initializing engine; 05:39:39.905 INFO FeatureManager - Using codec VCFCodec to read file file:///home/fmbuga/gatk4_gcp_wgs/06_vcf_raw/SRR16299720_dedup_AORRG_recal_raw.vcf; 05:39:40.108 INFO CNNScoreVariants - Done initializing engine; 05:39:40.109 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/fmbuga/.conda/envs/gatk4/share/gatk4-4.2.6.1-1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; 05:39:40.429 INFO CNNScoreVariants - Done scoring variants with CNN.; 05:39:40.429 INFO CNNScoreVariants - Shutting down engine; [October 9, 2022 5:39:40 AM PDT] org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants done. Elapsed time: 0.02 minutes.; Runtime.totalMemory()=1903165440; java.lang.NullPointerException; 	at org.broadinstitute.hellbender.utils.runtime.ProcessControllerAckResult.hasMessage(ProcessControllerAckResult.java:49); 	at org.broadinstitute.hellbender.utils.runtime.ProcessControllerAckResult.getDisplayMessage(ProcessControllerAckResult.java:69); 	at org.broadinstitute.hellbender.utils.runtime.StreamingProcessController.waitForAck(StreamingProcessController.java:235); 	at org.broadinstitute.hellbender.utils.python.StreamingPythonScriptExecutor.waitForAck(StreamingPythonScriptExecutor.java:216);",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7811#issuecomment-1274925490:3190,Load,Loading,3190,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7811#issuecomment-1274925490,1,['Load'],['Loading']
Performance,"spark.storage.BlockManager.reregister(BlockManager.scala:236); 	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:522); 	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:547); 	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547); 	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547); 	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953); 	at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:547); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308); 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180); 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); 17/10/18 17:35:58 INFO BlockManagerMaster: BlockManagerMaster stopped; 17/10/18 17:35:58 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker-1,5,main]; java.lang.OutOfMemoryError: Java heap space; 	at htsjdk.samtools.BAMRecordCodec.decode(BAMRecordCodec.java:208); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.getNextRecord(BAMFileReader.java:829); 	at htsjdk.samtools.BAMFileReader$BAMFileIndexIterator.getNextRecord(BAMFileReader.java:981); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.advance(BAMFileReader.java:803); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.java:797); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.java:765); 	at org.seqdoop.hadoop_bam.BAMRecordReader.nextKeyValue(BAMRecordRe",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-337554749:5700,concurren,concurrent,5700,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-337554749,1,['concurren'],['concurrent']
Performance,spatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); 	at com.sun.proxy.$Proxy2.stop(Unknown Source); 	at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:377); 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.testng.TestNGException:An error occurred while instantiating class org.broadinstitute.hellbender.engine.spark.ReadsPreprocessingPipelineSparkTestData. Check to make sure it can be instantiated; 	at org.testng.internal.InstanceCreator.createInstanceUsingObjectFactory(InstanceCreator.java:134); 	at org.testng.internal.InstanceCreator.createInstance(InstanceCreator.java:79); 	at org.testng.internal.ClassImpl.getDefaultInstance(ClassImpl.java:110); 	at org.testng.internal.ClassImpl.getInstances(ClassImpl.java:195); 	at org.testng.TestClass.getInstances(TestClass.java:102); 	at org.testng.TestClass.initTestClassesAndInstances(TestClass.java:82); 	at org.testng.TestClass.init(TestClass.java:74); 	at org.testng.TestClass.<init>(TestClass.java:39); 	at org.testng.TestRunner.initMethods(Tes,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5787#issuecomment-472107858:2131,concurren,concurrent,2131,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5787#issuecomment-472107858,1,['concurren'],['concurrent']
Performance,"st?. ```; 02 Apr 2022 16:34:31,433 DEBUG: 	[April 2, 2022 4:34:31 PM PDT] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 5,993.06 minutes.; 02 Apr 2022 16:34:31,438 DEBUG: 	Runtime.totalMemory()=178017796096; 02 Apr 2022 16:34:31,443 DEBUG: 	Tool returned:; 02 Apr 2022 16:34:31,448 DEBUG: 	true; 02 Apr 2022 16:34:34,663 INFO : Will consolidate the workspace using consolidate_genomicsdb_array; 02 Apr 2022 16:34:34,723 INFO : Consolidating contig folder: /home/exacloud/gscratch/prime-seq/workDir/344c6137-8a85-103a-821a-f8f3fc86deba/Job1.work/WGS_v2_713_2ndMerge.gdb/1$1$223616942; 02 Apr 2022 16:34:34,748 INFO : 	/home/exacloud/gscratch/prime-seq/bin/consolidate_genomicsdb_array -w /home/exacloud/gscratch/prime-seq/workDir/344c6137-8a85-103a-821a-f8f3fc86deba/Job1.work/WGS_v2_713_2ndMerge.gdb --shared-posixfs-optimizations -a 1$1$223616942; 02 Apr 2022 16:34:34,754 DEBUG: using path: /home/exacloud/gscratch/prime-seq/bin:/home/exacloud/gscratch/prime-seq/bin/:/home/exacloud/gscratch/prime-seq/java/current/bin/:/home/exacloud/gscratch/prime-seq/bin/:/usr/local/bin:/usr/bin; 02 Apr 2022 16:34:35,059 DEBUG: 	16:34:35.059 info consolidate_genomicsdb_array - pid=34848 tid=34848 Starting consolidation of 1$1$223616942 in /home/exacloud/gscratch/prime-seq/workDir/344c6137-8a85-103a-821a-f8f3fc86deba/Job1.work/WGS_v2_713_2ndMerge.gdb; 02 Apr 2022 16:34:36,091 DEBUG: 	Using buffer_size=10485760 for consolidation; 02 Apr 2022 16:34:36,097 DEBUG: 	Number of fragments to consolidate=26; 02 Apr 2022 16:34:36,101 DEBUG: 	Sat Apr 2 16:34:36 2022 Memory stats beginning consolidation size=483MB resident=379MB share=6MB text=13MB lib=0 data=371MB dt=0; 02 Apr 2022 16:34:36,105 DEBUG: 	Sat Apr 2 16:34:36 2022 Memory stats Start: batch 1/1 size=503MB resident=379MB share=6MB text=13MB lib=0 data=391MB dt=0; 02 Apr 2022 16:38:15,825 DEBUG: 	Sat Apr 2 16:38:15 2022 Memory stats after alloc for attribute=END size=25GB resident=25GB share=7MB text=13MB lib",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1087750975:1171,optimiz,optimizations,1171,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1087750975,1,['optimiz'],['optimizations']
Performance,stitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvbW9kZWxpbmcvQkdNTVZhcmlhbnRBbm5vdGF0aW9uc1Njb3Jlci5qYXZh) | `0.000% <0.000%> (ø)` | |; | [...oadinstitute/hellbender/utils/NaturalLogUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/7954/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9OYXR1cmFsTG9nVXRpbHMuamF2YQ==) | `77.143% <0.000%> (ø)` | |; | [...ls/clustering/BayesianGaussianMixtureModeller.java](https://codecov.io/gh/broadinstitute/gatk/pull/7954/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9jbHVzdGVyaW5nL0JheWVzaWFuR2F1c3NpYW5NaXh0dXJlTW9kZWxsZXIuamF2YQ==) | `0.000% <0.000%> (ø)` | |; | [.../tools/walkers/vqsr/scalable/data/VariantType.java](https://codecov.io/gh/broadinstitute/gatk/pull/7954/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvZGF0YS9WYXJpYW50VHlwZS5qYXZh) | `60.000% <60.000%> (ø)` | |; | [.../walkers/vqsr/scalable/SystemCommandUtilsTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/7954/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvU3lzdGVtQ29tbWFuZFV0aWxzVGVzdC5qYXZh) | `60.870% <60.870%> (ø)` | |; | [.../scalable/data/LabeledVariantAnnotationsDatum.java](https://codecov.io/gh/broadinstitute/gatk/pull/7954/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=b,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7954#issuecomment-1191010834:4340,scalab,scalable,4340,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7954#issuecomment-1191010834,1,['scalab'],['scalable']
Performance,"t how this should be done. Complete coverage here will be difficult and perhaps not worth the effort, but I can probably put in a few tests that make sure changing the hard-coded values in master and doing the same via the exposed parameters in this branch have the same effect on a few existing test cases. However, while I'm doing the last three, I wonder if we could run whatever canonical evaluations/optimizations we have to see whether it's worth consolidating some of the parameter sets at this stage? I think there's an argument for having at least two sets (haplotype-to-reference + read-to-haplotype), but I'm not sure how to justify having a separate set for dangling heads/tails. But also not sure which set the latter should be consolidated with---@jamesemery thoughts? Again, let me reiterate that it seems that many of these parameter values were chosen arbitrarily (or, if not, that the procedure for choosing them has been lost). As a start, you can see the results of some optimizations I did on the CHM mix on slide 15 at https://docs.google.com/presentation/d/1zGuquAZWSUQ-wNxp8D6HhGNjIaFcV0_X9WAS4LODbEo/edit?usp=sharing Here, I optimized over haplotype-to-reference + read-to-haplotype SW parameters on various metrics after variant normalization using vcfeval. These optimizations were done using the Bayesian optimization framework I prototyped long ago (see https://github.com/broadinstitute/gatk-evaluation/tree/master/pipeline-optimizer and https://docs.google.com/presentation/d/1t5WOAEOMp0xAzJgpKbP68BUnclNYfIVRrDSL9wl1-3A/edit?usp=sharing); this entailed running parameter scans using a local Cromwell on my desktop. Probably this optimization work could be redone relatively easily using the Neptune framework put together by @dalessioluca, which was still in development at the time I did this work. Happy to share the resources and scripts I used if we go down this route; they are pretty lightweight. See more discussion starting here: https://github.com/broadinstit",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6885#issuecomment-891907471:1520,optimiz,optimizations,1520,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6885#issuecomment-891907471,1,['optimiz'],['optimizations']
Performance,"t three, I wonder if we could run whatever canonical evaluations/optimizations we have to see whether it's worth consolidating some of the parameter sets at this stage? I think there's an argument for having at least two sets (haplotype-to-reference + read-to-haplotype), but I'm not sure how to justify having a separate set for dangling heads/tails. But also not sure which set the latter should be consolidated with---@jamesemery thoughts? Again, let me reiterate that it seems that many of these parameter values were chosen arbitrarily (or, if not, that the procedure for choosing them has been lost). As a start, you can see the results of some optimizations I did on the CHM mix on slide 15 at https://docs.google.com/presentation/d/1zGuquAZWSUQ-wNxp8D6HhGNjIaFcV0_X9WAS4LODbEo/edit?usp=sharing Here, I optimized over haplotype-to-reference + read-to-haplotype SW parameters on various metrics after variant normalization using vcfeval. These optimizations were done using the Bayesian optimization framework I prototyped long ago (see https://github.com/broadinstitute/gatk-evaluation/tree/master/pipeline-optimizer and https://docs.google.com/presentation/d/1t5WOAEOMp0xAzJgpKbP68BUnclNYfIVRrDSL9wl1-3A/edit?usp=sharing); this entailed running parameter scans using a local Cromwell on my desktop. Probably this optimization work could be redone relatively easily using the Neptune framework put together by @dalessioluca, which was still in development at the time I did this work. Happy to share the resources and scripts I used if we go down this route; they are pretty lightweight. See more discussion starting here: https://github.com/broadinstitute/gatk/issues/5564#issuecomment-710107566. Alternatively, we could merge this branch to expose the parameters now and punt on consolidating/optimizing them. I'm not completely convinced we should even do the former unless we are going to follow through on the latter, but happy to defer to others. Finally, note also there is one code opti",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6885#issuecomment-891907471:1819,optimiz,optimizations,1819,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6885#issuecomment-891907471,2,['optimiz'],"['optimization', 'optimizations']"
Performance,"t2 - HTSJDK Version: 3.0.5; > 14 15:07:52.440 INFO Mutect2 - Picard Version: 3.0.0; > 15 15:07:52.440 INFO Mutect2 - Built for Spark Version: 3.3.1; > 16 15:07:52.440 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 2; > 17 15:07:52.441 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; > 18 15:07:52.441 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; > 19 15:07:52.442 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; > 20 15:07:52.442 INFO Mutect2 - Deflater: IntelDeflater; > 21 15:07:52.442 INFO Mutect2 - Inflater: IntelInflater; > 22 15:07:52.442 INFO Mutect2 - GCS max retries/reopens: 20; > 23 15:07:52.443 INFO Mutect2 - Requester pays: disabled; > 24 15:07:52.443 INFO Mutect2 - Initializing engine; > 25 15:07:52.848 INFO FeatureManager - Using codec VCFCodec to read file file://ref_nobackup/af-only-gnomad.hg38.vcf.gz; > 26 15:07:53.126 INFO Mutect2 - Done initializing engine; > 27 15:07:53.196 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/scicore/soft/apps/GATK/4.4.0.0-GCCcore-10.3.0-Java-17/gatk-package-4.4.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; > 28 15:07:53.201 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/scicore/soft/apps/GATK/4.4.0.0-GCCcore-10.3.0-Java-17/gatk-package-4.4.0.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; > 29 15:07:53.223 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; > 30 15:07:53.223 INFO IntelPairHmm - Available threads: 2; > 31 15:07:53.224 INFO IntelPairHmm - Requested threads: 4; > 32 15:07:53.224 WARN IntelPairHmm - Using 2 available threads, but 4 were requested; > 33 15:07:53.224 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; > 34 15:07:53.231 WARN Mutect2 - Note that the Mutect2 reference confidence mode is in BETA -- the likelihoods model and output format are subject to change in subsequent versions.; > 35 15:07:53.314 IN",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7849#issuecomment-1597198632:2897,Load,Loading,2897,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7849#issuecomment-1597198632,1,['Load'],['Loading']
Performance,"tSpot(TM) 64-Bit Server VM, Java 1.8.0_91); Type in expressions to have them evaluated.; Type :help for more information.; Spark context available as sc (master = yarn-client, app id = application_1507683879816_0007).; Wed Oct 11 14:25:24 CST 2017 Thread[main,5,main] java.io.FileNotFoundException: derby.log (Permission denied); ----------------------------------------------------------------; Wed Oct 11 14:25:24 CST 2017:; Booting Derby version The Apache Software Foundation - Apache Derby - 10.11.1.1 - (1616546): instance a816c00e-015f-0a1b-f1bd-00002ce33928 ; on database directory /tmp/spark-98953d35-8594-4907-b4a5-0870f1d17b3e/metastore with class loader sun.misc.Launcher$AppClassLoader@5c647e05 ; Loaded from file:/opt/cloudera/parcels/CDH-5.12.1-1.cdh5.12.1.p0.3/jars/derby-10.11.1.1.jar; java.vendor=Oracle Corporation; java.runtime.version=1.8.0_91-b14; user.dir=/opt/Software/gatk; os.name=Linux; os.arch=amd64; os.version=3.10.0-514.el7.x86_64; derby.system.home=null; Database Class Loader started - derby.database.classpath=''; 17/10/11 14:25:33 WARN metastore.ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.1.0-cdh5.12.1; 17/10/11 14:25:33 WARN metastore.ObjectStore: Failed to get database default, returning NoSuchObjectException; SQL context available as sqlContext. **./gradlew bundle**; **[root@com1 gatk]# ./gradlew bundle; when I executed the command ”./gradlew bundle”， it appeared the error in the last ，did this matter？**. .......; [loading ZipFileIndexFileObject[/root/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.core/jackson-databind/2.6.5/d50be1723a09be903887099ff2014ea9020333/jackson-databind-2.6.5.jar(com/fasterxml/jackson/databind/annotation/JsonSerialize$Inclusion.class)]]; [loading ZipFileIndexFileObject[/root/.gradle/caches/modules-2/files-2.1/org.apache.logging.log4j/log4j-core/2.5/7ed845de1dfe070d43511fab1784e6c4118398/log4j-core-2.5.jar(org/apac",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-335696240:2341,Load,Loader,2341,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-335696240,1,['Load'],['Loader']
Performance,"ta used was pretty small: chr1-2 training (~20k positive training/truth variants, ~50k negative training variants; note also that the threshold for determining negative training was not tuned---a threshold corresponding to a 98% truth sensitivity was arbitrarily chosen), chr3 validation (~50k variants), and chr4-6 test (~150k variants). The LL score is calculated from a validation set held out from the training/truth positives used to train the model, while the F1 score is calculated using ""orthogonal truth"" positives/negatives determined using 3 families of ~30 trios each. However, there's some arbitrariness in how we define the boundary for the latter positives/negatives, and hence some arbitrariness in the F1 score itself. But I'd expect using gold-standard GIAB truth would be more straightforward. Not sure how much we can conclude, but that the validation and test F1s are similar and that the validation LL score isn't *too* far off are encouraging. That said, there is a pretty big drop in recall when optimizing LL. But we should also expect some discrepancy between LL and F1, according to one of the papers linked above. I would hope that with more variants or reliable training/truth (as in your data), things might stabilize or line up better. I'll try running with more malaria data, as well. The following trios x sites heatmap (top plot) for the validation set might better illustrate the arbitrariness in F1 (click to enlarge):. ![image](https://user-images.githubusercontent.com/11076296/158385585-1a0dfe8e-d4b7-4770-aed0-19ad81162c92.png). Here, yellow = het errors (since these are supposed to be clonal malaria samples), red = Mendelian errors, grey = no calls, green = Mendelian consistency, white = reference. The second plot shows the training/truth positives used to train the model and to calculate the LL score in the validation shard. The third plot shows the ""orthogonal truth"" positives/negatives used to calculate F1. So we can see that the difficulty in deri",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1067396431:1307,optimiz,optimizing,1307,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1067396431,1,['optimiz'],['optimizing']
Performance,"taProgramGroup.java. And `6. ` from above. ---; ## FilterLongReadAlignmentsSAMSpark. 1. In the one-line summary, I'm not clear on what is meant by ""Filters"". Based on the result file, seems like it collects metrics on each contig alignment.; 2. ; 3. If metrics, then DiagnosticsAndQCProgramGroup.java. And `6. ` from above. ---; ## FindBadGenomicKmersSpark. 1. The term ""copy number"" should be reserved in reference to CNV analyses. So instead, how about:; Identify sequence contexts that occur at high frequency in a reference; 2. Please define a kmer. If only a reference fasta is required (as listed under Inputs) great. But if the tool also depends on a FAI index and DICT dictionary, please do include them. Also, it would be good to provide an example of how such information is used in SV discovery, e.g. ""the resulting file can be given to FindBreakpointEvidenceSpark, which will then ignore such sequence contexts during analysis."" Also would be good to mention that the default kmer size (--k-size 51) is optimized for human if indeed this is the case.; 3. ReferenceProgramGroup.java. And `6. ` from above. ---; ## FindBreakpointEvidenceSpark. 1. Assembles and aligns contigs of genomic breakpoint regions associated with structural variants ; 2. Overview and Notes could use finessing but let's leave this for next year. One thing to include is a reference to FermiLite for those seeking more information. A publication would be best. And `6. ` from above. ---; ## StructuralVariationDiscoveryPipelineSpark. 1. Runs the structural variant discovery workflow on a single sample in Spark ; 2. Fyi we sanction a ""Caveats"" section, which is likely more appropriate for the PE expectation and the fact that low coverage data less than 30x will give suboptimal results. Also, should mention this workflow is meant only for WGS. Or is it the case one case use exome data? Second note on BwaMemIndexImageCreator could be consolidated with the same under Inputs. Same with third note. And `6. ` from",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3948#issuecomment-351467451:3315,optimiz,optimized,3315,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3948#issuecomment-351467451,1,['optimiz'],['optimized']
Performance,"tage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, xx.xx.xx.xx, executor 0): java.lang.IllegalStateException: unread block data; at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2740); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1567); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:309); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 18/04/24 14:34:27 INFO DAGScheduler: Job 0 failed: first at ReadsSparkSource.java:221, took 4.816635 s; ```; Our system is an HPC, where all the nodes share the same file system. I run my SPARK on only one node to test the software. I red elesewhere that this might be aproblem of missing jars, so I tried to inlcude these libraries in the SPARK jar folder and added the option:; `; --conf [--jars=""~/bin/spark-2.2.0-bin-hadoop2.7/jars/hbase-client-1.4.3.jar, ~/bin/spark-2.2.0-bin-hadoop2.7/jars/hbase-common-1.4.3.jar, ~/bin/spark-2.2.0-bin-hadoop2.7/jars/hbase-hadoop2-compat-1.4.3.jar, ~/bin/spark-2.2.0-bin-hadoop2.7/jars/hive-hbase-handler-1.2.1.spark2.jar"" ]`. But I still get the error. Is GATK using hbase? If yes shall some jars be included to a local SPARK system to enable it t",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383916494:1907,concurren,concurrent,1907,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383916494,1,['concurren'],['concurrent']
Performance,"tch/DBC/BCRBIOIN/SHARED/analysis/######/######/data/wgs/data/mutect2//tumour_samples/ML13_Ab/ML13_Ab.orientation_filtered.vcf.gz --exclude-filtered -O /scratch/DBC/BCRBIOIN/SHARED/analysis/######/######/data/wgs/data/mutect2//tumour_samples/ML13_Ab/ML13_Ab.passed.vcf.gz; Using GATK jar /gatk/gatk-package-4.1.8.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx12G -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -jar /gatk/gatk-package-4.1.8.1-local.jar SelectVariants -R /scratch/DBC/BCRBIOIN/SHARED/genomes/homo_sapiens/GRCh38/dna/GRCh38.d1.vd1.fa -V /scratch/DBC/BCRBIOIN/SHARED/analysis/######/######/data/wgs/data/mutect2//tumour_samples/ML13_Ab/ML13_Ab.orientation_filtered.vcf.gz --exclude-filtered -O /scratch/DBC/BCRBIOIN/SHARED/analysis/######/######/data/wgs/data/mutect2//tumour_samples/ML13_Ab/ML13_Ab.passed.vcf.gz; 10:52:40.838 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Sep 02, 2020 10:52:41 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 10:52:41.263 INFO SelectVariants - ------------------------------------------------------------; 10:52:41.263 INFO SelectVariants - The Genome Analysis Toolkit (GATK) v4.1.8.1; 10:52:41.264 INFO SelectVariants - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:52:41.264 INFO SelectVariants - Executing as ######@dav002.prv.davros.compute.estate on Linux v3.10.0-327.3.1.el7.x86_64 amd64; 10:52:41.264 INFO SelectVariants - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 10:52:41.265 INFO SelectVariants - Start Date/Time: September 2, 2020 10:52:40 AM GMT; 10:52:41.265 INFO SelectVariants - --------------------------",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6446#issuecomment-685695328:1938,Load,Loading,1938,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6446#issuecomment-685695328,1,['Load'],['Loading']
Performance,"te_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 /share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-spark.jar CountReadsSpark --input /project/casa/gcad/test/HG04302.alt_bwamem_GRCh38DH.20150718.GBR.low_coverage.cram --reference file:///restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --spark-master yarn; 2019-01-07 11:33:18 WARN SparkConf:66 - The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 2019-01-07 11:33:19 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 11:33:24.377 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 11:33:24.549 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 11:33:26.271 INFO CountReadsSpark - ------------------------------------------------------------; 11:33:26.272 INFO CountReadsSpark - The Genome Analysis Toolkit (GATK) v4.0.12.0; 11:33:26.272 INFO CountReadsSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:33:26.272 INFO CountReadsSpark - Executing as farrell@scc-hadoop.bu.edu on Linux v2.6.32-754.6.3.el6.x86_64 amd64; 11:33:26.273 INFO CountReadsSpark - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 11:33:26.273 INFO CountReadsSpark - Start Date/Time: January 7, 2019 11:33:24 AM EST; 11:33:26.273 INFO CountReadsSpark - ------------------------------------------------------------; 11:33:26.273 INFO CountReadsSpark - ------------------------------------------------------------; 11:33:26.275 INFO CountReadsSpark - HTSJDK Ve",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:2542,Load,Loading,2542,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,1,['Load'],['Loading']
Performance,tect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 11:47:51.056 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 11:47:51.057 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 11:47:51.057 INFO Mutect2 - Deflater: IntelDeflater; 11:47:51.057 INFO Mutect2 - Inflater: IntelInflater; 11:47:51.057 INFO Mutect2 - GCS max retries/reopens: 20; 11:47:51.057 INFO Mutect2 - Requester pays: disabled; 11:47:51.057 INFO Mutect2 - Initializing engine; 11:47:51.372 INFO FeatureManager - Using codec VCFCodec to read file file:///home/proj/stage/cancer/reference/GRCh37/variants/dbsnp_grch37_b138.vcf.gz; 11:47:51.457 INFO FeatureManager - Using codec BEDCodec to read file file:///home/proj/stage/cancer/reference/target_capture_bed/production/balsamic/gicfdna_3.1_hg19_design.bed; 11:47:51.465 INFO IntervalArgumentCollection - Processing 74592 bp from intervals; 11:47:51.474 INFO Mutect2 - Done initializing engine; 11:47:51.487 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/proj/bin/conda/envs/D_UMI_APJ/share/gatk4-4.1.8.0-0/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 11:47:51.489 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/proj/bin/conda/envs/D_UMI_APJ/share/gatk4-4.1.8.0-0/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 11:47:51.534 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; 11:47:51.534 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 11:47:51.534 INFO IntelPairHmm - Available threads: 16; 11:47:51.534 INFO IntelPairHmm - Requested threads: 4; 11:47:51.534 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 11:47:51.557 INFO ProgressMeter - Starting traversal; 11:47:51.557 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 11:47:52.683 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 0.0;,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-652912482:3647,Load,Loading,3647,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-652912482,1,['Load'],['Loading']
Performance,"tect2 - Start Date/Time: April 23, 2019 8:27:05 AM UT; 08:27:10.885 INFO Mutect2 - -----------------------------------------------------------; 08:27:10.886 INFO Mutect2 - -----------------------------------------------------------; 08:27:10.887 INFO Mutect2 - HTSJDK Version: 2.19.; 08:27:10.887 INFO Mutect2 - Picard Version: 2.19.; 08:27:10.887 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 2. 08:27:10.888 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : fals; 08:27:10.888 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : tru; 08:27:10.888 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : fals; 08:27:10.888 INFO Mutect2 - Deflater: IntelDeflate; 08:27:10.889 INFO Mutect2 - Inflater: IntelInflate; 08:27:10.889 INFO Mutect2 - GCS max retries/reopens: 2; 08:27:10.889 INFO Mutect2 - Requester pays: disable; 08:27:10.889 INFO Mutect2 - Initializing engin; 08:27:11.333 INFO Mutect2 - Done initializing engin; 08:27:11.381 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/gatk/gatk-package-4.1.1.0-local.jar!/com/intel/gkl/native/libgkl_utils.s; 08:27:11.383 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk/gatk-package-4.1.1.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.s; 08:27:11.426 INFO **IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHM**; 08:27:11.427 INFO IntelPairHmm - Available threads: 4; 08:27:11.428 INFO IntelPairHmm - Requested threads: 4; 08:27:11.428 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementatio; 08:27:11.432 INFO Mutect2 - Shutting down engin; [April 23, 2019 8:27:11 AM UTC] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.09 minutes.; Runtime.totalMemory()=190840832; java.lang.IllegalArgumentException: samples cannot be empt; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:724); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.Refer",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4665#issuecomment-485729136:2415,Load,Loading,2415,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4665#issuecomment-485729136,1,['Load'],['Loading']
Performance,"tened for clarity in the following commands. ```; bash faa.sh ; Using GATK jar /app/gatk-package-4.1.8.0-local.jar; Running:; /bin/java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /app/gatk-package-4.1.8.0-local.jar FilterAlignmentArtifacts -V /output/sample.FilterMutectCalls.vcf.gz -R /db/hs37d5.fa --bwa-mem-index-image /db/hg38.fa.img -I /output/sample.Mutect2.bam -O sample.somatic_filter.test.vcf.gz --use-jdk-inflater true; 19:11:56.929 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/app/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 19:11:56.943 INFO NativeLibraryLoader - Loading libgkl_smithwaterman.so from jar:file:/app/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_smithwaterman.so; 19:11:56.944 INFO SmithWatermanAligner - Using AVX accelerated SmithWaterman implementation; 19:11:57.168 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/app/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jul 19, 2020 7:11:57 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 19:11:57.324 INFO FilterAlignmentArtifacts - ------------------------------------------------------------; 19:11:57.324 INFO FilterAlignmentArtifacts - The Genome Analysis Toolkit (GATK) v4.1.8.0; 19:11:57.325 INFO FilterAlignmentArtifacts - For support and documentation go to https://software.broadinstitute.org/gatk/; 19:11:57.325 INFO FilterAlignmentArtifacts - Executing as foo@bar.local on Linux v2.6.32-696.6.3.el6.x86_64 amd64; 19:11:57.325 INFO FilterAlignmentArtifacts - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_261-b12; 19:11:57.325 INFO FilterAlignmentArtifacts - Start Date/Time: July 19, 2020 7:11:57 PM CST; 19:11:57.325 INFO FilterAlignmentArtifacts - -----",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-660645356:1348,Load,Loading,1348,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-660645356,1,['Load'],['Loading']
Performance,"ter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. 02:12 DEBUG: [kryo] Write: WrappedArray([NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED, NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED]); 18/04/24 17:41:31 INFO TaskSetManager: Starting task 0.2 in stage 2.0 (TID 7, xx.xx.xx.24, executor 1, partition 0, PROCESS_LOCAL, 6010 bytes); 18/04/24 17:41:31 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.24:44322 (size:",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:29005,concurren,concurrent,29005,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,1,['concurren'],['concurrent']
Performance,"ter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. 02:34 DEBUG: [kryo] Write: WrappedArray([NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED, NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED]); 18/04/24 17:41:53 INFO TaskSetManager: Starting task 0.3 in stage 2.0 (TID 8, xx.xx.xx.xx, executor 3, partition 0, PROCESS_LOCAL, 6010 bytes); 18/04/24 17:41:53 INFO TaskSetManager: Lost task 1.1 in stage 2.0 (TID 6) on xx.xx.xx.24, executor 1: o",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:31704,concurren,concurrent,31704,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,1,['concurren'],['concurrent']
Performance,"ter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. 18/04/24 17:40:52 INFO TaskSetManager: Lost task 0.0 in stage 2.0 (TID 3) on xx.xx.xx.25, executor 2: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory)) [duplicate 1]; 01:33 DEBUG: [kryo] Write: WrappedArray([NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED, NC_00",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:25818,concurren,concurrent,25818,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,1,['concurren'],['concurrent']
Performance,"ter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. Driver stacktrace:; 18/04/24 17:42:02 INFO DAGScheduler: Job 2 failed: count at PathSeqPipelineSpark.java:245, took 117.869179 s; 18/04/24 17:42:02 INFO SparkUI: Stopped Spark web UI at http://xx.xx.xx.xx:4040; 18/04/24 17:42:02 INFO StandaloneSchedulerBackend: Shutting down all executors; 18/04/24 17:42:02 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down; 1",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:35442,concurren,concurrent,35442,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,1,['concurren'],['concurrent']
Performance,ter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$c,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:40638,concurren,concurrent,40638,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,1,['concurren'],['concurrent']
Performance,"terFactory - IntelInflater is not supported, using Java.util.zip.Inflater; >; > 16:17:05.932 WARN IntelDeflaterFactory - IntelInflater is not supported, using Java.util.zip.Inflater; >; > 16:17:06.503 INFO FeatureManager - Using codec VCFCodec to read file file:///home/robert/test/snps.vcf; >; > 16:17:06.539 INFO IntervalArgumentCollection - Processing 61464 bp from intervals; >; > 16:17:06.551 INFO HaplotypeCaller - Done initializing engine; >; > 16:17:06.573 INFO HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output; >; > 16:17:06.588 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; >; > 16:17:06.589 **WARN** NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils347167544598047196.so: /tmp/libgkl_utils347167544598047196.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > 16:17:06.589 **WARN** IntelPairHmm - Intel GKL Utils not loaded; >; > 16:17:06.589 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; >; > 16:17:06.589 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; >; > 16:17:06.590 **WARN** NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils6186849302609329058.so: /tmp/libgkl_utils6186849302609329058.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > 16:17:06.590 **WARN** IntelPairHmm - Intel GKL Utils not loaded; >; > 16:17:06.591 **WARN** PairHMM - ***WARNING: Machine does not have the AVX instruction set support needed for the accelerated AVX PairHmm. F",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600:5389,load,load,5389,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600,1,['load'],['load']
Performance,"testBQSRBucket timing out... it took >10min for me, no wonder. Adding -L fixes that. Looks like there's a perf bug somewhere in my read-loading code.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/987#issuecomment-148549746:136,load,loading,136,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/987#issuecomment-148549746,1,['load'],['loading']
Performance,"the initial description here says there is some performance improvement for MD itself lurking in some of DA's branches. ; @davidadamsphd can you clarify what that is and what you think should be done in this ticket? I'm a bit confused here - the ticket mentions some mysterious code that is going to improve our lives etc but does not include a reference to it. Does the code exist, does it work, can we use it?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1100#issuecomment-159144515:48,perform,performance,48,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1100#issuecomment-159144515,1,['perform'],['performance']
Performance,"the reply!; Certainly. `umask `returns `0022`. As such I reckon that is not the issue. Stacktrace in the bottom. The folder permission of the datastore folder is as follows:; `drwx--S---+ 26 vidprijatelj group 4096 Mar 14 15:29 Vid_database`. When changing to 766, the error disappears. ```; Tue Mar 14 15:37:57 CET 2023; Using GATK jar /appl/tools/versions/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Djava.io.tmpdir=zzz_tmpdir -Xmx128G -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -jar /appl/tools/versions/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar GenotypeGVCFs --reference /data/Scratch/References/ucsc.hg38.fa --variant gendb://Vid_database --output Step05_MultiSampleCalling/Vid.vcf.gz --intervals /data/Scratch/References/hg38_exome_v2.0.2_merged_probes_sorted_validated.annotated.bed --genomicsdb-shared-posixfs-optimizations True --merge-input-intervals; 15:37:59.895 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/appl/tools/versions/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 15:38:00.018 INFO GenotypeGVCFs - ------------------------------------------------------------; 15:38:00.018 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.3.0.0; 15:38:00.018 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 15:38:00.018 INFO GenotypeGVCFs - Executing as user@server; 15:38:00.018 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_362-b08; 15:38:00.019 INFO GenotypeGVCFs - Start Date/Time: March 14, 2023 3:37:59 PM CET; 15:38:00.019 INFO GenotypeGVCFs - ------------------------------------------------------------; 15:38:00.019 INFO GenotypeGVCFs - ------------------------------------------------------------; 15:38:00.019 INFO GenotypeGVCFs - HTSJDK Version: 3.0.1; 15:38",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8233#issuecomment-1468228918:1010,optimiz,optimizations,1010,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8233#issuecomment-1468228918,1,['optimiz'],['optimizations']
Performance,this happens while running the command ; ```; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx15500m\ ; -jar /root/gatk.jar Mutect2 -R gs://genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.fasta\ ; -I gs://cclebams/hg38_wes/CDS-00rz9N.hg38.bam -tumor BC1_HAEMATOPOIETIC_AND_LYMPHOID_TISSUE --germline-resource gs://gcp-public-data--gnomad/release/3.0/vcf/genomes/gnomad.genomes.r3.0.sites.vcf.bgz\ ; -pon gs://gatk-best-practices/somatic-hg38/1000g_pon.hg38.vcf.gz\ ; -L gs://fc-secure-d2a2d895-a7af-4117-bdc7-652d7d268324/7a157f4a-7d93-4a3e-aaf4-c41833463f5a/Mutect2/3be8ce8e-1075-4063-bc43-6f61e386c3f5/call-SplitIntervals/cacheCopy/glob-0fc990c5ca95eebc97c4c204e3e303e1/0000-scattered.interval_list\ ; -O output.vcf.gz --f1r2-tar-gz f1r2.tar.gz --gcs-project-for-requester-pays broad-firecloud-ccle --genotype-germline-sites true --genotype-pon-sites true --emit-ref-confidence GVCF; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7849#issuecomment-1128909634:761,cache,cacheCopy,761,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7849#issuecomment-1128909634,1,['cache'],['cacheCopy']
Performance,this is done. I retested on latest master 4353d54fd2d64ea1b4c8429986f83eb873a4d687 and there's no performance problem when using intervals for subsetting.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1532#issuecomment-220162567:98,perform,performance,98,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1532#issuecomment-220162567,1,['perform'],['performance']
Performance,this will be fixed by loading from the classpath #1903. keeping this ticket open so that we confirm that the problem is fixed.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1692#issuecomment-224991564:22,load,loading,22,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1692#issuecomment-224991564,1,['load'],['loading']
Performance,"tion true --disableAllReadFilters true --fixedChunkSize 100000 --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false; [September 17, 2016 12:08:44 PM EDT] Executing as kh3@rgcaahauva08091.rgc.aws.com on Linux 3.13.0-91-generic amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_101-b13; Version: Version:4.alpha.2-45-ga30af5a-SNAPSHOT; 12:08:44.930 INFO BwaSpark - Defaults.BUFFER_SIZE : 131072; 12:08:44.930 INFO BwaSpark - Defaults.COMPRESSION_LEVEL : 1; 12:08:44.930 INFO BwaSpark - Defaults.CREATE_INDEX : false; 12:08:44.930 INFO BwaSpark - Defaults.CREATE_MD5 : false; 12:08:44.930 INFO BwaSpark - Defaults.CUSTOM_READER_FACTORY : ; 12:08:44.930 INFO BwaSpark - Defaults.EBI_REFERENCE_SERVICE_URL_MASK : http://www.ebi.ac.uk/ena/cram/md5/%s; 12:08:44.930 INFO BwaSpark - Defaults.NON_ZERO_BUFFER_SIZE : 131072; 12:08:44.930 INFO BwaSpark - Defaults.REFERENCE_FASTA : null; 12:08:44.930 INFO BwaSpark - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 12:08:44.930 INFO BwaSpark - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 12:08:44.930 INFO BwaSpark - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 12:08:44.931 INFO BwaSpark - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 12:08:44.931 INFO BwaSpark - Defaults.USE_CRAM_REF_DOWNLOAD : false; 12:08:44.931 INFO BwaSpark - Deflater IntelDeflater; 12:08:44.931 INFO BwaSpark - Initializing engine; 12:08:44.931 INFO BwaSpark - Done initializing engine; 12:08:45.439 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 12:08:47.488 INFO BwaSpark - Shutting down engine; [September 17, 2016 12:08:47 PM EDT] org.broadinstitute.hellbender.tools.spark.bwa.BwaSpark done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=499646464. ---. null. ---",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2171#issuecomment-247785408:2756,load,load,2756,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2171#issuecomment-247785408,1,['load'],['load']
Performance,titionsRDD.scala:52); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:121); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskS,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6513#issuecomment-601702690:6927,concurren,concurrent,6927,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6513#issuecomment-601702690,1,['concurren'],['concurrent']
Performance,"tk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; >; > 16:17:06.589 **WARN** NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils347167544598047196.so: /tmp/libgkl_utils347167544598047196.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > 16:17:06.589 **WARN** IntelPairHmm - Intel GKL Utils not loaded; >; > 16:17:06.589 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; >; > 16:17:06.589 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; >; > 16:17:06.590 **WARN** NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils6186849302609329058.so: /tmp/libgkl_utils6186849302609329058.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > 16:17:06.590 **WARN** IntelPairHmm - Intel GKL Utils not loaded; >; > 16:17:06.591 **WARN** PairHMM - ***WARNING: Machine does not have the AVX instruction set support needed for the accelerated AVX PairHmm. Falling back to the MUCH slower LOGLESS_CACHING implementation!; >; > Since the calculation takes quite long, I checked the WARN messages of the; > output above. Especially the last one about the AVX instruction set where; > it says that a *MUCH* slower implementation will be used. From the few; > WARN messages it seems like the root cause is the failure to load libgkl; > and that again seems to be related to my platform. Does anyone know more; > about this issue or how to work around it?; >; > Best regards,; > Robert; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/is",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600:6085,load,load,6085,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600,1,['load'],['load']
Performance,"to forkTest/callset.json; 16:28:04.156 INFO GenomicsDBImport - Complete VCF Header will be written to forkTest/vcfheader.vcf; 16:28:04.156 INFO GenomicsDBImport - Importing to array - forkTest/genomicsdb_array; 16:28:04.158 INFO ProgressMeter - Starting traversal; 16:28:04.158 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 16:28:05.198 INFO GenomicsDBImport - Starting batch input file preload; 16:29:23.571 INFO GenomicsDBImport - Finished batch preload; 16:48:46.140 INFO GenomicsDBImport - Shutting down engine; [May 4, 2018 4:48:46 PM EDT] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 20.96 minutes.; Runtime.totalMemory()=22281715712; java.util.concurrent.CompletionException: java.lang.OutOfMemoryError: Java heap space; at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273); at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1592); at java.util.concurrent.CompletableFuture$AsyncSupply.exec(CompletableFuture.java:1582); at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056); at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692); at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157); Caused by: java.lang.OutOfMemoryError: Java heap space; at com.intel.genomicsdb.importer.SilentByteBufferStream.<init>(SilentByteBufferStream.java:55); at com.intel.genomicsdb.importer.GenomicsDBImporterStreamWrapper.<init>(GenomicsDBImporterStreamWrapper.java:70); at com.intel.genomicsdb.importer.GenomicsDBImporter.addBufferStream(GenomicsDBImporter.java:397); at com.intel.genomicsdb.importer.GenomicsDBImporter.addSortedVariantContextIterator(GenomicsDBImporter.java:358); at com.intel.genomicsdb.importer.GenomicsDBImporter.<init>(Geno",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388079572:3692,concurren,concurrent,3692,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388079572,1,['concurren'],['concurrent']
Performance,"torage.BlockManager$$anonfun$reportAllBlocks$3.apply(BlockManager.scala:217); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at org.apache.spark.storage.BlockManager.reportAllBlocks(BlockManager.scala:217); 	at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:236); 	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:522); 	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:547); 	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547); 	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547); 	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953); 	at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:547); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308); 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180); 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); 17/10/18 17:35:58 INFO BlockManagerMaster: BlockManagerMaster stopped; 17/10/18 17:35:58 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker-1,5,main]; java.lang.OutOfMemoryError: Java heap space; 	at htsjdk.samtools.BAMRecordCodec.decode(BAMRecordCodec.java:208); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.getNextRecord(BAMFileReader.java:829); 	at htsjdk.samtools.BAMFileReader$BAMFileIndexIterator.getNextRecord(BAMFileReader.java:981); 	",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-337554749:5391,concurren,concurrent,5391,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-337554749,1,['concurren'],['concurrent']
Performance,"toutput.bam` as the problem, but why that's causing a problem under yarn is mysterious. ```; caused by: java.io.IOException: Mkdirs failed to create file:/home/unix/louisb/writeable/testoutput.bam/_temporary/0/_temporary/attempt_201601291710_0020_r_000000_3 (exists=false, cwd=file:/mnt/disk10/yarn/nm/usercache/louisb/appcache/application_1452219145116_0780/container_1452219145116_0780_01_000002); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:442); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:428); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:917); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:898); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:795); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:784); at org.seqdoop.hadoop_bam.BAMRecordWriter.<init>(BAMRecordWriter.java:74); at org.seqdoop.hadoop_bam.KeyIgnoringBAMRecordWriter.<init>(KeyIgnoringBAMRecordWriter.java:49); at org.seqdoop.hadoop_bam.KeyIgnoringBAMOutputFormat.getRecordWriter(KeyIgnoringBAMOutputFormat.java:91); at org.seqdoop.hadoop_bam.KeyIgnoringBAMOutputFormat.getRecordWriter(KeyIgnoringBAMOutputFormat.java:79); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink$SparkBAMOutputFormat.getRecordWriter(ReadsSparkSink.java:65); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1030); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1014); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:88); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1451#issuecomment-176996853:1966,concurren,concurrent,1966,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1451#issuecomment-176996853,2,['concurren'],['concurrent']
Performance,"tprocessGermlineCNVCalls, so that external dictionaries provided via `--sequence-dictionary` do not override those in the count files, and perhaps fail if one is provided for any of the tools (I don’t recall exactly how VCF indexing is triggered by providing one, as seems to be indicated by the tutorial, but hopefully we can disallow external dictionaries while still taking advantage of the relevant engine features for VCF writing). EDIT: Went digging in Slack to try to remind myself of the context of these changes, and found the following PR comment from 1/7 (although it seems to have mysteriously disappeared from GitHub):. > Just so I understand, are we allowing overriding of the sequence dictionary in the shards (and skipping the consistency check) by allowing the parameter --sequence-dictionary to be specified? If so, we might want to document. Otherwise, I'd be inclined to enforce using the sequence dictionary in the shards (and ensuring the consistency check across shards is performed) by changing the null check in getBestAvailableSequenceDictionary to a check that the dictionary has not been set via the command line. EDIT^2: I think I misremembered the details of how #6330 hooked up the sequence dictionary and how getBestAvailableSequenceDictionary in GATKTool works (which probably explains why that comment was deleted...). Now that I actually go back and look, the `--sequence-dictionary` is not hooked up at all, so there is no change to revert in point 4!. Note that after all of this, it will *still* be possible to get into trouble at the gCNV step if you make funky shards (e.g., you could have shard 1 contain intervals from chr1 and chr3, and shard 2 contain intervals from chr2). I don't think it is possible to check for this case early, but you would still fail at PostprocessGermlineCNVCalls as above. Of course, all of these possibilities can be avoided by simply using the WDL, but it will be good to harden checks for those still working at the command line",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6924#issuecomment-719576249:3068,perform,performed,3068,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6924#issuecomment-719576249,1,['perform'],['performed']
Performance,"tribble.readers.TabixReader.<init>(TabixReader.java:129); at htsjdk.tribble.TabixFeatureReader.<init>(TabixFeatureReader.java:80); at htsjdk.tribble.AbstractFeatureReader.getFeatureReader(AbstractFeatureReader.java:117); ... 9 more; ```. If the file is really missing:; ```java; (cerc_prod) [16:48 xxxxxxx@yyyyyy:test a]$ gatk MergeVcfs -I data/calling/erc_prod2.SM_V7_1.vcf.gz -I data/calling/cerc_prod2.SM_V7_ZW.vcf.gz -O out.vcf.gz; Using GATK jar /master/xxxxxxx/local/pckg/python/miniconda3/envs/cerc_prod/share/gatk4-4.1.7.0-0/gatk-package-4.1.7.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /master/xxxxxxx/local/pckg/python/miniconda3/envs/cerc_prod/share/gatk4-4.1.7.0-0/gatk-package-4.1.7.0-local.jar MergeVcfs -I data/calling/erc_prod2.SM_V7_1.vcf.gz -I data/calling/cerc_prod2.SM_V7_ZW.vcf.gz -O out.vcf.gz; 17:06:37.645 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/master/xxxxxxx/local/pckg/python/miniconda3/envs/cerc_prod/share/gatk4-4.1.7.0-0/gatk-package-4.1.7.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; [Mon Jun 22 17:06:37 CDT 2020] MergeVcfs --INPUT data/calling/erc_prod2.SM_V7_1.vcf.gz --INPUT data/calling/cerc_prod2.SM_V7_ZW.vcf.gz --OUTPUT out.vcf.gz --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX true --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false; Jun 22, 2020 5:06:37 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; [Mon Jun 22 17:06:37 CDT 2020] Executing as xxxxxxx@yyyyyy on Linux 3.10.0-693.11.1.el7.x86_64 amd64; OpenJDK 64-Bit Server VM 1.8.0_152-release-1056-b12; De",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6664#issuecomment-647808241:5041,Load,Loading,5041,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6664#issuecomment-647808241,1,['Load'],['Loading']
Performance,ue -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 /share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-spark.jar CountReadsSpark --input /project/casa/gcad/test/HG04302.alt_bwamem_GRCh38DH.20150718.GBR.low_coverage.cram --reference file:///restricted/projectnb/casa/wgs.hg38/pipelines/hc/cram.test/GRCh38_full_analysis_set_plus_decoy_hla.fa.gz --spark-master yarn; 2019-01-09 13:35:04 WARN SparkConf:66 - The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 2019-01-09 13:35:05 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 13:35:09.640 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 13:35:09.799 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 13:35:11.507 INFO CountReadsSpark - ------------------------------------------------------------; 13:35:11.508 INFO CountReadsSpark - The Genome Analysis Toolkit (GATK) v4.0.12.0; 13:35:11.508 INFO CountReadsSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:35:11.508 INFO CountReadsSpark - Executing as farrell@scc-hadoop.bu.edu on Linux v2.6.32-754.6.3.el6.x86_64 amd64; 13:35:11.508 INFO CountReadsSpark - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:1950,load,load,1950,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,1,['load'],['load']
Performance,"ue copy ratio.; - The only major difference is we no longer make a QC PoN or check for large events. This was performed awkwardly in the old pipeline, so I'd rather not port it over. Eventually we will do all denoising with the gCNV coverage model anyway.; - Pre/tangent-normalization copy ratio are now referred to as standardized/denoised copy ratio.; - [x] Old code is still used for GC-bias correction in `CreateReadCountPanelOfNormals`, and we still use the `AnnotateTargets` tool. We should port this over (possibly as part of `PreprocessIntervals`) at some point (actually, I think we will be forced to, since `PreprocessIntervals` will output a Picard interval list, and `AnnotateTargets` outputs a target file).; - [x] Integration tests are still needed for `CreateReadCountPanelOfNormals`. These might not test for correctness, but we could possibly compare to old PoNs. Segmentation/modeling:; - Instead of separate tools for copy-ratio segmentation (`PerformSegmentation`) and allele-fraction segmentation/union/modeling (`AllelicCNV`), there is now just a single segmentation/modeling tool (`ModelSegments`).; - Input is denoised copy ratio and/or allelic counts. If only one input is provided, then we only model only the corresponding quantity.; - There is no separate allele-fraction workflow. Unlike the old approach, we do not perform any genotyping or modeling before doing kernel segmentation.; - [x] Old code and classes are used for segment union. We should port or possibly replace this with a simple method that uses kernel segmentation. EDIT: Actually, just tried running a WGS sample and this is still a major bottleneck. EDIT 2: Hmm...actually doesn't seem to be an issue on my desktop (compared to my laptop, on which the run hangs here). Will try to track down the source of the discrepancy. EDIT 3: Added segment union based on single-changepoint detection using kernel segmentation.; - [x] Segment union should be replaced by a proper joint kernel segmentation. EDIT: I'",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:3122,Perform,PerformSegmentation,3122,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828,1,['Perform'],['PerformSegmentation']
Performance,uence dictionary from the reference. I run this on the cloud. ```; ./gatk-launch CountVariantsSpark -V hdfs:///user/akiezun/dbsnp_138.b37.20.21.vcf.blockgz.gz -L 20 -R hdfs:///user/akiezun/human_g1k_v37.fasta -- --sparkRunner GCS --cluster dataproc-cluster-3; ```. and I get. ```; java.lang.IllegalArgumentException: java.net.UnknownHostException: null; at org.apache.hadoop.security.SecurityUtil.buildTokenService(SecurityUtil.java:377); at org.apache.hadoop.hdfs.NameNodeProxies.createNonHAProxy(NameNodeProxies.java:310); at org.apache.hadoop.hdfs.NameNodeProxies.createProxy(NameNodeProxies.java:176); at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:678); at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:619); at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:149); at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2653); at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:92); at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2687); at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2669); at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:371); at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:170); at hdfs.jsr203.HadoopFileSystem.<init>(HadoopFileSystem.java:106); at hdfs.jsr203.HadoopFileSystemProvider.newFileSystem(HadoopFileSystemProvider.java:165); at java.nio.file.FileSystems.newFileSystem(FileSystems.java:336); at org.broadinstitute.hellbender.utils.io.IOUtils.getPath(IOUtils.java:528); at org.broadinstitute.hellbender.engine.datasources.ReferenceHadoopSource.getReferenceSequenceDictionary(ReferenceHadoopSource.java:39); at org.broadinstitute.hellbender.engine.datasources.ReferenceMultiSource.getReferenceSequenceDictionary(ReferenceMultiSource.java:110); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReference(GATKSparkTool.java:354); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSpar,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1936#issuecomment-229433523:1042,Cache,Cache,1042,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1936#issuecomment-229433523,1,['Cache'],['Cache']
Performance,"un(Thread.java:745); 15/07/14 13:14:53 ERROR util.SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker-0,5,main]; java.lang.IncompatibleClassChangeError: Found class org.apache.hadoop.mapreduce.TaskAttemptContext, but interface was expected; at com.cloudera.dataflow.spark.TemplatedTextOutputFormat.getOutputFile(TemplatedTextOutputFormat.java:50); at com.cloudera.dataflow.spark.TemplatedTextOutputFormat.getDefaultWorkFile(TemplatedTextOutputFormat.java:46); at org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:125); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$12.apply(PairRDDFunctions.scala:995); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$12.apply(PairRDDFunctions.scala:979); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61); at org.apache.spark.scheduler.Task.run(Task.scala:64); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 15/07/14 13:14:53 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 1.0 (TID 1, localhost): java.lang.IncompatibleClassChangeError: Found class org.apache.hadoop.mapreduce.TaskAttemptContext, but interface was expected; at com.cloudera.dataflow.spark.TemplatedTextOutputFormat.getOutputFile(TemplatedTextOutputFormat.java:50); at com.cloudera.dataflow.spark.TemplatedTextOutputFormat.getDefaultWorkFile(TemplatedTextOutputFormat.java:46); at org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:125); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$12.apply(PairRDDFunctions.scala:995); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$12.apply(PairRDDFunctions.scala:979); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61); at org.apache.sp",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/639#issuecomment-121313713:30575,concurren,concurrent,30575,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/639#issuecomment-121313713,1,['concurren'],['concurrent']
Performance,unch CountVariantsSpark -V hdfs:///user/akiezun/dbsnp_138.b37.20.21.vcf.blockgz.gz -L 20 -R hdfs:///user/akiezun/human_g1k_v37.fasta -- --sparkRunner GCS --cluster dataproc-cluster-3; ```. and I get. ```; java.lang.IllegalArgumentException: java.net.UnknownHostException: null; at org.apache.hadoop.security.SecurityUtil.buildTokenService(SecurityUtil.java:377); at org.apache.hadoop.hdfs.NameNodeProxies.createNonHAProxy(NameNodeProxies.java:310); at org.apache.hadoop.hdfs.NameNodeProxies.createProxy(NameNodeProxies.java:176); at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:678); at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:619); at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:149); at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2653); at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:92); at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2687); at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2669); at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:371); at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:170); at hdfs.jsr203.HadoopFileSystem.<init>(HadoopFileSystem.java:106); at hdfs.jsr203.HadoopFileSystemProvider.newFileSystem(HadoopFileSystemProvider.java:165); at java.nio.file.FileSystems.newFileSystem(FileSystems.java:336); at org.broadinstitute.hellbender.utils.io.IOUtils.getPath(IOUtils.java:528); at org.broadinstitute.hellbender.engine.datasources.ReferenceHadoopSource.getReferenceSequenceDictionary(ReferenceHadoopSource.java:39); at org.broadinstitute.hellbender.engine.datasources.ReferenceMultiSource.getReferenceSequenceDictionary(ReferenceMultiSource.java:110); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReference(GATKSparkTool.java:354); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:320); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1936#issuecomment-229433523:1118,Cache,Cache,1118,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1936#issuecomment-229433523,1,['Cache'],['Cache']
Performance,"update. i've been running the standalone consolidate tool, per chromosome. Below is chr 9. As you can see, it seems to take nearly a full day per attribute. Chr 9 is among the smaller contigs. In contrast, chr 1 has been stuck on the first attribute (END) for ~4 days at this point. I'm not sure if this was the right choice, but you will see this included ""--segment-size 32768"", based on the conversation above. ```; 03 Mar 2022 12:51:23,371 INFO : Consolidating contig folder: /home/exacloud/gscratch/prime-seq/workDir/0950f572-7565-103a-a738-f8f3fc8675d2/Job9.work/WGS_1852_consolidated.gdb/9$1$134124166; 03 Mar 2022 12:51:23,389 INFO : 	/home/exacloud/gscratch/prime-seq/bin/consolidate_genomicsdb_array -w /home/exacloud/gscratch/prime-seq/workDir/0950f572-7565-103a-a738-f8f3fc8675d2/Job9.work/WGS_1852_consolidated.gdb --shared-posixfs-optimizations --segment-size 32768 -a 9$1$134124166; 03 Mar 2022 12:51:23,423 DEBUG: using path: /home/exacloud/gscratch/prime-seq/bin:/home/exacloud/gscratch/prime-seq/bin/:/home/exacloud/gscratch/prime-seq/java/current/bin/:/home/exacloud/gscratch/prime-seq/bin/:/usr/local/bin:/usr/bin; 03 Mar 2022 12:51:23,510 DEBUG: 	12:51:23.510 info consolidate_genomicsdb_array - pid=233371 tid=233371 Starting consolidation of 9$1$134124166 in /home/exacloud/gscratch/prime-seq/workDir/0950f572-7565-103a-a738-f8f3fc8675d2/Job9.work/WGS_1852_consolidated.gdb; 03 Mar 2022 12:55:30,641 DEBUG: 	Using buffer_size=32768 for consolidation; 03 Mar 2022 12:55:30,656 DEBUG: 	12:55:30 Memory stats(pages) beginning consolidation size=9350950 resident=9324278 share=1814 text=3530 lib=0 data=9322130 dt=0; 03 Mar 2022 12:55:30,662 DEBUG: 	12:55:30 Memory stats(pages) after alloc for attribute=END size=9350984 resident=9324313 share=1821 text=3530 lib=0 data=9322164 dt=0; 05 Mar 2022 08:43:03,491 DEBUG: 	8:43:3 Memory stats(pages) after alloc for attribute=REF size=109159142 resident=108901310 share=1425 text=3530 lib=0 data=109130249 dt=0; 06 Mar 2022 06:28:14,322 ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1060723659:865,optimiz,optimizations,865,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1060723659,1,['optimiz'],['optimizations']
Performance,"ut.; - For all workflows, we always collect integer read counts; for WGS, these are output as both HDF5 and TSV and the HDF5 is used for subsequent input.; - For the case workflow, we always collect allelic counts at all sites and output as TSV.; - [x] We should output all data files as HDF5 by default and as TSV optionally. EDIT: This is done for `CollectFragmentCounts`.; - [x] We will need to update the workflows when @MartonKN and @asmirnov239 get `PreprocessIntervals` and `CollectReadCounts` merged, respectively. These tools will remove the awkwardness required by `PadTargets` and `CalculateTargetCoverage`/`SparkGenomeReadCounts`. Denoising:; - All parameters are exposed in the PoN creation tool (#3356).; - Without a PoN, standardization and optional GC correction are performed (#3570).; - Other than the minor point about sample mean/median being used inconsistently noted above, the denoising process is essentially exactly the same mathematically as before (""superficial"" differences include the vastly improved memory and I/O optimizations, the ability to adjust number of principal components used, the removal of redundant SVDs, the enforcement of consistent GC-bias correction).; - [ ] That said, I'll carry over this TODO from above: Revisit standardization procedure by checking with simulated data. We should make sure that the centering of the data does not rescale the true copy ratio.; - The only major difference is we no longer make a QC PoN or check for large events. This was performed awkwardly in the old pipeline, so I'd rather not port it over. Eventually we will do all denoising with the gCNV coverage model anyway.; - Pre/tangent-normalization copy ratio are now referred to as standardized/denoised copy ratio.; - [x] Old code is still used for GC-bias correction in `CreateReadCountPanelOfNormals`, and we still use the `AnnotateTargets` tool. We should port this over (possibly as part of `PreprocessIntervals`) at some point (actually, I think we will be for",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:1806,optimiz,optimizations,1806,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828,1,['optimiz'],['optimizations']
Performance,util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.traverseVariants(MultiplePassVariantWalker.java:75); 	at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.traverse(MultiplePassVariantWalker.java:40); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1048); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:163); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:206); 	at org.broadinstitute.hellbender.Main.main(Main.java:292); Caused by: java.util.concurrent.ExecutionException: org.broadinstitute.hellbender.exceptions.GATKException: Expected message of length 3 but only found 0 bytes; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at org.broadinstitute.hellbender.utils.runtime.StreamingProcessController.waitForAck(StreamingProcessController.java:228); 	... 26 more; Caused by: org.broadinstitute.hellbender.exceptions.GATKException: Expected message of length 3 but only found 0 bytes; 	at org.broadinstitute.hellbender.utils.runtime.StreamingProcessController.getBytesFromStream(StreamingProcessController.java:261); 	at org.broadinstitute.hellbender.utils.runtime.StreamingProcessController.lambda$waitForAck$0(StreamingProcessController.java:208); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExec,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7397#issuecomment-895854147:3447,concurren,concurrent,3447,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7397#issuecomment-895854147,1,['concurren'],['concurrent']
Performance,"utput from chr1. The output shows the Maximum resident set size (kbytes): **2630440**. Using GATK jar /share/pkg.7/gatk/4.2.6.1/install/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar defined in environment variable GATK_LOCAL_JAR; ```; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx200g -Xms16g -jar /share/pkg.7/gatk/4.2.6.1/install/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar GenomicsDBImport --sample-name-map sample_map.chr1 --genomicsdb-workspace-path genomicsDB.rb.bypass.time.chr1 --genomicsdb-shared-posixfs-optimizations True --tmp-dir tmp --bypass-feature-reader --L chr1 --batch-size 50 --reader-threads 4 --overwrite-existing-genomicsdb-workspace; Command being timed: ""gatk --java-options -Xmx200g -Xms16g GenomicsDBImport --sample-name-map sample_map.chr1 --genomicsdb-workspace-path genomicsDB.rb.bypass.time.chr1 --genomicsdb-shared-posixfs-optimizations True --tmp-dir tmp --bypass-feature-reader --L chr1 --batch-size 50 --reader-threads 4 --overwrite-existing-genomicsdb-workspace""; User time (seconds): 270716.45; System time (seconds): 1723.34; Percent of CPU this job got: 99%; Elapsed (wall clock) time (h:mm:ss or m:ss): 76:08:24; Average shared text size (kbytes): 0; Average unshared data size (kbytes): 0; Average stack size (kbytes): 0; Average total size (kbytes): 0; Maximum resident set size (kbytes): 2630440; Average resident set size (kbytes): 0; Major (requiring I/O) page faults: 5; Minor (reclaiming a frame) page faults: 206030721; Voluntary context switches: 11129822; Involuntary context switches: 176522; Swaps: 0; File system inputs: 627981312; File system outputs: 466730160; Socket messages sent: 0; Socket messages received: 0; Signals delivered: 0; Page size (bytes): 4096; Exit status: 0. ```. So using the import on reblocked gvcfs using --bypass-feature-reader was the fastest way to import our 3500 gVCFs and minimize memory.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1252598687:2253,optimiz,optimizations,2253,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1252598687,1,['optimiz'],['optimizations']
Performance,"v"",; ""NIST controlHCprocesshours"": ""90.94291388888888"",; ""NIST controlHCsystemhours"": ""0.182125"",; ""NIST controlHCwallclockhours"": ""63.56370277777778"",; ""NIST controlHCwallclockmax"": ""3.701625"",; ""NIST controlMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/e6f57e40-2025-46fd-9aa0-d591a3799007/call-NISTSampleHeadToHead/BenchmarkComparison/ccdb901c-fb8f-49e4-b542-cf42e011a623/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""NIST controlindelF1Score"": ""0.9902"",; ""NIST controlindelPrecision"": ""0.9903"",; ""NIST controlsnpF1Score"": ""0.9899"",; ""NIST controlsnpPrecision"": ""0.9887"",; ""NIST controlsnpRecall"": ""0.9911"",; ""NIST controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/e6f57e40-2025-46fd-9aa0-d591a3799007/call-NISTSampleHeadToHead/BenchmarkComparison/ccdb901c-fb8f-49e4-b542-cf42e011a623/call-BenchmarkVCFControlSample/Benchmark/6d64f12a-ca50-4ecd-8608-93dc53d241bb/call-CombineSummaries/summary.csv"",; ""NIST evalHCprocesshours"": ""95.62183055555556"",; ""NIST evalHCsystemhours"": ""0.18361111111111117"",; ""NIST evalHCwallclockhours"": ""64.22846111111112"",; ""NIST evalHCwallclockmax"": ""3.3683277777777776"",; ""NIST evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/e6f57e40-2025-46fd-9aa0-d591a3799007/call-NISTSampleHeadToHead/BenchmarkComparison/ccdb901c-fb8f-49e4-b542-cf42e011a623/call-EVALRuntimeTask/cacheCopy/monitoring.pdf"",; ""NIST evalindelF1Score"": ""0.9902"",; ""NIST evalindelPrecision"": ""0.9903"",; ""NIST evalsnpF1Score"": ""0.9899"",; ""NIST evalsnpPrecision"": ""0.9887"",; ""NIST evalsnpRecall"": ""0.9911"",; ""NIST evalsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/e6f57e40-2025-46fd-9aa0-d591a3799007/call-NISTSampleHeadToHead/BenchmarkComparison/ccdb901c-fb8f-49e4-b542-cf42e011a623/call-BenchmarkVCFTestSample/Benchmark/f0709402-e72d-4013-a781-e50d8d46e2c3/call-CombineSummaries/summary.csv""; }; } ; </pre> </details>",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7651#issuecomment-1069378815:14465,cache,cacheCopy,14465,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7651#issuecomment-1069378815,1,['cache'],['cacheCopy']
Performance,v; > 12:28:19.507 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/chr1_a_bed.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/chr1_a_bed/hg38/chr1_a_bed.tsv; > WARNING 2020-07-21 12:28:19 AsciiLineReader Creating an indexable source for an AsciiFeatureCodec using a stream that is neither a PositionalBufferedStream nor a BlockCompressedInputStream; > 12:28:19.512 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/cosmic_fusion.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/cosmic_fusion/hg38/cosmic_fusion.tsv; > 12:28:19.522 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/gencode.v28.annotation.REORDERED.gtf -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/gencode/hg38/gencode.v28.annotation.REORDERED.gtf; > 12:28:19.522 INFO DataSourceUtils - Setting lookahead cache for data source: Gencode : 100000; > 12:28:19.552 INFO FeatureManager - Using codec GencodeGtfCodec to read file file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/gencode/hg38/gencode.v28.annotation.REORDERED.gtf; > 12:28:19.589 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/gencode.v28.pc_transcripts.fa -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/gencode/hg38/gencode.v28.pc_transcripts.fa; > 12:28:27.529 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/dnaRepairGenes.20180524T145835.csv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/dna_repair_genes/hg38/dnaRepairGenes.20180524T145835.csv; > 12:28:27.546 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/gencode_xhgnc_v90_38.hg38.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6708#issuecomment-661776975:14710,cache,cache,14710,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6708#issuecomment-661776975,1,['cache'],['cache']
Performance,"va:132); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:182); 	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:164); 	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:412); 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64); 	at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 5.0 failed 1 times, most recent failure: Lost task 1.0 in stage 5.0 (TID 12, localhost, executor driver): java.util.ConcurrentModificationException; 	at java.util.ArrayList.sort(ArrayList.java:1464); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.<init>(ReadThreadingAssembler.java:81); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerReadThreadingAssemblerArgumentCollection.makeReadThreadingAssembler(HaplotypeCallerReadThreadingAssembler",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429:7438,concurren,concurrent,7438,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429,1,['concurren'],['concurrent']
Performance,veInputChangesStep.java:48); at org.gradle.internal.execution.steps.ResolveInputChangesStep.execute(ResolveInputChangesStep.java:33); at org.gradle.internal.execution.steps.CancelExecutionStep.execute(CancelExecutionStep.java:39); at org.gradle.internal.execution.steps.TimeoutStep.executeWithoutTimeout(TimeoutStep.java:73); at org.gradle.internal.execution.steps.TimeoutStep.execute(TimeoutStep.java:54); at org.gradle.internal.execution.steps.CatchExceptionStep.execute(CatchExceptionStep.java:35); at org.gradle.internal.execution.steps.CreateOutputsStep.execute(CreateOutputsStep.java:51); at org.gradle.internal.execution.steps.SnapshotOutputsStep.execute(SnapshotOutputsStep.java:45); at org.gradle.internal.execution.steps.SnapshotOutputsStep.execute(SnapshotOutputsStep.java:31); at org.gradle.internal.execution.steps.CacheStep.executeWithoutCache(CacheStep.java:208); at org.gradle.internal.execution.steps.CacheStep.execute(CacheStep.java:70); at org.gradle.internal.execution.steps.CacheStep.execute(CacheStep.java:45); at org.gradle.internal.execution.steps.BroadcastChangingOutputsStep.execute(BroadcastChangingOutputsStep.java:49); at org.gradle.internal.execution.steps.StoreSnapshotsStep.execute(StoreSnapshotsStep.java:43); at org.gradle.internal.execution.steps.StoreSnapshotsStep.execute(StoreSnapshotsStep.java:32); at org.gradle.internal.execution.steps.RecordOutputsStep.execute(RecordOutputsStep.java:38); at org.gradle.internal.execution.steps.RecordOutputsStep.execute(RecordOutputsStep.java:24); at org.gradle.internal.execution.steps.SkipUpToDateStep.executeBecause(SkipUpToDateStep.java:96); at org.gradle.internal.execution.steps.SkipUpToDateStep.lambda$execute$0(SkipUpToDateStep.java:89); at org.gradle.internal.execution.steps.SkipUpToDateStep.execute(SkipUpToDateStep.java:54); at org.gradle.internal.execution.steps.SkipUpToDateStep.execute(SkipUpToDateStep.java:38); at org.gradle.internal.execution.steps.ResolveChangesStep.execute(ResolveChangesStep.java:76); a,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6466#issuecomment-590387973:9242,Cache,CacheStep,9242,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6466#issuecomment-590387973,2,['Cache'],['CacheStep']
Performance,verage 86.642% 86.662% +0.020% ; - Complexity 38963 39097 +134 ; ===============================================; Files 2336 2341 +5 ; Lines 182730 183522 +792 ; Branches 20066 20117 +51 ; ===============================================; + Hits 158321 159043 +722 ; - Misses 17366 17399 +33 ; - Partials 7043 7080 +37 ; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/8132?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) | Coverage Δ | |; |---|---|---|; | [...lkers/vqsr/scalable/ExtractVariantAnnotations.java](https://codecov.io/gh/broadinstitute/gatk/pull/8132?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvRXh0cmFjdFZhcmlhbnRBbm5vdGF0aW9ucy5qYXZh) | `89.062% <ø> (-3.125%)` | :arrow_down: |; | [.../scalable/data/LabeledVariantAnnotationsDatum.java](https://codecov.io/gh/broadinstitute/gatk/pull/8132?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvZGF0YS9MYWJlbGVkVmFyaWFudEFubm90YXRpb25zRGF0dW0uamF2YQ==) | `63.158% <ø> (-5.263%)` | :arrow_down: |; | [...lable/modeling/VariantAnnotationsModelBackend.java](https://codecov.io/gh/broadinstitute/gatk/pull/8132?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvbW9kZWxpbmcvVmFyaWFudEFubm90YXRpb25zTW9kZWxCYWNrZW5kLmphdmE=) | `100.000% <ø> (ø)` | |; | [...sr/scalable/modeling/VariantAnnotationsScorer.java](https://codecov.io/gh/broadinstitute/gatk/pull/8132?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_conten,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8132#issuecomment-1370265333:1853,scalab,scalable,1853,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8132#issuecomment-1370265333,1,['scalab'],['scalable']
Performance,"void ContextBase<NUMBER>::initializeMatchToMatchProb() [with NUMBER = double]'; /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/common_data_structure.h:47:35: required from 'static void ContextBase<NUMBER>::initializeStaticMembers() [with NUMBER = double]'; /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/LoadTimeInitializer.cc:53:20: required from here; /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/common_data_structure.h:94:16: error: 'isinf' was not declared in this scope, and no declarations were found by argument-dependent lookup at the point of instantiation [-fpermissive]; if (isinf(small) == -1 || isinf(big) == -1); ^; In file included from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/headers.h:27:0,; from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/common_data_structure.h:4,; from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/utils.h:4,; from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/LoadTimeInitializer.cc:1:; /usr/local/Cellar/gcc/5.3.0/include/c++/5.3.0/cmath:853:5: note: 'template<class _Tp> typename __gnu_cxx::__enable_if<std::__is_arithmetic<_Tp>::__value, int>::__type std::isinf(_Tp)' declared here, later in the translation unit; isinf(_Tp __f); ^; In file included from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/utils.h:4:0,; from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/LoadTimeInitializer.cc:1:; /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/common_data_structure.h:94:38: error: 'isinf' was not declared in this scope, and no declarations were found by argument-dependent lookup at the point of instantiation [-fpermissive]; if (isinf(small) == -1 || isinf(big) == -1); ^; In file included from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/headers.h:27:0,; from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/common_data_structure.h:4,; from /User",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1504#issuecomment-187727343:13669,Load,LoadTimeInitializer,13669,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1504#issuecomment-187727343,1,['Load'],['LoadTimeInitializer']
Performance,we lose on the concordance/discordance comparison - this issue now tracks profiling and optimizing that case. Bumped the time estimate to 3.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1092#issuecomment-155450486:88,optimiz,optimizing,88,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1092#issuecomment-155450486,1,['optimiz'],['optimizing']
Performance,"when trying to build GATK fully I get this error:; ```; > Task :gatkDoc FAILED; Execution optimizations have been disabled for task ':gatkDoc' to ensure correctness due to the following reasons:; - Gradle detected a problem with the following location: '/home/jeremie/GATK/build/classes/java/main'. Reason: Task ':gatkDoc' uses this output of task ':condaStandardEnvironmentDefinition' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem.; - Gradle detected a problem with the following location: '/home/jeremie/GATK/build/resources/main'. Reason: Task ':gatkDoc' uses this output of task ':condaStandardEnvironmentDefinition' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':gatkDoc'.; > Javadoc generation failed. Generated Javadoc options file (useful for troubleshooting): '/home/jeremie/GATK/build/tmp/gatkDoc/javadoc.options'. * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full insights. * Get more help at https://help.gradle.org. Deprecated Gradle features were used in this build, making it incompatible with Gradle 8.0. You can use '--warning-mode all' to show the individual deprecation warnings and determine if they come from your own scripts or plugins. See https://docs.gradle.org/7.3.2/userguide/command_line_interface.html#sec:command_line_warnings. Execution optimizations have been disabled for 1 invalid unit",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7936#issuecomment-1202544500:90,optimiz,optimizations,90,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7936#issuecomment-1202544500,1,['optimiz'],['optimizations']
Performance,"which includes combining read-count files, which takes ~30s of I/O) and generated a 520MB PoN, and DenoiseReadCounts took ~30s (~10s of which was composing/writing results, as we are still forced to generate two ReadCountCollections). Resulting PTN and TN copy ratios were identical down to 1E-16 levels. Differences are only due to removing the unnecessary pseudoinverse computation. Results after filtering and before SVD are identical, despite the code being rewritten from scratch to be more memory efficient (e.g., filtering is performed in place)---phew!. If we stored read counts as HDF5 instead of as plain text, this would make things much faster. Perhaps it would be best to make TSV an optional output of the new coverage collection tool. As a bonus, it would then only take a little bit more code to allow previous PoNs to be provided via -I as an additional source of read counts. Remaining TODO's:. - [x] Allow DenoiseReadCounts to be run without a PoN. This will just perform standardization and optional GC correction. This gives us the ability to run the case sample pipeline without a PoN, which will give users more options and might be good enough if the data is not too noisy.; - [x] Actually, I'm not sure why we take perform SVD on the intervals x samples matrix and take the left-singular vectors. <s>Typically, SVD is performed on the samples x intervals matrix and the right-singular vectors are taken, which saves some extra computation that is necessary to calculate the left-singular vectors.</s> (EDIT: Actually, looks like Spark's SVD is faster on tall and skinny matrices, which might be due to the fact that the underlying implementation calls Fortran code. I still think that representing samples as row vectors has some benefits, so I've changed things to reflect this; I now just take a transpose before performing the SVD, so that we still operate on the same intervals x samples matrix.) This will also save us some transposing, which we do anyway to make HDF5 wr",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351:1555,perform,perform,1555,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351,1,['perform'],['perform']
Performance,why don't people just search the tickets and look for 'performance' ? I guess we can make a wiki with 'best practices' but the code already incorporates the best practices as we know them,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2012#issuecomment-234565326:55,perform,performance,55,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2012#issuecomment-234565326,1,['perform'],['performance']
Performance,xc3Ivc2NhbGFibGUvVHJhaW5WYXJpYW50QW5ub3RhdGlvbnNNb2RlbC5qYXZh) | `77.778% <66.667%> (-2.991%)` | :arrow_down: |; | [...lkers/vqsr/scalable/ExtractVariantAnnotations.java](https://codecov.io/gh/broadinstitute/gatk/pull/8074/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvRXh0cmFjdFZhcmlhbnRBbm5vdGF0aW9ucy5qYXZh) | `92.188% <100.000%> (+1.116%)` | :arrow_up: |; | [...walkers/vqsr/scalable/ScoreVariantAnnotations.java](https://codecov.io/gh/broadinstitute/gatk/pull/8074/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvU2NvcmVWYXJpYW50QW5ub3RhdGlvbnMuamF2YQ==) | `76.250% <100.000%> (+0.149%)` | :arrow_up: |; | [...r/scalable/data/LabeledVariantAnnotationsData.java](https://codecov.io/gh/broadinstitute/gatk/pull/8074/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvZGF0YS9MYWJlbGVkVmFyaWFudEFubm90YXRpb25zRGF0YS5qYXZh) | `75.510% <100.000%> (+1.283%)` | :arrow_up: |; | [.../hellbender/utils/genotyper/AlleleLikelihoods.java](https://codecov.io/gh/broadinstitute/gatk/pull/8074/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nZW5vdHlwZXIvQWxsZWxlTGlrZWxpaG9vZHMuamF2YQ==) | `84.201% <0.000%> (+0.186%)` | :arrow_up: |; | [...lbender/utils/variant/GATKVariantContextUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/8074/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comme,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8074#issuecomment-1294055323:3603,scalab,scalable,3603,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8074#issuecomment-1294055323,1,['scalab'],['scalable']
Performance,xecution.steps.CleanupOutputsStep.execute(CleanupOutputsStep.java:35); at org.gradle.internal.execution.steps.ResolveInputChangesStep.execute(ResolveInputChangesStep.java:48); at org.gradle.internal.execution.steps.ResolveInputChangesStep.execute(ResolveInputChangesStep.java:33); at org.gradle.internal.execution.steps.CancelExecutionStep.execute(CancelExecutionStep.java:39); at org.gradle.internal.execution.steps.TimeoutStep.executeWithoutTimeout(TimeoutStep.java:73); at org.gradle.internal.execution.steps.TimeoutStep.execute(TimeoutStep.java:54); at org.gradle.internal.execution.steps.CatchExceptionStep.execute(CatchExceptionStep.java:35); at org.gradle.internal.execution.steps.CreateOutputsStep.execute(CreateOutputsStep.java:51); at org.gradle.internal.execution.steps.SnapshotOutputsStep.execute(SnapshotOutputsStep.java:45); at org.gradle.internal.execution.steps.SnapshotOutputsStep.execute(SnapshotOutputsStep.java:31); at org.gradle.internal.execution.steps.CacheStep.executeWithoutCache(CacheStep.java:208); at org.gradle.internal.execution.steps.CacheStep.execute(CacheStep.java:70); at org.gradle.internal.execution.steps.CacheStep.execute(CacheStep.java:45); at org.gradle.internal.execution.steps.BroadcastChangingOutputsStep.execute(BroadcastChangingOutputsStep.java:49); at org.gradle.internal.execution.steps.StoreSnapshotsStep.execute(StoreSnapshotsStep.java:43); at org.gradle.internal.execution.steps.StoreSnapshotsStep.execute(StoreSnapshotsStep.java:32); at org.gradle.internal.execution.steps.RecordOutputsStep.execute(RecordOutputsStep.java:38); at org.gradle.internal.execution.steps.RecordOutputsStep.execute(RecordOutputsStep.java:24); at org.gradle.internal.execution.steps.SkipUpToDateStep.executeBecause(SkipUpToDateStep.java:96); at org.gradle.internal.execution.steps.SkipUpToDateStep.lambda$execute$0(SkipUpToDateStep.java:89); at org.gradle.internal.execution.steps.SkipUpToDateStep.execute(SkipUpToDateStep.java:54); at org.gradle.internal.execution.steps.S,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6466#issuecomment-590387973:9105,Cache,CacheStep,9105,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6466#issuecomment-590387973,2,['Cache'],['CacheStep']
Performance,"xecutor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:522); 	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:547); 	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547); 	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547); 	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953); 	at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:547); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308); 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180); 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); 17/10/18 17:35:58 INFO BlockManagerMaster: BlockManagerMaster stopped; 17/10/18 17:35:58 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker-1,5,main]; java.lang.OutOfMemoryError: Java heap space; 	at htsjdk.samtools.BAMRecordCodec.decode(BAMRecordCodec.java:208); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.getNextRecord(BAMFileReader.java:829); 	at htsjdk.samtools.BAMFileReader$BAMFileIndexIterator.getNextRecord(BAMFileReader.java:981); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.advance(BAMFileReader.java:803); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.java:797); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.java:765); 	at org.seqdoop.hadoop_bam.BAMRecordReader.nextKeyValue(BAMRecordReader.java:225); 	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.sc",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-337554749:5785,concurren,concurrent,5785,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-337554749,1,['concurren'],['concurrent']
Performance,"xt(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 2019-01-07 11:34:07 INFO TaskSetManager:54 - Starting task 1.1 in stage 0.0 (TID 3, scc-q21.scc.bu.edu, executor 1, partition 1, NODE_LOCAL, 7992 bytes); 2019-01-07 11:34:07 WARN TaskSetManager:66 - Lost task 3.0 in stage 0.0 (TID 2, scc-q21.scc.bu.edu, executor 1): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 1, start 4924320, span 190238, expected MD5 8a9ef2f91a78ffdc56561ece832e9f5d; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:24988,concurren,concurrent,24988,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,1,['concurren'],['concurrent']
Performance,"xt(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 2019-01-07 11:34:08 INFO TaskSetManager:54 - Starting task 3.1 in stage 0.0 (TID 4, scc-q21.scc.bu.edu, executor 1, partition 3, NODE_LOCAL, 7992 bytes); 2019-01-07 11:34:08 INFO TaskSetManager:54 - Lost task 1.1 in stage 0.0 (TID 3) on scc-q21.scc.bu.edu, executor 1: htsjdk.samtools.cram.CRAMException (Reference sequence MD5 mismatch for slice: sequence id 0, start 87545719, span 186383, expected MD5 492a29f6d7d6fcaf8dde06834861e7ae) [duplicate 1]; 2019-01-07 11:34:09 INFO TaskSetManager:54 - Starting task 9.0 in stage 0.0 (TID 5, scc-q12.scc.bu.edu, executor 2, partition 9, NODE_LOCAL, 7992 bytes); 2019-01-07 11:34:09 WARN TaskSetManager:66 - Lost task 2.0 in stage 0.0 (TID 0, scc-q12.scc.bu.edu, executor 2): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:26723,concurren,concurrent,26723,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,1,['concurren'],['concurrent']
Performance,"xt(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 2019-01-07 11:34:09 INFO TaskSetManager:54 - Starting task 1.2 in stage 0.0 (TID 6, scc-q21.scc.bu.edu, executor 1, partition 1, NODE_LOCAL, 7992 bytes); 2019-01-07 11:34:09 INFO TaskSetManager:54 - Lost task 3.1 in stage 0.0 (TID 4) on scc-q21.scc.bu.edu, executor 1: htsjdk.samtools.cram.CRAMException (Reference sequence MD5 mismatch for slice: sequence id 1, start 4924320, span 190238, expected MD5 8a9ef2f91a78ffdc56561ece832e9f5d) [duplicate 1]; 2019-01-07 11:34:10 INFO TaskSetManager:54 - Starting task 2.1 in stage 0.0 (TID 7, scc-q12.scc.bu.edu, executor 2, partition 2, NODE_LOCAL, 7992 bytes); 2019-01-07 11:34:10 WARN TaskSetManager:66 - Lost task 9.0 in stage 0.0 (TID 5, scc-q12.scc.bu.edu, executor 2): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence i",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:28914,concurren,concurrent,28914,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,1,['concurren'],['concurrent']
Performance,"xt(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 2019-01-07 11:34:10 INFO TaskSetManager:54 - Starting task 3.2 in stage 0.0 (TID 8, scc-q21.scc.bu.edu, executor 1, partition 3, NODE_LOCAL, 7992 bytes); 2019-01-07 11:34:10 INFO TaskSetManager:54 - Lost task 1.2 in stage 0.0 (TID 6) on scc-q21.scc.bu.edu, executor 1: htsjdk.samtools.cram.CRAMException (Reference sequence MD5 mismatch for slice: sequence id 0, start 87545719, span 186383, expected MD5 492a29f6d7d6fcaf8dde06834861e7ae) [duplicate 2]; 2019-01-07 11:34:11 INFO TaskSetManager:54 - Starting task 1.3 in stage 0.0 (TID 9, scc-q21.scc.bu.edu, executor 1, partition 1, NODE_LOCAL, 7992 bytes); 2019-01-07 11:34:11 INFO TaskSetManager:54 - Lost task 3.2 in stage 0.0 (TID 8) on scc-q21.scc.bu.edu, executor 1: htsjdk.samtools.cram.CRAMException (Reference sequence MD5 mismatch for slice: sequenc",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:31103,concurren,concurrent,31103,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,1,['concurren'],['concurrent']
Performance,"xt(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 2019-01-09 13:35:49 INFO TaskSetManager:54 - Starting task 7.0 in stage 0.0 (TID 3, scc-q01.scc.bu.edu, executor 1, partition 7, NODE_LOCAL, 7992 bytes); 2019-01-09 13:35:49 WARN TaskSetManager:66 - Lost task 2.0 in stage 0.0 (TID 1, scc-q01.scc.bu.edu, executor 1): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 0, start 160972515, span 170618, expected MD5 0cc5e1f5ec5c1b06d5a4bd5fff11b77f; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterato",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:24275,concurren,concurrent,24275,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,1,['concurren'],['concurrent']
Performance,"xt(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 2019-01-09 13:35:50 INFO TaskSetManager:54 - Starting task 1.1 in stage 0.0 (TID 4, scc-q20.scc.bu.edu, executor 2, partition 1, NODE_LOCAL, 7992 bytes); 2019-01-09 13:35:50 WARN TaskSetManager:66 - Lost task 4.0 in stage 0.0 (TID 2, scc-q20.scc.bu.edu, executor 2): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 1, start 93925364, span 266689, expected MD5 54babf05a23e9e88a8738dfbb20a1683; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:26012,concurren,concurrent,26012,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,1,['concurren'],['concurrent']
Performance,"xt(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 2019-01-09 13:35:51 INFO TaskSetManager:54 - Starting task 4.1 in stage 0.0 (TID 5, scc-q20.scc.bu.edu, executor 2, partition 4, NODE_LOCAL, 7992 bytes); 2019-01-09 13:35:51 INFO TaskSetManager:54 - Lost task 1.1 in stage 0.0 (TID 4) on scc-q20.scc.bu.edu, executor 2: htsjdk.samtools.cram.CRAMException (Reference sequence MD5 mismatch for slice: sequence id 0, start 87545719, span 186383, expected MD5 492a29f6d7d6fcaf8dde06834861e7ae) [duplicate 1]; 2019-01-09 13:35:52 INFO TaskSetManager:54 - Starting task 2.1 in stage 0.0 (TID 6, scc-q01.scc.bu.edu, executor 1, partition 2, NODE_LOCAL, 7992 bytes); 2019-01-09 13:35:52 WARN TaskSetManager:66 - Lost task 7.0 in stage 0.0 (TID 3, scc-q01.scc.bu.edu, executor 1): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:27748,concurren,concurrent,27748,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,1,['concurren'],['concurrent']
Performance,"xt(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 2019-01-09 13:35:52 INFO TaskSetManager:54 - Starting task 1.2 in stage 0.0 (TID 7, scc-q20.scc.bu.edu, executor 2, partition 1, NODE_LOCAL, 7992 bytes); 2019-01-09 13:35:52 INFO TaskSetManager:54 - Lost task 4.1 in stage 0.0 (TID 5) on scc-q20.scc.bu.edu, executor 2: htsjdk.samtools.cram.CRAMException (Reference sequence MD5 mismatch for slice: sequence id 1, start 93925364, span 266689, expected MD5 54babf05a23e9e88a8738dfbb20a1683) [duplicate 1]; 2019-01-09 13:35:53 INFO TaskSetManager:54 - Starting task 7.1 in stage 0.0 (TID 8, scc-q01.scc.bu.edu, executor 1, partition 7, NODE_LOCAL, 7992 bytes); 2019-01-09 13:35:53 INFO TaskSetManager:54 - Lost task 2.1 in stage 0.0 (TID 6) on scc-q01.scc.bu.edu, executor 1: htsjdk.samtools.cram.CRAMException (Reference sequence MD5 mismatch for slice: sequenc",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:29939,concurren,concurrent,29939,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,1,['concurren'],['concurrent']
Performance,"xt(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 2019-01-07 11:34:12 INFO DAGScheduler:54 - Job 0 failed: count at CountReadsSpark.java:80, took 9.419880 s; 2019-01-07 11:34:12 INFO AbstractConnector:318 - Stopped Spark@f1d88ea{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}; 2019-01-07 11:34:12 INFO SparkUI:54 - Stopped Spark web UI at http://scc-hadoop.bu.edu:4041; 2019-01-07 11:34:12 INFO YarnClientSchedulerBackend:54 - Interrupting monitor thread; 2019-01-07 11:34:12 INFO YarnClientSchedulerBackend:54 - Shutting down all executors; 2019-01-07 11:34:12 INFO YarnSchedulerBackend$YarnDriverEndpoint:54 - Asking each executor to shut down; 2019-01-07 11:34:12 INFO SchedulerExtensionServices:54 - Stopping SchedulerExtensionServices; (serviceOption=None,; services=List(),; started=false); 2019-01-07 11:34:12 INFO YarnClientSchedulerBackend:54",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:34893,concurren,concurrent,34893,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,1,['concurren'],['concurrent']
Performance,"xt(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 2019-01-09 13:35:56 INFO DAGScheduler:54 - Job 0 failed: count at CountReadsSpark.java:80, took 12.691336 s; 2019-01-09 13:35:56 INFO AbstractConnector:318 - Stopped Spark@22fda322{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}; 2019-01-09 13:35:56 INFO SparkUI:54 - Stopped Spark web UI at http://scc-hadoop.bu.edu:4041; 2019-01-09 13:35:56 INFO YarnClientSchedulerBackend:54 - Interrupting monitor thread; 2019-01-09 13:35:56 INFO YarnClientSchedulerBackend:54 - Shutting down all executors; 2019-01-09 13:35:56 INFO YarnSchedulerBackend$YarnDriverEndpoint:54 - Asking each executor to shut down; 2019-01-09 13:35:56 INFO SchedulerExtensionServices:54 - Stopping SchedulerExtensionServices; (serviceOption=None,; services=List(),; started=false); 2019-01-09 13:35:56 INFO YarnClientSchedulerBackend:",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:34643,concurren,concurrent,34643,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,1,['concurren'],['concurrent']
Performance,xt(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at scala.Opti,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:38109,concurren,concurrent,38109,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,2,['concurren'],['concurrent']
Performance,xt(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 2019-01-07 11:34:12 INFO ShutdownHookManager:54 - Shutdown hook called; 2019-01-07 11:34:12 INFO ShutdownHookManager:54 - Deleting directory /tmp/spark-1ac79f09-1a36-4668-92d9-0739775f98ed; 2019-01-07 11:34:12 INFO ShutdownHookManager:54 - Deleting directory /tmp/spark-ed279998-3783-4f41-8fe5-f44a4fac3ee4; ```. CountReads runs fine..... ```; gatk CountReads --input HG04302.alt_bwamem_GRCh38DH.20150718.GBR.low_coverage.cram --reference file:///restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa; Using GATK jar /share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /share/pkg/gatk/4.0.12.0/ins,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:43004,concurren,concurrent,43004,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,1,['concurren'],['concurrent']
Performance,"xt(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 2019-01-09 13:35:56 INFO ShutdownHookManager:54 - Shutdown hook called; 2019-01-09 13:35:56 INFO ShutdownHookManager:54 - Deleting directory /tmp/spark-69cc5c72-eff6-4259-8b3b-12fa6f8c42b0; 2019-01-09 13:35:56 INFO ShutdownHookManager:54 - Deleting directory /tmp/spark-0bd07e00-4f6d-43bd-b9d2-b1999376c72b; ```. Just to verify, the non-spark version still runs fine with the compressed fasta.... ```; gatk CountReads --input HG04302.alt_bwamem_GRCh38DH.20150718.GBR.low_coverage.cram --reference GRCh38_full_analysis_set_plus_decoy_hla.fa.gz; Using GATK jar /share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /shar",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:42756,concurren,concurrent,42756,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,1,['concurren'],['concurrent']
Performance,"xtensive (growing with the number of points in a segment), and 2) binary segmentation is a global, greedy algorithm. These both cause long events to be preferred over short events, and thus the first changepoints found (and retained after applying the penalty) may not include those for small, obvious events. For example, see performance on this simulated data, which includes events of size 10, 20, 30, and 40 within 100,000 points at S/N ratio 3:1 in addition to sine waves of various frequency at S/N ratio 1:2 (to roughly simulate GC waves). Changepoints arising from the sine waves will be found first, since these give rise to longer segments:. ![wave-kern-no-local](https://user-images.githubusercontent.com/11076296/29322673-4dd9a1ac-81ac-11e7-94f5-5c5494e44ac5.png). CBS similarly finds many false positive breakpoints:. ![wave-cbs](https://user-images.githubusercontent.com/11076296/29322677-5576e4ba-81ac-11e7-888b-07ed5bff27e3.png). However, when we tune down the sine waves to 1:10, ApproxKernSeg still gets tripped up, but CBS looks better:. ![wave-kern-no-local-small-waves](https://user-images.githubusercontent.com/11076296/29322732-815df58c-81ac-11e7-8305-6e1798616336.png); ![wave-cbs-small-waves](https://user-images.githubusercontent.com/11076296/29322737-836b78fe-81ac-11e7-93be-753a40011203.png). To improve ApproxKernSeg, we can 1) make the cost function intensive, by simply dividing by the number of points in a segment, and 2) add to the cost function a local term, given by the cost of making each point a changepoint within a local window of a determined size. This local term was inspired by methods such as SaRa (http://c2s2.yale.edu/software/sara/). The reasoning is that with events at higher S/N ratio, we typically don't need to perform a global test to see whether any given point is a suitable changepoint; using the data locally surrounding the point typically suffices. With these modifications, ApproxKernSeg can handle both scenarios:; ![wave-kern](https://us",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-322502045:1162,tune,tune,1162,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-322502045,1,['tune'],['tune']
Performance,"y implementation:. ```; public static <A,B> PCollection<B> apply(PCollection<? extends A> input, SerializableFunction<A, B> f){; return input.apply(DataflowUtils.lift(f));; }. public static <A,B> PTransform<PCollection<? extends A>, PCollection<B>> lift(SerializableFunction<A, B> f){; return ParDo.of(new DoFn<A, B>() {; @Override; public void processElement(ProcessContext c) throws Exception {; c.output(f.apply(c.element()));; }; });; }; ```. example usage:. ```; @Test; public void testApplyToString(){; Pipeline p = GATKTestPipeline.create();; PCollection<Integer> pints = p.apply(Create.of(Arrays.asList(1, 2, 3)));. PCollection<String> presults = DataflowUtils.apply( pints, Object::toString).setCoder(StringUtf8Coder.of());. DataflowAssert.that(presults).containsInAnyOrder(""1"",""2"",""3"");; p.run();; }; ```. note the `setCoder` call, if you don't include that you get. ```; SLF4J: Class path contains multiple SLF4J bindings.; SLF4J: Found binding in [jar:file:/Users/louisb/.gradle/caches/modules-2/files-2.1/org.slf4j/slf4j-jdk14/1.7.7/25d160723ea37a6cb84e87cd70773ff02997e857/slf4j-jdk14-1.7.7.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: Found binding in [jar:file:/Users/louisb/.gradle/caches/modules-2/files-2.1/org.slf4j/slf4j-log4j12/1.7.10/b3eeae7d1765f988a1f45ea81517191315c69c9e/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: Found binding in [jar:file:/Users/louisb/.gradle/caches/modules-2/files-2.1/org.slf4j/slf4j-log4j12/1.7.5/6edffc576ce104ec769d954618764f39f0f0f10d/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.; SLF4J: Actual binding is of type [org.slf4j.impl.JDK14LoggerFactory]; java.lang.IllegalStateException: Unable to infer a default Coder for AnonymousParDo.out [PCollection]; either correct the root cause below or use setCoder() to specify one explicitly. ; at com.google.cloud.dataflow.sdk.values.TypedPValue.getCoder(Typed",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/658#issuecomment-122314248:1105,cache,caches,1105,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/658#issuecomment-122314248,1,['cache'],['caches']
Performance,"y ratios were identical down to 1E-16 levels. Differences are only due to removing the unnecessary pseudoinverse computation. Results after filtering and before SVD are identical, despite the code being rewritten from scratch to be more memory efficient (e.g., filtering is performed in place)---phew!. If we stored read counts as HDF5 instead of as plain text, this would make things much faster. Perhaps it would be best to make TSV an optional output of the new coverage collection tool. As a bonus, it would then only take a little bit more code to allow previous PoNs to be provided via -I as an additional source of read counts. Remaining TODO's:. - [x] Allow DenoiseReadCounts to be run without a PoN. This will just perform standardization and optional GC correction. This gives us the ability to run the case sample pipeline without a PoN, which will give users more options and might be good enough if the data is not too noisy.; - [x] Actually, I'm not sure why we take perform SVD on the intervals x samples matrix and take the left-singular vectors. <s>Typically, SVD is performed on the samples x intervals matrix and the right-singular vectors are taken, which saves some extra computation that is necessary to calculate the left-singular vectors.</s> (EDIT: Actually, looks like Spark's SVD is faster on tall and skinny matrices, which might be due to the fact that the underlying implementation calls Fortran code. I still think that representing samples as row vectors has some benefits, so I've changed things to reflect this; I now just take a transpose before performing the SVD, so that we still operate on the same intervals x samples matrix.) This will also save us some transposing, which we do anyway to make HDF5 writes faster.; - [x] Change HDF5 matrix writing to allow matrices with NxM > MAX_INT, which can be done naively by chunking and writing to multiple HDF5 subdirectories. This will allow for smaller bin sizes. (EDIT: I implemented this in a way that allows one ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351:1812,perform,perform,1812,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351,1,['perform'],['perform']
Performance,y.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:125); 	at com.github.jengelman.gradle.plugins.shadow.tasks.ShadowCopyAction$1.execute(ShadowCopyAction.groovy:78); 	at com.github.jengelman.gradle.plugins.shadow.tasks.ShadowCopyAction$1$execute.call(Unknown Source); 	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:48); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:113); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:125); 	at com.github.jengelman.gradle.plugins.shadow.tasks.ShadowCopyAction.withResource(ShadowCopyAction.groovy:109); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:93); 	at org.codehaus.groovy.runtime.callsite.StaticMetaMethodSite$StaticMetaMethodSiteNoUnwrapNoCoerce.invoke(StaticMetaMethodSite.java:151); 	at org.codehaus.groovy.runtime.callsite.StaticMetaMethodSite.callStatic(StaticMetaMethodSite.java:102); 	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallStatic(CallSiteArray.java:56); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callStatic(AbstractCallSite.java:194); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callStatic(AbstractCallSite.java:214); 	at com.github.jengelman.gradle.plugins.shadow.tasks.ShadowCopyAction.execute(ShadowCopyAction.groovy:75); 	at org.gradle.api.internal.file.copy.NormalizingCopyActionDecorator.execute(NormalizingCopyActionDecorator.java:53); 	at org.gradle.api.internal.file.copy.DuplicateHandlingCopyActionDecorator.execute(DuplicateHandlingCopyActionDecorator.java:42); 	at org.gradle.api.internal.file.copy.CopyActionExecuter.execute(CopyAction,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5499#issuecomment-446253445:5606,Cache,CachedMethod,5606,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5499#issuecomment-446253445,1,['Cache'],['CachedMethod']
Performance,"yay!. On Mon, Feb 23, 2015 at 6:02 PM, droazen notifications@github.com wrote:. > Replicates most of the functionality of the old ROD system in ~5% of the; > code. The incomprehensible tangle of nested iterators, bindings, views,; > states,; > tracks, trackers, builders etc., etc., is gone, replaced by about 4 core; > classes:; > FeatureContext, FeatureDataSource, FeatureInput, and FeatureManager.; > ; > FeatureContext: This is tool-facing interface (replaces; > RefMetaDataTracker).; > Allows particular sources of Features to be queried.; > ; > FeatureDataSource: Handles the low-level details of querying a source of; > Features.; > Uses a caching scheme optimized for the use case of queries over; > intervals with gradually increasing start/stop positions.; > ; > FeatureInput: This is used to declared Feature arguments in tools; > (replaces RodBinding).; > The engine discovers all FeatureInput arguments declared in the tool's; > class; > hierarchy, and initializes data sources for each one that was specified; > on the command line.; > ; > FeatureManager: Manages the pool of data sources, as well as codec and; > file format; > discovery and type checking.; > ; > -ReadWalker interface has changed: apply() now takes a FeatureContext; > argument; > (will be null if there are no sources of Features).; > ; > -Included an example tool PrintReadsWithVariants to demonstrate use of the; > new; > ReadWalker interface.; > ; > -Since Feature files must be indexed in order to query them, I have; > provided a; > tool IndexFeatureFile that can index any Feature-containing file.; > ; > -Made required changes to the argument-parsing system. Feature argument; > discovery; > is as de-coupled as possible from the main arg parser.; > ; > -Made required changes to BQSR, and eliminated the temporary; > HACKRefMetaDataTracker.; > ; > ## -Comprehensive tests; > ; > You can view, comment on, or merge this pull request online at:; > ; > https://github.com/broadinstitute/hellbender/pull/224; > Co",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/224#issuecomment-75657392:662,optimiz,optimized,662,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/224#issuecomment-75657392,1,['optimiz'],['optimized']
Performance,"your recommendation is still to copy the workspace prior to merging/appending to it, then the distributed processing still means copying the original, and to my thinking copying each contig's folder into a new workspace, vs. copying each contig into the same workspace is basically the same overhead. We also tend to keep the long-lived copy on our warm storage, with processing happening on our cluster's lustre filesystem. . 2) Again, i dont think it's necessarily right to assume every job will operate on the same set of intervals. We generally would use the same pattern, but there are legitimate cases in which different intervals/job would better match the cluster's availability. If we're appending a limited number of samples and our cluster is busy, we might want to scatter using more intervals/job since each job would finish fairly quickly and the practical reality is fewer total jobs would complete quicker. if we are performing an operation that requires a lot of time/job (like creating a new workspace or appending a lot of samples), we might do one job/contig. It's also worth pointing out that macaque has 1000s of small unplaced contigs, and therefore we almost never do a simple 1:1 job:contig scheme.; ; 3) When I was originally thinking about how to scatter/gather the creation of a combined gVCF, the overhead of re-merging was huge. There was zero point in taking the per-contig gVCFs and concat/bgzipping a new one, just to split it again. When I started down this road, my idea was to make a folder holding each gVCF, and a top-level JSON file to map contig->filepath, so code could intelligently work with these. The latter essentially describes the structure of a GenomicsDB workspace. Unlike concatenating gVCFS, the overhead of moving directories around is practically zero. Sure, I could make a folder of GenomicsDB workspaces, but if I'm already moving them, what's the point in not merging? . I could understand that is the workspace lived on a shared filesystem and",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6620#issuecomment-640881049:1038,perform,performing,1038,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6620#issuecomment-640881049,1,['perform'],['performing']
Performance,"zer.read(DefaultArraySerializers.java:396); 	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.read(DefaultArraySerializers.java:307); 	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790); 	at org.apache.spark.serializer.KryoSerializerInstance.deserialize(KryoSerializer.scala:362); 	at org.apache.spark.scheduler.DirectTaskResult.value(TaskResult.scala:88); 	at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$1.apply$mcV$sp(TaskResultGetter.scala:72); 	at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$1.apply(TaskResultGetter.scala:63); 	at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$1.apply(TaskResultGetter.scala:63); 	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992); 	at org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:62); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: java.lang.NullPointerException; 	at htsjdk.variant.variantcontext.LazyGenotypesContext.decode(LazyGenotypesContext.java:158); 	at htsjdk.variant.variantcontext.LazyGenotypesContext.invalidateSampleOrdering(LazyGenotypesContext.java:205); 	at htsjdk.variant.variantcontext.GenotypesContext.add(GenotypesContext.java:353); 	at htsjdk.variant.variantcontext.GenotypesContext.add(GenotypesContext.java:46); 	at com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:134); 	at com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:40); 	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708); 	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125); 	... 18 more; 19/02/18 16:58:29 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@45c90a05{HTTP/1.1,[http/1.1]}{0.0.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3840#issuecomment-464825765:7760,concurren,concurrent,7760,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3840#issuecomment-464825765,1,['concurren'],['concurrent']
Performance,ø> (ø)` | `7 <0> (ø)` | :arrow_down: |; | [...ools/coveragemodel/germline/GermlineCNVCaller.java](https://codecov.io/gh/broadinstitute/gatk/pull/3027?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2dlcm1saW5lL0dlcm1saW5lQ05WQ2FsbGVyLmphdmE=) | `73.196% <ø> (ø)` | `13 <0> (ø)` | :arrow_down: |; | [...exome/sexgenotyper/TargetCoverageSexGenotyper.java](https://codecov.io/gh/broadinstitute/gatk/pull/3027?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9zZXhnZW5vdHlwZXIvVGFyZ2V0Q292ZXJhZ2VTZXhHZW5vdHlwZXIuamF2YQ==) | `84% <ø> (ø)` | `5 <0> (ø)` | :arrow_down: |; | [...der/tools/spark/sv/AlignAssembledContigsSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/3027?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9BbGlnbkFzc2VtYmxlZENvbnRpZ3NTcGFyay5qYXZh) | `100% <ø> (ø)` | `12 <0> (ø)` | :arrow_down: |; | [...egmentation/PerformAlleleFractionSegmentation.java](https://codecov.io/gh/broadinstitute/gatk/pull/3027?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9zZWdtZW50YXRpb24vUGVyZm9ybUFsbGVsZUZyYWN0aW9uU2VnbWVudGF0aW9uLmphdmE=) | `88.889% <ø> (ø)` | `2 <0> (ø)` | :arrow_down: |; | [...ools/walkers/contamination/GetPileupSummaries.java](https://codecov.io/gh/broadinstitute/gatk/pull/3027?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2NvbnRhbWluYXRpb24vR2V0UGlsZXVwU3VtbWFyaWVzLmphdmE=) | `83.333% <ø> (ø)` | `12 <0> (ø)` | :arrow_down: |; | [...tools/spark/sv/RunSGAViaProcessBuilderOnSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/3027?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9SdW5TR0FWaWFQcm9jZXNzQnVpbGRlck9uU3BhcmsuamF2YQ==) | `0% <ø> (ø)` | `0 <0> (ø)` | :arrow_down: |; | [...stitute/hellbender/tools/HaplotypeCallerSpark.java](https://codecov.io/gh/b,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3027#issuecomment-306340681:2804,Perform,PerformAlleleFractionSegmentation,2804,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3027#issuecomment-306340681,1,['Perform'],['PerformAlleleFractionSegmentation']
Safety," NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (/tmp/libgkl_compression3825249225068031371.so: /tmp/libgkl_compression3825249225068031371.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > 16:17:04.402 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; >; > 16:17:04.407 **WARN** NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (/tmp/libgkl_compression7506152962158874866.so: /tmp/libgkl_compression7506152962158874866.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > Sep 04, 2020 4:17:05 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; >; > INFO: Failed to detect whether we are running on Google Compute Engine.; >; > 16:17:05.842 INFO HaplotypeCaller - ------------------------------------------------------------; >; > 16:17:05.843 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.1.8.1; >; > 16:17:05.843 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; >; > 16:17:05.843 INFO HaplotypeCaller - Executing as robert@powerlinux on Linux v4.4.0-184-generic ppc64le; >; > 16:17:05.843 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_252-8u252-b09-1~16.04-b09; >; > 16:17:05.843 INFO HaplotypeCaller - Start Date/Time: September 4, 2020 4:17:04 PM UTC; >; > 16:17:05.843 INFO HaplotypeCaller - ------------------------------------------------------------; >; > 16:17:05.843 INFO HaplotypeCaller - ------------------------------------------------------------; >; > 16:17:05.844 INFO HaplotypeCaller - HTSJDK Version: 2.23.0; >; > 16:17:05.844 INFO HaplotypeCaller - ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600:2607,detect,detect,2607,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600,1,['detect'],['detect']
Safety," a dangling head which often causes problem and in this case causes the variant to sometimes not be correctly assembled. This is where the dangling head gets separated from garbage in the 20 threshold graph: ![Screen Shot 2022-01-25 at 4 13 52 PM](https://user-images.githubusercontent.com/16102845/151060728-5a0d4d95-2eb4-4777-a0e9-34b07b2e6196.png). And here is that spot in the 60 threshold graph:; ![Screen Shot 2022-01-25 at 4 16 43 PM](https://user-images.githubusercontent.com/16102845/151061165-fb803312-59b8-48c4-b196-b0e97d2e00ea.png). And here it is in the 1 threshold ; <img width=""303"" alt=""Screen Shot 2022-01-25 at 4 22 17 PM"" src=""https://user-images.githubusercontent.com/16102845/151061909-25d41a3d-39c2-461e-8fd3-938e859ef3d7.png"">; graph:. This seems to have caused the two thresholds to assemble different haplotypes after dangling end recovery (since all of these are dangling ends because the assembly engine can't do anything else because there is not enough padding provided) and it just so happens this failed assembly misses the correct haplotype in that 20 threshold graph and we end up throwing away most of the reads as incongruent with assembly as a result which is why the depth drops out so low at that site. This is a pretty rare edge case and I happened to be able to recover the 20 mq threshold variant with reasonable correct coverage by playing with the `--min-pruning 4` argument. In general though this issue might or might not have existed if the bam snippet provided (and especially the calling interval you provided of chr7:145945238-145945238) were not centered on one single point since assembly works best and is most likely to succeed when it has a few hundred bases of padding around the variant in question (typically for a SNP we end up with at least 100 bases of active window plus another 300 bases of padding on either side for assembly) which cuts down on the risk of assembly failures like this one. I'm curious if you observed this behavior on ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7124#issuecomment-1021629139:1118,recover,recovery,1118,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7124#issuecomment-1021629139,1,['recover'],['recovery']
Safety, at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.s,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:41555,abort,abortStage,41555,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,1,['abort'],['abortStage']
Safety," batch api for it? Multi layer docker builds are pretty standard from what I understand. . It sounds like your suggestions are talking about 2 slightly different issues to me. 1. Too many layers:. We typically have squashed the GATK docker images, but we recently switched to building our release images with google cloud build. Since squash is *STILL* an experimental feature in docker we've had trouble getting it to work there. Since the size reduction was pretty minimal from squashing we figured it would be ok to not prioritize it. It's definitely possible for us to consolidate various layers in the build. Or manually squash the images. We can take a look for our next release. Wide workflows on azure are something we need to support. 2. Docker size reduction:; I've spend a lot of time looking at this in the past. Our docker image is huge, but it's mostly due to the massive size of our python and R dependencies. I've done a bunch of work reducing temporary files in independent layers and using multiple stages to reduce the size. There's not much low hanging fruit left there. Similarly, moving to alpine is tricky an has limited benefit. GATK packages a number of C libraries which do not work out of the box on alpine due to the different C runtime. (At least that was the case the last time I investigated it a few years ago. ) I suspect there's a way to port things so they work on it, but it's not something we can do now. It also wouldn't be much of a help, the base image is completely dwarfed by piles of python and R dependencies which are very difficult to safely trim. Anyway, that's the state of things. We've considered a java only image for a while which would be much smaller than the current one. (although still fat by most docker standards...). We've never released one publicly because it seemed like it might cause confusion, but it's a reasonable possibility. . If you have any secret methods to reduce the size of python or R installations we're happy to take PRs!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8684#issuecomment-1934859427:1793,safe,safely,1793,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8684#issuecomment-1934859427,1,['safe'],['safely']
Safety," can hopefully rely on per-bin bias modeling to at least partially account for mappability in gCNV calling (and we certainly wouldn't want to filter out a significant fraction of the genome, in any case). Do we agree?. To answer your first question, the criterion for choosing the peak is quite hacky at the moment, but I found that filtering low-count bins to first check for the presence of a high peak and then falling back to the peak at zero works perfectly fine in practice. . We can certainly try to do something smarter, since, as you say, bin filtering may be desirable---even if we implement mappability filtering---to remove large germline events (it's true that the ""example"" I showed above is indeed from the PAR-like region on X, as you point out, but this is roughly how a large arm-level event would appear even after mappability filtering.) Although the model I fit above, which is simply a sparse mixture of NBs with regularly-spaced means (modulo some sample-specific and contig-specific jitter), could conceivably capture such events as well, we want to avoid models where a single NB might try to capture two or more peaks. Also, just to clarify, the weird mosaic examples are the bottom two plots out of the four above---you can see the shifted (non-X, in one of the examples) single peaks. However, it's interesting that the PARs are still showing up in XY---I'm pretty sure I used the blacklist you provided, although I will double check. Did that only include the ""official"" PARs, or also the additional ones you found?. In any case, are we comfortable calling in those regions (here I'm talking about gCNV, not ploidy)? As I show above, I don't think we need mappability to nail the baseline ploidy. Can we then rely on the per-bin bias to account for these regions in gCNV (pinning them back to the correct CN) without mappability filtering? And with mappability filtering, how substantial is the hit to coverage in these regions? Should we blacklist them for the time bein",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4558#issuecomment-375923639:1327,avoid,avoid,1327,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4558#issuecomment-375923639,1,['avoid'],['avoid']
Safety," first step to correctly identify the issue. So it seems a bit premature to even prototype a method, much less merge it. I think this PR, as is, muddies the waters quite a bit. For example, it introduces a new Record class that denotes this type of ""CNLOH"" with a `C`. If we want to merge this, I suggest that we first correctly identify the issue. If these events are not mosaic CNLOH, then we should clean up all mention of CNLOH in this code. Either way, can we quantify the level of improvement gained by filtering such events in a reproducible evaluation? If so, let's bring that into gatk-evaluation. Finally, there are many more options available to change the segmentation and/or resolution than the single one you mentioned. If the users you are working with can clearly specify their analysis goals in terms of resolution, then it might be possible to sidestep the problem entirely without adding more unsupported code. This would also buy us more time to put in a principled solution, without the risk of unsupported code getting entrenched in their workflows. > There are definitely events that get missed without the germline tagging, so this is an improvement over blacklisting alone. And while I have seen erroneous germline tagging (i.e. false calling a segment germline), it was only ever due to really noisy data (e.g. a bad PoN) or a poorly tuned segment caller. This is encouraging. This means that a straightforward approach to germline filtering, such as simply identifying overlapping posteriors as mentioned above, should work well. Prototyping this approach shouldn't take long at all, especially when the matched normal is guaranteed to be available, as it is in this workflow (tumor-only would require some work to identify the normal state, as mentioned previously). I'd rather just roll that, evaluate it, and merge it instead. Key here is that we sidestep the deficiencies of the current CR-only caller, which also shares the blame for this ""CNLOH"" issue (since these ev",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5450#issuecomment-461431199:3149,risk,risk,3149,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5450#issuecomment-461431199,1,['risk'],['risk']
Safety," in the generated VCF records; 13:00:15.145 info NativeGenomicsDB - pid=144146 tid=144147 No valid combination operation found for INFO field MLEAF - the field will NOT be part of INFO fields in the generated VCF records; 13:00:17.976 INFO FeatureManager - Using codec BEDCodec to read file file:///home/groups/prime-seq/production/Shared/@files/.referenceLibraries/128/tracks/NCBI_Mmul_10.softmask.bed; 13:00:28.734 INFO IntervalArgumentCollection - Initial include intervals span 3961776 loci; exclude intervals span 1586664325 loci; 13:00:28.738 INFO IntervalArgumentCollection - Excluding 2060069 loci from original intervals (52.00% reduction); 13:00:28.738 INFO IntervalArgumentCollection - Processing 1901707 bp from intervals; 13:00:28.816 INFO SelectVariants - Done initializing engine; 13:00:28.816 WARN SelectVariants - ***************************************************************************************************************************; 13:00:28.816 WARN SelectVariants - * Detected unsorted genotype fields on input. *; 13:00:28.816 WARN SelectVariants - * SelectVariants will sort the genotypes on output which could result in slow traversal as it involves genotype parsing. *; 13:00:28.816 WARN SelectVariants - ***************************************************************************************************************************; 13:00:28.941 INFO ProgressMeter - Starting traversal; 13:00:28.941 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; GENOMICSDB_TIMER,GenomicsDB iterator next() timer,Wall-clock time(s),0.21497791400000002,Cpu time(s),0.113811361; GENOMICSDB_TIMER,GenomicsDB iterator next() timer,Wall-clock time(s),0.9714307110000004,Cpu time(s),0.8294423339999996; GENOMICSDB_TIMER,GenomicsDB iterator next() timer,Wall-clock time(s),0.018746290999999998,Cpu time(s),0.018747005; GENOMICSDB_TIMER,GenomicsDB iterator next() timer,Wall-clock time(s),0.04312575600000001,Cpu time(s),0.04312843799999999; GENOMICSDB_TIMER,",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1209854842:1869,Detect,Detected,1869,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1209854842,1,['Detect'],['Detected']
Safety," require the Conda environment, but the tool itself will not. But I think this is probably preferable to writing test code to compare HDF5s, minimal though that might be, since the schema might change in the future.; - [x] Tool-level docs. Minor TODOs:. - [x] Parameter-level docs. Could perhaps expand on the `resources` parameter once the required labels are settled.; - [x] Parameter validation.; - [x] Clean up docs for parent walker.; - [x] Decide on required labels. I think ""training"" and ""calibration"" (rather than the legacy ""training"" and ""truth"") might be good candidates. EDIT: Switched ""truth"" to ""calibration"" throughout the codebase.; - [x] Validate privileged labels (snp, training, calibration) in parent walker.; ; Future work:. - [ ] Clean up unlabeled outputs. This includes 1) sorting the corresponding HDF5, and 2) outputting a corresponding sites-only VCF. Unlike the labeled sites, which are written individually to VCF as we traverse them, unlabeled sites are placed into a reservoir of fixed size for subsampling purposes. Thus, we cannot write them to VCF as with labeled sites; furthermore, after traversal, the unlabeled sites are not ordered within the reservoir. Ultimately, the lack of this VCF means that extracted, unlabeled sites cannot be tagged as such by the scoring tool in the final VCF.; - [ ] Consider downsampling of labeled data. This is not done because 1) of the complications just mentioned, 2) we assume that labeled data is precious and that one-time extraction of it will always be relatively cheap, especially compared to training (and that training implementations can always downsample, if needed), and 3) using -L functionality to subset genomic regions is perhaps a cleaner strategy for doing so.; - [x] I think we can probably clean up treatment of allele-specific annotations by automatically detecting whether an annotation is an array type. This would obviate the need for the parameter to turn on allele-specific mode. EDIT: Added in #8131.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7724#issuecomment-1067948059:4634,detect,detecting,4634,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7724#issuecomment-1067948059,1,['detect'],['detecting']
Safety," that. Sincerely,; Emily. From: ldgauthier ***@***.***>; Sent: Monday, March 28, 2022 2:39 PM; To: broadinstitute/gatk ***@***.***>; Cc: Emily Elizabeth Puckett (puckett3) ***@***.***>; Mention ***@***.***>; Subject: Re: [broadinstitute/gatk] CombineGVCFs: ERROR input alleles must contain <NON_REF> (Issue #7737). CAUTION: This email originated from outside of the organization. Do not click links or open attachments unless you recognize the sender and trust the content is safe. If I'm reading the process correctly, I don't actually think this should work. CombineGVCFs is specifically for combining GVCFs and it expects GVCFs to have <NON_REF> alleles. If you've already run the data through GenotypeGVCFs then you can't use CombineGVCFs again because the <NON_REF> likelihoods have been applied and those alleles are gone. The vcfcombine tool from bcftools is quite fast if all you want to do is join the samples together. -; Reply to this email directly, view it on GitHub<https://nam02.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fbroadinstitute%2Fgatk%2Fissues%2F7737%23issuecomment-1081062021&data=04%7C01%7CEmily.Puckett%40memphis.edu%7C51db6aa9f41b483e1ce408da10f2aa5d%7Cae145aeacdb2446ab05a7858dde5ddba%7C0%7C0%7C637840931685525269%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000&sdata=Pxg8joQfE51l5e3cUUbKA9bQEYDZjp0AxdX0aqDG1MY%3D&reserved=0>, or unsubscribe<https://nam02.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FALDFEHAXSKZ7YHSFGISLPUTVCIDGZANCNFSM5RZSK5PA&data=04%7C01%7CEmily.Puckett%40memphis.edu%7C51db6aa9f41b483e1ce408da10f2aa5d%7Cae145aeacdb2446ab05a7858dde5ddba%7C0%7C0%7C637840931685525269%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000&sdata=6Dkb6rbHDZpS05bYUHhlIRHJitgVtR%2FPB5rNHHFMg%2FQ%3D&reserved=0>.; You are receiving this because you were mentioned.Message ID: ***@***.******@***.***>>",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7737#issuecomment-1082170127:1486,safe,safelinks,1486,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7737#issuecomment-1082170127,1,['safe'],['safelinks']
Safety," the count likelihood is actually misspecified there. As an example, consider trying to fit a Poisson to data that is actually zero-inflated Poisson---fitting the histogram will actually result in a more robust estimate for the mean. Another benefit is that truncated data (as we have here) is straightforwardly handled in an unbiased way. In the special case of complete, trivially-binned data, the full, unbinned likelihood is recovered. I think this sort of histogram fitting is pretty standard in the astro/particle community. We can certainly change up the model to include strictly quantized + free-floating states as you describe (rather than the ""fuzzily quantized"" states I use here), but I just wanted to avoid having another level of mixtures/logsumexps for this quick prototype. However, note that modeling mosaicism on the autosomes is desirable, but there we also want the strong diploid prior to nail down the depth and per-contig bias. So we will have to be a little careful about how we introduce free-floating states. Also, since I was not using gcnvkernel, I had to integrate out all discrete parameters. It may be that we can write down a nice model with discrete parameters if we use your inference framework. Finally, I did not further bin the counts here (or rather, the bin size is 1), which already yields the maximum information, but I did use a maximum-count cutoff. If we use the same cutoff for all samples, this allows us to simply pass a non-ragged matrix from Java (with dimensions of samples x contigs x maximum count) as a TSV. However, we may run into trouble if we hit a case sample with very high depth. So some sort of sparse representation of the histogram might indeed be desirable, but I think it should be an exact representation of the full histogram. This would require us to sync up code to emit and consume the representation in both Java and python, so I'd like to avoid it if possible---I think I'd prefer just emitting the ragged matrix, in that case.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-376307522:2356,avoid,avoid,2356,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-376307522,1,['avoid'],['avoid']
Safety," tool, MosaicHunter. The option you suggest looks great. Do you mean that I should establish the GATK 4 developing environment and develop the MosaicHunterFilter tool? I may do that when I have some time. I found the document of GATK 4 at https://github.com/broadinstitute/gatk. Do you have any further advices?. Best regards,; Adam Yongxin Ye; Center for Bioinformatics; Peking University. At 2018-07-07 01:43:05, ""Geraldine Van der Auwera"" <notifications@github.com> wrote:. Hi @Yyx2626, I'm Geraldine, you may remember me from the Beijing training. It was great visiting your team! I'm sorry it took me so long to follow up on this discussion, and I want to thank you again for reaching out to us about integrating the tool that you developed into GATK. We are certainly very interested in providing this enhancement to the research community, and we are now ready to talk about the next steps. After examining your paper and the source code in Github, we think that the most efficient way to integrate the functionality you developed would be to adapt the filtering parts of your tool to run on the output of Mutect2. So this would be a standalone tool that you would run after Mutect2, much like the current FilterMutectCalls tool. If the results are comparable to your current tool, then we would take that into the official distribution of GATK. If somehow that integration does not yield satisfactory results, then we would look at integrating the entire tool, though we're hoping it won't be necessary, so we can avoid maintaining duplicate functionality for some of the boilerplate data transformations. David @davidbenjamin can provide some advice on how to implement this in GATK4; in brief you would need to write some code that applies the filters you developed to a variant context. Let us know if this is an option you'd like to explore; we'd be happy to help. —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4632#issuecomment-404104349:1747,avoid,avoid,1747,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4632#issuecomment-404104349,1,['avoid'],['avoid']
Safety," used is with absolute path as following:; ```; java1.8 -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compres; sion_level=2 -Xmx4g -jar /dsg_cent/packages/GATK/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar VariantRecalibrator \; -R /dsgmnt/llfs2/masterdata/geno/hg38/resources_broad_hg38_v0_Homo_sapiens_assembly38.fasta \; -V /dsguser/xhong/llfs_workdir/refinement/VQSR/gatk4100v2/c1joint_c1.filtered.SiteOnly.vcf \; --resource hapmap,known=false,training=true,truth=true,prior=15:/dsgmnt/db/region/ftpGATK/resources_broad_hg38_v0_hapmap_3.3.hg38.vcf.gz \; --resource omni,known=false,training=true,truth=false,prior=12:/dsgmnt/db/region/ftpGATK/resources_broad_hg38_v0_1000G_omni2.5.hg38.vcf.gz \; --resource 1000G,known=false,training=true,truth=false,prior=10:/dsgmnt/db/region/ftpGATK/resources_broad_hg38_v0_1000G_phase1.snps.high_confidence.hg38.vcf.gz \; --resource dbsnp,known=true,training=false,truth=false,prior=2:/dsgmnt/db/region/ftpGATK/resources_broad_hg38_v0_Homo_sapiens_assembly38.dbsnp138.vcf \; -an QD -an MQ -an MQRankSum -an ReadPosRankSum -an FS -an SOR -an DP \; -tranche 100.0 -tranche 99.95 -tranche 99.9 -tranche 99.8 -tranche 99.6 -tranche 99.5 -tranche 99.4 -tranche 99.3 -tranche 99.0 -tranche 98.0 -tranche 97.0 -tranche 90.0 \; -mode SNP --max-gaussians 6 \; -O /dsguser/xhong/llfs_workdir/refinement/VQSR/gatk4100v2/c1joint_c1.snp.recal \; --output-model /dsguser/xhong/llfs_workdir/refinement/VQSR/gatk4100v2/c1joint_c1.snp.model \; --tranches-file /dsguser/xhong/llfs_workdir/refinement/VQSR/gatk4100v2/c1joint_c1.snp.tranches \; --rscript-file /dsguser/xhong/llfs_workdir/refinement/VQSR/gatk4100v2/c1joint_c1.snp.plots.R; ```. Somehow the current path **/dsgmnt/seq4_llfs/work/xhong/refinement/VQSR/script/** was added in front of **hapmap**, which should not be there.; It would be very helpful if anyone can instruct me how to avoid this problem in GATK4.1.0.0 . Best,; Xin",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2199#issuecomment-484197400:2380,avoid,avoid,2380,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2199#issuecomment-484197400,1,['avoid'],['avoid']
Safety," we'd just want to let the user be able to specify the theano directory (rather than dump things in `~/.theano` unexpectedly). We should think about whether this should be opt-in, i.e., should we preserve the original behavior of using `~/.theano` by default?; > ; > @mwalker174 opinions? @droazen or engine team, thoughts on what the policy should be for python/R scripts doing this sort of thing? Is it generally true that the GATK leaves no trace, other than producing the expected output?. Dear samuelklee,. Thank you very much for you reply. I also found this problem last night. It seems that the problem is originally from Theano and Pymc3, rather than GATK 4.0. Some similar problems have been reported just like (1) https://github.com/pymc-devs/pymc3/issues/1463 (2) https://stackoverflow.com/questions/52270853/how-to-get-rid-of-theano-gof-compilelock and (3) https://groups.google.com/forum/#!topic/theano-users/eJ2vl2PUTk4. Last night, I have already tried to reset base_compiledir for theano, through two ways: (1) creating a ~/.theanorc file just like you suggested (2) modifying the file ~/.bashrc for my login node, by adding a line: export THEANO_FLAGS=""base_compiledir=/scratch/gatk-user1/z-Temp/z-Temp-Theano-$chr"". However, the truth is that, in our cluster, when I submit the 25 jobs (for each chromosomes), they are assigned to different computer nodes randomly. It means that I have to set THEANO environment variable for each corresponding random computer nodes respectively, which is quite difficult for me, as the nodes are random assigned. So, now I'm going to add lines like below to the ~/.theanorc in my login node, to see what will happen. Maybe It will work.; #######; [global]; config.compile.timeout = 100000 ; ######. However, I'm really appreciate it if some one in your team can help to add a function to specify a temporary directory for the theano directory, which can be bound to the corresponding node shared by other GATK threads. Thank you and Best regards.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6235#issuecomment-548557073:2816,timeout,timeout,2816,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6235#issuecomment-548557073,1,['timeout'],['timeout']
Safety," why this is happening or what I can do to overcome this problem? I have run `GenomicsDBImport` and `GenotypeGVCFs` successfully in the past (same version, same computer) on a different dataset, so I'm not sure what about this data is causing the problem. Any guidance is much appreciated!. Thanks,; Jessie. ```; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/jsalt/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar GenotypeGVCFs -R /nfs/data1/jsalt/3RAD/colinus_virginianus_13May2017_V3Fw6_newchrom.fasta -V gendb://odont_cyr_8_snp_db -O odont_cyr_8_snp_db.vcf; 14:59:47.866 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/jsalt/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 03, 2020 2:59:59 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 14:59:59.674 INFO GenotypeGVCFs - ------------------------------------------------------------; 14:59:59.675 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.1.2.0; 14:59:59.675 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 15:00:09.686 INFO GenotypeGVCFs - Executing as jsalt@mustard on Linux v3.10.0-957.1.3.el7.x86_64 amd64; 15:00:09.686 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_192-b01; 15:00:09.687 INFO GenotypeGVCFs - Start Date/Time: February 3, 2020 2:59:47 PM CST; 15:00:09.687 INFO GenotypeGVCFs - ------------------------------------------------------------; 15:00:09.687 INFO GenotypeGVCFs - ------------------------------------------------------------; 15:00:09.688 INFO GenotypeGVCFs - HTSJDK Version: 2.19.0; 15:00:09.688 INFO GenotypeGVCFs - Picard Version: 2.19.0; 15:00:09.689 INFO GenotypeGVCFs - HTSJDK Defaults.COMPRESSION_LEV",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6357#issuecomment-581619640:1411,detect,detect,1411,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6357#issuecomment-581619640,1,['detect'],['detect']
Safety,"!!!!!!!!!!!!!!!![0m. 20:12:42.725 INFO FilterAlignmentArtifacts - Initializing engine; 20:12:48.403 INFO FeatureManager - Using codec VCFCodec to read file gs://fc-secure-024a1aae-a4f9-4025-aa93-f759f93a8203/50383670-4607-4e59-9bfc-4db970980f0e/Mutect2/773a91ea-25be-4d49-b97c-16527076250c/call-Filter/cacheCopy/TN-20-36-filtered.vcf; 20:12:50.117 INFO FilterAlignmentArtifacts - Done initializing engine; 20:12:51.042 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk/gatk-package-4.1.9.0-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 20:12:51.099 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 20:12:51.100 INFO IntelPairHmm - Available threads: 14; 20:12:51.100 INFO IntelPairHmm - Requested threads: 4; 20:12:51.100 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 20:12:51.100 INFO ProgressMeter - Starting traversal; 20:12:51.100 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 20:20:25.766 INFO ProgressMeter - chr3:104142090 7.6 1000 132.0; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007efc9818177e, pid=24, tid=0x00007f13b3c76700; #; # JRE version: OpenJDK Runtime Environment (8.0_242-b08) (build 1.8.0_242-8u242-b08-0ubuntu3~18.04-b08); # Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 ); # Problematic frame:; # C [libgkl_smithwaterman1809483713436863458.so+0x177e] smithWatermanBackTrack(dnaSeqPair*, int, int, int, int, int*, int)+0x60e; #; # Core dump written. Default location: /cromwell_root/core or core.24; #; # An error report file with more information is saved as:; # /cromwell_root/hs_err_pid24.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-781673098:1650,detect,detected,1650,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-781673098,1,['detect'],['detected']
Safety,""":3101046070; },; {; ""name"":""GL000192.1"",; ""length"":547496,; ""tiledb_column_offset"":3101257243; },; {; ""name"":""NC_007605"",; ""length"":171823,; ""tiledb_column_offset"":3101804739; },; {; ""name"":""hs37d5"",; ""length"":35477943,; ""tiledb_column_offset"":3101976562; }; ]; }. And the header generated with GenomicsDBImport is:. ##fileformat=VCFv4.2; ##ALT=<ID=NON_REF,Description=""Represents any possible alternative allele at this location"">; ##FILTER=<ID=LowQual,Description=""Low quality"">; ##FILTER=<ID=PASS,Description=""All filters passed"">; ##FORMAT=<ID=AD,Number=R,Type=Integer,Description=""Allelic depths for the ref and alt alleles in the order listed"">; ##FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Approximate read depth (reads with MQ=255 or with bad mates are filtered)"">; ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=""Genotype Quality"">; ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; ##FORMAT=<ID=MIN_DP,Number=1,Type=Integer,Description=""Minimum DP observed within the GVCF block"">; ##FORMAT=<ID=PGT,Number=1,Type=String,Description=""Physical phasing haplotype information, describing how the alternate alleles are phased in relation to one another"">; ##FORMAT=<ID=PID,Number=1,Type=String,Description=""Physical phasing ID information, where each unique ID within a given sample (but not across samples) connects records within a phasing group"">; ##FORMAT=<ID=PL,Number=G,Type=Integer,Description=""Normalized, Phred-scaled likelihoods for genotypes as defined in the VCF specification"">; ##FORMAT=<ID=SB,Number=4,Type=Integer,Description=""Per-sample component statistics which comprise the Fisher's Exact Test to detect strand bias."">; ##GVCFBlock0-1=minGQ=0(inclusive),maxGQ=1(exclusive); ##GVCFBlock1-2=minGQ=1(inclusive),maxGQ=2(exclusive); ##GVCFBlock10-11=minGQ=10(inclusive),maxGQ=11(exclusive); ##GVCFBlock11-12=minGQ=11(inclusive),maxGQ=12(exclusive); ##GVCFBlock12-13=minGQ=12(inclusive),maxGQ=13(exclusive); ##GVCFBlock13-14=minGQ=13(inclusive),maxGQ=14(exclu",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4514#issuecomment-372215582:11810,detect,detect,11810,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4514#issuecomment-372215582,1,['detect'],['detect']
Safety,"### Feature request. I would like to be able to pull out the indel allele only from mixed records. I'm told this would take the form of a flag that would modify the -selectType behavior to additionally reach into mixed records and subset alleles. If this feature will be addressed, then it also needs to be backported to gsa-unstable. Additionally, I would like to be able to pull out variants based on allele frequency cutoff, e.g. `-select ""AF> [#]""`. This is so I can pull out common and low-frequency variants to the exclusion of rare variants. Currently using this option with the Phase 3 1000 Genomes vcf this gives the following error:. ```; ##### ERROR MESSAGE: Invalid JEXL expression detected for select-0 with message ![0,10]: 'AF > 0.001;' > error; ```. Laura says this error has do with multiallelic sites. Searching the forum for ""JEXL, AF, multiallelic"" I find two relevant discussions (links below) that tell me to do things well beyond my abilities. E.g. first I need to subset for biallelic sites etc. http://gatkforums.broadinstitute.org/gatk/discussion/6526/selectvariants-af-with-multiallelic-variants; http://gatkforums.broadinstitute.org/gatk/discussion/comment/21120#Comment_21120",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1537#issuecomment-190920186:694,detect,detected,694,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1537#issuecomment-190920186,1,['detect'],['detected']
Safety,"#### Guidelines for converting arguments to kebab case. We're not following an external spec doc, so here some guidelines to follow instead. Keep in mind that the main thing we're going for here is readability and consistency across tools, not absolute purity, so feel free to raise discussion on any cases where you feel the guidelines should be relaxed. Some things are more negotiable than others. . 1. Use all lower-case (yes, even for file formats).; 2. Use only dash (`-`) as separator, no underscores (because lots of newbies struggle to differentiate the two, and underscores take more effort to type than dashes).; 3. Separate words rather than smushing them together, eg use `--do-this-thing` rather than `--dothisthing` (this is really important for readability, especially for non-native English speakers).; 4. Avoid cryptic abbreviations and acronyms; eg use `--do-this-thing` rather than `--dtt`; 5. If you end up with `--really-long-argument-names-that-take-up-half-a-line`, please reach out and ask for a consult; maybe we can find a more succinct way of expressing what you need.; 6. If you run into any situation not covered above, please bring it up in this thread.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-346190915:823,Avoid,Avoid,823,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-346190915,1,['Avoid'],['Avoid']
Safety,"#### Hiding / deprecating tools and their docs. @samuelklee To add to @sooheelee's answer, if there are any tools that you definitely want gone and already have a replacement for, I would encourage you to kill them off (ie delete from the code) before the 4.0 launch. While we're still in beta we can remove anything at the drop of a hat. Once 4.0 is out, we'll have a deprecation policy (exact details TBD) that will allow us to prune unwanted tools over time, but it will be less trivial. And as Soo Hee said, everything that's in the current code release MUST be documented. We used to hide tools/docs in the past and it caused us more headaches than not. . That being said, as part of that TBD deprecation policy it will probably make sense to make a ""Deprecated"" program group where tools go to die. If there are tools you plan to kill but don't want to do it before 4.0 is released for whatever reason, you could put them there. Documentation standards can be less stringent for tools in that bucket. To be clear I think the deprecation group name should be generic, ie not named to match any particular use case or functionality. That will help us avoid seeing deprecation buckets proliferate for each variant class/ use case. Does that sound like a reasonable compromise?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-346189138:1155,avoid,avoid,1155,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-346189138,1,['avoid'],['avoid']
Safety,"#4093 detects ambiguities, but throws when it finds them. Reopening this to keep the history, since we should still probably invent some kind of quoting mechanism to allow the user to resolve ambiguities.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1438#issuecomment-356630862:6,detect,detects,6,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1438#issuecomment-356630862,1,['detect'],['detects']
Safety,$$anonfun$apply$mcJ$sp$2.apply; 0.0% 2 + 0 java.util.IdentityHashMap.resize; 0.0% 0 + 1 java.lang.Runtime.availableProcessors; 0.0% 0 + 1 java.io.FileInputStream.available; 0.0% 0 + 1 sun.management.GarbageCollectorImpl.getCollectionTime; 0.0% 0 + 1 java.lang.Thread.setPriority0; 0.0% 0 + 1 sun.nio.ch.FileDispatcherImpl.read0; 0.0% 0 + 1 sun.nio.ch.Net.socket0; 0.0% 1 + 0 org.seqdoop.hadoop_bam.SAMRecordWritable.set; 0.0% 1 + 0 org.apache.spark.util.collection.SizeTrackingAppendOnlyMap.org$apache$spark$util$collection$SizeTracker$$numUpdates; 0.0% 1 + 0 java.util.IdentityHashMap$Values.toArray; 0.0% 1 + 0 io.netty.buffer.AbstractByteBufAllocator.heapBuffer; 0.0% 1 + 0 sun.net.www.protocol.http.Handler.openConnection; 1.0% 106 + 55 Total interpreted (including elided). Compiled + native Method ; 13.7% 2116 + 2 com.ning.compress.lzf.impl.UnsafeChunkEncoderLE.tryCompress; 2.7% 409 + 0 htsjdk.samtools.util.BlockCompressedOutputStream.write; 2.4% 373 + 2 com.ning.compress.lzf.impl.UnsafeChunkDecoder.decodeChunk; 1.4% 214 + 0 htsjdk.samtools.SAMUtils.bytesToCompressedBases; 1.4% 211 + 0 htsjdk.samtools.BinaryTagCodec.readTags; 1.0% 147 + 3 htsjdk.samtools.BAMRecordCodec.encode; 0.7% 108 + 0 org.broadinstitute.hellbender.relocated.com.google.common.collect.ImmutableMultimap$Builder.build; 0.6% 99 + 0 java.util.Iterator.forEachRemaining; 0.6% 84 + 2 org.apache.spark.util.collection.TimSort$SortState.mergeLo; 0.5% 82 + 0 org.broadinstitute.hellbender.relocated.com.google.common.collect.AbstractMapBasedMultimap.put; 0.5% 73 + 2 scala.collection.Iterator$$anon$13.hasNext; 0.4% 68 + 0 htsjdk.samtools.BinaryTagCodec.readSingleValue; 0.4% 66 + 0 org.broadinstitute.hellbender.utils.read.markduplicates.OpticalDuplicateFinder.getRapidDefaultReadNameRegexSplit; 0.4% 63 + 1 java.util.stream.ReferencePipeline.collect; 0.4% 61 + 0 org.broadinstitute.hellbender.relocated.com.google.common.collect.Multimaps.index; 0.4% 59 + 1 org.apache.spark.util.collection.TimSort$SortState.mergeHi; 0.3%,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1702#issuecomment-210127581:7397,Unsafe,UnsafeChunkDecoder,7397,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1702#issuecomment-210127581,1,['Unsafe'],['UnsafeChunkDecoder']
Safety,% 0 + 6 java.io.FileOutputStream.close0; 73.9% 16 + 11236 Total stub (including elided). Thread-local ticks:; 60.2% 23027 Blocked (of total); 0.0% 1 Class loader; 0.0% 1 Unknown: thread_state; ```. and on igzip:. ```; Flat profile of 425.43 secs (38916 total ticks): Executor task launch worker-4. Interpreted + native Method ; 0.1% 0 + 23 java.net.Inet6AddressImpl.lookupAllHostAddr; 0.1% 0 + 16 java.io.UnixFileSystem.delete0; 0.1% 14 + 0 org.apache.spark.util.collection.TimSort.sort; 0.1% 10 + 0 org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1$$anonfun$12$$anonfun$apply$4.apply$mcV$sp; 0.1% 10 + 0 htsjdk.samtools.BAMRecordCodec.encode; 0.0% 0 + 5 java.net.SocketInputStream.socketRead0; 0.0% 5 + 0 org.apache.spark.util.collection.TimSort$SortState.mergeHi; 0.0% 0 + 3 java.net.Inet6AddressImpl.getHostByAddr; 0.0% 3 + 0 org.apache.spark.util.collection.ExternalSorter.insertAll; 0.0% 0 + 2 htsjdk.samtools.util.zip.IntelDeflater.deflateBytes; 0.0% 2 + 0 sun.misc.Unsafe.defineClass; 0.0% 2 + 0 sun.reflect.MethodAccessorGenerator.emitInvoke; 0.0% 2 + 0 org.apache.spark.deploy.SparkHadoopUtil$$anonfun$2$$anonfun$apply$mcJ$sp$2.apply; 0.0% 2 + 0 java.util.IdentityHashMap.resize; 0.0% 0 + 1 java.lang.Runtime.availableProcessors; 0.0% 0 + 1 java.io.FileInputStream.available; 0.0% 0 + 1 sun.management.GarbageCollectorImpl.getCollectionTime; 0.0% 0 + 1 java.lang.Thread.setPriority0; 0.0% 0 + 1 sun.nio.ch.FileDispatcherImpl.read0; 0.0% 0 + 1 sun.nio.ch.Net.socket0; 0.0% 1 + 0 org.seqdoop.hadoop_bam.SAMRecordWritable.set; 0.0% 1 + 0 org.apache.spark.util.collection.SizeTrackingAppendOnlyMap.org$apache$spark$util$collection$SizeTracker$$numUpdates; 0.0% 1 + 0 java.util.IdentityHashMap$Values.toArray; 0.0% 1 + 0 io.netty.buffer.AbstractByteBufAllocator.heapBuffer; 0.0% 1 + 0 sun.net.www.protocol.http.Handler.openConnection; 1.0% 106 + 55 Total interpreted (including elided). Compiled + native Method ; 13.7% 2116 + 2 com.ning.compress.lzf.impl.UnsafeChunkEncod,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1702#issuecomment-210127581:6266,Unsafe,Unsafe,6266,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1702#issuecomment-210127581,1,['Unsafe'],['Unsafe']
Safety,) will **not change** coverage.; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #2809 +/- ##; ===========================================; Coverage 79.973% 79.973% ; Complexity 16726 16726 ; ===========================================; Files 1139 1139 ; Lines 60894 60894 ; Branches 9436 9436 ; ===========================================; Hits 48699 48699 ; Misses 8399 8399 ; Partials 3796 3796; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2809?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...ools/archive/CalculatePulldownPhasePosteriors.java](https://codecov.io/gh/broadinstitute/gatk/pull/2809?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9hcmNoaXZlL0NhbGN1bGF0ZVB1bGxkb3duUGhhc2VQb3N0ZXJpb3JzLmphdmE=) | `85.366% <ø> (ø)` | `8 <0> (?)` | |; | [...ellbender/tools/archive/CoverageDropoutResult.java](https://codecov.io/gh/broadinstitute/gatk/pull/2809?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9hcmNoaXZlL0NvdmVyYWdlRHJvcG91dFJlc3VsdC5qYXZh) | `92.593% <ø> (ø)` | `17 <0> (?)` | |; | [...lbender/tools/archive/CoverageDropoutDetector.java](https://codecov.io/gh/broadinstitute/gatk/pull/2809?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9hcmNoaXZlL0NvdmVyYWdlRHJvcG91dERldGVjdG9yLmphdmE=) | `91.803% <ø> (ø)` | `20 <0> (?)` | |; | [...ellbender/tools/archive/DetectCoverageDropout.java](https://codecov.io/gh/broadinstitute/gatk/pull/2809?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9hcmNoaXZlL0RldGVjdENvdmVyYWdlRHJvcG91dC5qYXZh) | `84% <ø> (ø)` | `4 <0> (?)` | |; | [...lbender/tools/archive/DecomposeSingularValues.java](https://codecov.io/gh/broadinstitute/gatk/pull/2809?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9hcmNoaXZlL0RlY29tcG9zZVNpbmd1bGFyVmFsdWVzLmphdmE=) | `89.474% <ø> (ø)` | `5 <0> (?)` | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2809#issuecomment-306007372:1756,Detect,DetectCoverageDropout,1756,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2809#issuecomment-306007372,1,['Detect'],['DetectCoverageDropout']
Safety,"* For `PipelineOptions`, my understanding is that it is used for Google genomics API, and we seem to use it very infrequently in GATK (and never in SV), so it is safe to use null whenever engine level or other utility functions API needs it; * For the `END` and `START` annotation, there is NO`START` in VCF spec, but `POS`, so I don't know where the `start` comes from. And yes, I agree that BND records don't have a `start` either, it is merely a novel adjacency between two genomic locations, none of which is a start or end.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3476#issuecomment-325030125:162,safe,safe,162,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3476#issuecomment-325030125,1,['safe'],['safe']
Safety,"* _Large number of open file handles_: this was an issue in TileDB which got fixed as part of the restructuring that @nalinigans did for supporting HDFS/S3/GCS (#5017). I was too lazy to fix this again. If it's going to take some time for PR #5017 to be merged, I can submit a separate fix for this. This would fix any crashes/termination issues.; * _Performance of a single import process with a large number of intervals_; * Restating the obvious, but this is a single process (and by default, a single thread) with many intervals to import. As you increase the number of samples, this will become a performance pain point.; * More important than the number of intervals is the amount of data imported per interval. Each interval import involves opening the VCF files (loading index structures while creating FeatureReader objects), writing to TileDB/GenomicsDB. and closing the VCF file handles (destroying FeatureReader objects). If the amount of data written for each interval is sufficiently large, the cost of opening/closing the VCF files (creating/destroying FeatureReaders) is small relative to the total time taken.; * In the test cases I and Chris were trying, the amount of data written per interval was small (or 0 in many cases). The time taken in opening/closing the VCF files (and loading/destroying the index) dominates the total time.; * For a single import process (single thread), creating a large interval is better (or no worse) than passing several small intervals. TileDB/GenomicsDB has 0 overhead for regions with no data (for example, WES gVCFs). Having larger intervals will likely avoid issues described above. Hence, an advisory message will be beneficial.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5066#issuecomment-410576757:1610,avoid,avoid,1610,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5066#issuecomment-410576757,1,['avoid'],['avoid']
Safety,"**Update:**. Here's how the coverage looks like using `CollectReadCounts` (w/ and w/o MQ > 30 filter) vs. `CollectFragmentCounts`. The lines are offset by +10 and +20 for better visibility. Summary: marked improvement in all cases, however, the error modes are different. `CollectFragmentCounts` tends to underestimate the size of SV regions and uniformly leads to coverage depletion near the breakpoints, `CollectReadCounts` estimates the size of SV regions better, however, coverage near the breakpoints tend to be less predictable (sometimes depletion, sometimes accumulation). Still, IGV seems to do the best job. Any improvement over `CollectReadCounts` requires using supplementary alignment information (e.g. weight sharing among supplementary alignments; this will likely fix the coverage asymmetry of translocation breakpoints), read clipping information, and mismatches. The latter two require a base-level coverage collection strategy (like IGV and `CollectTargetBaseCallCoverage`). _Unbalanced translocation:_. ![unbtr-1](https://user-images.githubusercontent.com/15305869/37840319-1aba29ce-2e93-11e8-9d41-b9eafe450b6d.png). ![unbtr-2](https://user-images.githubusercontent.com/15305869/37840320-1bfff9a8-2e93-11e8-9842-39824f9fad64.png). ![unbtr-3](https://user-images.githubusercontent.com/15305869/37840321-1d82fa00-2e93-11e8-88ec-d7c40876594e.png). _Balanced translocation:_. ![baltr-1](https://user-images.githubusercontent.com/15305869/37840331-26a0962e-2e93-11e8-8dcf-0e69c8e45146.png). _Inversion:_. ![inv-1](https://user-images.githubusercontent.com/15305869/37840347-2f0d2f8e-2e93-11e8-8d36-64367951e7f2.png). ![inv-2](https://user-images.githubusercontent.com/15305869/37840350-306dedbe-2e93-11e8-9837-53369f5fb1f0.png). _Deletion:_. ![del-1](https://user-images.githubusercontent.com/15305869/37840366-3a32c0ea-2e93-11e8-99dc-d949985616d9.png). _Tandem Duplication:_. ![dup-1](https://user-images.githubusercontent.com/15305869/37840373-42052542-2e93-11e8-8891-fa9f79cc9f70.png",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4551#issuecomment-375720743:522,predict,predictable,522,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4551#issuecomment-375720743,1,['predict'],['predictable']
Safety,"*Complete log**: . ```; Using GATK jar /software/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx8G -jar /software/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar DetermineGermlineContigPloidy -L results/cnv/targets.preprocessed.interval_list -I results/cnv/hdf5/MGM20-0848_S4.hdf5 -I results/cnv/hdf5/MGM20-0872_S2.hdf5 -I results/cnv/hdf5/MGM20-1121_S4.hdf5 -I results/cnv/hdf5/MGM20-1543_S10.hdf5 --contig-ploidy-priors resources/contig_ploidy_priors.tsv --output-prefix ploidy -imr OVERLAPPING_ONLY -O results/cnv/ploidy; 15:09:27.326 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/software/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Oct 18, 2021 3:09:27 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 15:09:27.686 INFO DetermineGermlineContigPloidy - ------------------------------------------------------------; 15:09:27.686 INFO DetermineGermlineContigPloidy - The Genome Analysis Toolkit (GATK) v4.2.0.0; 15:09:27.687 INFO DetermineGermlineContigPloidy - For support and documentation go to https://software.broadinstitute.org/gatk/; 15:09:27.687 INFO DetermineGermlineContigPloidy - Executing as n.liorni@hpc001 on Linux v3.10.0-1127.el7.x86_64 amd64; 15:09:27.687 INFO DetermineGermlineContigPloidy - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_302-b08; 15:09:27.687 INFO DetermineGermlineContigPloidy - Start Date/Time: 18 ottobre 2021 15.09.27 CEST; 15:09:27.688 INFO DetermineGermlineContigPloidy - ------------------------------------------------------------; 15:09:27.688 INFO DetermineGermlineContigPloidy - ------------------------------------------------------------; 15:09:27.689 INFO DetermineGermlineContigPloidy - HTSJDK",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7444#issuecomment-945753905:1575,detect,detect,1575,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7444#issuecomment-945753905,1,['detect'],['detect']
Safety,"-29 15:14:32.662 INFO 12904 --- [ main] o.s.b.w.embedded.tomcat.TomcatWebServer : Tomcat initialized with port(s): 8282 (http); 2020-05-29 15:14:32.675 INFO 12904 --- [ main] o.a.coyote.http11.Http11NioProtocol : Initializing ProtocolHandler [""http-nio-8282""]; 2020-05-29 15:14:32.676 INFO 12904 --- [ main] o.apache.catalina.core.StandardService : Starting service [Tomcat]; 2020-05-29 15:14:32.677 INFO 12904 --- [ main] org.apache.catalina.core.StandardEngine : Starting Servlet engine: [Apache Tomcat/9.0.35]; 2020-05-29 15:14:32.802 INFO 12904 --- [ main] o.a.c.c.C.[Tomcat].[localhost].[/] : Initializing Spring embedded WebApplicationContext; 2020-05-29 15:14:32.802 INFO 12904 --- [ main] o.s.web.context.ContextLoader : Root WebApplicationContext: initialization completed in 1944 ms; 2020-05-29 15:14:32.899 INFO 12904 --- [ main] com.luz.push.utils.GcmUtils : start init gcm server; 2020-05-29 15:14:33.029 WARN 12904 --- [ main] c.g.a.oauth2.ComputeEngineCredentials : Failed to detect whether we are running on Google Compute Engine. java.net.SocketException: Network is unreachable: connect; 	at java.net.DualStackPlainSocketImpl.waitForConnect(Native Method); 	at java.net.DualStackPlainSocketImpl.socketConnect(DualStackPlainSocketImpl.java:85); 	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350); 	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206); 	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188); 	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172); 	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392); 	at java.net.Socket.connect(Socket.java:589); 	at sun.net.NetworkClient.doConnect(NetworkClient.java:175); 	at sun.net.www.http.HttpClient.openServer(HttpClient.java:432); 	at sun.net.www.http.HttpClient.openServer(HttpClient.java:527); 	at sun.net.www.http.HttpClient.<init>(HttpClient.java:211); 	at sun.net.www.http.HttpClient.New(HttpClient.java:308); 	at",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-635805233:12706,detect,detect,12706,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-635805233,1,['detect'],['detect']
Safety,-disqualification 0; .02 --pair-hmm-implementation FASTEST_AVAILABLE --pcr-indel-model CONSERVATIVE --phred-scaled-global-read-mismapping-rate 45 --disable-symmetric-hmm-normalizing false --disable-cap-base-qu; alities-to-map-quality false --enable-dynamic-read-disqualification-for-genotyping false --dynamic-read-disqualification-threshold 1.0 --native-pair-hmm-threads 4 --native-pair-hmm-use-dou; ble-precision false --flow-hmm-engine-min-indel-adjust 6 --flow-hmm-engine-flat-insertion-penatly 45 --flow-hmm-engine-flat-deletion-penatly 45 --pileup-detection false --pileup-detection-; enable-indel-pileup-calling false --num-artificial-haplotypes-to-add-per-allele 5 --artifical-haplotype-filtering-kmer-size 10 --pileup-detection-snp-alt-threshold 0.1 --pileup-detection-i; ndel-alt-threshold 0.5 --pileup-detection-absolute-alt-depth 0.0 --pileup-detection-snp-adjacent-to-assembled-indel-range 5 --pileup-detection-bad-read-tolerance 0.0 --pileup-detection-pro; per-pair-read-badness true --pileup-detection-edit-distance-read-badness-threshold 0.08 --pileup-detection-chimeric-read-badness true --pileup-detection-template-mean-badness-threshold 0.0; --pileup-detection-template-std-badness-threshold 0.0 --bam-writer-type CALLED_HAPLOTYPES --dont-use-soft-clipped-bases false --override-fragment-softclip-check false --min-base-quality-s; core 10 --smith-waterman JAVA --max-mnp-distance 0 --force-call-filtered-alleles false --reference-model-deletion-quality 30 --soft-clip-low-quality-ends false --allele-informative-reads-o; verlap-margin 2 --smith-waterman-dangling-end-match-value 25 --smith-waterman-dangling-end-mismatch-penalty -50 --smith-waterman-dangling-end-gap-open-penalty -110 --smith-waterman-danglin; g-end-gap-extend-penalty -6 --smith-waterman-haplotype-to-reference-match-value 200 --smith-waterman-haplotype-to-reference-mismatch-penalty -150 --smith-waterman-haplotype-to-reference-ga; p-open-penalty -260 --smith-waterman-haplotype-to-reference-gap-extend-penalty -1,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8574#issuecomment-1793390789:6965,detect,detection-pro,6965,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8574#issuecomment-1793390789,2,['detect'],"['detection-edit-distance-read-badness-threshold', 'detection-pro']"
Safety,-scaled-global-read-mismapping-rate 45 --disable-symmetric-hmm-normalizing false --disable-cap-base-qu; alities-to-map-quality false --enable-dynamic-read-disqualification-for-genotyping false --dynamic-read-disqualification-threshold 1.0 --native-pair-hmm-threads 4 --native-pair-hmm-use-dou; ble-precision false --flow-hmm-engine-min-indel-adjust 6 --flow-hmm-engine-flat-insertion-penatly 45 --flow-hmm-engine-flat-deletion-penatly 45 --pileup-detection false --pileup-detection-; enable-indel-pileup-calling false --num-artificial-haplotypes-to-add-per-allele 5 --artifical-haplotype-filtering-kmer-size 10 --pileup-detection-snp-alt-threshold 0.1 --pileup-detection-i; ndel-alt-threshold 0.5 --pileup-detection-absolute-alt-depth 0.0 --pileup-detection-snp-adjacent-to-assembled-indel-range 5 --pileup-detection-bad-read-tolerance 0.0 --pileup-detection-pro; per-pair-read-badness true --pileup-detection-edit-distance-read-badness-threshold 0.08 --pileup-detection-chimeric-read-badness true --pileup-detection-template-mean-badness-threshold 0.0; --pileup-detection-template-std-badness-threshold 0.0 --bam-writer-type CALLED_HAPLOTYPES --dont-use-soft-clipped-bases false --override-fragment-softclip-check false --min-base-quality-s; core 10 --smith-waterman JAVA --max-mnp-distance 0 --force-call-filtered-alleles false --reference-model-deletion-quality 30 --soft-clip-low-quality-ends false --allele-informative-reads-o; verlap-margin 2 --smith-waterman-dangling-end-match-value 25 --smith-waterman-dangling-end-mismatch-penalty -50 --smith-waterman-dangling-end-gap-open-penalty -110 --smith-waterman-danglin; g-end-gap-extend-penalty -6 --smith-waterman-haplotype-to-reference-match-value 200 --smith-waterman-haplotype-to-reference-mismatch-penalty -150 --smith-waterman-haplotype-to-reference-ga; p-open-penalty -260 --smith-waterman-haplotype-to-reference-gap-extend-penalty -11 --smith-waterman-read-to-haplotype-match-value 10 --smith-waterman-read-to-haplotype-mismatch-penalty -1,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8574#issuecomment-1793390789:7077,detect,detection-chimeric-read-badness,7077,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8574#issuecomment-1793390789,2,['detect'],"['detection-chimeric-read-badness', 'detection-template-mean-badness-threshold']"
Safety,"... so, is it safe to assume that you agree that #867 is good to merge?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/866#issuecomment-135537764:14,safe,safe,14,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/866#issuecomment-135537764,1,['safe'],['safe']
Safety,"...` yields the following snippet:. ```; Gradle suite > Gradle test > org.broadinstitute.hellbender.utils.downsampling.ReservoirDownsamplerUnitTest > testReservoirDownsampler[29](TestDataProvider(ReservoirDownsamplerTest: reservoirSize=10000 totalReads=10000 expectedNumReadsAfterDownsampling=10000 expectedNumDiscardedItems=0)) STANDARD_ERROR; 01:40:10.641 WARN gatk - Running test: TestDataProvider(ReservoirDownsamplerTest: reservoirSize=10000 totalReads=10000 expectedNumReadsAfterDownsampling=10000 expectedNumDiscardedItems=0); Finished 130000 tests; Finished 140000 tests. Gradle suite > Gradle test > org.broadinstitute.hellbender.utils.pairhmm.VectorPairHMMUnitTest STANDARD_ERROR; 01:40:14.522 WARN NativeLibraryLoader - Unable to load libgkl_pairhmm_fpga.so from native/libgkl_pairhmm_fpga.so (/tmp/libgkl_pairhmm_fpga17703278887667828152.so: libgkl_pairhmm_shacc.so: cannot open shared object file: No such file or directory); #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fe1a5cd00f2, pid=6969, tid=6997; #; # JRE version: OpenJDK Runtime Environment (11.0.2+9) (build 11.0.2+9); # Java VM: OpenJDK 64-Bit Server VM (11.0.2+9, mixed mode, tiered, compressed oops, g1 gc, linux-amd64); # Problematic frame:; # V [libjvm.so+0x8fd0f2] jni_GetByteArrayElements+0x72; #; # Core dump will be written. Default location: Core dumps may be processed with ""/usr/share/apport/apport %p %s %c %d %P"" (or dumping to /home/travis/build/broadinstitute/gatk/core.6969); #; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid6969.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; #; Starting process 'Gradle Test Executor 2'. Working directory: /home/travis/build/broadinstitute/gatk Command: /usr/local/lib/jvm/openjdk11/bin/java -Dgatk.spark.debug -Dorg.gradle.native=false -Dsamjdk.compression_level=2 -Dsamjdk.use_asyn",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607332088:998,detect,detected,998,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607332088,1,['detect'],['detected']
Safety,.0% 2 + 0 java.nio.HeapIntBuffer.<init>; 0.0% 2 + 0 sun.misc.Unsafe.defineClass; 0.0% 0 + 2 sun.misc.Unsafe.copyMemory; 0.0% 0 + 2 org.apache.hadoop.util.NativeCrc32.nativeComputeChunkedSums; 0.0% 2 + 0 java.text.DateFormatSymbols.getProviderInstance; 0.0% 2 + 0 org.broadinstitute.hellbender.utils.baq.BAQ$BAQCalculationResult.<init>; 0.0% 2 + 0 org.broadinstitute.hellbender.utils.read.markduplicates.ReadsKey.subkeyForFragment; 0.0% 2 + 0 htsjdk.samtools.BAMRecord.decodeBaseQualities; 0.8% 396 + 260 Total interpreted (including elided). Compiled + native Method ; 13.4% 10629 + 23 org.bdgenomics.adam.util.TwoBitFile$$anonfun$2.apply$mcZJ$sp; 8.6% 21 + 6794 org.broadinstitute.hellbender.utils.baq.BAQ.hmm_glocal; 5.7% 4492 + 0 org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine.updateRecalTablesForRead; 4.1% 3246 + 0 scala.collection.AbstractTraversable.genericBuilder; 2.4% 1932 + 0 scala.collection.AbstractSeq.size; 2.3% 1841 + 2 com.ning.compress.lzf.impl.UnsafeChunkEncoderLE.tryCompress; 2.0% 1560 + 0 org.broadinstitute.hellbender.transformers.BQSRReadTransformer.apply; 1.8% 1407 + 0 org.broadinstitute.hellbender.utils.collections.IntervalsSkipListOneContig.getOverlapping; 1.3% 1034 + 0 scala.collection.mutable.ArrayBuilder.sizeHint; 1.1% 898 + 0 com.ning.compress.lzf.impl.UnsafeChunkDecoder.decodeChunk; 1.1% 893 + 4 scala.collection.mutable.ArrayBuilder$.make; 1.1% 881 + 2 org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine.processRead; 1.0% 758 + 0 org.broadinstitute.hellbender.utils.recalibration.RecalUtils.computeCovariates; 0.9% 698 + 0 htsjdk.samtools.BinaryTagCodec.readTags; 0.7% 573 + 0 htsjdk.samtools.util.BlockCompressedOutputStream.write; 0.7% 535 + 0 org.broadinstitute.hellbender.utils.recalibration.covariates.ContextCovariate.recordValues; 0.6% 506 + 1 htsjdk.samtools.BinaryTagCodec.readSingleValue; 0.6% 459 + 1 scala.collection.Iterator$$anon$13.hasNext; 0.5% 400 + 0 org.broadinstitute.hellbender.relocated.com.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1657#issuecomment-208967490:2467,Unsafe,UnsafeChunkEncoderLE,2467,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1657#issuecomment-208967490,1,['Unsafe'],['UnsafeChunkEncoderLE']
Safety,.ExecuteActionsTaskExecuter.access$200(ExecuteActionsTaskExecuter.java:93); at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter$TaskExecution.execute(ExecuteActionsTaskExecuter.java:237); at org.gradle.internal.execution.steps.ExecuteStep.lambda$execute$1(ExecuteStep.java:33); at org.gradle.internal.execution.steps.ExecuteStep.execute(ExecuteStep.java:33); at org.gradle.internal.execution.steps.ExecuteStep.execute(ExecuteStep.java:26); at org.gradle.internal.execution.steps.CleanupOutputsStep.execute(CleanupOutputsStep.java:58); at org.gradle.internal.execution.steps.CleanupOutputsStep.execute(CleanupOutputsStep.java:35); at org.gradle.internal.execution.steps.ResolveInputChangesStep.execute(ResolveInputChangesStep.java:48); at org.gradle.internal.execution.steps.ResolveInputChangesStep.execute(ResolveInputChangesStep.java:33); at org.gradle.internal.execution.steps.CancelExecutionStep.execute(CancelExecutionStep.java:39); at org.gradle.internal.execution.steps.TimeoutStep.executeWithoutTimeout(TimeoutStep.java:73); at org.gradle.internal.execution.steps.TimeoutStep.execute(TimeoutStep.java:54); at org.gradle.internal.execution.steps.CatchExceptionStep.execute(CatchExceptionStep.java:35); at org.gradle.internal.execution.steps.CreateOutputsStep.execute(CreateOutputsStep.java:51); at org.gradle.internal.execution.steps.SnapshotOutputsStep.execute(SnapshotOutputsStep.java:45); at org.gradle.internal.execution.steps.SnapshotOutputsStep.execute(SnapshotOutputsStep.java:31); at org.gradle.internal.execution.steps.CacheStep.executeWithoutCache(CacheStep.java:208); at org.gradle.internal.execution.steps.CacheStep.execute(CacheStep.java:70); at org.gradle.internal.execution.steps.CacheStep.execute(CacheStep.java:45); at org.gradle.internal.execution.steps.BroadcastChangingOutputsStep.execute(BroadcastChangingOutputsStep.java:49); at org.gradle.internal.execution.steps.StoreSnapshotsStep.execute(StoreSnapshotsStep.java:43); at org.gradle.internal.execution.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6466#issuecomment-590387973:8517,Timeout,TimeoutStep,8517,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6466#issuecomment-590387973,2,['Timeout'],['TimeoutStep']
Safety,".Main.main(Main.java:292); ```. ## Cases when the error does not occur; * If I rename `test a` folder in `test-a` as previously said.; * If I copy my current `test a` in the `/tmp/` directory (`/tmp/test a/`). This may suggest that the path length plays a role.; * If I renamed the VCF files (first VCF becomes `a.vcf.gz`, second `b.vcf.gz`) (`gatk MergeVcfs -I data/calling/a.vcf.gz -I data/calling/b.vcf.gz -O out.vcf.gz`).; * If I rename the first VCF file with as many `a` character as characters found in the original filename. (aaaaaaaaaaaaaaaaaa.vcf.gz).; * If I rename the first VCF by replacing all alphabetical character with a (aaaa_aaaa2.aa_a7_1.vcf.gz); * If I introduce random `_` in the file name (aaaa_aaa_aaaa_aaaa.vcf.gz).; * If I rename the first VCF file by removing the first character (`cerc_prod2.SM_V7_1.vcf.gz` -> `erc_prod2.SM_V7_1.vcf.gz`); * If I rename the first VCF file by introducing a letter at the beginning (`cerc_prod2.SM_V7_1.vcf.gz` -> `ccerc_prod2.SM_V7_1.vcf.gz`). It really seems that the combination of the path lengh, white space and particular filename triggers this. I cannot get my head around this. I don't think this is coming from the content of the VCF as it works well in some cases. Let me know if you need me to make other tests. Fred. ----. ## Update. I investigated a little further after thinking about the tests I did. Because modifying the VCF filename did not trigger the issue and because of the presence of `tabix` related modules in the traces, I decided to see if removing `tbi` file will avoid having the error message. And it did!. After recreating the `tbi` file (`tabix data/calling/cerc_prod2.SM_V7_1.vcf.gz`), the error message appeared again. So it does not seem related to malformed index file. However, index file seems part of the problem. After renaming `test a` folder in `test-a` with the old or new index file, I did not get any error (as usual). Here is my tabix version in case:; ```bash; $ tabix -h. Version: 1.10.2; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6664#issuecomment-647808241:8607,avoid,avoid,8607,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6664#issuecomment-647808241,1,['avoid'],['avoid']
Safety,".apply(PairRDDFunctions.scala:979); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61); at org.apache.spark.scheduler.Task.run(Task.scala:64); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 15/07/14 13:14:53 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 1.0 (TID 1, localhost): java.lang.IncompatibleClassChangeError: Found class org.apache.hadoop.mapreduce.TaskAttemptContext, but interface was expected; at com.cloudera.dataflow.spark.TemplatedTextOutputFormat.getOutputFile(TemplatedTextOutputFormat.java:50); at com.cloudera.dataflow.spark.TemplatedTextOutputFormat.getDefaultWorkFile(TemplatedTextOutputFormat.java:46); at org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:125); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$12.apply(PairRDDFunctions.scala:995); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$12.apply(PairRDDFunctions.scala:979); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61); at org.apache.spark.scheduler.Task.run(Task.scala:64); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 15/07/14 13:14:53 ERROR scheduler.TaskSetManager: Task 0 in stage 1.0 failed 1 times; aborting job; 15/07/14 13:14:53 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool; 15/07/14 13:14:53 INFO scheduler.TaskSchedulerImpl: Cancelling stage 1; 15/07/14 13:14:53 INFO scheduler.DAGScheduler: Stage 1 (saveAsNewAPIHadoopFile at TransformTranslator.java:432) failed in 0.155 s; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/639#issuecomment-121313713:31990,abort,aborting,31990,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/639#issuecomment-121313713,1,['abort'],['aborting']
Safety,.broadinstitute.hellbender.utils.baq.BAQ.hmm_glocal; 0.0% 0 + 28 java.net.SocketInputStream.socketRead0; 0.0% 22 + 0 org.apache.spark.util.collection.TimSort.sort; 0.0% 15 + 0 java.util.Iterator.forEachRemaining; 0.0% 14 + 0 org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1$$anonfun$12$$anonfun$apply$4.apply$mcV$sp; 0.0% 13 + 0 com.esotericsoftware.kryo.serializers.FieldSerializer$ObjectField.read; 0.0% 6 + 0 org.broadinstitute.hellbender.utils.baq.BAQ.<init>; 0.0% 4 + 0 org.broadinstitute.hellbender.utils.recalibration.RecalUtils.combineTables; 0.0% 0 + 3 java.io.UnixFileSystem.getLength; 0.0% 3 + 0 org.apache.hadoop.hdfs.DFSOutputStream.waitAndQueueCurrentPacket; 0.0% 3 + 0 org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine.processRead; 0.0% 0 + 2 java.io.UnixFileSystem.createDirectory; 0.0% 1 + 1 java.lang.Class.getDeclaredFields0; 0.0% 2 + 0 java.nio.HeapIntBuffer.<init>; 0.0% 2 + 0 sun.misc.Unsafe.defineClass; 0.0% 0 + 2 sun.misc.Unsafe.copyMemory; 0.0% 0 + 2 org.apache.hadoop.util.NativeCrc32.nativeComputeChunkedSums; 0.0% 2 + 0 java.text.DateFormatSymbols.getProviderInstance; 0.0% 2 + 0 org.broadinstitute.hellbender.utils.baq.BAQ$BAQCalculationResult.<init>; 0.0% 2 + 0 org.broadinstitute.hellbender.utils.read.markduplicates.ReadsKey.subkeyForFragment; 0.0% 2 + 0 htsjdk.samtools.BAMRecord.decodeBaseQualities; 0.8% 396 + 260 Total interpreted (including elided). Compiled + native Method ; 13.4% 10629 + 23 org.bdgenomics.adam.util.TwoBitFile$$anonfun$2.apply$mcZJ$sp; 8.6% 21 + 6794 org.broadinstitute.hellbender.utils.baq.BAQ.hmm_glocal; 5.7% 4492 + 0 org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine.updateRecalTablesForRead; 4.1% 3246 + 0 scala.collection.AbstractTraversable.genericBuilder; 2.4% 1932 + 0 scala.collection.AbstractSeq.size; 2.3% 1841 + 2 com.ning.compress.lzf.impl.UnsafeChunkEncoderLE.tryCompress; 2.0% 1560 + 0 org.broadinstitute.hellbender.transformers.BQSRReadTransformer.appl,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1657#issuecomment-208967490:1578,Unsafe,Unsafe,1578,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1657#issuecomment-208967490,1,['Unsafe'],['Unsafe']
Safety,".local on Linux v3.10.0-1062.4.1.el7.x86_64 amd64; INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_232-b09; INFO GenotypeGVCFs - Start Date/Time: January 14, 2020 1:53:14 PM BRT; INFO GenotypeGVCFs - ------------------------------------------------------------; INFO GenotypeGVCFs - ------------------------------------------------------------; INFO GenotypeGVCFs - HTSJDK Version: 2.21.0; INFO GenotypeGVCFs - Picard Version: 2.21.2; INFO GenotypeGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL : 2; INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; INFO GenotypeGVCFs - Deflater: IntelDeflater; INFO GenotypeGVCFs - Inflater: IntelInflater; INFO GenotypeGVCFs - GCS max retries/reopens: 20; INFO GenotypeGVCFs - Requester pays: disabled; INFO GenotypeGVCFs - Initializing engine; ```. Run starts, a few variants are detected. Then It gets stuck in a region for about 30 minutes, and prints an out-of-memory error:. ```; INFO ProgressMeter - chrom2:4323711 0.3 2000 7859.6; INFO ProgressMeter - chrom2:4325583 0.6 3000 4753.5; INFO ProgressMeter - chrom2:4327262 0.8 4000 5010.5; INFO ProgressMeter - chrom2:4333146 1.1 7000 6493.1; INFO GenotypeGVCFs - Shutting down engine; ```. ```; Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space; at java.util.Arrays.copyOf(Arrays.java:3332); at java.lang.AbstractStringBuilder.ensureCapacityInternal(AbstractStringBuilder.java:124); at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:448); at java.lang.StringBuilder.append(StringBuilder.java:136); at htsjdk.tribble.util.ParsingUtils.split(ParsingUtils.java:266); at htsjdk.variant.vcf.AbstractVCFCodec.decodeLine(AbstractVCFCodec.java:375); at htsjdk.variant.vcf.AbstractVCFCodec.decode(AbstractVCFCodec.java:328); at htsjdk.variant.vcf.AbstractVCFCodec.decode(AbstractVCFCo",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6275#issuecomment-574329688:1720,detect,detected,1720,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6275#issuecomment-574329688,1,['detect'],['detected']
Safety,".pipelines.BwaAndMarkDuplicatesPipelineSpark done. Elapsed time: 269.29 minutes.; Runtime.totalMemory()=4172283904; org.apache.spark.SparkException: Job aborted due to stage failure: Task 607 in stage 3.0 failed 4 times, most recent failure: Lost task 607.13 in stage 3.0 (TID 14832, 12.9.68.0, executor 24): ExecutorLostFailure (executor 24 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 169939 ms; Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944); at org",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312758363:1613,abort,abortStage,1613,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312758363,1,['abort'],['abortStage']
Safety,"0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /nics/d/home/hchen3/bin/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar GenotypeGVCFs -R /lustre/haven/proj/UTHSC0013/Tristan_GATK/reference/genome.fa -V gendb:///lustre/haven/proj/UTHSC0013/Tristan_GATK//DB/chr7 -G StandardAnnotation --use-new-qual-calculator -O /lustre/haven/proj/UTHSC0013/Tristan_GATK//gvcf//merged//joint_called_gvcfs_chr7.vcf; 23:15:47.053 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 23:15:47.249 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/nics/d/home/hchen3/bin/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 07, 2020 11:15:49 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 23:15:49.543 INFO GenotypeGVCFs - ------------------------------------------------------------; 23:15:49.545 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.1.2.0; 23:15:49.546 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 23:15:49.547 INFO GenotypeGVCFs - Executing as hchen3@acf-knl002 on Linux v3.10.0-514.26.1.el7.x86_64 amd64; 23:15:49.548 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_131-b12; 23:15:49.548 INFO GenotypeGVCFs - Start Date/Time: January 7, 2020 11:15:47 PM EST; 23:15:49.549 INFO GenotypeGVCFs - ------------------------------------------------------------; 23:15:49.549 INFO GenotypeGVCFs - ------------------------------------------------------------; 23:15:49.551 INFO GenotypeGVCFs - HTSJDK Version: 2.19.0; 23:15:49.551 INFO GenotypeGVCFs - Picard Version: 2.19.0; 23:15:49.552 INFO GenotypeGVCFs - HTSJDK Defaults.COMPRESSIO",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6340#issuecomment-571886057:1118,detect,detect,1118,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6340#issuecomment-571886057,1,['detect'],['detect']
Safety,"01413230/document This method uses a low-rank approximation to the kernel to obtain an approximate segmentation in linear complexity in time and space. In practice, performance is actually quite impressive!. The implementation is relatively straightforward, clocking in at ~100 lines of python. Time complexity is O(log(maximum number of segments) * number of data points) and space complexity is O(number of data points * dimension of the kernel approximation), which makes use for WGS feasible. Segmentation of 10^6 simulated points with 100 segments takes about a minute and tends to recover segments accurately. Compare this with CBS, where segmentation of a WGS sample with ~700k points takes ~10 minutes---and note that these ~700k points are split up amongst ~20 chromosomes to start!. There are a small number of parameters that can affect the segmentation, but we can probably find good defaults in practice. What's also nice is that this method can find changepoints in moments of the distribution other than the mean, which means that it can straightforwardly be used for alternate-allele fraction segmentation. For example, all segments were recovered in the following simulated multimodal data, even though all of the segments have zero mean:. ![baf](https://user-images.githubusercontent.com/11076296/29100464-ad687946-7c79-11e7-99e4-962ab93709b4.png). Replacing the SNP segmentation in ACNV (which performs expensive maximum-likelihood estimation of the allele-fraction model) with this method would give a significant speedup there. Joint segmentation is straightforward and is simply given by addition of the kernels. However, complete data is still required. Given such a fast heuristic, I'm more amenable to augmenting this method with additional heuristics to clean up or improve the segmentation if necessary. We can also use it to initialize our more sophisticated HMM models, as well. @LeeTL1220 @mbabadi @davidbenjamin I'd be interested to hear your thoughts, if you have any.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-321121666:1263,recover,recovered,1263,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-321121666,1,['recover'],['recovered']
Safety,"019-01-07 11:34:12 INFO SchedulerExtensionServices:54 - Stopping SchedulerExtensionServices; (serviceOption=None,; services=List(),; started=false); 2019-01-07 11:34:12 INFO YarnClientSchedulerBackend:54 - Stopped; 2019-01-07 11:34:12 INFO MapOutputTrackerMasterEndpoint:54 - MapOutputTrackerMasterEndpoint stopped!; 2019-01-07 11:34:12 INFO MemoryStore:54 - MemoryStore cleared; 2019-01-07 11:34:12 INFO BlockManager:54 - BlockManager stopped; 2019-01-07 11:34:12 INFO BlockManagerMaster:54 - BlockManagerMaster stopped; 2019-01-07 11:34:12 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54 - OutputCommitCoordinator stopped!; 2019-01-07 11:34:12 INFO SparkContext:54 - Successfully stopped SparkContext; 11:34:12.605 INFO CountReadsSpark - Shutting down engine; [January 7, 2019 11:34:12 AM EST] org.broadinstitute.hellbender.tools.spark.pipelines.CountReadsSpark done. Elapsed time: 0.80 minutes.; Runtime.totalMemory()=1003487232; org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 4 times, most recent failure: Lost task 1.3 in stage 0.0 (TID 9, scc-q21.scc.bu.edu, executor 1): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 0, start 87545719, span 186383, expected MD5 492a29f6d7d6fcaf8dde06834861e7ae; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfu",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:36682,abort,aborted,36682,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,1,['abort'],['aborted']
Safety,"08); 	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125); 	... 18 more; 19/02/18 16:58:29 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@45c90a05{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 16:58:29.970 INFO PrintVariantsSpark - Shutting down engine; [February 18, 2019 4:58:29 PM UTC] org.broadinstitute.hellbender.tools.spark.pipelines.PrintVariantsSpark done. Elapsed time: 0.34 minutes.; Runtime.totalMemory()=1106771968; org.apache.spark.SparkException: Job aborted due to stage failure: Exception while getting task result: com.esotericsoftware.kryo.KryoException: java.lang.NullPointerException; Serialization trace:; genotypes (htsjdk.variant.variantcontext.VariantContext); interval (org.broadinstitute.hellbender.engine.spark.SparkSharder$PartitionLocatable); 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1872); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSche",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3840#issuecomment-464825765:9549,abort,abortStage,9549,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3840#issuecomment-464825765,1,['abort'],['abortStage']
Safety,"1. @nalinigans It's a very reasonable question. It's true, the --avoid-nio flag is technically redundant. You can recreate it with a combination of other flags. I added it because ; a) I didn't realize that was the when I started adding it. ; b) The combination of flags was kind of complicated so it was helpful to have something that gave you clear instructions about what you needed to enable. I think we could merge them, although I think there is one sanity check we do even when -bypass-feature-reader is turned on, that we need to turn off. I basically added ""something that works for Megan's project right now."" . 2. Yes, the various cases were getting complicated and I had a bug when -V was enabled so I just disabled it as an option. It would make sense to add -V support for azure files. I just didn't do it because I was in a rush and I figured it was better to disable it than to have it potentially be wrong. . 3. Yeah, that's the error I saw. It's definitely better than nothing. It would be great if it could be propagated back up to the java layer as a Java exception though. It currently ends the program with SIGABORT I think which doesn't play that nicely with various reporting and retry mechanisms. No super high priority, but nice if you have the cycles.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8632#issuecomment-1865021020:65,avoid,avoid-nio,65,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8632#issuecomment-1865021020,3,"['avoid', 'redund', 'sanity check']","['avoid-nio', 'redundant', 'sanity check']"
Safety,"124s --output filtered_variants/P1.avcf.gz --output-file-format VCF; > Using GATK jar /home/pkus/programs/gatk-4.1.8.0/gatk-package-4.1.8.0-local.jar; > Running:; > java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/pkus/programs/gatk-4.1.8.0/gatk-package-4.1.8.0-local.jar Funcotator --variant filtered_variants/P1.vcf.gz --reference /home/pkus/resources/hg38_for_bwa/hs38DH.fa --ref-version hg38 --data-sources-path /home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s --output filtered_variants/P1.avcf.gz --output-file-format VCF; > 12:28:16.251 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/pkus/programs/gatk-4.1.8.0/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; > Jul 21, 2020 12:28:16 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; > INFO: Failed to detect whether we are running on Google Compute Engine.; > 12:28:16.537 INFO Funcotator - ------------------------------------------------------------; > 12:28:16.538 INFO Funcotator - The Genome Analysis Toolkit (GATK) v4.1.8.0; > 12:28:16.538 INFO Funcotator - For support and documentation go to https://software.broadinstitute.org/gatk/; > 12:28:16.541 INFO Funcotator - Executing as xxx on Linux v3.10.0-123.20.1.el7.x86_64 amd64; > 12:28:16.541 INFO Funcotator - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_251-b08; > 12:28:16.542 INFO Funcotator - Start Date/Time: July 21, 2020 12:28:16 PM CEST; > 12:28:16.542 INFO Funcotator - ------------------------------------------------------------; > 12:28:16.542 INFO Funcotator - ------------------------------------------------------------; > 12:28:16.542 INFO Funcotator - HTSJDK Version: 2.22.0; > 12:28:16.543 INFO Funcotator - Picard Version: 2.22.8; > 12:28:16.543 INFO Funcotator - HTSJDK Defaults.COMPRESSION_LEVEL : 2; > 1",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6708#issuecomment-661776975:1374,detect,detect,1374,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6708#issuecomment-661776975,1,['detect'],['detect']
Safety,"2019-01-09 13:35:56 INFO SchedulerExtensionServices:54 - Stopping SchedulerExtensionServices; (serviceOption=None,; services=List(),; started=false); 2019-01-09 13:35:56 INFO YarnClientSchedulerBackend:54 - Stopped; 2019-01-09 13:35:56 INFO MapOutputTrackerMasterEndpoint:54 - MapOutputTrackerMasterEndpoint stopped!; 2019-01-09 13:35:56 INFO MemoryStore:54 - MemoryStore cleared; 2019-01-09 13:35:56 INFO BlockManager:54 - BlockManager stopped; 2019-01-09 13:35:56 INFO BlockManagerMaster:54 - BlockManagerMaster stopped; 2019-01-09 13:35:56 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54 - OutputCommitCoordinator stopped!; 2019-01-09 13:35:56 INFO SparkContext:54 - Successfully stopped SparkContext; 13:35:56.383 INFO CountReadsSpark - Shutting down engine; [January 9, 2019 1:35:56 PM EST] org.broadinstitute.hellbender.tools.spark.pipelines.CountReadsSpark done. Elapsed time: 0.78 minutes.; Runtime.totalMemory()=1009254400; org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 4 times, most recent failure: Lost task 1.3 in stage 0.0 (TID 11, scc-q20.scc.bu.edu, executor 2): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 0, start 87545719, span 186383, expected MD5 492a29f6d7d6fcaf8dde06834861e7ae; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonf",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:36433,abort,aborted,36433,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,1,['abort'],['aborted']
Safety,"22-01-18; OpenJDK Runtime Environment (build 17.0.2+8-86); OpenJDK 64-Bit Server VM (build 17.0.2+8-86, mixed mode, sharing); ```. Because it's a shared cluster, we aren't able to run Docker directly. But I attempted converting it in to a Singularity container and it didn't crash in the same way, but the job did end up failing. Logs are as follows -. For the ""bare metal"" known-crashing conditions (AMD-based machine), the final lines of the output are:; ```; 22:47:45.999 INFO ProgressMeter - Scaffold_1:21181812 551.0 125350 227.5; 22:47:56.192 INFO ProgressMeter - Scaffold_1:21203869 551.1 125450 227.6; 22:48:06.937 INFO ProgressMeter - Scaffold_1:21251889 551.3 125650 227.9; 22:48:18.177 INFO ProgressMeter - Scaffold_1:21271601 551.5 125750 228.0; 22:48:29.896 INFO ProgressMeter - Scaffold_1:21281660 551.7 125810 228.0; 22:48:40.223 INFO ProgressMeter - Scaffold_1:21284898 551.9 125830 228.0; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007f889b5be310, pid=1422929, tid=1422930; #; # JRE version: OpenJDK Runtime Environment (17.0.2+8) (build 17.0.2+8-86); # Java VM: OpenJDK 64-Bit Server VM (17.0.2+8-86, mixed mode, sharing, tiered, compressed oops, compressed class ptrs, g1 gc, linux-amd64); # Problematic frame:; # C [libc.so.6+0xcf310] __memset_avx2_unaligned_erms+0x60; #; # Core dump will be written. Default location: Core dumps may be processed with ""/usr/lib/systemd/systemd-coredump %P %u %g %s %t %c %h %e"" (or dumping to /bigdata/operations/ejaco020/gatk/core.1422929); #; # An error report file with more information is saved as:; # /bigdata/operations/ejaco020/gatk/hs_err_pid1422929.log; #; # If you would like to submit a bug report, please visit:; # https://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #; ```. When running on singularity (AMD-based machine):; ```; 07:07:35.120 INFO Progress",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8988#issuecomment-2386154680:1095,detect,detected,1095,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8988#issuecomment-2386154680,1,['detect'],['detected']
Safety,"3/envs/cerc_prod/share/gatk4-4.1.7.0-0/gatk-package-4.1.7.0-local.jar MergeVcfs -I data/calling/cerc_prod2.SM_V7_1.vcf.gz -I data/calling/cerc_prod2.SM_V7_ZW.vcf.gz -O out.vcf.gz; 16:48:58.710 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/master/xxxxxxx/local/pckg/python/miniconda3/envs/cerc_prod/share/gatk4-4.1.7.0-0/gatk-package-4.1.7.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; [Mon Jun 22 16:48:58 CDT 2020] MergeVcfs --INPUT data/calling/cerc_prod2.SM_V7_1.vcf.gz --INPUT data/calling/cerc_prod2.SM_V7_ZW.vcf.gz --OUTPUT out.vcf.gz --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX true --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false; Jun 22, 2020 4:48:58 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; [Mon Jun 22 16:48:58 CDT 2020] Executing as xxxxxxx@yyyyyy on Linux 3.10.0-693.11.1.el7.x86_64 amd64; OpenJDK 64-Bit Server VM 1.8.0_152-release-1056-b12; Deflater: Intel; Inflater: Intel; Provider GCS is available; Picard version: Version:4.1.7.0; [Mon Jun 22 16:48:58 CDT 2020] picard.vcf.MergeVcfs done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=1211105280; To get help, see http://broadinstitute.github.io/picard/index.html#GettingHelp; htsjdk.tribble.TribbleException$MalformedFeatureFile: Unable to create BasicFeatureReader using feature file , for input source: file:///data/infectious/schistosome/tmp/test%20a/data/calling/cerc_prod2.SM_V7_1.vcf.gz; at htsjdk.tribble.AbstractFeatureReader.getFeatureReader(AbstractFeatureReader.java:124); at htsjdk.tribble.AbstractFeatureReader.getFeatureReader(AbstractFeatureReader.java:81); at htsjdk.variant.vcf.VCFFileReader.<init>(VCFFileReader.java:148); at htsjdk.variant.vcf.V",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6664#issuecomment-647808241:1984,detect,detect,1984,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6664#issuecomment-647808241,1,['detect'],['detect']
Safety,"4 [addAssemblyQNames -> getKmerIntervals] mapPartitionsToPair, then reduceByKey, then mapPartitions; * Job 5 [getAssemblyQNames] mapPartitions twice and a collect; * Job 6 [generateFastqs] mapPartitions and combineByKey, then write FASTQ to files. A few observations:; * Jobs 0,1,3 are simple map jobs - very quick 1-2 mins each.; * Job 2 is a simple MR, with a tiny shuffle to sum by key (<1MB of shuffle data); * Job 4 takes a bit longer longer (3min), and shuffles ~3GB. This is a lot faster than when I ran it before with less memory, when it took 9 min. Is it creating a lot of garbage? If you wanted to speed things up you might look at what this is doing on a local machine and see if there are any opportunities to improve CPU efficiency.; * Job 5 takes ~8 mins, and has no shuffle. CPU intensive processing again?; * Job 6 take a little over 3 mins, shuffling ~3GB. Overall, it looks like it’s performing pretty well. There is very little data being shuffled relative to the size of the input (~6GB to 133GB input), so it’s not worth looking into optimizing the data structures there. The input data is being read multiple times, so it _might_ be worth seeing if it can be cached by Spark to avoid reading from disk over and over again. This is only worth it if you have sufficient memory available across the cluster to hold the input (which will be bigger than the on-disk size) _plus_ enough memory for the processing, which as we saw is quite memory hungry anyway. There might be some CPU efficiencies to pursue, especially if some code paths are creating a lot of objects that need garbage collecting (as Jobs 4 and 5 seem to be). Jobs 4 and 5 seem to have some skew (judging from the task time distribution in the UI). You might investigate this by logging the amount of data that each task processes (or rather than logging, generating another output that is some description of the task data - or use a Spark accumulator), and then seeing if there's some way to make it more uniform.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2458#issuecomment-292171884:2599,avoid,avoid,2599,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2458#issuecomment-292171884,1,['avoid'],['avoid']
Safety,"4.1.4.1-83-g031c407-SNAPSHOT/gatk-package-4.1.4.1-83-g031c407-SNAPSHOT-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar <XXX>/gatk-4.1.4.1-83-g031c407-SNAPSHOT/gatk-package-4.1.4.1-83-g031c407-SNAPSHOT-local.jar BaseRecalibrator -R Homo_sapiens_assembly38.fasta -I S3_2.unmapped.split.bam --use-original-qualities -O S3_2.unmapped.recal_data.csv -known-sites Homo_sapiens_assembly38.dbsnp138.vcf -known-sites Mills_and_1000G_gold_standard.indels.hg38.vcf.gz --known-sites Homo_sapiens_assembly38.known_indels.vcf.gz; 23:39:34.668 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:<XXX>/gatk-4.1.4.1-83-g031c407-SNAPSHOT/gatk-package-4.1.4.1-83-g031c407-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 26, 2020 11:39:34 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 23:39:34.915 INFO BaseRecalibrator - ------------------------------------------------------------; 23:39:34.915 INFO BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.1.4.1-83-g031c407-SNAPSHOT; 23:39:34.915 INFO BaseRecalibrator - For support and documentation go to https://software.broadinstitute.org/gatk/; 23:39:34.915 INFO BaseRecalibrator - Executing as <XXX@XXX> on Linux v3.10.0-957.12.1.el7.x86_64 amd64; 23:39:34.915 INFO BaseRecalibrator - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-b08; 23:39:34.916 INFO BaseRecalibrator - Start Date/Time: February 26, 2020 11:39:34 PM EST; 23:39:34.916 INFO BaseRecalibrator - ------------------------------------------------------------; 23:39:34.916 INFO BaseRecalibrator - ------------------------------------------------------------; 23:39:34.916 INFO BaseRecalibrator - HTSJDK Version: 2.21.2; 23:39:34.916 INFO BaseRecalibrator - Picard Version: 2.21.9; 23:39:34.916 IN",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6242#issuecomment-592005237:2005,detect,detect,2005,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6242#issuecomment-592005237,1,['detect'],['detect']
Safety,"72d292e2b7acc692670e; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Thu Dec 7 13:57:16 2023 -0500. revert backport of MeanField. commit 67fb373687d32ec7e0fa329d4f3864e2cccabc53; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Wed Dec 6 10:45:11 2023 -0500. just used sample_node, finally deterministic?!. commit 754c424566cb9c191b9775b02d464488d7f68f68; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Wed Dec 6 07:22:51 2023 -0500. port stable logsumexp, though it doesn't seem to make a difference in ploidy. commit 95c944d792642ba5ec8dceaaff67d3a35cb3eab0; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Tue Dec 5 23:51:01 2023 -0500. working with Mixture, still nondeterministic. commit 426375ac6473999ba249e775f2aa7d622534d510; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Tue Dec 5 22:32:44 2023 -0500. stochastic node cleanup. commit dc66f3f6a07dc82ba4258b3c0a65d990355da8d7; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Tue Dec 5 21:38:52 2023 -0500. safe log in log ploidy priors. commit 9712fa169a08edcf1a7a56622708e610b862631e; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Tue Dec 5 21:35:05 2023 -0500. still debugging stochastic node. commit a79762c616f8758e0fd073b9e8e5d1ad30c1d5d0; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Thu Nov 9 16:48:05 2023 -0500. blas in base, conda with libmamba solver. commit b4f5301c28a03689fca0b95ef652a51dd991686d; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Thu Nov 9 12:45:53 2023 -0500. freeze conda and set libmamba in base. commit f57a13a08fc3d049f0271e9aa94639ecb87b50f2; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Thu Nov 9 07:29:30 2023 -0500. libmamba 23.9.0. commit 45058f27aae9c9240a167f126e32a6bddd3353ff; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Wed Nov 8 23:33:51 2023 -0500. conda 23.9.0. commit d95494d282dd42ea3377de6c2d3554a3a5db65e4; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Mon Oct 30 17:04:34 2023 -0400. minor fixes to get ploidy working. commit c6a21f33f1",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8561#issuecomment-1854434322:5062,safe,safe,5062,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8561#issuecomment-1854434322,1,['safe'],['safe']
Safety,"9] malloc+0x169`: . ```; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x000014cfb1d504f9, pid=1182729, tid=1195264; #; # JRE version: OpenJDK Runtime Environment (17.0.3) (build 17.0.3-internal+0-adhoc..src); # Java VM: OpenJDK 64-Bit Server VM (17.0.3-internal+0-adhoc..src, mixed mode, sharing, tiered, compressed oops, compressed class ptrs, g1 gc, linux-amd64); # Problematic frame:; # C [libc.so.6+0xaf4f9] malloc+0x169; #; # Core dump will be written. Default location: Core dumps may be processed with ""/usr/lib/systemd/systemd-coredump %P %u %g %s %t %c %h"" (or dumping to /gpfs/gpfs_de6000/home/dalegre/projects/1000-Genomes/jointcalling-test/goast_workflows/JointCalling/test_samples-1000.1.3/core.1182729); #; # If you would like to submit a bug report, please visit:; # https://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; [dalegre@login4601 fdone]$ head -n 20 hs_err_pid1182729.log; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x000014cfb1d504f9, pid=1182729, tid=1195264; #; # JRE version: OpenJDK Runtime Environment (17.0.3) (build 17.0.3-internal+0-adhoc..src); # Java VM: OpenJDK 64-Bit Server VM (17.0.3-internal+0-adhoc..src, mixed mode, sharing, tiered, compressed oops, compressed class ptrs, g1 gc, linux-amd64); # Problematic frame:; # C [libc.so.6+0xaf4f9] malloc+0x169; #; # Core dump will be written. Default location: Core dumps may be processed with ""/usr/lib/systemd/systemd-coredump %P %u %g %s %t %c %h"" (or dumping to /gpfs/gpfs_de6000/home/dalegre/projects/1000-Genomes/jointcalling-test/goast_workflows/JointCalling/test_samples-1000.1.3/core.1182729); #; # If you would like to submit a bug report, please visit:; # https://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #. ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8683#issuecomment-1936285520:1230,detect,detected,1230,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8683#issuecomment-1936285520,1,['detect'],['detected']
Safety,9f29bba22fa8bb0512350bf93?src=pr&el=desc) will **increase** coverage by `0.078%`.; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #3043 +/- ##; ===============================================; + Coverage 79.902% 79.979% +0.077% ; - Complexity 16668 16733 +65 ; ===============================================; Files 1134 1139 +5 ; Lines 60702 60917 +215 ; Branches 9423 9438 +15 ; ===============================================; + Hits 48502 48721 +219 ; + Misses 8412 8400 -12 ; - Partials 3788 3796 +8; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/3043?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [.../tools/exome/CalculatePulldownPhasePosteriors.java](https://codecov.io/gh/broadinstitute/gatk/pull/3043?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9DYWxjdWxhdGVQdWxsZG93blBoYXNlUG9zdGVyaW9ycy5qYXZh) | `85.366% <0%> (ø)` | `8% <0%> (?)` | |; | [...detectcoveragedropout/CoverageDropoutDetector.java](https://codecov.io/gh/broadinstitute/gatk/pull/3043?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9kZXRlY3Rjb3ZlcmFnZWRyb3BvdXQvQ292ZXJhZ2VEcm9wb3V0RGV0ZWN0b3IuamF2YQ==) | `91.803% <0%> (ø)` | `20% <0%> (?)` | |; | [...e/detectcoveragedropout/DetectCoverageDropout.java](https://codecov.io/gh/broadinstitute/gatk/pull/3043?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9kZXRlY3Rjb3ZlcmFnZWRyb3BvdXQvRGV0ZWN0Q292ZXJhZ2VEcm9wb3V0LmphdmE=) | `84% <0%> (ø)` | `4% <0%> (?)` | |; | [...ellbender/tools/exome/DecomposeSingularValues.java](https://codecov.io/gh/broadinstitute/gatk/pull/3043?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9EZWNvbXBvc2VTaW5ndWxhclZhbHVlcy5qYXZh) | `89.474% <0%> (ø)` | `5% <0%> (?)` | |; | [...e/detectcoveragedropout/CoverageDropoutResult.java](https://codecov.io/gh/broadinstitute/gatk/pull/3043?src=,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3043#issuecomment-306585614:1227,detect,detectcoveragedropout,1227,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3043#issuecomment-306585614,1,['detect'],['detectcoveragedropout']
Safety,":08 2023 -0500. add back setup.py files. commit 8348f546de6b3d32e1f02f6851730226c0dbffc9; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Fri Dec 8 09:37:09 2023 -0500. update pymc version in init. commit 850d60ef95b6126c05af9cd7c2cb528a306e1224; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Fri Dec 8 09:32:42 2023 -0500. added pip editable docs. commit 9c51b311442b0796ab1224213e83290caea0f93f; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Fri Dec 8 09:24:50 2023 -0500. whitespace. commit d9b180385168fdd1ef55cee8a1069fc1f7928f38; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Fri Dec 8 09:24:10 2023 -0500. update setup_gcnvkernel.py and pin pytensor. commit 7ccbd6da3d4afd1c987a66f6874bc4918495f943; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Fri Dec 8 09:20:49 2023 -0500. update conda in base and python packages. commit 693a1f9de10ee9950000abb83ef598cde82e026b; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Fri Dec 8 08:55:53 2023 -0500. clean up Mixture, sample seeding, safelog. commit 2b211d7ed7875c798020ceaeed865523f25c7096; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Fri Dec 8 00:28:54 2023 -0500. fixed all determinism, need to clean up seeds. commit 799228dd5fafa2bf5d57e65e9aab256cdfc4698a; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Thu Dec 7 22:22:18 2023 -0500. more dCR, Mixture. commit 124073d9af37c19573f90270c3cb0f4ae5ba4dd0; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Thu Dec 7 19:39:55 2023 -0500. fix dCR sampling?. commit f6871c9f12dbd52d84e78bba4f03d276ba1efb72; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Thu Dec 7 15:34:26 2023 -0500. logsumexp cleanup. commit 0c6cba790d8c3566a1a872d292e2b7acc692670e; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Thu Dec 7 13:57:16 2023 -0500. revert backport of MeanField. commit 67fb373687d32ec7e0fa329d4f3864e2cccabc53; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Wed Dec 6 10:45:11 2023 -0500. just used sample_node, finally deterministic?!. commit 754",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8561#issuecomment-1854434322:3412,safe,safelog,3412,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8561#issuecomment-1854434322,1,['safe'],['safelog']
Safety,"; ##fileformat=VCFv4.2; ##ALT=<ID=NON_REF,Description=""Represents any possible alternative allele at this location"">; ##FORMAT=<ID=AD,Number=R,Type=Integer,Description=""Allelic depths (counting only informative reads out of the total reads) for the ref and alt alleles in the order listed"">; ##FORMAT=<ID=AF,Number=A,Type=Float,Description=""Allele fractions for alt alleles in the order listed"">; ##FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Approximate read depth (reads with MQ=255 or with bad mates are filtered)"">; ##FORMAT=<ID=F1R2,Number=R,Type=Integer,Description=""Count of reads in F1R2 pair orientation supporting each allele"">; ##FORMAT=<ID=F2R1,Number=R,Type=Integer,Description=""Count of reads in F2R1 pair orientation supporting each allele"">; ##FORMAT=<ID=GP,Number=G,Type=Float,Description=""Phred-scaled posterior probabilities for genotypes as defined in the VCF specification"">; ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=""Genotype Quality"">; ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; ##FORMAT=<ID=ICNT,Number=2,Type=Integer,Description=""Counts of INDEL informative reads based on the reference confidence model"">; ##FORMAT=<ID=MB,Number=4,Type=Integer,Description=""Per-sample component statistics to detect mate bias"">; ##FORMAT=<ID=MIN_DP,Number=1,Type=Integer,Description=""Minimum DP observed within the GVCF block"">; ##FORMAT=<ID=PL,Number=G,Type=Integer,Description=""Normalized, Phred-scaled likelihoods for genotypes as defined in the VCF specification"">; ##FORMAT=<ID=PRI,Number=G,Type=Float,Description=""Phred-scaled prior probabilities for genotypes"">; ##FORMAT=<ID=PS,Number=1,Type=Integer,Description=""Physical phasing ID information, where each unique ID within a given sample (but not across samples) connects records within a phasing group"">; ##FORMAT=<ID=SB,Number=4,Type=Integer,Description=""Per-sample component statistics which comprise the Fisher's Exact Test to detect strand bias"">; ##FORMAT=<ID=SPL,Number=.,Type=Integer,Descrip",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7797#issuecomment-1112612397:1423,detect,detect,1423,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7797#issuecomment-1112612397,2,['detect'],['detect']
Safety,============; Files 1134 1139 +5 ; Lines 60702 60917 +215 ; Branches 9423 9438 +15 ; ===============================================; + Hits 48502 48721 +219 ; + Misses 8412 8400 -12 ; - Partials 3788 3796 +8; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/3043?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [.../tools/exome/CalculatePulldownPhasePosteriors.java](https://codecov.io/gh/broadinstitute/gatk/pull/3043?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9DYWxjdWxhdGVQdWxsZG93blBoYXNlUG9zdGVyaW9ycy5qYXZh) | `85.366% <0%> (ø)` | `8% <0%> (?)` | |; | [...detectcoveragedropout/CoverageDropoutDetector.java](https://codecov.io/gh/broadinstitute/gatk/pull/3043?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9kZXRlY3Rjb3ZlcmFnZWRyb3BvdXQvQ292ZXJhZ2VEcm9wb3V0RGV0ZWN0b3IuamF2YQ==) | `91.803% <0%> (ø)` | `20% <0%> (?)` | |; | [...e/detectcoveragedropout/DetectCoverageDropout.java](https://codecov.io/gh/broadinstitute/gatk/pull/3043?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9kZXRlY3Rjb3ZlcmFnZWRyb3BvdXQvRGV0ZWN0Q292ZXJhZ2VEcm9wb3V0LmphdmE=) | `84% <0%> (ø)` | `4% <0%> (?)` | |; | [...ellbender/tools/exome/DecomposeSingularValues.java](https://codecov.io/gh/broadinstitute/gatk/pull/3043?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9EZWNvbXBvc2VTaW5ndWxhclZhbHVlcy5qYXZh) | `89.474% <0%> (ø)` | `5% <0%> (?)` | |; | [...e/detectcoveragedropout/CoverageDropoutResult.java](https://codecov.io/gh/broadinstitute/gatk/pull/3043?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9kZXRlY3Rjb3ZlcmFnZWRyb3BvdXQvQ292ZXJhZ2VEcm9wb3V0UmVzdWx0LmphdmE=) | `92.593% <0%> (ø)` | `17% <0%> (?)` | |; | [.../broadinstitute/hellbender/utils/tsv/DataLine.java](https://codecov.io/gh/broadinstitute/gatk/pull/3043?src=pr&e,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3043#issuecomment-306585614:1548,detect,detectcoveragedropout,1548,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3043#issuecomment-306585614,2,"['Detect', 'detect']","['DetectCoverageDropout', 'detectcoveragedropout']"
Safety,"> . @rsasch I don't believe that CreateFilteringFiles.java is used anywhere else except in GVS. As for JointVcfFiltering.wdl, I really tried to avoid changing it, but the changes I had to make were for memory and disk. At this point the changes are in our branch (ah_var_store) and when we merge with main, we can deal with the PR then. But, I also know there's an updated version of this WDL that we are supposed to update to - that will presumably happen before we merge with main.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8206#issuecomment-1444234025:144,avoid,avoid,144,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8206#issuecomment-1444234025,1,['avoid'],['avoid']
Safety,"> > > LGTM. Seems superficially odd that we ultimately want to _reduce_ the disk size for pgen on larger callsets, but I trust the results of your analysis; > > ; > > ; > > Disk size is less of a concern because (at least with GCP historically) going too low on disk size risks much slower I/O without commiserate savings.; > ; > Yes, that's also what I thought as well. That's why it seemed a little odd that this PR included a change that _lowered_ the default disk size on the VMs if they weren't specified from 500 to 200. I always thought going too low was the worry, and that disk size in general isn't much of a concern. But this PR appears to lower the default disk size by 60%, unless I am reading that completely wrong. The logs showed a max of 7% disk space, so it seemed fair to reduce it somewhat. But your comment reminded me that I also meant to adjust the `effective_extract_memory_gib` calculation, so I will do that now.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8979#issuecomment-2349192759:272,risk,risks,272,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8979#issuecomment-2349192759,1,['risk'],['risks']
Safety,"> > LGTM. Seems superficially odd that we ultimately want to _reduce_ the disk size for pgen on larger callsets, but I trust the results of your analysis; > ; > Disk size is less of a concern because (at least with GCP historically) going too low on disk size risks much slower I/O without commiserate savings. Yes, that's also what I thought as well. That's why it seemed a little odd that this PR included a change that _lowered_ the default disk size on the VMs if they weren't specified from 500 to 200. I always thought going too low was the worry, and that disk size in general isn't much of a concern. But this PR appears to lower the default disk size by 60%, unless I am reading that completely wrong.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8979#issuecomment-2349179555:260,risk,risks,260,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8979#issuecomment-2349179555,1,['risk'],['risks']
Safety,> @Bowen1992 Could you please try running with the latest GATK release (`4.2.6.1`) and reporting whether the issue persists?. Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx50G -Djava.io.tmpdir=./tmp -jar /public/home/gaoshibin/software/GATK/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar GenotypeGVCFs -R /public/home/gaoshibin/B73_REF/Zea_mays.AGPv4.dna.toplevel.fa -V gendb://./CHR7_gvcf_database -G StandardAnnotation --genomicsdb-shared-posixfs-optimizations true -O new_ALL_MATERIALS_chr7.g.vcf.gz; 17:49:50.404 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 17:49:50.653 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/public/home/gaoshibin/software/GATK/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 17:49:51.271 INFO GenotypeGVCFs - ------------------------------------------------------------; 17:49:51.273 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.6.1; 17:49:51.273 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:49:51.273 INFO GenotypeGVCFs - Executing as gaoshibin@comput6 on Linux v3.10.0-693.el7.x86_64 amd64; 17:49:51.274 INFO GenotypeGVCFs - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_211-b12; 17:49:51.274 INFO GenotypeGVCFs - Start Date/Time: 2022年5月22日 下午05时49分50秒; 17:49:51.274 INFO GenotypeGVCFs - ------------------------------------------------------------; 17:49:51.275 INFO GenotypeGVCFs - ------------------------------------------------------------; 17:49:51.276 INFO GenotypeGVCFs - HTSJDK Version: 2.24.1; 17:49:51.276 INFO GenotypeGVCFs - Picard Version: 2.27.1; 17:49:51.276 INFO GenotypeGVCFs - Built for Spark Version: 2.4.5; 17:49:51.277 INFO GenotypeGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 17:49:,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7866#issuecomment-1135302097:680,Redund,Redundant,680,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7866#issuecomment-1135302097,1,['Redund'],['Redundant']
Safety,"> @Siadjeu Don't worry about the ""Failed to detect"" message. It indicates some internal state in one of the google libraries but not an error we need to worry about. Generally you shouldn't worry about INFO messages if everything else is going fine and they don't say something particular about what you're doing. A WARNING or ERROR message would indicate a problem. This should maybe be downgraded to be a DEBUG level message or something but it's in a third library and convincing them to change it might be a hassle. Although there are results, but the size of the results is wrong, the results are too small.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6875#issuecomment-1598239834:44,detect,detect,44,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6875#issuecomment-1598239834,1,['detect'],['detect']
Safety,"> Another potential solution is to audit every Dataflow (and spark) code that can receive SAMRecords as input, and make sure they call some utility ""putHeadersBack"" function. I agree. I think this is the most practical solution. (Having the coder/serializer do it is difficult, as it's hard to get the header to the serializer, unless it is passed statically, which I think we'd rather avoid.). This would look something like `reads.map(read -> ReadUtils.addHeader(header, read))`, and would be added after every shuffle. Since we should be very aware where every shuffle is happening (and we want to minimize their number) it shouldn't be too onerous. The other related point that Uri has touched on is the need for an efficient encoding of reads (even without the header), which is critical to making the computations run in a reasonable amount of time. The approach I've taken in https://github.com/broadinstitute/hellbender/pull/899 is to use the htsjdk BAMEncoder to serialize reads (Hadoop-BAM does something very similar), and it works very well in my tests of sorting large BAMs. Does https://github.com/broadinstitute/hellbender/pull/899 plus ""putHeadersBack"" sound like a reasonable solution?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141004828:386,avoid,avoid,386,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141004828,1,['avoid'],['avoid']
Safety,"> Can you check the headers of your gvcf inputs to see if any of them has this old tag?. #####this is the tag for gatk4.4; ##fileformat=VCFv4.2; ##ALT=<ID=NON_REF,Description=""Represents any possible alternative allele not already represented at this location by REF and ALT"">; ##FILTER=<ID=LowQual,Description=""Low quality"">; ##FORMAT=<ID=AD,Number=R,Type=Integer,Description=""Allelic depths for the ref and alt alleles in the order listed"">; ##FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Approximate read depth (reads with MQ=255 or with bad mates are filtered)"">; ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=""Genotype Quality"">; ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; ##FORMAT=<ID=MIN_DP,Number=1,Type=Integer,Description=""Minimum DP observed within the GVCF block"">; ##FORMAT=<ID=PGT,Number=1,Type=String,Description=""Physical phasing haplotype information, describing how the alternate alleles are phased in relation to one another; will always be hetero; zygous and is not intended to describe called alleles"">; ##FORMAT=<ID=PID,Number=1,Type=String,Description=""Physical phasing ID information, where each unique ID within a given sample (but not across samples) connects records within a phasing gr; oup"">; ##FORMAT=<ID=PL,Number=G,Type=Integer,Description=""Normalized, Phred-scaled likelihoods for genotypes as defined in the VCF specification"">; ##FORMAT=<ID=PS,Number=1,Type=Integer,Description=""Phasing set (typically the position of the first variant in the set)"">; ##FORMAT=<ID=SB,Number=4,Type=Integer,Description=""Per-sample component statistics which comprise the Fisher's Exact Test to detect strand bias."">; ##GATKCommandLine=<ID=HaplotypeCaller,CommandLine=""HaplotypeCaller --emit-ref-confidence GVCF --output CMC_C_1.g.vcf --input CMC_C_1.sorted.markdup.addRG.bam --reference kxc_hic_final.fast; a --use-posteriors-to-calculate-qual false --dont-use-dragstr-priors false --use-new-qual-calculator true --annotate-with-num-discovered-alleles false",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8574#issuecomment-1793390789:1638,detect,detect,1638,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8574#issuecomment-1793390789,1,['detect'],['detect']
Safety,"> Hi Kevin, our team would like to get this merged into `ah_var_store` soon per VS-1254. I'm aware of only a handful of outstanding issues:; > ; > * The failing PGEN tests. I'm happy to help here in any way I can though right now I don't have a sense of what could be causing this beyond the platform differences you suggested.; > ; > * The `10` vs `10.0` change we discussed recently to avoid division by zero.; > ; > * We'll want to merge / rebase from `ah_var_store` and then build a new GATK Docker image which would be entered into `GetToolVersions` in `GvsUtils.wdl`. I'm happy to take on building this image once the merge / rebase is ready. Hi Miguel, sorry about the delay. I'm working on the failing tests issue this afternoon. I know what the issue is, so I just have to implement a fix, which I think should be fairly simple. Once I have that ready and have confirmed the tests aren't failing anymore, I'll do the rebase and then let you know.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8708#issuecomment-2004517723:388,avoid,avoid,388,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8708#issuecomment-2004517723,1,['avoid'],['avoid']
Safety,"> I agree with you that this is a real problem. I don't understand the logic, why HALF of PCR_ERROR_QUAL? if that's really 20, then it's way too low!!. The intent of this code is that downstream code will not do anything special to avoid over-counting ; overlapping mates, so that assigning half of the PCR qual effectively gives the full PCR qual for the fragment. Of course, this assumption is wrong in the case of M2. > I also object to the second part (when the bases disagree.) imagine that one base is A@Q2 and the other is T@Q60...why would you put both bases to Q0 in that case?. Good point. > We should take some time to figure out the model that allows for PCR error and then derive the posterior posteriors from that... Are you suggesting something like there are binary indicators for PCR error, read 1 sequencing error, read 2 sequencing error, with priors given by the PCR and base qualities, and we want the posteriors of these indicators given that the bases agree / disagree?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4958#issuecomment-400799135:232,avoid,avoid,232,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4958#issuecomment-400799135,1,['avoid'],['avoid']
Safety,"> I don't think we've made any guarantees about the thread safety of Funcotator or the associated datasource classes.; > ; > Also, this account seems to be a bot and I can't access its listed home page…; > ; > I can audit the class at some point. https://codesafe.qianxin.com/#/home",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7376#issuecomment-894740783:59,safe,safety,59,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7376#issuecomment-894740783,1,['safe'],['safety']
Safety,"> I have no objection to these changes, especially since this is just bringing us back to where we were in genomicsDB in the last release. We should spawn a ticket to track reintroducing these improvements and perhaps we should also add a macos test to our travis array so we can catch this kind of issue in the future? I think there is a macOS VM availible on travis that we could rerun some of the integration tests on. Yes, travis has macOS VM. It is very slow, so would recommend only sanity checks on it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6204#issuecomment-539574124:489,sanity check,sanity checks,489,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6204#issuecomment-539574124,1,['sanity check'],['sanity checks']
Safety,> Is there any drawback to them being so high? Maybe just a small additional cost risk?. If the import fails non-transiently (BQ becomes unavailable or perhaps a bug is introduced during development) the import could be retried many times. But even in this scenario the VM wouldn't be up very long before exiting non-0 so I don't think this is a significant risk.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7953#issuecomment-1190750331:82,risk,risk,82,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7953#issuecomment-1190750331,2,['risk'],['risk']
Safety,"> LGTM. Seems superficially odd that we ultimately want to _reduce_ the disk size for pgen on larger callsets, but I trust the results of your analysis. Disk size is less of a concern because (at least with GCP historically) going too low on disk size risks much slower I/O without commiserate savings.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8979#issuecomment-2349160369:252,risk,risks,252,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8979#issuecomment-2349160369,1,['risk'],['risks']
Safety,"> OK, looks like you can get around the compiler lock issues by pointing each invocation of GermlineCNVCaller to a different compilation directory. For example, invoke `gatk` by; > ; > `THEANORC=PATH/TO/THEANORC_# gatk GermlineCNVCaller ...`; > ; > This uses the `THEANORC` environment variable to set the `.theanorc` configuration file to `PATH/TO/THEANORC_#` for this instance of GATK (where you should fill in `#` appropriately). Each `PATH/TO/THEANORC_#` should be a file containing the following:; > ; > ```; > [global]; > base_compiledir = PATH/TO/COMPILEDIR_#; > ```; > ; > Where again, `#` is filled in appropriately. The goal is to point each GermlineCNVCaller instance to a different compilation directory. @xysj1989 can you let me know if this works for you?; > ; > This is a bit of a hack. We could probably avoid this by changing the GATK code to use a specified or temporary directory for the theano directory without too much effort.; > ; > However, there is an upside to using a non-temporary directory to avoid recompilation of the model upon subsequent runs. In this case, we'd just want to let the user be able to specify the theano directory (rather than dump things in `~/.theano` unexpectedly). We should think about whether this should be opt-in, i.e., should we preserve the original behavior of using `~/.theano` by default?; > ; > @mwalker174 opinions? @droazen or engine team, thoughts on what the policy should be for python/R scripts doing this sort of thing? Is it generally true that the GATK leaves no trace, other than producing the expected output?. Dear samuelklee,. Thank you very much for you reply. I also found this problem last night. It seems that the problem is originally from Theano and Pymc3, rather than GATK 4.0. Some similar problems have been reported just like (1) https://github.com/pymc-devs/pymc3/issues/1463 (2) https://stackoverflow.com/questions/52270853/how-to-get-rid-of-theano-gof-compilelock and (3) https://groups.google.com/forum/#!topic/t",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6235#issuecomment-548557073:820,avoid,avoid,820,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6235#issuecomment-548557073,1,['avoid'],['avoid']
Safety,"> Overall the refactoring looks good and makes sense… but I'm not seeing how this fixes the problem of eating exceptions we saw during a recent run. Can you explain what was happening before, and how the new code addresses it?. Sure! This code (besides refactoring so that it was only in one place) aims to fix two issues:; 1. if query results in an error, it gets run three more times and then, because of `while len(retry_delay) > 0`, it doesn’t run again and the `raise err` line never gets executed, so no error is ever raised; 2. if the query fails for a reason that has no chance of being fixed by a retry (eg. 404), it will still run three more times. I probably missed some errors that should be ""retry-able"" (maybe `Aborted `? `BadGateway`? `Cancelled `? [full list here](https://googleapis.dev/python/google-api-core/latest/exceptions.html)), but I still think it makes sense to not treat all errors the same.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7480#issuecomment-930239576:725,Abort,Aborted,725,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7480#issuecomment-930239576,1,['Abort'],['Aborted']
Safety,"> The error comes from two annotations: InbreedingCoeff and ExcessHet. One solution is to add ""-AX ExcessHet -AX InbreedingCoeff"". It doesnt exactly solve the problem, but it avoids hitting the problem code. Awesome! It is useful. Thank you very much!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7938#issuecomment-1238890115:175,avoid,avoids,175,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7938#issuecomment-1238890115,1,['avoid'],['avoids']
Safety,"> Why not break up these two things and push the annotation close to where the breakpoint-detected variants are created (ie. in discoverSimpleVariants or something), and then call the imprecise variant detector after that?. I wanted to do that but since the original code was brought in (and tested) that way with the imprecise variant logic, I refrained from doing that. ; Now done in commit 9fa39908fff6f382d899bc66f1760dbcfd22e540.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3934#issuecomment-357543055:90,detect,detected,90,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3934#issuecomment-357543055,2,['detect'],"['detected', 'detector']"
Safety,"> Yes please. Where we left it I think Louis was happy, but we wanted to ask Nalini if she had any suggestions to avoid threading the argument for genotypes all the way through the engine. Not sure we can avoid threading the argument for genotypes, but would using GenomicsDBOptions instead work?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4947#issuecomment-478089357:114,avoid,avoid,114,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4947#issuecomment-478089357,2,['avoid'],['avoid']
Safety,> could look into forking gatk. This seems problematic. How can we avoid this?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7142#issuecomment-839995728:67,avoid,avoid,67,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7142#issuecomment-839995728,1,['avoid'],['avoid']
Safety,"> why would we want to commit this?. I don't think we do, I was just looking for sanity checks that what I ran was correct.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8868#issuecomment-2165892753:81,sanity check,sanity checks,81,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8868#issuecomment-2165892753,1,['sanity check'],['sanity checks']
Safety,"@AJDCiarla . Hello. So, to preface my answers it's also important to note that I am having these issues on Firefox. Now that I tried it in Chromium the Sign in link works. `When did you start having trouble signing into your GATK account?`; The issue is actually not with my account. I cannot even get to the Sign-in screen that you show.; `What is the username/email address associated with your GATK Forum account?`; As I mentioned before, the issue is not account specific. In fact I don't really remember if I still have one. My plan was to try my email and if it was already in the system to recover it.; `Could you please walk me through more of what you are seeing/doing when trying to log into your existing GATK account?`; When I open the [forum page](https://gatk.broadinstitute.org/hc/en-us/community/topics) and click New Post button or if I just click the ""Sign in"" button in the top panel. ![image](https://user-images.githubusercontent.com/22867431/204882347-257314af-1421-4e74-87e2-dffe760480d1.png). I don't get redirected to the Sign in page, but to the main page of GATK; ; ![image](https://user-images.githubusercontent.com/22867431/204882697-43bd0d15-0dd8-479b-af69-78951bb7d56c.png). The URL that I see in the browser does have some extra info:. https://gatk.broadinstitute.org/hc/en-us/signin?return_to=https%3A%2F%2Fgatk.broadinstitute.org%2Fhc%2Fen-us%2Fcommunity%2Fposts%2Fnew . But it still doesn't change where I end up.; The same is happening eevn if I run Firefox with `--safe-mode` to see if it's due to any of my extensions.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8115#issuecomment-1332595323:597,recover,recover,597,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8115#issuecomment-1332595323,2,"['recover', 'safe']","['recover', 'safe-mode']"
Safety,@AJDCiarla The user should try re-running `GenotypeGVCFs` with `--max-genotype-count` set to a value greater than 1024. This should prevent the PLs from getting dropped and avoid the downstream error. The user may also need to increase `--max-alternate-alleles` as well.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7938#issuecomment-1187744382:173,avoid,avoid,173,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7938#issuecomment-1187744382,1,['avoid'],['avoid']
Safety,"@AlijahArcher We will need to clarify exactly what you intend by skipping the assembly. Here are some possibilities, with my initial thoughts:. * ""skip assembly"" = ""trust the alignments completely and use them directly for variant calling"": if your aligner is good this is reasonable though not ideal. If this is what you want you might as well use samtools for variant calling.; * ""skip assembly"" = ""every unique pattern of variants seen in your reads defines a haplotype"": the problem is that every sequencing error generates a new haplotype, so you need some way to cull bad haplotypes. Also, reads might only cover part of a haplotype so you need a way to sew them together.; * ""skip assembly"" = ""find all variants in your read alignments and let every combination thereof define a haplotype"": if I recall correctly this is FreeBayes. I would call this a quick-and-dirty assembly rather than skipping assembly entirely.; * ""skip assembly"" = ""avoid haplotypes altogether and genotype variants directly"": as you are aware, this is not possible within HC and M2. Hopefully that focuses the conversation somewhat on the two main questions: how do you generate haplotypes, and how do you refine the set of haplotypes to a few good candidates? What did you have in mind?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7064#issuecomment-770995444:946,avoid,avoid,946,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7064#issuecomment-770995444,1,['avoid'],['avoid']
Safety,"@AxVE Thanks for this PR. We really appreciate your interest and work on resolving this issue! It might take a little bit for me to get to reviewing it properly, we're currently preparing for our release and we're a bit swamped with various issues. I'm worried about changing the `userClassPathFirst` property. We added that a long time ago because it fixed some issues we were running into at the time. It's completely possible that we no longer have the same issue and it's a harmful remnant from a previous time, but I'm afraid that changing it might have unanticipated consequences in our own spark environment. Unfortunately we don't have good automated tests that would necessarily identify any issue. @cwhelan Would you be able to test your pipeline with - ""spark.driver.userClassPathFirst"" : ""false"" and see if you run into any issues? . I'm also a bit confused about why the change to the arguments is necessary. Clearly in your environment it is, but it goes against my understanding of how we set the arguments to spark submit, so I want to properly understand why the existing --deploy-mode arguments aren't working for you before adding an additional hardcoded argument to the launch script. (As I'm sure you've seen, the launch script is a pretty crufty and brittle piece of code that was really meant to be replaced with a more robust solution by now, so any additional complexity in would be great to avoid...)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3946#issuecomment-351430567:1417,avoid,avoid,1417,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3946#issuecomment-351430567,1,['avoid'],['avoid']
Safety,"@AxVE Thanks for this! Sorry for the long wait. We'll have to monitor to see if changing it introduces some obscure issues for us that we haven't been able to figure out yet, but it seems like it's probably safe.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3946#issuecomment-358123108:207,safe,safe,207,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3946#issuecomment-358123108,1,['safe'],['safe']
Safety,"@DanishIntizar Hello! Thank you for this pr. This is great to see an official plugin from amazon available. I appreciate that you took the time to make it an optional include. I think if we're going to include it we might as well just add it as one of our normal dependencies though. Assuming there aren't any dependency conflicts it **should** (always a risky statement) be independent from everything else. . Thanks also for identifying the different issues you mentioned. It's expected that it won't work with most picard tools as you discovered, but we're actively in the process of updating more of them too support Paths instead of Files so that will slowly improve. The second issue is more worrisome. We regularly use an equivalent provider with google to read reference files through the exact same code, so I suspect there is either some sort of mismatched assumptions in the way they are handling things. Maybe something strange with the Path.resolve methods or the like. (Or in in the much worse potential case a bug in their look ahead caching.). I'd like to look into that before we'd merge this. Ideally we would have tests for this. Are there any public AWS paths we could read from without any secret authentication?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8672#issuecomment-1930094721:355,risk,risky,355,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8672#issuecomment-1930094721,1,['risk'],['risky']
Safety,"@DarioS Are you running contamination checks on this sample first? Did that not fail your QC?. It's difficult for us to provide a failure QC metric like this because there are so many different cancer types, each with their own behaviors. The fact that variants are all near their limit of detection is not necessarily a failure of M2 or even your sample. Instead it sounds like maybe you need a tool that would take a VCF as input and produce summary statistics on the VCF that you could then decide on QC metrics for. Does that sound like it would solve your problem?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6674#issuecomment-648951268:290,detect,detection,290,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6674#issuecomment-648951268,1,['detect'],['detection']
Safety,"@DonFreed, I agree with @magicDGS's assessment about it. This feels like a fix that was applied to Gatk3 but doesn't translate to 4? Of course, there could be implementations of GATKRead that don't obey the given contract about copying, but it's worth fixing those since we were more careful to think about copy/no copy when we wrote the new interface. . Of note: if you haven't seen it, `GATKRead` provides a set of unsafe `getBaseQualitiesNoCopy()` methods for times when the copy is a performance bottleneck and you can guarantee safe use of the underlying array. . I'm going to close this. Feel free to reopen if you disagree / can provide a unit test that demonstrates the issue still exists.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4926#issuecomment-399219562:417,unsafe,unsafe,417,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4926#issuecomment-399219562,2,"['safe', 'unsafe']","['safe', 'unsafe']"
Safety,"@DuyDN This is a known issue in BQSR -- see https://github.com/broadinstitute/gatk/issues/6242. Sorry for the inconvenience! We hope to be able to develop a fix within the next several months. The fact that you ran into this error indicates that there may not actually be any usable reads in that particular read group -- they were likely all filtered out by one of the BQSR filters, which filter out malformed, low mapping quality, unmapped, and secondary alignments. You could likely avoid the error by filtering out that read group using the `ReadGroupBlackListReadFilter` in GATK while running ApplyBQSR (`--read-filter ReadGroupBlackListReadFilter`)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7549#issuecomment-963494490:486,avoid,avoid,486,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7549#issuecomment-963494490,1,['avoid'],['avoid']
Safety,"@EdwardDixon Sure, here's my suggested repair process:. 1. If you haven't already, add an ""upstream"" remote to your git clone via `git remote add upstream git@github.com:broadinstitute/gatk.git` (or `https://github.com/broadinstitute/gatk.git` if you don't have ssh authentication set up with github). 2. `git fetch upstream`. 3. Copy the files you actually intended to change in this PR into a temp directory somewhere. 4. Create a new temporary branch off of `upstream/master`: `git checkout -b avxcheck_repaired upstream/master`. 5. Copy the files you saved in step 3 back into their original locations in the working tree. 6. `git commit -a`. 7. Examine the diff against upstream/master via `git diff upstream/master HEAD`. Verify that the diff is what you expect. 8. Run `git rev-parse HEAD` and save the commit ID it outputs. 9. Switch back to the broken version of the branch: `git checkout avxcheck`. 10. Run `git reset --hard commit_id_from_step_8`. This will force the branch to point to the repaired commit we created in step 6. 11. Run `git push -f origin avxcheck:avxcheck` to force-push the repaired version of the branch into your fork. Then check that it looks ok on github. For avoiding this sort of thing in the future, here's a few tips:. * Never run `git merge` or `git pull`. Always update your branch with changes from the latest gatk master branch via the command: `git fetch upstream && git rebase -i upstream/master`, followed by `git push -f` to push the rebased branch into your fork. * If you've never run `git rebase` before, read a tutorial on it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-437415495:1195,avoid,avoiding,1195,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-437415495,1,['avoid'],['avoiding']
Safety,"@LeeTL1220 A few minor remaining comments. Do what you will. How much of a performance impact does the change have? You said it slows it down, is it significant? It might be faster if you make it a long instead of an atomic long which should be safe it it's single threaded and you don't use parallel streams anywhere.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3305#issuecomment-316490330:245,safe,safe,245,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3305#issuecomment-316490330,1,['safe'],['safe']
Safety,"@LeeTL1220 Having started to implement this. I have a number of design questions that would be informed by your usecases. . Firstly, is there a reason to preserve symbolic alleles? It seems as though spanning deletions could/should be dropped as in most cases there is another variant context representing that deletion elsewhere in your file? Should there be validation around dropping spanning deletion symbolic alleles to ensure we aren't dropping a spanning deletion that isn't represented anywhere else? What about nocalls? . Your example suggests that we rely on the header line counts for subsetting annotations, if there is a disagreement in the header do you want any more sophisticated behavior than just throwing? My understanding is that we are lenient with splitting in htsjdk and there have been some mislabeled header lines in the past that would make this an expected state. Furthermore, most allele specific annotators are of type string because there is no standard for ""|"" delimiters which makes them hard to handle properly. @ldgauthier do you have any suggestions as to how to detect and handle allele specific annotations?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4976#issuecomment-404949363:1098,detect,detect,1098,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4976#issuecomment-404949363,1,['detect'],['detect']
Safety,"@LeeTL1220 OK, tweaked the message a bit. I think I'm OK with this going in for the next point release. This is the sort of thing for which it will be nice to have the automatic validations, as a sanity check.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4292#issuecomment-363828979:196,sanity check,sanity check,196,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4292#issuecomment-363828979,1,['sanity check'],['sanity check']
Safety,"@LeeTL1220 do you have any opinions on making the somatic CNV workflow scatter by contig? This could allow WGS to complete basically ~20x faster and could allow us to avoid issues such as #4734. A few issues:. 1) Do we want a single WDL that can optionally scatter, depending on WES vs. WGS? It would be nicer to have just one workflow, but I haven't thought about how an optional scatter might look in WDL. 2) For segmentation and modeling, scattering should have little impact on the final result (although there are a few global-level quantities in the models that would be reduced to contig-level quantities, which might slightly affect the quality of their inference). However, we'd want to concatenate all per-contig results for both plotting and segment calling. It'd be relatively easy to either have a separate tool to concatenate AbstractLocatableCollections (there is already a method to do this that is used for the gCNV pipeline), or to make the plotting and calling tools take in parallel inputs and combine them. I'd tend towards the former, just so we don't have to deliver per-contig files.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4728#issuecomment-386268150:167,avoid,avoid,167,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4728#issuecomment-386268150,1,['avoid'],['avoid']
Safety,@PPWEST522 I suspect that you may not be limiting allelic-count collection to a set of common variant sites (provided as input to `-L` when running CollectAllelicCounts). See discussion in the tutorial at https://gatk.broadinstitute.org/hc/en-us/articles/360035890011--How-to-part-II-Sensitively-detect-copy-ratio-alterations-and-allelic-segments. Feel free to reopen and provide more details if this is not the case!,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7633#issuecomment-1009563051:296,detect,detect-copy-ratio-alterations-and-allelic-segments,296,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7633#issuecomment-1009563051,1,['detect'],['detect-copy-ratio-alterations-and-allelic-segments']
Safety,"@SHuang-Broad Could you explain what happens with and without the count? . Spark is deliberately lazy... i.e. without the count you would expect nothing to be computed until success.isEmpty() is called. (Note that success is a different RDD than results, so calling success.isEmpty() and then success.map(..).saveAsTextFile(..) will probably result in success being re-filtered from results twice. You could try caching success to avoid that. In either case it should avoid recomputing results though when failure.isEmpty() is computed.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1877#issuecomment-225221197:431,avoid,avoid,431,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1877#issuecomment-225221197,2,['avoid'],['avoid']
Safety,"@SHuang-Broad Two other potential options:; 1. Potentially modify bwa's xassert so that it instead of calling abort() directly, it has a function pointer that by default points to abort() but our JNI code instead points it to a new method which will throw a java exception instead. ; 2. Patch BWA to do something similar during the jbwa build process. (gross...). Is 1 feasible? It would be good if we could globally avoid exits and redirect them to java exceptions.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2123#issuecomment-243228784:110,abort,abort,110,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2123#issuecomment-243228784,3,"['abort', 'avoid']","['abort', 'avoid']"
Safety,"@SHuang-Broad We'd like to have all of those abort conditions throw exceptions instead of crashing though. Seems like if we could intercept the abort call it would help us in many situations, not just the 1 bad index.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2123#issuecomment-243321053:45,abort,abort,45,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2123#issuecomment-243321053,2,['abort'],['abort']
Safety,"@Siadjeu Don't worry about the ""Failed to detect"" message. It indicates some internal state in one of the google libraries but not an error we need to worry about. Generally you shouldn't worry about INFO messages if everything else is going fine and they don't say something particular about what you're doing. A WARNING or ERROR message would indicate a problem. This should maybe be downgraded to be a DEBUG level message or something but it's in a third library and convincing them to change it might be a hassle.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6875#issuecomment-708443156:42,detect,detect,42,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6875#issuecomment-708443156,1,['detect'],['detect']
Safety,@TedBrookings I implemented these suggestions of yours:. - replace CLUSTER_NAME with SANITIZED_BAM in the results directory; - allow output directory to be overridden by setting SV_OUTPUT_DIR; - rename `runWholePipeline` to `run-whole-pipeline`. I also found and fixed two other bugs:. - added a `set -f` to the create cluster script to avoid having bash expand the wildcard glob for that step (this caused a problem if a file matching the pattern was present locally); - changed how the copy results script got cluster info so that it parses the result header (this fixes a problem when using preemptible workers because the number of columns in the results of the `gcloud dataproc clusters list` command had a different number of columns. Can you give these changes a quick re-review?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4646#issuecomment-383718666:337,avoid,avoid,337,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4646#issuecomment-383718666,1,['avoid'],['avoid']
Safety,"@Unip0rn Thank you for the pr. I'm not sure I understand what you're trying to do here though. currently when I run `./gatk --list` it prints the list of gatk tools. ex:; ```; USAGE: <program name> [-h]. Available Programs:; --------------------------------------------------------------------------------------; Base Calling: Tools that process sequencing machine data, e.g. Illumina base calls, and detect sequencing level attributes, e.g. adapters; CheckIlluminaDirectory (Picard) Asserts the validity for specified Illumina basecalling data.; CollectIlluminaBasecallingMetrics (Picard) Collects Illumina Basecalling metrics for a sequencing run.; CollectIlluminaLaneMetrics (Picard) Collects Illumina lane metrics for the given BaseCalling analysis directory.; ExtractIlluminaBarcodes (Picard) Tool determines the barcode for each read in an Illumina lane.; IlluminaBasecallsToFastq (Picard) Generate FASTQ file(s) from Illumina basecall read data. ...; ```. With this change it instead prints the gatk launcher help, which is not the intended result. ; ```; Usage template for all tools (uses --spark-runner LOCAL when used with a Spark tool); gatk AnyTool toolArgs. Usage template for Spark tools (will NOT work on non-Spark tools); gatk SparkTool toolArgs [ -- --spark-runner <LOCAL | SPARK | GCS> sparkArgs ]. Getting help; gatk --list Print the list of available tools. gatk Tool --help Print help on a particular tool. Configuration File Specification; --gatk-config-file PATH/TO/GATK/PROPERTIES/FILE. gatk forwards commands to GATK and adds some sugar for submitting spark jobs. --spark-runner <target> controls how spark tools are run; valid targets are:; LOCAL: run using the in-memory spark runner; SPARK: run using spark-submit on an existing cluster; --spark-master must be specified; --spark-submit-command may be specified to control the Spark submit command; arguments to spark-submit may optionally be specified after --; GCS: run using Google cloud dataproc; commands after the --",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5541#issuecomment-449068030:401,detect,detect,401,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5541#issuecomment-449068030,1,['detect'],['detect']
Safety,"@adaykin The difference between the chr1, chr2, etc. convention and the 1, 2, etc. convention is more than just a difference in naming. Different versions of the human reference use different naming schemes. For example the b37 reference uses 1, 2, etc., while the hg38 reference uses chr1, chr2, etc. For this reason, it is not safe to simply translate the contig names on-the-fly. You need to do a proper liftover from one reference to another using a tool such as `LiftoverVcf`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7538#issuecomment-963507876:329,safe,safe,329,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7538#issuecomment-963507876,1,['safe'],['safe']
Safety,"@akiezun @lindenb I can help. It's probably going to be a slightly complicated process though, especially actually building the cross platform jar. I assume we're targeting OSX and x86-64 to start with, and then hopefully expanding to POWER8 in the future? . The general idea is to prebuild the c code for whatever platforms we want to support. Then package that in a structured way into a jar, and write some java code which will detect the platform at runtime and extract the correct executable into a temporary location. Then we can publish that jar as a maven artifact. We have an example of how to do the extraction in the `VectorLoglessPairHMM` constructor. It's not perfect and probably needs a bit of refactoring to make it more general but it's the right idea. Other libraries that package native code have similar examples. I.e. Snappy-java https://github.com/xerial/snappy-java. We're going to be performing similar packaging for other native dependencies that we have, so standardizing it is a good idea.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1750#issuecomment-215820135:431,detect,detect,431,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1750#issuecomment-215820135,1,['detect'],['detect']
Safety,"@akiezun Changed the .travis.yml so that the tests don't run on cloud side of things. It's super ugly though, I thought there would be a clean way to do it, but I can't figure one out, it's just a bunch of if statements in the travis file. . We're getting a coverage failure though, since tests cases are covering less code now. We could avoid this by disabling coverage entirely for external pull requests. Thoughts?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1050#issuecomment-151265843:338,avoid,avoid,338,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1050#issuecomment-151265843,1,['avoid'],['avoid']
Safety,"@akiezun I think there's a case to be made for having the ability to separate closure of resources from generation of final output on success, even if it's not currently needed -- I'd be ok with keeping both methods provided we can settle on the right names to avoid confusion, and provided we update the docs to make it clear when each method should be overridden. @lbergelson's suggestion of `onTraversalSuccess()` and `cleanup()` seems reasonable.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1743#issuecomment-212629504:261,avoid,avoid,261,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1743#issuecomment-212629504,1,['avoid'],['avoid']
Safety,"@akiezun If you think the problem might extend beyond `FilterVcf`, and that our sequence dictionary detection might be broken in general in some way for vcfs, could you create a separate ticket to investigate, and assign to @cmnbroad?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1728#issuecomment-212036904:100,detect,detection,100,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1728#issuecomment-212036904,1,['detect'],['detection']
Safety,"@akiezun Instead of adding these overloads would we see the same speedup if we cached the result of isUnmapped and isPaired in the adapter? That would have the downside of complicating the adapter but it might avoid adding these strange methods to the interface. . If caching seems like a bad alternative, I think maybe these methods should have names that make it clear that they're some sort of performance hack and you should generally prefer the standard ones. 'getContigUnsafe` for instance.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235101307:210,avoid,avoid,210,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235101307,1,['avoid'],['avoid']
Safety,"@akiezun Is there a reason these are `LinkedLists`? It seems like it should be List instead. I tried to find usages that depend on the linkedlistness of it, but the only thing I see is that one private method relies on it being a `Deque`, but that could be trivially rewritten. It seems like you'd avoid even more allocation by switching it to use List and then using the non-allocating `Collections.emptyList()`. `getBetween` uses the linkedness of it to use `addFirst`. This could be rewritten to either build the list in the reverse order and reverse it, or maybe provide a guava `Lists.reverse` view.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1795#issuecomment-216362302:298,avoid,avoid,298,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1795#issuecomment-216362302,1,['avoid'],['avoid']
Safety,"@akiezun It has to do with the code, one part laziness in avoiding to implement a bona fide `GATKRead` adapter for `AlignmentRecord`, and one part due to the interval filter operating only on `SAMRecord`s (`samRecordOverlaps`). Assuming ADAM does become a more important use case to support, we can refactor to improve this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1067#issuecomment-152358785:58,avoid,avoiding,58,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1067#issuecomment-152358785,1,['avoid'],['avoiding']
Safety,"@akiezun That's why I was suggesting invalidating all cached values on every call to any setter -- that way we don't have to think about the nuances of when it's necessary to recalculate, and greatly reduce the risks that normally come with caching while still getting most of the performance benefit in typical usage.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235136126:211,risk,risks,211,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235136126,1,['risk'],['risks']
Safety,@akiezun We shouldn't waste time trying to fix it -- let's just add a prominent warning in htsjdk that it's not safe to use and move on.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1638#issuecomment-212034501:112,safe,safe,112,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638#issuecomment-212034501,1,['safe'],['safe']
Safety,"@akiezun You need to allow a set of comparison `FeatureInputs` (what GATK3 calls ""getCompRodBindings"") to be passed in at construction, in addition to the DBSNP `FeatureInput`. Then you need to modify your `initializeOverlapAnnotator()` method slightly to add entries for the non-DBSNP ""comp"" `FeatureInputs` as well, and it should just work, I think. Could you please do this + add a test? This feature is needed/used by the HC, among other tools. Relevant code from GATK3:. ```; final Map<RodBinding<VariantContext>, String> overlapBindings = new LinkedHashMap<>();; for ( final RodBinding<VariantContext> b : walker.getCompRodBindings()); if ( b.isBound() ) overlapBindings.put(b, b.getName());; if ( dbSNPBinding != null && ! overlapBindings.keySet().contains(VCFConstants.DBSNP_KEY) ); overlapBindings.put(dbSNPBinding, VCFConstants.DBSNP_KEY); // add overlap detection with DBSNP by default. variantOverlapAnnotator = new VariantOverlapAnnotator(dbSNPBinding, overlapBindings, engine.getGenomeLocParser());; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1485#issuecomment-183559648:865,detect,detection,865,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1485#issuecomment-183559648,1,['detect'],['detection']
Safety,"@asmirnov239 I think that we probably want all shard sizes to be as close to equal as possible, so that we avoid any possible issues arising from different model capacities across shards. Actually, is this even an issue at the GermlineCNVCaller step? Maybe we just need to worry about it during ploidy/postprocessing?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6559#issuecomment-617321553:107,avoid,avoid,107,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6559#issuecomment-617321553,1,['avoid'],['avoid']
Safety,"@asmirnov239 I've borrowed the CopyNumberTestUtils class from #7889 into which you moved the method for detecting deltas in the doubles. I'm going to merge this PR once tests pass, so just be aware of this when rebasing your branch if you make any further changes. We might consider adding a simple test of the test method itself. I'll let you do it in your branch, or we can file an issue and tackle it after everything is merged.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7652#issuecomment-1165717972:104,detect,detecting,104,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7652#issuecomment-1165717972,1,['detect'],['detecting']
Safety,"@bbimber File locking doesn't work on all the filesystems GenomicsDB supports (hdfs/cloud, for instance) and is a pain on others (NFS, for instance -- painful enough that we've sometimes had to recommend users disable the existing file locking). For that reason, I wouldn't want users to depend on file locking for correctness. Unfortunately, I think the cleanest approach is for the user to ensure correctness by avoiding the read/write conflicts themselves.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6558#issuecomment-617448100:414,avoid,avoiding,414,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6558#issuecomment-617448100,1,['avoid'],['avoiding']
Safety,"@bbimber Thanks -- I think we'll upload the files to a public Google storage bucket instead, to avoid the need for sharing passwords out in the open like this. We should be able to get to this after the GATK point release goes out tomorrow. I'll make a post here once they're uploaded.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/616#issuecomment-360580547:96,avoid,avoid,96,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/616#issuecomment-360580547,1,['avoid'],['avoid']
Safety,"@bbimber Yes, the timeouts are not unusual. I restarted the (1 of the 8) jobs in the build matrix that failed on your PR. It will probably be fine, but I'll keep an eye on it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4495#issuecomment-384694242:18,timeout,timeouts,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4495#issuecomment-384694242,1,['timeout'],['timeouts']
Safety,"@bhandsaker In hellbender we do already ship the header around ""out-of-band"" much as you described (via the spark broadcast mechanism and the dataflow ""side input"" mechanism), and use that header singleton for operations such as looking up the sample, library, platform, etc. We wrote utility methods in hellbender that take both a read and a separate header for all operations that previously required a header within the read, to avoid having to actually restore the header into the reads themselves after every operation that involves a shuffle over the network. Our goal is just to ensure that `SAMRecord` behaves in a consistent, well-documented way when it lacks a header without imposing any additional burden on other users of htsjdk. The only code that would have to be ""headerless-aware"" as a result of this effort would be code that has explicitly opted-in to stripping the headers (eg., hellbender itself, or projects that depend on hellbender).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/900#issuecomment-142410461:432,avoid,avoid,432,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-142410461,1,['avoid'],['avoid']
Safety,"@bhanugandham @fleharty this issue touches upon our discussion of https://gatkforums.broadinstitute.org/gatk/discussion/24335/loh-detection-using-gatk4s-somatic-cnv-workflow. We might consider just a simple modification of the genotyping step (e.g., keeping all ROHs longer than a hard threshold) to start, which would probably cover the most common use cases with minimal effort. Can use 100% HCC1143 in tumor-only mode as an initial test, but it would be good to collect other examples.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3915#issuecomment-531833700:130,detect,detection-using-,130,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3915#issuecomment-531833700,1,['detect'],['detection-using-']
Safety,"@chapmanb ; _What we can do in bcbio is avoid adding any of these until after running the joint calling so they all get on the final joint VCF rather than the gVCFs._. I think that would be the right way to go (irrespective of the inconsistencies in the headers). My understanding as a layman (I'm not a bioinformatician) is that many of these annotation fields are (population/group) statistics. By storing these annotations in the individual gVCFs and then importing them into GenomicsDB, the size of the storage and the time to import are increased significantly (since the statistics are being stored in every sample for every position with a record in the gVCF).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5045#issuecomment-407503807:40,avoid,avoid,40,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5045#issuecomment-407503807,1,['avoid'],['avoid']
Safety,"@cmnbroad : thanks for the reply. yes, obviously tests would need to be added/updated. there's no question it needs robust testing. . my main concern is that VariantEval is a fairly sprawling tool with all sorts of add-ons. The majority of the untouched code taken verbatim from GATK3 isnt going to pass muster based on the bar of our last PR without a lot of petty revision (and maybe some useful updates). There are certainly some code improvements one could make across VariantEval, but I'm just not that keen on combing through the whole thing if it can be avoided. . how about this: while tests need to be updated (as discussed above, they work on the GATK3 data, which isnt checked in), the code in this PR is functional. Would you be willing to review a couple classes, maybe VariantEval itself and a few ancillary classes to see what scope of work we're talking about?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-413642544:561,avoid,avoided,561,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-413642544,1,['avoid'],['avoided']
Safety,"@cmnbroad @ldgauthier Out of curiosity, we may be able to avoid the complexity entirely if we just dropped the founderID argument from the tools. founderIDs didn't exist in gatk3, it looks like it was added to GATK4 at some point when a tool needed to produce a pedigree annotation before we had support for parsing ped files. Thoughts?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5663#issuecomment-470283695:58,avoid,avoid,58,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5663#issuecomment-470283695,1,['avoid'],['avoid']
Safety,"@cmnbroad How about we detect the common case of filters composed using only AND, and use simplified output in that case, and revert back to the complex output when filters are composed in more complex ways? That would resolve the problem in practice, since (as far as I know) all of the filters we actually use are composed using only AND.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3520#issuecomment-367072562:23,detect,detect,23,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3520#issuecomment-367072562,1,['detect'],['detect']
Safety,"@cmnbroad I did a pass with a little cleanup. I think this is ready for review, but has a couple things I left the might help review:. 1) As noted above, the primary purpose here is to migrate to MultiVariantWalkerGroupedOnStart, and remove the redundant re-querying of comp alleles. This seems to work, but has the effect of altering behavior in some cases, described more above. In VariantEvalUtils.java I left some debugging code that illustrates the behavior difference that will occur. . 2) It is a fair question as to whether changing the behavior of what is or isnt considered an overlap is appropriate. For now I'm making changes as though it is, since it's sort of a fringe case and this is a beta tool, but it should be discussed. 3) There are ~6 tests with altered expectations, due to that change in detecting overlaps. I just checked in their updated expectations, since it helps illustrate how the iteration change would impact results",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-732478578:245,redund,redundant,245,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-732478578,2,"['detect', 'redund']","['detecting', 'redundant']"
Safety,"@cmnbroad I see. The ""CI"" variable does seem brittle, especially since I'm not strictly sure where it is set. I think a somewhat safer place would be to add some global test flag to the docker image would be to add it to the run_unit_tests.sh script. That way we know it is getting triggered exactly before we run the tests in just the docker environment. Is there some way of detecting what conda environment is active outside of conda.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5819#issuecomment-474871354:129,safe,safer,129,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5819#issuecomment-474871354,2,"['detect', 'safe']","['detecting', 'safer']"
Safety,"@cmnbroad I understand that I could have retained a bunch of single-use text files, but it seemed like the more permutations one adds, the less it makes sense to have a separate, very redundant, static text file to check each scenario. There's a ton of VariantContext-related tests that parse the output VCF to test some feature as opposed to checking in a bunch of VCF text files.... While I'll grant the 4th test case I added (where we pass chr 2) isnt especially compelling over just testing chr 1, one could argue more breadth is a good thing here. if you want clarity, pulling that VariantEval report parsing code into a method called extractUniqueContigsFromEvalReport(), or simply adding a comment line, supports this goal. Anyway, I'm checking in slightly clarified version of this now, simply to get tests running. If you respond to the above, maybe we go with that. In the interest of time, I'll stage and check in the version which restores the text files and goes that route.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7238#issuecomment-831459741:184,redund,redundant,184,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7238#issuecomment-831459741,1,['redund'],['redundant']
Safety,"@cmnbroad OK, thanks. I need to do more investigation, but my initial thoughts/investigation were two-fold:. 1) Our main use-case if our VariantQC tool, which makes several instances of VariantEval in kind of a hacky way and calls apply() on them. Therefore refactoring a VariantEvalEngine out of VariantEval has value on this front, even if tricky. 2) From what I could tell, the worst perf is the iteration pattern. Using something like MultiVariantWalkerGroupedOnStart would be a big savings and avoid re-querying the component VCFs.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5439#issuecomment-720617033:499,avoid,avoid,499,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5439#issuecomment-720617033,1,['avoid'],['avoid']
Safety,"@cmnbroad OK, that's what I was afraid of. Has your group given thought to how barclay might attempt to de-convolute ""identical"" arguments defined across diverse plugins like this? . Anyway, I can try to follow the pattern of pedigree. I was trying to avoid special-casing these arguments, but I am already making my own PluginDescriptor anyway. My use-case is effectively to have a VariantAnnotator that supports more annotations.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7213#issuecomment-823493118:252,avoid,avoid,252,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7213#issuecomment-823493118,1,['avoid'],['avoid']
Safety,"@cmnbroad Thanks for the reply. I will look through that code to see if I turn up where this is happening. Is there a way to override/skip this repair? I understand as the header is coming from SVABA the VCF header is out of spec for GATK. It is one of the reasons I am using the tool to fix the dictionary of the VCF. It would be helpful to avoid the repair of other header fields in such cases. In the meantime, I will add a step to re-repair that particular header line so that downstream python scripts don't throw errors for the type mismatch. If there is no current way to avoid the header repair, and because it is a spec issue within GATK, I can close the issue after your reply.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8629#issuecomment-1861509061:342,avoid,avoid,342,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8629#issuecomment-1861509061,2,['avoid'],['avoid']
Safety,"@cmnbroad one of the tests failed; however, it seems to just be a timeout:. https://api.travis-ci.org/v3/job/457478297/log.txt. are you able to restart them? again, I believe this addresses all concerns listed above except for the iteration (which will be addressed in the engine in a new PR), and the naming of CompRod and EvalRod classes. I'm fine changing these and associated outputs; however, I would appreciate suggestions on the best new names. CompInput, CompFeatureInput, CompSource, CompTrack, or something like that?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-440333816:66,timeout,timeout,66,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-440333816,1,['timeout'],['timeout']
Safety,"@cmnbroad 👍 Rebase and merge when ready. I think people expect to be able to write files with any extension they feel like, and the risks are pretty minimal.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2046#issuecomment-243827012:132,risk,risks,132,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2046#issuecomment-243827012,1,['risk'],['risks']
Safety,"@cmnbroad, now this is prepared. I'm not using commons-io but the all `FASTA_EXTENSIONS` for detecting what to modify in the name.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2243#issuecomment-267036006:93,detect,detecting,93,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2243#issuecomment-267036006,1,['detect'],['detecting']
Safety,"@cwhelan , updated with the forced repartition to avoid the partition problem we saw.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1997#issuecomment-243876166:50,avoid,avoid,50,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1997#issuecomment-243876166,1,['avoid'],['avoid']
Safety,"@cwhelan . Thanks for the review!; I've incorporated most of your review suggestions, with the fowling exception because I need to think about what need to be done to make less review rounds. > This logic does more than detect variants, though.. it also annotates existing variants with the imprecise evidence. I'm also a little hesitant to move this all into its own separate class -- we really should be moving towards a model where we look at all three sources of evidence (breakpoint assemblies, imprecise evidence clusters, and coverage) simultaneously for eg @mwalker174 's work, and splitting handling of imprecise evidence into its own class seems like a step in the wrong direction. I agree. That's what I'm thinking about for complex inversions as well. So what about the following in this particular PR:. 1. move `StructuralVariationDiscoveryPipelineSpark.makeEvidenceLinkTree()` into `ImpreciseVariantDetector`;; 2. drop `ImpreciseVariantDetector.detectImpreciseVariantsAndAddReadAnnotations()` considering it really only delegates to `processEvidenceTargetLinks()`; 3. rename `ImpreciseVariantDetector` as `EvidenceTargetLinkHandler`; 4. reduce the work of `DiscoverVariantsFromContigAlignmentsSAMSpark.discoverVariantsAndWriteVCF()` into detecting only simple variants based on assemblies and name it `discoverSimpleVariants()`; 5. let `StructuralVariationDiscoveryPipelineSpark` call into `EvidenceTargetLinkHandler.processEvidenceTargetLinks()` to get back VariantContexts, then write VCF . `processEvidenceTargetLinks()` really does two things at the moment: annotation on breakpoints and call imprecise deletions; preferably, we should go the all-evidence-at-the-same-time approach and decouple the two but I am trying to not mess with it right now. If you agree, I'll implement it in a separate commit.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3934#issuecomment-357345426:220,detect,detect,220,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3934#issuecomment-357345426,3,['detect'],"['detect', 'detectImpreciseVariantsAndAddReadAnnotations', 'detecting']"
Safety,"@cwhelan I was actually debating with myself about whether to include the initialization script here, as it was living in the bucket referred to in the creation script.; So we could do this:; always store the initialization script locally with the creation script instead of referring to a script living remotely, and makes that a required argument. The good: this makes it easier to track changes; The bad: initialization script must be removed from the bucket to avoid tracking possible different versions. A non-technical issue: we are ""delivering"" SGA in the initialization script, if that comes in to this repo, legal might have a problem with it. On the other hand, if the initialization script lives in a place only we can access, we are ""installing SGA for our own use"", which is not a problem with the GPL license.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2435#issuecomment-285093289:465,avoid,avoid,465,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2435#issuecomment-285093289,1,['avoid'],['avoid']
Safety,"@davidbenjamin & @ldgauthier Sorry for commenting on a closed/merged PR but I wasn't sure where else to take the discussion. If there's a more appropriate place please redirect me!. First off, this is very cool and I'm so glad to see this making it's way into HC/M2! It's super helpful for functional annotation/clinical interpretation. Thanks for working on this!. I had two thoughts which maybe belong as separate issues, but I figured I'd raise them here first and see what you thought:. 1. It would actually be useful to be able to combine this behavior with GVCF mode in some cases. I understand all the caveats about merging and joint-genotyping when this has been done, but there are use cases for single-sample calling where both GCVF and MNP mode combined would be useful. E.g. in a clinical setting it's very useful to have the GVCF with the reference blocks, and also call MNPs as MNPs. There would be no merging in this case. Any chance this could be allowed, perhaps with a warning or requirement that `--unsafe` be on?; 2. IIRC RBP would also phase combinations of `indel-SNP` and `indel-indel` in addition to `SNP-SNP`. I'm curious how hard it would be to apply the same grouping logic across indels as well? I tried to read the code in the PR, but honestly I don't think I understand the ramifications of including indels sufficiently well. Would there be any conceptual objections or road-blocks to doing this?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4650#issuecomment-396290602:1018,unsafe,unsafe,1018,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4650#issuecomment-396290602,1,['unsafe'],['unsafe']
Safety,"@davidbenjamin @jonn-smith I have pushed the latest version of the code. Most of the changes since this was last shown are adding checks to about every level of the code for infinite loops (several places in the dangling end recovery code, and several places the new BestHaplotypeFinder). Additionally tests have been updated to capture these cases as well as several of the changes to functionality (no longer forcing reference start kmer to have a junction tree, limiting the cases where we actually attempt to follow an unsupported reference path when constructing a haplotype, reintroducing reference path weight to the calculation, etc...).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6034#issuecomment-520991019:225,recover,recovery,225,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6034#issuecomment-520991019,1,['recover'],['recovery']
Safety,@davidbenjamin Can you comment on this? Is this argument actually redundant?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7731#issuecomment-1074250007:66,redund,redundant,66,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7731#issuecomment-1074250007,1,['redund'],['redundant']
Safety,@davidbenjamin Could you take a look at this? @TedBrookings thinks it might be as simple as changing the check to allow 0 length reads when initializing the pairHmm. Neither of us are sure that that's a great solution though. . It seems like if you have no read bases you can't do any useful calculation. Should there be an earlier check in mutect that avoids assembling a region if there aren't any reads with bases?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5543#issuecomment-465193131:353,avoid,avoids,353,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5543#issuecomment-465193131,1,['avoid'],['avoids']
Safety,@davidbenjamin I think that this issue will be addressed by the AFCalculator refactoring one way or another (e.g. by lifting up the max-alt-allele restrictions or simply avoid adding the NON-REF allele before calling the AFCalculator).,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1858#issuecomment-221770394:170,avoid,avoid,170,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1858#issuecomment-221770394,1,['avoid'],['avoid']
Safety,"@davidbenjamin I tried and this time its a different error. ; ```; 14:55:53.232 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/shollizeck/clustering.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 09, 2020 2:55:53 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 14:55:53.432 INFO FilterMutectCalls - ------------------------------------------------------------; 14:55:53.433 INFO FilterMutectCalls - The Genome Analysis Toolkit (GATK) v4.1.4.1-6-g6bb31a7-SNAPSHOT; 14:55:53.433 INFO FilterMutectCalls - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:55:53.433 INFO FilterMutectCalls - Executing as shollizeck@stpr-res-compute02.unix.petermac.org.au on Linux v3.10.0-1062.4.3.el7.x86_64 amd64; 14:55:53.433 INFO FilterMutectCalls - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_232-b09; 14:55:53.434 INFO FilterMutectCalls - Start Date/Time: 9 January 2020 2:55:53 PM; 14:55:53.434 INFO FilterMutectCalls - ------------------------------------------------------------; 14:55:53.434 INFO FilterMutectCalls - ------------------------------------------------------------; 14:55:53.434 INFO FilterMutectCalls - HTSJDK Version: 2.21.0; 14:55:53.435 INFO FilterMutectCalls - Picard Version: 2.21.2; 14:55:53.435 INFO FilterMutectCalls - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 14:55:53.435 INFO FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 14:55:53.435 INFO FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 14:55:53.435 INFO FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 14:55:53.435 INFO FilterMutectCalls - Deflater: IntelDeflater; 14:55:53.435 INFO FilterMutectCalls - Inflater: IntelInflater; 14:55:53.435 INFO FilterMutectCalls - GCS max retries/reopens: 20; 14:55:53.435 INFO FilterMutectCalls - Requester pays: disabled; 14:55:",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6202#issuecomment-572799341:357,detect,detect,357,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6202#issuecomment-572799341,1,['detect'],['detect']
Safety,"@davidbenjamin Thanks for the workaround in the 4.1.9.0 release!. I tested the updated `CreateSomaticPanelOfNormals` with `genomicsDBs` computed in 4.1.7.0 as above and it seems that the workaround recovers a lot of multiallelic variants that were already missing in 4.1.7.0. Using the record and variant counts in 4.1.7.0 as 100% reference, I'm getting 57% more records (all multiallelic) or 142% more variants. No sites from 4.1.7.0 are missing in 4.1.9.0. As a side note, all of the new records have `FRACTION=1` and most (90%) have `BETA=1,1;FRACTION=1`. Among shared records, all multiallelic sites also have `FRACTION=1` and almost always different beta parameter estimates compared to 4.1.7.0. As expected, biallelic sites are unchanged. As far as I understand, these annotations are irrelevant in deciding whether a site should be output or not, so this is not a concern.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6744#issuecomment-707740253:198,recover,recovers,198,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6744#issuecomment-707740253,1,['recover'],['recovers']
Safety,"@davidbenjamin there is one test that failed. is this possible an intermittent /timeout problem? i dont have permission to restart them, but i dont see an actual test failure in it: https://travis-ci.com/broadinstitute/gatk/jobs/283688682",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6406#issuecomment-582418410:80,timeout,timeout,80,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6406#issuecomment-582418410,1,['timeout'],['timeout']
Safety,"@droazen @jamesemery It does seem like it's by design, and as James found the behavior is to multiply the evidence by the length of the soft clip, which seems weird. We avoid this in Mutect2 and treat it like an indel. I would vote for doing the same and just eliminating the `HIGH_QUALITY_SOFT_CLIP` category.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5767#issuecomment-470612215:169,avoid,avoid,169,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5767#issuecomment-470612215,1,['avoid'],['avoid']
Safety,"@droazen @ldgauthier I'm assuming the travis failure is not caused by this? looking at some of the travis logs its complaining about ""Timeout waiting for network availability"" and the previous travis CI in master failed too...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5970#issuecomment-519716794:134,Timeout,Timeout,134,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5970#issuecomment-519716794,1,['Timeout'],['Timeout']
Safety,@droazen Adam did profiling that showed that overlaps detector was strictly better. We also had some suspicion that there was some bug lurking in the skip list implementation because of weird non-deterministic results of something that relied on it.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4154#issuecomment-358031235:54,detect,detector,54,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4154#issuecomment-358031235,1,['detect'],['detector']
Safety,"@droazen For some reason, the fix in `CommandLineProgram` from #2190 stopped working in this branch (although I couldn't reproduce locally), which is why I tried making the `CommandLineProgram` field in `FeatureManager` transient again to see if it fixed the issue. Which it did. I've now avoided the issue entirely by removing the need for the `CommandLineProgram` field, by passing the instance to the method that needs it. This also addresses the point that @cmnbroad made about serializing the whole tool. The tests pass with this change. This probably needs testing on Google Cloud. @jean-philippe-martin have you run this successfully with the new NIO library (0.5.1)?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2220#issuecomment-258810989:289,avoid,avoided,289,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2220#issuecomment-258810989,1,['avoid'],['avoided']
Safety,"@droazen I believe @DonFreed's new code can be inserted before my new code with no change to either. His code deals with the non-hard-clipped part of the read, and all my code does is add the hard clips to the cigar. I think it's safe to add naively.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3494#issuecomment-418471397:230,safe,safe,230,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3494#issuecomment-418471397,1,['safe'],['safe']
Safety,@droazen I have added the tests you were asking for and changed the reference base code to the slower but safer version (plus this version should hopefully pass the tests now). Let me know if this looks good to you! Then I can rebase and merge.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/894#issuecomment-142348807:106,safe,safer,106,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/894#issuecomment-142348807,1,['safe'],['safer']
Safety,"@droazen I implemented the pr skipping on push builds if there's a pr branch. It seems to work. It has to spin up a vm to do the check, but that takes about a minute instead of many, and it avoids running tests and downloading lfs. The good things is that if it fails for some reason it should just continue on with the build, so flakiness in the github api or network connectivitiy will just result in some extra builds completing rather than extra failures. . <img width=""1054"" alt=""screen shot 2018-09-05 at 11 19 14 am"" src=""https://user-images.githubusercontent.com/4700332/45103843-c5b5ae00-b0fe-11e8-9934-0025af9836ee.png"">. I think I should add a github token though so we don't get api throttling. Should I just add one from my own account?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5156#issuecomment-418773830:190,avoid,avoids,190,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5156#issuecomment-418773830,1,['avoid'],['avoids']
Safety,@droazen I just looked and it seems that the only other big one I added was `--disable-artificial-haplotype-recovery` and that one is very esoteric indeed and doesn't need to be exposed I don't think.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6737#issuecomment-668197410:108,recover,recovery,108,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6737#issuecomment-668197410,1,['recover'],['recovery']
Safety,"@droazen I posted the complete command line I used (the version is above). I posted a test.sam that reproducibly fails on my machine (OSX). And below is the log from my machine:. ```; 22:42:22.298 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/nhomer/miniconda3/envs/bfx/share/gatk4-4.1.8.1-0/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.dylib; Aug 01, 2020 10:42:22 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 22:42:22.412 INFO HaplotypeCaller - ------------------------------------------------------------; 22:42:22.412 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.1.8.1; 22:42:22.412 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 22:42:22.412 INFO HaplotypeCaller - Executing as nhomer@ip-192-168-7-102.ec2.internal on Mac OS X v10.14.6 x86_64; 22:42:22.412 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_192-b01; 22:42:22.412 INFO HaplotypeCaller - Start Date/Time: August 1, 2020 10:42:22 PM MST; 22:42:22.412 INFO HaplotypeCaller - ------------------------------------------------------------; 22:42:22.412 INFO HaplotypeCaller - ------------------------------------------------------------; 22:42:22.413 INFO HaplotypeCaller - HTSJDK Version: 2.23.0; 22:42:22.413 INFO HaplotypeCaller - Picard Version: 2.22.8; 22:42:22.413 INFO HaplotypeCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 22:42:22.413 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 22:42:22.413 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 22:42:22.413 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 22:42:22.413 INFO HaplotypeCaller - Deflater: IntelDeflater; 22:42:22.413 INFO HaplotypeCaller - Inflater: IntelInflater; 22:42:22.413 INFO HaplotypeCaller ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6733#issuecomment-667631737:536,detect,detect,536,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6733#issuecomment-667631737,1,['detect'],['detect']
Safety,"@droazen I ran the latest version but the message about google is still there!. 14:08:05.607 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/cm/shared/unil/software/8.3/GATK/4.1; .9.0-GCCcore-8.3.0-Java-8/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Oct 14, 2020 2:08:06 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6875#issuecomment-708360241:434,detect,detect,434,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6875#issuecomment-708360241,1,['detect'],['detect']
Safety,"@droazen I'm not sure this is an improvement. We want the fundamental unit of spark tool to be the transform, not the cli wrapper around it. If we do this then we're pushing more of the contract of the transform outside of itself, i.e. see the newly duplicated bqsr code. I think that it was a deliberate decision to lift all reads into the initial rdd and then filter them in the transforms to what was needed by that transform. This is paying some performance cost in multi-stage pipelines which will potentially apply the same filters over and over again, but it simplifies the code because the filters can be baked into the transform and the pipeline writer doesn't have to think about them. It would be nice if we had a mechanism for adding metadata to an RDD so we can say ""this is a sorted RDD filtered with X,Y,and Z filters"", so we could intelligently avoid re-filtering.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1159#issuecomment-158168856:861,avoid,avoid,861,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1159#issuecomment-158168856,1,['avoid'],['avoid']
Safety,@droazen If we're willing to pay the cost of always calculating those fields on initialization and certain updates then we can just use a `boolean` and avoid the null check on read. Probably depends on the balance of reads/updates (although I would think that read is much more more common than update).,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235103083:152,avoid,avoid,152,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235103083,1,['avoid'],['avoid']
Safety,"@droazen It's not a hard fix but it's bad that we don't have any way of detecting it... The way we set system properties is very gross and error prone. We set them in 2 place in build.gradle AND in gatk-launch, and we have to be careful to duplicate the changes in build.gradle in gatk-protected (which I don't think we ever end up doing...)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2316#issuecomment-267121107:72,detect,detecting,72,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2316#issuecomment-267121107,1,['detect'],['detecting']
Safety,"@droazen Off the top of my head, we. * cache `log10(n)` and `log10(n!)` up to some large value.; * have a fast version of `log10SumLog10(double a, double a)` that works as follows: we want to compute `log10(10^a + 10^b)`. WLOG `a < b`, so this comes out to `a + log10(1 + 10^(a - b))`. I believe we cache the values of `log10(1 + 10^(x)` over a finely-spaced grid and round `a-b` to the nearest cached `x`. . There might not be anything else. There's a lot of stuff to keep calculations in log space for numerical stability but those don't avoid `log10()` and `Math.pow()`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2577#issuecomment-292584322:540,avoid,avoid,540,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2577#issuecomment-292584322,1,['avoid'],['avoid']
Safety,"@droazen Responded to comments, note that i added a further escape condition where getMatchingPriors is avoided altogether if the VCpriors list is empty. Remember that this method is in a performance sensitive part of the code so every little bit of speed counts.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5616#issuecomment-461597381:104,avoid,avoided,104,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5616#issuecomment-461597381,1,['avoid'],['avoided']
Safety,"@droazen Right - so the reason the pseudo code is formulated as something to detect deletions is because Laura had mentioned in the original ticket (and in the meeting we had last week) that she thought we could handle this case like it was a deletion. Quoting from her ticket:. > I still want to reject the 1/2 trans phased MNPs because they will complicate joint calling, but for a single sample with a single multinucleotide alt allele, it should behave nearly the same way as a deletion. So, yes the second part of the OR expression (we dropped length > 1, since that will be the case if number of mismatching bases > 1) detects MNP. And if the MNPs Laura describes above should be treated as deletions this may be an easy way to do so.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6500#issuecomment-603334267:77,detect,detect,77,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6500#issuecomment-603334267,2,['detect'],"['detect', 'detects']"
Safety,@droazen That is correct. It was a necessary step to avoid having to make a class equality check on the likelihoods object itself. @davidbenjamin I am open to suggestions if you have an idea of how better to encapsulate the separation between these two likelihood objects.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4865#issuecomment-395890633:53,avoid,avoid,53,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4865#issuecomment-395890633,1,['avoid'],['avoid']
Safety,@droazen Weeks away is a safe assumption. We're going through the process to approve releasing the code as Open Source. I'll see if there is a way to expedite.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1867#issuecomment-223063906:25,safe,safe,25,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1867#issuecomment-223063906,1,['safe'],['safe']
Safety,"@droazen a headerless SAMRecord would be convenient, for sure. They would be less efficient because we need to copy the contig and mate contig names onto every read but perhaps that cost is worth it to you. . There is a risk we may re-introduce something like the header when we move to more efficient versions of SAMRecord.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141235290:220,risk,risk,220,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141235290,1,['risk'],['risk']
Safety,@droazen are you sure thats not transient ? The first (connection timeout) failures that happened don't appear in previous builds that succeeded.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3179#issuecomment-311408683:66,timeout,timeout,66,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3179#issuecomment-311408683,1,['timeout'],['timeout']
Safety,"@droazen correct. ; Generally, one issue is that this slows down the docker image creation in a somewhat substantial way. Around 10 minutes currently. Half of this is unzipping the bundled jar, and another piece is some redundant gradle downloading that can be alleviated with cache shenanigans.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4955#issuecomment-400797666:220,redund,redundant,220,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4955#issuecomment-400797666,1,['redund'],['redundant']
Safety,"@droazen great! I faced this problem because I am using a self deployed Docker Swarm and so I was using a GATK version of few weeks ago. Sorry for opening an useful issue. For completeness, I used a VM with double of resources and it took 36,92 minutes, as predictable.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4479#issuecomment-369994289:257,predict,predictable,257,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4479#issuecomment-369994289,1,['predict'],['predictable']
Safety,"@droazen here are the error messages with gatk4.1.8.1 and gatk4.1.4.1:. 15:01:44.424 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/cm/shared/unil/software/8.3/GATK/4.1.4.1-GCCcore-8.3.0-Java-8/gatk-package-4.1.4.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Oct 09, 2020 3:01:45 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine. 14:28:22.786 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/cm/shared/unil/software/8.3/GATK/4.1.8.1-GCCcore-8.3.0-Java-8/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Oct 09, 2020 2:28:23 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6875#issuecomment-707085229:424,detect,detect,424,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6875#issuecomment-707085229,2,['detect'],['detect']
Safety,@droazen production informed me that recalibration has passed (and that now they have a different downstream problem...as predicted!),MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6625#issuecomment-641575854:122,predict,predicted,122,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6625#issuecomment-641575854,1,['predict'],['predicted']
Safety,@droazen sorry for a late response. I agree moving to java 17 would help. I do see that GATK itself is using the newer version of log4j but then its the transitive dependencies for the libraries used that bring in the older version of log4j. . this creates situations that the final compiled jar has both version of the log4j and this could create problems. . Gatk being a very useful tool gets integrated in multiple other tools and pipelines so in a way affecting the security posture of where its being used. The risk might be low being a standalone cli tool but its a very hard conversation with info security :) . May I ask for a ballpark ETA for the new version? Appreciate the work thats gone into this tool.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8215#issuecomment-1448897264:516,risk,risk,516,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8215#issuecomment-1448897264,1,['risk'],['risk']
Safety,"@droazen this behavior hasn't changed in the most recent GenomicsDB release. . Short recap: this happens because bcf codec doesn't support the 64 bit values that GenomicsDB is returning. Running with `--genomicsdb-use-vcf-codec` will resolve it. From our discussions in the office hours, I thought we had decided to change the behavior in htsjdk so that it doesn't try to decode the type if it doesn't recognize it. (maybe I should have filed https://github.com/broadinstitute/gatk/issues/6548 in htsjdk instead? I thought I was told to do in GATK, but its been long enough that I can't remember). Another possibility is to make `--genomicsdb-use-vcf-codec` the default - though I recall you had some potential performance concerns about that. Lastly, we could change GenomicsDB to throw out a warning if a 64 bit value is needed and we're using bcf codec. Of course, this would still require the user to (re)run with `--genomicsdb-use-vcf-codec` to avoid hitting the NPE (or whatever other failure would be hit if the NPE was changed to something a bit more meaningful).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6667#issuecomment-646167430:950,avoid,avoid,950,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6667#issuecomment-646167430,1,['avoid'],['avoid']
Safety,"@droazen, @lbergelson I have the following argument to the tool:. ```java; @Argument(fullName = ""read-tags"", doc = ""read tag names to recover""); public List<String> readTags = DEFAULT_READ_TAGS;; ```. On the command line I want to say. ```; java -jar gatk.jar …; ...; --read-tags RX; ```. and want readTags to be a singleton list. But in my test it's not parsing the arguments correctly---what am I doing wrong here?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7739#issuecomment-1081090913:134,recover,recover,134,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7739#issuecomment-1081090913,1,['recover'],['recover']
Safety,"@droazen, how do you interpret these results? I see one failure is a timeout, the other I'm not sure about but neither look really related to this code.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5378#issuecomment-458336902:69,timeout,timeout,69,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5378#issuecomment-458336902,1,['timeout'],['timeout']
Safety,@drozen htsjdk PR is [here](https://github.com/samtools/htsjdk/pull/968/). I'm running gatk tests locally as a sanity check.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3448#issuecomment-322799380:111,sanity check,sanity check,111,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3448#issuecomment-322799380,1,['sanity check'],['sanity check']
Safety,@eitanbanks I believe they're discussing CombineVariants. This just happens to be in the thread of CombineGVCFs bc there was some question of whether the two were redundant or not.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/16#issuecomment-66816695:163,redund,redundant,163,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/16#issuecomment-66816695,1,['redund'],['redundant']
Safety,"@erniebrau, the mesage and log file corresponds to the latest master (GKL 0.5.8); for the GKL 0.5.3 the error message is the following:. ```; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGILL (0x4) at pc=0x000000011dc557f4, pid=20586, tid=20739; #; # JRE version: Java(TM) SE Runtime Environment (8.0_60-b27) (build 1.8.0_60-b27); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.60-b23 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # C [libgkl_compression1417468606951982528.dylib+0x17f4] Java_com_intel_gkl_compression_IntelDeflater_resetNative+0x164; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /Users/daniel/workspaces/ReadTools/hs_err_pid20586.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #; ```. And the log file: [hs_err_pid20586.log.txt](https://github.com/broadinstitute/gatk/files/1264191/hs_err_pid20586.log.txt). Let me know if you need more information.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3532#issuecomment-326031152:170,detect,detected,170,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3532#issuecomment-326031152,1,['detect'],['detected']
Safety,"@fnothaft Thanks for this pr. I fixed the problem that was causing compilation to fail, but now we're getting real errors. ex:; ```; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1419.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1419.0 (TID 3897, localhost): java.lang.IllegalArgumentException: requirement failed: Failed when trying to create region 21 10006438 10006545 on null strand.; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4044#issuecomment-356012620:170,abort,aborted,170,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4044#issuecomment-356012620,1,['abort'],['aborted']
Safety,"@frank-y-liu In these situations it often helps to do the squash and the rebase onto upstream separately. First, make sure you've cancelled the failed rebase via `git rebase --abort` (`git status` should then show no rebase in progress). Then, rebase onto your current divergence point from master using the following command:. ```; git rebase -i 37f19b8732ef8690e79d646da5e1db97ed009ac1^; ```. (note the `^` at the end -- it's important!) When the pick/squash screen comes up, change all commits except for the first to ""squash"", then save. . With the squash completed, you can then do the rebase onto upstream as a separate step:. ```; git fetch upstream ; git rebase -i upstream/master; ```. If you still get conflicts this time, fix them by editing the affected files (`git status` will tell you which files are in conflict), running `git add` on them to mark them as resolved, and then `git rebase --continue`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1776#issuecomment-215604842:176,abort,abort,176,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1776#issuecomment-215604842,1,['abort'],['abort']
Safety,"@frank-y-liu Something has gone slightly wrong in this branch git-wise -- it looks like you've duplicated some commits from master, and the merge commits in your history imply that your git workflow needs some tweaking. In general, you want to always `rebase` rather than `merge` or `pull` (and avoid mixing the two, which can cause problems), since `rebase` produces a much cleaner history. Since you're working in a fork, you should create an ""upstream"" remote if you haven't already:. `git remote add upstream https://github.com/broadinstitute/gatk.git`. Then when you're working in a branch to which you've made one or more commits, and you want to update your branch with the latest changes from our master, do this:. `git fetch upstream`; `git rebase -i upstream/master`. This will bring up a screen allowing you to ""squash"" (combine) your work into a single commit that is suitable for merging into our master branch. If you do this with the current version of your branch, and select ""squash"" for all but the top commit, I believe you'll succeed in repairing your git history. . Note that after each `rebase`, when you want to push your changes to github you'll need to do a `git push -f` instead of a simple `git push`, since `rebase` changes your commit history. Try it out and let me know how it goes!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1776#issuecomment-215474210:295,avoid,avoid,295,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1776#issuecomment-215474210,1,['avoid'],['avoid']
Safety,"@frank-y-liu Thanks for the pull request! Looks good to me except for a very minor nitpick about tabs. In general we always use spaces. You should be able to set your IDE to autoconvert them. . I'm happy to merge without the cloud tests. It's a security hazard to let pull request from forks have access to those tokens, so they don't get passed to builds from forks. We separate the clouds tests explicitly so they can be skipped without breaking the rest of the tests when this happens. Your code should not effect any of the cloud functionality so I'm not worried if those tests didn't run. We can give you direct push access as well if that's more convenient for you. Then the tests will all run.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1761#issuecomment-213575978:254,hazard,hazard,254,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1761#issuecomment-213575978,1,['hazard'],['hazard']
Safety,"@gspowley Can you comment on whether this would be appropriate for a possible vectorized implementation in the GKL?. @davidbenjamin @vruano Can you comment on some of the existing approaches GATK takes to try to avoid expensive calls to `Math.log10()`, etc.? I think we rely (or used to rely) extensively on caching, correct?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2577#issuecomment-292557225:212,avoid,avoid,212,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2577#issuecomment-292557225,1,['avoid'],['avoid']
Safety,@gspowley Could you add the option to skip building the c library if an environment variable is set? maybe `GATK_SKIP_NATIVE_BUILD=true`. I think it's important that we have an opt in mechanism for avoiding the C compilation.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1504#issuecomment-185810475:198,avoid,avoiding,198,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1504#issuecomment-185810475,1,['avoid'],['avoiding']
Safety,@gspowley I think we should probably move it to `src/main/cpp`. We can revisit it when they update the java plugin to use the new software model. If it's not easy to avoid that hardcoded path then don't worry about it. I just figure the fewer hardcoded paths the better. I think it's OK to use sudo. It looks like the travis infrastructure has improved since we last looked into that. I just wanted to explain why we didn't just use sudo apt get and had that weird travis apt block.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1504#issuecomment-186443336:166,avoid,avoid,166,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1504#issuecomment-186443336,1,['avoid'],['avoid']
Safety,"@gspowley Yes, defining the env. var. avoided crash. It just says unable to load GKL. thx!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2302#issuecomment-265880530:38,avoid,avoided,38,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2302#issuecomment-265880530,1,['avoid'],['avoided']
Safety,"@gudeqing I think you are referrring to the calls to `GetPileupSummaries`, where we have both `-L` and `-V` arguments with the same variable. This is actually not redundant, though I admit it is clumsy. This is a consequence of `GetPileupSummaries` being written as a GATK `LocusWalker`, which is necessary for optimal performance.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7731#issuecomment-1154211085:163,redund,redundant,163,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7731#issuecomment-1154211085,1,['redund'],['redundant']
Safety,"@heliac2000 Thanks for the trace. I suspect that once the timeout initially occurs, the GATK process terminates, but the Python process is still trying to write back to it and gets the ""broken pipe"" return code. We already have plan to make the IPC/timeout more robust. In the meantime it would be interesting to see the last 50-100 lines of the journal file if it contains contains anything other than the repeated `Sending: [vqsr_cnn.score_and_write_batch(args, model, tempFile, fifoFile, 2, 2, '')` lines (which are expected).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4696#issuecomment-384300005:58,timeout,timeout,58,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4696#issuecomment-384300005,2,['timeout'],['timeout']
Safety,"@hliang I see that many of the tasks are failing and it looks like one of the executors crashed. To find the cause, you can check the error logs of these tasks through the web UI. I suspect increasing executor memory will fix the problem. Heartbeat timeouts usually occur when an executor JVM runs out of memory or requests more memory than the node will allow.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-313197140:249,timeout,timeouts,249,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-313197140,1,['timeout'],['timeouts']
Safety,"@ilyasoifer Looks like this user got tricked by some of the flow based annotations that don't work on their data. I would like to cut down on the risk that this happens for users. If we had more foresight I would advocate renaming all of the flow specific annotations to something like ""flowbased_#####"". How destructive would this be for your pipelines? . We have some appropriate checks in GATK for the flow-ness of the bam that give warnings more broadly about flow-based mode but we don't currently have any safeguards in the annotations. Thoughts?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8788#issuecomment-2073082102:146,risk,risk,146,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8788#issuecomment-2073082102,2,"['risk', 'safe']","['risk', 'safeguards']"
Safety,"@jamesemery - I think that the rebase is done. I'd like to have this in as soon as it can be, to avoid the extra-work of rebasing due to new tests or refactoring of them.... Thank you in advance!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3475#issuecomment-338990541:97,avoid,avoid,97,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3475#issuecomment-338990541,1,['avoid'],['avoid']
Safety,"@jamesemery - we should get this merge as soon as possible to avoid conflicts that pop up in every round of comments. Once this is in, I can go to the open PRs to point out the conflicts and the new structure (e.g., change the new tests to extend `GATKBaseTest`). I added a new commit addressing the issues and I will rebase to resolve conflicts again.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3475#issuecomment-340724214:62,avoid,avoid,62,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3475#issuecomment-340724214,1,['avoid'],['avoid']
Safety,"@jamesemery @droazen I've updated this branch to ensure all read and write paths to shared state in `GenotypeLikelihoodCalculators` is synchronized. I then wrote a little [test](https://github.com/broadinstitute/gatk/commit/3bb178746b1dd286f55ba77e6939e2104ced98d0) using `AlleleSubsettingUtils` to access `GenotypeLikelihoodCalculators` 10^6 times to see the effect of adding synchronization. R session (times are in millis):; ```; > without_sync = c(10166, 10049, 10306, 10059, 10165); > with_sync = c(10700, 10384, 9923, 10097, 10190); > t.test(without_sync, with_sync, paired=TRUE). 	Paired t-test. data: without_sync and with_sync; t = -0.70447, df = 4, p-value = 0.52; alternative hypothesis: true difference in means is not equal to 0; 95 percent confidence interval:; -542.5421 322.9421; sample estimates:; mean of the differences ; -109.8 ; ```. The p-value is not less than 0.05, so we can't reject the null hypothesis (that the mean times are the same). So adding synchronization doesn't seem to make any difference in this test. BTW, I noticed that `GenotypeLikelihoods` has synchronization, so there is some precedent for thread-safety using this means.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5071#issuecomment-426338479:1142,safe,safety,1142,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5071#issuecomment-426338479,1,['safe'],['safety']
Safety,"@jamesemery Back to you, at long last. I adopted your suggestion of a proper search that doesn't revisit already-seen vertices and came up with a better way of seeding the ""good"" subgraph that is safe from your STR concern. As far as code is concerned it's a total rewrite — you can pretend the first PR commit doesn't exist. The new criterion for seeding the search is chains with good log odds on both ends and which are incident on a vertex with multiple good out-edges or multiple good in-edges. The rationale is that the adjacency of two bad edges may have good log odds (Suppose a bad edge comes in and two bad edges come out. One is a new error on top of the original error and one is the continuation of the original error) but two have two outgoing edges with good log odds requires an actual real variant. On our M2 validations this essentially no effect on sensitivity and a mild reduction in false positives. I will leave it to you (or to me when I don't have to work like a vampire) to investigate how well it interacts with junction trees. As a first step I wrote a basic unit test for the basic pathology of the old method.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6520#issuecomment-624265441:196,safe,safe,196,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6520#issuecomment-624265441,1,['safe'],['safe']
Safety,"@jamesemery I agree - all access (read and write) to `GenotypeLikelihoodCalculators` instance variables needs to be synchronized to make it safe. I think it would be sufficient to make `getInstance()` and `calculateGenotypeCountUsingTables()` synchronized. @droazen, are you concerned about performance for the Spark case? For the walker version, presumably the access is single-threaded, and hence [uncontended, which is very cheap](https://books.google.co.uk/books?id=mzgFCAAAQBAJ&pg=PA230&lpg=PA230&dq=java+uncontended+synchronization+goetz&source=bl&ots=7W4J807faW&sig=YALE1qdWoAUELPqLRhIedz-bZ20&hl=en&sa=X&ved=2ahUKEwj4jJeko8zdAhXVFsAKHazkBrcQ6AEwB3oECAIQAQ#v=onepage&q=java%20uncontended%20synchronization%20goetz&f=false). Another option would be to maintain a separate instance of `GenotypeLikelihoodCalculators` per genotyping engine. The size of the table is ploidy * alleles, so not too large?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5071#issuecomment-423546586:140,safe,safe,140,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5071#issuecomment-423546586,1,['safe'],['safe']
Safety,"@jamesemery I will gladly review. If I understand the code change it seems like there was already basically the right logic to avoid this _but_ the code was neglecting to put the force calling alleles in a representation consistent with the output VCF. And the fix is simply to compute `forcedAlleles = AssemblyBasedCallerUtils.getAllelesConsistentWithGivenAlleles(givenAlleles, vc)` a bit upstream of where we were doing so previously. If I've got that right, this PR gets my :thumbsup:.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7740#issuecomment-1081342841:127,avoid,avoid,127,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7740#issuecomment-1081342841,1,['avoid'],['avoid']
Safety,"@jean-philippe-martin Can you comment on this error with your thoughts? Despite now doing a channel reopen on `UnknownHostException` in our fork of the NIO library, all reopens are failing, which implies that this error can't be recovered from via a simple retry. Could there be something wrong in our authentication setup?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-412906931:229,recover,recovered,229,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-412906931,1,['recover'],['recovered']
Safety,"@jean-philippe-martin I disagree that just because there are getters for the index, the index has to be there. It is already allowed to be absent for various reasons (via the special value `NO_ALIGNMENT_REFERENCE_INDEX`). Since use of the index instead of the name is mostly a performance optimization, I think we can get away with allowing records that have the name filled in but not the index. Relying on contig indices in general is a bad idea, as they are quite brittle, particularly when querying data from multiple sources. We purposefully moved away from using the indices in hellbender in favor of names when we migrated from `GenomeLoc` to `SimpleInterval`, and are, I think, willing to pay the extra cost of string parsing to avoid the subtle bugs that historically resulted from relying on the indices. This is a micro-optimization that probably isn't worth pursuing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141264817:737,avoid,avoid,737,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141264817,1,['avoid'],['avoid']
Safety,"@jean-philippe-martin I like your counter proposal in general for testing path integration. I think writing to GCS over NIO is an important enough feature that we should have at least 1 test in gatk that actually writes to a real GCS bucket in case there's ever an issue specifically with GCS (authentication issues are one potential problem I can imagine). . It seems like we should be able to design in a way that avoids collisions. What does `Files.createTempFile()` do with gcs? My guess is that it probably doesn't do the right thing, but maybe we could fix it so it would? Or use some sort of scheme with random UUID's like the methods in BucketUtils that we have already.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2558#issuecomment-332235140:416,avoid,avoids,416,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2558#issuecomment-332235140,1,['avoid'],['avoids']
Safety,"@jean-philippe-martin Sorry, I originally typed ""you can't compare `Iterator<CRAMRecord>` with `Iterator<SAMRecord>`"" , but I didn't quote it, so it displays as ""you can't compare Iterator with Iterator"". Anyway, it looks like you're not doing that. Thanks for adding the CRAM tests. Rather than adding separate data providers and methods for them though, can you just change the existing providers and methods to have an output extension and a reference (null is OK), and then thread those through the test code. I made a branch of your branch with a commit [here](https://github.com/broadinstitute/gatk/commit/5e52fca813e57065713852d12f80a7599fcbc3ce) to make sure that would work - it eliminates a lot of redundant code.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2558#issuecomment-332381114:708,redund,redundant,708,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2558#issuecomment-332381114,1,['redund'],['redundant']
Safety,"@jean-philippe-martin Thanks for adding the additional test, but by ""integration test"", I meant something that exercises an actual tool (which is why I mentioned SelectVariants) with a non-default provider, not another unit test that uses GCS. I suggested SelectVariants since I thought it would be easy:. > all the previous comments have been addressed with the exception of adding a SelectVariants integration test. It should be pretty easy to clone an existing case and change it use a non-default nio provider. I think this last test is redundant with the one you already added. My apologies if that was confusing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5378#issuecomment-455668135:541,redund,redundant,541,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5378#issuecomment-455668135,1,['redund'],['redundant']
Safety,"@jean-philippe-martin We've found that with the current NIO retry settings, we're still exhausting all retries and failing ~1-2% of the time on large runs. This PR increases the number of retries from 3 to 20, which should trigger waits of several minutes rather than several seconds in the later retries. It also increases the timeout settings in `BucketUtils.setGenerousTimeouts()` by quite a bit, also with the goal of allowing waits in minutes rather than seconds when necessary. We'll try running with this and see what happens, but in the meantime please review.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3072#issuecomment-307404453:328,timeout,timeout,328,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3072#issuecomment-307404453,1,['timeout'],['timeout']
Safety,"@jean-philippe-martin has added support for requester pays to gcloud. ; See https://github.com/GoogleCloudPlatform/google-cloud-java/pull/3406 . I've set up a new fork of the project at https://github.com/broadinstitute/google-cloud-java. I have a branch https://github.com/broadinstitute/google-cloud-java/tree/lb_update_pom_to_publish_to_orgbroad which I believe should make the changes necessary to run on dataproc and avoid https://github.com/GoogleCloudPlatform/google-cloud-java/issues/2453. However, if you rollback the dependencies the project no longer compiles. You can compile the nio-subproject, but the parent project can no longer build against the old dependencies. That makes me very nervous because it seems likely that we will encounter runtime errors if we substitute them. . JP created a small test case to reproduce the error and it seems like the dataproc team is looking at it. Hopefully they can resolve the issue and we can switch to the base library instead of needing to publish an additional sketchy version of it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4828#issuecomment-404322757:422,avoid,avoid,422,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4828#issuecomment-404322757,1,['avoid'],['avoid']
Safety,"@jjfarrell . After talking with @cmnbroad this afternoon, we'd like to ask you to perform an experiment to limit the scope where hunt down the issue. Is it possible for you to run `PrintReadsSpark` on the same cluster? That is, something similar to . ```bash; gatk --java-options ""-Djava.io.tmpdir=tmp"" \; PrintReadsSpark \; -R $REF \; -I $CRAM_DIR/$SAMPLE.cram \; -O hdfs:///project/casa/gcad/$CENTER/sv/$SAMPLE.cram \; -- \; --spark-runner SPARK \; --spark-master yarn \; --deploy-mode client \; --executor-memory 85G \; --driver-memory 30g \; --num-executors 40 \; --executor-cores 4 \; --conf spark.yarn.submit.waitAppCompletion=false \; --name ""$SAMPLE"" \; --conf spark.yarn.executor.memoryOverhead=5000 \; --conf spark.network.timeout=600 \; --conf spark.executor.heartbeatInterval=120; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5942#issuecomment-495357208:733,timeout,timeout,733,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5942#issuecomment-495357208,1,['timeout'],['timeout']
Safety,@jjfarrell Huh. I expected that sort of annoying delay from splitting on a cloud system but not on a hadoop one. Does running with splitting index avoid the delay?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4506#issuecomment-371292714:147,avoid,avoid,147,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4506#issuecomment-371292714,1,['avoid'],['avoid']
Safety,"@jkobject @jnktsj @lydiarck We have a prospective fix for this issue that at least avoids the crash: https://github.com/broadinstitute/gatk/pull/7513. It should be part of the next GATK release, or you can try it out yourself if you're comfortable building the GATK from source.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6289#issuecomment-948791097:83,avoid,avoids,83,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6289#issuecomment-948791097,1,['avoid'],['avoids']
Safety,"@jkobject This problem has to do with indels and predicted protein change sequences. I'm starting a refactor of how the predicted protein changes get created. When that's complete, this issue will be fixed. In the meantime, can you post the stack trace and share the example workspace you mention in #6289 ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6651#issuecomment-1181815133:49,predict,predicted,49,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6651#issuecomment-1181815133,2,['predict'],['predicted']
Safety,"@jonn-smith @LeeTL1220 The CNV test timeout was a temporary issue, but its been fixed. I'm pretty sure if you rebase on current master, it will go away.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4063#issuecomment-355795000:36,timeout,timeout,36,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4063#issuecomment-355795000,1,['timeout'],['timeout']
Safety,"@jonn-smith Does that happen in reality? That sounds like it would be a bug in the transcript data. If you detect that the transcript has no exons ""NA""?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5187#issuecomment-430779052:107,detect,detect,107,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5187#issuecomment-430779052,1,['detect'],['detect']
Safety,"@jonn-smith When you get a chance, would you mind quickly reviewing this `LocatableXsvFuncotationFactory` class for thread safety? The issue identified above could be legit",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7376#issuecomment-891237272:123,safe,safety,123,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7376#issuecomment-891237272,1,['safe'],['safety']
Safety,"@kachulis In the past, htsjdk had bugs that resulted in bad index files. Those checks are an attempt to try to detect and reject such files, since they can result in subtle downstream problems. The 1 and -1 allowances are a compromise for common cases that are out-of-spec, but legitimate. So it's a compromise between being too defensive and too aggressive. A value like -2147483647 winds up getting rejected.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7755#issuecomment-1095568749:111,detect,detect,111,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7755#issuecomment-1095568749,1,['detect'],['detect']
Safety,"@kgururaj @ldgauthier I'd propose that we add defensive code to detect this, as @kgururaj proposed above. Maybe throw with a message identifying the name of the field and the issue ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5045#issuecomment-408081250:64,detect,detect,64,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5045#issuecomment-408081250,1,['detect'],['detect']
Safety,@ksw9 can you run the vcf validator tool suggested by @komalsrathi in #5045 [here](https://github.com/broadinstitute/gatk/issues/5045#issuecomment-407476343)? The issue is primarily caused by mismatch in the field description in the VCF header and the data lines. @droazen can you comment on the [sanity check that I suggested here](https://github.com/broadinstitute/gatk/issues/5045#issuecomment-407501684)?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5113#issuecomment-413282882:297,sanity check,sanity check,297,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5113#issuecomment-413282882,1,['sanity check'],['sanity check']
Safety,"@kuangtianhui It looks like the bam index on `/home/wangh/kth/Mydata/NAM/NAM_7/Bam/7-99.sorted.markdup.bam` is older than the bam itself. This may indicate that the index is out-of-date with respect to the bam. I recommend that you reindex the bam using `samtools index`. The warning about the missing sequence dictionary is not a big deal, and you can safely process the VCF without a sequence dictionary in the header, provided you've manually confirmed that the VCF uses the same reference as your BAMs.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7228#issuecomment-831444745:353,safe,safely,353,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7228#issuecomment-831444745,1,['safe'],['safely']
Safety,"@kvinter1 In future, it's a good idea to wait for tests to pass before merging, otherwise you risk the potential penalty of having to buy the team beer if test fail once it's in master. Doc changes are pretty low risk, but you never know.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5104#issuecomment-412913465:94,risk,risk,94,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5104#issuecomment-412913465,2,['risk'],['risk']
Safety,"@lbergelson , 1 is not impossible, but it could turn out to be a bigger-than-expected project, because the bwa code, as I understand it, is accumulated through the years. For this specify error message we saw, the abort call is actually made by some low level code in bwa that once modified could throw away many other parts as well.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2123#issuecomment-243288136:214,abort,abort,214,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2123#issuecomment-243288136,1,['abort'],['abort']
Safety,"@lbergelson @droazen @kgururaj ; 1. I was playing around with the test codes in GATK and did not push GenomicsDB tests in this PR. Will push it in the next commit.; 2. This is at the top of our discussion list for next week. GenomicsDB interfaces use these JSON files today which contain input configuration, list of samples, mapping between sample IDs and TileDB row indexes and stream ids for the input VCFs. If this tool takes the list of VCFs and intervals as input, we'd have to recreate JSON files internally and pass it to GenomicsDB. I wanted to avoid this for now as we are thinking about overhauling the input methodology completely in GenomicsDB with protocol buffers, but this is going to take a while. Also, we need to decide what's the best way to maintain the callset mappings.; 3. Will let you know asap. -Kushal.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-277320579:554,avoid,avoid,554,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-277320579,1,['avoid'],['avoid']
Safety,@lbergelson @vruano @LeeTL1220 Would appreciate a quick review when you guys get a chance. (Want to avoid people wasting time on kebab-case updates to or conflicts with this code.),MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3935#issuecomment-350770988:100,avoid,avoid,100,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3935#issuecomment-350770988,1,['avoid'],['avoid']
Safety,"@lbergelson I agree that your result is undesirable. In fact, it's hard to imagine when you'd want it to work that way. I think folks need to be able to specify:; 1. ""starts within"" the interval so that scatter/gather can work and not generate redundant variants, since a variant can only start in one of a set of non-overlapping intervals; 2. ""overlaps with"" so that when calling a subset of the genome (e.g. for capture experiments) we can output all variants that involve our regions of interest ; 3. ""contained within"" because I suspect there are times you want (e.g. in SelectVariants) to be able to find only variants wholly contained in the region you are interested in",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6339#issuecomment-573089728:244,redund,redundant,244,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6339#issuecomment-573089728,1,['redund'],['redundant']
Safety,@lbergelson I'd prefer a targeted patch to the retry code in our fork as the lowest-risk option for now.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4888#issuecomment-396632448:84,risk,risk,84,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4888#issuecomment-396632448,1,['risk'],['risk']
Safety,"@lbergelson If you query for output after you've terminated the process, the query will fail immediately because the Futures will have been completed with a CancellationException when the pipes were broken by the termination. But I think even that might be subject to a race condition. Previously we were dependent on stdout/stderr for synchronization and error detection, but with the ack fifo and the python exception handler installed, we really aren't anymore. We do need to fix https://github.com/broadinstitute/gatk/issues/5100, and have a better logging integration strategy, but in general I think we should seek to eliminate all use of stdout/stderr except for advisory purposes. On a separate tangent, what I'd really like to do is unify the two PythonExecutors into a single one. All of these features I'm adding like profiling, version checking, logging integration etc., will have to be done in both of them otherwise.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5097#issuecomment-413575698:362,detect,detection,362,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5097#issuecomment-413575698,1,['detect'],['detection']
Safety,"@lbergelson It's an improvement in the sense that we had a bug where the spark tools were not doing the same filtering as their walker equivalents, and this is a quick fix for the bug that will also help avoid the problem in the future. I agree that the current design is problematic for multi-tool pipelines, but do you think we could address this in a separate ticket after alpha, and live with an imperfect design for now?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1159#issuecomment-158173582:204,avoid,avoid,204,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1159#issuecomment-158173582,1,['avoid'],['avoid']
Safety,@lbergelson Not sure we should merge this until we solve the intermittent timeout issue in the docker tests.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2804#issuecomment-305921296:74,timeout,timeout,74,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2804#issuecomment-305921296,1,['timeout'],['timeout']
Safety,"@lbergelson counter-proposal: since writing to a temp location in GCS would risk collisions if multiple people run the test, how about writing to JimFS instead? It's a RAM filesystem so each test machine gets its own, and it still requires the code to use the Path objects correctly since any conversion to File would fail. As a bonus, we do not incur Cloud charges and the test is much faster, so we can keep it as a unit test instead of an integration test.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2558#issuecomment-332078512:76,risk,risk,76,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2558#issuecomment-332078512,1,['risk'],['risk']
Safety,@lbergelson i think thread-safety code is gone. Is it worth the trouble pushing this code down to htsjdk?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/89#issuecomment-94350285:27,safe,safety,27,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/89#issuecomment-94350285,1,['safe'],['safety']
Safety,@lbergelson please take a look. I will use it in a future PR. Just trying to avoid large PRs.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3676#issuecomment-334855268:77,avoid,avoid,77,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3676#issuecomment-334855268,1,['avoid'],['avoid']
Safety,"@lbergelson you beat me because I was stuck trying to actually run a Picard tool in the integration test. (For future reference, that needs a workaround because the test running adds the ERROR level logging to all command lines and Barclay can't parse that for Picard tools for some reason.). The big reason I was using this instead of IntervalListTools is because the Picard version creates a terrible output file structure that I was having trouble capturing with a simple glob in WDL. I agree that the functionality here is largely redundant, but it was helping me get my workflow working faster at the moment.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5392#issuecomment-435894196:535,redund,redundant,535,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5392#issuecomment-435894196,1,['redund'],['redundant']
Safety,"@ldgauthier - Any thoughts on whether removing toString() from VariantAnnotation is safe - see comments above ? If not, we plan to remove it since there doesn't appear to be any code in GATK that relies on it (at least, no tests fail).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7041#issuecomment-775429828:84,safe,safe,84,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7041#issuecomment-775429828,1,['safe'],['safe']
Safety,"@ldgauthier ; hey Laura...; A new update regarding this ""topic""... ; As I said before (https://github.com/broadinstitute/gatk/pull/7725), when we were working with the ReblockGVCF from the snapshot you sent to us (https://console.cloud.google.com/gcr/images/broad-dsde-methods/US/gatk_subset_dragen_allele_frac@sha256:f5e93bda2278f1c999bd9def027c6851eeb098736b47a93469c524863b46c21f/details) and the JointGenotype pipeline (no Gnarly) using GATK 4.2.5, we could complete the analysis. As we received the instruction to use GATK lastest (4.2.6.1) we tried to run the entire pipeline using the latest one (since ReblockGVCF til JointGenotype)...Unfortunately, it seems that the latest ReblockGVCF hasn't the needed changes to work completely with the JoinGenotype Pipeline (without Gnarly), because we received the error below, once again. I think maybe we'll need to run Snapshop reblock with the newest GATK for the other steps. Please, let me know if you think it's safe to use this approach (snapshot Reblock - 4.2.3~, plus GATK 4.2.6.1 entire JG pipeline). Maybe this info can help regarding this open issue.. ```; 18:19:49.888 INFO GenomicsDBImport - Importing batch 1 with 50 samples; 18:19:53.652 erro NativeGenomicsDB - pid=15219 tid=15235 conflicting field description in the vid JSON and the VCF header of file: 20210421-006_stream; terminate called after throwing an instance of 'VCFAdapterException'; what():VCFAdapterException : Conflicting field length descriptors and/or field lengths in the vid JSON and VCF header for field LOD; Using GATK jar /gatk/gatk-package-4.2.6.1-local.jar; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7797#issuecomment-1108926001:967,safe,safe,967,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7797#issuecomment-1108926001,1,['safe'],['safe']
Safety,@ldgauthier @davidbenjamin please take a look. . Don't allow the number of lines changed intimidate you... (most are in test resource files). The first commit contains the actual main code changes. . The second and third commits update the test resources (where most of the changed lines come from) and test code. . The very last commit changes the default radius to 2... I was planning to set it to 0 since it is more parsimonious (less complex configuration) but it may well affect sensitivity and certainly changes the PL/QUAL values so I guess set the value two the current 2 (for PLs) is a safer and more conservative approach until we evaluate what is the optimal value for this parameter. . Perhaps @davidbenjamin would like to have a different default for Mutec. This is last minute change and may break some of the integration test so bear with me if that is the case. However I think you can start reviewing the code at this point.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6055#issuecomment-516992042:595,safe,safer,595,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6055#issuecomment-516992042,1,['safe'],['safer']
Safety,"@ldgauthier @lucidtronix I'm updating this with a proposed list of CNNScoreVariants issues I think need to be resolved before we can remove the `@Beta` tag (actually is currently marked `@Experimental`). Let me know what you think:. - https://github.com/broadinstitute/gatk/issues/4538 (Python factoring/PEP-8/code review); - factor python args handling (minimally factor out the inference args); - there is only one 2D test, which I think has no reads overlapping any of the variants; - we should add a test that specifies one or more intervals; - the tool currently adds standard VQSR header lines via addVQSRStandardHeaderLines, which is unnecssary; - integrate read downsampling; - determine/handle the failure mode when the user supplies a mix (of mismatched) 1D/2D arch and weights inputs. Other (not necessarily blockers):; - establish all defaults (weights/arch/etc) in Java code; - default arch is 1D - should this change to 2D ?; - see if we can remove the artificially small inference/batch sizes (1) used in the tests. I think we added these due to timeouts which should no longer be an issue.; - remove the `newExpectations` code paths in integration tests",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4540#issuecomment-429074231:1061,timeout,timeouts,1061,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4540#issuecomment-429074231,1,['timeout'],['timeouts']
Safety,"@ldgauthier @yfarjoun We have an update on this! We've identified the bug:. * When `AbstractFeatureReader.getFeatureReader()` tries to open a `.vcf.gz` that doesn't have an index, it returns a `TribbleIndexedFeatureReader` instead of a `TabixFeatureReader`, because `methods.isTabix()` returns false when an index is not present.; * `TribbleIndexedFeatureReader`, in turn, opens a Java vanilla `GZIPInputStream`, instead of the `BlockCompressedInputStream` that gets opened when you create a `TabixFeatureReader`.; * `GZIPInputStream`, in turn, has a *confirmed bug* filed against it in Oracle's bug tracker (see https://bugs.java.com/bugdatabase/view_bug.do?bug_id=7036144#), that it inappropriately relies on the `available()` method to detect end-of-file, which is never safe to do given the contract of `available()`; * As the final piece in the ghastly puzzle, implementations of `SeekableStream` in htsjdk do not implement `available()` at all, instead using the default implementation which always returns 0. As a result of this combination of bugs in Java's `GZIPInputStream` itself and bugs in htsjdk's `SeekableStream` classes, end-of-file can be detected prematurely when within 26 bytes of the end of a block, due to the following code in `GZIPInputStream.readTrailer()`:. ```; if (this.in.available() > 0 || n > 26) {; ....; }; return true; // EOF; ```. Where `n` is the number of bytes left to inflate in the current block. The solution is to replace all usages of the bugged `GZIPInputStream` with `BlockCompressedInputStream` in tribble in htsjdk (at least, for points in the code where the input is known to be block-gzipped rather than regular gzipped). For due diligence we should also implement `available()` correctly for all implementations of `SeekableStream` in htsjdk.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4224#issuecomment-360282461:739,detect,detect,739,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4224#issuecomment-360282461,3,"['detect', 'safe']","['detect', 'detected', 'safe']"
Safety,"@ldgauthier I'm about to submit a bug fix PR. In the line you found the `.intersect(region)` should be `.intersect(region.getPaddedSpan())`. The intersection is to avoid a bug where the requested trimmed padded region is bigger than the original padded region, but `intersect(region)` causes it to lie within the original unpadded region, which is unnecessary and probably harmful to sensitivity (the trimming cigar didn't hurt sensitivity, but I wonder if this mistake may have offset a net benefit that it should have created). I replicated @jemunro's error in the branch, fixed it (of course), and wrote an equivalent regression test that fails before and passes after the PR. The rest is just the usual annoying updating of integration test files, which as of right now I'm in the middle of.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6495#issuecomment-599885755:164,avoid,avoid,164,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6495#issuecomment-599885755,1,['avoid'],['avoid']
Safety,@ldgauthier Should we patch `GenotypeGVCFs` to detect reblocked input and throw a `UserException` with an explanatory message?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7437#issuecomment-906617099:47,detect,detect,47,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7437#issuecomment-906617099,1,['detect'],['detect']
Safety,"@ldgauthier Thanks for looking into this! Below is the info requested. Let me know if you need anything else. . I am not so sure that it is such a rare case. I have attached a DP histogram of all the GQ=0 for chr19. There are many more than I expected to see. . For the one SNP rs429358, GQ=0 occurred in 1% of the samples. This happens to be one of the 2 SNPs that are used to determine the APOE genotypes for Alzheimers Disease risk. These are all APOE 33. . Sample 1 gVCF record:; `chr19 44908684 . T <NON_REF> . . END=44908688 GT:DP:GQ:MIN_DP:PL 0/0:44:0:41:0,0,1097`. Sample 1: vcf output via gvcf: ; `GT:AD:DP:GQ:PL 0/0:41,0:41:0:0,0,1097`. Sample 1: vcf output without gvcf; `GT:AD:DP:GQ:PL 0/0:37,0:37:99:0,111,1236`. Here is the IGV screenshot.; ![sample1](https://user-images.githubusercontent.com/1960717/49054868-bad15d80-f1c3-11e8-8059-d26e2beb21a2.png). This is a histogram of the DP for each GQ=0 genotype on chr19 for 5000 samples. ![gq0_dp](https://user-images.githubusercontent.com/1960717/49055161-a3df3b00-f1c4-11e8-9acb-904667f78d38.png)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5445#issuecomment-441887865:430,risk,risk,430,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5445#issuecomment-441887865,1,['risk'],['risk']
Safety,@ldgauthier Thanks for the comments! I made the changes you requested and deleted the redundant test. Let me know if you have any more edits in mind.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5129#issuecomment-415436791:86,redund,redundant,86,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5129#issuecomment-415436791,1,['redund'],['redundant']
Safety,"@ldgauthier They are use-at-your-own-risk, though in general we try to help users with problems. However, in this case the report is so old, and `ReadsPipelineSpark` has changed so much since then, that it's unclear whether this is still an issue with the current version.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5481#issuecomment-592082423:37,risk,risk,37,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5481#issuecomment-592082423,1,['risk'],['risk']
Safety,@ldgauthier thank you so much for your detailed answer!. I have also prepared a ~20k shard file here:. https://github.com/EvanTheB/joint_call_shards. I just avoided known gene regions. Maybe this is pointless but feels a bit safer than splitting any-old-ware. Do you have any information about how the hg38 20k shards file was created?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5905#issuecomment-486906387:157,avoid,avoided,157,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5905#issuecomment-486906387,2,"['avoid', 'safe']","['avoided', 'safer']"
Safety,"@lucidtronix Specifically it would be useful to try this with various transfer/inference batch sizes now that the timeouts are gone. I tried with the original defaults (256/128), and got no timeouts, but the larger values seemed to result in longer runtimes.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4757#issuecomment-388100436:114,timeout,timeouts,114,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4757#issuecomment-388100436,2,['timeout'],['timeouts']
Safety,"@lucidtronix Whats the motivation for wanting to keep it ? I think we really want to avoid it if at all possible, but if there is a reason its necessary let us know and we can discuss.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4465#issuecomment-369244401:85,avoid,avoid,85,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4465#issuecomment-369244401,1,['avoid'],['avoid']
Safety,"@lucidtronix https://github.com/broadinstitute/gatk/pull/4218 is merged now so you should be able to rebase this on master. It looks like when you squashed you left in some of the timeout changes, so you'll have to resolve the resulting conflicts in favor of master.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4097#issuecomment-360524534:180,timeout,timeout,180,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4097#issuecomment-360524534,1,['timeout'],['timeout']
Safety,@lucidtronix looks like I got an intermittent 10-minute timeout failure in the CNN WDL test.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5307#issuecomment-430040420:56,timeout,timeout,56,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5307#issuecomment-430040420,1,['timeout'],['timeout']
Safety,"@magicDGS I think the context for this issue is the Pathogen sequence detection tools, which are Spark tools. `CountingReadFilter` isn't inherently reducible, and shouldn't be used in a Spark tool. Also, I can't say love the idea of using a filter as a control flow mechanism; but I don't have a better idea (maybe a separate tool as @mwalker174 mentioned above ?). One other note. As things currently stand, wrapping a WellFormedReadFilter in a counting filter (which we currently do for walkers) wouldn't give summary counts at the granular level, because WellFormedReadFilter is composed from non-CountingRead filters. Wrapping it yields a single, outer level summary/count. It would be easy enough to create one though that provided detailed summary/counts though.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3454#issuecomment-323738847:70,detect,detection,70,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3454#issuecomment-323738847,1,['detect'],['detection']
Safety,"@magicDGS I'd strongly prefer not to introduce a read filter descriptor hierarchy if we can avoid it, as it will be tricky to get right, and add complexity. We definitely need to be able to extend the package list used by the descriptor to find plugins, but as you point out we'll be able to use the configuration mechanism for that. For before/after-analysis filters, I expect that we'll just add that directly to the existing plugin once we resolve https://github.com/broadinstitute/gatk/pull/2085 (which I hope to get to this week). I think the rest of the cases can be addressed by overriding makeReadFilter and providing custom behavior of filter merging. If this turns out to be something truly common, we could consider allowing the tool to inject an argument collection into the plugin.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2353#issuecomment-274970451:92,avoid,avoid,92,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2353#issuecomment-274970451,1,['avoid'],['avoid']
Safety,"@magicDGS That sounds perfectly fine to use #3614 . Regardless, I think you are safe to merge unless that quick test is worrisome.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2321#issuecomment-332609705:80,safe,safe,80,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2321#issuecomment-332609705,1,['safe'],['safe']
Safety,"@magigDGS There are 4 separate issues in here, and I have slightly different feelings about each of them. Some comments:. - Removing the RNA string seems fine.; - I deliberately left the GATK test program group in because even though there is a Picard one, its harmless, and easier for people to find.; - I have reservations about exposing and sharing the super category maps. The Picard docgen process is pretty much unused and unmaintained at this point. The `getSuperCategoryMap` mthod really shouldn't be public, and it may even be removed in the near future. The GATK supercategory map ""truth"" should be defined by GATK.; - I'm reluctant to make the GATK `getSuperCategoryMap` public, because I don't think it can have any kind of useful contract. Its tied to the GATK doc templates and doc process, which we need to be able to change freely. It seems much safer for ReadTools to define it's own set categories (the overlap would probably pretty minimal I think - maybe just ReadFilters, right ?).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4247#issuecomment-360832234:862,safe,safer,862,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4247#issuecomment-360832234,1,['safe'],['safer']
Safety,"@marchoeppner `MarkDuplicatesGATK` was removed because it had fallen out-of-date with respect to the version in Picard, and as an unmaintained tool was in our view not safe for use, and was causing confusion for our users. The loss of CRAM support is an unfortunate side effect of its removal. We've been doing a lot of work on our parallel version of `MarkDuplicates`, however, which is called `MarkDuplicatesSpark`. This version is fully up-to-date with respect to the Picard version, can run much faster than the Picard version when multiple cores or multiple machines are available, and will fully support CRAM in the future. CRAM support in that tool will come as a side effect of our migration to the new Disq library (https://github.com/disq-bio/disq), which is scheduled to happen within the next few months. In the meantime, I'd suggest continuing to request the Picard community to add CRAM support to their version. It's likely not a lot of work, and may simply require passing the reference through to the reader class, which could be a ~1 line change!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5218#issuecomment-424882567:168,safe,safe,168,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5218#issuecomment-424882567,1,['safe'],['safe']
Safety,"@maxim-h Thank you for this follow-up information. Please note that if you have any pop-up blockers enabled or are browsing in ""incognito"" or ""safe mode,"" you may encounter these problems. Please ensure you have disabled any blockers and are trying to sign in through a regular browser. . I hope this helps! Please let me know if this leads you to success. Anthony",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8115#issuecomment-1332748705:143,safe,safe,143,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8115#issuecomment-1332748705,1,['safe'],['safe']
Safety,"@mbabadi Not sure - we'll need to discuss how to do handle that. It looks like the docker image already has g++ installed, which is a start. Having said that, the docker image is getting pretty huge rapidly... BTW, do you know if there is way to programmatically query theano to determine whether it will compile the graph, vs running python/numpy ? I know it prints out a message, but it would be nice if we could detect that from theano directly.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3912#issuecomment-349733375:415,detect,detect,415,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3912#issuecomment-349733375,1,['detect'],['detect']
Safety,"@mcovarr @koncheto-broad Had a talk today with @KevinCLydon about this PR, and he made a convincing case that the pgen project is more likely to require additional stuff from the GVS branch than from gatk/master. To avoid a nightmare scenario of constantly having to request additional transplants from the GVS branch, I'm on board with the idea of having Kevin try to work off of the GVS branch for the pgen project for now, provided that we can get timely rebases of `ah_var_store` onto master if we need them.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8355#issuecomment-1634847183:216,avoid,avoid,216,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8355#issuecomment-1634847183,1,['avoid'],['avoid']
Safety,"@mcovarr Huh. I'm surprised that didn't fail the docker build. I would have expected the command failing to abort it, but I guess that's not the case. . This step is not really necessary. If those files aren't checked out at build time gradle will do it for you, so that's why it didn't seem to cause any problems.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7806#issuecomment-1109042816:108,abort,abort,108,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7806#issuecomment-1109042816,1,['abort'],['abort']
Safety,"@mepowers to clarify, this is not a “long read”, rather a short read with a long indel. For cases like this, I’d expect if the intel HMM fails, GATK should fall back to the Java one automatically. Or can you detect what’s “too long” and use the Java version?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6733#issuecomment-674247944:208,detect,detect,208,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6733#issuecomment-674247944,1,['detect'],['detect']
Safety,"@mlathara As suggested, I removed all MNPs from normal vcf files and changed the bed file format and it worked. But it is not going beyond chromosome 1. Here are the stack trace:. 15:44:02.495 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/akansha/vivekruhela/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 16, 2021 3:44:02 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 15:44:02.750 INFO GenomicsDBImport - ------------------------------------------------------------; 15:44:02.750 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.1.9.0; 15:44:02.750 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 15:44:02.750 INFO GenomicsDBImport - Executing as akansha@sbilab on Linux v4.4.0-169-generic amd64; 15:44:02.751 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_265-8u265-b01-0ubuntu2~16.04-b01; 15:44:02.751 INFO GenomicsDBImport - Start Date/Time: January 16, 2021 3:44:02 PM IST; 15:44:02.751 INFO GenomicsDBImport - ------------------------------------------------------------; 15:44:02.751 INFO GenomicsDBImport - ------------------------------------------------------------; 15:44:02.751 INFO GenomicsDBImport - HTSJDK Version: 2.23.0; 15:44:02.751 INFO GenomicsDBImport - Picard Version: 2.23.3; 15:44:02.752 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 2 ; 15:44:02.752 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 15:44:02.752 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 15:44:02.752 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 15:44:02.752 INFO GenomicsDBImport - Deflater: IntelDeflater; 15:44:02.752 INFO GenomicsDBImport - Inflater: IntelInflater; 15:44:02.752 INFO GenomicsDBImport ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7037#issuecomment-761558811:508,detect,detect,508,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7037#issuecomment-761558811,1,['detect'],['detect']
Safety,"@mlathara Our primary use case involves calling variants on a constantly growing large dataset of WGS and WXS data. As you probably realize, the CombineGVCFs/GenomicsDBImport step is incredibly time consuming, and scatter/gather is pretty much essential to make these operations work in any halfway reasonable period of time. I have off-hand heard people from the broad mention large WXS datasets, and keep in mind we're working mostly w/ WGS. Regarding processing: our main downstream use right now is GenotypeGVCFs, and yes we expect to run that scatter/gather as well. I agree that in principle we could maintain these data as a folder of workspaces. In fact that was my original plan before I realized the GenomicsDB workspace already is essentially a folder of per-contig folders. The reason I like the solution of copying around the folders is b/c our end product is in an official file format that tools understand how to use. . A related point, before we decided to try GenomicsDB, my plan was to create a scheme (""file format"") that would allow our code to better operate on a folder of per-contig CombinedGVCF file. I would probably have written out a top-level JSON file that served the same purpose as the JSON files in a GenomicsDB workspace. As noted above, GenomicsDB is essentially already doing this for me. To the question about usage and support: perhaps that ways to think about this would be interval-based split and merge tools for GenomicsDB workspaces? This would obscure the internal structure of the workspace from the user (even if they basically just to folder copying). The split tool should be really simple and not have many caveats. The merge tool could have a lot of limits on what kind of workspaces can or cannot be merged. Perhaps it could do sanity checking on the JSON files to make sure they're compatible, and then copy the folders into this new merged workspace?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6620#issuecomment-635338868:1779,sanity check,sanity checking,1779,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6620#issuecomment-635338868,1,['sanity check'],['sanity checking']
Safety,"@mlathara Travis is totally borked. The timeout may or may not have to do with the fact that everyone is rerunning tests since they're failing sporadically. This looks good to me, so you can merge once tests pass. Also, if you send @droazen some notes on the new functionality to add to the release notes that would be a big help.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5970#issuecomment-519952186:40,timeout,timeout,40,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5970#issuecomment-519952186,1,['timeout'],['timeout']
Safety,"@mlathara We were actually trying to craft a boolean expression to detect MNPs, not deletions. The one we came up with was: ref and alt are the same length, the length is > 1, and the number of mismatching bases is > 1. The last condition (mismatching bases > 1) is necessary to rule out padded SNPs.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6500#issuecomment-603299769:67,detect,detect,67,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6500#issuecomment-603299769,1,['detect'],['detect']
Safety,"@mlathara thanks, but one additional question. what occurs is --consolidate runs on a folder that contained additional, redundant/identical arrays? I ask b/c some of our jobs finished already on folders that I am fairly certain contained redundant arrays, but they completed w/o error. note: i think the second bullet in your answer above addresses this, saying they will be consolidated, but I would like to be absolutely certain.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6910#issuecomment-722731714:120,redund,redundant,120,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6910#issuecomment-722731714,2,['redund'],['redundant']
Safety,"@mwalker174 @asmirnov239 decided not to tackle this yet. There are a few options: 1) warn and/or filter out such contigs in PreprocessIntervals (not a fan of this), 2) filter in FilterIntervals, 3) warn/fail/filter a little earlier at DetermineGermlineContigPloidy (and also GermlineCNVCaller to be safe), 4) warn/filter at PostprocessGermlineCNVCalls, 5) fix the theano code to treat such contigs specially (haven't looked closely at it, but probably has something to do with patching the foward-backward code to handle such cases). Probably option 5 is the right answer, but only if the inferences for such single-interval contigs are at all meaningful. Otherwise I'm inclined to do option 2 and add some warnings/exceptions downstream. Might be other options as well.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5852#issuecomment-550352252:299,safe,safe,299,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5852#issuecomment-550352252,1,['safe'],['safe']
Safety,"@nalinigans OK, thanks. I'll try to get docker going w/ intellij. to the original question/bug: is there anything inherent about GenotypeGVCFs with GenomicsDB as the source vs. a gVCF as the source that one would expect to change how GATK determines the reference allele? I have not actually run this yet, but if I'm correct on the problem this probably should repo it (an addition to GenotypeGVCFsIntegrationTest):. ```. @Test(timeOut = 1000000); public void testGenotypeGVCFsWithGenomicsDbAndForceOutput() throws IOException {; final File input = CEUTRIO_20_21_GATK3_4_G_VCF;; SimpleInterval interval = new SimpleInterval(""20"", 1, 11_000_000);; final File tempGenomicsDB = GenomicsDBTestUtils.createTempGenomicsDB(input, interval);; final String genomicsDBUri = GenomicsDBTestUtils.makeGenomicsDBUri(tempGenomicsDB);. File expected = getTestFile(""CEUTrio.20.gatk3.7_30_ga4f720357.expected.vcf"");; String reference = b37_reference_20_21;. runGenotypeGVCFSAndAssertSomething(genomicsDBUri, expected, Arrays.asList(""--"" + GenotypeGVCFs.FORCE_OUTPUT_INTERVALS_NAME, ""20:10-20""), GenotypeGVCFsIntegrationTest::assertVariantsContextsHaveNonAmgibuousRefs, reference);; }. ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7005#issuecomment-754120858:428,timeOut,timeOut,428,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7005#issuecomment-754120858,1,['timeOut'],['timeOut']
Safety,"@nalinigans the set of jobs was ultimately able to work. While our issues with duplication and bad fragments are most likely our fault, any future improvements to GenomicsDbImport to detect and/or fix these would be useful. Thanks for all your help on this front.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6910#issuecomment-733064796:183,detect,detect,183,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6910#issuecomment-733064796,1,['detect'],['detect']
Safety,"@nh13 As a first step I'd suggest filing a ticket against the GKL (https://github.com/Intel-HLS/GKL) so that Intel engineers can have a look (but leave this GATK ticket open so that we can track it here as well). . This will be difficult to debug without a test case that Intel can run to reproduce the issue on their end. Does the crash only occur with this one particular sample, or have you seen it on more than one sample? If you could get to the point where you can reproduce it on a shareable bam snippet, that would obviously maximize the chances of this getting fixed. Intel is currently (at our request) doing a pass on the GKL with `valgrind` to find and fix memory safety issues (https://github.com/Intel-HLS/GKL/issues/107), so we expect the next GKL release to fix a bunch of ""use after free""-type errors. Maybe they'll get lucky and fix this one as well. Timeline for the release is within the next ~2-3 months. . After that we've asked them to test the GKL with long reads data, which is also known to trigger crashes like this (https://github.com/Intel-HLS/GKL/issues/105). If the problem in your case is that you've exceeded some hardcoded length limitation, the tests on long reads data might reveal the problem.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6733#issuecomment-667189113:676,safe,safety,676,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6733#issuecomment-667189113,1,['safe'],['safety']
Safety,"@nh13 Thank you for clarifying. As of today GKL does not auto-detect when there's a read that's ""too long,"" ie a read length we haven't validated with GKL. We should be able to build that into our pending release. I agree we should also make sure that if the GKL pairHMM fails, the JAVA version is called instead. @Kmannth @droazen let's discuss this in our next sync.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6733#issuecomment-674250782:62,detect,detect,62,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6733#issuecomment-674250782,1,['detect'],['detect']
Safety,@nh13 Thank you for your question. I agree that it is particularly painful to lower the mapping quality currently since there are two seperate arguments `--mapping-quality-threshold` and `--minimum-mapping-quality`. In order to lower the threshold they must BOTH be set otherwise nothing will change and low MQ reads will not be handled. This is necessary due to the order in which Barclay parses fields and the way this interacts with the read filte. We either need flags to be able to fill multiple fields in different places or to use reflection to extract the mapping quality threshold to be used for the assembly engine based on the read filters used after the fact which runs into issues if the user has disabled the mapping quality filter. These both still fail to capture the case where you want to explicitly use a different threshold for active region detection and for calling. . Would it be helpful if the new argument were renamed to something like `--mapping-quality-threshold-for-genotyping`?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7034#issuecomment-758852673:862,detect,detection,862,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7034#issuecomment-758852673,1,['detect'],['detection']
Safety,"@owensgl Sorry you're running into problems. We typically speed up the process by running multiple GenotypeGVCF processes in parallel, subsetting by genomic intervals. GenotypeGVCFs isn't really multicore, you'll probably be best off giving each process 1 or 2 cores. (You'll see better performance with 2 since java has parallel garbage collection, but it might be more cost effective to run twice as many slower processes...) If you do that you'll want to run with `--only-output-calls-starting-in-intervals` enabled in order to avoid problems on the edges of intervals. . Things tend to bog down with many highly multi-allelic sites. If you have a population with very high diversity you may be hitting lots of sites like that. I'm not sure why it's as slow as you say it is though. It should be faster than 800bp / 30 minutes even with old qual. If you could provide a subset of your data we might be able to profile and see if there's some pathological case we're not handling well. I believe new-qual handles multi-allelic sites more efficiently which I suspect is why it's going faster.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4161#issuecomment-358054994:531,avoid,avoid,531,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4161#issuecomment-358054994,1,['avoid'],['avoid']
Safety,"@pieterlukasse Yeah - I'm planning on updating some of the Funcotator core to be more permissive for input data types and to fix a few long-standing bugs, but have been unable to do so because of other high-priority tasks (as @lbergelson said). The output formats are pretty well-established, so I don't think there's any risk in writing an additional parser. However if you simply want to view the outputs, you can render the annotations in `MAF` format and that will produce a `MAF` (TSV) file that is much more easily viewed / parsed.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8154#issuecomment-1379018376:322,risk,risk,322,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8154#issuecomment-1379018376,1,['risk'],['risk']
Safety,"@ronlevine I know this a port from gatk3, but I think theres a bit of refactoring that can be done. It seems like it's more complicated than it needs to be. Could you take a look and see? In particular I'm not sure why things get converted to a bitset, it looks like you should just be able to derive the indecies directly and avoid creating a bitset. If I'm missing some detail and it can't be simplified let me know.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1852#issuecomment-242102049:327,avoid,avoid,327,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1852#issuecomment-242102049,1,['avoid'],['avoid']
Safety,"@ronlevine just pointed out this is redundant w/ another issue (see above). for that issue you guys requested I write a unit test, which I did this morning. Feel free to apply that or not. It's attached to that thread as a patch (sorry, dont have a good local enlistment right now). It's nothing special, but tests are rarely a bad thing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3252#issuecomment-314527518:36,redund,redundant,36,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3252#issuecomment-314527518,1,['redund'],['redundant']
Safety,"@rsasch my thought was to keep it around for now, but in the future we could add a step that removes it once we know the samples are safely loaded. It could also be that we get rid of the ""is_loaded' flag in sample_info instead since this data is more detailed…",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7573#issuecomment-983938451:133,safe,safely,133,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7573#issuecomment-983938451,1,['safe'],['safely']
Safety,"@samuelklee DRAGEN STRE model doesn't actually make any alterations to the smith waterman parameters or how they work, it just works by adjusting the indel gap penalties that are used for the PairHMM. At one point we were concerned about SW parameters being different with dragen but as it turns out the biggest visible effect of the SW parameters on the output (the alignment we perform after haplotypes discovery) is irrelevant since they don't realign their reads internally. We kept the default gatk alignment behavior and thus the SW parameters that are used (for dangling head recovery which I believe are the old arguments) still match. As far as unifying the parameters I suspect it could be done though one wonders if there aren't risks where the different contexts in which we use the parameters will not perform as well with a unified set. Speculation on my part though. I agree with David that we should be cautious about making changes that will affect the HaplotypeCaller before November. . I support including an argument in any case (possibly multiple) to include the SW parameters. I would actually advocate we read these files in as tables of parameters where you simply point to on the command line to configure new parameters.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6863#issuecomment-705611993:583,recover,recovery,583,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6863#issuecomment-705611993,2,"['recover', 'risk']","['recovery', 'risks']"
Safety,"@samuelklee I think it's right for what we're doing. We mount the test data as `/testdata` and then create a symlink from src/test/resources to /testdata to provide it to the test files. It seems to work.; ; I'm not clear what they get more of the other way around. More tests? Are they using our create docker script? Or our travis file? Or something else? I think we might just be able to just directly mount test data to src/test/resources and avoid the symlink, but I probably had a reason when I set it up that way... I think this is a non-issue unless they can provide more information.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3730#issuecomment-339439156:447,avoid,avoid,447,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3730#issuecomment-339439156,1,['avoid'],['avoid']
Safety,"@shuaiwang2 Hi, we don't currently support indexes that long. We use a bai index for bams and tabix for vcf which only support up to 512 M. You need to use a CSI index for references that large but we don't support writing those. (Reading them is weird, I think we can read BAM csi indexes but not VCF ones). . It might be possible to work around this issue by setting `--create-output-variant-index false`, although downstream gatk tools would need an index if you're sharding them. Otherwise I recommend splitting your chromosomes into two separate parts and calling on the split chromosomes. Splitting along a long region of N's should be a safe way to avoid missing any useful calls. (The telemere might be a good spot unless you have a T2T reference.). . We should probably improve that error message to make it clear what the problem is.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8192#issuecomment-1422828609:644,safe,safe,644,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8192#issuecomment-1422828609,2,"['avoid', 'safe']","['avoid', 'safe']"
Safety,"@sooheelee By the way, can we avoid using ""ACNV""? Let's just call this tool ModelSegments to distinguish it from the old GATK CNV -> GATK ACNV pipeline.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382838158:30,avoid,avoid,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382838158,1,['avoid'],['avoid']
Safety,"@sooheelee Do we have a known case in GATK4? I thought we had some code to avoid it but when I recently looked I couldn't find it. So perhaps it was a figment of my imagination, in which case it's certainly an issue.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3061#issuecomment-350088492:75,avoid,avoid,75,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3061#issuecomment-350088492,1,['avoid'],['avoid']
Safety,"@sooheelee I can't speak for CNV, but there isn't any general reason to prefer Picard interval lists in GATK. There was previously an issue with parsing interval queries that used contig names that contained "":"", but thats fixed now. The only time we prefer a Picard list is the theoretical case were you use a query interval against a sequence dictionary that contains contigs that make that query ambiguous (hg38 is not one of those). GATK will detect and reject such a query and suggest using a Picard interval file to disambiguate it. @magicDGS I'm not sure how/if writing tests against existing files in the repository will be useful. I want to restate that we don't want to take ports of these tools if they're marked `@Experimental `or `@Beta` because they haven't been validated, or don't have good test coverage. We need to find a way need to have valid tests so they'll be production ready.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-371528161:447,detect,detect,447,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-371528161,1,['detect'],['detect']
Safety,"@sooheelee I don't see, at least not immediately, how artifacts would be more of a risk than they are already. If anything I could see this making it a bit harder for false positives to get by.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4647#issuecomment-380521889:83,risk,risk,83,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4647#issuecomment-380521889,1,['risk'],['risk']
Safety,"@sooheelee I have a suggestion regarding categories. Can we change ""Contamination"" to ""Metagenomics"" and perhaps move the ""CalculateContamination"" tool to the ""Diagnostics and Quality Control"" category? . IMO, contamination has a connotation of introducing foreign matter unintentionally. Strictly speaking, PathSeq is not just for detecting sample contaminants but also endogenous organisms in various biological sample types (like stool or saliva). I think users with metagenomic data might overlook this if they are labeled as being for ""contamination.""",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-349089820:332,detect,detecting,332,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-349089820,1,['detect'],['detecting']
Safety,"@sooheelee I think this was answered above by @ldgauthier and @kgururaj. The warnings are safe to ignore for best-practices workflows, as @kgururaj mentioned above in https://github.com/broadinstitute/gatk/issues/2689#issuecomment-370295035. And as @ldgauthier explained in https://github.com/broadinstitute/gatk/issues/2689#issuecomment-371500366, the annotations in question either get recalculated in `GenotypeGVCFs` or are obsolete.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2689#issuecomment-431168544:90,safe,safe,90,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2689#issuecomment-431168544,1,['safe'],['safe']
Safety,@sooheelee Let's take this discussion offline to avoid introducing noise into this dev issue.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/317#issuecomment-170653787:49,avoid,avoid,49,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/317#issuecomment-170653787,1,['avoid'],['avoid']
Safety,"@sooheelee That's a pitfall we haven't solved yet. I think we'll eventually infer the value locally from the AC of nearby variants in the germline resource. Until then, I would use the smaller value i.e. the one for coding regions. The only possible harm would be a very few rare germline events that aren't in gnomAD, whereas if you set it too high you risk filtering true somatic events.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4366#issuecomment-363851445:354,risk,risk,354,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4366#issuecomment-363851445,1,['risk'],['risk']
Safety,"@sooheelee We plan to have internal pilots running by then but haven't discussed releasing to the public in an beta or full release yet. We will have some unsupported WDL pipelines and light documentation available for external users, and we can provide benchmarking stats courtesy of our collaborators. It's probably safer to tag the tool as 'alpha' or 'beta' for the January release, though. We should be ready for full release sometime in the first half of next year, I expect.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3769#issuecomment-341538906:318,safe,safer,318,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3769#issuecomment-341538906,1,['safe'],['safer']
Safety,@takutosato The one test failure occurs because when there is low normal coverage germline risk might get triggered too often. I might need to adjust the default value but it won't affect the code otherwise.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4690#issuecomment-383357595:91,risk,risk,91,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4690#issuecomment-383357595,1,['risk'],['risk']
Safety,"@tedsharpe Thanks for checking. In general I've seen the CPB tends to help a lot when reading through long contiguous stretches of BAM file and less when doing anything on smaller or fragmented data. I'm surprised it didn't make any difference here, but seems like it doesn't so that's fine. I've seen catastrophic interactions between insufficiently buffered index inputs with it disabled where it ended up performing an http request for every byte, but hopefully that's avoided just by using a buffered reader for it. . I have a plan to someday enable the more intelligent -L aware prefetcher that will use the list of actual positions of interest to buffer more intelligently, but that's not happening on any specific schedule.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8031#issuecomment-1358245320:472,avoid,avoided,472,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8031#issuecomment-1358245320,1,['avoid'],['avoided']
Safety,"@tedsharpe Thanks for your patience! Review complete for now. I have a lot of stylistic comments and a few request for refactoring / some redesign of what appear to me to be redundant classes. Some stylistic comments:. I didn't comment everywhere that I saw it, but we try to avoid any unbracketed control structures. Even 1-liners should have brackets unless they're so short that the statement fits on the same line as the predicate. We typically have starting brackets at the end of the line though instead of on their own line, which saves some pain associated with single line brackets. I.e. gatk typically uses. ```; if ( something ) {; doThing; } else {; otherThing; }; ```. rather than. ```; if ( something ) ; {; doThing; }; else ; {; otherThing; }; ```. 2) You use a lot of raw iterators, which is fine and is necessary in many cases. In other cases those operations can be written much more succinctly with either a for-each loop, or a stream. i.e. . ```; List<Integer> values;; Iterator itr = iterable.iterator();; while(itr.hasNext()){; Element elem = itr.next();; int value = someFunction(elem); if ( value > SOME_CONSTANT) {; values.append(value); }; }; return values;; ```. can be . ```; return StreamSupport.stream(iterable.spliterator, false); .map( elem -> someFunction(elem)); .filter( value -> value > SOME_CONSTANT ); .collect( Collectors.toList()); ```. We should probably add a utilty function to convert an iterator to a stream directly so we can stream iterators easily even if there is no associated iteratable. . 3) The tools need tests. This is important. 4) It would be good to think about how the tools can be composited into a spark pipline and run without writing intermediate files. . 5) Bitwise operations are a rarity in GATK and many of our users will not be very comfortable with them. Please avoid bit twiddling tricks when possible. When it's not possible (i.e. when you are performing tricks to treat a long as a set of byte pairs) please add detailed explanat",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1435#issuecomment-172985394:174,redund,redundant,174,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1435#issuecomment-172985394,2,"['avoid', 'redund']","['avoid', 'redundant']"
Safety,@tomwhite I ran your branch manually on gcs and get a new error which I believe is a GCS NIO bug that we discussed in https://github.com/samtools/htsjdk/pull/724. . ```; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1899); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1913); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:912); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDD,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2450#issuecomment-285797337:362,abort,abortStage,362,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2450#issuecomment-285797337,3,['abort'],['abortStage']
Safety,"@tomwhite If there are a million reads mapped to the same position, my hope was that we could avoid loading them into an RDD at all, if possible (ie., eliminate as they are first read in), rather than load them into an RDD and then downsample. This is why I proposed doing this at the Hadoop-BAM layer. Do you think this is possible?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1644#issuecomment-205861300:94,avoid,avoid,94,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1644#issuecomment-205861300,1,['avoid'],['avoid']
Safety,"@tomwhite If you can address the one issue I had with the classes, it could also be accomplished by sterner commenting on the relevant methods and classes just so long as we are avoiding confusion somehow, then I think we can try to get this in quickly for Wednesdays release.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5127#issuecomment-426012345:178,avoid,avoiding,178,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5127#issuecomment-426012345,1,['avoid'],['avoiding']
Safety,"@tomwhite On the GATK side, I think the steps are:; 1. Rev our Hadoop-BAM dependency once https://github.com/HadoopGenomics/Hadoop-BAM/pull/49 is in.; 2. Take one of our existing test inputs and create a copy of it sorted by query name using the hellbender tool `SortSam`. I think a good file to use would be `src/test/resources/org/broadinstitute/hellbender/tools/BQSR/CEUTrio.HiSeq.WGS.b37.ch20.1m-1m1k.NA12878.noMD.noBQSR.bam`, as that file hasn't had either MD or BQSR run on it, and it's the primary input in the `ReadsPipelineSparkIntegrationTest`.; 3. Before making any actual changes to MarkDuplicatesSpark, write a new integration test proving that MarkDuplicatesSpark can read in the queryname-sorted bam above, and produces the same result as when run on the coordinate-sorted version of the bam. The output in both cases when running with `--shardedOutput false` should be a coordinate-sorted bam.; 4. Now the potentially tricky part: in `MarkDuplicatesSpark`, we want to detect queryname-sorted reads using the `SAMFileHeader` SO attribute (`SAMFileHeader.getSortOrder()`), and if we have them, avoid performing the first `groupByKey()` operation in `MarkDuplicatesSparkUtils.transformReads()`, instead relying on the natural ordering of the reads to create the `PairedEnds` objects.; 5. Make sure that the test you wrote in step 3 doesn't break after doing step 4 :)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1411#issuecomment-170043162:984,detect,detect,984,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1411#issuecomment-170043162,2,"['avoid', 'detect']","['avoid', 'detect']"
Safety,"@tomwhite To clarify, I think that the caller of `ensureCapacity()`, namely `GenotypeLikelihoodCalculators.calculateGenotypeCountUsingTables()`, also needs to be synchronized in order to avoid some unlikely but still-possible races. Given this, I think that we should consider whether `ThreadLocal` might be a better option here. It's not 100% clear to me whether a `ThreadLocal` `get()` call is cheaper than a synchronized method call, but some casual googling suggests that it might be. If we're going to end up entering a synchronized method on every single call to `GenotypeLikelihoodCalculators.getInstance()`, we might want to do some research into whether `ThreadLocal` + no synchronization would be faster, since I believe that this is a performance-sensitive section of code.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5071#issuecomment-422171244:187,avoid,avoid,187,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5071#issuecomment-422171244,1,['avoid'],['avoid']
Safety,"@tomwhite We just sat down and had a look at this class. @droazen was suggesting that this might still be unsafe, and that the outer layer where we compute the genotype likelihood should either be synchronized as well or the whole thing should be wrapped in a thread local object",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5071#issuecomment-422154723:106,unsafe,unsafe,106,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5071#issuecomment-422154723,1,['unsafe'],['unsafe']
Safety,@tomwhite and/or @laserson Could we please get your expert opinions as to whether it's safe to remove the protective copying operations from the spark BQSR like this? (these were necessary in dataflow due to sibling fusion optimization).,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/925#issuecomment-143236364:87,safe,safe,87,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/925#issuecomment-143236364,1,['safe'],['safe']
Safety,"@vdauwera I didn't even know that README existed. It's horribly out of date in a lot of ways. @LeeTL1220 Before I spend time fixing it, is there any way I could drastically shrink this file, like to 3 lines or so, or delete it entirely? I mean, most of it is redundant if you just read the WDL, and I really don't want to add maintenance of this README on top of Terra and the two official M2 wdls we still have to drag around.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5889#issuecomment-484355457:259,redund,redundant,259,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5889#issuecomment-484355457,1,['redund'],['redundant']
Safety,@vdauwera That sounds like a pretty risky change to be making right when we're trying to tie-out our version of HaplotypeCaller. Perhaps this would be better done incrementally.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2314#issuecomment-267110437:36,risk,risky,36,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2314#issuecomment-267110437,1,['risk'],['risky']
Safety,"@vdauwera Well, the full truth is that the M2 WDL tests are experiencing intermittent failures at the moment due to out-of-disk-space issues on travis. You can safely merge this if only the M2 WDL tests failed.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3167#issuecomment-311502730:160,safe,safely,160,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3167#issuecomment-311502730,1,['safe'],['safely']
Safety,@vdauwera not in my radar either.... the rewrite of the assembly may fix some of these cases where there is actually some variation that we fail to detect (false negative) that would explain those soft clips. However I don't think that would fix all the cases.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/269#issuecomment-279013205:148,detect,detect,148,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/269#issuecomment-279013205,1,['detect'],['detect']
Safety,"@vruano The AF calculator is not where the alleles are removed. It calculates an honest probability that each exists. For example, if we have two alts and PLs are `(999,0,999,0,999,999)` -- that is, it's definitely a het, and we have no clue which alt allele it is, then it correctly says that the posterior probability of each alt allele is 0.5. Now, this probability of 0.5 is quite low for a real variant (below the default confidence threshold). `calculateOutputAlleleSubset` sees that and evaluates each allele independently, thus dropping both, but that's a consequence of relying on marginal probabilities. which this example suggests we ought not to do. The logic in `Mutect2` is different and avoids this issue. It does model comparison to see if we can remove alt alleles one at a time, starting with the alt allele with the worst marginal likelihood. Thus, it drops one alt allele, because there is only a tiny likelihood cost since the other alt allele can explain the data, but does not drop a second alt allele since then half the reads wouldn't fit. This is exactly what Yossi proposed.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6364#issuecomment-576775674:702,avoid,avoids,702,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6364#issuecomment-576775674,1,['avoid'],['avoids']
Safety,"@vruano The HGDP crams also trigger this error. . ftp:/­/­ftp.­1000genomes.­ebi.­ac.­uk/­vol1/­ftp/­data_collections/­HGDP/­data/­Brahui/­HGDP00001/­alignment/­HGDP00001.­alt_bwamem_GRCh38DH.­20181023.­Brahui.­cram. ```; 14:32:34.745 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Apr 17, 2021 2:32:34 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 14:32:34.877 INFO CalibrateDragstrModel - ------------------------------------------------------------; 14:32:34.877 INFO CalibrateDragstrModel - The Genome Analysis Toolkit (GATK) v4.2.0.0; 14:32:34.877 INFO CalibrateDragstrModel - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:32:34.877 INFO CalibrateDragstrModel - Executing as farrell@scc-gh3.scc.bu.edu on Linux v3.10.0-1160.15.2.el7.x86_64 amd64; 14:32:34.877 INFO CalibrateDragstrModel - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_172-b11; 14:32:34.877 INFO CalibrateDragstrModel - Start Date/Time: April 17, 2021 2:32:34 PM EDT; 14:32:34.878 INFO CalibrateDragstrModel - ------------------------------------------------------------; 14:32:34.878 INFO CalibrateDragstrModel - ------------------------------------------------------------; 14:32:34.878 INFO CalibrateDragstrModel - HTSJDK Version: 2.24.0; 14:32:34.878 INFO CalibrateDragstrModel - Picard Version: 2.25.0; 14:32:34.878 INFO CalibrateDragstrModel - Built for Spark Version: 2.4.5; 14:32:34.878 INFO CalibrateDragstrModel - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 14:32:34.878 INFO CalibrateDragstrModel - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 14:32:34.878 INFO CalibrateDragstrModel - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 14:32:34.878 INFO CalibrateDragstrModel - HTSJDK Defaults.USE_ASYNC_IO",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7182#issuecomment-821876394:548,detect,detect,548,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7182#issuecomment-821876394,1,['detect'],['detect']
Safety,"@xysj1989 I would think that if you use the python `gatk` launch script, prefaced immediately by `THEANORC` as above, you should be able to tie each GermlineCNVCaller run to a separate compilation directory even if you don’t have control over which nodes you are running on. Increasing the timeout means that different runs will not be able to compile models at the same time, which will add some overhead; however, I think setting separate directories avoids this. In any case, I will try to issue a PR allowing you to directly set the directory or use a temporary one soon. Thanks again for raising the issue!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6235#issuecomment-548587855:290,timeout,timeout,290,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6235#issuecomment-548587855,2,"['avoid', 'timeout']","['avoids', 'timeout']"
Safety,A UserException when you intersect two interval files and get nothing seems like the behavior we want in almost every case. I'm hesitant to add a global override to avoid it. It does seem like working around this in wdl would be a big pain though.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6209#issuecomment-540746640:165,avoid,avoid,165,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6209#issuecomment-540746640,1,['avoid'],['avoid']
Safety,A command line that triggered the crash in our testing: ; ```; gatk HaplotypeCaller -R /cromwell_root/gcp-public-data--broad-references/hg38/v0/dragen_reference/Homo_sapiens_assembly38_masked.fasta -I gs://broad-dsde-methods-dragen/reprocessed_data_v3.7.5_masked/CH1_CHM13_WGS1/CH1_CHM13_WGS1.bam -L /cromwell_root/fc-971fd540-210c-4e5a-87ce-d3f8c91c7557/submissions/0586864c-d263-4797-89f1-b517e487ad2a/VariantCalling/b177671c-e96b-433e-824a-f9d641184e75/call-ScatterIntervalList/glob-cb4648beeaff920acb03de7603c06f98/109scattered.interval_list -O CH1_CHM13_WGS1.vcf.gz --pileup-detection --pileup-detection-absolute-alt-depth 0 --pileup-detection-bad-read-tolerance 0.4 --pileup-detection-enable-indel-pileup-calling --pileup-detection-active-region-phred-threshold 3.0 --use-pdhmm -contamination 0.0 -G StandardAnnotation -G StandardHCAnnotation --dragen-mode --disable-spanning-event-genotyping --dragstr-params-path /cromwell_root/fc-971fd540-210c-4e5a-87ce-d3f8c91c7557/submissions/0586864c-d263-4797-89f1-b517e487ad2a/VariantCalling/b177671c-e96b-433e-824a-f9d641184e75/call-DragstrAutoCalibration/CH1_CHM13_WGS1.bam.dragstr -GQB 10 -GQB 20 -GQB 30 -GQB 40 -GQB 50 -GQB 60 -GQB 70 -GQB 80 -GQB 90; ```. `--dragstr-params-path /cromwell_root/fc-971fd540-210c-4e5a-87ce-d3f8c91c7557/submissions/0586864c-d263-4797-89f1-b517e487ad2a/VariantCalling/b177671c-e96b-433e-824a-f9d641184e75/call-DragstrAutoCalibration/CH1_CHM13_WGS1.bam.dragstr` is likely optional. `-L` is optional and any interval will likely work to reproduce the error.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8712#issuecomment-2034726852:580,detect,detection,580,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8712#issuecomment-2034726852,5,['detect'],"['detection', 'detection-absolute-alt-depth', 'detection-active-region-phred-threshold', 'detection-bad-read-tolerance', 'detection-enable-indel-pileup-calling']"
Safety,A holdover for this is currently in place where we detect if no funcotations were produced at all. In that case we warn the user that they may have configured the reference version and data sources incorrectly.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4978#issuecomment-503219497:51,detect,detect,51,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4978#issuecomment-503219497,1,['detect'],['detect']
Safety,"A quick look at the code results in the following finding:. The error message : `""SA-BWT inconsistency: seq_len is not the same.""` is generated in function `bwt_restore_sa()` defined in `bwt.c`, and resulted in an `abort()` call.; `bwt_restore_sa()` itself was called in `bwa_idx_load_bwt()` defined in `bwa.c`, when trying to restore the suffix array from a file with extension "".sa"". This function call is issued when bwa mem (in its main) tries to load the reference information. This happens before input are read. Because of the `abort()` call, letting the `BwaMem` class handle the error is difficult, so I would propose two possible solutions:; 1. changing the behavior of `BwaIndex` class, so that it eagerly loads the reference, and throws an `Java.lang.IOException` when this error is encountered. Of course this must lead to code duplication, i.e. copying a large part of index loading code from bwa itself and walk around`abort()`ing.; 2. ask @lh3 to change the behavior so that it will not `abort()` any more, but return a null pointer. But the null pointer return error covers so many error cases that this might not be a good idea.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2123#issuecomment-243181189:215,abort,abort,215,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2123#issuecomment-243181189,4,['abort'],['abort']
Safety,"A reminder: we can also make the PoN optional. One thought: if users want to run ModelSegments for germline, what should they do? Presumably they could run the pair WDL in ""tumor-only"" mode and just use the normal as input. So maybe we should change all instances of ""tumor"" to ""case"" and ""normal"" to ""matched_normal""? Or is this still too confusing? I'd like to avoid having separate case and pair WDLs if possible.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3983#issuecomment-362693835:363,avoid,avoid,363,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3983#issuecomment-362693835,1,['avoid'],['avoid']
Safety,"A researcher is asking about the status of the ports for FindCoveredIntervals, DiagnoseTargets, and DepthOfCoverage at https://gatkforums.broadinstitute.org/gatk/discussion/12950/. In addition, they have some suggestions for features for DiagnoseTargets. I've recapitulated their post below:. ---. This is a GATK3 issue, but I hope that GATK4 will incorporate the following suggestion. And while we are on the subject, what is the roadmap for FindCoveredIntervals, DiagnoseTargets, and DepthOfCoverage (or something even more awesome) in GATK4?. A common pattern is to use `FindCoveredIntervals --uncovered` to create an interval list that serves as input to `DiagnoseTargets`. On a good day, FindCoveredIntervals --uncovered might create an interval list file that is empty, i.e., all the tested intervals were covered well enough by the given criteria. In this case DiagnoseTargets will first emit a warning like; ```; #WARN 18:11:43,557 IntervalUtils - The interval file /gpfs/share/cmoco_sys_dev/nfs/storage/cromwell/cromwell-executions/somatic_workflow/6a32b126-f616-42c5-9ec6-2994a4e4bd81/call-BamQCGatk/shard-4/bam_qc/2a4033e0-ea75-4d33-91ff-7ba7656536fe/call-FindUncoveredIntervals/shard-1/execution/VAL18-248-NT-D@VAL18-248-NT-D.08302018JH.Lymphoma.uncovered_intervals.interval_list contains no intervals that could be parsed.; ```; and then it raises an error like; ```; ###### ERROR MESSAGE: This tool only works if you provide one or more intervals (use the -L argument). If you want to run whole genome, use -T DepthOfCoverage instead.; ```; and exit with a non-zero return code. It is certainly possible to trap the empty interval list and skip DiagnoseTargets, but it I think it would be better if DiagnoseTargets emitted the warning but then quit with empty output and a zero rc. This would avoid the problem of a workflow that was developed with test data that always had some uncovered intervals suddenly failing when there is excellent coverage.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/19#issuecomment-426093222:1807,avoid,avoid,1807,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/19#issuecomment-426093222,1,['avoid'],['avoid']
Safety,"A single site changed by a small amount in CNN score which was expected. (We weren't 100% sure we were testing any sites where downsampling would kick in, so this is good from a test coverage perspective. I think this is safe to include. I'm updating the test file and will push once tests pass locally.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5622#issuecomment-458765209:221,safe,safe,221,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5622#issuecomment-458765209,1,['safe'],['safe']
Safety,"AM per task. 768 tasks (16 nodes) in total.; ```; %CPU WallTime Time Lim RSS mem memlim cpus; normal-exe = open&run; 105581211 R ds6924 hm82 genotype 4 00:18:25 02:00:00 1064GB 1064GB 3072GB 768; ```; - Jobs eventually finish if not running out of allocated time.; - Takes a long time to begin processing the first set of variants.; ```; 13:51:37.925 INFO GenotypeGVCFs - ------------------------------------------------------------; 13:51:39.736 INFO GenotypeGVCFs - Done initializing engine; 13:51:39.923 INFO ProgressMeter - Starting traversal; 13:51:39.923 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 14:23:57.323 WARN ReferenceConfidenceVariantContextMerger - Detected invalid annotations: When trying to merge variant contexts at location chr17:18363145 the annotation AS_RAW_MQ=64800.000|50400.000|0.000 was not a numerical value and was ignored; 14:23:57.346 WARN ReferenceConfidenceVariantContextMerger - Reducible annotation 'AS_RAW_MQ' detected, add -G Standard -G AS_Standard to the command to annotate in the final VC with this annotation.; 14:23:58.180 INFO ProgressMeter - chr17:18363854 32.3 1000 31.0; 14:24:13.258 INFO ProgressMeter - chr17:18376854 32.6 14000 430.0; 14:24:58.358 INFO ProgressMeter - chr17:18382854 33.3 20000 600.5; 14:32:49.287 INFO ProgressMeter - chr17:18393855 41.2 31000 753.2; 14:33:39.240 INFO ProgressMeter - chr17:18405856 42.0 43000 1024.1; 14:33:49.493 INFO ProgressMeter - chr17:18411856 42.2 49000 1162.3; 14:34:17.285 INFO ProgressMeter - chr17:18425856 42.6 63000 1478.1; ```. CPU utilisation does not improve after the variants begin processing after half an hour preparing traversal. ```; %CPU WallTime Time Lim RSS mem memlim cpus; normal-exe = open&run; 105581211 R ds6924 hm82 genotype 4 00:42:34 02:00:00 1200GB 1200GB 3072GB 768; ```. - Excellent CPU efficiency if running serially (but defeats the purpose of a H.P.C. with Lustre). ```; %CPU WallTime Time Lim RSS mem memlim cpus; normal-exe = open",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8637#issuecomment-1879551089:1355,detect,detected,1355,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8637#issuecomment-1879551089,1,['detect'],['detected']
Safety,"According to this paper https://www.nature.com/articles/s41467-018-03590-5. it is: ""Each resulting qualified captured library with the SureSelect Human; All Exon kit (Aglient) was then loaded on *BGISEQ-5000 *sequencing; platforms, and we performed high-throughput sequencing for each captured; library. High-quality reads were aligned to the human reference genome; (GRCh37) using the Burrows-Wheeler Aligner (BWA v0.7.15) software. All; genomic variations, including single-nucleotide polymorphisms and InDels; were detected by *HaplotypeCaller of GATK *(v3.0.0).; "". On Wed, Dec 12, 2018 at 3:18 PM Louis Bergelson <notifications@github.com>; wrote:. > @yfarjoun <https://github.com/yfarjoun> Do you know if BGI's sequencing; > is compatible with our tools without any special treatment?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/5517#issuecomment-446729153>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACnk0mujY7YzxUJ-6IPU8B7jPiZWQuzMks5u4WR3gaJpZM4ZQNxZ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5517#issuecomment-446769514:518,detect,detected,518,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5517#issuecomment-446769514,1,['detect'],['detected']
Safety,"Additional feedback from the user for the mutect2 workflow. > ""Of note, it is really difficult and not really 'user-friendly' to have to predict disc space and runtime for Funcotator, which seem to depend (based on calculations you copied above from other Functotator workflows) on outputs of Mutect2 (eg vcf sizes), when here Mutect and Funcotator and bundled together. So I cannot see output of Mutect to predict values for Funcotator - especially not when I get to run this over hundreds of samples. It is also pricey to have jobs failing because of this. It would be much better to have these variables encoded, so that the algorithm uses Mutect outputs to predict memory etc. that it will need to run Funcotator downstream. If this is really how things work (and this is my current understanding), I really do not know how to estimate this for many samples without 'trial and error' that is both costly and it will take extremely long time....""",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6680#issuecomment-651354230:137,predict,predict,137,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6680#issuecomment-651354230,3,['predict'],['predict']
Safety,"After extensive discussion, Chris and I are leaning towards taking out PID and PGT, since they're now redundant with the spec-compliant PS and and phased GT (and less accurate in some cases). @davidbenjamin given that the phasing code is shared between HC and M2, do you think this would be a serious problem for Mutect2 users?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6432#issuecomment-705706545:102,redund,redundant,102,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6432#issuecomment-705706545,1,['redund'],['redundant']
Safety,"Agreed that it would be nice if we abstract away the manner in which import tasks get distributed across multiple nodes. At this point, we're not considering adding a tool that supports that. Sure, we could probably look at Spark or something, but it ; a) is a heavier lift ; b) would probably involve assumptions about user infrastructure. I'm reluctant to support generic split/merge because then we'd be tied to this notion of a meta-workspace (or workspace of workspaces), and how to cleanly track/manipulate those. I'd like to avoid that potential can of worms. Of course, users are free to work with it themselves. . The steps 1-5 you outline match roughly what we have in mind, though there would be a step 0 for initializing the workspace. We haven't decided yet if each independent import job would specify the intervals, or have all the intervals to be imported specified in the initialize phase and then the independent import jobs select intervals by specifying a range of indices/ranks corresponding to those intervals. . Doing the latter would piggyback a bit on how incremental import is currently done. It would also better support the multiple contigs in a single folder mode I mentioned in the last comment.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6620#issuecomment-641525195:532,avoid,avoid,532,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6620#issuecomment-641525195,1,['avoid'],['avoid']
Safety,"Ah, so the `ml.dmlc` dependency is the real xgboost library and the tool for predicting from trained models? Got it, thanks for the clarification.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7950#issuecomment-1189379178:77,predict,predicting,77,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7950#issuecomment-1189379178,1,['predict'],['predicting']
Safety,"Ah, yes, that's kind of confusing actually... ; The **shadow** jar includes a copy of spark and all of it's dependencies, so if you want to run spark tools locally you can use the shadow jar. If you want to use an existing spark cluster, which may have slightly different versions of spark/spark's dependencies then you need to use the **spark** jar. The spark jar doesn't include it's own copy of spark and expects that the spark cluster will provide the necessary dependencies. This avoids conflicts between different dependency versions. . You don't need to use `gatk-launch` ever, but it can make it easier if you want to potentially run your code in different environments. It knows about 3 different potential ways to invoke spark, 1) running in local mode with --sparkMaster local, 2) running on a cluster using spark-submit and 3) running on a google dataproc cluster using gcloud. Gatk-launch knows which environment needs which jar and will prompt you to create one if you don't have it. . gatk-launch also applies some default arguments when running on spark, you may have to supply them yourself if you're not using it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2345#issuecomment-273307091:485,avoid,avoids,485,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2345#issuecomment-273307091,1,['avoid'],['avoids']
Safety,"Aha, one interesting wrinkle that came when trying to remove `getReferenceFile()` entirely in favor of `getReferencePath()` : It is valid to have references of the form `gg://foobar` (referencing the Google Genomics Reference API). You can apparently put that string in a `File` just fine, but a `Path` will try to find the matching provider, and will fail since there is no NIO provider for ""gg"" (it doesn't behave like a filesystem). The work around is to use getReferenceFileName(). The risk is that some places that allowed a relative path (relying on the call to getAbsolutePath) may now require an absolute path.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3921#issuecomment-349825595:490,risk,risk,490,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3921#issuecomment-349825595,1,['risk'],['risk']
Safety,"All comments addressed. Since tests on travis are broken at the moment, we're unfortunately forced to merge this without travis passing, however since only documentation is touched this should be safe.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3158#issuecomment-310772135:196,safe,safe,196,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3158#issuecomment-310772135,1,['safe'],['safe']
Safety,"Also - you asked about workarounds. The attached is not pretty, but it is a minimal way to let tools like VariantEval provide their old behavior. It essentially adds a method in FeatureInput to check whether the name matches the default value it would use. Upstream code can call FeautreInput.isUserSuppliedName() and act accordingly. Rather than use the scheme I do here with string matching, we could make it more explicit in ParsedArgument and actually pass a boolean it we detected one in the parsed argument value. Not necessarily advocating this as a production solution, just as one way to make it work; [FeatureInputWorkaround.patch.txt](https://github.com/broadinstitute/gatk/files/1745946/FeatureInputWorkaround.patch.txt); .",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4426#issuecomment-367498695:477,detect,detected,477,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4426#issuecomment-367498695,1,['detect'],['detected']
Safety,"Also intermittent segfaults in the Java 11 unit tests:. ```; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007f946d77f0f2, pid=7513, tid=7540; #; # JRE version: OpenJDK Runtime Environment (11.0.2+9) (build 11.0.2+9); # Java VM: OpenJDK 64-Bit Server VM (11.0.2+9, mixed mode, tiered, compressed oops, g1 gc, linux-amd64); # Problematic frame:; # V [libjvm.so+0x8fd0f2] jni_GetByteArrayElements+0x72; #; # Core dump will be written. Default location: Core dumps may be processed with ""/usr/share/apport/apport %p %s %c %d %P"" (or dumping to /home/travis/build/broadinstitute/gatk/core.7513); #; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid7513.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; #; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6513#issuecomment-602077546:89,detect,detected,89,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6513#issuecomment-602077546,1,['detect'],['detected']
Safety,Also some tests for collection classes. Refactoring may avoid code duplication here.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3916#issuecomment-352080910:56,avoid,avoid,56,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3916#issuecomment-352080910,1,['avoid'],['avoid']
Safety,"Also, the current process risks people dropping a bunch of extraneous files into the image (such as irrelevant build artifacts) that slow down the build process and make the image itself larger.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2700#issuecomment-300582980:26,risk,risks,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2700#issuecomment-300582980,1,['risk'],['risks']
Safety,"Also, this should ideally be done after https://github.com/broadinstitute/hellbender/issues/961, https://github.com/broadinstitute/hellbender/issues/959, and https://github.com/broadinstitute/hellbender/issues/958 as a final pre-alpha sanity check, as those tickets (and others) will have a huge effect on consistency across tools.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/968#issuecomment-146303198:235,sanity check,sanity check,235,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/968#issuecomment-146303198,1,['sanity check'],['sanity check']
Safety,"Also, you had comment about a test being removed, but the comment got lost before I replied to it. The test that I removed was testing for a condition that got converted into an exception, and then avoided by restructuring the calling class to avoid having that condition come up. I added tests to make sure that it throws properly in those cases though.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/859#issuecomment-170123169:198,avoid,avoided,198,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/859#issuecomment-170123169,2,['avoid'],"['avoid', 'avoided']"
Safety,"An update/unwanted solve:; Caveat: I'm sorry I didn't provide more information about this issue beforehand, in fact, I create the vcf file by calling:. ```; gatk SelectVariants \; -R REF.fasta \; -V allsites.allsamples.vcf \; -O sample.vcf \; --remove-unused-alternates \; --exclude-filtered \; --sample-name ""sample"" \; -select 'vc.getGenotype(""sample"").hasAD() && vc.isVariant() && (vc.getGenotype(""sample"").getAD().1 > 2.0)'; ```. (some of the select checks might be redundant but I wanted to be on the safe side); **The error doesn't happen when I remove ""--remove-unused-alternates""**; This is very unfortunate as I need that to force the sites to be biallelic and to filter my variants and I will have to find a workaround. Does it happen because it's looking for an alternate that's no longer there? Could it be a dictionary mismatch/confusion?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7433#issuecomment-904202463:470,redund,redundant,470,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7433#issuecomment-904202463,2,"['redund', 'safe']","['redundant', 'safe']"
Safety,"And @hanalangoallen and @mlaylwar what ref and alt alleles are you seeing when the error happens? Are they also cases with redundant equal bases at the end of each allele that, if properly trimmed, would yield a SNP?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6473#issuecomment-690829403:123,redund,redundant,123,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6473#issuecomment-690829403,1,['redund'],['redundant']
Safety,"Another update. It looks like the problem lies in Hadoop-BAM, in the part of the code that looks for the start of records in the BGZF block compressed file. If BAM files have an index (produced by SplittingBAMIndexer) then it will use them, but if not, then it uses BAMSplitGuesser for heuristically finding the start of a record. In this case it is trying to find the first record, but it gets the wrong offset (6505, instead of 6506) which then causes the error. We should fix the code in BAMSplitGuesser that causes this. I haven't managed to find where the bug is yet - it may take a while. It might also be worth seeing if it's possible to write indexes from ReadsSparkSink to avoid the BAMSplitGuesser having to be used. I've uploaded the problematic BAM here: https://github.com/broadinstitute/gatk/blob/tw_debug_invalid_bam/part-r-00686.bam",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1005#issuecomment-149481788:682,avoid,avoid,682,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1005#issuecomment-149481788,1,['avoid'],['avoid']
Safety,Any way to detect how many cores are available and toggle it off automatically in that case?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1588#issuecomment-197946187:11,detect,detect,11,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1588#issuecomment-197946187,1,['detect'],['detect']
Safety,"Apparently related, just running IndexFeatureFile on my machine results in several stack traces:; ```; Sep 21, 2017 4:10:53 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; WARNING: Failed to detect whether we are running on Google Compute Engine.; java.net.ConnectException: Host is down (connect failed); 	at java.net.PlainSocketImpl.socketConnect(Native Method); 	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350); 	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206); 	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188); 	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392); 	at java.net.Socket.connect(Socket.java:589); 	at sun.net.NetworkClient.doConnect(NetworkClient.java:175); 	at sun.net.www.http.HttpClient.openServer(HttpClient.java:463); 	at sun.net.www.http.HttpClient.openServer(HttpClient.java:558); 	at sun.net.www.http.HttpClient.<init>(HttpClient.java:242); 	at sun.net.www.http.HttpClient.New(HttpClient.java:339); 	at sun.net.www.http.HttpClient.New(HttpClient.java:357); 	at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1202); 	at sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1138); 	at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1032); 	at sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:966); 	at shaded.cloud_nio.com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93); 	at shaded.cloud_nio.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:981); 	at shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials.runningOnComputeEngine(ComputeEngineCredentials.java:176); 	at shaded.cloud_nio.com.google.auth.oauth2.DefaultCredentialsProvider.tryGetComputeCredentials(DefaultCredentialsProvider.java:270); 	at shaded.cloud_nio.com.google.auth.oauth2",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3591#issuecomment-331269235:235,detect,detect,235,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3591#issuecomment-331269235,1,['detect'],['detect']
Safety,ApplicationDefault(GoogleCredentials.java:86); 	at com.google.cloud.ServiceOptions.defaultCredentials(ServiceOptions.java:277); 	at com.google.cloud.ServiceOptions.<init>(ServiceOptions.java:252); 	at com.google.cloud.storage.StorageOptions.<init>(StorageOptions.java:82); 	at com.google.cloud.storage.StorageOptions.<init>(StorageOptions.java:30); 	at com.google.cloud.storage.StorageOptions$Builder.build(StorageOptions.java:77); 	at org.broadinstitute.hellbender.utils.gcs.BucketUtils.setGlobalNIODefaultOptions(BucketUtils.java:361); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:195); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:131); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:152); 	at org.broadinstitute.hellbender.Main.main(Main.java:233); ```; and ; ```; WARNING: Failed to detect whether we are running on Google Compute Engine.; java.net.ConnectException: Host is down (connect failed); 	at java.net.PlainSocketImpl.socketConnect(Native Method); 	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350); 	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206); 	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188); 	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392); 	at java.net.Socket.connect(Socket.java:589); 	at sun.net.NetworkClient.doConnect(NetworkClient.java:175); 	at sun.net.www.http.HttpClient.openServer(HttpClient.java:463); 	at sun.net.www.http.HttpClient.openServer(HttpClient.java:558); 	at sun.net.www.http.HttpClient.<init>(HttpClient.java:242); 	at sun.net.www.http.HttpClient.New(HttpClient.java:339); 	at sun.net.www.http.HttpClient.New(HttpClient.java:357); 	at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1202); 	at sun,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3591#issuecomment-331269235:3408,detect,detect,3408,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3591#issuecomment-331269235,1,['detect'],['detect']
Safety,"Are the errors below part of this, when starting BwaSpark with spark-submit?; I activated ""--disable-sequence-dictionary-validation true"", but that doesn't help. It is very unclear, why a BAM is not recognized as a BAM file. I have tried all kinds of ways to make sure that it is a BAM and not a SAM file.; The documentation for BwaSpark also says ""BAM/SAM/CRAM file containing reads"", so if SAM files are really not possible, that should probably be changed.; ...; Even on verbosity DEBUG, the comments are not at all helpful to get at the problem.; E.g. ""Cannot retrieve file pointers within SAM text files.""; Is that a general statement about SAM files? Or does it only say, that in this specific SAM file (which is actually a BAM file), file pointers cannot be found?; What pointers are meant exactly?; How could this be fixed?. ```; ""SamReaderFactory	Unable to detect file format from input URL or stream, assuming SAM format.""; Which URL?; Which stream?; Why would this happen? What could be the error?; The SAM/BAM distinction seems very unclear. It would be more helpful, if some specific missing aspect (e.g. not queryname sorted) would be clearly declared as the culprit.; ...; 00:29 DEBUG: [kryo] Write: SAMFileHeader{VN=1.5, SO=queryname}; ...; WARNING	2018-01-16 02:11:25	SamReaderFactory	Unable to detect file format from input URL or stream, assuming SAM format.; ...; java.lang.UnsupportedOperationException: Cannot retrieve file pointers within SAM text files.; 	at htsjdk.samtools.SAMTextReader.getFilePointerSpanningReads(SAMTextReader.java:185); ...; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4131#issuecomment-357838062:866,detect,detect,866,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4131#issuecomment-357838062,2,['detect'],['detect']
Safety,"Are there any updates on this feature? The use of soft-clipping is not only confusing, but can negatively affect the performance of other tools that use this sort of information. Ignoring soft-clipped reads altogether, if possible at all, is not a good solution. We are forced to use GATK3 because the output of the GATK4 version does not work well with others tools we need for the detection of certain variants in RNA-seq.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7356#issuecomment-1846884589:383,detect,detection,383,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7356#issuecomment-1846884589,1,['detect'],['detection']
Safety,"As a sanity check, HaplotypeCaller run in GVCF mode over a bam subsetted to only chr15 on my laptop:; master: ; ```; real	4m28.600s; user	5m48.484s; sys	0m4.510s; ```; this branch: ; ```; real	3m58.043s; user	5m15.625s; sys	0m4.012s; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5469#issuecomment-443322836:5,sanity check,sanity check,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5469#issuecomment-443322836,1,['sanity check'],['sanity check']
Safety,"As determined by @davidadamsphd , the `Copyable` interface idea won't work:. The recommendation from the Dataflow team was to make a narrow API and do the copying part of the API. I started down this route, and I think it might be doable for things like the walker interface. The idea is to make a Copyable interface and have our interfaces extend that. . However, we have unsafe code already in the engine. I tried to make this SafeDoFn approach, however it became clear quickly that we'd have a combinatorial explosion of classes because we don't just have `DoFn<GATKRead,POut>`, but also `<Iterable<GATKRead>,POut>`, and many others. So, this approach will not work for the engine. I then tried to make a general purpose solution (using coders to write to bytes and then recreate a new class). This doesn't work for a few reasons, most critical is that the coder registry isn't Serializable, so that can't be passed down deep enough to get this to work. While working on this, I chatted with someone on the Dataflow team who is working on the verification on the direct runner. He has a PR out and likely going to get it approved soon. So, for the engine, we could always test using the direct runner and know for sure there are not issues (once we can use his code). However, there are two downsides:. 1) We will need to wait for a cut of the SDK (which looking at their previous clip is likely ~ two weeks away). . 2) I don't know if we want the direct runner test as our general purpose solution. Can we expect Comp Bios to always test with the direct runner first? Will they write anything more complex than functions that use the Walker interface?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/702#issuecomment-127403661:373,unsafe,unsafe,373,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/702#issuecomment-127403661,2,"['Safe', 'unsafe']","['SafeDoFn', 'unsafe']"
Safety,"As we discussed, it's possible that these are simply common germline CNVs that are being median-normalized out in CR by the PoN. Let's investigate the sample-median-normalized counts in some of the questionable regions, along with the per-bin medians in the PoN. I do not think a gCNV run is necessary (it will probably be a bit expensive, anyway). More generally, I think a better approach to germline tagging would be to avoid the caller entirely. Let's take the ModelSegments output for a normal, and then tag ModelSegments segments in the tumor that sufficiently overlap any normal segment in CR-AF-genomic space (where we have some freedom to define the overlap criteria). Essentially, let's just try to highlight differences between the tumor and normal in CR-AF space. This would rescue events in the normal that may be further amplified or deleted in the tumor. Subsequently, simple filtering of these events would be less misleading than imputation. I do not think such tagging should be implemented in Java, if we can avoid it. Rather, a relatively simple python script that runs through each tumor segment and checks for overlaps would suffice. This script could output a tagged/filtered ModelSegments result, as well as do the conversion step for downstream tools. This also obviates the need for the Java code for combining segment breakpoints and additional CNV collection classes in the current post-processing tools. What do you think?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5450#issuecomment-442911810:423,avoid,avoid,423,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5450#issuecomment-442911810,2,['avoid'],['avoid']
Safety,"At the end of the stdout/stderr it reports 101 warnings and 1 error, so I think it's safe to say that these complaints about the `@VisibleForTesting` annotation are irrelevant. If we further exclude:. * lines saying that a class was imported that follow a complaint about the class; * lines with only a timestamp and a `^` symbol. we obtain the following:. ```; 2022-08-16T00:09:07.2545204Z src/main/java/org/broadinstitute/hellbender/cmdline/CommandLineProgram.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T00:09:07.2547467Z src/main/java/org/broadinstitute/hellbender/cmdline/CommandLineProgram.java:4: error: package com.google.common.base does not exist; 2022-08-16T00:09:07.2647018Z src/main/java/org/broadinstitute/hellbender/engine/FeatureInput.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T00:09:07.2671678Z src/main/java/org/broadinstitute/hellbender/tools/walkers/variantutils/PosteriorProbabilitiesUtils.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T00:09:07.2726493Z src/main/java/org/broadinstitute/hellbender/engine/FeatureContext.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T00:09:07.2743559Z src/main/java/org/broadinstitute/hellbender/utils/io/BlockCompressedIntervalStream.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T00:09:07.2775681Z src/main/java/org/broadinstitute/hellbender/engine/filters/CountingVariantFilter.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T00:09:07.2833952Z src/main/java/org/broadinstitute/hellbender/engine/FeatureManager.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T00:09:07.2841948Z src/main/java/org/broadinstitute/hellbender/cmdline/argumentcollections/IntervalArgumentCollection.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T00:09:07.2856913Z src/main/java/org/broadinstitute/he",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217242480:85,safe,safe,85,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217242480,1,['safe'],['safe']
Safety,"Audit results, there are two (very related issues).; Issue (1); `setPosition` from `SAMRecordToGATKReadAdapter` calls `setReferenceName` from `SAMRecord`. ```; public void setReferenceName(final String value) {; /* String.intern() is surprisingly expensive, so avoid it by looking up in sequence dictionary if possible */; if (NO_ALIGNMENT_REFERENCE_NAME.equals(value)) {; mReferenceName = NO_ALIGNMENT_REFERENCE_NAME;; mReferenceIndex = NO_ALIGNMENT_REFERENCE_INDEX;; return;; } else if (mHeader != null) {; final int referenceIndex = mHeader.getSequenceIndex(value);; if (referenceIndex != -1) {; setReferenceIndex(referenceIndex);; return;; }; }; // Drop through from above if nothing done.; mReferenceName = value.intern();; mReferenceIndex = null;; }; ```. Issue (2); `setMatePosition` from `SAMRecordToGATKReadAdapter` calls `setMateReferenceName` from `SAMRecord`. ```; public void setMateReferenceName(final String mateReferenceName) {; /* String.intern() is surprisingly expensive, so avoid it by looking up in sequence dictionary if possible */; if (NO_ALIGNMENT_REFERENCE_NAME.equals(mateReferenceName)) {; mMateReferenceName = NO_ALIGNMENT_REFERENCE_NAME;; mMateReferenceIndex = NO_ALIGNMENT_REFERENCE_INDEX;; return;; } else if (mHeader != null) {; final int referenceIndex = mHeader.getSequenceIndex(mateReferenceName);; if (referenceIndex != -1) {; setMateReferenceIndex(referenceIndex);; return;; }; }; // Drop through from above if nothing done.; this.mMateReferenceName = mateReferenceName.intern();; mMateReferenceIndex = null;; }; ```. @lbergelson, what's the fix?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141122913:261,avoid,avoid,261,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141122913,2,['avoid'],['avoid']
Safety,"Augh, looks like we'll have to actually rebase this onto samtools/htsjdk#796 to get it to pass tests, due to a change in codec detection.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2344#issuecomment-277355740:127,detect,detection,127,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2344#issuecomment-277355740,1,['detect'],['detection']
Safety,"Awesome @lbergelson! Now the tests are passing here, so I will probably rebase all of my PRs soon to avoid the annoying ""red cross of death"". Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2401#issuecomment-281515727:101,avoid,avoid,101,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2401#issuecomment-281515727,1,['avoid'],['avoid']
Safety,"BTW, if any can provide the output from an interactive python session that tries (and fails) to use Intel-TF on non-AVX hardware it might help us figure out how to detect that case.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-428293864:164,detect,detect,164,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-428293864,1,['detect'],['detect']
Safety,"Bleh. ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007f513eecd0f2, pid=6936, tid=6963; #; # JRE version: OpenJDK Runtime Environment (11.0.2+9) (build 11.0.2+9); # Java VM: OpenJDK 64-Bit Server VM (11.0.2+9, mixed mode, tiered, compressed oops, g1 gc, linux-amd64); # Problematic frame:; # V [libjvm.so+0x8fd0f2] jni_GetByteArrayElements+0x72; #; # Core dump will be written. Default location: Core dumps may be processed with ""/usr/share/apport/apport %p %s %c %d %P"" (or dumping to /home/travis/build/broadinstitute/gatk/core.6936); #; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid6936.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; ```. Then exit 134 which is ""something is fucked but we don't know what"". . Looks like a seg fault somewhere with some JNI something....",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-606830417:36,detect,detected,36,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-606830417,1,['detect'],['detected']
Safety,"CKTRACE_ON_USER_EXCEPTION=true') to print the stack trace.; [ccastane9@andersserver-01 GenomicsDB]$ bash *_genotype.3.sh; Using GATK jar /data1/_software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Xmx16g -jar /data1/_software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar GenotypeGVCFs --genomicsdb-shared-posixfs-optimizations --reference /data1/EquCab/_ECA30/Equus_caballus.EquCab3.0.dna_sm.toplevel.fa/ -V gendb://ECA3_GenomicsDB_260/3 -O ECA3_GenomicsDB_260.3.g.vcf.gz; 16:27:53.573 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data1/_software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 06, 2021 4:27:54 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 16:27:54.132 INFO GenotypeGVCFs - ------------------------------------------------------------; 16:27:54.133 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.1.8.1; 16:27:54.133 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 16:27:54.143 INFO GenotypeGVCFs - Executing as ccastane9@andersserver-01.cvm.tamu.edu on Linux v3.10.0-1127.19.1.el7.x86_64 amd64; 16:27:54.143 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_275-b01; 16:27:54.144 INFO GenotypeGVCFs - Start Date/Time: January 6, 2021 4:27:53 PM CST; 16:27:54.144 INFO GenotypeGVCFs - ------------------------------------------------------------; 16:27:54.144 INFO GenotypeGVCFs - ------------------------------------------------------------; 16:27:54.145 INFO GenotypeGVCFs - HTSJDK Version: 2.23.0; 16:27:54.145 INFO GenotypeGVCFs - Picard Version: 2.22.8; 16:27:54.145 INFO GenotypeGVCFs - HTSJD",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-755760402:4604,detect,detect,4604,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-755760402,1,['detect'],['detect']
Safety,"CRAM w/o NIO is also ~3 cents per sample (it was marginally more expensive than CRAM w/ NIO, but within the noise). CRAM w/o NIO w/ SSD is ~5 cents. So I'd say CRAM w/ or w/o NIO is fine. Strictly speaking, we can't directly compare the BAM and CRAM costs, since they were done on different sets of TCGA samples. But both are well under the goal of ~15 cents per sample, so I think it's safe to say that we can turn our attention to optimizing inference costs.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5715#issuecomment-467612453:387,safe,safe,387,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5715#issuecomment-467612453,1,['safe'],['safe']
Safety,Can I replace dictionaries in *hdf5 files ? Will this avoid reruning GermlineCNVCaller step? ; It's just that this step took quite a long time at my machine. About 42 hours for each shard.....; This is why it seems to me that reporting the error in the early stages would be more useful.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6924#issuecomment-720056200:54,avoid,avoid,54,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6924#issuecomment-720056200,1,['avoid'],['avoid']
Safety,"Can someone modify the underlying read without calling a setter?. On Monday, July 25, 2016, droazen notifications@github.com wrote:. > @akiezun https://github.com/akiezun That's why I was suggesting; > invalidating all cached values on every call to any setter -- that way we; > don't have to think about the nuances of when it's necessary to; > recalculate, and greatly reduce the risks that normally come with caching; > while still getting most of the performance benefit in typical usage.; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235136126,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/AB5rL8K8hmn3JygZbx39Covn8lc14S5sks5qZWIxgaJpZM4JR8AP; > . ## . Sent from Gmail Mobile",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235139023:382,risk,risks,382,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235139023,1,['risk'],['risks']
Safety,"Can you have a look to this proposal, @droazen? I really need to have this in before the release of GATK4 to be able to update my dependency for the release one. Otherwise, I will need a version bump or a hacked CLP class (which I prefer to avoid). Thank you very much in advance!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3998#issuecomment-353034378:241,avoid,avoid,241,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3998#issuecomment-353034378,1,['avoid'],['avoid']
Safety,Closed via https://github.com/broadinstitute/gatk/pull/4757 where the timeouts were removed altogether.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4221#issuecomment-397080074:70,timeout,timeouts,70,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4221#issuecomment-397080074,1,['timeout'],['timeouts']
Safety,Context.finishBeanFactoryInitialization(AbstractApplicationContext.java:878); 	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:550); 	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:143); 	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:758); 	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:750); 	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:397); 	at org.springframework.boot.SpringApplication.run(SpringApplication.java:315); 	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1237); 	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1226); 	at com.luz.push.PushApplication.main(PushApplication.java:10). 2020-05-29 15:14:33.032 WARN 12904 --- [ main] c.g.a.oauth2.ComputeEngineCredentials : Failed to detect whether we are running on Google Compute Engine. java.net.SocketException: Network is unreachable: connect; 	at java.net.DualStackPlainSocketImpl.waitForConnect(Native Method); 	at java.net.DualStackPlainSocketImpl.socketConnect(DualStackPlainSocketImpl.java:85); 	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350); 	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206); 	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188); 	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172); 	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392); 	at java.net.Socket.connect(Socket.java:589); 	at sun.net.NetworkClient.doConnect(NetworkClient.java:175); 	at sun.net.www.http.HttpClient.openServer(HttpClient.java:432); 	at sun.net.www.http.HttpClient.openServer(HttpClient.java:527); 	at sun.net.www.http.HttpClient.<init>(HttpClient.java:211); 	at sun.net.www.http.HttpClient.New(HttpClient.java:308); 	at,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-635805233:18046,detect,detect,18046,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-635805233,1,['detect'],['detect']
Safety,Context.finishBeanFactoryInitialization(AbstractApplicationContext.java:878); 	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:550); 	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:143); 	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:758); 	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:750); 	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:397); 	at org.springframework.boot.SpringApplication.run(SpringApplication.java:315); 	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1237); 	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1226); 	at com.luz.push.PushApplication.main(PushApplication.java:10). 2020-05-29 15:14:33.035 WARN 12904 --- [ main] c.g.a.oauth2.ComputeEngineCredentials : Failed to detect whether we are running on Google Compute Engine. java.net.SocketException: Network is unreachable: connect; 	at java.net.DualStackPlainSocketImpl.waitForConnect(Native Method); 	at java.net.DualStackPlainSocketImpl.socketConnect(DualStackPlainSocketImpl.java:85); 	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350); 	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206); 	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188); 	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172); 	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392); 	at java.net.Socket.connect(Socket.java:589); 	at sun.net.NetworkClient.doConnect(NetworkClient.java:175); 	at sun.net.www.http.HttpClient.openServer(HttpClient.java:432); 	at sun.net.www.http.HttpClient.openServer(HttpClient.java:527); 	at sun.net.www.http.HttpClient.<init>(HttpClient.java:211); 	at sun.net.www.http.HttpClient.New(HttpClient.java:308); 	at,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-635805233:23386,detect,detect,23386,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-635805233,1,['detect'],['detect']
Safety,"Couldn't open hdf5 files.; ![Screenshot_2020-10-29_17-20-17](https://user-images.githubusercontent.com/29140765/97586406-459fe000-1a0b-11eb-86cf-d70a28c55637.png); Running without the optional --sequence-dictionary argument also causes an error.; `17:00:00.556 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/lmbs02/bio/biosoft/gatk/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Oct 29, 2020 5:00:00 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 17:00:00.683 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------; 17:00:00.684 INFO PostprocessGermlineCNVCalls - The Genome Analysis Toolkit (GATK) v4.1.9.0; 17:00:00.684 INFO PostprocessGermlineCNVCalls - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:00:00.684 INFO PostprocessGermlineCNVCalls - Executing as lmbs02@Lmbs01 on Linux v5.4.0-48-generic amd64; 17:00:00.684 INFO PostprocessGermlineCNVCalls - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_265-8u265-b01-0ubuntu2~18.04-b01; 17:00:00.684 INFO PostprocessGermlineCNVCalls - Start Date/Time: October 29, 2020 5:00:00 PM MSK; 17:00:00.684 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------; 17:00:00.684 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------; 17:00:00.684 INFO PostprocessGermlineCNVCalls - HTSJDK Version: 2.23.0; 17:00:00.684 INFO PostprocessGermlineCNVCalls - Picard Version: 2.23.3; 17:00:00.684 INFO PostprocessGermlineCNVCalls - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 17:00:00.685 INFO PostprocessGermlineCNVCalls - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:00:00.685 INFO PostprocessGermlineCNVCalls - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 17:00:00.685 INFO PostprocessGermlineCNV",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6924#issuecomment-718787427:580,detect,detect,580,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6924#issuecomment-718787427,1,['detect'],['detect']
Safety,"Current status of this: The tool can physically run on 11k samples, but with a 1-5% failure rate, depending on the combination of arguments used. The failures are almost all due to https://github.com/broadinstitute/gatk/issues/2685 (see the stack trace in https://github.com/broadinstitute/gatk/issues/2685#issuecomment-308541727 for a representative example error). . One possibility is that we are being throttled in a way that GATK itself can't recover from. GATK is retrying in the face of these SSL errors 20 times, with increasing wait times between each attempt, and still running out of retries. See @jean-philippe-martin 's latest hypothesis in https://github.com/broadinstitute/gatk/issues/2685#issuecomment-308586876. @kcibul @Horneth take note.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2633#issuecomment-308755736:448,recover,recover,448,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2633#issuecomment-308755736,1,['recover'],['recover']
Safety,DD.iterator(RDD.scala:288); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:121); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6513#issuecomment-601702690:7250,abort,abortStage,7250,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6513#issuecomment-601702690,1,['abort'],['abortStage']
Safety,DD.scala:310); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:123); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSche,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429:10792,abort,abortStage,10792,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429,1,['abort'],['abortStage']
Safety,"David and Lee;; Thanks for the thoughts and heads up on ploidy. I normally only set this for mitochondrial and chrX/Y but it's not a big deal to have diploid calls throughout. Avoiding representing minor variants in the ploidy field would be helpful for downstream processing. Having multiple alleles on a single line per allele is within spec (1.6.1 in the 4.3 spec: ""It is permitted to have multiple records with the same POS."") and a lot of downstream tools deal with them this way. bcftools and vt normalization produces these and I know GEMINI does this to correctly handle multi-alleles.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3564#issuecomment-330549263:176,Avoid,Avoiding,176,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3564#issuecomment-330549263,1,['Avoid'],['Avoiding']
Safety,"David, thanks for the CWL suggestion. As far as I know most CWL runners don't attempt to edit or mount the internal container `/etc/passwd` unfortunately. They do try to match with the external user outside of Docker to avoid file permission issues. Does Cromwell deal with this problem? We're actively looking to make more use of Cromwell for CWL runs. If we could make that happen that would resolve a lot of issues and I could leave my workaround for other non-conforming callers. Tom, that is a great suggestion and I thought would work as well but we do this (https://github.com/bcbio/bcbio-nextgen/blob/bd03e259877d410045468046a949f6b9724605c5/bcbio/broad/__init__.py#L152) and Spark/Hadoop still wants to look up the user in `/etc/passwd` even if missing. If there is a way to skip that being present I'm happy to tweak that as well. Thanks so much for all this discussion and suggestions.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4626#issuecomment-380123018:220,avoid,avoid,220,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4626#issuecomment-380123018,1,['avoid'],['avoid']
Safety,"Discussed in person with @cmnbroad. We decided to go with a different approach that avoids the need for a central registry + connecting code, would be 100% pluggable, and would be easily extensible to variant annotations if we want to add `@Argument` capability there as well:. -replace lambda read filters with explicit class definitions; -patch the argument system to add the generic ability to discover arguments within plugin classes, and pass on populated instances of those plugin classes to the tool (likely involves only a very small amount of additional code, based on our reading of `CommandLineParser`)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1900#issuecomment-228180490:84,avoid,avoids,84,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1900#issuecomment-228180490,1,['avoid'],['avoids']
Safety,"Do you agree that in theory the data is already where it needs to be in this scenario, and the problem is just that spark can't detect that this is the case?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1248#issuecomment-161963248:128,detect,detect,128,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1248#issuecomment-161963248,1,['detect'],['detect']
Safety,"Do you have a dataset and a commandline that demonstrates the problem?. On Wed, Apr 6, 2016 at 7:42 AM, Geraldine Van der Auwera <; notifications@github.com> wrote:. > Reading in the contents and doing any sort of operation on them. I believe; > @droazen https://github.com/droazen is aware of why this occurs in; > GATK3 -- he explained it to me a long time ago. At the time we put it down; > as a known limitation but after discussing some challenges associated with; > Hg38 with @ebanks https://github.com/ebanks it sounds like it's going; > to be important to avoid or mitigate the problem in GATK4.; > ; > —; > You are receiving this because you commented.; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/gatk/issues/1688#issuecomment-206328891",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1688#issuecomment-206439062:564,avoid,avoid,564,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1688#issuecomment-206439062,1,['avoid'],['avoid']
Safety,"Does the problem go away if you use an output path with the 'hdfs://' scheme? E.g. _hdfs://namenode:8020/user/yaron/output.bam_ (where _namenode_ is the hostname of the namenode). There are two libraries being used internally for accessing the filesystem - the Hadoop filesystem API, and the NIO API - and they have slightly different behaviour if no scheme is provided. So to avoid problems it's best to give full paths with URI schemes for all input and output paths.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3066#issuecomment-407791242:377,avoid,avoid,377,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3066#issuecomment-407791242,1,['avoid'],['avoid']
Safety,"Downsampling doesn’t have to be to an exact level of coverage, it's just to avoid very high areas, so you might be able to do this on a Spark partition basis. Use mapPartitions to iterate through the reads in a partition, downsampling as you go (a bit like LocusIteratorByState does). If reads for a given alignment start cross a partition, then this would just mean you get up to 2C coverage at that point, where C is the target downsample coverage. This would solve the problem you outlined, without having to push the downsampling logic into Hadoop-BAM. Could that work?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1644#issuecomment-205751905:76,avoid,avoid,76,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1644#issuecomment-205751905,1,['avoid'],['avoid']
Safety,"Even if I'll refactore it to be void, the part that I changed is need it: there is no other part which handle the `CommandLineException` and if the `customCommandLineValidation()` throws the exception no error is printed in the terminal. I guess that returning a `String[]` in Picard is done to output several errors in the command line to avoid the user to re-run with another bug not reported. Nevertheless, I prefer you approach. I'm changing now the code in this PR to add what you suggested. Thanks again for make my development smoother, @lbergelson!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2226#issuecomment-255847377:340,avoid,avoid,340,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2226#issuecomment-255847377,1,['avoid'],['avoid']
Safety,FGQBands 22 --GVCFGQBands 23 --GVCFGQBands 24 --GVCFGQBands 25 --GVCFGQBands 26 --GVCFGQBands 27 --GVCFGQBands 28 --GVCFGQBands 29 --GVCFGQBands 30 --GVCFGQBands 31 --GVCFGQBands 32 --GVCFGQBands 33 --GVCFGQBands 34 --GVCFGQBands 35 --GVCFGQBands 36 --GVCFGQBands 37 --GVCFGQBands 38 --GVCFGQBands 39 --GVCFGQBands 40 --GVCFGQBands 41 --GVCFGQBands 42 --GVCFGQBands 43 --GVCFGQBands 44 --GVCFGQBands 45 --GVCFGQBands 46 --GVCFGQBands 47 --GVCFGQBands 48 --GVCFGQBands 49 --GVCFGQBands 50 --GVCFGQBands 51 --GVCFGQBands 52 --GVCFGQBands 53 --GVCFGQBands 54 --GVCFGQBands 55 --GVCFGQBands 56 --GVCFGQBands 57 --GVCFGQBands 58 --GVCFGQBands 59 --GVCFGQBands 60 --GVCFGQBands 70 --GVCFGQBands 80 --GVCFGQBands 90 --GVCFGQBands 99 --indelSizeToEliminateInRefModel 10 --useAllelesTrigger false --dontTrimActiveRegions false --maxDiscARExtension 25 --maxGGAARExtension 300 --paddingAroundIndels 150 --paddingAroundSNPs 20 --kmerSize 10 --kmerSize 25 --dontIncreaseKmerSizesForCycles false --allowNonUniqueKmersInRef false --numPruningSamples 1 --recoverDanglingHeads false --doNotRecoverDanglingBranches false --minDanglingBranchLength 4 --consensus false --maxNumHaplotypesInPopulation 128 --errorCorrectKmers false --minPruning 2 --debugGraphTransformations false --kmerLengthForReadErrorCorrection 25 --minObservationsForKmerToBeSolid 20 --likelihoodCalculationEngine PairHMM --base_quality_score_threshold 18 --gcpHMM 10 --pair_hmm_implementation FASTEST_AVAILABLE --pcr_indel_model CONSERVATIVE --phredScaledGlobalReadMismappingRate 45 --nativePairHmmThreads 4 --useDoublePrecision false --debug false --useFilteredReadsForAnnotations false --emitRefConfidence NONE --bamWriterType CALLED_HAPLOTYPES --disableOptimizations false --justDetermineActiveRegions false --dontGenotype false --dontUseSoftClippedBases false --captureAssemblyFailureBAM false --errorCorrectReads false --doNotRunPhysicalPhasing false --min_base_quality_score 10 --useNewAFCalculator false --annotateNDA false --heterozygosity 0.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3154#issuecomment-334840678:2869,recover,recoverDanglingHeads,2869,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3154#issuecomment-334840678,1,['recover'],['recoverDanglingHeads']
Safety,FYI in Classic GATK we are reworking filters so that MalformedReadFilter will centralize all these checks including what was in BadCigar. So cigar sanity checking will be part of basic read format checking. . Recommend waiting on this and porting when that is done.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/348#issuecomment-94344636:147,sanity check,sanity checking,147,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/348#issuecomment-94344636,1,['sanity check'],['sanity checking']
Safety,"FYI, Spark 2.4.0 upgraded to Kryo 4.0.0 in [3e033035](https://github.com/apache/spark/commit/3e033035a3c0b7d46c2ae18d0d322d4af3808711). There does not appear to be a back port to older spark versions. Did y'all happen to also try [this](https://github.com/EsotericSoftware/kryo/pull/527/files):. ```java; Kryo kryo = new Kryo();; kryo.setReferences(false);; ```. I'd like to avoid asking Hail users to set JVM options, so this approach is appealing to me. Curious if you all had experience trying it out.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1524#issuecomment-471013025:375,avoid,avoid,375,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1524#issuecomment-471013025,1,['avoid'],['avoid']
Safety,"Failing tests were from a debug bamout to my Desktop, which of course fails on Travis. Fixed that. Just need to write a unit test with `recoverAll = true`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5693#issuecomment-466143854:136,recover,recoverAll,136,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5693#issuecomment-466143854,1,['recover'],['recoverAll']
Safety,First-pass review complete -- back to @tomwhite. . Do you agree that we should wait to merge this until https://github.com/HadoopGenomics/Hadoop-BAM/pull/49 propagates to hellbender (since it won't be safe to use queryname-sorted input until that's in)?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1429#issuecomment-172114131:201,safe,safe,201,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1429#issuecomment-172114131,1,['safe'],['safe']
Safety,"First-pass review complete -- back to @tomwhite. Many of my suggestions center around pushing arguments and functionality up into `GATKSparkTool` as much as possible, even if they're not applicable to every tool, as we ideally want to spare tool authors from having to manually manage these low-level Spark parameters when they don't want/need to, and we also want to enforce consistency across tools and avoid duplicated boilerplate code. At the same time, there should be clear mechanisms for tools to override the defaults when they have to (eg., overridable methods in `GATKSparkTool`), as I'm not sure whether tools like BQSR are going to be happy with the new 128 MB default input split size.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1432#issuecomment-172100907:405,avoid,avoid,405,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1432#issuecomment-172100907,1,['avoid'],['avoid']
Safety,"For a quick analysis, I made a serialized versions of DBSNP (13572728 variants from `dbsnp_135.b37.excluding_sites_after_129.vcf`), size of VCF on disk 2175071049 bytes (2.0G). (all false postive probs are predicted, it'd be easy to measure it too). Map keys are contig names. ```; Map of String->BloomFilter with 0.001 false positive prob = 27320529 bytes (26M); Map of String->BloomFilter with 0.0004 false positive prob = 30943001 bytes (30M); Map of String->BloomFilter with 0.0001 false positive prob = 36423625 bytes (35M); Map of String->BloomFilter with 0.00004 false positive prob = 40046089 bytes (38M); Map of String->BloomFilter with 0.00001 false positive prob = 45526681 bytes (43M); Map of String->BloomFilter with 0.000001 false positive prob = 54629745 bytes (52M); Map of String->int[] of positions = 60790452 bytes (58M); List<GATKVariant> made just like the one in spark BQSR = 366463957 bytes (349M); ```. Variants from dbSNP cover 0.004 of the genome (15195436 bases of 3101804739) so if we want reasonable precision (number of false positives over all reported hits), say 0.9 precision (of 10 hits only 1 can be false) we need (1-0.9) x 0.004 false positive prob = 0.0004. For 0.99 precision (of 100 hits only 1 can be false) we need (1-0.99) x 0.004 false postive prob = 0.00004. These are approximations of course. Given these numbers, I conclude that, for now, exploring BloomFilters does not seem to make sense (too little saving and too many complications with using a probabilistic data structure - eg we'd need to use it too for the walker BQSR). It does make sense however to explore alternatives to the list of GATKVariants because it's very big when serialized (maybe Kryo does a better job but it's still a big object). A simple alternative like sorted int[] may be sufficient and has attractive properties (trivial to implement and understand, O(log) lookups, 0% false positives, small size when serialized).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1407#issuecomment-203727133:206,predict,predicted,206,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1407#issuecomment-203727133,1,['predict'],['predicted']
Safety,"For reporting the number of reads that fails each of the filters, the composed filter could be changed by a `CountingReadFilter`; using the `getSummaryLine()` method will provide the number of reads failing each of the components. Developers could have in their tools a field with the `WellFormedReadFilter` and call a new method for reporting the summary, to log a warning/debug line. For exploding depending on the tool, maybe an advance/hidden argument can be added to the filter (something like `--failOnMalformed`) to throw an exception if true; developers might add a default filter with this value equals to true if they want to enforce by default this behaviour. I think that this a simpler idea for allow the developer to choose, and give some flexibility to the user to change the behaviour as its own risk (they can disable all filters anyway, which is also risky).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3454#issuecomment-323698699:812,risk,risk,812,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3454#issuecomment-323698699,2,['risk'],"['risk', 'risky']"
Safety,Friendly ping @fleharty! I will really appreaciatte if this can go into before #2185 to avoid the maintenance of the deprecated `PerReadAlleleLikelihoodMap` just for me. Thanks a lot!,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2154#issuecomment-255859144:88,avoid,avoid,88,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2154#issuecomment-255859144,1,['avoid'],['avoid']
Safety,GATK forum docs mentioning hdfview are:; https://gatk.broadinstitute.org/hc/en-us/articles/360035531712-HDF5-format; https://gatk.broadinstitute.org/hc/en-us/articles/360035889651-Are-there-any-Broad-specific-instructions-for-using-GATK-; https://gatk.broadinstitute.org/hc/en-us/articles/360035531152; https://gatk.broadinstitute.org/hc/en-us/articles/360035531092--How-to-part-I-Sensitively-detect-copy-ratio-alterations-and-allelic-segments; and maybe a few more -- the forum search is terrible. Maybe @gbrandt6 can take care of those?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6927#issuecomment-729948755:393,detect,detect-copy-ratio-alterations-and-allelic-segments,393,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6927#issuecomment-729948755,1,['detect'],['detect-copy-ratio-alterations-and-allelic-segments']
Safety,George -- thanks much for debugging and identifying the underlying problem. I can confirm that we're able to avoid the error by removing `-XX:+UseSerialGC` and moving back to parallel GC. We'd initially introduced the serial GC usage to avoid problems when running multiple HaplotypeCaller commands simultaneously on a single machine but by letting the Spark implementation take care of parallelizing we should no longer need to worry about that. Thanks again for the workaround and the tip on using `spark.local.dir`. Much appreciated.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3605#issuecomment-333837665:109,avoid,avoid,109,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3605#issuecomment-333837665,2,['avoid'],['avoid']
Safety,"Going to risk it and take the ""I like this"" as approval :smirk:",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5157#issuecomment-418943215:9,risk,risk,9,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5157#issuecomment-418943215,1,['risk'],['risk']
Safety,Good. We need them on board if we want to avoid forking.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1210#issuecomment-161338397:42,avoid,avoid,42,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1210#issuecomment-161338397,1,['avoid'],['avoid']
Safety,"Got another one in the same branch at https://travis-ci.com/github/broadinstitute/gatk/jobs/300319727, this time in HaplotypeCallerSparkIntegrationTest.testNonStrictVCFModeIsConsistentWithPastResults. @lbergelson this branch already updates the base image to 18.04, but I haven't yet made any updates to .travis.yml as you do in https://github.com/broadinstitute/gatk/tree/lb_update_docker_ubuntu. Think that could be causing these issues?. I'll try to debug a bit once I sort out the python stuff. ```; org.broadinstitute.hellbender.tools.HaplotypeCallerSparkIntegrationTest > testNonStrictVCFModeIsConsistentWithPastResults[0](/gatkCloneMountPoint/src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam, /gatkCloneMountPoint/src/test/resources/large/human_g1k_v37.20.21.fasta) FAILED; org.apache.spark.SparkException: Job aborted.; at org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:100); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1083); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1081); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1081); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); at org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopDataset(PairRDDFunctions.scala:1081); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply$mcV$sp(PairRDDFunctions.scala:1000); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply(PairRDDFunctions.scala:991); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply(PairRDDFunctions.scala:991); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationS",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6513#issuecomment-601702690:834,abort,aborted,834,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6513#issuecomment-601702690,1,['abort'],['aborted']
Safety,"Great detective work!. Hadoop-BAM appears to still be an active-ish project, it's worth telling them we found a bug and have a BAM that can reproduce the problem. Hopefully, they can pick it up and fix it quickly (if not we can fix it). However, in even in the best case they still need to go another cut for us to get the fix.; Two things:; - (long shot) is it fixed in `org.seqdoop:hadoop-bam:7.2.0`?; - Try to write out the index for each shard. From a quick skim, I didn't see anything about writing out indices in Hadoop-BAM, so I think we'd have to get the name of each BAM, put that into an RDD, and in a forEach read in the bam and create the index. @droazen, does that sound hard?. If it's not hard, I'd like to get this unblocked by writing out the index and then wait/work on the real fix.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1005#issuecomment-149566941:6,detect,detective,6,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1005#issuecomment-149566941,1,['detect'],['detective']
Safety,"Great, thanks for adding this @cmnbroad! Let me manually check that there weren't any numerical changes in the gCNV WDL tests. @asmirnov239 and Jack might also want to test on some small, real data. Probably overkill, but just to be safe... We'll get back to you!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6494#issuecomment-597333928:233,safe,safe,233,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6494#issuecomment-597333928,1,['safe'],['safe']
Safety,"HI @lbergelson - ; I'm working on a bug/warning in the variant calling workflow where it's complaining about not finding a logger:. `21:04:59.525 INFO ProgressMeter - Starting traversal; 21:04:59.526 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; log4j:WARN No appenders could be found for logger (io.grpc.netty.shaded.io.netty.util.internal.logging.InternalLoggerFactory).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 21:05:10.018 INFO ProgressMeter - chr1:4642050 0.2 205000 1172992.6`. I found that if I had gatk's build.gradle NOT exclude the log4j.properties file I get rid of that warning, so I'm trying to understand the issue [here](https://github.com/broadinstitute/gatk/blob/33bda5e08b6a09b40a729ee525d2e3083e0ecdf8/build.gradle#L441): (where you found log4j.properties clashed with log4j2.xml) . James Emery is on the git blame for that, but he thinks that's because of the refactoring he did. Thanks in advance - I'm not sure if there's something else I should be doing with the xml version of that file to avoid this warning.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7778#issuecomment-1098029722:1157,avoid,avoid,1157,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7778#issuecomment-1098029722,1,['avoid'],['avoid']
Safety,"Hang on, it looks like this work might have been redundant with changes in https://github.com/broadinstitute/gatk/pull/3917/files.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3911#issuecomment-349428395:49,redund,redundant,49,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3911#issuecomment-349428395,1,['redund'],['redundant']
Safety,HeapIntBuffer.<init>; 0.0% 1 + 0 org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetDataEncryptionKeyRequestProto.newBuilder; 0.0% 1 + 0 java.util.IdentityHashMap$Values.size; 0.0% 1 + 0 org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.<clinit>; 0.0% 1 + 0 org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$Rename2RequestProto.<clinit>; 0.0% 1 + 0 org.bdgenomics.adam.models.SingleReadBucketSerializer.<init>; 0.0% 1 + 0 java.util.regex.Pattern$Branch.<init>; 0.0% 1 + 0 hbparquet.hadoop.util.ContextUtil.getConfiguration; 0.0% 1 + 0 sun.reflect.generics.visitor.Reifier.reifyTypeArguments; 0.0% 1 + 0 scala.collection.immutable.Map$Map3.<init>; 0.0% 1 + 0 sun.reflect.ByteVectorImpl.trim; 0.6% 62 + 27 Total interpreted (including elided). Compiled + native Method ; 9.5% 1441 + 0 com.ning.compress.lzf.impl.UnsafeChunkEncoderLE.tryCompress; 2.0% 304 + 0 htsjdk.samtools.util.BlockCompressedOutputStream.write; 1.8% 275 + 0 com.ning.compress.lzf.impl.UnsafeChunkDecoder.decodeChunk; 1.2% 176 + 2 htsjdk.samtools.BinaryTagCodec.readTags; 1.2% 176 + 0 org.broadinstitute.hellbender.relocated.com.google.common.collect.Multimaps.index; 1.1% 174 + 1 htsjdk.samtools.SAMUtils.bytesToCompressedBases; 0.6% 82 + 2 htsjdk.samtools.BAMRecordCodec.encode; 0.5% 83 + 0 org.broadinstitute.hellbender.relocated.com.google.common.collect.AbstractMapBasedMultimap.put; 0.5% 82 + 0 java.util.Iterator.forEachRemaining; 0.5% 74 + 0 org.apache.spark.util.collection.TimSort$SortState.mergeLo; 0.5% 70 + 1 org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSparkUtils.lambda$markPairedEnds$66146993$1; 0.4% 65 + 0 java.util.stream.ReferencePipeline.collect; 0.4% 64 + 0 htsjdk.samtools.util.BlockCompressedInputStream.read; 0.4% 56 + 2 org.apache.spark.util.collection.TimSort$SortState.mergeHi; 0.3% 47 + 0 org.broadinstitute.hellbender.utils.read.markduplicates.OpticalDuplicateFinder.getRapidDefaultReadNameRegexSplit; 0.3% 46 + 0 htsjdk.samtools,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1702#issuecomment-210127581:2459,Unsafe,UnsafeChunkDecoder,2459,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1702#issuecomment-210127581,1,['Unsafe'],['UnsafeChunkDecoder']
Safety,"Hello @abdohlman, apologies for the late response. A heartbeat timeout usually means one of the executors is crashing. If you are able to inspect the error logs from each worker node, you may find which one it was and why. It is likely that one or more are running out of memory.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4725#issuecomment-398506155:63,timeout,timeout,63,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4725#issuecomment-398506155,1,['timeout'],['timeout']
Safety,Hello @bbimber thank you for the response. I would recommend using the read filters (in your case `-rf MappingQualityReadFilter --minimum-mapping-quality ##` to achieve the same functionality as the `-mmq` argument from GATK3. When porting over the tool we tried to push as much functionality from obscure arguments into the existing filtering framework as possible and `-mmq` was one of the ones that was redundant as it was a simple filter placed on the reads before counting them which the existing filtering code was able to handle. I will add some lines to the documentation clarifying this for users in the future.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6617#issuecomment-634752928:406,redund,redundant,406,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6617#issuecomment-634752928,1,['redund'],['redundant']
Safety,Hello @cmnbroad @ldgauthier - just following up here. It seems like we have passing tests and general approval of this change. I believe the only question is whether removing toString() from VariantAnnotation is safe. As noted above it doesnt seem to be needed.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7041#issuecomment-780808041:212,safe,safe,212,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7041#issuecomment-780808041,1,['safe'],['safe']
Safety,"Hello @nalinigans,. As part of gatk-sv pipeline we are using GATK : v4.1.8.1 which doesn't have bypass-feature-reader option. Also, we didn’t capture strace for the run with just ""--genomicsdb-shared-posixfs-optimizations"" so wont be able to share the FUTEX process counts. So after using v4.2.4.1 we get below results. 	- Using ""--genomicsdb-shared-posixfs-optimizations"" & ""--bypass-feature-reader"" the process took 118 mins.; ""FUTEX_WAIT_PRIVATE, 0, NULL"" : 1266. 	- Using ""--genomicsdb-shared-posixfs-optimizations"" & ""--bypass-feature-reader"" and ; TILEDB_UPLOAD_BUFFER_SIZE=5242880 as env variable the process took 113 mins.; 	""FUTEX_WAIT_PRIVATE, 0, NULL"" : 3. 	- Even using 10 MB as buffer size resulted in same execution time of 113 mins.; 	- Using a buffer size bigger i.e. 50 MBs caused the process to run slower so we aborted it. Please let us know if we can improve it further.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7646#issuecomment-1040947845:830,abort,aborted,830,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7646#issuecomment-1040947845,1,['abort'],['aborted']
Safety,"Hello, Could you tell me the exact source websites of funcotator_dataSources.v1.7.20200521g? I did not find it in your Google Cloud (genomics-public-data).; Besides, I used this code to download`./gatk-4.1.9.0/gatk FuncotatorDataSourceDownloader --germline --validate-integrity --extract-after-download; `, but the error appeared as following:`Nov 18, 2023 1:15:05 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 13:15:05.202 INFO FuncotatorDataSourceDownloader - ------------------------------------------------------------; 13:15:05.203 INFO FuncotatorDataSourceDownloader - The Genome Analysis Toolkit (GATK) v4.1.9.0; 13:15:05.203 INFO FuncotatorDataSourceDownloader - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:15:05.203 INFO FuncotatorDataSourceDownloader - Executing as yaoxq@mu01 on Linux v3.10.0-693.el7.x86_64 amd64; 13:15:05.203 INFO FuncotatorDataSourceDownloader - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_151-b12; 13:15:05.203 INFO FuncotatorDataSourceDownloader - Start Date/Time: November 18, 2023 1:15:04 PM CST; 13:15:05.203 INFO FuncotatorDataSourceDownloader - ------------------------------------------------------------; 13:15:05.203 INFO FuncotatorDataSourceDownloader - ------------------------------------------------------------; 13:15:05.204 INFO FuncotatorDataSourceDownloader - HTSJDK Version: 2.23.0; 13:15:05.204 INFO FuncotatorDataSourceDownloader - Picard Version: 2.23.3; 13:15:05.204 INFO FuncotatorDataSourceDownloader - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 13:15:05.204 INFO FuncotatorDataSourceDownloader - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 13:15:05.204 INFO FuncotatorDataSourceDownloader - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 13:15:05.204 INFO FuncotatorDataSourceDownloader - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 13:15:05.204 I",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8275#issuecomment-1817434417:473,detect,detect,473,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8275#issuecomment-1817434417,1,['detect'],['detect']
Safety,"Hello:. Thanks for info. . I don’t suppose it is my role to militate/plead for the solid fix of the this omission. But I must say that it would be appreciated and in its own way advance science. Thanks for any consideration. Cheers,; Chuck. > On 10/Feb/2017, at 9:45 AM, Valentin Ruano Rubio <notifications@github.com> wrote:; > ; > @vruano not in my radar either.... the rewrite of the assembly may fix some of these cases where there is actually some variation that we fail to detect (false negative) that would explain those soft clips. However I don't think that would fix all the cases.; > ; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub, or mute the thread.; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/269#issuecomment-279021246:479,detect,detect,479,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/269#issuecomment-279021246,1,['detect'],['detect']
Safety,"Here the error log ; Using GATK jar /share/scientific_bin/gatk/4.1.4.1/gatk-package-4.1.4.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /share/scientific_bin/gatk/4.1.4.1/gatk-package-4.1.4.1-local.jar IndexFeatureFile -I output/called/final/allsites.filtered.vcf; 00:57:08.257 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/scientific_bin/gatk/4.1.4.1/gatk-package-4.1.4.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Sep 11, 2023 12:57:08 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 00:57:08.789 INFO IndexFeatureFile - ------------------------------------------------------------; 00:57:08.789 INFO IndexFeatureFile - The Genome Analysis Toolkit (GATK) v4.1.4.1; 00:57:08.789 INFO IndexFeatureFile - For support and documentation go to https://software.broadinstitute.org/gatk/; 00:57:08.790 INFO IndexFeatureFile - Executing as [ychrysostomakis@compute-0-3.local](mailto:ychrysostomakis@compute-0-3.local) on Linux v3.10.0-1160.53.1.el7.x86_64 amd64; 00:57:08.790 INFO IndexFeatureFile - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_231-b11; 00:57:08.790 INFO IndexFeatureFile - Start Date/Time: 11. September 2023 00:57:07 MESZ; 00:57:08.790 INFO IndexFeatureFile - ------------------------------------------------------------; 00:57:08.790 INFO IndexFeatureFile - ------------------------------------------------------------; 00:57:08.790 INFO IndexFeatureFile - HTSJDK Version: 2.21.0; 00:57:08.790 INFO IndexFeatureFile - Picard Version: 2.21.2; 00:57:08.790 INFO IndexFeatureFile - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 00:57:08.790 INFO IndexFeatureFile - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 00:57:08.790 INFO IndexFeatureFile - HTSJDK Defaults.USE_ASYNC_I",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8372#issuecomment-1733069316:732,detect,detect,732,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8372#issuecomment-1733069316,1,['detect'],['detect']
Safety,"Here's a stack trace of the area I think the two minute wait may be occurring. The below example fails-fast and prints out stack trace when there is no internet. I suspect that the slow-and-quiet alternative occurs when the connection to [google](https://github.com/googleapis/google-cloud-java/blob/v0.72.0/google-cloud-clients/google-cloud-core/src/main/java/com/google/cloud/ServiceOptions.java#L450) is blocked vs. completely unavailable. ```java; Dec 02, 2018 7:50:25 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; WARNING: Failed to detect whether we are running on Google Compute Engine.; java.net.ConnectException: Connection refused (Connection refused); at java.net.PlainSocketImpl.socketConnect(Native Method); at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350); at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206); at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188); at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392); at java.net.Socket.connect(Socket.java:589); at sun.net.NetworkClient.doConnect(NetworkClient.java:175); at sun.net.www.http.HttpClient.openServer(HttpClient.java:463); at sun.net.www.http.HttpClient.openServer(HttpClient.java:558); at sun.net.www.http.HttpClient.<init>(HttpClient.java:242); at sun.net.www.http.HttpClient.New(HttpClient.java:339); at sun.net.www.http.HttpClient.New(HttpClient.java:357); at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1220); at sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1156); at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1050); at sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:984); at shaded.cloud_nio.com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:104); at shaded.cloud_nio.com.google.api.client.http.HttpRe",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3491#issuecomment-443830843:584,detect,detect,584,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3491#issuecomment-443830843,1,['detect'],['detect']
Safety,"Here's a suggested set of things to look at as part of this ticket:. -See if we can avoid fetching all headers on startup by passing in the needed info via alternate args (https://github.com/broadinstitute/gatk/issues/2639). -Do profiling to find an appropriate value for the --batchSize argument,; once it's merged (https://github.com/broadinstitute/gatk/issues/2641). -Shrink NIO buffers (--cloudPrefetchBuffer and --cloudIndexPrefetchBuffer) down to the smallest values that still produce acceptable performance (https://github.com/broadinstitute/gatk/issues/2640). Thibault of red team aka @Horneth has agreed to take this on.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2633#issuecomment-298338616:84,avoid,avoid,84,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2633#issuecomment-298338616,1,['avoid'],['avoid']
Safety,"Here's the work I've done so far to implement an overlaps partitioner: https://github.com/broadinstitute/gatk/compare/tw_overlap_partitioner. The idea is to coalesce neighbouring partitions so that the task processing them can see (but not process) reads from neighbouring partitions. E.g. the task processing partition _i_ can see reads from partition _i-1_ and partition _i+1_. This is done using Spark 2's _coalesce_ method that takes a PartitionCoalescer: http://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html#coalesce(int,%20boolean,%20scala.Option,%20scala.math.Ordering). In this PR I backported some of that logic so it works with Spark 1.6. It turns out that this is not faster than the groupByKey, primarily since it has to scan over the input twice: first to find the extents of the reads in each partition, second to process each (coalesced) partition. It should be possible to avoid the first scan by just reading the first read in each partition and using a sequence dictionary to find the end points of contigs. This is a bit more involved, although it should be faster than groupByKey. (I tried caching the input to see if this helped, but it didn't.)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1988#issuecomment-241773735:911,avoid,avoid,911,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1988#issuecomment-241773735,1,['avoid'],['avoid']
Safety,"Here's what the code will be, I think:; ```; if (refSeq.length == altSeq.length && IntStream.range(0, refSeq.length).filter(n -> refSeq[n] != altSeq[n]).count() < 3) {; return same shortcut as before; }; ```. So 5 was actually an overestimate, unless we avoid the stream.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5459#issuecomment-442897874:254,avoid,avoid,254,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5459#issuecomment-442897874,1,['avoid'],['avoid']
Safety,"Hey @jean-philippe-martin, this looks good. I've made some very small changes to avoid the back-and-forth of a review, and rebased the branch onto latest master. . The main change I made was in `IOUtils.getPath()`. It now traps the `IOException` and throws a `UserException` instead. This has the benefit of not requiring client code to put `throws IOException` (or catch the `IOException`) everywhere, and is more consistent with the rest of the codebase, which typically traps checked exceptions as early as possible and wraps them in unchecked exceptions (either `UserException` or `GATKException`, depending on whether it's likely to be the user's fault or not). Also made a small change in `NioBam` to make it use a logger instead of `System.out.println()` for debug output.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2224#issuecomment-259798014:81,avoid,avoid,81,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2224#issuecomment-259798014,1,['avoid'],['avoid']
Safety,"Hi . I have a few questions about using Mutect2 versus HaplotypeCaller to call mitochondrial (or chloroplast) variants (I'm working on plants). 1) In article 11127, it says; ""The tool can run on unmatched tumors but this produces high rates of false positives. Technically speaking, somatic variants are both (i) different from the control sample and (ii) different from the reference."". In my case, there isn't a ""normal"" sample to compare the ""tumour"" (i.e. mitochondria) to, just a reference. Are my results likely to be prone to false positives then? Or is it that the case for mitochondria is different because we are not truly looking for somatic variants but rather variants in general that may not be 100% ""pure"" (because of heteroplasty?). Is the latter point the justification for Mutect vs HaplotypeCaller in the first place?. 2) I am interested in both variant and invariant sites (relative to the reference). Ultimately, I want to be able to go base for base and make a call (ref, alt, or ""N""; where ""N"" is used where I have no confidence in the base call at that site). The goal is to have a full haplotype sequence for the mitochondria/chloroplast of each sample. . In HaplotypeCaller, I read that I could use --emit-ref-confidence BP_RESOLUTION to get the confidence of a site being homozygous reference. Does --out-mode EMIT_ALL_CONFIDENT_SITES give similar information?. 3) In this thread, it's said that the --min-pruning argument is very important. I'm very new to this and don't fully understand what this parameter is doing. Is the advice to set this parameter = ceiling(average coverage/500) general? Or specific to the project mentioned above?. 4) I don't why we don't have to set the sample ploidy to 1? Is this only for applications where we want to detect heteroplasmy? If we are after the majority haplotype, does it make sense to set this parameter?. Many thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5193#issuecomment-432443695:1776,detect,detect,1776,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5193#issuecomment-432443695,1,['detect'],['detect']
Safety,"Hi @OgnjenMilicevic ,. Is this the latest GATK version? Highly multi-allelic sites are known to cause a slowdown in GenotypeGVCFs, but this was dramatically improved with the `-newQual` option, made default in 4.1.5.0. 110 whole genomes should be manageable, but excluding the low complexity regions could avoid some of the highly multi-allelic variants.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6896#issuecomment-710147958:306,avoid,avoid,306,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6896#issuecomment-710147958,1,['avoid'],['avoid']
Safety,"Hi @Yyx2626, I'm Geraldine, you may remember me from the Beijing training. It was great visiting your team! I'm sorry it took me so long to follow up on this discussion, and I want to thank you again for reaching out to us about integrating the tool that you developed into GATK. We are certainly very interested in providing this enhancement to the research community, and we are now ready to talk about the next steps. . After examining your paper and the source code in Github, we think that the most efficient way to integrate the functionality you developed would be to adapt the filtering parts of your tool to run on the output of Mutect2. So this would be a standalone tool that you would run after Mutect2, much like the current FilterMutectCalls tool. . If the results are comparable to your current tool, then we would take that into the official distribution of GATK. If somehow that integration does not yield satisfactory results, then we would look at integrating the entire tool, though we're hoping it won't be necessary, so we can avoid maintaining duplicate functionality for some of the boilerplate data transformations. . David @davidbenjamin can provide some advice on how to implement this in GATK4; in brief you would need to write some code that applies the filters you developed to a variant context. Let us know if this is an option you'd like to explore; we'd be happy to help.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4632#issuecomment-403101973:1049,avoid,avoid,1049,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4632#issuecomment-403101973,1,['avoid'],['avoid']
Safety,"Hi @bbimber . So regarding this first question:. > the guts of a GenomicsDB workspace has one folder per contig anyway. Is there a reasonable way to merge multiple workspaces together?. You can try to pull together each of the contig folders under a single workspace. You just need a single copy of the callset.json, vidmap.json, vcfheader.vcf, and __tiledb_workspace.tdb in the merged workspace. As @ldgauthier suggests, it falls under the ""probably works but unsupported"" umbrella. Most of the obvious pitfalls can be avoided by ensuring the same set of VCF files get used by the `GenomicsDBImport` call for each of the multiple workspaces (so the only difference in each command should be the interval list). I can't quite be sure, but the last part of your comment/question might be asking if some tool or option supports reading/merging from multiple genomicsdb workspaces. No such tool currently exists.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6557#issuecomment-630573640:520,avoid,avoided,520,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6557#issuecomment-630573640,1,['avoid'],['avoided']
Safety,"Hi @bbimber, our next release is imminent -- we're just waiting on 1-2 final PRs to be merged. I can safely say it will go out this week.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7322#issuecomment-865082820:101,safe,safely,101,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7322#issuecomment-865082820,1,['safe'],['safely']
Safety,"Hi @hh1985 . Memory tuning is pretty tricky and can depend on a lot of things. How is your cluster configured? ; Are you using YARN? Are you running in client or cluster mode? . I'm assuming you're running with YARN. Mesos should also work but I don't have any experience configuring it. . BQSR should run safely with 4g of memory per core. (It should really work with much less I think, but 4 should definitely be sufficient.) There are a few different parameters that can help you adjust the memory ratios.; A good tuning might be something like; ; ```; --num-executors 5 ; --executor-cores 8 ; --executor-memory 32g ; ```. if you're not running with gatk-launch you'll need to set; ```; --conf spark.yarn.executor.memoryOverhead=600; ```; Without setting a higher than default yarn memory overhead like this we see consistent crashes, it's included in the settings gatk-launch applies already. That should run 5 separate executors with 8 cores each and give each one 32g, so 4g / core. . If you're running in cluster mode you'll have to carve out some memory and cores for the driver. You can set the driver settings with ; ```; --driver-cores 2; --driver-memory 4g; ``` ; or something along those lines. The driver doesn't need much memory or computer for BQSR. In general we've had better luck using the entire cluster for one job and running jobs in sequence rather than trying to run two jobs simultaneously using a subset of the cluster.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3465#issuecomment-324064738:306,safe,safely,306,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3465#issuecomment-324064738,1,['safe'],['safely']
Safety,"Hi @jjfarrell,. It's hard to know what might be going wrong in these files. Can you describe how you are running `StructuralVariationDiscoveryPipelineSpark` -- on a local Spark cluster, on GCS dataproc, or in Spark local mode? Can you provide the command line?. Is there anything you can identify as being different about the failing files, maybe from other WGS metrics: higher coverage, high duplicate rate or chimera rate, etc? Are these human germline or cancer samples?. One initial thought could be that something is running out of memory when processing these samples, or getting bogged down in garbage collection. You could try increasing the parameters you give for `--driver-memory`, `--executor-memory`, or `--conf spark.yarn.executor.memoryOverhead`. There may be other Spark parameters you could try adjusting as well. Our default scripts run with these Spark options on a GCS Dataproc cluster:. ```; -- \; --spark-runner GCS \; --cluster ""${CLUSTER_NAME}"" \; --num-executors ${NUM_EXECUTORS} \; --driver-memory 30G \; --executor-memory 30G \; --conf spark.yarn.executor.memoryOverhead=5000 \; --conf spark.network.timeout=600 \; --conf spark.executor.heartbeatInterval=120; ```. You could try increasing those memory values (if you have the resources) and see if that helps.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4635#issuecomment-380130398:1127,timeout,timeout,1127,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4635#issuecomment-380130398,1,['timeout'],['timeout']
Safety,Hi @lbergelson and @droazen: it looks like @lbergelson added a limit to avoid excessive sources. Does that satisfy issues remaining on this PR?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8752#issuecomment-2052269547:72,avoid,avoid,72,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8752#issuecomment-2052269547,1,['avoid'],['avoid']
Safety,"Hi @lbergelson thank you for looking into this. After a lot of trial and error that's what I figured as well. It would be interesting to know what assumptions are broken and if there is a way to avoid it or even to make the --remove-unused-alternates option compatible with FastaAlternateReferenceMaker.; What I still don't know is if the tool grabs the correct alternate when it sees more than 1 at a site. If yes, the above option is almost unnecessary, but it would be nice if it worked.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7433#issuecomment-904755077:195,avoid,avoid,195,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7433#issuecomment-904755077,1,['avoid'],['avoid']
Safety,"Hi @lbergelson. Thanks for the quick response!. I believe we started with a FASTQ file that had the header I listed above:; `HWI-ST700660_163:1:1101:1243:1870#1@0/1`. Which we later converted to a bam using samtools that contains this header:; `HWI-ST700660_163:1:1101:1243:1870#1@0`; after alignment with BWA-MEM2. . My team thinks it might be the `@` from the FASTQ header since modifying it to use a format without the additional `@`, such as `@SRR5456220.1 1 length=101`, seemed to allow it to work properly in our workflow. Thank you for the suggestions! We will try that out so we can try to avoid detecting and changing each header. We really appreciate it!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8134#issuecomment-1360234528:598,avoid,avoid,598,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8134#issuecomment-1360234528,2,"['avoid', 'detect']","['avoid', 'detecting']"
Safety,"Hi @pieterlukasse. This is a great question and somewhat timely. Funcotator hasn't gotten the attention it needs lately because the engineer who's most involved has been extremely busy with other very high priority projects. We intend to support it going forward but it's unclear if that means bug fixes and reactive support or if we're able to make major upgrades. We're currently in the middle of somewhat of a resource crunch, and we are actively planning how to prioritize our attention in the future. . I think if you're basically happy with features now, you should feel safe investing the time into an output parser. If there are major improvements you need I would wait a week or two and ask again then because we'll have more clarity about what we can do. . If you're interested I would consider contributing back your code the GATK core. There's some existing utility code that could probably help you, but there's a long standing gap in our tooling for users to make sense of the funcotations and we'd welcome improvements or new tools. @jonn-smith @droazen",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8154#issuecomment-1378997296:577,safe,safe,577,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8154#issuecomment-1378997296,1,['safe'],['safe']
Safety,"Hi @wir963, looking back on this I see that the documentation is a bit ambiguous. `--bwa-score-threshold` is applied first during the microbe alignment step and actually is actually passed directly to `bwa mem` as the `-T` parameter (see http://bio-bwa.sourceforge.net/bwa.shtml). `--min-score-identity` is, as you noted, a fractional value of the read length between 0 and 1 that is applied during the final scoring phase to adjust how stringently aligned reads should be categorized as either known microbial or non-host/non-microbial (unknown). Both will affect sensitivity to microbe detection, but I would generally recommend only adjusting the latter.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6818#issuecomment-705021856:588,detect,detection,588,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6818#issuecomment-705021856,1,['detect'],['detection']
Safety,"Hi Ted, ; After we talked offline I went back and read the code again.; I still have some questions regarding the method; `static Cigar findLargeDeletions( final GATKRead read ) `. 1. It seems this will do redundant work in the following sense: for the same query sequence which has several alignments, each `GATKRead` (which is actually an alignment record) will be touched by this method, hence imagine one query sequence with two corresponding `GATKRead`s, the current implementation will emit two new `GATKRead`s. Seems unnecessary and double counting evidence.; 2. I don't see any check on same-chromosome-ness, i.e. `fields[0]`, which is formatted `SA:Z:<CHR_NAME>`.; 3. I see checking of overlap length on the query sequence between two alignment records, but I don't see a check of gap size on the query sequence between two alignment records. As we talked about this offline, it is a hard problem when the two alignment records are gapped away both on the reference and the query sequence, if you want to merge the two records into a single record.; 4. I am still not quite understanding line 1652 about why you are using the clip length to decide orders.; 5. Is there a reason to limit the del length to 2, which is the same as the threshold on `overlapLength`?. For the related method `recplaceCigar`, the method is not checking for negative values on `overlapLength`, which judged from `int Interval.overlapLength(Interval)` is possible.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6092#issuecomment-522145406:206,redund,redundant,206,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6092#issuecomment-522145406,1,['redund'],['redundant']
Safety,"Hi everyone! i partially solved the problem ""WARN GencodeFuncotationFactory - Cannot create complete funcotation for variant at chr__:___:___ due to alternate allele: *"".; The origin of the problem is that we have complex datasets that contain more than one sample. In the set of samples, more than one alternative allele is detected, including the ""*"". The idea is to have one line for each variant because, apparently, Funcotator reads it properly. I applied the following commands and it worked perfectly:. 1) Normalize: ; bcftools norm -m - cohort.vcf > cohort_norm.vcf. 2) Select SNPs (I haven't tried it for indels yet); gatk SelectVariants -R hg38.fa -V ""cohort_norm.vcf"" --select-type SNP -O ""cohort_snp.vcf.gz"". 3) Remove the * variants remaining:; awk -F'\t' '$5 != ""*""' cohort_snp.vcf > filtered_cohort_snp.vcf. 4) Apply Funcotator. At the moment this works perfectly for me. If anyone has a better solution please upload it. Regards",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6651#issuecomment-1733778016:325,detect,detected,325,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6651#issuecomment-1733778016,1,['detect'],['detected']
Safety,"Hi mwalker174,; I tried both command lines. As lbergelson predicted, the one with --spark-runner LOCAL produces the same error as before (see below, could you explain me why?), while the one with --spark-runner SPARK runs smoothly. . Is this option ok with running a master-workers system? Can I use this option safely with . I have now a different issue with PathSeqPipelineSpark? As I tried, the first error solved, but I have another issue (I think it's better to open a new thred for that, since it is about an input file not found). Thank you! . ```; -bash-4.1$ ../../../gatk PathSeqPipelineSpark --spark-master spark://xx.xx.xx.xx:7077 --input test_sample.bam --filter-bwa-image hg19mini.fasta.img --kmer-file hg19mini.hss --min-clipped-read-length 70 --microbe-fasta e_coli_k12.fasta --microbe-bwa-image e_coli_k12.fasta.img --taxonomy-file e_coli_k12.db --output output.pathseq.bam --verbosity DEBUG --scores-output output.pathseq.txt --spark-runner SPARK; Using GATK jar /scratch/home/int/eva/username/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-spark.jar; Running:; /home/int/eva/username/bin/spark-2.2.0-bin-hadoop2.7//bin/spark-submit --master spark://xx.xx.xx.xx:7077 --conf spark.driver.userClassPathFirst=false --conf spark.io.compression.codec=lzf --conf spark.driver.maxResultSize=0 --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 /scratch/home/int/eva/username/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-spark.jar PathSeqPipelineSpark --spark-master spark://xx.xx.xx.xx:7077 --input",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:58,predict,predicted,58,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,2,"['predict', 'safe']","['predicted', 'safely']"
Safety,"Hi mwalker174. I tryed with CountReadsSpark, same problem indeed. I do not think it is a java version problem, as without the option --spark-master the software runs smoothly (I guess it uses an included spark and libraries set). So this command runs:; CountReadsSpark --input test_sample.bam --output output.readcount.txt --verbosity DEBUG. While this does not:; CountReadsSpark --spark-master spark://xx.xx.xx.xx:7077 --input test_sample.bam --output output.readcount.txt --verbosity DEBUG ; And I get the error:; ```; 18/04/24 14:34:27 ERROR TaskSetManager: Task 0 in stage 0.0 failed 4 times; aborting job; 18/04/24 14:34:27 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool; 18/04/24 14:34:27 INFO TaskSchedulerImpl: Cancelling stage 0; 18/04/24 14:34:27 INFO DAGScheduler: ResultStage 0 (first at ReadsSparkSource.java:221) failed in 4.532 s due to Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, xx.xx.xx.xx, executor 0): java.lang.IllegalStateException: unread block data; at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2740); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1567); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:309); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383916494:597,abort,aborting,597,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383916494,2,['abort'],"['aborted', 'aborting']"
Safety,"Hi! I have the same issue as @chandrans.; When I run Mutect2 this is the error:; `(gatk) root@d387db9e4351:/Desktop# gatk Mutect2 -R /Desktop/UCSC_hg19_genome.fasta -I /Desktop/HP0049.bam -O /Desktop/HP0049.vcf.g; Using GATK jar /gatk/gatk-package-4.1.1.0-local.ja; Running:. java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.1.1.0-local.jar Mutect2 -R /Desktop/UCSC_hg19_genome.fasta -I /Desktop/HP0049.bam -O /Desktop/HP0049.vcf.g; 08:27:06.032 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.1.0-local.jar!/com/intel/gkl/native/libgkl_compression.s; Apr 23, 2019 8:27:10 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngin; INFO: Failed to detect whether we are running on Google Compute Engine. 08:27:10.882 INFO Mutect2 - -----------------------------------------------------------; 08:27:10.883 INFO Mutect2 - The Genome Analysis Toolkit (GATK) v4.1.1.; 08:27:10.883 INFO Mutect2 - For support and documentation go to https://software.broadinstitute.org/gatk/. 08:27:10.884 INFO Mutect2 - Executing as root@d387db9e4351 on Linux v4.9.125-linuxkit amd64. 08:27:10.884 INFO Mutect2 - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_191-8u191-b12-0ubuntu0.16.04.1-b12. 08:27:10.885 INFO Mutect2 - Start Date/Time: April 23, 2019 8:27:05 AM UT; 08:27:10.885 INFO Mutect2 - -----------------------------------------------------------; 08:27:10.886 INFO Mutect2 - -----------------------------------------------------------; 08:27:10.887 INFO Mutect2 - HTSJDK Version: 2.19.; 08:27:10.887 INFO Mutect2 - Picard Version: 2.19.; 08:27:10.887 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 2. 08:27:10.888 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : fals; 08:27:10.888 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : tru; 08:27:10.888 INFO Mutec",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4665#issuecomment-485729136:863,detect,detect,863,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4665#issuecomment-485729136,1,['detect'],['detect']
Safety,"Hi, ; for those looking to run containers within a multi-user HPC environment, running a container with default root privileges presents a potential data security risk. Adding something like :. RUN useradd -ms /bin/bash gatk; WORKDIR /home/gatk; USER gatk. to the Docker file would greatly reduce the risk and bring the current containers in line with general best practice, e.g https://medium.com/@mccode/processes-in-containers-should-not-run-as-root-2feae3f0df3b. There should be no downsides to running in this manner. Singularity could help but the current configuration will be picked up and prevented from running by any site using a container security scanner, e.g. Aqua.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3644#issuecomment-494457377:163,risk,risk,163,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3644#issuecomment-494457377,2,['risk'],['risk']
Safety,"Hi, regarding making --linked-de-bruijn-graph the default, I wanted to share that I had recently run mutect2 (gatkv4.2.6.1) on a larger cohort of samples with that option, some of which had variant calls from a previous mutect2 run (gatkv4.1.0.0) without this option. I noticed that plenty of known cancer drivers (e.g. KRAS p.G12C or PIK3CA p.E545* or BRAF p.V600*) that were present in a substantial number of samples (>10%ish) in the old calls were completely absent in the new calls. I had to add the option --recover-all-dangling-branches to recover those known hotspot mutations. They also all have very sufficient coverage (O(100)x) and high VAF to make them obvious true positives. I'd expect from mutect2 to be always able to call presence or absence of known hotspot mutations, so you should either look into further testing/debugging the linked de Bruijn graph option or also make it a default to recover all dangling branches.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7809#issuecomment-1125355262:514,recover,recover-all-dangling-branches,514,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7809#issuecomment-1125355262,3,['recover'],"['recover', 'recover-all-dangling-branches']"
Safety,"Hi. I encounter the same error with GATK4.0.4.0 and the python environment created by gatkcondaenv.yml. ```; 14:49:35.361 INFO CNNScoreVariants - Using key:CNN_1D for CNN architecture:/tmp/--------/1d_cnn_mix_train_full_bn.4552731615279398677.json and weights:/tmp/--------/1d_cnn_mix_train_full_bn.2635334538442041575.hd5; 14:49:36.747 INFO ProgressMeter - Starting traversal; 14:49:36.747 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 14:49:36.768 INFO ProgressMeter - unmapped 0.0 1 3157.9; 14:49:36.769 INFO ProgressMeter - Traversal complete. Processed 1 total variants in 0.0 minutes.; 14:49:36.772 INFO CNNScoreVariants - Shutting down engine; [May 9, 2018 2:49:36 PM CST] org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants done. Elapsed time: 0.08 minutes.; Runtime.totalMemory()=41160278016; org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException: Traceback detected: Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/home/--------/anaconda3/envs/gatk/lib/python3.6/site-packages/vqsr_cnn/vqsr_cnn/inference.py"", line 51, in score_and_write_batch; reference_batch.append(reference_string_to_tensor(fifo_data[4])); File ""/home/--------/anaconda3/envs/gatk/lib/python3.6/site-packages/vqsr_cnn/vqsr_cnn/inference.py"", line 107, in reference_string_to_tensor; raise ValueError('Error! Unknown code:', b); ValueError: ('Error! Unknown code:', '\x00'); >>>; 	at org.broadinstitute.hellbender.utils.python.StreamingPythonScriptExecutor.getAccumulatedOutput(StreamingPythonScriptExecutor.java:214); 	at org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants.onTraversalSuccess(CNNScoreVariants.java:390); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:894); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgr",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4727#issuecomment-387639368:941,detect,detected,941,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4727#issuecomment-387639368,1,['detect'],['detected']
Safety,"Hmm, looks like we lose events 1 and 3 with CollectReadCounts at 250bp using analogous ModelSegments parameters. However, I experimented with tweaking the segmentation to work on the copy ratios (rather than the log2 copy ratios), which seems to recover them. Although one of the goals of having evaluations backed by SV truth sets is to tune such parameters/methods, I'm beginning to think that SV integration might benefit from using the CNV tools in a more customized pipeline---especially if maximizing sensitivity at resolutions of ~100bp jointly with breakpoint evidence is the goal. For example, you might imagine a tool that directly uses CNV backend code to collect coverage over regions specified by `-L`, builds a PoN, denoises, and segments on the fly. Or we can put together a custom WDL optimized for sensitivity. Let's discuss in person?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4519#issuecomment-372875222:246,recover,recover,246,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4519#issuecomment-372875222,1,['recover'],['recover']
Safety,"Hmn, I'm not sure I know enough about this one to declare the .dict file dead. It seems like a good idea to remove it if it's redundant with the .fai though. Not high priority in any case.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/226#issuecomment-93486432:126,redund,redundant,126,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/226#issuecomment-93486432,1,['redund'],['redundant']
Safety,"How about externalcommandlineprogram ?. On Sunday, April 24, 2016, Steve Huang notifications@github.com wrote:. > I apologize, Adam. Final question before merging: do you think class; > CommandLineProgramModule should be renamed to something else (like; > ShellProgramModule or NonJavaProgramModule), to avoid confusion with; > CommandLineProgram?; > Thank you.; > ; > —; > You are receiving this because you were assigned.; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/gatk/pull/1701#issuecomment-214111637. ## . Sent from Gmail Mobile",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1701#issuecomment-214270997:304,avoid,avoid,304,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1701#issuecomment-214270997,1,['avoid'],['avoid']
Safety,"How much does count collection cost at the desired bin size? How does this compare to bincov? Perhaps we could eliminate one of these steps if redundant. Note that the read counts are read once and stored in memory, so unless this takes a significant amount of time, then indexing is probably not the highest priority here (although I agree it would be nice to have in general). One related issue, as you mention, is file localization---since each shard only operates on a portion of the counts in each sample, it is a bit wasteful to localize the whole file. But how much does file localization cost? I can't imagine that it is the lowest hanging fruit. One of the more important issues, which you also mention, is optimizing parameters for inference. This includes not only the minimum number of epochs for training, but also things like the learning rate, annealing schedule, iterations per epoch, conditions for epoch convergence, etc. I'll be talking about how to tune these inference parameters---as well as other things in the pipeline---at the next BSV meeting. Let's brainstorm more things to try and prioritize them.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5288#issuecomment-427562932:143,redund,redundant,143,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5288#issuecomment-427562932,1,['redund'],['redundant']
Safety,"I addressed partially your comments (and fixed a compilation error due to the tests using the previous arguments). One of the major points of discussion are the following:. * `Collection` instead of `List`: I think that the first is more flexible, because a client maybe wants to have a `LinkedHashSet` as the argument to avoid repetition of the same filter. I agree that the abstract class should discourage not honoring the user order.; * Access to methods/fields: I think that the plugin could be used outside GATK in a different way by extending it. I explained some of my usage cases in one of the comments in the code, but just by overriding a simple method the whole plugin could be used very nicely in some of them. I would prefer to do that than copy your code and re-implement the bits that I would like to change. Back to you for your ideas on this, @cmnbroad!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2355#issuecomment-275359208:322,avoid,avoid,322,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2355#issuecomment-275359208,1,['avoid'],['avoid']
Safety,"I addressed the comments, @droazen. But I found that the way to detect the codec for the files is relying on `canDecode`. But even if the a block-compressed file could be read by a codec through the `AbstractFeatureReader`, the `canDecode` for bed files returns false for block-compressed extension. This should be change at the htsjdk level if GATK is planing to read block-compressed feature inputs. The same for the codecs implemented in the framework (I did it for the `SAMPileupCodec`), but for instance the table codec could not be used in a compressed way because you can't create a tabix index for it...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2131#issuecomment-246296390:64,detect,detect,64,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2131#issuecomment-246296390,1,['detect'],['detect']
Safety,I agree a lot of the detailed content is redundant and not worth maintaining in separate places. Reducing to 2 or 3 lines seems a bit too drastic though -- I would love to see something like a half-page high-level overview to serve as cliff notes for the WDL.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5889#issuecomment-484356108:41,redund,redundant,41,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5889#issuecomment-484356108,1,['redund'],['redundant']
Safety,"I agree about avoiding `AuthHolder`, so here's a new PR that uses NIO for getting the header from GCS. I haven't set the reference on the `SamReaderFactory` since there is no `Path`-based method. This means that reading CRAMs from GCS will not work, but that's no worse than it is now.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2450#issuecomment-286163712:14,avoid,avoiding,14,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2450#issuecomment-286163712,1,['avoid'],['avoiding']
Safety,"I agree w/ Adam. Full names are more specific, avoid namespace collisions, and (depending on package sizes) could reduce compile time a bit (maybe insignificant for us though).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/138#issuecomment-71038147:47,avoid,avoid,47,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/138#issuecomment-71038147,1,['avoid'],['avoid']
Safety,"I also have the same problem here. I don't really care about the GQ field, but I do care about the AD fields. I get over-counting due to overlapping reads. @davidbenjamin what options should I use with Mutect2 to avoid this issue?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5436#issuecomment-521379039:213,avoid,avoid,213,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5436#issuecomment-521379039,1,['avoid'],['avoid']
Safety,"I am also seeing this warning 3x with 4.0.11.0 on a cluster but outside of docker (centos 6). . ```; 18:05:08.861 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg/gatk/4.0.11.0/install/bin/gatk-package-4.0.11.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Nov 24, 2018 6:05:09 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; WARNING: Failed to detect whether we are running on Google Compute Engine.; java.net.NoRouteToHostException: No route to host (Host unreachable); at java.net.PlainSocketImpl.socketConnect(Native Method); at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350); at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206); at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188); at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392); at java.net.Socket.connect(Socket.java:589); at sun.net.NetworkClient.doConnect(NetworkClient.java:175); at sun.net.www.http.HttpClient.openServer(HttpClient.java:463); at sun.net.www.http.HttpClient.openServer(HttpClient.java:558); at sun.net.www.http.HttpClient.<init>(HttpClient.java:242); at sun.net.www.http.HttpClient.New(HttpClient.java:339); at sun.net.www.http.HttpClient.New(HttpClient.java:357); at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1220); at sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1156); at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1050); at sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:984); at shaded.cloud_nio.com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:104); at shaded.cloud_nio.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:981); at shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials.runningOnComputeEngine(ComputeEngineCredential",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-441873417:431,detect,detect,431,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-441873417,1,['detect'],['detect']
Safety,"I am encountering a similar error: ; ```; Using GATK jar /nics/d/home/hchen3/bin/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /nics/d/home/hchen3/bin/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar GenotypeGVCFs -R /lustre/haven/proj/UTHSC0013/Tristan_GATK/reference/genome.fa -V gendb:///lustre/haven/proj/UTHSC0013/Tristan_GATK//DB/chr7 -G StandardAnnotation --use-new-qual-calculator -O /lustre/haven/proj/UTHSC0013/Tristan_GATK//gvcf//merged//joint_called_gvcfs_chr7.vcf; 23:15:47.053 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 23:15:47.249 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/nics/d/home/hchen3/bin/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 07, 2020 11:15:49 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 23:15:49.543 INFO GenotypeGVCFs - ------------------------------------------------------------; 23:15:49.545 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.1.2.0; 23:15:49.546 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 23:15:49.547 INFO GenotypeGVCFs - Executing as hchen3@acf-knl002 on Linux v3.10.0-514.26.1.el7.x86_64 amd64; 23:15:49.548 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_131-b12; 23:15:49.548 INFO GenotypeGVCFs - Start Date/Time: January 7, 2020 11:15:47 PM EST; 23:15:49.549 INFO GenotypeGVCFs - ------------------------------------------------------------; 23:15:49.549 INFO GenotypeGVCFs - ------------------------------------------------------------; 23:15:49.551 INFO GenotypeGVCFs - HTSJDK Version: 2.19.0; 23:15:",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6340#issuecomment-571886057:698,Redund,Redundant,698,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6340#issuecomment-571886057,1,['Redund'],['Redundant']
Safety,I am having the same issue with exome capture sequencing on 7 samples. The `CombineGVCFs` always aborts for some reason so `GenotypeGVCFs` fails. . The workaround mentioned above also doesn't work for me since I have to use a `csi` index and then `GenomicsDBImport` fails to find the index. I also have no idea how to define the the whole genome in the `-L` arguement,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6357#issuecomment-996278538:97,abort,aborts,97,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6357#issuecomment-996278538,1,['abort'],['aborts']
Safety,"I am still receiving security warnings about GATK 4.4.0.0:. Detected by File Paths: gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar; Detected by Library: pkg:java/log4j:log4j; CPE: cpe:/a:apache:log4j:1.2.17; Version End of Life Date: August 4th, 2015 at 7:00 PM",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8215#issuecomment-1513816621:60,Detect,Detected,60,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8215#issuecomment-1513816621,2,['Detect'],['Detected']
Safety,"I apologize, Adam. Final question before merging: do you think class `CommandLineProgramModule` should be renamed to something else (like `ShellProgramModule` or `NonJavaProgramModule`), to avoid confusion with `CommandLineProgram`?; Thank you.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1701#issuecomment-214111637:190,avoid,avoid,190,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1701#issuecomment-214111637,1,['avoid'],['avoid']
Safety,I believe this is the same issue as #1315. We need to either fix this or have gatk-launch detect and disallow local.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1386#issuecomment-166156617:90,detect,detect,90,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1386#issuecomment-166156617,1,['detect'],['detect']
Safety,"I confirmed gatk-4.4.0.0 HaplotypeCaller results with OpenJDK-17.0.7+07 ( Linux x64 and Linux arm version downloaded from https://adoptium.net/temurin/archive/?version=17 ) on x64 CPU ( AMD Ryzen Threadripper 1950X) and Arm CPU (ARMv8 Cortex-A53) .; A following variant was detected on Arm CPU but not detected on x64. `chr20 29521758 . A G 55.64 . AC=1;AF=0.500;AN=2;BaseQRankSum=0.311;DP=41;ExcessHet=0.0000;FS=8.502;MLEAC=1;MLEAF=0.500;MQ=56.14;MQRankSum=-4.689;QD=1.36;ReadPosRankSum=0.020;SOR=1.886 GT:AD:DP:GQ:PL 0/1:36,5:41:63:63,0,1373`. Additionally, QUAL value of some variants were different between on Arm CPU and x64 CPU. By Modifeing computeLogPenaltyScore in kBestHaplotype.java from Math.log10 to StrictMath.log10 as previous comment, same variant call results were produced on both CPUs.; https://github.com/broadinstitute/gatk/issues/8338#issuecomment-1560470696. I placed vcf files in following URL. You can see these differences.; https://pezycomputing-my.sharepoint.com/:f:/g/personal/sakai_pezy_co_jp/Eo5Gvfau1BpMszGCcfDrD14BOfMgxvk7Mt2JCFqcDfgItQ?e=wzZbpL. On x64: PFDATCV2HG002.pz_pipeline.GRCh38.chr20-29520758-29522758.vcf; On x64 Modified to StrictMath: PFDATCV2HG002.pz_pipeline.GRCh38.chr20-29520758-29522758.StrictMath.vcf; On ARM: raspberrypi2.vcf; On ARM Modified to StrictMath: raspberrypi2.StrictMath.vcf",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8338#issuecomment-1563841558:274,detect,detected,274,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8338#issuecomment-1563841558,2,['detect'],['detected']
Safety,I did a quick sanity check in htsjdk PrintReadsExample with and without intern() in SAMRecord and SAMSequenceRecord and see no difference on bam of 1.2GB. So this excessive intern() time is either Spark-specific or not htsjdk-related.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1657#issuecomment-209058404:14,sanity check,sanity check,14,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1657#issuecomment-209058404,1,['sanity check'],['sanity check']
Safety,I don't know... @cmnbroad Is there a way to detect if a cram requires a reference without reading the whole cram?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6665#issuecomment-645554060:44,detect,detect,44,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6665#issuecomment-645554060,1,['detect'],['detect']
Safety,"I don't see any reason that we need to make redundant disable be an error, since there is no harm in it. (We chose to throw for redundant **enable** because it has a small perf ramification; also, we can't just prune it out since if the redundant filter has args there will be ambiguity). @magicDGS I'd much prefer that we converge on the rules in this ticket before we make any more PRs, and then make one PR with all the changes, rather than fragmenting the fixes across multiple PRs. It makes it much easier to review, more likely that we'll get it right, and won't fragment the discussion across multiple tickets. Thx!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2377#issuecomment-276987423:44,redund,redundant,44,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2377#issuecomment-276987423,3,['redund'],['redundant']
Safety,"I don't think that hiding/disable arguments would work in every case: sometimes, an argument shouldn't be exposed but still available to set programmatically, or maybe just reduce visibility making it `@Hidden` and/or `@Advance`. What is the problem of making an interface for the top-level argument to the GATK? Changing the interface or the `CommadnLineProgram` has the same effect, but the API user can still behave the same as before. It is much more extensible and downstream-friendly. What's about making the `CLPConfigurationArgumentCollection` an interface always returning defaults to be able to change it in a proper way? The cycle of development of a new argument will be: 1) add a new method to the interface with a default returning what will be expected from the previous behaviour, 2) add and return by the argument in the GATK implementation, 3) use the getter in the CLP for perform the operation. This only adds the first point, and operating in 3 classes instead of 3. For API user it is really easy to maintain the previous behavior when upgrading the dependency by just using their own implementation of the class, or include the top-level new arguments by using the GATK implementation. It is much more flexible and extensible (I always think about GATK also as a library). In addition, I think that this approach is also important for evolving GATK. For example, if a new top-level argument is tagged as experimental (still not supported but requested in Barclay), removing it would allow to keep the interface (no version bump) the same and final users can still operate with the experimental argument. The same applies to the `GATKTool` base class (https://github.com/broadinstitute/gatk/issues/4341), and for downstream projects the aim should be to be able to extend safely the `CommandLineProgram` directly to implement their own toolkit using the powerful GATK framework.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3998#issuecomment-366185003:1794,safe,safely,1794,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3998#issuecomment-366185003,1,['safe'],['safely']
Safety,"I don't think we've made any guarantees about the thread safety of Funcotator or the associated datasource classes. . Also, this account seems to be a bot and I can't access its listed home page…. I can audit the class at some point.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7376#issuecomment-891860172:57,safe,safety,57,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7376#issuecomment-891860172,1,['safe'],['safety']
Safety,"I feel like this is going to be problematic in a different way than what @magicDGS is mentioning. We expect many versions of gatk to be compatible with the same python environment. Also for performance reasons we want to start avoid rebuilding the conda environment on every push and bake it into the base docker instead. This change means we definitely have to build it every time. . It feels like we need something more sophisticated. Instead of stamping the conda environment with the gatk version that matches it, maybe we should be stamping the gatk jar and the conda environment with some version based on the conda.env? Maybe we can do something like taking the md5 of the conda.yml and pushing that into both the jar manifest and the conda environment in some way? I'm guessing this scheme has an issue with the actual python code in the gatk since I think that's installed with conda as well? I'd really like to be able to preinstall the various dependencies though and then only update the code that's part of the gatk.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5081#issuecomment-411214721:227,avoid,avoid,227,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5081#issuecomment-411214721,1,['avoid'],['avoid']
Safety,"I fixed the problems, squashed the commits, pulled upstream master and rebased on that. But it doesn't look right - there is an unrelated commit in there. I'll probably just apply these changes to a new branch and do a PR on that just to be safe. I'll let you know when its ready to look at.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/549#issuecomment-113624185:241,safe,safe,241,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/549#issuecomment-113624185,1,['safe'],['safe']
Safety,"I fourth Joel on behalf of the users.Also, would it be possible to allow overlapping intervals without merging? Users have asked for this in order to handle alternative transcripts in walkers like DepthOfCoverage.On Wed, Dec 10, 2014 at 10:22 AM, ldgauthier notifications@github.com wrote: I third Joel. On Wed, Dec 10, 2014 at 10:20 AM, amilev notifications@github.com wrote:. > I second Joel, I had to produce base level resolution gVCF to avoid it; > ; > which is an ""ugly"" workaround; > ; > —; > ; > Reply to this email directly or view it on GitHub; > ; > https://github.com/broadinstitute/hellbender/issues/4#issuecomment-66466849; > ; > . —Reply to this email directly or view it on GitHub.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4#issuecomment-66469302:442,avoid,avoid,442,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4#issuecomment-66469302,1,['avoid'],['avoid']
Safety,"I have a good feeling about numerical instability from this point forward because:. * My terminology was lazy. It's not really ""numerical instability,"" which is a deep and frightening topic, but rather just plain old finite precision, which is not nearly so hydra-headed a problem.; * I learned the general rule for avoiding finite precision problems with a qual score, which is: always calculate probabilities of alleles being absent. Previously I was calculating the probability that samples had an allele and subtracting (in log space) that from 1. The problem with that is that for very good GQs this probability is so closed to 1 that quals can become infinite. In this PR we add up the probabilities of genotypes that don't have the allele, which is small but non-zero and everything works fine.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4614#issuecomment-434769078:316,avoid,avoiding,316,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4614#issuecomment-434769078,1,['avoid'],['avoiding']
Safety,"I have a version of this working in the branch `cw_phase_star_allele`, but am holding off on making a PR until https://github.com/broadinstitute/gatk/pull/6859 can be merged to avoid conflicting changes to integration test files.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5651#issuecomment-712158375:177,avoid,avoid,177,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5651#issuecomment-712158375,1,['avoid'],['avoid']
Safety,"I have no problems whatsoever with the code, but I do have some concerns about the design:. In the SV group's pipeline, we distribute this multi-gig file from its home in the cloud once at cluster-creation time, and then reuse it for multiple client executions. There are no superfluous copies lying about anywhere, and no redundant copying operations. We can give it any name we wish, and put it anywhere we desire (except that the path must be the same on every worker). This code, if I'm reading it correctly, will redistribute the file from a non-permanent home on the master's local file system or on the HDFS (to which it must be copied redundantly at least once per cluster instantiation), and then it will further be redundantly copied to a temporary location on each worker's local file system with every client execution. I don't know if that's overhead that we can live with, or whether that might prevent us from writing clients with brief execution times. I'm just opening the issue for discussion. We also lose a little flexibility in that the image must live in the same directory as the reference, though I don't think that's a serious drawback -- it's a perfectly logical place for it. However, since we're just appending a fixed extension ("".img"") to the reference name we can only have one image file per reference, which may be a problem because different images need to be created for different versions of bwa and for various options such as the list of alt contigs. We can handle the first problem by insisting that all clients on a particular cluster stick to one version of bwa, which is probably a good idea, anyway, but I think we're stuck if clients need to specify various alt contig lists. It might be better to provide a default path of ""ref-name""+"".img"", but allow that default to be overridden. Also, just to twist the knife a bit, it's too bad we never reviewed my PR for gatk-bwamem-jni, which version-stamped the images for safety. It's now languished since July, a",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3643#issuecomment-333598350:323,redund,redundant,323,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3643#issuecomment-333598350,3,['redund'],"['redundant', 'redundantly']"
Safety,"I have not had time to do any profiling, but I have looked at a lot of commits. I think it's likely that my recent changes cause some haplotypes with leading indels to be kept when previously they may have been dropped. It's hard to believe that this could cause a 10-20% slowdown via a commensurate increase in the number of haplotypes assembled. However, haplotypes with leading indels would have a disproportionate pair HMM cost since they would spoil caching of the read-haplotype pair HMM matrix at the very beginning of the matrix. That is, in addition to being particularly expensive haplotypes because they would diverge from the previous haplotype at the first position and therefore not benefit from caching at all, they would also completely destroy whatever caching the previous haplotype would have gotten. We ought to think about haplotypes that start or end with indels. It seems to me that they are bad news and very likely artifacts of assembly windows and/or reads that end in the middle of an STR. I would worry about discarding them outright, because what if all the real variation is attached to haplotypes like this. Therefore, I think the best thing to do is to choose assembly windows more carefully and increase or decrease padding to avoid ending in an STR. Avoiding assembly windows that end in STRs is a wise thing to do regardless, so how about I make a branch for that and we can see if the performance regression goes away?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6567#issuecomment-623054595:1260,avoid,avoid,1260,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6567#issuecomment-623054595,2,"['Avoid', 'avoid']","['Avoiding', 'avoid']"
Safety,"I have the seam problem with app engine and Mac OS ; INFORMACIÓN: Failed to detect whether we are running on Google Compute Engine.; I init cloud, I am SU my IDE is Eclipse.; Run as - App Engine, I don't have a problem with ubuntu, just MAC",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-559145335:76,detect,detect,76,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-559145335,1,['detect'],['detect']
Safety,"I have this class from an ancient branch that makes dealing with colors nicer in some ways. It tries to avoid printing colors to non-interactive things and it makes it harder to forget a reset:. ```; /**; * Provides ANSI colors for the terminal output *; */; public final class TerminalColors {. private TerminalColors(){};. private enum TerminalColor{; CYAN(""\u001B[36m""),; RED(""\u001B[31m""),; GREEN(""\u001B[32m""),; WHITE(""\u001B[37m""),; BOLD(""\u001B[1m""),; RESET(""\u001B[0m""); // reset the colors. private final String color;. TerminalColor(String color){; this.color = color;; }. public String getColorString(){; return color;; }. }. public static boolean isInteractive(){; return !(System.console() == null);; }. public static String cyan(String toColor){; return colorString(toColor, TerminalColor.CYAN);; }. public static String red(String toColor){; return colorString(toColor, TerminalColor.RED);; }. public static String green(String toColor){; return colorString(toColor, TerminalColor.GREEN);; }. public static String white(String toColor){; return colorString(toColor, TerminalColor.WHITE);; }. public static String bold(String toBold){; return colorString(toBold, TerminalColor.BOLD);; }. public static String colorString(String toColor, TerminalColor color) {; if(isInteractive()) {; return color.getColorString() + toColor + TerminalColor.RESET.getColorString();; } else {; return toColor;; }; }. public static String stripColorsFromString(String colorString){; String stripped = colorString;; for(TerminalColor color : TerminalColor.values()) {; stripped = stripped.replace(color.getColorString(),"""");; }; return stripped;; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4429#issuecomment-367141169:104,avoid,avoid,104,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4429#issuecomment-367141169,1,['avoid'],['avoid']
Safety,"I just heard that production is moving to GatherVCFs, which is getting some update that might be pertinent. I know that the workflows say that they are using MergeVCFs because of other issues:. ```; # using MergeVcfs instead of GatherVcfs so we can create indices; # WARNING 2015-10-28 15:01:48 GatherVcfs Index creation not currently supported when gathering block compressed VCFs.; ```. ---. This concern is germane to any WGS analyses and perhaps not concerning for WES analyses. So perhaps our current WDL workflows that use SplitIntervals could expressly state that they are not safe for WGS variant calling analyses.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3061#issuecomment-306888339:584,safe,safe,584,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3061#issuecomment-306888339,1,['safe'],['safe']
Safety,I just ran a preliminary test and it appears that the transient attribute field is apparently getting purged between `if (trimmingResult.hasLeftFlankingRegion())` and `if (trimmingResult.hasRightFlankingRegion())` `ReferenceConfidenceModel` calls. That means this should be a safe operation but I would rather be absolutely that this won't cause problems. To that end I would like to explicitly clean the transient attribute fields before every reference confidence call for an added layer of protection.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5908#issuecomment-488329795:276,safe,safe,276,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5908#issuecomment-488329795,1,['safe'],['safe']
Safety,"I like the idea of the modified regexes, that seems like the best balance of usability and flexibility/power. I'd rather avoid having a slew of new special-cased arguments.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/588#issuecomment-309815640:121,avoid,avoid,121,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/588#issuecomment-309815640,1,['avoid'],['avoid']
Safety,"I love the idea of annotating dependencies. +1. On Tue, Feb 24, 2015 at 1:00 PM, Valentin Ruano Rubio <; notifications@github.com> wrote:. > Some annotations may use as input some other annotations. In this case the; > former's code should be run after the all the depends are run.; > ; > For example QD uses AD and if not present defaults into other sources to; > determine the read-depth. It can be the case that QD is applied before AD; > an in the resulting output these two are inconsistent.; > ; > In this case if AD is be annotated it should happen before QD is annotated; > to avoid inconsistencies.; > ; > Here we may consider either to fail if the dependences are not in the list; > of requested annotations or force the application of those given the list; > of requested annotations. Non requested annotations could be subsequently; > stripped from the final output (perhaps this should be explicitly requested; > by the user).; > ; > We could use Java @Annotations https://github.com/Annotations to; > indicate depends between variant annotations (e.g. listing reference to the; > annotion class object).; > ; > —; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/hellbender/issues/225. ## . Geraldine A. Van der Auwera, Ph.D.; Bioinformatics Scientist II; GATK Support & Outreach; Broad Institute",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/225#issuecomment-75859549:585,avoid,avoid,585,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/225#issuecomment-75859549,1,['avoid'],['avoid']
Safety,"I misread what they said, I blame sleep deprivation. They're releasing THIS month. Probably on the 15th, so I think if we can wait until then to incorporate these changes we can avoid any problems. Are other PR's blocked on this one?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2247#issuecomment-259194901:178,avoid,avoid,178,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2247#issuecomment-259194901,1,['avoid'],['avoid']
Safety,"I reached out to our comms team and they fixed the ordering bug in the list of versions so 4.1.6.0 should at least be the first one on the page now. They are aware that there are some issues with the doc being hard to find/search but there's limited developer resources to make changes to the website right now. We're doing what we can to improve things!. So I think arguments that marked as *Deprecated* are listed in the doc, but if an argument is actually *removed* then it's no longer mentioned anywhere. We try to deprecate arguments for a period of time before removing them to give people warning that things might go away, but we don't have a set policy on how long/how many versions. In general we try not to remove stuff very often to avoid this kind of issue, but sometimes people do get caught by it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6547#issuecomment-613426645:745,avoid,avoid,745,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6547#issuecomment-613426645,1,['avoid'],['avoid']
Safety,"I reran this with retry enabled. It says it finished successfully. There was an error in the middle but it continued anyways (this isn't shown in the final output, but there was a second bar).; I'm not sure about the output, though. Where is this command supposed to leave `output.bam`? It's not on my desktop. ```; [Stage 1:=====================================> (375 + 2) / 553]17/03/30 18:18:46 WARN org.apache.hadoop.hdfs.DFSClient: DFSOutputStream ResponseProcessor exception for block BP-369249695-10.240.0.8-1490738675068:blk_1073745922_5098; java.io.EOFException: Premature EOF: no length prefix available; 	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2282); 	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:244); 	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:733); 17/03/30 18:18:46 WARN org.apache.hadoop.hdfs.DFSClient: Error Recovery for block BP-369249695-10.240.0.8-1490738675068:blk_1073745922_5098 in pipeline DatanodeInfoWithStorage[10.240.0.4:50010,DS-5596b1b5-b89c-4c39-bbd8-7423614eae0e,DISK], DatanodeInfoWithStorage[10.240.0.3:50010,DS-a0c20806-0af3-4679-b8cd-9cae6ca25071,DISK]: bad datanode DatanodeInfoWithStorage[10.240.0.4:50010,DS-5596b1b5-b89c-4c39-bbd8-7423614eae0e,DISK]; 17/03/30 20:00:21 INFO org.spark_project.jetty.server.ServerConnector: Stopped ServerConnector@61cda923{HTTP/1.1}{0.0.0.0:4040}; 20:00:21.366 INFO MarkDuplicatesSpark - Shutting down engine; [March 30, 2017 8:00:21 PM UTC] org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark done. Elapsed time: 165.11 minutes.; Runtime.totalMemory()=1222115328; Job [ac3f4131-f19f-47db-8cc3-82b243ad4b72] finished successfully.; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2517#issuecomment-290541369:959,Recover,Recovery,959,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2517#issuecomment-290541369,1,['Recover'],['Recovery']
Safety,"I second Joel, I had to produce base level resolution gVCF to avoid it which is an ""ugly"" workaround",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4#issuecomment-66466849:62,avoid,avoid,62,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4#issuecomment-66466849,1,['avoid'],['avoid']
Safety,"I see `Timeout (30 minutes) reached. Terminating ""./gradlew jacocoTestReport""`. It's not clear to me how my changes could have introduced a deadlock or similar problem. . I ran the full test suite (`./gradlew test`) locally to take a look and it passes. Took 20min. Running `SeekableByteChannelPrefetcherTest` by itself also passes, unsurprisingly.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2391#issuecomment-277399501:7,Timeout,Timeout,7,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2391#issuecomment-277399501,1,['Timeout'],['Timeout']
Safety,"I see, then it seems safe to use the protobuf java format. We intend to release the 0.6.0 version asap, preferably today.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2634#issuecomment-298024935:21,safe,safe,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2634#issuecomment-298024935,1,['safe'],['safe']
Safety,"I see. Yes, I think this is closer to the issue at hand -- for instance, we'll have variants that are called, then when we go back to the original alignments there will be no indication of alternate observations at the loci which the variants were called (and vice versa, we'll have original alignments that suggest a variant, but won't get called). In the worst cases scenarios, the haplotype alignments seem ridiculous compared to the original reads (e.g. my GATK forum post: https://gatk.broadinstitute.org/hc/en-us/community/posts/360075749392-Bamout-haplotypes-are-much-worse-than-bamout-tumor-reads-would-suggest). So, right, it's not the realignment of the reads, it's the alignment of the assembled haplotypes that complicates things. Thinking about it in this context though, I see we might be at an impasse (or back at square one with considering the assembly process). . My true fear is that we'll lose what appear to be definite-somatic variants in the original alignments because of the internal assembly + haplotype alignment engine. But I will say, it does seem like we see these problems for lower-coverage areas/variants, so I might be exaggerating the issue at hand. Maybe the solution for testing our expectations is to use higher depths to avoid bad assemblies and to apply some post-processing to the calls. (And wait and see if there are truly troubling cases of ""obvious"" variants getting lost)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7064#issuecomment-773628190:1260,avoid,avoid,1260,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7064#issuecomment-773628190,1,['avoid'],['avoid']
Safety,"I set TEST_TYPE to ""all"" and was able to run this test without failure. The command I used is:; ```; ./gradlew test --tests org.broadinstitute.hellbender.utils.nio.GcsNioIntegrationTest.openPublicFile; ```; I ran it 10 times and got the same result every time:; `BUILD SUCCESSFUL`. It looks like this was a transient problem: either the internet connection was stalled or the authentication server was down temporarily. As this happens at the very beginning of the execution, it's probably not a very big deal: the user can just retry. Incidentally, PR #2506 that is under review lengthens the connection timeouts, if I am not mistaken. This will probably make the problem less likely to reoccur.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2514#issuecomment-288852260:605,timeout,timeouts,605,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2514#issuecomment-288852260,1,['timeout'],['timeouts']
Safety,"I still got the same error with version 4.1.9.0. . > Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/orange/reed/nhouse/Raw_seqs/SEQ9_samples/tmp; 11:30:50.248 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 11:30:50.478 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/apps/gatk/4.1.9.0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Oct 26, 2020 11:30:50 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 11:30:50.791 INFO CombineGVCFs - ------------------------------------------------------------; 11:30:50.791 INFO CombineGVCFs - The Genome Analysis Toolkit (GATK) v4.1.9.0; 11:30:50.792 INFO CombineGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:30:50.792 INFO CombineGVCFs - Executing as nwijewardena@c3a-s8.ufhpc on Linux v3.10.0-1062.18.1.el7.x86_64 amd64; 11:30:50.792 INFO CombineGVCFs - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_31-b13; 11:30:50.792 INFO CombineGVCFs - Start Date/Time: October 26, 2020 11:30:50 AM EDT; 11:30:50.793 INFO CombineGVCFs - ------------------------------------------------------------; 11:30:50.793 INFO CombineGVCFs - ------------------------------------------------------------; 11:30:50.794 INFO CombineGVCFs - HTSJDK Version: 2.23.0; 11:30:50.794 INFO CombineGVCFs - Picard Version: 2.23.3; 11:30:50.794 INFO CombineGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 11:30:50.794 INFO CombineGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 11:30:50.794 INFO CombineGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 11:30:50.794 INFO CombineGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 11:30:50.795 INFO CombineGVCFs - Deflater: IntelDeflater; 11:30:50.795 INFO CombineGVCFs - Inflater: IntelInflater; 11:30:50",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6766#issuecomment-716640444:193,Redund,Redundant,193,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6766#issuecomment-716640444,2,"['Redund', 'detect']","['Redundant', 'detect']"
Safety,I suspect that many of the recently reactivated tests aren't so much fixed as they are temporarily avoided. I think the ones that were disabled due to problems with unmapped reads will still have problems if we read from gcs since that still removes unmapped reads I think.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1042#issuecomment-150337876:99,avoid,avoided,99,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1042#issuecomment-150337876,1,['avoid'],['avoided']
Safety,I talked to travis support and I believe this is now fixed:. From travis support:; > I've happily increased the timeout to 70 minutes on your broadinstitute/gatk repository. Please let me know if you need to increase it further.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2808#issuecomment-306233923:112,timeout,timeout,112,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2808#issuecomment-306233923,1,['timeout'],['timeout']
Safety,"I think for SQ you could limit the lines you write out to the contigs that are covered in your intervals list, ignore the rest. So you can access that info as soon as you've parsed the command line. Or if you're running without an intervals list you could supply a seq dict file through a separate arg. But the former seems safer. . For other modes: EXTREME would hardcode what we consider required. Then you could potentially do ARBITRARY to allow passing strings for specific attributes that you want to drop. . None of these would have to depend on what's in the calls, I think.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2233#issuecomment-266059687:324,safe,safer,324,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2233#issuecomment-266059687,1,['safe'],['safer']
Safety,"I think it triggers in certain situations where a firewall is blocking the connection. If the internet is simply unreachable it doesn't happen, so I don't know what the exact error case is. It happened consistently for people inside Intel's firewall or vpn. . An option to disable gcs support isn't a bad idea, it's kind of a hack though, it would be better if we could understand and avoid triggering the problem. If we could only initialize GCS support when we are sure that we actually are accessing files from google that could be a useful, but it doesn't seem like there's any single point we can plug into to detect that, it would have to be spread over everything that uses paths.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3491#issuecomment-427432141:385,avoid,avoid,385,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3491#issuecomment-427432141,2,"['avoid', 'detect']","['avoid', 'detect']"
Safety,"I think it would be OK to make an `ImpreciseVariantDetector` that is only responsible for returning a list of imprecise variants. But I'd rather not hide the fact that it's annotating the breakpoint-detected variants inside that class. Why not break up these two things and push the annotation close to where the breakpoint-detected variants are created (ie. in `discoverSimpleVariants` or something), and then call the imprecise variant detector after that?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3934#issuecomment-357354110:199,detect,detected,199,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3934#issuecomment-357354110,3,['detect'],"['detected', 'detector']"
Safety,"I think our plan is to NOT share test files except for some very basic ones like a reference sequence. Better to just duplicate them. That way if a test needs to change it, it can change it without affecting anyone else. Git should be able to detect duplicates and only store 1 copy, so it shouldn't affect our file size much.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/445#issuecomment-96074370:243,detect,detect,243,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/445#issuecomment-96074370,1,['detect'],['detect']
Safety,"I think that’s a great idea, @cmnbroad. I think this avoids most if not all of the pitfalls I mentioned above. Moreover, if each tool also has an argset containing default values for all optional parameters, it probably also solves the problem of having to sync defaults in the WDL.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385682838:53,avoid,avoids,53,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385682838,1,['avoid'],['avoids']
Safety,"I think the `Optimized` version deals with avoiding replicating reads in a more nuanced manner, but if I understand correctly, it doesn't seem to me to avoid the shuffle. It looks like it essentially ignores the concept of data locality entirely, and potentially transfers a lot of data over the network. (Equivalent to performing a shuffle on a sorted file in HDFS.). IMO, the current ""shuffle"" implementation is already a ""Spark-y"" way to do it, but with multiple inefficiencies:; - Reads/variants are keyed to their corresponding shards, replicating reads if they cross over shard boundaries. This necessitates an `aggregateByKey` operation that potentially reshuffles the entire data set at the end to deal with neighboring shards that could be hashed to different machines.; - The impl uses `cogroup` and `groupByKey`, which require materializing all values for a key in memory (which could be large). Best to avoid these if possible.; - And related to the previous issue, the join strategy for reads and variants is basically a cross-product-and-filter, which is not very efficient, especially considering that the data can be ordered. I think the best implementation here would steal JP's method of sharding the reads/variants, but make use of `repartitionAndSortWithinPartition`, which lets you specify what partition to use and also sorts all the values within a given partition. This means that we could employ a sort-merge on each partition, and only scan through the datasets once after shuffling them. Do you already have an impl for doing a sort-merge of `Locatable`s? These can be a bit tricky. I wrote one for the `ShuffleRegionJoin` impl in ADAM, but there are semantic differences that would make it less efficient to use. (Specifically, it would require the `aggregateByKey` operation and also creating `SimpleInterval`-style objects from a separate model.). Finally, I would also add the ability to specify which join strategy to use separately for the reference bases and the vari",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1007#issuecomment-151721602:43,avoid,avoiding,43,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1007#issuecomment-151721602,3,['avoid'],"['avoid', 'avoiding']"
Safety,"I think there are two points to this issue: 1. validation or no-validation and 2. need to pass GenomeLocParser around... . Is 1. about performance? otherwise we prefer to have validated locations, right? and If it is about performance I think it should be shown that it really makes a difference to remove those checks. About, 2., can be solved by being able to recover the sequence-dictonary/reference object (called Reference from this point on) from a genome loc and then you can ask it for a new genome-loc if the appropriate methods are added instead of depending on that annoying middle man called GenomeLocParser. I would say that is unlikely to be in the situation where you want to create a GenomeLoc out of the blur without having already a reference to another GenomeLoc or Reference object available. It is an issue if GenomeLoc holds on to a reference to the Reference object? (or the instantiating class is a inner class of the Reference?)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/100#issuecomment-69802695:362,recover,recover,362,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/100#issuecomment-69802695,1,['recover'],['recover']
Safety,"I think we should do this in 2 PRs. First @asmirnov239 can first add his modification to run multiple samples in case mode. I can do a subsequent PR for ""double-chunking"" intervals to avoid PAPI/quota errors for high-resolution WGS.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5054#issuecomment-408629583:184,avoid,avoid,184,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5054#issuecomment-408629583,1,['avoid'],['avoid']
Safety,"I third Joel. On Wed, Dec 10, 2014 at 10:20 AM, amilev notifications@github.com wrote:. > I second Joel, I had to produce base level resolution gVCF to avoid it; > which is an ""ugly"" workaround; > ; > —; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/hellbender/issues/4#issuecomment-66466849; > .",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4#issuecomment-66467086:152,avoid,avoid,152,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4#issuecomment-66467086,1,['avoid'],['avoid']
Safety,I tried the latest GATK release and also reported errors.; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx50G -Djava.io.tmpdir=./tmp -jar /public/home/gaoshibin/software/GATK/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar GenotypeGVCFs -R /public/home/gaoshibin/B73_REF/Zea_mays.AGPv4.dna.toplevel.fa -V gendb://./CHR7_gvcf_database -G StandardAnnotation --genomicsdb-shared-posixfs-optimizations true -O new_ALL_MATERIALS_chr7.g.vcf.gz; 17:49:50.404 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 17:49:50.653 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/public/home/gaoshibin/software/GATK/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 17:49:51.271 INFO GenotypeGVCFs - ------------------------------------------------------------; 17:49:51.273 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.6.1; 17:49:51.273 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:49:51.273 INFO GenotypeGVCFs - Executing as gaoshibin@comput6 on Linux v3.10.0-693.el7.x86_64 amd64; 17:49:51.274 INFO GenotypeGVCFs - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_211-b12; 17:49:51.274 INFO GenotypeGVCFs - Start Date/Time: 2022年5月22日 下午05时49分50秒; 17:49:51.274 INFO GenotypeGVCFs - ------------------------------------------------------------; 17:49:51.275 INFO GenotypeGVCFs - ------------------------------------------------------------; 17:49:51.276 INFO GenotypeGVCFs - HTSJDK Version: 2.24.1; 17:49:51.276 INFO GenotypeGVCFs - Picard Version: 2.27.1; 17:49:51.276 INFO GenotypeGVCFs - Built for Spark Version: 2.4.5; 17:49:51.277 INFO GenotypeGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 17:49:51.277 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_S,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7866#issuecomment-1135301848:613,Redund,Redundant,613,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7866#issuecomment-1135301848,1,['Redund'],['Redundant']
Safety,"I tried this again carefully (12 runs) and only one failed, due to a ""remote server unavailable"" error. This is the same we see before this PR (we need to retry more aggressively) so I think we can merge safely.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2391#issuecomment-281430631:204,safe,safely,204,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2391#issuecomment-281430631,1,['safe'],['safely']
Safety,"I vote no -- the danger (and need to worry about the danger!) is not worth the relatively small gain. `HaplotypeCaller` might very possibly create cyclical object references, so I don't think this is merely hypothetical. I recommend we stick to safe optimizations.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1734#issuecomment-212134553:245,safe,safe,245,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1734#issuecomment-212134553,1,['safe'],['safe']
Safety,"I was looking into this because it is useful for me, and I have found that there is going to be redundancy between the `VariantAnnotatorEngine`code and the plugin. Here a couple of suggestions after trying to implement something in this regard time ago:. * Remove/deprecate the private class `AnnotationManager` in favor of the plugin. The current code is performing reflection operations by itself, and this can cause some problems.; * Refactor the `VariantAnnotatorEngine` constructors in favor of a constructor from the barclay plugin and a list of annotations to apply, to avoid the `AnnotationManager` implementation.; * Remove/deprecate static methods for creating an annotator engine (`ofAllMinusExcluded` and `ofSelectedMinusExcluded`) in favor of handling this in the plugin.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3287#issuecomment-316077922:96,redund,redundancy,96,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3287#issuecomment-316077922,2,"['avoid', 'redund']","['avoid', 'redundancy']"
Safety,I was push an old branch and needed to pull from master and rebase. So I closed this one to avoid confusion @droazen,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5761#issuecomment-469880173:92,avoid,avoid,92,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5761#issuecomment-469880173,1,['avoid'],['avoid']
Safety,"I was thinking more carefully on this and another option is create methods in `ReadPileup` to fix the overlaps after construction and/or getBaseCounts without overlaps. This won't break the behaviour of LIBS and it is up to the user to change overlaps. But for performance issues, I would like to have a variable in `ReadPileup` for track if the overlaps are corrected/fixed, to avoid recomputation. I can implement this in a different PR or in this one if the basic idea behind this one is not accepted.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2041#issuecomment-235993555:379,avoid,avoid,379,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2041#issuecomment-235993555,1,['avoid'],['avoid']
Safety,"I wasn't sure if they should be removed from the input because they were entirely invalid for all purposes, or just needed to be filtered out for this particular tool. It may make no sense though. It's fine if DataflowReadsPipeline doesn't make sense for this. A library of useful functions could definitely be the right way to go. We just want avoid having every tool need to write the input / output code itself.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/556#issuecomment-111212845:345,avoid,avoid,345,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/556#issuecomment-111212845,1,['avoid'],['avoid']
Safety,"I will start working on this off of the work that currently resides in #6034. The proposal will be to perform KBestHaplotype finding for multiple source/sink vertexes and then perform smith waterman on the resulting ""dangling"" haplotypes that are created in order to recover the probable dangling sequence. Hopefully the number of haplotypes will have been brought down by enough that this operation will be tolerable in terms of cost.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5957#issuecomment-511024376:267,recover,recover,267,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5957#issuecomment-511024376,1,['recover'],['recover']
Safety,"I would suggest adding the --use-jdk-deflater & --use-jdk-inflater options in all the steps to avoid this kind of error, which seems to be random.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7582#issuecomment-991514366:95,avoid,avoid,95,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7582#issuecomment-991514366,1,['avoid'],['avoid']
Safety,"I'd also be hesitant to break the previous expectation that IntervalArgumentCollection contains a non-empty list of intervals. If I understand correctly (and apologies if not, I'm glancing at the repo between paternity-leave duties and am quite sleep deprived!), all calling code would have to add an explicit check that the new option isn't enabled or risk failing ungracefully downstream. For CNV code, this might be as simple as changing the validation method `CopyNumberArgumentValidationUtils.validateIntervalArgumentCollection`, but I wouldn't generally expect it to be so straightforward to add such checks throughout the codebase. I also agree with @lbergelson that the expected behavior might not be immediately clear and that perhaps this could be addressed in the scattering step---seems like shards could just be limited to regions that cover the resource at the outset. Consider also an older comment at https://github.com/broadinstitute/gatk/pull/5392#issuecomment-435588845 about whether or not we should just use the equivalent Picard tool (horrible glob aside).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6209#issuecomment-540740687:353,risk,risk,353,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6209#issuecomment-540740687,1,['risk'],['risk']
Safety,"I'm a big fan of the htsjdk precision code: https://github.com/samtools/htsjdk/blob/master/src/main/java/htsjdk/variant/vcf/VCFEncoder.java . That being said, fixed point is fine. We don't really need the precision of scientific for the standard GATK annotations. I would like the decimal point to always be present (somewhere I made an assumption about that in order to avoid a bunch of `instanceof` calls) and total number of digits being constant is good.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4047#issuecomment-356085053:371,avoid,avoid,371,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4047#issuecomment-356085053,1,['avoid'],['avoid']
Safety,"I'm getting a similar error. any solutions?; ```; 17:14:13.170 INFO FuncotateSegments - The following datasources support funcotation on segments:; 17:14:13.171 INFO FuncotateSegments - Gencode 34 CANONICAL; 17:14:13.209 INFO FuncotatorEngine - VCF sequence dictionary detected as B37 in HG19 annotation mode. Performing conversion.; 17:14:13.209 WARN FuncotatorEngine - WARNING: You are using B37 as a reference. Funcotator will convert your variants to GRCh37, and this will be fine in the vast majority of cases. There MAY be some errors (e.g. in the Y chromosome, but possibly in other places as well) due to changes between the two references.; 17:14:13.411 INFO ProgressMeter - Starting traversal; 17:14:13.412 INFO ProgressMeter - Current Locus Elapsed Minutes Features Processed Features/Minute; 17:14:15.391 INFO FuncotateSegments - Shutting down engine; [September 11, 2022 5:14:15 PM GMT] org.broadinstitute.hellbender.tools.funcotator.FuncotateSegments done. Elapsed time: 0.30 minutes.; Runtime.totalMemory()=1752170496; java.lang.IllegalArgumentException: Invalid interval. Contig:chr1 start:917445 end:911649; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:804); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:59); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:35); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.segment.SegmentExonUtils.findInclusiveExonIndex(SegmentExonUtils.java:95); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.segment.SegmentExonUtils.determineSegmentExonPosition(SegmentExonUtils.java:63); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createSegmentFuncotations(GencodeFuncotationFactory.java:2939); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createSegmentFuncotations(GencodeFuncotationFactory.java:2914); at ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6598#issuecomment-1243013314:269,detect,detected,269,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6598#issuecomment-1243013314,1,['detect'],['detected']
Safety,"I'm having a similar issue on gatk 4.2.6.1; ```; 17:14:13.170 INFO FuncotateSegments - The following datasources support funcotation on segments:; 17:14:13.171 INFO FuncotateSegments - Gencode 34 CANONICAL; 17:14:13.209 INFO FuncotatorEngine - VCF sequence dictionary detected as B37 in HG19 annotation mode. Performing conversion.; 17:14:13.209 WARN FuncotatorEngine - WARNING: You are using B37 as a reference. Funcotator will convert your variants to GRCh37, and this will be fine in the vast majority of cases. There MAY be some errors (e.g. in the Y chromosome, but possibly in other places as well) due to changes between the two references.; 17:14:13.411 INFO ProgressMeter - Starting traversal; 17:14:13.412 INFO ProgressMeter - Current Locus Elapsed Minutes Features Processed Features/Minute; 17:14:15.391 INFO FuncotateSegments - Shutting down engine; [September 11, 2022 5:14:15 PM GMT] org.broadinstitute.hellbender.tools.funcotator.FuncotateSegments done. Elapsed time: 0.30 minutes.; Runtime.totalMemory()=1752170496; java.lang.IllegalArgumentException: Invalid interval. Contig:chr1 start:917445 end:911649; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:804); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:59); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:35); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.segment.SegmentExonUtils.findInclusiveExonIndex(SegmentExonUtils.java:95); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.segment.SegmentExonUtils.determineSegmentExonPosition(SegmentExonUtils.java:63); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createSegmentFuncotations(GencodeFuncotationFactory.java:2939); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createSegmentFuncotations(GencodeFuncotationFactory.java:2914); at o",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7676#issuecomment-1252518062:268,detect,detected,268,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7676#issuecomment-1252518062,1,['detect'],['detected']
Safety,"I'm honestly not sure what's going on here. The picard/gatk differences will be resolved soon and then the sed command can go away. I don't think it's worth doing anything until then. We should be able to write vcf.gz so if there's an issue there we want to know about it. There could either be an issue in the TabixIndex writer (which we know for sure has issues, one is a blocker on the 11k project at the moment. ) Alternatively the input file could just be invalid, in which case our vcf writer should be detecting the error and is not.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3030#issuecomment-306632877:509,detect,detecting,509,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3030#issuecomment-306632877,1,['detect'],['detecting']
Safety,"I'm making improvements to the overlap detector api, suggestions welcome. I; should have a pr today. On Thursday, April 14, 2016, Tom White notifications@github.com wrote:. > Got it. The index is used to filter the input splits (by removing any that; > don't match, or trimming those that do match down to the minimum extent of; > the match). See; > https://github.com/HadoopGenomics/Hadoop-BAM/blob/master/src/main/java/org/seqdoop/hadoop_bam/BAMInputFormat.java#L288-L355; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/gatk/issues/1532#issuecomment-209894805. ## . Sent from Gmail Mobile",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1532#issuecomment-209906190:39,detect,detector,39,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1532#issuecomment-209906190,1,['detect'],['detector']
Safety,"I'm opposed to including 2 entire references since it will raise our git lfs files to somewhere around 5gb. This is a significant drag on downloading / building / testing gatk and should be avoided if possible. I understand that I may be overruled here, but keeping the test files to a reasonable size was and should remain an important goal of gatk4. . It looks like there may be some options to slim down the existing test files that we should take advantage of if possible. There are a number of large vcfs and fasta files which are NOT currently compressed in our large files. We should compress them.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5111#issuecomment-423617603:190,avoid,avoided,190,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5111#issuecomment-423617603,1,['avoid'],['avoided']
Safety,"I'm surprised none of the variant calling integration tests change. @cmnbroad Would you expect this to change the behavior in any common use cases or is this more of a safety check?. It's admittedly also possible that the MT calling in the Mutect2 integration test does change slightly, but that's a very lenient concordance check.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5594#issuecomment-456900212:168,safe,safety,168,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5594#issuecomment-456900212,1,['safe'],['safety']
Safety,"I've added a comment there -- basically I need more information on the advantages of this approach over the one in https://github.com/samtools/htsjdk/pull/211 (which could in theory be merged now rather than in October). Is the main advantage avoiding adding the hadoop dependency in htsjdk, or are there other benefits as well? Do you think it would be cleaner/less invasive, and generalize well to the rest of htsjdk (beyond the reference classes)?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/831#issuecomment-133046952:243,avoid,avoiding,243,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/831#issuecomment-133046952,1,['avoid'],['avoiding']
Safety,I've checked the README file (see below). I think it's the latest one because I cannot see the newer version in the ftp site. Funcotator was able to produce some output. So I tried Funcotator with the same VCF input but with only first 5 loci. The program terminated safely. I hypothesized that the bug happened with only some records in the VCF file which was produced from Mutect2. Please help. +---------------------------------------------+; | Data Source Version Information |; +---------------------------------------------+. Version: 1.2.20180329; Source: ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/funcotator/funcotator_dataSources.v1.2.20180; 329.tar.gz; Alternate Source: gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.2.20180329.tar.gz,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-386193333:267,safe,safely,267,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-386193333,1,['safe'],['safely']
Safety,"I've implemented the Gaussian-kernel binary-segmentation algorithm from this paper: https://hal.inria.fr/hal-01413230/document This method uses a low-rank approximation to the kernel to obtain an approximate segmentation in linear complexity in time and space. In practice, performance is actually quite impressive!. The implementation is relatively straightforward, clocking in at ~100 lines of python. Time complexity is O(log(maximum number of segments) * number of data points) and space complexity is O(number of data points * dimension of the kernel approximation), which makes use for WGS feasible. Segmentation of 10^6 simulated points with 100 segments takes about a minute and tends to recover segments accurately. Compare this with CBS, where segmentation of a WGS sample with ~700k points takes ~10 minutes---and note that these ~700k points are split up amongst ~20 chromosomes to start!. There are a small number of parameters that can affect the segmentation, but we can probably find good defaults in practice. What's also nice is that this method can find changepoints in moments of the distribution other than the mean, which means that it can straightforwardly be used for alternate-allele fraction segmentation. For example, all segments were recovered in the following simulated multimodal data, even though all of the segments have zero mean:. ![baf](https://user-images.githubusercontent.com/11076296/29100464-ad687946-7c79-11e7-99e4-962ab93709b4.png). Replacing the SNP segmentation in ACNV (which performs expensive maximum-likelihood estimation of the allele-fraction model) with this method would give a significant speedup there. Joint segmentation is straightforward and is simply given by addition of the kernels. However, complete data is still required. Given such a fast heuristic, I'm more amenable to augmenting this method with additional heuristics to clean up or improve the segmentation if necessary. We can also use it to initialize our more sophisticated HMM m",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-321121666:696,recover,recover,696,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-321121666,1,['recover'],['recover']
Safety,"If consolidate ran without issues, that array/contig folder didn't have any fragments that were incomplete. And yes, the duplicates were consolidated down so everything should be fine. . I should add, my last bullet about sanity checks regarding number of fragments does not apply to consolidated arrays. Those will, of course, only have a single fragment.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6910#issuecomment-722733110:222,sanity check,sanity checks,222,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6910#issuecomment-722733110,1,['sanity check'],['sanity checks']
Safety,"If it helps, I have seen this error when using local drives exclusively (not attached to a shared file system). . Twice it has manifested as a core dump that points to ` C [libc.so.6+0xaf4f9] malloc+0x169`: . ```; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x000014cfb1d504f9, pid=1182729, tid=1195264; #; # JRE version: OpenJDK Runtime Environment (17.0.3) (build 17.0.3-internal+0-adhoc..src); # Java VM: OpenJDK 64-Bit Server VM (17.0.3-internal+0-adhoc..src, mixed mode, sharing, tiered, compressed oops, compressed class ptrs, g1 gc, linux-amd64); # Problematic frame:; # C [libc.so.6+0xaf4f9] malloc+0x169; #; # Core dump will be written. Default location: Core dumps may be processed with ""/usr/lib/systemd/systemd-coredump %P %u %g %s %t %c %h"" (or dumping to /gpfs/gpfs_de6000/home/dalegre/projects/1000-Genomes/jointcalling-test/goast_workflows/JointCalling/test_samples-1000.1.3/core.1182729); #; # If you would like to submit a bug report, please visit:; # https://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; [dalegre@login4601 fdone]$ head -n 20 hs_err_pid1182729.log; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x000014cfb1d504f9, pid=1182729, tid=1195264; #; # JRE version: OpenJDK Runtime Environment (17.0.3) (build 17.0.3-internal+0-adhoc..src); # Java VM: OpenJDK 64-Bit Server VM (17.0.3-internal+0-adhoc..src, mixed mode, sharing, tiered, compressed oops, compressed class ptrs, g1 gc, linux-amd64); # Problematic frame:; # C [libc.so.6+0xaf4f9] malloc+0x169; #; # Core dump will be written. Default location: Core dumps may be processed with ""/usr/lib/systemd/systemd-coredump %P %u %g %s %t %c %h"" (or dumping to /gpfs/gpfs_de6000/home/dalegre/projects/1000-Genomes/jointcalling-test/goast_workflows/JointCalling/test_samples-1000.1.3/core.1182729); #; # If you would like to submit a bug report, please v",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8683#issuecomment-1936285520:242,detect,detected,242,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8683#issuecomment-1936285520,1,['detect'],['detected']
Safety,If it's easier we should just detect when this happens and add a warning annotation to these records.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3749#issuecomment-376249723:30,detect,detect,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3749#issuecomment-376249723,1,['detect'],['detect']
Safety,"In GATK4, the way to make a tool multithreaded is to implement it as a Spark tool. All Spark tools can be trivially parallelized across multiple threads using the local runner, and across a cluster using spark-submit or gcloud. . We wanted to avoid the complexities of implementing our own map/reduce framework, as was done in previous versions of the GATK, and instead rely on a standard, third-party framework to keep the GATK4 engine as simple as possible.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2345#issuecomment-273206164:243,avoid,avoid,243,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2345#issuecomment-273206164,1,['avoid'],['avoid']
Safety,"In a similar vein, would it be feasible to allow sample-matched RNA-seq data to be specified as input, so that the annotation is based on the actual isoform(s) that is (are) transcribed in a particular sample? The same SNV may be annotated in two different ways in two different samples, if the isoform(s) inferred by RNA-seq data differ (e.g. exonic for Patient A, intronic for Patient B). It avoids subjective prioritisation lists like the ones above and is instead data-driven and contextual.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7631#issuecomment-1032216007:394,avoid,avoids,394,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7631#issuecomment-1032216007,1,['avoid'],['avoids']
Safety,"In general our germline tools are designed for short variants. I don't think any of them will handle a millions long indel well or at all. The SV or CNV tools sound like a better fit although I'm not sure exactly if they cover your use case exactly. Typically we process short variants and long variants like this separately. . We should be detecting this variant up front on when loading into genomicsDB if it's going to be problematic to retrieve it, and we should be giving a better error message. I don't think we'll be able to handle it through GenotypeGVCFs in any helpful way though. (The best I can imagine it doing is passing it through ungenotyped.)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7976#issuecomment-1376029770:341,detect,detecting,341,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7976#issuecomment-1376029770,1,['detect'],['detecting']
Safety,"In resolving a user issue I have recently witnessed somewhat of a novelty, specifically I witnessed the case where a user had a variant Gained as a result of dangling end recovery. Below is the graph that triggered this is posted. What happened is that a fork in the main (900+ support) path caused the dangling end recovery to fail, yet dangling end recovery succeeded for the rightmost path. Thus all of those reads ended up being aligned to the erroneous T+G variant haplotype. We need to better handle dangling ends, fortunately that is on the docket for the recent Assembly engine work #5957.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/266#issuecomment-518720783:171,recover,recovery,171,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/266#issuecomment-518720783,3,['recover'],['recovery']
Safety,"In the HaplotypeCaller test, about 80% of the Smith-Waterman calls result in a substring match to the reference without any indels. I'm working on adding a substring search to SWPairwiseAlignment to avoid running the full Smith-Waterman for these cases.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1629#issuecomment-202540188:199,avoid,avoid,199,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1629#issuecomment-202540188,1,['avoid'],['avoid']
Safety,"In the WGS SV pipeline, for deletions and duplications that the pipeline believes to be biallelic we do the following:. - ALT: `<DEL>` or `<DUP>`; - SVTYPE: `DEL` or `DUP`; - GT: `0/0` or `0/1` or `1/1`. We currently report depth based copy number and quality for these variants in custom format fields `RD_CN` and `RD_GQ` if they are available; we could possibly move those values to the standard `CN` and `CNQ`, but there is some complexity in how to handle events detected by paired end and split reads without good read depth support; ie those under 1kb or so depending on our depth binning size and the coverage. Our depth genotyping module makes estimates of copy number for these sites but sometimes these can be very inaccurate so at the moment we prefer not to report total copy number in those fields. Probably what we _should_ do is fill in CN with 0, 1, or 2 based on the genotype we emitted and set CNQ to the value we computed for GQ. For multiallelic CNVs (i.e. sites where our model is not sure that the variant is bi-allelic) we write:. - ALT: `<CNV>`; - SVTYPE: `CNV`; - GT and GQ: `.`; - CN and CNQ: estimate of total (diploid/unphased) copy number and quality of the depth evidence. I think there are some tradeoffs in completely characterizing the evidence for and quality of each call and enabling easy searching across the whole VCF without having to parse and understand the entire record. Older versions of our pipeline used to put the diploid copy number of the event into the GT field, I think similarly to what's being described above. This is incorrect VCF -- GT values should be indices into the allele list for the variant, and should be a list of length equal to the ploidy. . My view is that if you can confidently infer the alleles present at the site in the sample set you should use a GT value of the form `0/1`, and if you don't know or aren't interested in trying to infer them you should use CN for total copy number and CNQ for the quality. CNF is also availabl",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6167#issuecomment-622053171:467,detect,detected,467,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6167#issuecomment-622053171,1,['detect'],['detected']
Safety,"In this context, for a given mutation, there might be a hundred or so reads; and each cell is only contributing one to three reads. For other; mutations, maybe there's less than 10 reads corresponding to less than 10; cells, and it can vary pretty dramatically. The total number of cells; represented in a single sample can be thousands to tens of thousands,; usually - but could be many more as the tech advances. My hack for it at the moment is to encode both the cell barcode and the UMI; information into the read name. Then, for each variant, I query the reads; that overlap that variant in the bam file and analyze each read for; supporting the variant or the REF allele - then I can count the reads; according to the specific cells and also deal with any UMI redundancy per; cell. This works pretty well except for the cases where the HC reassembly; provides evidence for the variant and I can't track it to the originally; aligned reads. Also, mostly I think the difficulty here relates to indels; around homopolymers with our pacbio long isoform reads in our rna-seq; variant pipeline that leverages the gatk rna-seq protocol with HC. On Thu, Feb 29, 2024 at 8:58 AM Gökalp Çelik ***@***.***>; wrote:. > Since each cell has a barcode wouldn't it be nice to use them as their; > Read Group ID and Sample Name within the BAM so that variant callers will; > distinguish each cell from their Sample Name and produce a multisample VCF; > for that variant site. Once IDs and Sample Names are split per cell you may; > be able to color them differently in IGV to even visually observe those; > events.; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/8703#issuecomment-1971203108>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ABZRKX6LYHUXDUMGDU3AIFLYV4ZZLAVCNFSM6AAAAABD4OZKJ6VHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMYTSNZRGIYDGMJQHA>; > .; > You are receiving this because you were mentioned.Message ID:; > ***@",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8703#issuecomment-1971223953:766,redund,redundancy,766,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8703#issuecomment-1971223953,1,['redund'],['redundancy']
Safety,"Is the VCF in your screenshot a germline resource file, such as gnomAD? Could you also post the output of `CreateSomaticPanelOfNormals` at these same sites? What samples went into the `ls *.vcf.gz` input in `GenomicDBImport`? How many samples did you use? How did you run `Mutect2` to generate these?. Also, just to be safe, a command like `ls *.vcf.gz` could easily introduce some stray VCFs that happen to be in the same directory. Are you positive that these include only the output of `Mutect2` run on normal samples?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7215#issuecomment-833498893:319,safe,safe,319,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7215#issuecomment-833498893,1,['safe'],['safe']
Safety,Is there an easy way to tell what version the index was created with? . It seems like detecting a bad index should happen at the jbwa level instead of GATK itself?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2123#issuecomment-242818261:86,detect,detecting,86,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2123#issuecomment-242818261,1,['detect'],['detecting']
Safety,"Is this issue still open? I'm getting a similar error like this:; ```; .; .; .; INFO: Failed to detect whether we are running on Google Compute Engine.; 19:14:42.635 INFO BaseRecalibrator - ------------------------------------------------------------; 19:14:42.635 INFO BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.1.8.1; 19:14:42.635 INFO BaseRecalibrator - For support and documentation go to https://software.broadinstitute.org/gatk/; 19:14:42.638 INFO BaseRecalibrator - Executing as XXX on Linux v3.10.0-957.12.2.el7.x86_64 amd64; 19:14:42.638 INFO BaseRecalibrator - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_212-b04; 19:14:42.638 INFO BaseRecalibrator - Start Date/Time: September 12, 2020 7:14:42 PM PDT ; 19:14:42.638 INFO BaseRecalibrator - ------------------------------------------------------------; 19:14:42.638 INFO BaseRecalibrator - ------------------------------------------------------------; 19:14:42.638 INFO BaseRecalibrator - HTSJDK Version: 2.23.0; 19:14:42.638 INFO BaseRecalibrator - Picard Version: 2.22.8; 19:14:42.638 INFO BaseRecalibrator - HTSJDK Defaults.COMPRESSION_LEVEL : 2 ; 19:14:42.638 INFO BaseRecalibrator - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 19:14:42.639 INFO BaseRecalibrator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 19:14:42.639 INFO BaseRecalibrator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 19:14:42.639 INFO BaseRecalibrator - Deflater: IntelDeflater; 19:14:42.639 INFO BaseRecalibrator - Inflater: IntelInflater; 19:14:42.639 INFO BaseRecalibrator - GCS max retries/reopens: 20; 19:14:42.639 INFO BaseRecalibrator - Requester pays: disabled; 19:14:42.639 INFO BaseRecalibrator - Initializing engine; 19:14:43.472 INFO FeatureManager - Using codec BEDCodec to read file XXX; 19:14:43.726 WARN IndexUtils - Feature file XXX appears to contain no sequence dictionary. Attempting to retrieve a sequence dictionary from the associated index file; 19:14:43.755 INFO BaseRecalibrator - Done ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5807#issuecomment-691600264:96,detect,detect,96,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5807#issuecomment-691600264,1,['detect'],['detect']
Safety,Is this to avoid having to pass AuthHolder around? We're going to remove most of the gcs path handling stuff once #1547 is finished which should hopefully be within the next week or two. (depends on google publishing something),MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1887#issuecomment-224364607:11,avoid,avoid,11,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1887#issuecomment-224364607,1,['avoid'],['avoid']
Safety,"It also fails in Mac OS X 10.11.6 x86_64. I'm trying to update my project to the latest version of GATK and this dependency throws the following error with some of my gradle tests and while running an uber-jar (using `--use_jdk_deflater false`):. ```; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGILL (0x4) at pc=0x000000011d925644, pid=7088, tid=20739; #; # JRE version: Java(TM) SE Runtime Environment (8.0_60-b27) (build 1.8.0_60-b27); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.60-b23 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # C [libgkl_compression8215566221555962564.dylib+0x1644] Java_com_intel_gkl_compression_IntelDeflater_resetNative+0x164; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /Users/daniel/workspaces/ReadTools/hs_err_pid7088.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #; ```. Find attached the log: [hs_err_pid7088.log.txt](https://github.com/broadinstitute/gatk/files/652421/hs_err_pid7088.log.txt). Should I open a different issue for this?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2302#issuecomment-267103689:280,detect,detected,280,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2302#issuecomment-267103689,1,['detect'],['detected']
Safety,"It can be removed. I'm working in a new branch and full training was required, thus the predictor-only package is not beneficial.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7839#issuecomment-1122810265:88,predict,predictor-only,88,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7839#issuecomment-1122810265,1,['predict'],['predictor-only']
Safety,"It looks like this is happening because the `GATKAnnotationPluginDescriptor` only propagates pedigree arguments to annotations that derive from the `PedigreeAnnotation` class. In the forum post comments, the one report that includes an input command line specifies the `PossibleDeNovo` annotation, which isn't part of the`PedigreeAnnotation` hierarchy. So it doesn't get properly populated. I assume the other case is similar. This fix is change the hierarchy (or better yet, find a more type-safe way to identify pedigree annotations). Also, the tool doesn't appear in the docs because it doesn't have a `@DocumentedFeature` annotation.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4987#issuecomment-403487947:493,safe,safe,493,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4987#issuecomment-403487947,1,['safe'],['safe']
Safety,It looks like we recommend this docker image in one of our tutorials [(How to part I) Sensitively detect copy ratio alterations and allelic segments](https://gatk.broadinstitute.org/hc/en-us/articles/360035531092--How-to-part-I-Sensitively-detect-copy-ratio-alterations-and-allelic-segments) but I do not recognize the user that created it.; @TatyanaLev could you post this issue on our forum? https://gatk.broadinstitute.org/hc/en-us/community/topics,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6836#issuecomment-696327611:98,detect,detect,98,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6836#issuecomment-696327611,2,['detect'],"['detect', 'detect-copy-ratio-alterations-and-allelic-segments']"
Safety,"It seems like the patch in 4.1.6 didn't go far enough and that exception needs to be replaced with a `continue` in all cases. This seems to be occurring for haplotypes with long indels inside their assembly padding that don't have enough spanning sequence to resolve. Since the variation is inside the padding, it seems safe to ignore. Increasing padding resolves the issue, alhtough this is at the cost of runtime and should not be necessary. For example, suppose we have a ref haplotype ABCDD, where A, B, and C represent sequences of, say, 100 bases and D is a sequence of 50 bases. Suppose further that A and DD are the padding. Then the cigar of an alt haplotype ABCD gets aligned as a 350 base match that doesn't span the full padded reference region, leading to the error. I still need to figure out why this didn't happen in 4.1.4 (my guess is that elsewhere the code effectively skipped these haplotypes before the exception).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6533#issuecomment-607059533:320,safe,safe,320,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6533#issuecomment-607059533,1,['safe'],['safe']
Safety,"It seems to me the `Header definition line` encompasses the information given by the `VCF Field` so this latter is redundant. . It would definitely be useful to categorize INFO (cohort) versus FORMAT (SAMPLE) level annotations. I'm not clear on the significance of the `Type` nor `Category` fields. `Type` might be the groupings, e.g. HaplotypeCaller standard annotations versus Mutect2 standard annotations.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3809#issuecomment-344423143:115,redund,redundant,115,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3809#issuecomment-344423143,1,['redund'],['redundant']
Safety,"It's `LineReaderUtil` and `VariantContextWriterBuilder`. Looks like `Defaults.USE_ASYNC_IO` has been hooked up to those since 2013/14. . Splitting the setting into `USE_SAM_ASYNC_IO` and `USE_TRIBBLE_ASYNC_IO`, or making asynch I/O an option on the reader/writer factories, would be safer options than removal (unless you're convinced that tribble asynch I/O is so broken that no one should ever use it).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1597#issuecomment-198533932:283,safe,safer,283,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1597#issuecomment-198533932,1,['safe'],['safer']
Safety,"It's likely that this persists in GATK4, but this isn't high priority because in practice we've found that most of our users ignore the spanning deletion alleles or actively dislike them. There are a variety of known issues surrounding spanning deletions, including filtering of * genotypes when the upstream deletion is filtered. I've attached our b37/GRCh37 WGS interval list (no decoy contig), which is split at Ns in the reference. There are 626 intervals. If recovering all the spanning deletion at shard boundaries is important to you, you can use that list to generate your shards and not subdivide further, though I can't guarantee they will be balanced. [wgs_calling_regions.v1.interval_list.txt](https://github.com/broadinstitute/gatk/files/3116941/wgs_calling_regions.v1.interval_list.txt). I had to add a .txt extension for Github, so you'll want to rename it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5905#issuecomment-486663930:464,recover,recovering,464,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5905#issuecomment-486663930,1,['recover'],['recovering']
Safety,"It's possible to override on the command line. . I tested it on our cluster and tools seemed to run fine. Removing the ""spark.driver.userClassPathFirst=true` causes problems on our cluster but the executor doesn't seem to make any difference. I'm worried we'll run into a problem where we need this to be both true and false to avoid 2 different simultaneous errors. My understanding is that the errors are happening because one class is being loaded by 2 different class loaders, but I don't understand how to fix that directly.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1518#issuecomment-190795541:328,avoid,avoid,328,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1518#issuecomment-190795541,1,['avoid'],['avoid']
Safety,It's some sort of race condition; ```; org.apache.spark.SparkException: Job aborted.; 	at org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:100); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1083); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1081); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1081); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:385); 	at org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopDataset(PairRDDFunctions.scala:1081); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply$mcV$sp(PairRDDFunctions.scala:1000); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply(PairRDDFunctions.scala:991); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply(PairRDDFunctions.scala:991); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:385); 	at org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:991); 	at org.apache.spark.api.java.JavaPairRDD.saveAsNewAPIHadoopFile(JavaPairRDD.scala:823); 	at org.disq_bio.disq.impl.formats.vcf.VcfSink.save(VcfSink.java:80); 	at org.disq_bio.disq.HtsjdkVariantsRddStorage.write(HtsjdkVariantsRddStorage.java:156); 	at org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSink.writeVariantsSingle(VariantsSparkSink.java:134); 	at org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSink.writeVariants(VariantsSparkSink.java:110); 	a,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429:76,abort,aborted,76,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429,1,['abort'],['aborted']
Safety,"It's using one of the large files (`NA12878_20_21_WGS_bam`), so not surprising that it's slow. We might be able to just remove it, as the test case just before it (for `CEUTrio.HiSeq.WGS.b37.ch20.1m-1m1k.NA12878.bam`) proved sufficient on its own to detect the recent ordering bugs in `ReadsSparkSink`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1342#issuecomment-163767792:250,detect,detect,250,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1342#issuecomment-163767792,1,['detect'],['detect']
Safety,I’m just now getting around to trying this. . I packed the gatk spark jar into the spark-operator’s spark docker container. Now I’m just wondering if it’s possible to stream the genomes from S3 to the executors? I’m trying to avoid any HDFS usage.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6198#issuecomment-541462343:226,avoid,avoid,226,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6198#issuecomment-541462343,1,['avoid'],['avoid']
Safety,"Java 17.0.12 from [Oracle](https://www.oracle.com/java/technologies/downloads/#java17) seems to display the same behavior. ```; 12:19:27.622 INFO ProgressMeter - Scaffold_1:21175995 247.8 125320 505.8; 12:19:49.612 INFO ProgressMeter - Scaffold_1:21178224 248.1 125330 505.1; 12:20:02.383 INFO ProgressMeter - Scaffold_1:21179909 248.4 125340 504.7; 12:20:14.545 INFO ProgressMeter - Scaffold_1:21183582 248.6 125360 504.4; 12:20:25.422 INFO ProgressMeter - Scaffold_1:21255583 248.7 125670 505.2; 12:20:36.810 INFO ProgressMeter - Scaffold_1:21281660 248.9 125810 505.4; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007f4ad4d94291, pid=3638446, tid=3638447; #; # JRE version: Java(TM) SE Runtime Environment (17.0.12+8) (build 17.0.12+8-LTS-286); # Java VM: Java HotSpot(TM) 64-Bit Server VM (17.0.12+8-LTS-286, mixed mode, sharing, tiered, compressed oops, compressed class ptrs, g1 gc, linux-amd64); # Problematic frame:; # C [libc.so.6+0xcf291] __memset_avx2_erms+0x11; #; # Core dump will be written. Default location: Core dumps may be processed with ""/usr/lib/systemd/systemd-coredump %P %u %g %s %t %c %h %e"" (or dumping to /bigdata/operations/ejaco020/gatk/core.3638446); #; # An error report file with more information is saved as:; # /bigdata/operations/ejaco020/gatk/hs_err_pid3638446.log; #; # If you would like to submit a bug report, please visit:; # https://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #; ```. I also attempted running within a singularity container, allocating 64GB of memory to the job and specifying -Xmx60G. Still seemed to silently ""crash"". Command I ran was:; ```; singularity run gatk_4.6.0.0.sif gatk HaplotypeCaller --java-options -Xmx60G -R /rhome/ejaco020/bigdata/gatk/Cclementina_182_v1_2.fa -I AlignedCalToCcl_Scaffolds_MarkDupOut.bam \ ; -O sing.vcf.gz \ ; -ERC GVCF; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8988#issuecomment-2389450721:600,detect,detected,600,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8988#issuecomment-2389450721,1,['detect'],['detected']
Safety,"Jumping in here to add that I'd very much like the opposite behavior in other tools. My understanding is that HaplotypeCaller and GenotypeGVCFs interpret intervals the other way - only emitting variants that start within the `--intervals` given. @yfarjoun pointed out to me that this is desirable when e.g. scatter/gathering WGS samples. But it would be nice for capture data to be able to cause the HC to emit all variants that _overlap_ any of the intervals given. For example if you are capturing a gene panel, it is reasonable to want to see all deletions that delete one or more bases of any exon, regardless of whether the start of the deletion is within the exon. Right now this requires padding the intervals by an estimated ""max deletion length"" to be safe, which then causes more variants that are totally outside the intervals to also be emitted. Might I instead suggest that an argument be introduced like:. `--variant-interval-matching [STARTS_WITHIN|CONTAINED|OVERLAPS]`. that would then have different defaults based on the historical behavior of various tools?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6339#issuecomment-572626422:761,safe,safe,761,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6339#issuecomment-572626422,1,['safe'],['safe']
Safety,"Just for future reference, note that comments in `testVariantRecalibratorSNPMaxAttempts` are also incorrect or out of date. The test passes even if you limit it to one attempt. ```; // For this test, we deliberately *DON'T* sample a single random int as above; this causes; // the tool to require 4 attempts to acquire enough negative training data to succeed; ```. So again, the tests were already ""broken."" But still, rather than attempt to fix them, I think it's best to follow the principle of not changing both production and test code to the extent that it is possible in this scenario. We've already updated enough exact-match expected results to make me a bit uncomfortable!. Someone else may want to tackle fixing the tests in a separate push, but I think it makes sense for me to focus on avoiding these sorts of issues when writing tests for the new tools. EDIT: For the record, I confirmed that the undesired behavior in this test that the RNG hack was trying to avoid was fixed (and hence, the test was ""broken"") in #6425. Probably wasn't noticed because this is the only non-exact-match test and the test isn't strict enough to check that attempts 1-3 fail, it only checks that we succeed by attempt 4. Again, someone else may feel free to examine the actual coverage of this test and whether it's safe to remove it and/or clean up all the duct tape---but at some point, it becomes difficult to tell which pieces of duct tape are load bearing!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7709#issuecomment-1064236628:799,avoid,avoiding,799,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7709#issuecomment-1064236628,3,"['avoid', 'safe']","['avoid', 'avoiding', 'safe']"
Safety,"Just noting here that I saw a lot of intermittent 60-minute Travis timeouts for the gCNV case mode WDL tests in #7411. I'm pretty sure that at some point we were running cohort + case together, so not sure why case alone is now hitting the limit. So might be worth investigating and tightening up the tests/data. Also note @ldgauthier's concerns about using a fake dictionary in the simulated data in #6957.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4007#issuecomment-899474100:67,timeout,timeouts,67,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4007#issuecomment-899474100,1,['timeout'],['timeouts']
Safety,"Just to make sure I understand the issue---will this cause technical problems in the Firecloud environment, or is it more of a style issue?. If the latter, one reason I prefer the use of optional file inputs to trigger tool-level ""modes"" when possible is that it propagates more naturally from the tool level. For example, let's consider a tool that can operate in either tumor-only or matched-pair mode. It is natural at the tool level to make the tumor a required input and the normal optional. The other options are quite awkward: 1) make both inputs required and switch between using the normal or not with a flag (in which case it is very easy for the user to shoot themselves in the foot if they forget to set the flag right, and we'd have to pass a dummy normal every time we want to run tumor only if we don't actually have a pair), 2) leave the normal as optional but add a flag anyway, which would be redundant and require an additional validation (i.e., if the flag is set to matched mode but we don't have a normal, we should fail early), or 3) write separate tools for each mode with the corresponding required inputs. If we accept that optional file input is the way to handle such a scenario at the tool level but not at the workflow level, then we will simply run into the same problems at the workflow level. I'm sure there are more complex scenarios when triggering on file presence/absence doesn't uniquely specify a workflow, in which case flags are a must. But for simple scenarios, I'm not sure why we shouldn't take advantage of the ability to specify optional file inputs in WDL (actually, I'm not sure how else we are supposed to use them?). However, if this is a problem for Firecloud, then I'd like to understand why---and what possible solutions there might be.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3657#issuecomment-334046444:911,redund,redundant,911,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3657#issuecomment-334046444,1,['redund'],['redundant']
Safety,"Just to provide additional context, all of the machinery in the AbstractRecordCollection classes for reading/writing CNV input/output TSVs was meant to make passing metadata (dictionaries, sample names, etc.) from tool to tool as automatic and consistent as possible. This avoids having to re-provide sample names, dictionaries, etc. at each tool/step---which often led to dictionary inconsistencies, contig/sample ordering bugs, etc. in older versions of the pipelines---at the cost of 1) redundantly carrying along this metadata in input/output TSVs, and 2) requiring consistent dictionaries in all initial BAM inputs. I think these are small costs to pay.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6957#issuecomment-726973610:273,avoid,avoids,273,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6957#issuecomment-726973610,2,"['avoid', 'redund']","['avoids', 'redundantly']"
Safety,"Karthik;; Thanks for this, I've done that in bcbio so hopefully will avoid the issue going forward. Feel free to close here unless you want to try and trace down further what is happening. Thanks again for the pointer that led us to the underlying issue.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5045#issuecomment-407512076:69,avoid,avoid,69,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5045#issuecomment-407512076,1,['avoid'],['avoid']
Safety,"Komal -- thanks for raising this issue and providing so much detail. Karthik -- Thanks for the debugging and pointers on this, I hadn't thought from the core dump to be looking at the annotation fields so this is a really helpful lead. What we can do in bcbio is avoid adding any of these until after running the joint calling so they all get on the final joint VCF rather than the gVCFs. This should hopefully avoid needing to dig too deeply into this and we can take our lesson as: don't annotate gVCFs with too much information. Thank you again for helping with diagnosing the underlying issue.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5045#issuecomment-407494531:263,avoid,avoid,263,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5045#issuecomment-407494531,2,['avoid'],['avoid']
Safety,"Let's please repair the branch before merge, rather than risk clobbering master. Squash/rebase does not interact nicely with merge commits in the history, particular if the merge commits contain changes due to conflict resolution.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2558#issuecomment-341790241:57,risk,risk,57,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2558#issuecomment-341790241,1,['risk'],['risk']
Safety,"Looking at the code in both GATK's `GenomicsDBImport` and the GenomicsDB library itself, I don't think the sample name map was ever intended as a mechanism to rename samples. It was just added as a way to avoid the up-front download of all the VCF headers. As evidence for this, we have a couple of asserts like this in the code:. ```; assert sampleName.equals(((VCFHeader) reader.getHeader()).getGenotypeSamples().get(0));; ```. However, using the map file to rename samples is a pretty natural thing for clients to want to do. At a minimum, we need to throw if a rename is attempted until sample renaming via the map file is officially supported and tested.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3814#issuecomment-343261932:205,avoid,avoid,205,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3814#issuecomment-343261932,1,['avoid'],['avoid']
Safety,"Looking at the three tools {GetPileupSummaries, CollectAllelicCounts, CollectPerBaseCounts} it definitely seems possible to eliminate a lot of redundancy. I think adding an option to count bases per read as opposed to per fragment, the current default in this tool, would essentially accomplish that---the idea being that you may want to look at read bases for modelling errors. I understand, though, that I might be kibitzing something I know far too little about and there might be many more implications to consider, but those are my general thoughts for the three tools. This seems to agree with the discussion [#4717 (comment)](https://github.com/broadinstitute/gatk/issues/4717#issuecomment-386734926).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6545#issuecomment-610591143:143,redund,redundancy,143,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6545#issuecomment-610591143,1,['redund'],['redundancy']
Safety,Looks good modulo the possibly redundant registration I asked about.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1721#issuecomment-212084711:31,redund,redundant,31,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1721#issuecomment-212084711,1,['redund'],['redundant']
Safety,"Looks like this failed on travis. I think given that given the lateness of the hour (release wise), we might want to take the original change that removes the libgcc-ng dependency, since that passed on travis, and rely on the simple workarounds for osx, which we'll have to convey out-of-band. Anything that requires changing the docker image seems risky at this point, not to mention that the image is already at 5.2 gig, which is way over our desired target. @samuelklee Any thoughts on this ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4087#issuecomment-356131086:349,risk,risky,349,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4087#issuecomment-356131086,1,['risk'],['risky']
Safety,"Looks like this is my fault... I didn't realize BWA produces SAM output and the non-spark tool was correcting my mistake automatically (by checking for a magic number). Can we make the error message more informative like: ""BAM file must start with BGZF magic number""? . It would be great to detect whether it's SAM or BAM by checking the file contents, as in non-spark tools that use htsjdk, rather than the extension. Is this easily done?. @lbergelson To clarify I was using the regular BWA binaries not the GATK BWA tool.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3488#issuecomment-324959378:291,detect,detect,291,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3488#issuecomment-324959378,1,['detect'],['detect']
Safety,M --ba; se-quality-score-threshold 18 --dragstr-het-hom-ratio 2 --dont-use-dragstr-pair-hmm-scores false --pair-hmm-gap-continuation-penalty 10 --expected-mismatch-rate-for-read-disqualification 0; .02 --pair-hmm-implementation FASTEST_AVAILABLE --pcr-indel-model CONSERVATIVE --phred-scaled-global-read-mismapping-rate 45 --disable-symmetric-hmm-normalizing false --disable-cap-base-qu; alities-to-map-quality false --enable-dynamic-read-disqualification-for-genotyping false --dynamic-read-disqualification-threshold 1.0 --native-pair-hmm-threads 4 --native-pair-hmm-use-dou; ble-precision false --flow-hmm-engine-min-indel-adjust 6 --flow-hmm-engine-flat-insertion-penatly 45 --flow-hmm-engine-flat-deletion-penatly 45 --pileup-detection false --pileup-detection-; enable-indel-pileup-calling false --num-artificial-haplotypes-to-add-per-allele 5 --artifical-haplotype-filtering-kmer-size 10 --pileup-detection-snp-alt-threshold 0.1 --pileup-detection-i; ndel-alt-threshold 0.5 --pileup-detection-absolute-alt-depth 0.0 --pileup-detection-snp-adjacent-to-assembled-indel-range 5 --pileup-detection-bad-read-tolerance 0.0 --pileup-detection-pro; per-pair-read-badness true --pileup-detection-edit-distance-read-badness-threshold 0.08 --pileup-detection-chimeric-read-badness true --pileup-detection-template-mean-badness-threshold 0.0; --pileup-detection-template-std-badness-threshold 0.0 --bam-writer-type CALLED_HAPLOTYPES --dont-use-soft-clipped-bases false --override-fragment-softclip-check false --min-base-quality-s; core 10 --smith-waterman JAVA --max-mnp-distance 0 --force-call-filtered-alleles false --reference-model-deletion-quality 30 --soft-clip-low-quality-ends false --allele-informative-reads-o; verlap-margin 2 --smith-waterman-dangling-end-match-value 25 --smith-waterman-dangling-end-mismatch-penalty -50 --smith-waterman-dangling-end-gap-open-penalty -110 --smith-waterman-danglin; g-end-gap-extend-penalty -6 --smith-waterman-haplotype-to-reference-match-value 200 --smith-w,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8574#issuecomment-1793390789:6822,detect,detection-absolute-alt-depth,6822,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8574#issuecomment-1793390789,1,['detect'],['detection-absolute-alt-depth']
Safety,MapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:41458,abort,abortStage,41458,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,1,['abort'],['abortStage']
Safety,"Master stats:; Discovered 31120 intervals.; Killed 388 intervals that were near reference gaps.; Killed 174 intervals that had >1000x coverage.; Discovered 9480784 mapped template names.; Ignoring 19200460 genomically common kmers.; Discovered 39739968 kmers.; Discovered 34170333 unique template names for assembly.; Wrote SAM file of aligned contigs.; Discovered 6255 variants.; INV: 239; DEL: 3644; DUP: 1123; INS: 1249; Elapsed time: 47.34 minutes. This PR:; Discovered 31125 intervals.; Killed 390 intervals that were near reference gaps.; Killed 174 intervals that had >1000x coverage.; Discovered 9480874 mapped template names.; Ignoring 19200460 genomically common kmers.; Discovered 39730495 kmers.; Discovered 34154214 unique template names for assembly.; Wrote SAM file of aligned contigs.; Discovered 6234 variants.; INV: 233; DEL: 3635; DUP: 1125; INS: 1241; Elapsed time: 46.77 minutes. We did find a few extra intervals by gluing evidence across partition boundaries. The number of variants detected has decreased by just a little. I think this is likely due to calculating read metadata at the library level, rather than the read group level.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2766#issuecomment-304373035:1006,detect,detected,1006,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2766#issuecomment-304373035,1,['detect'],['detected']
Safety,"Meh, I think it may too much hassle for a 5-7% gain. Caching comes with the; risk of getting out of synch with the state. Maybe we should table this for; now. On Monday, July 25, 2016, droazen notifications@github.com wrote:. > @lbergelson https://github.com/lbergelson I'm not convinced we want to; > pre-calculate it at construction time, though. The null check is no big; > deal, and only needed in one place for each field (the method that; > retrieves or recalulates the cached value).; > ; > —; > You are receiving this because you were assigned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235127503,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/AB5rLzA4C5ZBD8ul-RspK7TpQbACMj-Bks5qZVS7gaJpZM4JR8AP; > . ## . Sent from Gmail Mobile",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235129333:77,risk,risk,77,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235129333,1,['risk'],['risk']
Safety,"Meta-comments for reviewers: the new program groups in this PR are based on @sooheelee 's spreadsheet, including some that are placeholders for things that will soon live in Picard, but aren't accessible from there yet. I've left the old program groups intact because they're still being referenced by tools. As the doc PRs are merged in, eventually these will be left dangling with no references, and then we'll remove them. In the meantime we'll have some redundancies (ReadProgramGroup will be replaced by ReadDataProgramGroup, or whatever we settle on).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3924#issuecomment-349722389:458,redund,redundancies,458,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3924#issuecomment-349722389,1,['redund'],['redundancies']
Safety,"More info from @ldgauthier:. ```; I’ve only been trying the same GenomicsDBImport over and over again. I estimate it to take about; 30 hours if it’s ever successful. The exception happens in different batches every time. Sam F. ; said he saw the exception too but he could eventually resubmit his way through it. His jobs are; shorter running.; ```. So it seems like the error is nondeterministic, but can't be recovered from within the same VM instance / process.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-412917164:411,recover,recovered,411,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-412917164,1,['recover'],['recovered']
Safety,"Most of the scaling issues in Cromwell/Terra have been resolved. Terra still has limitations on workflow metadata size, and passing long file arrays to ever task in large scatters (i.e. the full list of counts files is passed into every gCNV shard) can limit our batch sizes for workflows that embed gCNV (e.g. GatherBatchEvidence in gatk-sv). gCNV workflows also don't call cache reliably (presumably due to timeouts) probably again due to the large file arrays, including 2D arrays.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4397#issuecomment-928168236:409,timeout,timeouts,409,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4397#issuecomment-928168236,1,['timeout'],['timeouts']
Safety,"My suspicion was wrong. We also include a safety check which cause us to correctly reject most accidental matches. If we detect 2 chromosomes with the same name but different lengths we fail even if we detect otherwise matching chromosomes. I've run all the dictionaries I could find in the gatk bundle against each and only b37 and b37_decoy are compatible with each other which is the desired behavior I believe. | | hg18 | hg19 | b37 | b37_decoy | hg38 |; | -- |------|-----|------|-----------|-------|; | hg18 | ✅ | | | | |; | hg19 | | ✅ | | | |; | b37 | | | ✅ | ✅ | |; | b37_decoy | | | ✅ | ✅ | |; | hg38 | | | | | ✅ |. ```; @DataProvider; public Iterator<Object[]> getComparisons(){; final ArrayList<Object[]> comparisons = new ArrayList<>();; final List<String> dicts = Arrays.asList(""Homo_sapiens_assembly18.dict"",; ""ucsc.hg19.dict"",; ""human_b36_both.dict"",; ""human_g1k_v37.dict"",; ""human_g1k_v37_decoy.dict"",; ""Homo_sapiens_assembly38.dict"");; for( String left : dicts) {; for (String right: dicts){; Path leftDict =Paths.get(""/Users/louisb/Downloads/dicts"", left);; Path rightDict = Paths.get(""/Users/louisb/Downloads/dicts"", right);. comparisons.add( new Object[] {leftDict, rightDict});; }; }; return comparisons.iterator();; }. @Test(dataProvider = ""getComparisons""); public void testSequenceDictionariesAgainstEachother(Path left, Path right){; String leftName = left.getFileName().toString();; String rightName = right.getFileName().toString();; SequenceDictionaryUtils.validateDictionaries(leftName,; SAMSequenceDictionaryExtractor.extractDictionary(left),; rightName,; SAMSequenceDictionaryExtractor.extractDictionary(right));; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3754#issuecomment-494924193:42,safe,safety,42,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3754#issuecomment-494924193,3,"['detect', 'safe']","['detect', 'safety']"
Safety,"NIO streaming is currently used to avoid BAM localization in `CollectCounts` and `CollectAllelicCounts`, which is particularly beneficial when running on a limited set of intervals. It is also used in `JointSegmentation` to stream VCFs, which saves on disk space. Additional tasks that could benefit from streaming are those that use counts files inputs: `FilterIntervals`, `GermlineCNVCallerCaseMode`, `GermlineCNVCallerCohortMode`, `DetermineGermlineContigPloidyCaseMode`, and `DetermineGermlineContigPloidyCohortMode`, `CreateReadCountPanelOfNormals`, and `DenoiseReadCounts`. These would require that read counts are in TSV format, which we should move to using exclusively (instead of HDF5).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4806#issuecomment-926124144:35,avoid,avoid,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4806#issuecomment-926124144,1,['avoid'],['avoid']
Safety,Next step is to evaluate using Kryo for the deep copying in the SafeDoFn approach (uses reflection to deep copy all fields via direct assignment (no byte array intermediary)).,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/702#issuecomment-127404027:64,Safe,SafeDoFn,64,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/702#issuecomment-127404027,1,['Safe'],['SafeDoFn']
Safety,"No did not. Fereshteh Izadi; ***@***.***?anonymous&ep=bwmEmailSignature> Book time to meet with ***@***.***?anonymous&ep=bwmEmailSignature>; ________________________________; From: mcollodetti ***@***.***>; Sent: Monday, 4 March 2024 19:51; To: broadinstitute/gatk ***@***.***>; Cc: Angel Izadi ***@***.***>; Author ***@***.***>; Subject: Re: [broadinstitute/gatk] I need a PON vcf (Issue #8477). CAUTION: This email originated from outside of Swansea University. Do not click links or open attachments unless you recognise the sender and know the content is safe. RHYBUDD: Daeth yr e-bost hwn o'r tu allan i Brifysgol Abertawe. Peidiwch â chlicio ar atodiadau neu agor atodiadau oni bai eich bod chi'n adnabod yr anfonwr a'ch bod yn gwybod bod y cynnwys yn ddiogel. Did it work? I had the same problem and that change didn't do it. —; Reply to this email directly, view it on GitHub<https://github.com/broadinstitute/gatk/issues/8477#issuecomment-1977332718>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AUS3HJIINO56ONF7EHSUANDYWTGFLAVCNFSM6AAAAAA3SP5AQKVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMYTSNZXGMZTENZRHA>.; You are receiving this because you authored the thread.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8477#issuecomment-1993978579:559,safe,safe,559,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8477#issuecomment-1993978579,1,['safe'],['safe']
Safety,"No, this dependency is not from the main xgboost project, it's a 3rd-party library that allows you to make predictions using saved model files. At the time, the plan was to train models in python, then use this java tool to apply the trained predictor. The GQ filtering project trains using a java tool, so I have to bring in the real xgboost library.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7950#issuecomment-1189369117:107,predict,predictions,107,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7950#issuecomment-1189369117,2,['predict'],"['predictions', 'predictor']"
Safety,"Not a bad idea, will look into that tomorrow. Note that you are using Tensorflow 1.4 or 1.5 and that from v1.6 even the; non-Intel optimized build supports only AVX capable machines. On Thu 11 Oct 2018, 21:07 droazen, <notifications@github.com> wrote:. > *@droazen* commented on this pull request.; > ------------------------------; >; > In; > src/main/java/org/broadinstitute/hellbender/tools/walkers/vqsr/CNNScoreVariants.java; > <https://github.com/broadinstitute/gatk/pull/5291#discussion_r224587026>:; >; > > @@ -198,6 +200,13 @@; > return new String[]{""No default architecture for tensor type:"" + tensorType.name()};; > }; > }; > +; > + IntelGKLUtils utils = new IntelGKLUtils();; > + if (utils.isAvxSupported() == false); > + {; > + return new String[]{CNNScoreVariants.AVXREQUIRED_ERROR};; >; > Maybe the answer is for the conda environments to set an extra environment; > variable that would allow GATK to detect which conda environment it's in.; > Then you could have a check in CNNScoreVariants that aborts the tool only; > if AVX is not present AND you're running in the Intel conda environment,; > and point the user to the non-Intel conda environment in the error message.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/pull/5291#discussion_r224587026>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AG6lr8HM6ItLWqfSaTKeVY4yCp07il29ks5uj6TugaJpZM4XNHdi>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-429109651:915,detect,detect,915,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-429109651,2,"['abort', 'detect']","['aborts', 'detect']"
Safety,Note that David R. is only on these issues because he's the one that ported them over from gatk-protected. I think it's safe to close this---I'd hope future denoising models would be more generalizable and able to handle FFPE (even if this might require e.g. FFPE-specific resource tracks).,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2847#issuecomment-926784945:120,safe,safe,120,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2847#issuecomment-926784945,1,['safe'],['safe']
Safety,"Note to self: we should get rid of the MAX_READ_BATCH check as part of this, since that was motivated by timeouts IIRC.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4757#issuecomment-388113766:105,timeout,timeouts,105,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4757#issuecomment-388113766,1,['timeout'],['timeouts']
Safety,"OK - PedigreeValidationType is now set in the constructor and is final. This does not separate the two intertwined codepaths around PedigreeFile vs. FounderIds, but that was a pre-existing problem. It doesnt doesnt change the pre-existing weirdness around the timing of setting pedigreeFile and/or founderIds within GATKAnnotationPluginDescriptor, where PedigreeAnnotation gets special treatment. I dont think this makes that situation any worse. if you still have concerns on this proposal, I actually think I could make our code work if you simply exposed a protected getPedigreeFile() method on PedigreeAnnotation. I can make the SampleDB instance in my code without needed to share code here. It seemed useful to expose some of that code to avoid duplication, but if it's going to over-complicate we can remove it. Also: that one test failure seems potentially unrelated (https://travis-ci.com/github/broadinstitute/gatk/jobs/510624560)? A compile issue with javadoc?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7277#issuecomment-853986169:745,avoid,avoid,745,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7277#issuecomment-853986169,1,['avoid'],['avoid']
Safety,"OK, finally tracked down that original issue from Mehrtash concerning the bundling: https://github.com/broadinstitute/gatk/issues/4397 As we discussed, there was a lot of back and forth to try to resolve this issue, and it was confounded by a lexicographical bug (which may have been reintroduced here). The last chapter in this saga was https://github.com/broadinstitute/gatk/pull/5490. If the matrix transpose is still troublesome and we can avoid it by being more clever with WDL indexing, then maybe we can explore that. Or we can just see if there are analogous existing WDLs and borrow their solution. However, note that @mwalker174 indicated that the *creation* of the matrix itself is troublesome for call caching. If bundling is the only answer and we are willing to pay the cost of localizing all gCNV results to all shards, it might make things easier to first bundle everything up at the end of each gCNV task. Also, would Cromwell be able to handle things if we change the bundling from a) *all* gCNV results (i.e., across all samples and shards) to b) a single bundled global quantity (model + interval lists) + calls bundled (across shards) per sample? Each postprocessing task would then take the global bundle + the bundle containing calls for that sample. That seems like it would resolve Mehrtash's original complaint, while still minimizing the number of files whizzing around. We also discussed batching by sample at the postprocessing task level, but I think we want to keep this task at the per-sample level for parallelism.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6607#issuecomment-632303744:444,avoid,avoid,444,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6607#issuecomment-632303744,1,['avoid'],['avoid']
Safety,"OK, looks like you can get around the compiler lock issues by pointing each invocation of GermlineCNVCaller to a different compilation directory. For example, invoke `gatk` by. `THEANORC=PATH/TO/THEANORC_# gatk GermlineCNVCaller ...`. This uses the `THEANORC` environment variable to set the `.theanorc` configuration file to `PATH/TO/THEANORC_#` for this instance of GATK (where you should fill in `#` appropriately). Each `PATH/TO/THEANORC_#` should be a file containing the following:. ````; [global]; base_compiledir = PATH/TO/COMPILEDIR_#; ````. Where again, `#` is filled in appropriately. The goal is to point each GermlineCNVCaller instance to a different compilation directory. @xysj1989 can you let me know if this works for you?. This is a bit of a hack. We could probably avoid this by changing the GATK code to use a specified or temporary directory for the theano directory without too much effort. However, there is an upside to using a non-temporary directory to avoid recompilation of the model upon subsequent runs. In this case, we'd just want to let the user be able to specify the theano directory (rather than dump things in `~/.theano` unexpectedly). We should think about whether this should be opt-in, i.e., should we preserve the original behavior of using `~/.theano` by default?. @mwalker174 opinions? @droazen or engine team, thoughts on what the policy should be for python/R scripts doing this sort of thing? Is it generally true that the GATK leaves no trace, other than producing the expected output?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6235#issuecomment-548430809:784,avoid,avoid,784,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6235#issuecomment-548430809,2,['avoid'],['avoid']
Safety,"OK, relaxed the exact match to a delta of 1E-6 (chosen because doubles are formatted in somatic CNV outputs as `""%.6f""`) and tests pass on Travis (modulo an unrelated intermittent timeout failure). Note also that I was also able to reproduce locally by switching between Java 8 and 11. Had to add some quick test code for doing the comparisons; not actually sure if we have other utility methods to do so somewhere in the codebase. Another interesting note: I tried to clean up the offending use of log10factorial in AlleleFractionLikelihoods, but this introduced numerical differences at the ~1E-3 level. I think all of the round tripping between log and log10 actually adds up. Some digging revealed that this was introduced way back in gatk-protected in https://github.com/broadinstitute/gatk-protected/commit/aeec297e104db9f5196cb8f8e6691133302474bc#diff-34bd76cb2a416a212e25cbfb11298207265fb9cced775918aefcdb6b91ebc247. Despite the fact that we could easily replace the use of log10factorial with a private logGamma cache, at this point I think it makes more sense to freeze the current behavior. But if similar numerical changes are introduced to ModelSegments in the future, then it might make sense to clean this up at that point as well. Anyway, changed the title of the PR to reflect this update. Should be ready to go!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7652#issuecomment-1023793014:180,timeout,timeout,180,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7652#issuecomment-1023793014,1,['timeout'],['timeout']
Safety,"OK, thanks @drifty914. Note that the file with num_intervals_per_scatter = 20 is a minimal test case that is run with our continuous integration tests. In real-world use, you want enough intervals in each shard to fit a denoising model---probably 5000 or more is safe. I am wondering if your issue is related to https://github.com/broadinstitute/gatk/issues/4782 and https://askubuntu.com/questions/162229/how-do-i-increase-the-open-files-limit-for-a-non-root-user. It may be that your user ulimit is not high enough for the theano compilation directory?. Let me try to put together a fix for that issue and see if it addresses yours as well.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5714#issuecomment-467085960:263,safe,safe,263,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714#issuecomment-467085960,1,['safe'],['safe']
Safety,"OK, the first test run I tried was with 1kb bins and *no additional normals*. Coverage takes about an hour to collect per BAM and ploidy inference takes about 10 minutes. A few things:. 1) Looks like we are concordant with the truth CN on X for all but 3/40 of the samples. The GQs for these discordant calls are low (~3, 23, and 25 compared with ~400 for most of the others). 2) However, we are striking out on over half of the samples on Y. We mostly call 1 copy when the truth calls 0. Mehrtash thinks this is because a) I didn't mask out any PARs or otherwise troublesome regions on Y and b) I didn't include any other normals. I'll try rerunning with a mask first, then with other normals, and then with both. Hopefully this should clear up with just the mask. 3) There are a few samples where we strike out because the truth calls 2 copies on Y and we call 1. Mehrtash pointed out that this is most likely because the prior table we put together assumes Y can have at most 1 copy. So hopefully these are trivially recovered once we relax this. 4) The GQs are weirdly high on 1, X, and Y compared to the rest of the autosomes. @ldgauthier any idea why this might be? If there's no reason, then something funny is going on within the tool. I haven't gotten a chance to plot any of the counts data yet, either, which may make things more obvious. I'll do this today.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-364234449:1020,recover,recovered,1020,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-364234449,1,['recover'],['recovered']
Safety,"OK. However, don't forget that the denoising model is fit independently in each block. So introducing too many blocks could cause overfitting, in a sense. Also, you want to make sure that you have enough bins in each block to learn the model. 10k seems safe, but I'd spot check results first if you want to go down to 1k.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4397#issuecomment-391071615:253,safe,safe,253,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4397#issuecomment-391071615,1,['safe'],['safe']
Safety,"OK. In GATK3, the sharding size is calculated in `GenomeAnalysisEngine.getShardStrategy`. Since `GenotypeGVCFs` is a RodWalker and does not use input reads (BAM file), the shard size is `1,000,000`. ; This might be more than a thread safety bug (which is easy to fix, by making `GenotypingEngine.calculateOutputAlleleSubset() ` `synchronized`). What worries me is If the cache of upstream deletions spans intervals, this code will not work since the processing is asynchronous. For example, if there are 2 threads and the removed deletion crosses the shard barrier and the downstream interval thread is first to process, it will not see the upstream removed deletion.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2326#issuecomment-270467197:234,safe,safety,234,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2326#issuecomment-270467197,1,['safe'],['safety']
Safety,"OfBins) + "" should be >= 0."");; >; > @asmirnov <https://github.com/asmirnov> and @samuelklee; > <https://github.com/samuelklee> are both correct, but for the future in; > cases where you *would* want an IllegalArgumentException you should use; > Utils.validateArg to render this sort of thing a one-liner.; > ------------------------------; >; > In src/main/java/org/broadinstitute/hellbender/tools/copynumber/; > CreateBinningIntervals.java; > <https://github.com/broadinstitute/gatk/pull/3597#discussion_r140646132>:; >; > > + doc = ""width of the bins"",; > + fullName = WIDTH_OF_BINS_LONG_NAME,; > + shortName = WIDTH_OF_BINS_SHORT_NAME,; > + optional = true,; > + minValue = 1; > + ); > + private int widthOfBins = 1;; > +; > + @Argument(; > + doc = ""width of the padding regions"",; > + fullName = PADDING_LONG_NAME,; > + shortName = PADDING_SHORT_NAME,; > + optional = true,; > + minValue = 0; > + ); > + private int padding = 0;; >; > . . . and if this padding is different from the inherited padding then; > this demands a comment to avoid confusion.; > ------------------------------; >; > In src/main/java/org/broadinstitute/hellbender/tools/copynumber/; > CreateBinningIntervals.java; > <https://github.com/broadinstitute/gatk/pull/3597#discussion_r140646146>:; >; > > +; > + // check if the bin widths are set appropriately; > + if(widthOfBins <= 0) {; > + throw new IllegalArgumentException(""Width of bins "" + Integer.toString(widthOfBins) + "" should be >= 0."");; > + }; > +; > + // get the sequence dictionary; > + final SAMSequenceDictionary sequenceDictionary = getBestAvailableSequenceDictionary();; > + final List<SimpleInterval> intervals = hasIntervals() ? intervalArgumentCollection.getIntervals(sequenceDictionary); > + : IntervalUtils.getAllIntervalsForReference(sequenceDictionary);; > +; > + // create an IntervalList by copying all elements of 'intervals' into it; > + IntervalList intervalList = new IntervalList(sequenceDictionary);; > + intervals.stream().map(si -> new Inte",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3597#issuecomment-331744211:5045,avoid,avoid,5045,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3597#issuecomment-331744211,1,['avoid'],['avoid']
Safety,"Oh nice, that seems to be the ticket. All reads are used when setting that parameter. May I ask what the logic behind performing the downsampling is? Isn't there a risk of removing valid alignments that contribute to low abundance variation events? This would maybe only really be a problem when you are analysing sequences from a population of cells/microbes, but maybe the reward is greater than the risk?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7873#issuecomment-1139111543:164,risk,risk,164,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7873#issuecomment-1139111543,2,['risk'],['risk']
Safety,"Oh wait, I just checked out the background. I still don't like the redundancy. Can't we just replace `PID` with `PS`?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5318#issuecomment-430747813:67,redund,redundancy,67,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5318#issuecomment-430747813,1,['redund'],['redundancy']
Safety,"Oh, that's interesting... I wonder if there is a sane way to detect the version mismatch. It's weird that it breaks with a NEWER version of spark. I would expect 2.1.0 to be compatible with 2.0.2. . Incidentally, it would be a good idea to upgrade to the newest spark version. #2555",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2545#issuecomment-290764935:61,detect,detect,61,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2545#issuecomment-290764935,1,['detect'],['detect']
Safety,"Oh, that's right, I'd forgotten about the SGA license issue. Since we're; about to move to fermi-lite (hopefully), let's just hold off on checking in; the initialization script until that's done, keeping it in the known bucket; location. On Wed, Mar 8, 2017 at 11:37 AM, Steve Huang <notifications@github.com>; wrote:. > @cwhelan <https://github.com/cwhelan> I was actually debating with myself; > about whether to include the initialization script here, as it was living; > in the bucket referred to in the creation script.; > So we could do this:; > always store the initialization script locally with the creation script; > instead of referring to a script living remotely, and makes that a required; > argument. The good: this makes it easier to track changes; The bad:; > initialization script must be removed from the bucket to avoid tracking; > possible different versions.; >; > A non-technical issue: we are ""delivering"" SGA in the initialization; > script, if that comes in to this repo, legal might have a problem with it.; > On the other hand, it the initialization script lives in a place only we; > can access, we are ""installing SGA for our own use"", which is not a problem; > with the GPL license.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/pull/2435#issuecomment-285093289>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AArTZZPv4WyEYz-yYaZZIIjH8LBMOhZ4ks5rjtlCgaJpZM4MTqFc>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2435#issuecomment-285105258:834,avoid,avoid,834,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2435#issuecomment-285105258,1,['avoid'],['avoid']
Safety,"Ok, safe to merge now.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3961#issuecomment-355113083:4,safe,safe,4,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3961#issuecomment-355113083,1,['safe'],['safe']
Safety,"Okay apparently there is not a version number in the picard.jar file downloaded from https://github.com/broadinstitute/picard/releases and thus if easybuild detects a cache copy, it will use that instead of downloading it which was an older version. They forced it to download and now version is correct. I will re-try and see if the error persists. Thanks for catching that.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3154#issuecomment-1419783633:157,detect,detects,157,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3154#issuecomment-1419783633,1,['detect'],['detects']
Safety,"Okay, I have a new panel for hg38 here: gs://broad-dsde-methods-davidben/mutect2-2023-panel-of-normals/mutect2-hg38-pon.vcf.gz. It has all the variants of the old panel, plus more that arose in more recent versions of Mutect2. It is also generally somewhat more conservative, with a greater bias toward precision than the previous one. This panel is intended to be used at your own risk. I can vouch that it doesn't wreck the results of our own validations but I do not have time to vet it thoroughly enough to put it in the best practices google bucket. Likewise, I cannot promise that it will improve specificity in any particular set of samples. Within several months I hope we are all running the next version of Mutect and never need to see a panel of normals again.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1430315034:382,risk,risk,382,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1430315034,1,['risk'],['risk']
Safety,"One of our goals for alpha (https://github.com/broadinstitute/gatk/issues/961) is actually to wrap `spark-submit` and its many options to make it easier to run hellbender tools on spark. We want users to be able to type a simple command like `./hellbender ToolName [toolArgs] --sparkMaster X`, and have hellbender figure out whether to invoke `spark-submit` or `gcloud dataproc` on their behalf, and provide sensible defaults for all relevant spark options. . Perhaps there is a way in `SparkCommandLineProgram` to detect whether an option has already been set externally, and allow the default to be overridden if it has been?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1070#issuecomment-152538633:515,detect,detect,515,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1070#issuecomment-152538633,1,['detect'],['detect']
Safety,One timeout and one 137 :(,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7624#issuecomment-1004386181:4,timeout,timeout,4,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7624#issuecomment-1004386181,1,['timeout'],['timeout']
Safety,Otherwise shall we try to pack the libraries up as part of the build? Since we don't have a pure-Java failover implementation I think this option is risky.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/503#issuecomment-100297035:149,risk,risky,149,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/503#issuecomment-100297035,1,['risk'],['risky']
Safety,"Per discussions with @fleharty, we are looking to significantly revamp the automated somatic CNV evaluations in preparation for benchmarking the TH prototype. The existing evaluations use a few unsupported/experimental tools and idiosyncratic/redundant classes (e.g., the `src/main/java/org/broadinstitute/hellbender/tools/copynumber/utils/annotatedinterval` class this issue concerns), the functionality of which we can hopefully move to python-based validation code. . The aforementioned code was purposefully decoupled from supported CNV code, but since then it has been incorporated into `Funcotator` tools and `ValidateBasicSomaticShortMutations`, at least. @jonn-smith @davidbenjamin can we discuss a plan for cleaning this code up? Would it be easy to use an existing TSV/XSV class to handle the functionality needed for these tools?. @jonn-smith perhaps we should also discuss the plan for future `FuncotateSegments` development/integration with @fleharty.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3884#issuecomment-526226506:243,redund,redundant,243,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3884#issuecomment-526226506,1,['redund'],['redundant']
Safety,Probably safe then.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5678#issuecomment-463780367:9,safe,safe,9,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5678#issuecomment-463780367,1,['safe'],['safe']
Safety,"QandDP,Number=2,Type=Integer,Description=""Raw data (sum of squared MQ and total depth) for improved RMS Mapping Quality calculation. Incompatible with deprecated RAW_MQ for; mulation."">; ##INFO=<ID=ReadPosRankSum,Number=1,Type=Float,Description=""Z-score from Wilcoxon rank sum test of Alt vs. Ref read position bias"">. ###and this is the tag for gatk 4.2; ##fileformat=VCFv4.2; ##ALT=<ID=NON_REF,Description=""Represents any possible alternative allele at this location"">; ##FORMAT=<ID=AD,Number=R,Type=Integer,Description=""Allelic depths for the ref and alt alleles in the order listed"">; ##FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Read depth"">; ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=""Genotype quality"">; ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; ##FORMAT=<ID=MIN_DP,Number=1,Type=Integer,Description=""Minimum DP observed within the GVCF block"">; ##FORMAT=<ID=PGT,Number=1,Type=String,Description=""Physical phasing haplotype information, describing how the alternate alleles are phased in relation to one another"">; ##FORMAT=<ID=PID,Number=1,Type=String,Description=""Physical phasing ID information, where each unique ID within a given sample (but not across samples) connects records within a phasing gr; oup"">; ##FORMAT=<ID=PL,Number=G,Type=Integer,Description=""The phred-scaled genotype likelihoods rounded to the closest integer"">; ##FORMAT=<ID=SB,Number=4,Type=Integer,Description=""Per-sample component statistics which comprise the Fisher's Exact Test to detect strand bias"">; ##INFO=<ID=BaseQRankSum,Number=1,Type=Float,Description=""Z-score from Wilcoxon rank sum test of Alt Vs. Ref base qualities"">; ##INFO=<ID=ClippingRankSum,Number=1,Type=Float,Description=""Z-score From Wilcoxon rank sum test of Alt vs. Ref number of hard clipped bases"">; ##INFO=<ID=DP,Number=1,Type=Integer,Description=""Combined depth across samples"">; ##INFO=<ID=END,Number=1,Type=Integer,Description=""Stop position of the interval"">; ##INFO=<ID=ExcessHet,Number=1,Type=Float,Des",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8574#issuecomment-1793390789:16363,detect,detect,16363,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8574#issuecomment-1793390789,1,['detect'],['detect']
Safety,"Rationale for engine changes:; This tool opens a large number of feature files (TSVs, not VariantContexts) and iterates over them simultaneously. No querying, just a single pass through each.; Issue 1: When a feature file lives in the cloud, it takes unacceptably long (several seconds, typically) to initialize it. A few seconds doesn't seem like a long time, but when there are large numbers of feature files to open, it adds up. This is caused by a large number of codecs (mostly the vcf-processing codecs) opening and reading the first few bytes of the file in the canDecode method. To avoid this I've reversed the order in which we test each codec, checking first if it produces the correct subtype of Feature, and only then calling canDecode. If you don't know what specific subtype you need, you can just ask for any Feature by passing Feature.class. It's much faster that way.; Issue 2: Each open feature source soaks up a huge amount of memory. That's because text-based feature reading is optimized for VCFs, which can have enormously long lines. So huge buffers are allocated. The problem is compounded for cloud-based feature files for which we allocate a large cloud prefetch buffer. (Though that feature can be turned off, which helps a little.) But the biggest memory hog is the TabixReader, which always reads in the index, regardless of whether it's used or not. Tabix indices are very large. To avoid this, I've created a smaller, simpler FeatureReader subclass called a TextFeatureReader that loads the index only when necessary. The revisions allow the new tool to run using an order of magnitude less memory. Faster, too.; Issue 3: The code in FeatureDataSource that creates a FeatureReader is brittle, and tests for various subclasses. To allow use of the new TextFeatureReader, I added a FeatureReaderFactory interface that allows one to ask the codec for an appropriate FeatureReader.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8031#issuecomment-1284340770:590,avoid,avoid,590,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8031#issuecomment-1284340770,2,['avoid'],['avoid']
Safety,Reading in the contents and doing any sort of operation on them. I believe @droazen is aware of why this occurs in GATK3 -- he explained it to me a long time ago. At the time we put it down as a known limitation but after discussing some challenges associated with Hg38 with @ebanks it sounds like it's going to be important to avoid or mitigate the problem in GATK4.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1688#issuecomment-206328891:328,avoid,avoid,328,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1688#issuecomment-206328891,1,['avoid'],['avoid']
Safety,"Same here, @jhl667 - good to get additional confirmation. May I ask whether you used the same gold-standard call set or another one, and whether yours was WGS or WES? . I’m linking @droazen here as well since he acted as the release manager of the affected releases. I think it would be good to get clarity soon, since probably tens of thousands of clinical samples have been processed with the affected versions in the last two years, and a reduction in precision of 10-20% (absolute) may have been relevant for clinical decisions in at least some of these cases. Also, I know how hard it is to avoid such things in what is essentially research software (and such a great one to boot), so this is not about blaming anyone but about fixing the root cause as soon as possible. If it turns out that the issue only affects this particular reference call set and no patients (for some arcane reason), then all the better in my view.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1171653305:596,avoid,avoid,596,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1171653305,1,['avoid'],['avoid']
Safety,"Samir;; I'm not entirely sure about the fix, but based on where I think it's coming from you'll want to work around the problem by avoiding at least 10bp towards the start/end of a chromosome. That's the amount of sequence context it's filling around the variant. Hope this helps for your problem case.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5130#issuecomment-416777409:131,avoid,avoiding,131,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5130#issuecomment-416777409,1,['avoid'],['avoiding']
Safety,"Second-pass review complete, back to @meganshand. Main recommendation is to include the `presorted = false` fix in this branch to avoid checking in a disabled test.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1853#issuecomment-223976572:130,avoid,avoid,130,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1853#issuecomment-223976572,1,['avoid'],['avoid']
Safety,"See #6221. On Fri, Oct 18, 2019 at 9:50 AM droazen <notifications@github.com> wrote:. > Thanks for the clarification @ldgauthier <https://github.com/ldgauthier>.; > Maybe we should add a note to that effect in the docs for the PGT; > annotation to avoid future confusion (eg., the doc string for PGT in; > GATKVCFHeaderLines)? If you submit a one-line PR, I'll approve it :); >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/6220?email_source=notifications&email_token=ABSGC5F5F3TNBJPGRPPGSJ3QPG5KVA5CNFSM4JCHKRSKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEBURCTQ#issuecomment-543756622>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ABSGC5AQ3YAHJTV5TEASZCDQPG5KVANCNFSM4JCHKRSA>; > .; >. -- ; Laura Doyle Gauthier, Ph.D. (she/her); Associate Director, Germline Methods; Data Sciences Platform; gauthier@broadinstitute.org; Broad Institute of MIT & Harvard; 320 Charles St.; Cambridge MA 0214",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6220#issuecomment-543803156:248,avoid,avoid,248,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6220#issuecomment-543803156,1,['avoid'],['avoid']
Safety,Seems like something like https://github.com/broadinstitute/gatk/issues/4794 could be avoided if we rewrote this. It seems like a pretty simple rewrite too...,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4535#issuecomment-391044481:86,avoid,avoided,86,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4535#issuecomment-391044481,1,['avoid'],['avoided']
Safety,"Short answer: No, it's not using any information from the genotypes at all. You should get the same results with or without genotypes. Long technical answer: Both of these issues are related to the fact that when combining multiple possible variants at a site (i.e. variants with different alleles from the GGA `-alleles` input as well as any alleles detected in the input sample data), we often need to do remapping of the reference and alternate alleles so that they can all be described in the same context. In the places that caused these bugs we were trying to do this remapping to the alleles directly, but the fact that the variants had genotypes which referred to the original set of non-remapped alleles meant that those original alleles were still kept around and caused an error in the downstream code that tries to put together the final variant record to be emitted at the site. So it was just a case of links within the data structures that hold the genotypes interfering with the intended result, which should be based only on the input set of alternate alleles.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5355#issuecomment-433468224:351,detect,detected,351,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5355#issuecomment-433468224,1,['detect'],['detected']
Safety,Should I wait for a bug fix ? Can I do something to avoid the error? Thanks.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-388403388:52,avoid,avoid,52,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-388403388,1,['avoid'],['avoid']
Safety,"Should this be included here or should we fix it in htsjdk and not do a sanity check here, @cmnbroad ?; This had been addressed in the https://github.com/samtools/htsjdk/pull/835 PR which is now closed",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7069#issuecomment-773392506:72,sanity check,sanity check,72,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7069#issuecomment-773392506,1,['sanity check'],['sanity check']
Safety,"Since the code isn't reviewed yet I took the liberty of adding one much push with a single change: adding a ""synchronized"" to protect against a potential data race in `SeekableByteChannelPrefetcher`. The contract for `ReadableByteChannel` (which this implements) requires `read` to be thread-safe. I don't know whether this was the cause of #2516. I haven't been able to reproduce it since, but then again even before this change it wasn't easy to trigger.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2506#issuecomment-290811886:292,safe,safe,292,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2506#issuecomment-290811886,1,['safe'],['safe']
Safety,"Since this touches a lot of files, I'll categorize changes here:; 1. Convenience Script Changes; - bug fixes for running on Linux; scripts/sv/manage_sv_pipeline.sh; - detect number of preemptible workers to choose NUM_EXECUTORS correctly; scripts/sv/run_whole_pipeline.sh; - allow sanity_checks.sh to run from outside GATK_DIR, exit correctly on error; scripts/sv/sanity_checks.sh. 2. Minor changes to existing utils to support new filter; - allow construction of IntHistogram.CDF from known cdfFractions and nCounts; src/main/java/org/broadinstitute/hellbender/tools/spark/utils/IntHistogram.java; - in constructor, coverage is passed as a float; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/ReadMetadata.java; - add xgboost maven repository for gradle; build.gradle. 3. Significant changes to existing code to support/invoke new filter; - add arguments for XGBoostEvidenceFilter, changes for scaling density filter by coverage; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/StructuralVariationDiscoveryArgumentCollection.java; - replace calls to BreakpointDensityFilter with calls to BreakpointFilterFactory; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/FindBreakpointEvidenceSpark.java; - input coverage-scaled thresholds, convert to absolute internally. Allow thresholds to be double instead of int; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilter.java; - getter functions added to calculate properties for XGBoostEvidenceFilter. Also fromStringRep() and helper constructors added for testing; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointEvidence.java; - updates to tests reflecting changes to these interfaces; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilterTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/FindBreakpointEvidenceSparkUnitTest.java; src/test/java/org/broadinstitute/hel",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477:167,detect,detect,167,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477,1,['detect'],['detect']
Safety,"So at the risk of sounding thick, what exact path do I use in my command when I'm calling the docker? Imagine I'm a six year old who doesn't understand what is the internal structure of the GATK docker image or what ""gatk-launch is in the standard docker image in /gatk"" means. Note that right now to call the jar I use /root/gatk.jar",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3334#issuecomment-317457895:10,risk,risk,10,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3334#issuecomment-317457895,1,['risk'],['risk']
Safety,"So here's what you should do to clean up your workspace:; - first, delete any fragment that doesn't have the full complement of 39 files that it is supposed to have; - next, if there are any duplicate fragments delete all but one of the duplicates. it doesn't matter which one you delete, the fragments have identical data (make sure this is the case by checking md5sums for ALL files in the duplicate fragments). As an aside, this step is optional. That is to say, your workspace is still legal if you leave the duplicate fragments, but you MUST remove the incomplete ones. Not removing the duplicates will just take up extra space. And it'll make the last verification/sanity check a little trickier; - If you've deleted duplicate and incomplete fragments, you should now have the same number of fragments in all your contig folders. If you have fewer fragments in a contig folder, then it is likely that some import process may have failed midway. If you have too many fragments...some other weird corruption may have happened which may be hindering the redundancy check.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6910#issuecomment-722722209:671,sanity check,sanity check,671,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6910#issuecomment-722722209,2,"['redund', 'sanity check']","['redundancy', 'sanity check']"
Safety,"So there may be some unfortunate performance implications with some of these changes. Utils.nonNull(value, message) and it's compatriots will always compute the message even if the error condition is not met. Using any message which isn't a constant will generate garbage in the form of strings. In most cases this isn't a problem, but it is not ideal if it's placed in a tight loop. . We could offset the problem by adding a family of Utils functions that take a lambda String producer instead of a string itself, this would allow the message to be computed only when the error condition is triggered avoiding garbage creation.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1979#issuecomment-231126857:602,avoid,avoiding,602,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1979#issuecomment-231126857,1,['avoid'],['avoiding']
Safety,"Sorry @magicDGS -- since this is failing tests and needs a rebase/review, and we're extremely pressed for time this morning, this is going to have to wait until the next point release. But don't worry, I think I can safely say that we plan to do point releases frequently -- I'd expect the first one within a couple of weeks.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3998#issuecomment-356314345:216,safe,safely,216,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3998#issuecomment-356314345,1,['safe'],['safely']
Safety,"Sorry for the confusion, but assuming the iterator is iterating through an ordered container is not safe, I think. What about this:. ```java; public static <T> Stream<T> stream(final Iterator<T> iterator) {; return stream(() -> iterator);; }; ```. It is reusing the version working with `Iterable`'s.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2330#issuecomment-270465136:100,safe,safe,100,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2330#issuecomment-270465136,1,['safe'],['safe']
Safety,"Sorry for the delayed response @cmatKhan. The fix here is a little complicated because its a confluence of expected behaviors adding up to this. . Specifically in `-ERC GVCF` mode in HaplotypeCaller we merge adjacent regions with similar `GQ` scores but within those merged reference blocks we track the DP (mean dp) and the MIN_DP. When we go to genotype in GenotypeGVCFs we fall back to Min_DP when reporting the DP since it must be at least that high at the sight in question. For Diploid samples and with the default reference blocking we use, this works fine and for most of the range up to 30+ bases you are able to recover a pretty close approximation of what your DP is in most cases. However for Haploid data the assumptions at play mean that the `GQ` gets very high and starts to max out the field (99) so you end up with extremely long reference confidence blocks with very high confidence and a min_DP of some low value (in your case for the data you shared with us of 4, which is the threshold for hitting GQ=90 in the haploid model). This is also part of why adjusting the intervals for traversal was relevant, as if they were too long they were pulling in bases 1000+ bp away that only have 4fold coverage with reads and torpedoing the reported DP. So far nothing seems to obviously be bugged (we don't test/use haploid calling mode very often so its less well tested and this sort of issue is not surprising). We might be over-confident about reporting `GQ` for haploid reference blocks (though mathematically its sound that you only need a few reference observations to be sure about the ref call). Short of adjusting that model drastically there will always be some threshold where ""after X fold coverage of reads everything is merged into a big adjacent ref block"". Unfortunately with block merging on we never expect the DP to be exact (since it should vary over the block and its expensive to store that information exactly) but the issue is much more pronounced in your haploid s",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8943#issuecomment-2318660429:622,recover,recover,622,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8943#issuecomment-2318660429,1,['recover'],['recover']
Safety,"Sorry. I get your thinking, but I think it would be cleaner to pass the param to each of those two classes. It cleans up the dependency tree -- those two classes don't depend on any of the ReadMetadata state but for that one param -- and avoids having to have the SVReadFilter be both Java and Kryo serializable. I'd prefer it, but won't insist upon it.; Adding the filter values to the read metadata output file makes sense, though. Doesn't need to be done now.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3469#issuecomment-324467678:238,avoid,avoids,238,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3469#issuecomment-324467678,1,['avoid'],['avoids']
Safety,"Step #1: Port as much of `SequenceDictionaryUtils` as needed from the old GATK (remove garbage like validation exclusions, fix terrible things like non-canonical human order check if possible, if not create tickets). Port unit tests as well. Step #2: Hook up sequence dictionary validation to hellbender engine, and add integration tests to prove that dictionary incompatibilities are detected. Best place to hook this up is probably `GATKTool.onStartup()`, since it manages all the engine-level inputs for tools.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/101#issuecomment-113275417:385,detect,detected,385,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/101#issuecomment-113275417,1,['detect'],['detected']
Safety,"Summary information about this bug:. This issue affects GATK versions 4.3.0.0 through 4.5.0.0, and is fixed in GATK 4.6.0.0. The PR with the fix is: https://github.com/broadinstitute/gatk/pull/8900. This issue also affects Picard versions 2.27.3 through 3.1.1, and is fixed in Picard 3.2.0. This bug is triggered when writing a CRAM file using one of the affected GATK/Picard versions, and both of the following conditions are met:; ; * At least one read is mapped to the very first base of a reference contig; * The file contains more than one CRAM container (10,000 reads) with reads mapped to that same reference contig. When both of these conditions are met, the resulting CRAM file may have corrupt containers associated with that contig containing reads with an incorrect sequence. . Since many common references such as hg38 have N's at the very beginning of the autosomes and X/Y, many pipelines will not be affected by this bug. However, users of a telomere-to-telomere reference, users doing mitochondrial calling, and users with reads aligned to the alt sequences will want to scan their CRAM files for possible corruption. The other mitigating circumstance is that when a CRAM is affected, the signal will be overwhelmingly obvious, with the mismatch rate typically jumping from sub-1% to 80-90% for the affected regions, making it likely to be caught by standard QC processes. A CRAM scanning tool called `CRAMIssue8768Detector` that can detect whether a particular CRAM file is affected by this bug was added in https://github.com/broadinstitute/gatk/pull/8819, and was released as part of GATK 4.6.0.0",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8768#issuecomment-2198315437:1451,detect,detect,1451,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8768#issuecomment-2198315437,1,['detect'],['detect']
Safety,"Thank you @mehrzads for your contribution. It would appear that this optimization is aimed to incorporate the knowledge that we could never possibly visit a given vertex more than K times in the first K best paths. Since the graphs are prone to exponential expansion of paths this seems like an important safeguard against this exponential expansion of the graph. . Looking at the code and the algorithm behavior it is intending to copy I see that there is a degenerate case in the current code that can cause the results to be order dependent. My belief is that this code can fall over by virtue of the fact that we refuse to make new incoming edges to a given vertex if there are already too many incoming edges for that vertex. Unfortunately this heuristic doesn't strike me as being valid, because those incoming edges can have any weight, including very high weights because they are bad paths through the graph that we created at a previous step. . I think a more correct optimization would be to limit the number of edges we create LEAVING a given vertex. The logic for this is that while we may not necessarily see all of the incoming edges in the correct weight order we will necessarily see all of the leaving edges in the correct order because those paths are pulled off of the priority queue in the correct order. Thus we can safely ignore any additional paths we see leaving a given edge because by construction as they would necessarily have at least one path that is cheaper than all of the paths leaving the current node.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5907#issuecomment-494105417:305,safe,safeguard,305,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5907#issuecomment-494105417,2,['safe'],"['safeguard', 'safely']"
Safety,"Thank you @mwalker174 . The input bamfile is about 7 GB. If no `--bamPartitionSize` is specified, the job would stuck at the first step `collect at ReadsSparkSource.java:220`, until we killed it. So I tried `--bamPartitionSize 4000000`, and it went through, but the Spark web interface showed errors in `sortByKey` steps:; ![sparkjob](https://user-images.githubusercontent.com/812850/27811313-9000019c-6097-11e7-82ac-aac557be31db.PNG).; And the program failed eventually:; ```; 18:24:57.885 INFO BwaAndMarkDuplicatesPipelineSpark - Shutting down engine; [July 3, 2017 6:24:57 PM CST] org.broadinstitute.hellbender.tools.spark.pipelines.BwaAndMarkDuplicatesPipelineSpark done. Elapsed time: 269.29 minutes.; Runtime.totalMemory()=4172283904; org.apache.spark.SparkException: Job aborted due to stage failure: Task 607 in stage 3.0 failed 4 times, most recent failure: Lost task 607.13 in stage 3.0 (TID 14832, 12.9.68.0, executor 24): ExecutorLostFailure (executor 24 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 169939 ms; Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apa",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312758363:778,abort,aborted,778,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312758363,1,['abort'],['aborted']
Safety,"Thank you for replying!; I have parallelized the GATK4 Mutect2 using thread pools in Java. I tested the parallelized GATK4 Mutect2 using a WGS data with control. The result came out that, about 0.6% variants were different from the original results. I found that the difference was caused by the random number generator in ReservoirDownsampler. The order of the input intervals after parallelism were different from the original, so the random numbers generated for each position with redundant reads were possibly different. Is there any solutions for this problem?; Thank you very much!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4325#issuecomment-382586592:485,redund,redundant,485,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4325#issuecomment-382586592,1,['redund'],['redundant']
Safety,"Thank you for taking a look into this. I followed recommendation of @gbrandt6 and reduced the --pruning-lod-threshold but this call is still unable to make it to the output of Mutect2. I tried different thresholds from 1.3, 0.7, 0.5 and even 0.1 but it did not lead to any difference in detecting this call.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7232#issuecomment-829649061:287,detect,detecting,287,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7232#issuecomment-829649061,1,['detect'],['detecting']
Safety,"Thank you for the speedy review @davidbenjamin. I agree with you that the obvious place to trim the alleles is in `removeAltAllelesIfTooManyGenotypes(ploidy, alleleMapper, mergedVC)` as it is the place where we actually edit the output. Indeed my first attempt at this fix was to make that change. Unfortunately, because the `readAlleleLikelihoods` object is constructed with the un-trimmed alleles in the `alleleMapper` was causing failures because the Liklihoods object would have mismatching alleles. To fix `removeAltAllelesIfTooManyGenotypes(ploidy, alleleMapper, mergedVC)` we would have to edit the alleleMapper object, which would be difficult given that I would prefer to just use the allele trimming library object. . Another proposal would have been to just hold onto the `mergedVC` object before we cull the extra alleles and then just compare the alleles at the end. Unfortunately due to engine code optimizations we have enabled an unsafe allele list copy for these alleles in the HaplotypeCaller (to save ourselves the cost of allocating dozens of identical ArrayLists to store Haplotypes every time we use the VariantContextBuilder). . To clarify, it is possible to move the check to the right place its likely to force me to write a non-library implementation of the trimming code that tracks what edits it made and I was trying to avoid doing that.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6044#issuecomment-512355693:946,unsafe,unsafe,946,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6044#issuecomment-512355693,2,"['avoid', 'unsafe']","['avoid', 'unsafe']"
Safety,"Thank you so much, I will do that. Sincerely,; Emily. From: ldgauthier ***@***.***>; Sent: Monday, March 28, 2022 2:39 PM; To: broadinstitute/gatk ***@***.***>; Cc: Emily Elizabeth Puckett (puckett3) ***@***.***>; Mention ***@***.***>; Subject: Re: [broadinstitute/gatk] CombineGVCFs: ERROR input alleles must contain <NON_REF> (Issue #7737). CAUTION: This email originated from outside of the organization. Do not click links or open attachments unless you recognize the sender and trust the content is safe. If I'm reading the process correctly, I don't actually think this should work. CombineGVCFs is specifically for combining GVCFs and it expects GVCFs to have <NON_REF> alleles. If you've already run the data through GenotypeGVCFs then you can't use CombineGVCFs again because the <NON_REF> likelihoods have been applied and those alleles are gone. The vcfcombine tool from bcftools is quite fast if all you want to do is join the samples together. -; Reply to this email directly, view it on GitHub<https://nam02.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fbroadinstitute%2Fgatk%2Fissues%2F7737%23issuecomment-1081062021&data=04%7C01%7CEmily.Puckett%40memphis.edu%7C51db6aa9f41b483e1ce408da10f2aa5d%7Cae145aeacdb2446ab05a7858dde5ddba%7C0%7C0%7C637840931685525269%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000&sdata=Pxg8joQfE51l5e3cUUbKA9bQEYDZjp0AxdX0aqDG1MY%3D&reserved=0>, or unsubscribe<https://nam02.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FALDFEHAXSKZ7YHSFGISLPUTVCIDGZANCNFSM5RZSK5PA&data=04%7C01%7CEmily.Puckett%40memphis.edu%7C51db6aa9f41b483e1ce408da10f2aa5d%7Cae145aeacdb2446ab05a7858dde5ddba%7C0%7C0%7C637840931685525269%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000&sdata=6Dkb6rbHDZpS05bYUHhlIRHJitgVtR%2FPB5rNHHFMg%2FQ%3D&reserved=0>.; You are receiving this because you were mentioned.Message I",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7737#issuecomment-1082170127:504,safe,safe,504,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7737#issuecomment-1082170127,1,['safe'],['safe']
Safety,"Thanks @lbergelson - nice to meet you too. Sorry for the delay here. I had to set up gsutils on my system and am having gdb issues. . Submitting `sudo gdb /nfsdata-tmp/tools/gatk /home/bduser/mepowers/core.114856` I get back . ```; Missing separate debuginfo for the main executable file; Try: yum --enablerepo='*debug*' install /usr/lib/debug/.build-id/6c/../../../jvm/java-1.8.0-openjdk-1.8.0.111-1.b15.el7_2.x86_64/bin/java; Core was generated by `java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samt'.; Program terminated with signal 6, Aborted.; ```; I did try the yum --enablerepo, but it am getting the same error. . Any quick workarounds? Thanks in advance for the help. Will try again on Monday.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-466588771:568,Abort,Aborted,568,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-466588771,1,['Abort'],['Aborted']
Safety,"Thanks @ldgauthier. @gbrandt6 I’d appreciate it if you want to take a look, but I might ask if you can do it by Friday afternoon—I’m out after then through all of next week. Would like to merge before I head out to avoid any more rebasing and/or updating of exact-match tests. Happy to look at any changes to the docs you might make in a subsequent PR, though!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7394#issuecomment-972003949:215,avoid,avoid,215,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7394#issuecomment-972003949,1,['avoid'],['avoid']
Safety,"Thanks @ruqianl, you may want to read through the comments at https://github.com/broadinstitute/gatk/issues/6235 and the corresponding PR https://github.com/broadinstitute/gatk/pull/6244, which both address this issue. See also the following bit of documentation added in that PR:. > Advanced users may wish to set the THEANO_FLAGS environment variable to override the GATK theano configuration. For example, by running THEANO_FLAGS=""base_compiledir=PATH/TO/BASE_COMPILEDIR"" gatk GermlineCNVCaller ..., users can specify the theano compilation directory (which is set to $HOME/.theano by default). See theano documentation at https://theano-pymc.readthedocs.io/en/latest/library/config.html. So you can specify a unique compilation directory for each of your jobs to avoid the compilelock, e.g., `THEANO_FLAGS=""base_compiledir=PATH/TO/BASE_COMPILEDIR/FOR/JOB/0"" gatk GermlineCNVCaller ...`, `THEANO_FLAGS=""base_compiledir=PATH/TO/BASE_COMPILEDIR/FOR/JOB/1"" gatk GermlineCNVCaller ...`, etc. Alternatively, you can increase `config.compile.timeout` as discussed in those comments.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7411#issuecomment-905070899:767,avoid,avoid,767,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7411#issuecomment-905070899,2,"['avoid', 'timeout']","['avoid', 'timeout']"
Safety,"Thanks a lot for all your feedback about this @lbergelson and @droazen. From my side this could be close now, although it may be useful to have some of this information in the Wiki to avoid confusion. Thank you very much again!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2345#issuecomment-273732039:184,avoid,avoid,184,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2345#issuecomment-273732039,1,['avoid'],['avoid']
Safety,Thanks everyone for working this out. Is there any chance we can detect this problem on load and fail rather than silently giving bad output?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3429#issuecomment-324390105:65,detect,detect,65,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3429#issuecomment-324390105,1,['detect'],['detect']
Safety,"Thanks for adding this! Let me discuss further with @mwalker174 to understand the need and typical use cases (e.g., combining fixed-grid bins) to make sure we don't run into any gotchas downstream. I'll try to review by EOD, but in the meantime, you might want to address a few issues I see at first glance:. 1) Correct the name of the tool (PreprocessIntervals) in the commit message and description.; 2) Add descriptions of the new parameters to the tool Javadoc.; 3) Amend the corresponding WDL task and expose the new parameters in all relevant germline and somatic WDLs.; 4) We should be sure to update the relevant documentation for all germline and somatic WDLs, which emphasizes how PreprocessIntervals should be run differently for WES and WGS, if we plan on changing the default behavior of the tool in the future.; 5) Tests are failing due to a compilation warning about a redundant cast to int.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5701#issuecomment-465978387:884,redund,redundant,884,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5701#issuecomment-465978387,1,['redund'],['redundant']
Safety,"Thanks for helping me to understand why you didn't mark the metadata. This may seem like quibbling, but I'd suggest that we mark the metadata with a comment character, and let the pandas/R users remove it. They'll notice if they forget to do that, because the columns won't be named as expected, and they'll have to fix it up. Whereas the risk for automated programs is that they'll simply delete the first row, which might be real data if the file has been reordered for some reason, or if the tool implementing the standard is non-compliant. The resulting bugs will be subtle, and might easily go undetected. Building in behavior to delete lines starting with ""CHROM\t"" seems odd and fraught with peril in a way that building in behavior to strip comments, doesn't. That's my take, anyway.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-480956381:339,risk,risk,339,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-480956381,1,['risk'],['risk']
Safety,"Thanks for making this change! I might include more detail with the note (""Substantially improves results on FFPE samples""), for posterity---it's probably true, but we can't really say anything definitive with only N=1 and without the cross-validation procedure I mentioned on Slack. That is, the higher degree of denoising might just be an artifact of effectively removing more PCs with GC-bias correction (since I'm assuming the same number of PCs were explicitly removed in both cases), but it's possible that removing the optimal number of PCs without GC-bias correction could achieve a better result. Since our correction procedure is relatively naive, there may also be some dependence on bin size. However, I think it's probably not worth a detailed analysis, and that it's generally safe to enable correction by default.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5966#issuecomment-496933928:791,safe,safe,791,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5966#issuecomment-496933928,1,['safe'],['safe']
Safety,"Thanks for taking a look @davidadamsphd. Regarding the singleton pattern, by the looks of things it was introduced for testing with the intention of avoiding the 10+ second overhead of creating the reference table, however `RefAPISourceUnitTest` still builds the table in `queryReferenceAPI` for each test. We could avoid that by using TestNG's `@BeforeClass` to create the `@RefAPISource` just once, as I mentioned above. Otherwise, the API is not called in tests, since a mock `ReferenceDataflowSource` is used. Let's work out a common references interface - @jean-philippe-martin and @akiezun, let me know what you prefer here. BTW the Travis test is failing as I pushed this to my own branch rather than one on hellbender. I'll push to a new one when this gets closer to being ready.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/804#issuecomment-130604350:149,avoid,avoiding,149,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/804#issuecomment-130604350,2,['avoid'],"['avoid', 'avoiding']"
Safety,Thanks for that explanation. I'd rather set it to a safer value like 100 as a default - @asmirnov239 do you have any sense of how much this affects run time?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5754#issuecomment-922018024:52,safe,safer,52,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5754#issuecomment-922018024,1,['safe'],['safer']
Safety,"Thanks for that info and for sharing the files, @asmirnov239. I suspect that there are essentially two types of bins: ""nice"" and ""not so nice"". The sampling noise in the former is determined by Poisson observation noise, whereas that in the latter is determined by uncertainty in the bias posteriors. This is a bit hard to see in the plots above, and even in this version where I tried to adjust the point size and alpha:. ![image](https://user-images.githubusercontent.com/11076296/137733810-16a79ea9-ea7b-47cc-a42f-40130a949015.png). However, plotting a measure of the difference in the dCRs (from 20 and 200 posterior samples) vs. the dCR is more suggestive:. ![image](https://user-images.githubusercontent.com/11076296/137734587-1b9f6551-74b2-4097-a02c-f51d7341251c.png). As are the dCR histograms:. ![image](https://user-images.githubusercontent.com/11076296/137733867-ce0f5573-a5cc-412c-9060-56fbb09d1ef0.png). I would guess that the nice spike around CR ~ 2 and the fatter base extending up to dCR ~ 100 are distinct populations of bins. So the punchline would be that differences at high dCR are probably just noise within the noise. For ""nice"" bins at dCR ~ few, the sampling noise looks to be <1%. Not really sure what's going on at very high dCR, but I think it's safe to say that these are ""not so nice"" bins!. I've seen this pattern in other WES cohorts when plotting the posterior means vs. std devs for the biases; tried to dig up the plots on Slack, but I can't find them at the moment. Perhaps something along those lines might be worth visualizing in your model-criticism notebooks, if you don't already?. Again, hard to say this is indeed the case from the dCRs alone, but if so, it might be worth baking this sort of mixture into future versions of the model or coming up with other strategies to deal with such bins.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5754#issuecomment-945731946:1275,safe,safe,1275,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5754#issuecomment-945731946,1,['safe'],['safe']
Safety,"Thanks for the clarification @ldgauthier. Maybe we should add a note to that effect in the docs for the PGT annotation to avoid future confusion (eg., the doc string for PGT in `GATKVCFHeaderLines`)? If you submit a one-line PR, I'll approve it :)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6220#issuecomment-543756622:122,avoid,avoid,122,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6220#issuecomment-543756622,1,['avoid'],['avoid']
Safety,"Thanks for the suggestions! The SV jobs are all running fine with no hanging after increasing the memory. The commandline below completed on 100 30x crams without any issues. . ```; gatk --java-options ""-Djava.io.tmpdir=tmp"" StructuralVariationDiscoveryPipelineSpark \; -R $REF \; --aligner-index-image GRCh38_full_analysis_set_plus_decoy_hla.fa.img \; --kmers-to-ignore GRCh38_ignored_kmers.txt \; --contig-sam-file hdfs:///user/farrell/adni/sv/$SAMPLE.contig-sam-file\; -I $CRAM_DIR/$SAMPLE.cram \; -O hdfs:///user/farrell/$CENTER/sv/$SAMPLE.sv.vcf \; -- \; --spark-runner SPARK --spark-master yarn --deploy-mode cluster \; --executor-memory 60G\; --driver-memory 40g\; --num-executors 12\; --executor-cores 4\; --files $REF.img,GRCh38_ignored_kmers.txt \; --name ""$SAMPLE"" --conf spark.yarn.submit.waitAppCompletion=false\; --conf spark.yarn.executor.memoryOverhead=5000 \; --conf spark.network.timeout=600 \; --conf spark.executor.heartbeatInterval=120. ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4635#issuecomment-381441151:898,timeout,timeout,898,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4635#issuecomment-381441151,1,['timeout'],['timeout']
Safety,"Thanks for the thoughts. Singularity is definitely awesome and I'm hoping to support it as an alternative choice to Docker for local HPC clusters where we won't require equivalent root permissions to run. So it helps avoid some of the potential external permission errors by creating a potentially cleaner path to running. Unfortunately it doesn't deal with the underlying issue of needing to map users inside of the containers so that Spark is happy with them. Having something more lightweight than needing user updates in the internal `/etc/passwd` would also help with potential issues on other container enginer (Singularity, rkt).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4626#issuecomment-381642529:217,avoid,avoid,217,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4626#issuecomment-381642529,1,['avoid'],['avoid']
Safety,"Thanks for your feedback, @cmnbroad. In my case, I think that `IntegrationTestSpec` is a good way of avoid complicated code to test tool results, but it is true that it have some problems (one that I had was the usage for testing programs where the outputs are determined by a prefix in the command line, but with different suffixes). I think, from the API user point of view, that a class like `IntegrationTestSpec` to facilitate program output testing (including user exceptions) will be nice for developing purposes. Nevertheless, this is just a convenience that I asked for here, but I can try to solve the issues with the `BaseTest` instead. By the way, I would love to have this interface in GATK at least for now, because several of my tools rely on the `IntegrationTestSpecs` for development...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2122#issuecomment-243124889:101,avoid,avoid,101,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2122#issuecomment-243124889,1,['avoid'],['avoid']
Safety,Thanks for your suggestion @wir963. The `--min-score-identity` and `--host-min-identity` parameters can be used to tune your desired specificity/sensitivity for microbe read detection. The default settings should guarantee that the identified microbial reads have better alignments to the microbe reference than host.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6819#issuecomment-705027510:174,detect,detection,174,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6819#issuecomment-705027510,1,['detect'],['detection']
Safety,"Thanks!. Okay I will make sure to do that in the future. Thanks for the help!. On Tue, Aug 14, 2018 at 11:30 AM, Louis Bergelson <notifications@github.com>; wrote:. > @kvinter1 <https://github.com/kvinter1> In future, it's a good idea to; > wait for tests to pass before merging, otherwise you risk the potential; > penalty of having to buy the team beer if test fail once it's in master.; > Doc changes are pretty low risk, but you never know.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/pull/5104#issuecomment-412913465>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AoMkWKCRyhFXvfzSUI7C26_4qRZPivIoks5uQu0NgaJpZM4V8n05>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5104#issuecomment-412917430:294,risk,risk,294,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5104#issuecomment-412917430,2,['risk'],['risk']
Safety,"Thanks, Tom. But I'm afraid that that's only one piece of the puzzle. For one thing, `ctx.sc().executorMemory()` doesn't give the right answer in local mode. (Workaround: `ctx.sc().getExecutorMemoryStatus()` does give a sensible answer in local mode, at the small cost of getting redundant info for each executor.) However, to know how much is available for the execution of each task, one would have to divide by the number of cores per executor. Seems like `ctx.getConf().get(""spark.executor.cores"")` might do the job, but, at least in local mode, it doesn't: that key is not set. Any further insights?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1456#issuecomment-178125240:280,redund,redundant,280,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1456#issuecomment-178125240,1,['redund'],['redundant']
Safety,"That example data from the tutorial is good @sooheelee, but maybe it could be reduced in size to avoid adding it to the large file directory? It will be nice to include that example in the `RealignerTargetCreator` PR (#3112)...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3104#issuecomment-319627989:97,avoid,avoid,97,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3104#issuecomment-319627989,1,['avoid'],['avoid']
Safety,"That is a separate matter altogether from both 1) unifying the allele-count collection tools, and 2) standardizing the format of tabular data. The most appropriate place for integration of Mutect2 SNV calls would be as input to the tumor-heterogeneity tool (along with the ModelSegments output) further downstream. This is because it is unlikely that including the SNVs as input to ModelSegments would significantly improve either segmentation or modeling there. If the allele-count collection tools are unified, I think that the only redundant work done across both pipelines would be the calling of hets from the pileups, which is extremely cheap. However, we should certainly also unify the code to do this (which I've spoken to @davidbenjamin about as well).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-386734926:535,redund,redundant,535,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-386734926,1,['redund'],['redundant']
Safety,"That's a good point--the documentation should be updated. I think it's safe to do so here before I updated actual workspace itself, although technically it has the potential to create a small window in which the documentation talks about features that are not there. I'll update the ticket in Jira to explicitly mention updating the documentation as well, as it should be among the AC.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8010#issuecomment-1238401881:71,safe,safe,71,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8010#issuecomment-1238401881,1,['safe'],['safe']
Safety,"The ""WDL test"" CI failures are not related to the changes in this PR, please see this [sanity check PR](https://github.com/broadinstitute/gatk/pull/8369) which is also currently aflame.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8362#issuecomment-1599633824:87,sanity check,sanity check,87,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8362#issuecomment-1599633824,1,['sanity check'],['sanity check']
Safety,"The DREAM SMC-RNA does not pertain to SNv and indel calling. Rather, it's only for fusion and isoform prediction.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5427#issuecomment-592086085:102,predict,prediction,102,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5427#issuecomment-592086085,1,['predict'],['prediction']
Safety,"The GATK VCF header issue causing the underlying problem was fixed in #3351, so a new release of GATK4 should work correctly and avoid losing variants during GenomicsDB import/output for joint calling. I agree with Louis that failing with an error would be better than the current silent failures in case of any future issues. Thank you all again for the help debugging this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3429#issuecomment-325024708:129,avoid,avoid,129,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3429#issuecomment-325024708,1,['avoid'],['avoid']
Safety,The GATK's reference requirement makes it difficult to develop (and run integration tests) on laptops - I'd like to avoid it where feasible.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/100#issuecomment-69814592:116,avoid,avoid,116,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/100#issuecomment-69814592,1,['avoid'],['avoid']
Safety,"The `Poisson` arises because we want to our model to generate the *occurrences*, assuming that each *count bin* provides equal weight---rather than the counts themselves. As usual, modeling each bin as Poisson is close enough to modeling all bins as multinomial for our purposes. If we directly use the NB likelihood and simply weight the count likelihood by occurrences, occurrences in the peak will strongly affect the result, adversely so if the count likelihood is actually misspecified there. As an example, consider trying to fit a Poisson to data that is actually zero-inflated Poisson---fitting the histogram will actually result in a more robust estimate for the mean. Another benefit is that truncated data (as we have here) is straightforwardly handled in an unbiased way. In the special case of complete, trivially-binned data, the full, unbinned likelihood is recovered. I think this sort of histogram fitting is pretty standard in the astro/particle community. We can certainly change up the model to include strictly quantized + free-floating states as you describe (rather than the ""fuzzily quantized"" states I use here), but I just wanted to avoid having another level of mixtures/logsumexps for this quick prototype. However, note that modeling mosaicism on the autosomes is desirable, but there we also want the strong diploid prior to nail down the depth and per-contig bias. So we will have to be a little careful about how we introduce free-floating states. Also, since I was not using gcnvkernel, I had to integrate out all discrete parameters. It may be that we can write down a nice model with discrete parameters if we use your inference framework. Finally, I did not further bin the counts here (or rather, the bin size is 1), which already yields the maximum information, but I did use a maximum-count cutoff. If we use the same cutoff for all samples, this allows us to simply pass a non-ragged matrix from Java (with dimensions of samples x contigs x maximum count) as a ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-376307522:873,recover,recovered,873,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-376307522,1,['recover'],['recovered']
Safety,"The change is because we started using VariantsSparkSink to write VCFs on the cluster, rather than writing them in a single thread from the driver (which doesn't scale). VariantsSparkSink only supports .bgz extensions currently, not .gz. So if you change the output extension to .bgz it will work. Gzip is not splittable so if possible we'd avoid outputting it at all. Hail for example will not load .gz unless the force option is used. (There are actually two force options, one to read it as regular gzip, the other to read it as bgzip, so you have to know which flavour of gzip it is...). We could do one of the following:. 1. Throw an error if the extension is .gz.; 2. Write bgzipped output to the .gz file.; 3. Write regular gzipped output to the .gz file. Thoughts?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3725#issuecomment-341689307:341,avoid,avoid,341,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3725#issuecomment-341689307,1,['avoid'],['avoid']
Safety,The current multi-interval iterator in the CRAM reader is pretty inefficient (to put it mildly) - I actually would have predicted that with that many intervals it would be much worse. I recently rewrote much of the query implementation - if you're so inclined it would be interesting to know if the branch in https://github.com/samtools/htsjdk/pull/552 has better perf (and this would be a great test to check if the results are correct).,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1787#issuecomment-215411570:120,predict,predicted,120,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1787#issuecomment-215411570,1,['predict'],['predicted']
Safety,"The error comes from two annotations: InbreedingCoeff and ExcessHet. One solution is to add ""-AX ExcessHet -AX InbreedingCoeff"". It doesnt exactly solve the problem, but it avoids hitting the problem code.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7938#issuecomment-1238883942:173,avoid,avoids,173,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7938#issuecomment-1238883942,1,['avoid'],['avoids']
Safety,"The functions for getting paired and unpaired reads now go with the second suggestion to partition the reads by read names (rather than use groupBy), then map the entire partitions to a list of paired reads and a list of unpaired reads. . Lots of angle brackets but I think this approach minimizes the number of lines while yielding substantial efficiency gains over the original approach, mainly by avoiding a needless second shuffle.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2664#issuecomment-300290475:400,avoid,avoiding,400,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2664#issuecomment-300290475,1,['avoid'],['avoiding']
Safety,"The issue is that only the linked list version of hash sets/maps has a predictable iteration order. So, any algorithm that is order dependent may get different results between different Java versions. The only downside is the linked list will take up some additional storage.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1844#issuecomment-220625823:71,predict,predictable,71,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1844#issuecomment-220625823,1,['predict'],['predictable']
Safety,"The last call stack that could be gathered with @droazen pointers to a docker image was -; ```; (gdb) bt; #0 0x00007f1124858067 in __GI_raise (sig=sig@entry=6) at ../nptl/sysdeps/unix/sysv/linux/raise.c:56; #1 0x00007f1124859448 in __GI_abort () at abort.c:89; #2 0x00007f1124851266 in __assert_fail_base (fmt=0x7f1124989f18 ""%s%s%s:%u: %s%sAssertion `%s' failed.\n%n"", ; assertion=assertion@entry=0x7f112520df60 ""new_prio == -1 || (new_prio >= __sched_fifo_min_prio && new_prio <= __sched_fifo_max_prio)"", file=file@entry=0x7f112520df54 ""tpp.c"", line=line@entry=62, ; function=function@entry=0x7f112520e030 <__PRETTY_FUNCTION__.8458> ""__pthread_tpp_change_priority""); at assert.c:92; #3 0x00007f1124851312 in __GI___assert_fail (; assertion=assertion@entry=0x7f112520df60 ""new_prio == -1 || (new_prio >= __sched_fifo_min_prio && new_prio <= __sched_fifo_max_prio)"", file=file@entry=0x7f112520df54 ""tpp.c"", line=line@entry=62, ; function=function@entry=0x7f112520e030 <__PRETTY_FUNCTION__.8458> ""__pthread_tpp_change_priority""); at assert.c:101; #4 0x00007f112520c5ef in __pthread_tpp_change_priority (previous_prio=previous_prio@entry=-1, ; new_prio=new_prio@entry=6271) at tpp.c:60; #5 0x00007f1125201e5f in __pthread_mutex_lock_full (mutex=0x7f111cd7e340) at ../nptl/pthread_mutex_lock.c:453; #6 0x00007f10f4f83a2d in mutex_lock(pthread_mutex_t*) (); from /cromwell_root/tmp.034550cf/root/libtiledbgenomicsdb5206530589131345400.so; #7 0x00007f10f4efe245 in StorageManager::array_close(std::string const&) (); from /cromwell_root/tmp.034550cf/root/libtiledbgenomicsdb5206530589131345400.so; #8 0x00007f10f4efe8b7 in StorageManager::array_iterator_finalize(ArrayIterator*) (); from /cromwell_root/tmp.034550cf/root/libtiledbgenomicsdb5206530589131345400.so; #9 0x00007f10f4ef77af in tiledb_array_iterator_finalize (); from /cromwell_root/tmp.034550cf/root/libtiledbgenomicsdb5206530589131345400.so; #10 0x00007f10f4e44730 in GenomicsDBBCFGenerator::~GenomicsDBBCFGenerator() (); from /cromwell_root/t",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4518#issuecomment-436579324:249,abort,abort,249,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4518#issuecomment-436579324,1,['abort'],['abort']
Safety,The problem is in practice if a read is unmapped along with its mate then there is no way that you can recover that read with any interval given to these tools. If you wish not to lose any reads then it is important not to provide any interval to ApplyBQSR step and run the tool as a single entity to collect all reads together. If you are ApplyBQSR in parallel to collect reads then you may not be able to rescue any unmapped pairs. Mapped reads with an unmapped pair may be rescued but I am not sure how other filters will affect. When I checked my own bam files sorted and bqsr applied bam files have the same number of reads inside already. . Also you may need to check your bam file to see if there are any problematic reads present.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8523#issuecomment-1725286670:103,recover,recover,103,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8523#issuecomment-1725286670,1,['recover'],['recover']
Safety,"The retry parameters are in BucketUtils.java:setGenerousTimeouts. For the case where we're opening thousands and thousands of connections, it may be helpful to increase those timeouts a little.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-303784746:175,timeout,timeouts,175,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-303784746,1,['timeout'],['timeouts']
Safety,"The segments VCFs contain the event segmentation results for each sample. Exact breakpoints are not given by this tool. Plots can be made using the denoised copy ratio TSV outputs. Currently our germline workflow does not incorporate the B-allele frequency evidence required for LOH detection, although that's a future goal.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6320#issuecomment-572176949:283,detect,detection,283,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6320#issuecomment-572176949,1,['detect'],['detection']
Safety,"The sequences of 150,119 genomes in the UK Biobank.](https://pubmed.ncbi.nlm.nih.gov/35859178/; Halldorsson BV, et al. Nature. 2022 Jul;607(7920):732-740. doi: 10.1038/s41586-022-04965-x. Epub 2022 Jul 20.PMID: 35859178. On page 69+ of this pdf, they describe the problem and how they cleverly worked around it. ; ; https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-022-04965-x/MediaObjects/41586_2022_4965_MOESM1_ESM.pdf. _It should be noted that running GATK out of the box will cause every job to read the entire; gVCF index file (.tbi) for each of the 150,119 samples. The average size of the index files is ; 4.15MB, so each job would have to read 4.15*150,126 = 623GB of data on top of the actual; gVCF slice data. For 60,000 jobs, this would amount to 623GB*60,000 = 37PB or 25.2GB/sec; of additional read overhead if the jobs are run on 20,000 cores in 17 days. This read; overhead will definitely prevent 20,000 cores from being used simultaneously. However,; this problem was avoided by pre-processing the .tbi files and modifying the software; reading the gVCF files from the central storage in a similar fashion as we did for GraphTyper; and the CRAM index files (.crai)._. This explains why chr1 requires more memory than chr22 despite running on the same number of samples. The larger chr1 tbi index is the source of the memory problem. The Decode solution is too limit the reading of the tbi index to the part that indexes the scattered region. There is a long pause at the beginning of the running GenotypeGVCFs which I never understood. GATK must be the reading of all the sample's gvcfs tbi into memory during that pause. So the reblocking of the gvcfs above reduced the memory foot print by decreasing the tbi size. Decode reduced it by chopping up the index so for each scattered region, GATK could only read a small subset of the index needed for that region. The combination of reblocking and chopping up the tbi would help with the memory requirements even more. Ho",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1374348579:1142,avoid,avoided,1142,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1374348579,1,['avoid'],['avoided']
Safety,"The test failures in the branch build are clearly related to the recent travis key migration. The PR build (which is the one we care about) passes, so this should be safe to merge.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7393#issuecomment-953279491:166,safe,safe,166,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7393#issuecomment-953279491,1,['safe'],['safe']
Safety,"The tutorial data would be an option, except that it covers reference territory not currently in the repo. It looks like that would add nearly 1 gig just for the reference data, which I think we'd really want to avoid. Ideally we'd have just one PR for each of the (two) tools. If you want to keep `ConstrainedMateFixingManager` and `NWaySAMFileWriter` as separate PRs, thats fine, but I don't think we'd take them until we know there is, or will very soon be, code that depends on them. Certainly using `@Experimental` for the tools could make sense, once we're certain that we have a way forward for test coverage. One other note, it's very helpful to include the original GATK3 file as the first commit, as you did in this PR.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-366271857:212,avoid,avoid,212,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-366271857,1,['avoid'],['avoid']
Safety,"The warnings the user is seeing are due to spanning deletion alleles which are currently not annotated with Funcotator. The bug here is what is causing the stack trace. It's in the protein sequence prediction code and I suspect that it has to do with the position of the variant relative to the exon/transcript boundaries. I have not been able to look at it yet, but thanks to the user posting the variants that are causing issues, it should be straight-forward to track down.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6651#issuecomment-644794980:198,predict,prediction,198,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6651#issuecomment-644794980,1,['predict'],['prediction']
Safety,"There are currently a few known issues that reviewers should bear in mind (these will become tickets shortly):. -All codecs marked as implementing `ReferenceDependentFeatureCodec` are currently non-functional. I need to either refactor them to not require a GenomeLocParser or delete them entirely. -The `IndexFeatureFile` tool is not currently working on block-compressed files -- will fix this soon. -`IndexFeatureFile` needs integration tests (will work on this during code review). -`BQSR` needs integration tests for the case of multiple simultaneous known sites files (now that it supports them!). -All codecs should move to tribble, and must implement `canDecode()` correctly (this is a new requirement, since it's no longer possible to manually request a particular codec). Most `canDecode()` implementations can be file-extension-based; only things like VCF format detection need to examine file contents to determine file type. -`FeatureDataSource` supports querying by interval, as well as full traversals, but not full traversal by a set of intervals (yet). This latter feature will be needed for the `VariantWalker` traversal.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/224#issuecomment-75658867:874,detect,detection,874,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/224#issuecomment-75658867,1,['detect'],['detection']
Safety,"There are many ways of corrupting data in TileDB/GenomicsDB - I am listing the protections available.; * Under the current setup, data is imported once by a single process - the _failIfUpdating_ flag is used in the GATK-4 import tool to ensure this.; * If an importer process crashes in the middle of execution, only the fragments that are fully completed will be visible to downstream queries/reads. Partially written fragments are on disk but ignored. This is achieved by renaming the fragment directory from _.<fragment_id>_ to _fragment_id_. While this is not an atomic operation, under the current use case, I cannot think of any way that will cause issues. At most, you waste some cycles re-importing data. What's not protected:; * Mapping data - sample name to TileDB row id, chromosome name to TileDB column interval. No plans to protect this in the current implementation. Once we have the PostgreSQL based storage for the mapping data, we will be able to detect and prevent inconsistencies.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2536#issuecomment-305557716:965,detect,detect,965,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2536#issuecomment-305557716,1,['detect'],['detect']
Safety,"There has been no activity on this for two years, and the two classes already inherit from different superclasses, and the current ""has a"" implementation avoids code duplication nicely.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4580#issuecomment-592146189:154,avoid,avoids,154,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4580#issuecomment-592146189,1,['avoid'],['avoids']
Safety,"There have been a few instances like this though. This one was obviously accidental, but things like the correct spelling of @magicDGS actual name seem like reasonable things to be able to include in the source. Also, testing non-ascii characters seems like something that is going to be increasingly common as we support new versions of the spec so it seems like we should learn to avoid this problem...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5936#issuecomment-492761095:383,avoid,avoid,383,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5936#issuecomment-492761095,1,['avoid'],['avoid']
Safety,There may be some legitimate reasons to do it (I will review what we currently enable) but my stance going in is that we should avoid it.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/890#issuecomment-143815791:128,avoid,avoid,128,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/890#issuecomment-143815791,1,['avoid'],['avoid']
Safety,There's no reason to even risk it becoming a problem in the future -- let's just include the fully-packaged jars in the docker image (since that is our standard binary distribution format anyway).,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4409#issuecomment-365993526:26,risk,risk,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4409#issuecomment-365993526,1,['risk'],['risk']
Safety,These initial results suggest that the savings from a pure-Spark pipeline are in the 15-30% range. @tomwhite Do you attribute these savings mostly to avoiding writing/reading intermediate outputs?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3395#issuecomment-341788281:150,avoid,avoiding,150,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3395#issuecomment-341788281,1,['avoid'],['avoiding']
Safety,"They are retried. You're seeing this message because it failed more than `maxChannelReopens` times. The new version which you really want to use for any test at this point is described there:; https://github.com/broadinstitute/gatk/issues/2685#issuecomment-302798685. Among other things, this new version puts in a message about 'retry failed' when it runs out of retries to eliminate the very confusion that you ran into. GenomicsDBImport opens a large number of parallel connections and as a result is getting throttled fairly heavily (by the host GCE machine if nothing else). This results in timeouts and dropped connections. One way forward is to increase the retry delays, another is to find a way to do the same work with fewer parallel open connections.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2749#issuecomment-304123753:596,timeout,timeouts,596,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2749#issuecomment-304123753,1,['timeout'],['timeouts']
Safety,"They discuss that add more ram.; I try with 8 cpu and 500 Go of RAM, but still not working. Error for one bam file:. ```. 15:47:36.554 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/genouest/uni_limoges_fr/jpollet/.conda/envs/myd88/share/gatk4-4.1.4.0-1/gatk-package-4.1.4.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Nov 28, 2019 3:47:37 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 15:47:37.239 INFO Mutect2 - ------------------------------------------------------------; 15:47:37.240 INFO Mutect2 - The Genome Analysis Toolkit (GATK) v4.1.4.0; 15:47:37.240 INFO Mutect2 - For support and documentation go to https://software.broadinstitute.org/gatk/; 15:47:37.240 INFO Mutect2 - Executing as jpollet@cl1n031.genouest.org on Linux v3.10.0-693.21.1.el7.x86_64 amd64; 15:47:37.240 INFO Mutect2 - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_192-b01; 15:47:37.246 INFO Mutect2 - Start Date/Time: 28 novembre 2019 15:47:36 CET; 15:47:37.246 INFO Mutect2 - ------------------------------------------------------------; 15:47:37.246 INFO Mutect2 - ------------------------------------------------------------; 15:47:37.246 INFO Mutect2 - HTSJDK Version: 2.20.3; 15:47:37.246 INFO Mutect2 - Picard Version: 2.21.1; 15:47:37.247 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 15:47:37.247 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 15:47:37.247 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 15:47:37.247 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 15:47:37.247 INFO Mutect2 - Deflater: IntelDeflater; 15:47:37.247 INFO Mutect2 - Inflater: IntelInflater; 15:47:37.247 INFO Mutect2 - GCS max retries/reopens: 20; 15:47:37.247 INFO Mutect2 - Requester pays: disabled; 15:47:37.247 INFO Mutect2 - Initializing engine; 15:47:41.204 INFO Mutect2 - Done initializi",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6271#issuecomment-559553558:489,detect,detect,489,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6271#issuecomment-559553558,1,['detect'],['detect']
Safety,"They now take about the same time, we avoid paying the cost of R installation but pay a cost to build the docker instead.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2965#issuecomment-349742450:38,avoid,avoid,38,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2965#issuecomment-349742450,1,['avoid'],['avoid']
Safety,"This ""fix"" to the timeout issue doesn't work -- closing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2805#issuecomment-305931466:18,timeout,timeout,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2805#issuecomment-305931466,1,['timeout'],['timeout']
Safety,"This bug has become part of a bigger effort to address how configure the gatk. We're working on a general solution to avoid this sort of issue in the future. We haven't addressed this specific subcase yet though. For now the workaround I described above should work for you. If it doesn't, let me know.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2300#issuecomment-274899565:118,avoid,avoid,118,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2300#issuecomment-274899565,1,['avoid'],['avoid']
Safety,"This change implements your suggestion, @droazen. However, this doesn't quite work, since now SAMRecord equality breaks due to the reference index not being the same: before being serialized the reference index might be set, while after being deserialized it will be -1. The problem is that on the write side (serialization) we can't get the index value from SAMRecord in the general case (i.e. we can't read the field if there's no header, and we certainly can't detect if it's null), and on the read side (deserialization) SAMRecord does not permit both the index and the name to be set to arbitrary values. Thoughts on how to solve this one?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1127#issuecomment-157763232:464,detect,detect,464,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1127#issuecomment-157763232,1,['detect'],['detect']
Safety,"This is a serious issue. Here's another error I've seen when running with `--sparkRunner SPARK` and `--sparkMaster local[*]`:. ```; ./gatk-launch PrintReadsSpark -I src/test/resources/org/broadinstitute/hellbender/tools/flag_stat.bam -O foo.bam -- --sparkRunner SPARK --sparkMaster local[*]. org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost): ; java.lang.ClassCastException: htsjdk.samtools.SAMFileHeader cannot be cast to htsjdk.samtools.SAMFileHeader; at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.lambda$writeReadsSingle$4e0a7f18$1(ReadsSparkSink.java:186); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink$$Lambda$86/1825278638.call(Unknown Source); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:153); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:153); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:686); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:686); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277); at org.apache.spark.rdd.RDD.iterator(RDD.scala:244); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277); at org.apache.spark.rdd.RDD.iterator(RDD.scala:244); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63); at org.apache.spark.scheduler.Task.run(Task.scala:70); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1315#issuecomment-164085312:329,abort,aborted,329,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1315#issuecomment-164085312,1,['abort'],['aborted']
Safety,"This is because GATK3 creates sequence dictionaries from VCF indices, and when an index is not present it creates one on-the-fly (see `RMDTrackBuilder.getFeatureSource()`). We could do the same in GATK4 (though we may not want to index vcfs on-the-fly if they're unindexed, as that is a source of race conditions -- safer to throw and make the user index the files).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1029#issuecomment-156497237:316,safe,safer,316,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1029#issuecomment-156497237,1,['safe'],['safer']
Safety,"This is because walkers like `SelectVariants` use file format auto-detection via the `FeatureManager`, whereas picard tools like `SortVcf` hardcode the codec they want to use.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1304#issuecomment-163320820:67,detect,detection,67,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1304#issuecomment-163320820,1,['detect'],['detection']
Safety,"This is great!; However, I have a two concerns:; (1) There are some functions that can be called on `SAMRecordToGATKReadAdapter` that cause the header to be queried (or produce different results if the header is missing). This is something that @jean-philippe-martin and @droazen noticed. We definitely want to avoid shipping around the header if at all possible (which I think we can do), but we need to solve _that_ problem before we turn this on. (2) We don't have sufficient testing in place to catch the issue from (1).; I propose that I create a ticket on getting rid of the header and we discuss the possible solutions for that problem over there. For this PR, I think it should go forward but without hooking it up to `GATKRegistrator`. You can write (copy) a unit test that hooks it up and shows that it works. In the mean time, we're collectively porting mark dupes and BQSR/ApplyBQSR. Once we have enough in place to have some confidence that we'd catch subtle serialization bugs (like missing headers), we should hook it up.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/899#issuecomment-140927907:311,avoid,avoid,311,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/899#issuecomment-140927907,1,['avoid'],['avoid']
Safety,"This is ready for review again, @droazen. I split the walkers in two: `ReadSliderWalker` and `VariantSliderWalker`, both based on `ReadWalker` and `VariantWalker` to traverse in a sliding-window approach. I also implemented some `ArgumentCollectionDefinition` for the three parameters (window-size, window-step and window-padding). There are some changes that I would like to discuss with you:; - `ShardSource<T>`: a generic class for lazily load sources of certain type. Now `ReadShard` extends this class, but there is actually no change in the definitions. I wonder if it is worthy to maintain `ReadShard` as a separate class, because it seems that the only usage is in `AssemblyRegionWalker` and it could be easily changed by a `ShardSource<GATKRead>`.; - `FilteringIterator<T>`: another generic class for filter on iteration records. Now `ReadFilteringIterator` extends this class, but again I don't find any usage that requires an specific case instead of a generic.; - There is some repetition in the test code: `ShardSourceUnitTest` and `ReadShardUnitTest` are exactly the same. I didn't want to remove the later, but I think that it is redundant; the same for the `FilteringIteratorUnitTest` adn `ReadFilteringIterator.; - Because of the inclusion of `ArgumentCollectionDefinition`s for window-related parameters, I just want to know if I should include them in `AssemblyRegionWalker` to maintain consistency in the user options. Back to you @droazen, and thanks again for all the help.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-224348995:1145,redund,redundant,1145,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-224348995,1,['redund'],['redundant']
Safety,This is redundant with https://github.com/broadinstitute/gatk/issues/1609 because https://github.com/broadinstitute/gatk/issues/1609 will run spark tools on hdfs for performance. Closing this one.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1402#issuecomment-226808573:8,redund,redundant,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1402#issuecomment-226808573,1,['redund'],['redundant']
Safety,"This is solved -- @cmnbroad patched htsjdk to make using headerless reads directly safer, we modified `ReadsSparkSource` to null out headers, and @tomwhite implemented a custom serializer based on the BAM codec that is both fast and safe for headerless reads.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/900#issuecomment-158442360:83,safe,safer,83,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-158442360,2,['safe'],"['safe', 'safer']"
Safety,"This looks promising, at a minimum we should try setting up the docker with hadoop native libraries so the performance gains can be extended to most use cases. This might also include adding some magic to the gatk launch script inside the docker to detect and run with the correct version of the hadoop libraries.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4746#issuecomment-387519906:249,detect,detect,249,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4746#issuecomment-387519906,1,['detect'],['detect']
Safety,"This may require us to decide whether to support both GATK-style interval files and Picard-style interval files, or standardize on a single format. We also need to improve the code that detects the interval file format (currently catches exception from Picard to detect GATK interval files)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4#issuecomment-69801755:186,detect,detects,186,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4#issuecomment-69801755,2,['detect'],"['detect', 'detects']"
Safety,This might allow us to avoid the expensive final concatenation for outputs in Spark.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4492#issuecomment-370504855:23,avoid,avoid,23,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4492#issuecomment-370504855,1,['avoid'],['avoid']
Safety,"This should add support for reading bam files with CSI indexes, as well as porting the FastaReferenceWriter to htsjdk and a lot of other changes to htsjdk. @samuelklee This includes the overlap detector optimizations you wanted as well as the changes to let you write interval file to paths.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5812#issuecomment-474098651:194,detect,detector,194,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5812#issuecomment-474098651,1,['detect'],['detector']
Safety,"This should be straight-forward once the discussion above is resolved. Just detect if the variant is on a known Mitochondrial contig. If so, use the MT codon table, otherwise use the regular codon table.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4863#issuecomment-430312996:76,detect,detect,76,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4863#issuecomment-430312996,1,['detect'],['detect']
Safety,"This task is to take the training data generated in issue #3092 and learn something from it, for example a regression model that predicts a distribution of artifactual read fractions. Using the learned model in filtering is a separate issue.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2973#issuecomment-307680993:129,predict,predicts,129,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2973#issuecomment-307680993,1,['predict'],['predicts']
Safety,This will be the next bug I look into. It's part of a family of issues that all relate to how the predicted protein change is created.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6289#issuecomment-688996786:98,predict,predicted,98,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6289#issuecomment-688996786,1,['predict'],['predicted']
Safety,"To add some commentary to why this is happening: It looks like multiple threads are hitting this line simultaneously and based on the overload of `ArrayList.add()` this error could be triggered by multiple calls to `ensureCapacityInternal()` inside the add method:; ```; final List<ReadsPathDataSource> readSources = new ArrayList<>(threads);; final ThreadLocal<ReadsPathDataSource> threadReadSource = ThreadLocal.withInitial(; () -> {; final ReadsPathDataSource result = new ReadsPathDataSource(readArguments.getReadPaths(), factory);; readSources.add(result);; return result;; });; ```; The fix should be simple you just have to make sure ti synchronize the initialization or swap out the readSources object to one that is itself thread safe. @vruano",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7403#issuecomment-899732721:739,safe,safe,739,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7403#issuecomment-899732721,1,['safe'],['safe']
Safety,"To avoid further confusion during review, and to make sure tests still pass here, I've rebased this branch onto the final merged version of https://github.com/broadinstitute/gatk/pull/1127. . @tomwhite you might need to do a `git reset --hard da0c66f8370be89d8c422d4134402e1b47ab912c` next time you touch this branch.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1174#issuecomment-158447492:3,avoid,avoid,3,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1174#issuecomment-158447492,1,['avoid'],['avoid']
Safety,To avoid user confusion. Discussed in person with @jean-philippe-martin who gave his :+1:,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1172#issuecomment-158410403:3,avoid,avoid,3,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1172#issuecomment-158410403,1,['avoid'],['avoid']
Safety,Tools are:. - DetectCoverageDropout; - DecomposeSingularValues; - CalculatePulldownPhasePosteriors. Nine related docs are:. - CalculatePulldownPhasePosteriors.java			; - CoverageDropoutDetectorTest.java			; - DecomposeSingularValuesIntegrationTest.java; - CalculatePulldownPhasePosteriorsIntegrationTest.java	; - CoverageDropoutResult.java				; - DetectCoverageDropout.java; - CoverageDropoutDetector.java				; - DecomposeSingularValues.java				; - DetectCoverageDropoutIntegrationTest.java,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2809#issuecomment-305998381:14,Detect,DetectCoverageDropout,14,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2809#issuecomment-305998381,3,['Detect'],"['DetectCoverageDropout', 'DetectCoverageDropoutIntegrationTest']"
Safety,"Total time for cloud tests: 10min, 29s.; Travis timeout: 10min",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/812#issuecomment-132396181:48,timeout,timeout,48,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/812#issuecomment-132396181,1,['timeout'],['timeout']
Safety,"Two documentation nitpicks. Looks good. It's good that you're fixing this, since i think the entire point of this class was to avoid copying the bases, and it ended up just copying the bases for the hashcode.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1620#issuecomment-200883757:127,avoid,avoid,127,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1620#issuecomment-200883757,1,['avoid'],['avoid']
Safety,"Two high level questions/concerns about the implementation… . 1. I think when things are parallelized, we then lose the retries. In order to do that, we might need to get a little more python async-y than I was originally thinking. The retries are pretty important to recover from transient errors in BQ. 2. I think perhaps the ordering/dependecies are lost between queries? It looks like we fire them all off and then wait for them all to complete. But really we need to wait for the vet_new queries to finish before launching the pet queries since they depend on data from the vet_new. If vet_new hasn't started this would fail… but if vet_new has created the table we'll just get the wrong results",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7505#issuecomment-944301753:268,recover,recover,268,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7505#issuecomment-944301753,1,['recover'],['recover']
Safety,"Two more things:. 1) attached is the error log. There error is:. ```; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fb23d5aba19, pid=147396, tid=140403504084736; #; # JRE version: Java(TM) SE Runtime Environment (8.0_60-b27) (build 1.8.0_60-b27); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.60-b23 mixed mode linux-amd64 ); # Problematic frame:; # C [libtiledbgenomicsdb6950059158101672698.so+0x3e3a19] ArraySchema::tile_num(void const*) const+0x79; #; # Core dump written. Default location: /home/groups/MgapGenomicsDb/@files/sequenceOutputPipeline/SequenceOutput_2020-11-04_09-17-57/Job44/core or core.147396; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #. ```. 2) I was mistaken on when the error happens. The last line logged by GenomicsDBImport is ""GenomicsDBImport - Importing batch 4 with 33 samples"". There are 4 total batches, with a batchSize of 50. I dont know whether batch 4 finished or if this error is mid-batch import. [hs_err_pid147396.log.txt](https://github.com/broadinstitute/gatk/files/5496539/hs_err_pid147396.log.txt)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6910#issuecomment-722594280:98,detect,detected,98,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6910#issuecomment-722594280,1,['detect'],['detected']
Safety,"Updated plan. ----------; ## Small improvements in new interpretation tool; ; - [x] Output bam instead of sam for assembly alignments; - [x] Instead of creating directory, new interpretation tool writes files (behavior consistent with current interpretation tool); - [x] Prefix with sample name for output files' names; - [x] Add `INSLEN` annotation when there's `INSSEQ`; - [x] Clarify the boundary between `AlignedContig` and `AssemblyContigWithFineTunedAlignments`; - [x] Increase test coverage for `AssemblyContigAlignmentsConfigPicker`; ; ----------; ## Consolidate logic, bump test coverage and update how variants are represented. ### consolidate logic; When initially prototyped, there's redundancy in logic for simple variants, now it's time to consolidate. - [x] `AssemblyContigWithFineTunedAlignments`; - [x] `hasIncompletePicture()`. - [x] `AssemblyContigAlignmentSignatureClassifier`; - [x] Don't make so many splits; - [x] Reduce `RawTypes` into fewer cases; ; - [x] `ChimericAlignment`; - [x] update documentation; - [x] implement a `getCoordinateSortedRefSpans()`, and use in `BreakpointsInference`; - [x] `isNeitherSimpleTranslocationNorIncompletePicture()`; - [x] `extractSimpleChimera()`. ### bump test coverage; Once code above is consolidated, bump test coverage, particularly for the classes above and the following poorly-covered classes; - [x] `ChimericAlignment`; - [x] `isForwardStrandRepresentation()`; - [x] `splitPairStrongEnoughEvidenceForCA()` ; - [x] `parseOneContig()` (needs testing because we need it for simple-re-interpretation for CPX variants) Note that `nextAlignmentMayBeInsertion()` is currently broken in the sense that when using this to filter out alignments whose ref span is contained by another, check if the two alignments involved are head/tail. - [x] `BreakpointsInference` & `BreakpointComplications`. - [x] `NovelAdjacencyAndAltHaplotype`; - [x] `toSimpleOrBNDTypes()`. - [x] `SimpleNovelAdjacencyAndChimericAlignmentEvidence`; - [x] serialization ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4111#issuecomment-375438021:696,redund,redundancy,696,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4111#issuecomment-375438021,1,['redund'],['redundancy']
Safety,"We already added the functionality needed for the gCNV workflow to IntervalListTools in https://github.com/broadinstitute/picard/pull/1208. The issue is that the tool outputs each scattered interval list to a separate directory if the number of scatters is greater than 1, but it just outputs to a file (essentially a noop) if we don't need to scatter. Not sure the reason for this design, but it makes things difficult from the perspective of WDL. Would be easier if the expected output was always `Array[File]+`. I don't really see why IntervalListTools needs to create those intermediate directories (nor why the naming scheme is determined by `DecimalFormat(""0000"")`---don't think this is documented, either), but I am not sure if that behavior is expected by now or if it is safe to modify it. Pretty sure SplitIntervals is just calling the same backend class used by IntervalListTools. Perhaps that tool might've been spun off before we exposed the Picard tools? See e.g. https://github.com/broadinstitute/gatk/pull/5392#issuecomment-435588845. I think we should try to avoid writing such custom/utility GATK tools unless really warranted.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6502#issuecomment-599639907:780,safe,safe,780,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6502#issuecomment-599639907,2,"['avoid', 'safe']","['avoid', 'safe']"
Safety,"We are rewriting that tool in java to prevent issues like this. You could pull that branch from here: https://github.com/broadinstitute/gatk/pull/4800.; Otherwise, I think pysam handles gzipped indexed vcfs better than plain text ones. So if you gzip your resource files with ; `; bgzip -c hapmap_3.3.hg19.sites.vcf > hapmap_3.3.hg19.sites.vcf.gz; `; then index them with tabix:; `; tabix -p vcf hapmap_3.3.hg19.sites.vcf.gz; `; you may avoid this crash.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4794#issuecomment-391385377:437,avoid,avoid,437,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4794#issuecomment-391385377,1,['avoid'],['avoid']
Safety,"We can add this as an option, but I think there are a few arguments against outright replacement (which may have led to this design decision in the first place):. 1) I don't believe any CNV tools take in sample name as input at the GATK command line, by design. We instead take the BAM basename as the entity ID during the CollectCounts task; this ID is then passed along at the WDL level and is only used to construct the filenames of output files. This obviates the need for tools like GetSampleName and avoids issues with parsing funky sample names at the command line, handling/passing special characters at all levels (WDL, Java, python), etc. (The implicit assumption is that the BAM basename is more likely to be well formed.). 2) I would argue that specifying a sample index is relatively user friendly, especially if we are typically running on all samples. In that case, all you need to know is the total number of samples and that we use zero indexing, and then you simply iterate over all indices. (If you want to run on a single, particular sample, then perhaps using the sample name might be more friendly, but I'd argue that this use case is not typical.). 3) If you want to go ahead and add this option, I would probably keep the directory structure of the GermlineCNVCaller output the same (i.e., with folders named ""SAMPLE_#""), and just check the sample_name.txt files at the PostprocessGermlineCNVCalls step. I don't think this should require GermlineCNVCaller code changes, right?. 4) We may require additional code at the WDL level if we want to both switch over to primarily using sample names but also get rid of bundling (i.e., by passing only the calls for each sample when needed). Locally, you can always just search all output for directories containing the appropriate sample_name.txt. But on the cloud, you'd want to make sure that the postprocessing step for a particular sample gets only its corresponding directories, which would have to happen at the WDL level; the c",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6659#issuecomment-644829765:506,avoid,avoids,506,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6659#issuecomment-644829765,1,['avoid'],['avoids']
Safety,"We can certainly modify IntervalListTools to make the behavior more intuitive (e.g. add a new mode for `INTERVAL_COUNT_WITH_OVREFLOW`), but I'm not sure I understand the issue. We're just trying to avoid calling the GATK command if the scatter is going to be a noop?. The GATK SplitIntervals has slightly different behavior that's helpful in some cases.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6502#issuecomment-599618889:198,avoid,avoid,198,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6502#issuecomment-599618889,1,['avoid'],['avoid']
Safety,"We can implement this, but first can you explain why your `canDecode()` methods can't unambiguously detect your file formats? The VCF/BCF codecs use a magic value at the start of the file to detect the format -- are your codecs guessing as to the intended format? Is there no way to be sure what the format is?. If we have multiple codecs able to decode a file, and only one produces `Features` that match the type parameter of the `FeatureInput`, would it solve your problem if we selected that codec rather than blow up? This would be a bit simpler/easier than a new annotation.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1184#issuecomment-163297503:100,detect,detect,100,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1184#issuecomment-163297503,2,['detect'],['detect']
Safety,"We decided to remove the ""conversion"" to AllelicCapseg output from ModelSegments, since this was an ill defined procedure. The models used by AllelicCapseg and AllelicCNV/ModelSegments are simply different, so it's not possible to define a unique conversion between their model parameters. Compounding this, we also had difficulty finding up-to-date documentation about the models used by various versions of both AllelicCapseg and ABSOLUTE. That said, some of this removed functionality can be found in unsupported WDLs at https://github.com/broadinstitute/gatk/tree/master/scripts/unsupported/combine_tracks_postprocessing_cnv (specifically, see the PrototypeACSConversion task in combine_tracks.wdl). These scripts also attempt to perform rudimentary filtering of germline events found in the matched normal; see first link below for some additional caveats. Note that we cannot really answer further questions or otherwise support these scripts (and it's possible that the experimental/beta GATK tools used in the WDLs may be removed in the future), and the developer responsible for them has moved on from the Broad---use them at your own risk. See also https://gatkforums.broadinstitute.org/gatk/discussion/comment/59467 https://github.com/broadinstitute/gatk/pull/5450 https://github.com/broadinstitute/gatk/issues/5804 for additional context.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6685#issuecomment-652407603:1144,risk,risk,1144,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6685#issuecomment-652407603,1,['risk'],['risk']
Safety,"We decided to replace SparkGenomeReadCounts with a relatively simple ReadWalker to avoid various bugs we were running into (some of which were due to Hadoop-BAM). We found that these bugs gave rise to a relatively high failure rate---roughly 1 in 50 TCGA BAMs. Like any ReadWalker, you can specify custom read filters using GATK engine arguments such as `--disable-default-read-filters` and `--read-filter ...` However, because we count fragment centers (rather than read starts, as in SparkGenomeReadCounts), disabling filters which check that reads are properly paired may lead to unexpected behavior. In principle, we could write a similar ReadWalkerSpark version of the tool. However, our experience running the tool showed that CollectFragmentCounts was already faster than SparkGenomeReadCounts in Spark local mode, sometimes by a factor of ~5 (and, more importantly, it didn't run into Hadoop-BAM failures). We may do some more careful profiling and roll a ReadWalkerSpark version in the future, but these aren't too high priority at the moment.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4185#issuecomment-358660583:83,avoid,avoid,83,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4185#issuecomment-358660583,1,['avoid'],['avoid']
Safety,"We have [a researcher reporting that even when running locally, they are getting messages related to the GCS](https://gatkforums.broadinstitute.org/gatk/discussion/13015/failed-to-detect-whether-we-are-running-on-google-compute-engine#latest), which they find puzzling. > WARNING: Failed to detect whether we are running on Google Compute Engine. plus what looks like an error stacktrace in the middle of the stdout. Is this intentional?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4369#issuecomment-424004818:180,detect,detect-whether-we-are-running-on-google-compute-engine,180,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4369#issuecomment-424004818,2,['detect'],"['detect', 'detect-whether-we-are-running-on-google-compute-engine']"
Safety,We have asked the green team to run their pipeline tests on this branch to at least limit the risk of more full sample failures. It will probably be a few more days before we have those results. @gbggrant,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6533#issuecomment-614126868:94,risk,risk,94,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6533#issuecomment-614126868,1,['risk'],['risk']
Safety,We run contamination checks on the somatic VCF using the AMBER / COBALT / PURPLE pipeline. It does not flag contamination. [PURPLE](https://github.com/hartwigmedical/hmftools/tree/master/purity-ploidy-estimator) is a purity and ploidy estimator but often struggles with low purity samples and erroneously reports a samples such as this as purity 1.00 and ploidy 2.00. I wonder if such a sample could be detected using some tool from GATK?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6674#issuecomment-649095750:403,detect,detected,403,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6674#issuecomment-649095750,1,['detect'],['detected']
Safety,"We should be able to move to just using JoinReadsWithVariants, and avoid using AddContextDataToReadSpark (which uses JoinReadsWithRefBases). I did something like that in these two commits: https://github.com/broadinstitute/hellbender/commit/eb82408af03b2ddfdad947689fc9d22b2b18e82e, https://github.com/broadinstitute/hellbender/commit/864040a780c93d85dcfa73fa385187a128e069e9. Also, using a 2bit representation would be great as it would use less memory. If it's a lot more work we can use the existing ReferenceBases implementation to start with.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/909#issuecomment-141987032:67,avoid,avoid,67,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/909#issuecomment-141987032,1,['avoid'],['avoid']
Safety,"We should only turn on parallel tests if it's multi-process parallelism, rather than multi-threading parallelism. Dealing with thread safety issues in the test suite would be a nightmare.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/221#issuecomment-75817361:134,safe,safety,134,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/221#issuecomment-75817361,1,['safe'],['safety']
Safety,"We test against 8 and 11 so I would expect either of those to work with out any weird issues. . I can't guarantee that 17 is without problems, but I think if you're running it and the output looks reasonable than there's nothing I know of that would cause subtle errors. You'll definitely have issues with things like building the documentation, but I suspect that if something runs without crashing it's probably ok. It's not officially supported right now though, so it's a bit of use at your own risk.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7842#issuecomment-1122762387:499,risk,risk,499,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7842#issuecomment-1122762387,1,['risk'],['risk']
Safety,"We used to call this `--unsafe`, but I feel like you've just put lipstick on the pig. Usually you're more conservative than I am, so if you're okay with this then so am I.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6589#issuecomment-625427966:24,unsafe,unsafe,24,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6589#issuecomment-625427966,1,['unsafe'],['unsafe']
Safety,"We will certainly output VCFs for our final calls, but we also have useful intermediate file formats that would benefit from having the sample name (and sequence dictionary, if appropriate) attached---for example, our read-count and allelic-count files. I see no reason that these shouldn't have a header with the appropriate tags generated from the input BAM and be read/written in the Tribble framework. However, there are a few exceptions in the CNV pipeline for files that not collections of Features but are still associated with a sample, so I think it would be nice to extract out the sample-name related code across the various tools. This is not only to avoid duplicating effort, but also to standardize how we store this information in our various file formats.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3726#issuecomment-338288560:663,avoid,avoid,663,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3726#issuecomment-338288560,1,['avoid'],['avoid']
Safety,"We're also not building Prometheus for people who use GATK on bacteria, or plants, or weird cave-dwelling fish, so I expect that the analysis service will not be available to them. Should they turn to another platform? I think it would be a big mistake (re: science, re: public perception and re: competition with other software) to make the new GATK effectively not runnable outside the Broad infrastructure. I understand the need to focus on primary objectives, but there is a real risk of shooting ourselves in the foot if the vision is too narrow. . So yes, I look forward to helping work out these details :)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/16#issuecomment-66951909:484,risk,risk,484,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/16#issuecomment-66951909,1,['risk'],['risk']
Safety,We're having issues with all of our tests today. Github is refusing our git-lfs requests because they're over quota and we need to figure out how to either authenticate our requests in a safe way from travis or figure out why we're suddenly going over quota. It happened very suddenly and I suspect there might be an issue on github's end..,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2506#issuecomment-292005363:187,safe,safe,187,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2506#issuecomment-292005363,1,['safe'],['safe']
Safety,"Well, I definitely like the safety and generality of this design better, thought as it stands it wastes the first turn of the (`ReadsDataSource`) crank, which would be nice to avoid. Let me take a closer look. Maybe we can replace `TwoPassReadWalker` with this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5985#issuecomment-499482109:28,safe,safety,28,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5985#issuecomment-499482109,2,"['avoid', 'safe']","['avoid', 'safety']"
Safety,"Well, you're welcome to use gatk-launch as a launch script if you'd like (and feel free to rename to whatever you like...) A. There are a few reasons we have spark and non-spark versions of the tools. . 1. We wanted to port and validate certain tools as quickly as possible and doing a direct port from gatk3 -> gatk4 was easier than making them sparkified at the same time. 2. There's a tradeoff in using spark where you end up spending more total cpu hours in order to finish a job faster. Ideally this would be 1:1, double the number of cores and you halve the time to finish a job. It never scales perfectly though, there's always some overhead for being parallel. Our production pipelines are extremely sensitive to cost and not very sensitive to runtime, so they prefer we have a version that's optimized to use the least cpu hours even if that means a longer runtime. Other users prefer to be able to finish a job quickly and are willing to pay slightly more to do so, so we also have a spark version. . 3. Some tool are complicated to make work well spark. Spark works best when you can divide the input data into independent shards and then process them separately. This is complicated for things like the AssemblyRegion walker where you need context around each location of interest. We had to do things like add extra overlapping padding and things like that to avoid boundary issues where there are shard divisions. We don't yet fully understand spark performance and it's caveats, we're looking into that actively now. We hope that we'll be able to optimize our tools so that a spark pipeline of several tools in series is faster than running the individual non-spark versions, since it lets us avoid doing things like loading the bam file multiple times from disk. Whether or not we can achieve this is still and open question though.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2345#issuecomment-273318100:1373,avoid,avoid,1373,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2345#issuecomment-273318100,2,['avoid'],['avoid']
Safety,"What do you mean by more automated? It looks like you're allocating space based on the input file sizes and some padding, which is already more automated than the user adjusting disk size by hand. Do you mean that Cromwell should allocate space appropriately given the inputs? The issue is that you also need space for the outputs, which is harder to predict unless you have a sense of what the task is doing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4737#issuecomment-386594579:351,predict,predict,351,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4737#issuecomment-386594579,1,['predict'],['predict']
Safety,"When the engine ""marginalizes"" haplotype likelihoods into allele likelihoods it avoids double-counting of both the MNP at 151 and the SNP at 152. That is, the CT->TC MNP haplotype is consistent at 152 with the T->C SNP, but it has a different start position and therefore is not marginalized into the evidence for the SNP. So the fact that the DP in sample1 is much less at 152 than at 151 makes sense. I am also confused about sample2. If we're both still confused tomorrow, let's take a look in IGV. Might even need IntelliJ. Might even be a bug -- if you look in `AssemblyBasedCallerGenotypingEngine.createAlleleMapper`, you'll see that the overlapping event logic assumes we're dealing with upstream spanning deletions. Maybe MNPs need to be treated differently.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5513#issuecomment-447033286:80,avoid,avoids,80,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5513#issuecomment-447033286,1,['avoid'],['avoids']
Safety,"With some tweaks I see a good improvement. . First i tried it on our testing bam (on dr_runnable_haplotypecaller) and saw some speedup:. before, in GVCF mode. ```; real 6m6.430s; user 7m54.014s; sys 0m4.540s; ```. after. ```; real 5m58.246s; user 7m53.737s; sys 0m4.403s; ```. Then I rewrote it a bit by avoiding the `new String` calls and directly using the static `lastIndexOf` method from String that works on arrays (required copying code from the Java libraries because the right method is not accessible). The result was a 30s improvement over baseline (6%). ```; real 5m44.213s; user 7m26.240s; sys 0m3.916s; ```. So that would be a nice improvement but I'm not sure if copy/pasting code from the JDK is kosher license-wise. To be completely safe, @gspowley can you implement a clean-room version of `lastIndexOf(byte[] source, byte[] target)` so that we can replace `new String(reference).lastIndexOf(new String(alternate))` with `lastIndexOf(reference, alternate)`? Or let's find a library that we can use for this. I want to avoid `String` creation here because it's pretty expensive.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1677#issuecomment-204206963:304,avoid,avoiding,304,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1677#issuecomment-204206963,3,"['avoid', 'safe']","['avoid', 'avoiding', 'safe']"
Safety,"Would this be useful: In the graph, if the these extremely short contigs are orphans/islands (not overlapping with any other contigs), we can probably safely ignore them, as these would suggest, even if they generate a chimera, the signal could be poorly-supported. In this regard, I think fermi-lite has a very-nice feature that's desirable: for the final graph it outputs, the number of reads used to generate the contig is given, plus the coverage for each base .",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2031#issuecomment-234288860:151,safe,safely,151,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2031#issuecomment-234288860,1,['safe'],['safely']
Safety,"Wow. How on Earth did we avoid this for so long?. On Wed, Jan 24, 2018 at 4:39 PM droazen <notifications@github.com> wrote:. > @ldgauthier <https://github.com/ldgauthier> @yfarjoun; > <https://github.com/yfarjoun> We have an update on this! We've identified; > the bug:; >; > - When AbstractFeatureReader.getFeatureReader() tries to open a .vcf.gz; > that doesn't have an index, it returns a TribbleIndexedFeatureReader; > instead of a TabixFeatureReader, because methods.isTabix() returns; > false when an index is not present.; > - TribbleIndexedFeatureReader, in turn, opens a Java vanilla; > GZIPInputStream, instead of the BlockCompressedInputStream that gets; > opened when you create a TabixFeatureReader.; > - GZIPInputStream, in turn, has a *confirmed bug* filed against it in; > Oracle's bug tracker (see; > https://bugs.java.com/bugdatabase/view_bug.do?bug_id=7036144#), that; > it inappropriately relies on the available() method to detect; > end-of-file, which is never safe to do given the contract of; > available(); > - As the final piece in the ghastly puzzle, implementations of; > SeekableStream in htsjdk do not implement available() at all, instead; > using the default implementation which always returns 0.; >; > As a result of this combination of bugs in Java's GZIPInputStream itself; > and bugs in htsjdk's SeekableStream classes, end-of-file can be detected; > prematurely when within 26 bytes of the end of a block, due to the; > following code in GZIPInputStream.readTrailer():; >; > if (this.in.available() > 0 || n > 26) {; > ....; > }; > return true; // EOF; >; > Where n is the number of bytes left to inflate in the current block.; >; > The solution is to replace all usages of the bugged GZIPInputStream with; > BlockCompressedInputStream in tribble in htsjdk (at least, for points in; > the code where the input is known to be block-gzipped rather than regular; > gzipped). For due diligence we should also implement available(); > correctly for all implementations",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4224#issuecomment-360304725:25,avoid,avoid,25,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4224#issuecomment-360304725,1,['avoid'],['avoid']
Safety,Yea ideally we would have a more principled solution like the overall Viterbi sequence likelihood that Sam was trying to get to work. But it has been certainly very useful for now as a basic sanity check.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7150#issuecomment-811364073:191,sanity check,sanity check,191,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7150#issuecomment-811364073,1,['sanity check'],['sanity check']
Safety,"Yeah that's one of the reasons we didn't catch this (and one other thread safety bug), we don't run multithreaded in prod and don't test for it systematically.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2326#issuecomment-270439784:74,safe,safety,74,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2326#issuecomment-270439784,1,['safe'],['safety']
Safety,"Yes @rong923, go with >= 4.1.1.0 and you are safe. I ve been running hundreds of pipelines without a problem now.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5543#issuecomment-487724137:45,safe,safe,45,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5543#issuecomment-487724137,1,['safe'],['safe']
Safety,"Yes PathSeq has generally been used with longer reads in the past and also with BWA. I suspect with the shorter reads, novel splice sites are getting missed by PathSeq. You could turn down the match threshold to 16 (half the read length) to help this further, or even hard-filter any aligned read, but you do risk losing some microbial reads at this length. . You're also right that in your case, using the mate information would be useful, but this would not be appropriate, for example, when there are host-pathogen chimeras, such as virus integration events.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6687#issuecomment-652638341:309,risk,risk,309,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6687#issuecomment-652638341,1,['risk'],['risk']
Safety,"Yes please. Where we left it I think Louis was happy, but we wanted to ask Nalini if; she had any suggestions to avoid threading the argument for genotypes all; the way through the engine. On Thu, Mar 28, 2019 at 11:49 AM droazen <notifications@github.com> wrote:. > What's the status of this one @ldgauthier <https://github.com/ldgauthier>; > ? Do you need a new reviewer with @lbergelson; > <https://github.com/lbergelson> out?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/pull/4947#issuecomment-477654500>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AGRhdJBn0j6n9Dfp-sz763M3mP5b1oyDks5vbOSHgaJpZM4U4KK0>; > .; >. -- ; Laura Doyle Gauthier, Ph.D.; Associate Director, Germline Methods; Data Sciences Platform; gauthier@broadinstitute.org; Broad Institute of MIT & Harvard; 320 Charles St.; Cambridge MA 0214",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4947#issuecomment-477720872:113,avoid,avoid,113,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4947#issuecomment-477720872,1,['avoid'],['avoid']
Safety,"Yes we do, we run on a list of calling intervals that avoids empty/blackhole/timesuck regions.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2591#issuecomment-297588499:54,avoid,avoids,54,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2591#issuecomment-297588499,1,['avoid'],['avoids']
Safety,"Yes, that's the error message. You should see [this part of the GenomicsDB](https://github.com/Intel-HLS/GenomicsDB/blob/stack_ovf_fix/src/main/cpp/src/genomicsdb/variant_storage_manager.cc#L540) code that retries the function when an error is detected. So, you don't need to be concerned by those error messages. They are annoying though and I will disable them from being printed.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407440149:244,detect,detected,244,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407440149,1,['detect'],['detected']
Safety,"Yes, the whole motivation behind this effort was based on our belief that the split guessing in Hadoop BAM wasn't safe for production use, and so we wanted a more bulletproof mode of operation.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1568#issuecomment-195369059:114,safe,safe,114,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1568#issuecomment-195369059,1,['safe'],['safe']
Safety,"Yes, this is one reason why I was saying that we should use an alternate version of the BAM codec that doesn't query reference indices at all, instead of using the actual bam codec. @cmnbroad, who spent several weeks patching htsjdk to allow headerless reads to function in a consistent and well-documented way, agrees, as the bam codec could not be modified to make it safe for headerless reads. Let's literally copy `BAMRecordCodec` into hellbender, rename it to `GATKReadSparkCodec`, and modify it to serialize reference names instead of indices, and not call `getReferenceIndex()`/`getMateReferenceIndex()` at all. @tomwhite could you do this?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1127#issuecomment-157444249:370,safe,safe,370,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1127#issuecomment-157444249,1,['safe'],['safe']
Safety,"Yes, we can pretty easily detect this exit code and generate an error message suggesting it might be an OOM issue. I'll make a PR.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6362#issuecomment-572769179:26,detect,detect,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6362#issuecomment-572769179,1,['detect'],['detect']
Safety,"Yes. We should expect that PCR error shouldn't affect the base-quality, so two high quality, disagreeing bases are an indication of a PCR error, while one low-quality base, and one high quality base that have differing qualities looks more like a sequencing error. We might be able to obtain a data-driven model for that using the overlapping bases themselves (over monomorphic sites). The only problem is that this is only true when the reads haven't been processed by Consensus calling....but if we have a good model for consensus calling within haplotype caller we could avoid doing that upfront and simply deal with everything within haplotype caller. **That** would be ideal!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4958#issuecomment-400815571:574,avoid,avoid,574,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4958#issuecomment-400815571,1,['avoid'],['avoid']
Safety,You can also individually disable tool default read filters using the `--disable-read-filter` or `-DF` argument. . In general if you disable the basic safeguards the tool requires you're likely to run into issues though.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8224#issuecomment-1452506814:151,safe,safeguards,151,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8224#issuecomment-1452506814,1,['safe'],['safeguards']
Safety,"You can detect whether a tool has failed in bash by checking whether the exit status code is non-zero. In bash, the exit status code of the last command run is stored in the variable `$?`; ; In general, you should ask questions like these on the GATK forum (https://gatkforums.broadinstitute.org/gatk) instead of here, however -- this is for bug reports rather than support requests.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4242#issuecomment-359955349:8,detect,detect,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4242#issuecomment-359955349,1,['detect'],['detect']
Safety,"You're right, @cmnbroad - the `@Argument` annotation is unsafe if it is included in an argument collection. Maybe a `@DocumentedField` annotation in barclay will help, by populating its information and include it in the json. This will allow to document also constant fields specific to a tool (e.g., a default value for a BAM tag, explaining why it is the default). That will be nice for several use cases, including the one here...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3947#issuecomment-351299081:56,unsafe,unsafe,56,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3947#issuecomment-351299081,1,['unsafe'],['unsafe']
Safety,"Your solution doesn't address your third listed drawback to the current; approach, though I'm not sure there's any way to do that that wouldn't; require a pretty dramatic change. It's not obvious to me why we wanted the given alleles in the graph; originally. Maybe the use case was variants from UG that we didn't; necessarily believe were aligned properly?. I don't have any objections, but I'd feel better if we had a better guess; at what the original method was trying to do. On Wed, Apr 3, 2019 at 9:56 PM David Benjamin <notifications@github.com>; wrote:. > In Mutect2 and HaplotypeCaller, we force-call alleles by injecting them; > into the ref haplotype, then threading these constructed haplotypes into; > the assembly graph with a large edge weight. There are several drawbacks to; > this approach:; >; > - The strange edge weights interfere with the AdaptiveChainPruner.; > - The large edge weights may not be large enough to avoid pruning when; > depth is extremely high.; > - The alleles may be lost if assembly fails.; > - If the alleles actually exist but are in phase with another variant; > we end up putting an enormous amount of weight on a false haplotype.; >; > We can get around these issue with the following method:; >; > - assemble haplotypes without regard to the force-called alleles.; > - if an allele is present in these haplotypes, do nothing further.; > - otherwise, add a haplotype in which the allele is injected into the; > reference haplotype.; >; > @LeeTL1220 <https://github.com/LeeTL1220> I prototyped this and it seems; > to resolve the missed forced alleles that Ziao found.; >; > @ldgauthier <https://github.com/ldgauthier> Can you think of any; > objections to making this change in HaplotypeCaller?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/5857>, or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AGRhdMcaTJg47gn",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5857#issuecomment-479916767:938,avoid,avoid,938,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5857#issuecomment-479916767,1,['avoid'],['avoid']
Safety,"Yup, as you might recall from some discussions with @bhanugandham and @mwalker174, getting automated pipeline-level CNV evaluations up and running was highest on my list before I handed over the role and went on paternity leave. I think these tests would be more useful than unit/integration tests for correctness, but they would almost certainly have to run on CARROT. That said, the current level of unit/integration test coverage is a bit different from that for the somatic tools, because 1) it's difficult to run gCNV in any useful way on Travis infrastructure, and 2) we hadn't decided on a framework/convention for python unit tests at the time the production code went in (although I think @ldgauthier has added some python unit tests by now). So we currently only have plumbing WDL/integration tests on very small data for gCNV---and these only test that the tools run, not for correctness. For somatic CNV, we have unit tests for correctness on small simulated data (e.g., for things like segmentation and modeling classes), but integration tests don't cover correctness (and it would be pretty redundant to use the same simulated data for integration, so I'd rather put effort towards pipeline-level tests on real data). It might be good for you and @mwalker174 to review the current level of testing coverage and understand where things need to be shored up---happy to discuss more.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7261#issuecomment-859563124:1105,redund,redundant,1105,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7261#issuecomment-859563124,1,['redund'],['redundant']
Safety,"_Ab.passed.vcf.gz; Using GATK jar /gatk/gatk-package-4.1.8.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx12G -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -jar /gatk/gatk-package-4.1.8.1-local.jar SelectVariants -R /scratch/DBC/BCRBIOIN/SHARED/genomes/homo_sapiens/GRCh38/dna/GRCh38.d1.vd1.fa -V /scratch/DBC/BCRBIOIN/SHARED/analysis/######/######/data/wgs/data/mutect2//tumour_samples/ML13_Ab/ML13_Ab.orientation_filtered.vcf.gz --exclude-filtered -O /scratch/DBC/BCRBIOIN/SHARED/analysis/######/######/data/wgs/data/mutect2//tumour_samples/ML13_Ab/ML13_Ab.passed.vcf.gz; 10:52:40.838 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Sep 02, 2020 10:52:41 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 10:52:41.263 INFO SelectVariants - ------------------------------------------------------------; 10:52:41.263 INFO SelectVariants - The Genome Analysis Toolkit (GATK) v4.1.8.1; 10:52:41.264 INFO SelectVariants - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:52:41.264 INFO SelectVariants - Executing as ######@dav002.prv.davros.compute.estate on Linux v3.10.0-327.3.1.el7.x86_64 amd64; 10:52:41.264 INFO SelectVariants - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 10:52:41.265 INFO SelectVariants - Start Date/Time: September 2, 2020 10:52:40 AM GMT; 10:52:41.265 INFO SelectVariants - ------------------------------------------------------------; 10:52:41.265 INFO SelectVariants - ------------------------------------------------------------; 10:52:41.266 INFO SelectVariants - HTSJDK Version: 2.23.0; 10:52:41.266 INFO SelectVariants - Picard Version: 2.22.8; 10",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6446#issuecomment-685695328:2194,detect,detect,2194,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6446#issuecomment-685695328,1,['detect'],['detect']
Safety,"_READ_FOR_SAMTOOLS : false; 13:40:59.660 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 13:40:59.661 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 13:40:59.661 INFO GenotypeGVCFs - Deflater: IntelDeflater; 13:40:59.661 INFO GenotypeGVCFs - Inflater: IntelInflater; 13:40:59.661 INFO GenotypeGVCFs - GCS max retries/reopens: 20; 13:40:59.662 INFO GenotypeGVCFs - Requester pays: disabled; 13:40:59.663 INFO GenotypeGVCFs - Initializing engine; 13:40:59.909 INFO FeatureManager - Using codec VCFCodec to read file file:///omics/groups/OE0540/internal/users/gleixner/cropseq_uli/rep_ex/test3/output.g.vcf; 13:40:59.951 INFO GenotypeGVCFs - Done initializing engine; 13:41:00.006 INFO ProgressMeter - Starting traversal; 13:41:00.007 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 13:41:00.406 WARN ReferenceConfidenceVariantContextMerger - Detected invalid annotations: When trying to merge variant contexts at location chr19:55910646 the annotation MLEAC=[1, 0] was not a numerical value and was ignored; 13:41:00.476 WARN InbreedingCoeff - InbreedingCoeff will not be calculated at position chr19:55910646 and possibly subsequent; at least 10 samples must have called genotypes; 13:41:00.528 INFO ProgressMeter - unmapped 0.0 37 4277.5; 13:41:00.528 INFO ProgressMeter - Traversal complete. Processed 37 total variants in 0.0 minutes.; 13:41:00.580 INFO GenotypeGVCFs - Shutting down engine; [October 26, 2023 at 1:41:00 PM CEST] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 0.02 minutes.; Runtime.totalMemory()=285212672; ```; ```; ##source=HaplotypeCaller; ##bcftools_viewVersion=1.16+htslib-1.16; ##bcftools_viewCommand=view -c1 output.vcf; Date=Thu Oct 26 13:41:08 2023; ##bcftools_annotateVersion=1.16+htslib-1.16; ##bcftools_annotateCommand=annotate -x INFO,FORMAT/SB,FORMAT/PL; Date=Thu Oct 26 13:41:08 2023; #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT CGAAG",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5727#issuecomment-1781017195:19312,Detect,Detected,19312,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5727#issuecomment-1781017195,1,['Detect'],['Detected']
Safety,"_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/proj/bin/conda/envs/D_UMI_APJ/share/gatk4-4.1.8.0-0/gatk-package-4.1.8.0-local.jar Mutect2 -R /home/proj/stage/cancer/reference/GRCh37/genome/human_g1k_v37_decoy.fasta -L /home/proj/stage/cancer/reference/target_capture_bed/production/balsamic/gicfdna_3.1_hg19_design.bed -I consensus/concatenated_ACC5611A5_XXXXXX_consensusalign_ss_r2.bam --germline-resource /home/proj/stage/cancer/reference/GRCh37/variants/dbsnp_grch37_b138.vcf.gz -O mutect2/concatenated_ACC5611A5_XXXXXX_mutect2_unfiltered_ss_r2.vcf.gz; 11:47:50.850 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/proj/bin/conda/envs/D_UMI_APJ/share/gatk4-4.1.8.0-0/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jul 02, 2020 11:47:51 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 11:47:51.054 INFO Mutect2 - ------------------------------------------------------------; 11:47:51.055 INFO Mutect2 - The Genome Analysis Toolkit (GATK) v4.1.8.0; 11:47:51.055 INFO Mutect2 - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:47:51.055 INFO Mutect2 - Executing as ashwini.jeggari@compute-0-0.local on Linux v3.10.0-1062.4.1.el7.x86_64 amd64; 11:47:51.055 INFO Mutect2 - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_152-release-1056-b12; 11:47:51.055 INFO Mutect2 - Start Date/Time: July 2, 2020 11:47:50 AM CEST; 11:47:51.056 INFO Mutect2 - ------------------------------------------------------------; 11:47:51.056 INFO Mutect2 - ------------------------------------------------------------; 11:47:51.056 INFO Mutect2 - HTSJDK Version: 2.22.0; 11:47:51.056 INFO Mutect2 - Picard Version: 2.22.8; 11:47:51.056 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 11:47:51.056 INFO Mutect2 ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-652912482:1653,detect,detect,1653,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-652912482,1,['detect'],['detect']
Safety,"`CombineGVCFs` scales exponentially (claim made by @eitanbanks), and we would like to see the solution scale an order of magnitude faster (up to 100K gvcfs). Another acceptable use case is to achieve the same results by removing `CombineGVCFs` from the gvcf creation to genotype gvcfs process, reducing the run time of that process by the same amount as if you had scaled `CombineGVCFs`. Basically, I am trying to avoid telling you how.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/366#issuecomment-93498826:414,avoid,avoid,414,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/366#issuecomment-93498826,1,['avoid'],['avoid']
Safety,`IntervalUtils` already exists -- I'd be fine with moving it there as a compromise. We just need to avoid the classic GATK3 mistake of embedding general utilities within tools.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1497#issuecomment-185888586:100,avoid,avoid,100,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1497#issuecomment-185888586,1,['avoid'],['avoid']
Safety,`SafeDoFn` seems to be the winner -- let's implement it,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/702#issuecomment-126427528:1,Safe,SafeDoFn,1,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/702#issuecomment-126427528,1,['Safe'],['SafeDoFn']
Safety,aW9ycy5qYXZh) | `85.366% <0%> (ø)` | `8% <0%> (?)` | |; | [...detectcoveragedropout/CoverageDropoutDetector.java](https://codecov.io/gh/broadinstitute/gatk/pull/3043?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9kZXRlY3Rjb3ZlcmFnZWRyb3BvdXQvQ292ZXJhZ2VEcm9wb3V0RGV0ZWN0b3IuamF2YQ==) | `91.803% <0%> (ø)` | `20% <0%> (?)` | |; | [...e/detectcoveragedropout/DetectCoverageDropout.java](https://codecov.io/gh/broadinstitute/gatk/pull/3043?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9kZXRlY3Rjb3ZlcmFnZWRyb3BvdXQvRGV0ZWN0Q292ZXJhZ2VEcm9wb3V0LmphdmE=) | `84% <0%> (ø)` | `4% <0%> (?)` | |; | [...ellbender/tools/exome/DecomposeSingularValues.java](https://codecov.io/gh/broadinstitute/gatk/pull/3043?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9EZWNvbXBvc2VTaW5ndWxhclZhbHVlcy5qYXZh) | `89.474% <0%> (ø)` | `5% <0%> (?)` | |; | [...e/detectcoveragedropout/CoverageDropoutResult.java](https://codecov.io/gh/broadinstitute/gatk/pull/3043?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9kZXRlY3Rjb3ZlcmFnZWRyb3BvdXQvQ292ZXJhZ2VEcm9wb3V0UmVzdWx0LmphdmE=) | `92.593% <0%> (ø)` | `17% <0%> (?)` | |; | [.../broadinstitute/hellbender/utils/tsv/DataLine.java](https://codecov.io/gh/broadinstitute/gatk/pull/3043?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90c3YvRGF0YUxpbmUuamF2YQ==) | `91.453% <0%> (+0.855%)` | `63% <0%> (+1%)` | :arrow_up: |; | [...itute/hellbender/tools/walkers/SplitIntervals.java](https://codecov.io/gh/broadinstitute/gatk/pull/3043?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL1NwbGl0SW50ZXJ2YWxzLmphdmE=) | `90.625% <0%> (+2.39%)` | `12% <0%> (+6%)` | :arrow_up: |; | [...ols/exome/alleliccount/AllelicCountCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/3043?src=pr&el=tree#diff-c,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3043#issuecomment-306585614:2144,detect,detectcoveragedropout,2144,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3043#issuecomment-306585614,1,['detect'],['detectcoveragedropout']
Safety,ang.Class.getDeclaredMethods0; 0.0% 0 + 1 java.io.FileOutputStream.writeBytes; 0.0% 1 + 0 java.nio.HeapCharBuffer.<init>; 0.0% 1 + 0 java.nio.HeapIntBuffer.<init>; 0.0% 1 + 0 org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetDataEncryptionKeyRequestProto.newBuilder; 0.0% 1 + 0 java.util.IdentityHashMap$Values.size; 0.0% 1 + 0 org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.<clinit>; 0.0% 1 + 0 org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$Rename2RequestProto.<clinit>; 0.0% 1 + 0 org.bdgenomics.adam.models.SingleReadBucketSerializer.<init>; 0.0% 1 + 0 java.util.regex.Pattern$Branch.<init>; 0.0% 1 + 0 hbparquet.hadoop.util.ContextUtil.getConfiguration; 0.0% 1 + 0 sun.reflect.generics.visitor.Reifier.reifyTypeArguments; 0.0% 1 + 0 scala.collection.immutable.Map$Map3.<init>; 0.0% 1 + 0 sun.reflect.ByteVectorImpl.trim; 0.6% 62 + 27 Total interpreted (including elided). Compiled + native Method ; 9.5% 1441 + 0 com.ning.compress.lzf.impl.UnsafeChunkEncoderLE.tryCompress; 2.0% 304 + 0 htsjdk.samtools.util.BlockCompressedOutputStream.write; 1.8% 275 + 0 com.ning.compress.lzf.impl.UnsafeChunkDecoder.decodeChunk; 1.2% 176 + 2 htsjdk.samtools.BinaryTagCodec.readTags; 1.2% 176 + 0 org.broadinstitute.hellbender.relocated.com.google.common.collect.Multimaps.index; 1.1% 174 + 1 htsjdk.samtools.SAMUtils.bytesToCompressedBases; 0.6% 82 + 2 htsjdk.samtools.BAMRecordCodec.encode; 0.5% 83 + 0 org.broadinstitute.hellbender.relocated.com.google.common.collect.AbstractMapBasedMultimap.put; 0.5% 82 + 0 java.util.Iterator.forEachRemaining; 0.5% 74 + 0 org.apache.spark.util.collection.TimSort$SortState.mergeLo; 0.5% 70 + 1 org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSparkUtils.lambda$markPairedEnds$66146993$1; 0.4% 65 + 0 java.util.stream.ReferencePipeline.collect; 0.4% 64 + 0 htsjdk.samtools.util.BlockCompressedInputStream.read; 0.4% 56 + 2 org.apache.spark.util.collection.TimSort$SortState.mergeHi; 0.3% 47 + 0,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1702#issuecomment-210127581:2316,Unsafe,UnsafeChunkEncoderLE,2316,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1702#issuecomment-210127581,1,['Unsafe'],['UnsafeChunkEncoderLE']
Safety,"aplotypeCaller to create the gvcf just the reblock. . ```; Using GATK jar /share/pkg.7/gatk/4.2.2.0/install/gatk-4.2.2.0/gatk-package-4.2.2.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /share/pkg.7/gatk/4.2.2.0/install/gatk-4.2.2.0/gatk-package-4.2.2.0-local.jar ReblockGVCF -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -V gvcf.gather/GARDWGSN00001.autosome.g.vcf.gz -drop-low-quals -rgq-threshold 20 -do-qual-approx -O g1.test.reblock.g.vcf.gz; 00:54:40.318 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.2.0/install/gatk-4.2.2.0/gatk-package-4.2.2.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Aug 25, 2021 12:54:40 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 00:54:40.501 INFO ReblockGVCF - ------------------------------------------------------------; 00:54:40.501 INFO ReblockGVCF - The Genome Analysis Toolkit (GATK) v4.2.2.0; 00:54:40.501 INFO ReblockGVCF - For support and documentation go to https://software.broadinstitute.org/gatk/; 00:54:40.501 INFO ReblockGVCF - Executing as farrell@scc-hadoop.bu.edu on Linux v3.10.0-1160.36.2.el7.x86_64 amd64; 00:54:40.502 INFO ReblockGVCF - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 00:54:40.502 INFO ReblockGVCF - Start Date/Time: August 25, 2021 12:54:40 AM EDT; 00:54:40.502 INFO ReblockGVCF - ------------------------------------------------------------; 00:54:40.502 INFO ReblockGVCF - ------------------------------------------------------------; 00:54:40.503 INFO ReblockGVCF - HTSJDK Version: 2.24.1; 00:54:40.503 INFO ReblockGVCF - Picard Version: 2.25.4; 00:54:40.503 INFO ReblockGVCF - Built for Spark Version: 2.4.5",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7334#issuecomment-905183643:1086,detect,detect,1086,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7334#issuecomment-905183643,1,['detect'],['detect']
Safety,"aps pushing a fresh branch to this repo might make it a little easier for us to check it out for review---again, not a big deal, so I'll leave it up to you. 2) We try to adhere to the Google style guide https://google.github.io/styleguide/javaguide.html, so the review may yield a lot of seemingly minor and nitpicky change requests. Don't take these personally---the goal is just to make the code base as uniform and easy to maintain as possible! If you prefer, I'm sure we can find a GATK developer to take a quick once over of your branch and make these minor changes. 3) Since the new tool borrows so heavily from CollectAllelicCounts, I think it might be worth consolidating shared code and reducing code duplication---again, with the goal of making future maintenance more straightforward. I'll try to identify some places this can be done during my review. Again, we can make these changes on our end during the once over, or you can address them after the review (or we could also do this on our end in a separate PR after this one goes in). 4) In the near future, I think we should finally make the effort to replace both GetPileupSummaries and CollectAllelicCounts with this new tool. As mentioned in our email thread, @davidbenjamin and I discussed this long ago, e.g. https://github.com/broadinstitute/gatk/issues/4717#issuecomment-386734926. From a methods perspective, we'd simply need expand the current functionality of your tool to also report the reference allele and do some quick sanity checks to make sure that the differences in count definition and read filtering don't have any undesired downstream effects. However, as we also discussed, this will come with some additional overhead---we'll need to update documentation, workshop slides, tutorials, WDLs, and make sure that any changes in output formats are clearly highlighted in the release notes. I'll leave this effort to @davidbenjamin and @mwalker174. Thanks again for doing this. Let us know how you'd like to proceed!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6543#issuecomment-610462293:1851,sanity check,sanity checks,1851,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6543#issuecomment-610462293,1,['sanity check'],['sanity checks']
Safety,assigning to me to avoid confusion but @cmnbroad is working on it.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/419#issuecomment-114319818:19,avoid,avoid,19,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/419#issuecomment-114319818,1,['avoid'],['avoid']
Safety,"bble=false -Dsamjdk.compression_level=2 -jar /app/gatk-package-4.1.8.0-local.jar FilterAlignmentArtifacts -V /output/sample.FilterMutectCalls.vcf.gz -R /db/hs37d5.fa --bwa-mem-index-image /db/hg38.fa.img -I /output/sample.Mutect2.bam -O sample.somatic_filter.test.vcf.gz --use-jdk-inflater true; 19:11:56.929 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/app/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 19:11:56.943 INFO NativeLibraryLoader - Loading libgkl_smithwaterman.so from jar:file:/app/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_smithwaterman.so; 19:11:56.944 INFO SmithWatermanAligner - Using AVX accelerated SmithWaterman implementation; 19:11:57.168 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/app/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jul 19, 2020 7:11:57 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 19:11:57.324 INFO FilterAlignmentArtifacts - ------------------------------------------------------------; 19:11:57.324 INFO FilterAlignmentArtifacts - The Genome Analysis Toolkit (GATK) v4.1.8.0; 19:11:57.325 INFO FilterAlignmentArtifacts - For support and documentation go to https://software.broadinstitute.org/gatk/; 19:11:57.325 INFO FilterAlignmentArtifacts - Executing as foo@bar.local on Linux v2.6.32-696.6.3.el6.x86_64 amd64; 19:11:57.325 INFO FilterAlignmentArtifacts - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_261-b12; 19:11:57.325 INFO FilterAlignmentArtifacts - Start Date/Time: July 19, 2020 7:11:57 PM CST; 19:11:57.325 INFO FilterAlignmentArtifacts - ------------------------------------------------------------; 19:11:57.325 INFO FilterAlignmentArtifacts - ------------------------------------------------------------; 19:11:57.325 INFO FilterAlignmentArtifacts - HTSJDK Version: 2.22.0; 19:11:57.325 INFO ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-660645356:1602,detect,detect,1602,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-660645356,1,['detect'],['detect']
Safety,"breakpoint}, where the complication is mainly used for two purposes: 1) adjusting the exact locations of the breakpoints, and 2) useful for downstream annotations for the associated VCF records.; * NARL is constructed from an input CA, which in turn has two ways of being constructed: 1) deliberate construction with two neighboring alignments on the contig, 2) construction from an input contig whose alignments are scanned through in a semi pair-wise fashion. The second way is the master version currently in use in out pipeline, and is planned to be phased out eventually. Note that the second way extracts neighboring alignments that it considers good quality and send the information to the first version for actual construction. ; * The ctor for CA that takes two neighboring alignments also takes in a string representation of mapping locations of suspected insertions. In master version, this information is extracted from the alignments that are considered not strong enough, e.g. lower MQ, shorter alignment length, but the actual inserted sequence is not extracted in CA, but rather in BC.; * The BC field in NARL, holding inserted sequence, micro-homology and other information (e.g. for duplication), does not contain where the inserted sequence, if any, is mapped, and the inserted sequence is extracted from the distance on the contig between two neighboring alignments stored in the input CA.; * The variant, after its NARL locations are pinned down, is annotated by information stored in both CA and BC, where the information for BC is critical for reconstructing the alt haplotype and those in CA mainly for evaluating evidence strength. . Hence we should put the suspected inserted sequence and mapping in the same class, proposed to be in BC. But I think this PR is getting bigger than it should be, and the cigar operation is what I now need for the graph-based cpx sv detection. So I am thinking of creating a ticket for this issue, and follow up in a month. What you do think?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3464#issuecomment-331931810:2326,detect,detection,2326,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3464#issuecomment-331931810,1,['detect'],['detection']
Safety,c.Unsafe.defineClass; 0.0% 2 + 0 sun.reflect.MethodAccessorGenerator.emitInvoke; 0.0% 2 + 0 org.apache.spark.deploy.SparkHadoopUtil$$anonfun$2$$anonfun$apply$mcJ$sp$2.apply; 0.0% 2 + 0 java.util.IdentityHashMap.resize; 0.0% 0 + 1 java.lang.Runtime.availableProcessors; 0.0% 0 + 1 java.io.FileInputStream.available; 0.0% 0 + 1 sun.management.GarbageCollectorImpl.getCollectionTime; 0.0% 0 + 1 java.lang.Thread.setPriority0; 0.0% 0 + 1 sun.nio.ch.FileDispatcherImpl.read0; 0.0% 0 + 1 sun.nio.ch.Net.socket0; 0.0% 1 + 0 org.seqdoop.hadoop_bam.SAMRecordWritable.set; 0.0% 1 + 0 org.apache.spark.util.collection.SizeTrackingAppendOnlyMap.org$apache$spark$util$collection$SizeTracker$$numUpdates; 0.0% 1 + 0 java.util.IdentityHashMap$Values.toArray; 0.0% 1 + 0 io.netty.buffer.AbstractByteBufAllocator.heapBuffer; 0.0% 1 + 0 sun.net.www.protocol.http.Handler.openConnection; 1.0% 106 + 55 Total interpreted (including elided). Compiled + native Method ; 13.7% 2116 + 2 com.ning.compress.lzf.impl.UnsafeChunkEncoderLE.tryCompress; 2.7% 409 + 0 htsjdk.samtools.util.BlockCompressedOutputStream.write; 2.4% 373 + 2 com.ning.compress.lzf.impl.UnsafeChunkDecoder.decodeChunk; 1.4% 214 + 0 htsjdk.samtools.SAMUtils.bytesToCompressedBases; 1.4% 211 + 0 htsjdk.samtools.BinaryTagCodec.readTags; 1.0% 147 + 3 htsjdk.samtools.BAMRecordCodec.encode; 0.7% 108 + 0 org.broadinstitute.hellbender.relocated.com.google.common.collect.ImmutableMultimap$Builder.build; 0.6% 99 + 0 java.util.Iterator.forEachRemaining; 0.6% 84 + 2 org.apache.spark.util.collection.TimSort$SortState.mergeLo; 0.5% 82 + 0 org.broadinstitute.hellbender.relocated.com.google.common.collect.AbstractMapBasedMultimap.put; 0.5% 73 + 2 scala.collection.Iterator$$anon$13.hasNext; 0.4% 68 + 0 htsjdk.samtools.BinaryTagCodec.readSingleValue; 0.4% 66 + 0 org.broadinstitute.hellbender.utils.read.markduplicates.OpticalDuplicateFinder.getRapidDefaultReadNameRegexSplit; 0.4% 63 + 1 java.util.stream.ReferencePipeline.collect; 0.4% 61 + 0 org.broadinstitut,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1702#issuecomment-210127581:7254,Unsafe,UnsafeChunkEncoderLE,7254,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1702#issuecomment-210127581,1,['Unsafe'],['UnsafeChunkEncoderLE']
Safety,converted to a draft to make sure I don't merge before it's safe (due to call caching),MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8930#issuecomment-2258545439:60,safe,safe,60,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8930#issuecomment-2258545439,1,['safe'],['safe']
Safety,"d should not be used for production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 19:11:57.326 INFO FilterAlignmentArtifacts - Initializing engine; 19:11:57.666 INFO FeatureManager - Using codec VCFCodec to read file file:///output/sample.FilterMutectCalls.vcf.gz; 19:11:57.757 INFO FilterAlignmentArtifacts - Done initializing engine; 19:11:57.827 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/app/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 19:11:57.861 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 19:11:57.862 INFO IntelPairHmm - Available threads: 4; 19:11:57.862 INFO IntelPairHmm - Requested threads: 4; 19:11:57.862 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 19:11:57.862 INFO ProgressMeter - Starting traversal; 19:11:57.862 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; *** glibc detected *** /for/bar/bin/java: double free or corruption (out): 0x00007f450af58700 ***; ======= Backtrace: =========; /lib64/libc.so.6(+0x3d01675dee)[0x7f45058afdee]; /lib64/libc.so.6(+0x3d01678c80)[0x7f45058b2c80]; /tmp/libgkl_smithwaterman410767516409374085.so(_Z19runSWOnePairBT_avx2iiiiPhS_iiaPcPs+0x338)[0x7f4499f4cfa8]; /tmp/libgkl_smithwaterman410767516409374085.so(Java_com_intel_gkl_smithwaterman_IntelSmithWaterman_alignNative+0xd8)[0x7f4499f4cbf8]; [0x7f44f58be6a2]; ======= Memory map: ========; ```. Then we **disabled** AVX2 in the newer cluster using Intels [sde64](https://software.intel.com/en-us/articles/intel-software-development-emulator) with `-ivb`, which directed GATK to use the Java implementation, and the filter worked without core dump. ```; sde64 -ivb -- faa.sh; Using GATK jar /app/gatk-package-4.1.8.0-local.jar; Running:; /bin/java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_le",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-660645356:4472,detect,detected,4472,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-660645356,1,['detect'],['detected']
Safety,"da3/envs/cerc_prod/share/gatk4-4.1.7.0-0/gatk-package-4.1.7.0-local.jar MergeVcfs -I data/calling/erc_prod2.SM_V7_1.vcf.gz -I data/calling/cerc_prod2.SM_V7_ZW.vcf.gz -O out.vcf.gz; 17:06:37.645 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/master/xxxxxxx/local/pckg/python/miniconda3/envs/cerc_prod/share/gatk4-4.1.7.0-0/gatk-package-4.1.7.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; [Mon Jun 22 17:06:37 CDT 2020] MergeVcfs --INPUT data/calling/erc_prod2.SM_V7_1.vcf.gz --INPUT data/calling/cerc_prod2.SM_V7_ZW.vcf.gz --OUTPUT out.vcf.gz --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX true --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false; Jun 22, 2020 5:06:37 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; [Mon Jun 22 17:06:37 CDT 2020] Executing as xxxxxxx@yyyyyy on Linux 3.10.0-693.11.1.el7.x86_64 amd64; OpenJDK 64-Bit Server VM 1.8.0_152-release-1056-b12; Deflater: Intel; Inflater: Intel; Provider GCS is available; Picard version: Version:4.1.7.0; [Mon Jun 22 17:06:37 CDT 2020] picard.vcf.MergeVcfs done. Elapsed time: 0.00 minutes.; Runtime.totalMemory()=1249378304; To get help, see http://broadinstitute.github.io/picard/index.html#GettingHelp; htsjdk.samtools.SAMException: Cannot read non-existent file: file:///data/infectious/schistosome/tmp/test%20a/data/calling/erc_prod2.SM_V7_1.vcf.gz; at htsjdk.samtools.util.IOUtil.assertFileIsReadable(IOUtil.java:498); at htsjdk.samtools.util.IOUtil.assertFileIsReadable(IOUtil.java:485); at picard.vcf.MergeVcfs.doWork(MergeVcfs.java:173); at picard.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:305); at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgramEx",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6664#issuecomment-647808241:5825,detect,detect,5825,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6664#issuecomment-647808241,1,['detect'],['detect']
Safety,"dec to read file file:///orange/reed/nhouse/Raw_seqs/SEQ1_samples/SEQ1_gvcf/renamed_seq1trimq10_LHA_AS02_1.raw_variants.g.vcf; 11:30:53.805 INFO FeatureManager - Using codec VCFCodec to read file file:///orange/reed/nhouse/Raw_seqs/SEQ1_samples/SEQ1_gvcf/renamed_seq1trimq10_LHA_AS184_1.raw_variants.g.vcf; 11:30:53.894 INFO FeatureManager - Using codec VCFCodec to read file file:///orange/reed/nhouse/Raw_seqs/SEQ1_samples/SEQ1_gvcf/renamed_seq1trimq10_LHA_AS201_1.raw_variants.g.vcf; 11:30:54.000 INFO FeatureManager - Using codec VCFCodec to read file file:///orange/reed/nhouse/Raw_seqs/SEQ1_samples/SEQ1_gvcf/renamed_seq1trimq10_LHA_AS209_1.raw_variants.g.vcf; 11:34:25.030 INFO CombineGVCFs - Done initializing engine; 11:34:25.154 INFO ProgressMeter - Starting traversal; 11:34:25.155 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 11:34:25.473 WARN ReferenceConfidenceVariantContextMerger - Detected invalid annotations: When trying to merge variant contexts at location DS235882:44 the annotation MLEAC=[1, 0] was not a numerical value and was ignored; 11:34:25.944 INFO CombineGVCFs - Shutting down engine; [October 26, 2020 11:34:25 AM EDT] org.broadinstitute.hellbender.tools.walkers.CombineGVCFs done. Elapsed time: 3.59 minutes.; Runtime.totalMemory()=3738173440; java.lang.NumberFormatException: empty String; 	at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:1842); 	at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); 	at java.lang.Double.parseDouble(Double.java:538); 	at htsjdk.variant.vcf.VCFUtils.parseVcfDouble(VCFUtils.java:262); 	at htsjdk.variant.vcf.AbstractVCFCodec.createGenotypeMap(AbstractVCFCodec.java:808); 	at htsjdk.variant.vcf.AbstractVCFCodec$LazyVCFGenotypesParser.parse(AbstractVCFCodec.java:121); 	at htsjdk.variant.variantcontext.LazyGenotypesContext.decode(LazyGenotypesContext.java:158); 	at htsjdk.variant.variantcontext.LazyGenotypesContext.ensureSampleNameMap(LazyGenotypesContex",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6766#issuecomment-716640444:6704,Detect,Detected,6704,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6766#issuecomment-716640444,1,['Detect'],['Detected']
Safety,"dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:182); 	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:164); 	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:412); 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64); 	at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 5.0 failed 1 times, most recent failure: Lost task 1.0 in stage 5.0 (TID 12, localhost, executor driver): java.util.ConcurrentModificationException; 	at java.util.ArrayList.sort(ArrayList.java:1464); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.<init>(ReadThreadingAssembler.java:81); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerReadThreadingAssemblerArgumentCollection.makeReadThreadingAssembler(HaplotypeCallerReadThreadingAssemblerArgumentCollection.java:37); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerArgumentCollection.createReadThreadingAssembler(AssemblyBasedCallerArgumentCollection.java:36); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.initialize(HaplotypeCallerEngine.java:231); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.<init>(HaplotypeCal",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429:7873,abort,aborted,7873,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429,1,['abort'],['aborted']
Safety,"dk.samtools.cram.CRAMException (Reference sequence MD5 mismatch for slice: sequence id 0, start 160972515, span 170618, expected MD5 0cc5e1f5ec5c1b06d5a4bd5fff11b77f) [duplicate 1]; 2019-01-07 11:34:12 INFO TaskSetManager:54 - Starting task 3.3 in stage 0.0 (TID 11, scc-q21.scc.bu.edu, executor 1, partition 3, NODE_LOCAL, 7992 bytes); 2019-01-07 11:34:12 INFO TaskSetManager:54 - Lost task 1.3 in stage 0.0 (TID 9) on scc-q21.scc.bu.edu, executor 1: htsjdk.samtools.cram.CRAMException (Reference sequence MD5 mismatch for slice: sequence id 0, start 87545719, span 186383, expected MD5 492a29f6d7d6fcaf8dde06834861e7ae) [duplicate 3]; 2019-01-07 11:34:12 ERROR TaskSetManager:70 - Task 1 in stage 0.0 failed 4 times; aborting job; 2019-01-07 11:34:12 INFO YarnScheduler:54 - Cancelling stage 0; 2019-01-07 11:34:12 INFO YarnScheduler:54 - Stage 0 was cancelled; 2019-01-07 11:34:12 INFO DAGScheduler:54 - ResultStage 0 (count at CountReadsSpark.java:80) failed in 9.293 s due to Job aborted due to stage failure: Task 1 in stage 0.0 failed 4 times, most recent failure: Lost task 1.3 in stage 0.0 (TID 9, scc-q21.scc.bu.edu, executor 1): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 0, start 87545719, span 186383, expected MD5 492a29f6d7d6fcaf8dde06834861e7ae; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:33466,abort,aborted,33466,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,1,['abort'],['aborted']
Safety,"e deleted, and its use `support = range(lo, hi)` should become `support = new IndexRange(lo, hi)`. Then `IntStream.of(support).mapToDouble(___).toArray()` becomes `range.mapToDouble(___)` and `apply(promote(support), ___)` becomes `support.mapToDouble(___)`. * `final double relErr = 1 + pow(10, -7)` should become a `static` constant. * The steps; ```java; final double maxD1 = arrayMax(d1);; final double[] d2 = apply(apply(d1, d -> d - maxD1), d -> exp(d));; final double sumD2 = sum(d2);; return apply(d2, d -> d/sumD2);; ```; inside `dnHyper` are just a home-brewed way to normalize the log-space array `d1`. Let's go throught the steps: 1) find the max. 2) subtract the max -- subtracting a constant in log-space is equivalent to dividing by a constant in real space, and since we're normalizing in the end this constant is arbitrary. It's done for numerical stability. 3) exponentiate to get an unnormalized real-space array. 4) find the sum. 5) divide by the sum to get the normalized result. The log-10 version of this is `MathUtils::normalizeFromLog10ToLinearSpace(d1)`. You could either calculate `d1` in log-10 space or convert it, replacing the above line with `return MathUtils.normalizeFromLog10ToLinearSpace(MathUtils.applyToArrayInPlace(d1, MathUtils::logToLog10))`. The latter option is simpler. * Import static should be avoided except to escape horrible clutter, which is not the case here. * The second argument of `Utils.validateArg(condition, calculated string expression)` should become `Utils.validateArg(condition, () -> calculated string expression)`. In the first version, the expression is calculated *even if* the condition is satisfied, whereas in the second it is only calculated as needed. It's not critical here but it's a good habit to get into. PS I am a zealot of the Boy Scout Rule (always leave the camp site cleaner than you found it). It is a great way to get familiar with a large code base and a great way to make the code more readable for the next person.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266263155:1960,avoid,avoided,1960,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266263155,1,['avoid'],['avoided']
Safety,"e or directory)) [duplicate 1]; 02:34 DEBUG: [kryo] Write: WrappedArray(null); 18/04/24 17:41:53 INFO TaskSetManager: Starting task 1.2 in stage 2.0 (TID 9, xx.xx.xx.25, executor 6, partition 1, PROCESS_LOCAL, 5371 bytes); 18/04/24 17:41:53 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.xx:45142 (size: 6.4 KB, free: 366.3 MB); 18/04/24 17:42:02 INFO TaskSetManager: Lost task 0.3 in stage 2.0 (TID 8) on xx.xx.xx.xx, executor 3: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory)) [duplicate 2]; 18/04/24 17:42:02 ERROR TaskSetManager: Task 0 in stage 2.0 failed 4 times; aborting job; 18/04/24 17:42:02 INFO TaskSchedulerImpl: Cancelling stage 2; 18/04/24 17:42:02 INFO TaskSchedulerImpl: Stage 2 was cancelled; 18/04/24 17:42:02 INFO DAGScheduler: ShuffleMapStage 2 (mapToPair at PSFilter.java:125) failed in 117.782 s due to Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 8, xx.xx.xx.xx, executor 3): org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:33857,abort,aborted,33857,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,1,['abort'],['aborted']
Safety,"e to old PoNs. Segmentation/modeling:; - Instead of separate tools for copy-ratio segmentation (`PerformSegmentation`) and allele-fraction segmentation/union/modeling (`AllelicCNV`), there is now just a single segmentation/modeling tool (`ModelSegments`).; - Input is denoised copy ratio and/or allelic counts. If only one input is provided, then we only model only the corresponding quantity.; - There is no separate allele-fraction workflow. Unlike the old approach, we do not perform any genotyping or modeling before doing kernel segmentation.; - [x] Old code and classes are used for segment union. We should port or possibly replace this with a simple method that uses kernel segmentation. EDIT: Actually, just tried running a WGS sample and this is still a major bottleneck. EDIT 2: Hmm...actually doesn't seem to be an issue on my desktop (compared to my laptop, on which the run hangs here). Will try to track down the source of the discrepancy. EDIT 3: Added segment union based on single-changepoint detection using kernel segmentation.; - [x] Segment union should be replaced by a proper joint kernel segmentation. EDIT: I've added this, but there could be some minor improvements. Right now, only use one het per copy-ratio interval and throw away those off-target/bin. There are a few percent of targets/bins that have more than one het, and we could potentially rescue the off-target/bin hets as well with some care.; - Old code and models are used for modeling. Since the old allele-fraction model only models hets, we perform a `GetHetCoverage`-like binomial genotyping step (and output the results) before modeling. However, instead of assuming the null hypothesis of het (f = 0.5) and accepting a site when we cannot reject the null, we assume the null hypothesis of hom (f = error rate or 1 - error rate) and accept a site when we can reject the null. This entire process is very similar to what @davidbenjamin does in https://github.com/broadinstitute/gatk/pull/3638. We should co",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:4036,detect,detection,4036,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828,1,['detect'],['detection']
Safety,"e. Elapsed time: 2,531.64 minutes.; Runtime.totalMemory()=9711910912; Tool returned:; true; **Calling Variants Attempt**; Using GATK jar /data1/_software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Xmx32g -jar /data1/_software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar GenotypeGVCFs --genomicsdb-shared-posixfs-optimizations --reference /data1/EquCab/_ECA30/Equus_caballus.EquCab3.0.dna_sm.toplevel.fa/ -V gendb://ECA3_GenomicsDB_260/3 -O ECA3_GenomicsDB_260.3.g.vcf.gz; 21:16:35.251 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data1/_software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 17, 2021 9:16:35 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 21:16:35.496 INFO GenotypeGVCFs - ------------------------------------------------------------; 21:16:35.497 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.1.8.1; 21:16:35.497 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 21:16:35.497 INFO GenotypeGVCFs - Executing as ccastane9@andersserver-01.cvm.tamu.edu on Linux v3.10.0-1127.19.1.el7.x86_64 amd64; 21:16:35.497 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_275-b01; 21:16:35.497 INFO GenotypeGVCFs - Start Date/Time: January 17, 2021 9:16:35 PM CST; 21:16:35.497 INFO GenotypeGVCFs - ------------------------------------------------------------; 21:16:35.497 INFO GenotypeGVCFs - ------------------------------------------------------------; 21:16:35.498 INFO GenotypeGVCFs - HTSJDK Version: 2.23.0; 21:16:35.498 INFO GenotypeGVCFs - Picard Version: 2.22.8; 21:16:35.498 INFO GenotypeGVCFs - HTSJ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-761953839:2716,detect,detect,2716,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-761953839,1,['detect'],['detect']
Safety,"e; 19/02/18 16:58:29 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@45c90a05{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 16:58:29.970 INFO PrintVariantsSpark - Shutting down engine; [February 18, 2019 4:58:29 PM UTC] org.broadinstitute.hellbender.tools.spark.pipelines.PrintVariantsSpark done. Elapsed time: 0.34 minutes.; Runtime.totalMemory()=1106771968; org.apache.spark.SparkException: Job aborted due to stage failure: Exception while getting task result: com.esotericsoftware.kryo.KryoException: java.lang.NullPointerException; Serialization trace:; genotypes (htsjdk.variant.variantcontext.VariantContext); interval (org.broadinstitute.hellbender.engine.spark.SparkSharder$PartitionLocatable); 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1872); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.ap",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3840#issuecomment-464825765:9647,abort,abortStage,9647,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3840#issuecomment-464825765,1,['abort'],['abortStage']
Safety,"eSampleOrdering(LazyGenotypesContext.java:205); 	at htsjdk.variant.variantcontext.GenotypesContext.add(GenotypesContext.java:353); 	at htsjdk.variant.variantcontext.GenotypesContext.add(GenotypesContext.java:46); 	at com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:134); 	at com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:40); 	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708); 	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125); 	... 18 more; 19/02/18 16:58:29 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@45c90a05{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 16:58:29.970 INFO PrintVariantsSpark - Shutting down engine; [February 18, 2019 4:58:29 PM UTC] org.broadinstitute.hellbender.tools.spark.pipelines.PrintVariantsSpark done. Elapsed time: 0.34 minutes.; Runtime.totalMemory()=1106771968; org.apache.spark.SparkException: Job aborted due to stage failure: Exception while getting task result: com.esotericsoftware.kryo.KryoException: java.lang.NullPointerException; Serialization trace:; genotypes (htsjdk.variant.variantcontext.VariantContext); interval (org.broadinstitute.hellbender.engine.spark.SparkSharder$PartitionLocatable); 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGSchedul",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3840#issuecomment-464825765:9050,abort,aborted,9050,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3840#issuecomment-464825765,1,['abort'],['aborted']
Safety,ead.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); at org,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:41797,abort,abortStage,41797,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,1,['abort'],['abortStage']
Safety,ecution.ExecuteActionsTaskExecuter$TaskExecution.execute(ExecuteActionsTaskExecuter.java:237); at org.gradle.internal.execution.steps.ExecuteStep.lambda$execute$1(ExecuteStep.java:33); at org.gradle.internal.execution.steps.ExecuteStep.execute(ExecuteStep.java:33); at org.gradle.internal.execution.steps.ExecuteStep.execute(ExecuteStep.java:26); at org.gradle.internal.execution.steps.CleanupOutputsStep.execute(CleanupOutputsStep.java:58); at org.gradle.internal.execution.steps.CleanupOutputsStep.execute(CleanupOutputsStep.java:35); at org.gradle.internal.execution.steps.ResolveInputChangesStep.execute(ResolveInputChangesStep.java:48); at org.gradle.internal.execution.steps.ResolveInputChangesStep.execute(ResolveInputChangesStep.java:33); at org.gradle.internal.execution.steps.CancelExecutionStep.execute(CancelExecutionStep.java:39); at org.gradle.internal.execution.steps.TimeoutStep.executeWithoutTimeout(TimeoutStep.java:73); at org.gradle.internal.execution.steps.TimeoutStep.execute(TimeoutStep.java:54); at org.gradle.internal.execution.steps.CatchExceptionStep.execute(CatchExceptionStep.java:35); at org.gradle.internal.execution.steps.CreateOutputsStep.execute(CreateOutputsStep.java:51); at org.gradle.internal.execution.steps.SnapshotOutputsStep.execute(SnapshotOutputsStep.java:45); at org.gradle.internal.execution.steps.SnapshotOutputsStep.execute(SnapshotOutputsStep.java:31); at org.gradle.internal.execution.steps.CacheStep.executeWithoutCache(CacheStep.java:208); at org.gradle.internal.execution.steps.CacheStep.execute(CacheStep.java:70); at org.gradle.internal.execution.steps.CacheStep.execute(CacheStep.java:45); at org.gradle.internal.execution.steps.BroadcastChangingOutputsStep.execute(BroadcastChangingOutputsStep.java:49); at org.gradle.internal.execution.steps.StoreSnapshotsStep.execute(StoreSnapshotsStep.java:43); at org.gradle.internal.execution.steps.StoreSnapshotsStep.execute(StoreSnapshotsStep.java:32); at org.gradle.internal.execution.steps.RecordOutp,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6466#issuecomment-590387973:8632,Timeout,TimeoutStep,8632,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6466#issuecomment-590387973,2,['Timeout'],['TimeoutStep']
Safety,"ed with single line brackets. I.e. gatk typically uses. ```; if ( something ) {; doThing; } else {; otherThing; }; ```. rather than. ```; if ( something ) ; {; doThing; }; else ; {; otherThing; }; ```. 2) You use a lot of raw iterators, which is fine and is necessary in many cases. In other cases those operations can be written much more succinctly with either a for-each loop, or a stream. i.e. . ```; List<Integer> values;; Iterator itr = iterable.iterator();; while(itr.hasNext()){; Element elem = itr.next();; int value = someFunction(elem); if ( value > SOME_CONSTANT) {; values.append(value); }; }; return values;; ```. can be . ```; return StreamSupport.stream(iterable.spliterator, false); .map( elem -> someFunction(elem)); .filter( value -> value > SOME_CONSTANT ); .collect( Collectors.toList()); ```. We should probably add a utilty function to convert an iterator to a stream directly so we can stream iterators easily even if there is no associated iteratable. . 3) The tools need tests. This is important. 4) It would be good to think about how the tools can be composited into a spark pipline and run without writing intermediate files. . 5) Bitwise operations are a rarity in GATK and many of our users will not be very comfortable with them. Please avoid bit twiddling tricks when possible. When it's not possible (i.e. when you are performing tricks to treat a long as a set of byte pairs) please add detailed explanation to the comments so that readers who are less familiar will be able to follow along. Likewise all magic values should be named constants unless they are extremely obvious. . 6) Avoid state like the plague. Everything that can reasonable be static should be. Some things will require mutable state, but avoid it as much as possible. . Similarly, static mutable objects should be avoided like a plague infected with more plagues. Never expose a static object that could be mutated. (you have a static array, it's ok because it's private and nothing mutates it)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1435#issuecomment-172985394:1831,avoid,avoid,1831,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1435#issuecomment-172985394,4,"['Avoid', 'avoid']","['Avoid', 'avoid', 'avoided']"
Safety,ed=false disableDithering=false maxRuntime=-1 maxRuntimeUnits=MINUTES downsampling_type=BY_SAMPLE downsample_to_fraction=null downsample_to_coverage=1000 baq=OFF baqGapOpenPenalty=40.0 refactor_NDN_cigar_string=false fix_misencoded_quality_scores=false allow_potentially_misencoded_quality_scores=false useOriginalQualities=false defaultBaseQualities=-1 performanceLog=null BQSR=null quantize_quals=0 static_quantized_quals=null round_down_quantized=false disable_indel_quals=false emit_original_quals=false preserve_qscores_less_than=6 globalQScorePrior=-1.0 validation_strictness=SILENT remove_program_records=false keep_program_records=false sample_rename_mapping_file=null unsafe=null disable_auto_index_creation_and_locking_when_reading_rods=false no_cmdline_in_header=false sites_only=false never_trim_vcf_format_field=false bcf=false bam_compression=null simplifyBAM=false disable_bam_indexing=false generate_md5=false num_threads=1 num_cpu_threads_per_data_thread=1 num_io_threads=0 monitorThreadEfficiency=false num_bam_file_handles=null read_group_black_list=null pedigree=[] pedigreeString=[] pedigreeValidationType=STRICT allow_intervals_with_unindexed_bam=false generateShadowBCF=false variant_index_type=DYNAMIC_SEEK variant_index_parameter=-1 reference_window_stop=0 logging_level=INFO log_to_file=null help=false version=false variant=(RodBinding name=variant source=/humgen/gsa-hpprojects/dev/gauthier/AS_GTEx/GTEx_AS/GTEx_AS.recal.multialleleics.AS.recalibrated.vcf) discordance=(RodBinding name= source=UNBOUND) concordance=(RodBinding name= source=UNBOUND) out=/humgen/gsa-hpprojects/dev/gauthier/AS_GTEx/GTEx_AS/GTEx_AS.recal.multialleleics.AS.recalibrated.mixed.vcf sample_name=[] sample_expressions=null sample_file=null exclude_sample_name=[] exclude_sample_file=[] exclude_sample_expressions=[] selectexpressions=[] invertselect=false excludeNonVariants=false excludeFiltered=false preserveAlleles=false removeUnusedAlternates=false restrictAllelesTo=ALL keepOriginalAC=false ,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4252#issuecomment-364237834:4392,unsafe,unsafe,4392,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4252#issuecomment-364237834,1,['unsafe'],['unsafe']
Safety,ed=false disableDithering=false maxRuntime=-1 maxRuntimeUnits=MINUTES downsampling_type=BY_SAMPLE downsample_to_fraction=null downsample_to_coverage=1000 baq=OFF baqGapOpenPenalty=40.0 refactor_NDN_cigar_string=false fix_misencoded_quality_scores=false allow_potentially_misencoded_quality_scores=false useOriginalQualities=false defaultBaseQualities=-1 performanceLog=null BQSR=null quantize_quals=0 static_quantized_quals=null round_down_quantized=false disable_indel_quals=false emit_original_quals=false preserve_qscores_less_than=6 globalQScorePrior=-1.0 validation_strictness=SILENT remove_program_records=false keep_program_records=false sample_rename_mapping_file=null unsafe=null disable_auto_index_creation_and_locking_when_reading_rods=false no_cmdline_in_header=false sites_only=true never_trim_vcf_format_field=false bcf=false bam_compression=null simplifyBAM=false disable_bam_indexing=false generate_md5=false num_threads=1 num_cpu_threads_per_data_thread=1 num_io_threads=0 monitorThreadEfficiency=false num_bam_file_handles=null read_group_black_list=null pedigree=[] pedigreeString=[] pedigreeValidationType=STRICT allow_intervals_with_unindexed_bam=false generateShadowBCF=false variant_index_type=DYNAMIC_SEEK variant_index_parameter=-1 reference_window_stop=0 logging_level=INFO log_to_file=null help=false version=false variant=(RodBinding name=variant source=/humgen/gsa-hpprojects/dev/gauthier/AS_GTEx/GTEx_AS/GTEx_AS.recal.multialleleics.AS.recalibrated.mixed.vcf) discordance=(RodBinding name= source=UNBOUND) concordance=(RodBinding name= source=UNBOUND) out=/humgen/gsa-hpprojects/dev/gauthier/AS_GTEx/VQSR.AStest.input.vcf sample_name=[] sample_expressions=null sample_file=null exclude_sample_name=[] exclude_sample_file=[] exclude_sample_expressions=[] selectexpressions=[] invertselect=false excludeNonVariants=false excludeFiltered=false preserveAlleles=false removeUnusedAlternates=false restrictAllelesTo=ALL keepOriginalAC=false keepOriginalDP=false mendelianViola,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4252#issuecomment-364237834:1285,unsafe,unsafe,1285,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4252#issuecomment-364237834,1,['unsafe'],['unsafe']
Safety,"es. I'll have to do some performance testing to see whether or not this is the case. Will try to get to this within the next few weeks, but the QC project has immediate priority. [...]. Discussed this with Ryan -- we agreed that the right thing to do is to move the enforcement of the hard cap on the total number of reads that can be in an active region from the HC walker to the engine, and have the size of the cap be controlled by a new argument (not dcov). That way you never pay the cost of storing the undownsampled reads for an active region in memory. We'd also have to educate users on exactly what the various downsampling arguments do for active region walkers. [...]. Making the hardcoded per-active-region cap settable from the command line is the easy part -- what seems hard is:; - Determining whether we can avoid storing all undownsampled reads in memory at once without affecting the quality of calls. Currently, as outlined in earlier comments on this ticket, we do a downsampling pass per locus which respects dcov (in LocusIteratorByState) but keep all undownsampled reads in memory anyway (defeating the main purpose of that first pass), then do a second downsampling pass per active region that does not respect dcov (uses the hardcoded per-region limit).; - If we find that we can't avoid storing all of the undownsampled reads in memory at once for some reason, then perhaps the right thing to do would be to completely disable the downsampling pass in LocusIteratorByState for active region traversals, and disallow the -dcov argument for active region walkers. Downsampling would then by controlled solely by the new argument to set the max # of reads per active region.; - Clarifying the meaning of the per-locus DP annotation for the HC given things like realignment of reads to haplotypes and region-based downsampling. ---. Eventually it was agreed that this would never be fixed in Classic GATK but should be taken into account in designing Hellbender's downsampling.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345:6173,avoid,avoid,6173,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345,1,['avoid'],['avoid']
Safety,"executor 1: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory)) [duplicate 1]; 02:34 DEBUG: [kryo] Write: WrappedArray(null); 18/04/24 17:41:53 INFO TaskSetManager: Starting task 1.2 in stage 2.0 (TID 9, xx.xx.xx.25, executor 6, partition 1, PROCESS_LOCAL, 5371 bytes); 18/04/24 17:41:53 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.xx:45142 (size: 6.4 KB, free: 366.3 MB); 18/04/24 17:42:02 INFO TaskSetManager: Lost task 0.3 in stage 2.0 (TID 8) on xx.xx.xx.xx, executor 3: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory)) [duplicate 2]; 18/04/24 17:42:02 ERROR TaskSetManager: Task 0 in stage 2.0 failed 4 times; aborting job; 18/04/24 17:42:02 INFO TaskSchedulerImpl: Cancelling stage 2; 18/04/24 17:42:02 INFO TaskSchedulerImpl: Stage 2 was cancelled; 18/04/24 17:42:02 INFO DAGScheduler: ShuffleMapStage 2 (mapToPair at PSFilter.java:125) failed in 117.782 s due to Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 8, xx.xx.xx.xx, executor 3): org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.ca",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:33597,abort,aborting,33597,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,1,['abort'],['aborting']
Safety,"explain what the splitting index is a bit better and then it will make more sense I think. Spark works by splitting files up into similar sized chunks and passing those chunks to different worker machines. ; Bam files are hard to split nicely into chunks. The way they're structured makes it hard to identify where safe boundaries are to split on. If you don't have a splitting index, we have an algorithm to start reading at essentially random locations and look for safe splitting points, but we've had some issues in the past where you can misidentify a split (which results in a crash) or miss good splits. The splitting index is a precomputed list of split points, which works around the problem of having to find the splits again next time. It's only used by spark tools that load bams, so it won't benefit Mutect2 because that's not built on spark. . We should add some documentation about this somewhere... #4235",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4219#issuecomment-359826965:315,safe,safe,315,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4219#issuecomment-359826965,2,['safe'],['safe']
Safety,"gatk-4.4.0.0 out there and still, `gatk HaplotypeCaller --sample-name 7-Wu-FF ...` bails on me with:. `A USER ERROR has occurred: Argument --sample_name has a bad value: Specified name does not exist in input bam files`. Indeed, there is not `@RG` line at all, how how about adding an extra error/warning message somewhere more above in the code path?. ```; $ samtools view -H 7-Wu-FF.sorted.bam; @HD VN:1.6 SO:coordinate; @SQ SN:7-Wu-FF LN:443; @PG ID:bwa PN:bwa VN:0.7.17-r1188 CL:bwa mem -t 32 -o exact_matches/7-Wu-FF.sam 7-Wu-FF.fasta 7-Wu-FF_R1.fastq.gz 7-Wu-FF_R2.fastq.gz; @PG ID:samtools PN:samtools PP:bwa VN:1.17 CL:samtools sort 7-Wu-FF.sorted.bam 7-Wu-FF.sam; @PG ID:samtools.1 PN:samtools PP:samtools VN:1.17 CL:samtools view -H 7-Wu-FF.sorted.bam; $; ```. Like others I agree that if there are no samples found by GATK then *all* data should be used. I also think that `@SQ SN:` is enough for a sanity check. No need for `@RG SN:` tag. Finally, fix the `--sample_name` with `--sample-name` in the error message output, the old syntax with an underscore is not accepted anymore.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6501#issuecomment-1832051700:910,sanity check,sanity check,910,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6501#issuecomment-1832051700,1,['sanity check'],['sanity check']
Safety,"gets separated from garbage in the 20 threshold graph: ![Screen Shot 2022-01-25 at 4 13 52 PM](https://user-images.githubusercontent.com/16102845/151060728-5a0d4d95-2eb4-4777-a0e9-34b07b2e6196.png). And here is that spot in the 60 threshold graph:; ![Screen Shot 2022-01-25 at 4 16 43 PM](https://user-images.githubusercontent.com/16102845/151061165-fb803312-59b8-48c4-b196-b0e97d2e00ea.png). And here it is in the 1 threshold ; <img width=""303"" alt=""Screen Shot 2022-01-25 at 4 22 17 PM"" src=""https://user-images.githubusercontent.com/16102845/151061909-25d41a3d-39c2-461e-8fd3-938e859ef3d7.png"">; graph:. This seems to have caused the two thresholds to assemble different haplotypes after dangling end recovery (since all of these are dangling ends because the assembly engine can't do anything else because there is not enough padding provided) and it just so happens this failed assembly misses the correct haplotype in that 20 threshold graph and we end up throwing away most of the reads as incongruent with assembly as a result which is why the depth drops out so low at that site. This is a pretty rare edge case and I happened to be able to recover the 20 mq threshold variant with reasonable correct coverage by playing with the `--min-pruning 4` argument. In general though this issue might or might not have existed if the bam snippet provided (and especially the calling interval you provided of chr7:145945238-145945238) were not centered on one single point since assembly works best and is most likely to succeed when it has a few hundred bases of padding around the variant in question (typically for a SNP we end up with at least 100 bases of active window plus another 300 bases of padding on either side for assembly) which cuts down on the risk of assembly failures like this one. I'm curious if you observed this behavior on the initial variantcalling run of HaplotypeCaller on this data or only when you try scalpel out the SNP at chr7:145945238 for testing in this experiment.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7124#issuecomment-1021629139:1564,recover,recover,1564,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7124#issuecomment-1021629139,2,"['recover', 'risk']","['recover', 'risk']"
Safety,git-blame says that this is quite old code by MdP 2013... and is difficult to recover the history since is before the maven refactoring. Here I would just apply amnesty and remove one of the tests; let's the lack of coverage do the talking. . Of the two test method names `testStartInMiddleWithBubble` seems the most plausible one given the code but I'm not 100% sure about that.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1081#issuecomment-166013743:78,recover,recover,78,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1081#issuecomment-166013743,1,['recover'],['recover']
Safety,"gsutil working is not a good predictor of gatk working. It's possible for default credentials to be wrong but gsutil credentials to be fine at the same time. Here is an example of how to get into this situation:. ```; // set application credentials; gcloud auth login; // unset default credentials (alternatively, forget to set them in the first place); gcloud auth application-default revoke; // gsutil works; gsutil cat $VCF > /dev/null; // GATK does not work; ./gatk-launch SelectVariants --verbosity=DEBUG -V $VCF -L 1:1000-2000 -O /tmp/foo.vcf; A USER ERROR has occurred: Couldn't read file (...); ```. Please make sure to set up Google Cloud access as follows:; ```; $ gcloud auth application-default login; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2415#issuecomment-281526964:29,predict,predictor,29,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2415#issuecomment-281526964,1,['predict'],['predictor']
Safety,"gz \; -V /home/xuql/copyNAM/CML277/CML277ToB73.gvcf.gz \; -V /home/xuql/copyNAM/CML52/CML52ToB73.gvcf.gz \; -V /home/xuql/copyNAM/Il14H/Il14HToB73.gvcf.gz \; -V /home/xuql/copyNAM/Ky21/Ky21ToB73.gvcf.gz \; -V /home/xuql/copyNAM/Mo18W/Mo18WToB73.gvcf.gz \; -V /home/xuql/copyNAM/NC358/NC358ToB73.gvcf.gz \; -V /home/xuql/copyNAM/P39/P39ToB73.gvcf.gz \; -V /home/xuql/copyNAM/CML228/CML228ToB73.gvcf.gz \; -V /home/xuql/copyNAM/CML322/CML322ToB73.gvcf.gz \; -V /home/xuql/copyNAM/CML69/CML69ToB73.gvcf.gz \; -V /home/xuql/copyNAM/Ki11/Ki11ToB73.gvcf.gz \; -V /home/xuql/copyNAM/M162W/M162WToB73.gvcf.gz \; -V /home/xuql/copyNAM/Ms71/Ms71ToB73.gvcf.gz \; -V /home/xuql/copyNAM/Oh43/Oh43ToB73.gvcf.gz \; -V /home/xuql/copyNAM/Tx303/Tx303ToB73.gvcf.gz \; --batch-size 1 \; --genomicsdb-workspace-path /home/xuql/copyNAM/NAM_out_gatk9_1 \; --genomicsdb-segment-size 1048576 --genomicsdb-vcf-buffer-size 50000000 -L 9. #Elapsed time: 52.78 minutes. Runtime.totalMemory()=6761218048; ```. I run `LeftAlignAndTrimVariants` at default parameter, ordered all length of > 200 indels from long to short for all chromosomes. the result is as followed, we found the aborted location nearby the super-indel.; ```; ['10:56:14.335 INFO LeftAlignAndTrimVariants - Indel is too long (34461688) at position 9:3695105; skipping that record. Set --max-indel-length >= 34461688\n',; '10:56:30.429 INFO LeftAlignAndTrimVariants - Indel is too long (10668738) at position 10:33212598; skipping that record. Set --max-indel-length >= 10668738\n',; '10:56:28.937 INFO LeftAlignAndTrimVariants - Indel is too long (9101264) at position 10:14179; skipping that record. Set --max-indel-length >= 9101264\n',; '10:56:30.038 INFO LeftAlignAndTrimVariants - Indel is too long (7918835) at position 10:22996027; skipping that record. Set --max-indel-length >= 7918835\n',; '11:31:49.968 INFO LeftAlignAndTrimVariants - Indel is too long (7154442) at position 6:16715313; skipping that record. Set --max-indel-length >= 7154442\n',; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7976#issuecomment-1367332059:2743,abort,aborted,2743,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7976#issuecomment-1367332059,1,['abort'],['aborted']
Safety,"h the code, but I do have some concerns about the design:. In the SV group's pipeline, we distribute this multi-gig file from its home in the cloud once at cluster-creation time, and then reuse it for multiple client executions. There are no superfluous copies lying about anywhere, and no redundant copying operations. We can give it any name we wish, and put it anywhere we desire (except that the path must be the same on every worker). This code, if I'm reading it correctly, will redistribute the file from a non-permanent home on the master's local file system or on the HDFS (to which it must be copied redundantly at least once per cluster instantiation), and then it will further be redundantly copied to a temporary location on each worker's local file system with every client execution. I don't know if that's overhead that we can live with, or whether that might prevent us from writing clients with brief execution times. I'm just opening the issue for discussion. We also lose a little flexibility in that the image must live in the same directory as the reference, though I don't think that's a serious drawback -- it's a perfectly logical place for it. However, since we're just appending a fixed extension ("".img"") to the reference name we can only have one image file per reference, which may be a problem because different images need to be created for different versions of bwa and for various options such as the list of alt contigs. We can handle the first problem by insisting that all clients on a particular cluster stick to one version of bwa, which is probably a good idea, anyway, but I think we're stuck if clients need to specify various alt contig lists. It might be better to provide a default path of ""ref-name""+"".img"", but allow that default to be overridden. Also, just to twist the knife a bit, it's too bad we never reviewed my PR for gatk-bwamem-jni, which version-stamped the images for safety. It's now languished since July, and has suffered serious bit rot.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3643#issuecomment-333598350:1960,safe,safety,1960,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3643#issuecomment-333598350,1,['safe'],['safety']
Safety,"he 99.9 -tranche 99.5 -tranche 99.0 -tranche 97.0 -tranche 96.0 -tranche 95.0 -tranche 94.0 -tranche 93.5 -tranche 93.0 -tranche 92.0 -tranche 91.0 -tranche 90.0 -an DP -an FS -an MQRankSum -an QD -an ReadPosRankSum -an SOR -mode INDEL --max-gaussians 4 -resource:mills,known=false,training=true,truth=true,prior=12 ~/db/mutect2_support/b37/Mills_and_1000G_gold_standard.indels.b37.sites.vcf.gz -resource:dbsnp,known=true,training=false,truth=false,prior=2 ~/db/mutect2_support/b37/hg19_v0_dbsnp_138.b37.vcf.gz --use-allele-specific-annotations -resource:axiomPoly,known=false,training=true,truth=false,prior=10 ~/db/mutect2_support/b37/Axiom_Exome_Plus.genotypes.all_populations.poly.b37.vcf.gz. 14:58:10.389 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:~/bin/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Nov 12, 2020 2:58:10 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 14:58:10.555 INFO VariantRecalibrator - ------------------------------------------------------------; 14:58:10.555 INFO VariantRecalibrator - The Genome Analysis Toolkit (GATK) v4.1.9.0; 14:58:10.555 INFO VariantRecalibrator - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:58:10.555 INFO VariantRecalibrator - Executing as y@c001 on Linux v3.10.0-957.el7.x86_64 amd64; 14:58:10.555 INFO VariantRecalibrator - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_152-release-1056-b12; 14:58:10.556 INFO VariantRecalibrator - Start Date/Time: November 12, 2020 2:58:10 PM CST; 14:58:10.556 INFO VariantRecalibrator - ------------------------------------------------------------; 14:58:10.556 INFO VariantRecalibrator - ------------------------------------------------------------; 14:58:10.556 INFO VariantRecalibrator - HTSJDK Version: 2.23.0; 14:58:10.556 INFO VariantRecalibrator - Picard Version: 2.23.3; 14",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6701#issuecomment-726406532:1890,detect,detect,1890,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6701#issuecomment-726406532,1,['detect'],['detect']
Safety,"hi @KevinCLydon, @davidbenjamin , thanks for this fix. I am preparing a custom version of 4.1.4.1 with just this patch on top of it. One question I had is: is this a safe patch to have on its own, or does it need to go in tandem with any other patch to form a proper release (and not break anything else)? Given this fix is now ~7 months old, I'm hoping insiders can maybe tell me if any other patch ended up being needed after this one, either to deal with the same issue, or to deal with any new issues introduced by it (if any).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6327#issuecomment-659117371:166,safe,safe,166,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6327#issuecomment-659117371,1,['safe'],['safe']
Safety,"hmm, to avoid _massive_ code duplication, a lot of the PairHMM would need to move to gatk-native-bindings (most of PairHMMModel etc). We could duplicate code but that's just asking for trouble later on. . @droazen what do you want to do?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2030#issuecomment-234610015:8,avoid,avoid,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2030#issuecomment-234610015,1,['avoid'],['avoid']
Safety,"http://docs.travis-ci.com/user/build-timeouts/#Build-times-out-because-no-output-was-received; use that to extend the waiting time.; On Aug 18, 2015 9:13 PM, ""JP Martin"" notifications@github.com wrote:. > Marking cloud test as ""todo"" for now so I can merge.; > ; > —; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/hellbender/pull/812#issuecomment-132408527; > .",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/812#issuecomment-132409715:37,timeout,timeouts,37,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/812#issuecomment-132409715,1,['timeout'],['timeouts']
Safety,https://github.com/broadinstitute/gatk/commit/9a4fb6d4e5fd2a226db820a8b4062c139b66e2ef; There is a standard math variant of log(x) for calculating the common log(1+x). It avoids numerical error for small x.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4980#issuecomment-402623148:171,avoid,avoids,171,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4980#issuecomment-402623148,1,['avoid'],['avoids']
Safety,i'm closing this. I can't detect an improvement in runtime,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1702#issuecomment-210137469:26,detect,detect,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1702#issuecomment-210137469,1,['detect'],['detect']
Safety,id 20 --likelihood-calculation-engine PairHMM --ba; se-quality-score-threshold 18 --dragstr-het-hom-ratio 2 --dont-use-dragstr-pair-hmm-scores false --pair-hmm-gap-continuation-penalty 10 --expected-mismatch-rate-for-read-disqualification 0; .02 --pair-hmm-implementation FASTEST_AVAILABLE --pcr-indel-model CONSERVATIVE --phred-scaled-global-read-mismapping-rate 45 --disable-symmetric-hmm-normalizing false --disable-cap-base-qu; alities-to-map-quality false --enable-dynamic-read-disqualification-for-genotyping false --dynamic-read-disqualification-threshold 1.0 --native-pair-hmm-threads 4 --native-pair-hmm-use-dou; ble-precision false --flow-hmm-engine-min-indel-adjust 6 --flow-hmm-engine-flat-insertion-penatly 45 --flow-hmm-engine-flat-deletion-penatly 45 --pileup-detection false --pileup-detection-; enable-indel-pileup-calling false --num-artificial-haplotypes-to-add-per-allele 5 --artifical-haplotype-filtering-kmer-size 10 --pileup-detection-snp-alt-threshold 0.1 --pileup-detection-i; ndel-alt-threshold 0.5 --pileup-detection-absolute-alt-depth 0.0 --pileup-detection-snp-adjacent-to-assembled-indel-range 5 --pileup-detection-bad-read-tolerance 0.0 --pileup-detection-pro; per-pair-read-badness true --pileup-detection-edit-distance-read-badness-threshold 0.08 --pileup-detection-chimeric-read-badness true --pileup-detection-template-mean-badness-threshold 0.0; --pileup-detection-template-std-badness-threshold 0.0 --bam-writer-type CALLED_HAPLOTYPES --dont-use-soft-clipped-bases false --override-fragment-softclip-check false --min-base-quality-s; core 10 --smith-waterman JAVA --max-mnp-distance 0 --force-call-filtered-alleles false --reference-model-deletion-quality 30 --soft-clip-low-quality-ends false --allele-informative-reads-o; verlap-margin 2 --smith-waterman-dangling-end-match-value 25 --smith-waterman-dangling-end-mismatch-penalty -50 --smith-waterman-dangling-end-gap-open-penalty -110 --smith-waterman-danglin; g-end-gap-extend-penalty -6 --smith-waterman-haplo,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8574#issuecomment-1793390789:6777,detect,detection-i,6777,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8574#issuecomment-1793390789,1,['detect'],['detection-i']
Safety,"implementation is different from the one in `ReadWindowWalker`: first, the overlap between windows is only in one direction; second, `SlidingWindowWalker` is more like a reference/interval walker, from the beginning of the reference (or interval) till the end, it walks in overlapping windows. One example is the following (window-size 10, window-step 5, the - represent the window):. ```; Reference: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _; Windows1: _ _ _ _ _ _ _ _ _ _; Windows2: _ _ _ _ _ _ _ _ _ _; Windows3: _ _ _ _ _ _ _ _ _ _; Windows4: _ _ _ _ _ _ _ _ _ _; Windows5: _ _ _ _ _ _ _ _ _ _; ```. Of course, after having a look to `ReadWindowWalker` I think that several things could be improved in my implementation for a general `SlidingWindowWalker`:; - Apply function similar to the `ReadWindowWalker`, with `ReadWindow` being empty if reads are not provided.; - Three window options: `windowSize` (the actual size of the window), `windowStep` (how much advance for the following window) and `windowPadding` (how much extend the window in both directions). Using this abstraction, `ReadWindowWalker` could be implemented setting `windowSize=windowStep`, and the problem that I need to solve could be implemented setting `windowPadding=0`. The simplest way to acomplish this is to use the current implementation of `ReadWindowWalker` to develop a `SlidingWindowWalker` adding three abstract methods for the three parameters (`getWindowSize()`, `getWindowStep()` and `getWindowPadding()`, and implement `ReadWindowWalker` as a extension of this interface setting `getWindowStep()` to return `getWindowSize()` and `requiresReads()` to true. I can do this once the PR #1567 is accepted and generate the two interfaces (to be sure that the integration with the HC engine is working as expected with the changes), or just implement the `SlidingWindowWalker` and you can include it in the HC PR, or update afterwards to avoid redundancy in the code. What do you think, @droazen?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1528#issuecomment-198438775:2524,avoid,avoid,2524,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1528#issuecomment-198438775,2,"['avoid', 'redund']","['avoid', 'redundancy']"
Safety,"in the count files, and perhaps fail if one is provided for any of the tools (I don’t recall exactly how VCF indexing is triggered by providing one, as seems to be indicated by the tutorial, but hopefully we can disallow external dictionaries while still taking advantage of the relevant engine features for VCF writing). EDIT: Went digging in Slack to try to remind myself of the context of these changes, and found the following PR comment from 1/7 (although it seems to have mysteriously disappeared from GitHub):. > Just so I understand, are we allowing overriding of the sequence dictionary in the shards (and skipping the consistency check) by allowing the parameter --sequence-dictionary to be specified? If so, we might want to document. Otherwise, I'd be inclined to enforce using the sequence dictionary in the shards (and ensuring the consistency check across shards is performed) by changing the null check in getBestAvailableSequenceDictionary to a check that the dictionary has not been set via the command line. EDIT^2: I think I misremembered the details of how #6330 hooked up the sequence dictionary and how getBestAvailableSequenceDictionary in GATKTool works (which probably explains why that comment was deleted...). Now that I actually go back and look, the `--sequence-dictionary` is not hooked up at all, so there is no change to revert in point 4!. Note that after all of this, it will *still* be possible to get into trouble at the gCNV step if you make funky shards (e.g., you could have shard 1 contain intervals from chr1 and chr3, and shard 2 contain intervals from chr2). I don't think it is possible to check for this case early, but you would still fail at PostprocessGermlineCNVCalls as above. Of course, all of these possibilities can be avoided by simply using the WDL, but it will be good to harden checks for those still working at the command line. @ldgauthier @droazen @mwalker174 what do you think? Happy to review later, but OK if I pass this off to you all?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6924#issuecomment-719576249:3960,avoid,avoided,3960,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6924#issuecomment-719576249,1,['avoid'],['avoided']
Safety,"ion methods. Agree. **EDIT**: done in d4d42444986aae848e98d19dfb8a4d3fd8031775. > I'm not sure we should filter out very small and very large variants, especially very large ones. We should get graded on making those calls. Very small ones I can see excluding because by definition they might not be in the truth set, but I don't think that applies to large ones. I agree it is not optimal, particularly for filtering out the huge variants. ; The huge variants come from 2 sources: `<DEL>`, `<INV>`.; For huge deletions, the 50% (or a custom value) reciprocal overlap should classify almost all of them as FP; actually the PacBio call sets (CHM1, CHM13, and the mixture) do not contain any deletion records that are over 30K in size.; For huge inversions though, I am not sure exactly what to do with them. The scripts currently avoid overlap analysis on the inversions actually because IMO we are overly confident in the `<INV>` variants, and the new variant interpretation methods will not emit <INV> records at all&mdash;they become BND's with appripriate annotations and are submitted to a yet-to-be-implemented unit for further interpretation if it is a dispersed duplication or inversion. > What's the reason for storing compiled Rdata objects in with the scripts? I don't necessarily see anything that needs that, and it will make things very hard to maintain. Historical reason. They were used for storing R functions. Will remove them. **EDIT**: done in 546a36465f7860f8c85e28cf40ca8f3851ba9d4c. > `masterVsFeature.sh` appears to set its working directory in the parent directory of where it is run from. If these scripts are in the gatk repo that will end up being in the scripts/sv/evaluation dir and will look like an untracked directory by git there. Its location should be a parameter just like the other working directories. Will do as suggested. **EDIT**: done in 23da9d41667bc21a978134d92f49b65d9af55b35. > Running masterVsFeature should be optional; sometimes we'll just want to run",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4406#issuecomment-368655983:1528,avoid,avoid,1528,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4406#issuecomment-368655983,1,['avoid'],['avoid']
Safety,"is it related to this code in SparkContextFactory?. ```; // remap the Hadoop FS implementation for file:// URIs to RawLocalFileSystem to avoid writing CRC files for local files; .put(""spark.hadoop.fs.file.impl"", ""org.apache.hadoop.fs.RawLocalFileSystem""); ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1389#issuecomment-166357781:137,avoid,avoid,137,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1389#issuecomment-166357781,1,['avoid'],['avoid']
Safety,"is there a ticket number?. On Sun, Apr 19, 2015 at 10:58 PM, Geraldine Van der Auwera <; notifications@github.com> wrote:. > FYI in Classic GATK we are reworking filters so that MalformedReadFilter; > will centralize all these checks including what was in BadCigar. So cigar; > sanity checking will be part of basic read format checking.; > ; > Recommend waiting on this and porting when that is done.; > ; > —; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/hellbender/issues/348#issuecomment-94344636; > .",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/348#issuecomment-94344750:278,sanity check,sanity checking,278,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/348#issuecomment-94344750,1,['sanity check'],['sanity checking']
Safety,is this the answer? https://github.com/broadinstitute/gatk/issues/6370; the doc should then state it to avoid confusion.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6547#issuecomment-612841129:104,avoid,avoid,104,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6547#issuecomment-612841129,1,['avoid'],['avoid']
Safety,"is time there was: ""cannot load book-keeping: Reading MBR failed"" (output below) ; 3. available memory ~89Gb; 4. I am running -Xmx16g java option. Newest output:; Using GATK jar /data1/_software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx16g -jar /data1/_software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar GenotypeGVCFs --genomicsdb-shared-posixfs-optimizations --reference /data1/EquCab/_ECA30/Equus_caballus.EquCab3.0.dna_sm.toplevel.fa/ -V gendb://ECA3_GenomicsDB_260/3 -O ECA3_GenomicsDB_260.3.g.vcf.gz; 16:26:34.912 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data1/_software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 06, 2021 4:26:35 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 16:26:35.417 INFO GenotypeGVCFs - ------------------------------------------------------------; 16:26:35.418 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.1.8.1; 16:26:35.418 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 16:26:35.420 INFO GenotypeGVCFs - Executing as ccastane9@andersserver-01.cvm.tamu.edu on Linux v3.10.0-1127.19.1.el7.x86_64 amd64; 16:26:35.421 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_275-b01; 16:26:35.421 INFO GenotypeGVCFs - Start Date/Time: January 6, 2021 4:26:34 PM CST; 16:26:35.421 INFO GenotypeGVCFs - ------------------------------------------------------------; 16:26:35.421 INFO GenotypeGVCFs - ------------------------------------------------------------; 16:26:35.422 INFO GenotypeGVCFs - HTSJDK Version: 2.23.0; 16:26:35.423 INFO GenotypeGVCFs - Picard Version: 2.22.8; 16:26:35.423 INFO GenotypeGVCFs - HTSJD",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-755760402:1249,detect,detect,1249,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-755760402,1,['detect'],['detect']
Safety,"k.samtools.cram.CRAMException (Reference sequence MD5 mismatch for slice: sequence id 1, start 93925364, span 266689, expected MD5 54babf05a23e9e88a8738dfbb20a1683) [duplicate 2]; 2019-01-09 13:35:56 INFO TaskSetManager:54 - Starting task 4.3 in stage 0.0 (TID 12, scc-q20.scc.bu.edu, executor 2, partition 4, NODE_LOCAL, 7992 bytes); 2019-01-09 13:35:56 INFO TaskSetManager:54 - Lost task 1.3 in stage 0.0 (TID 11) on scc-q20.scc.bu.edu, executor 2: htsjdk.samtools.cram.CRAMException (Reference sequence MD5 mismatch for slice: sequence id 0, start 87545719, span 186383, expected MD5 492a29f6d7d6fcaf8dde06834861e7ae) [duplicate 3]; 2019-01-09 13:35:56 ERROR TaskSetManager:70 - Task 1 in stage 0.0 failed 4 times; aborting job; 2019-01-09 13:35:56 INFO YarnScheduler:54 - Cancelling stage 0; 2019-01-09 13:35:56 INFO YarnScheduler:54 - Stage 0 was cancelled; 2019-01-09 13:35:56 INFO DAGScheduler:54 - ResultStage 0 (count at CountReadsSpark.java:80) failed in 12.543 s due to Job aborted due to stage failure: Task 1 in stage 0.0 failed 4 times, most recent failure: Lost task 1.3 in stage 0.0 (TID 11, scc-q20.scc.bu.edu, executor 2): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 0, start 87545719, span 186383, expected MD5 492a29f6d7d6fcaf8dde06834861e7ae; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:33215,abort,aborted,33215,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,1,['abort'],['aborted']
Safety,la:52); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:121); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.s,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6513#issuecomment-601702690:7347,abort,abortStage,7347,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6513#issuecomment-601702690,1,['abort'],['abortStage']
Safety,lambda$apply$26a6df3e$1; 0.1% 42 + 0 org.broadinstitute.hellbender.utils.baq.BAQ.hmm_glocal; 0.0% 0 + 28 java.net.SocketInputStream.socketRead0; 0.0% 22 + 0 org.apache.spark.util.collection.TimSort.sort; 0.0% 15 + 0 java.util.Iterator.forEachRemaining; 0.0% 14 + 0 org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1$$anonfun$12$$anonfun$apply$4.apply$mcV$sp; 0.0% 13 + 0 com.esotericsoftware.kryo.serializers.FieldSerializer$ObjectField.read; 0.0% 6 + 0 org.broadinstitute.hellbender.utils.baq.BAQ.<init>; 0.0% 4 + 0 org.broadinstitute.hellbender.utils.recalibration.RecalUtils.combineTables; 0.0% 0 + 3 java.io.UnixFileSystem.getLength; 0.0% 3 + 0 org.apache.hadoop.hdfs.DFSOutputStream.waitAndQueueCurrentPacket; 0.0% 3 + 0 org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine.processRead; 0.0% 0 + 2 java.io.UnixFileSystem.createDirectory; 0.0% 1 + 1 java.lang.Class.getDeclaredFields0; 0.0% 2 + 0 java.nio.HeapIntBuffer.<init>; 0.0% 2 + 0 sun.misc.Unsafe.defineClass; 0.0% 0 + 2 sun.misc.Unsafe.copyMemory; 0.0% 0 + 2 org.apache.hadoop.util.NativeCrc32.nativeComputeChunkedSums; 0.0% 2 + 0 java.text.DateFormatSymbols.getProviderInstance; 0.0% 2 + 0 org.broadinstitute.hellbender.utils.baq.BAQ$BAQCalculationResult.<init>; 0.0% 2 + 0 org.broadinstitute.hellbender.utils.read.markduplicates.ReadsKey.subkeyForFragment; 0.0% 2 + 0 htsjdk.samtools.BAMRecord.decodeBaseQualities; 0.8% 396 + 260 Total interpreted (including elided). Compiled + native Method ; 13.4% 10629 + 23 org.bdgenomics.adam.util.TwoBitFile$$anonfun$2.apply$mcZJ$sp; 8.6% 21 + 6794 org.broadinstitute.hellbender.utils.baq.BAQ.hmm_glocal; 5.7% 4492 + 0 org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine.updateRecalTablesForRead; 4.1% 3246 + 0 scala.collection.AbstractTraversable.genericBuilder; 2.4% 1932 + 0 scala.collection.AbstractSeq.size; 2.3% 1841 + 2 com.ning.compress.lzf.impl.UnsafeChunkEncoderLE.tryCompress; 2.0% 1560 + 0 org.broadinstitute.hellbend,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1657#issuecomment-208967490:1538,Unsafe,Unsafe,1538,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1657#issuecomment-208967490,1,['Unsafe'],['Unsafe']
Safety,"let's measure. 2 reruns of HC should tell us all we need to know. On Thu, Jul 7, 2016 at 12:06 PM, Louis Bergelson notifications@github.com; wrote:. > So there may be some unfortunate performance implications with some of; > these changes. Utils.nonNull(value, message) and it's compatriots will; > always compute the message even if the error condition is not met. Using; > any message which isn't a constant will generate garbage in the form of; > strings. In most cases this isn't a problem, but it is not ideal if it's; > placed in a tight loop.; > ; > We could offset the problem by adding a family of Utils functions that; > take a lambda String producer instead of a string itself, this would allow; > the message to be computed only when the error condition is triggered; > avoiding garbage creation.; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/gatk/pull/1979#issuecomment-231126857,; > or mute the thread; > https://github.com/notifications/unsubscribe/AB5rL-76TnggBn8C75KiUGdpb_0ZgIjdks5qTSQigaJpZM4JGaiQ; > .",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1979#issuecomment-231127666:782,avoid,avoiding,782,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1979#issuecomment-231127666,1,['avoid'],['avoiding']
Safety,"like you can get around the compiler lock issues by pointing each invocation of GermlineCNVCaller to a different compilation directory. For example, invoke `gatk` by; > ; > `THEANORC=PATH/TO/THEANORC_# gatk GermlineCNVCaller ...`; > ; > This uses the `THEANORC` environment variable to set the `.theanorc` configuration file to `PATH/TO/THEANORC_#` for this instance of GATK (where you should fill in `#` appropriately). Each `PATH/TO/THEANORC_#` should be a file containing the following:; > ; > ```; > [global]; > base_compiledir = PATH/TO/COMPILEDIR_#; > ```; > ; > Where again, `#` is filled in appropriately. The goal is to point each GermlineCNVCaller instance to a different compilation directory. @xysj1989 can you let me know if this works for you?; > ; > This is a bit of a hack. We could probably avoid this by changing the GATK code to use a specified or temporary directory for the theano directory without too much effort.; > ; > However, there is an upside to using a non-temporary directory to avoid recompilation of the model upon subsequent runs. In this case, we'd just want to let the user be able to specify the theano directory (rather than dump things in `~/.theano` unexpectedly). We should think about whether this should be opt-in, i.e., should we preserve the original behavior of using `~/.theano` by default?; > ; > @mwalker174 opinions? @droazen or engine team, thoughts on what the policy should be for python/R scripts doing this sort of thing? Is it generally true that the GATK leaves no trace, other than producing the expected output?. Dear samuelklee,. Thank you very much for you reply. I also found this problem last night. It seems that the problem is originally from Theano and Pymc3, rather than GATK 4.0. Some similar problems have been reported just like (1) https://github.com/pymc-devs/pymc3/issues/1463 (2) https://stackoverflow.com/questions/52270853/how-to-get-rid-of-theano-gof-compilelock and (3) https://groups.google.com/forum/#!topic/theano-users",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6235#issuecomment-548557073:1022,avoid,avoid,1022,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6235#issuecomment-548557073,1,['avoid'],['avoid']
Safety,"lson I am trying latest release, I use following command:. ```; ./gatk-4.1.3.0/gatk --java-options ""-Xmx4g"" FilterMutectCalls -O Filtered.vcf -V Try.vcf.gz -R ~/human.fa/ucsc.hg19.fasta; ```. and got following Info:. ```; Using GATK jar /mnt/md0/DataProcess/Ranshi/Mutect2/gatk-4.1.3.0/gatk-package-4.1.3.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx4g -jar /mnt/md0/DataProcess/Ranshi/Mutect2/gatk-4.1.3.0/gatk-package-4.1.3.0-local.jar FilterMutectCalls -O Filtered.vcf -V Try.vcf.gz -R /home/imp/human.fa/ucsc.hg19.fasta; 09:44:27.763 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/mnt/md0/DataProcess/Ranshi/Mutect2/gatk-4.1.3.0/gatk-package-4.1.3.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Aug 21, 2019 9:44:29 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 09:44:29.499 INFO FilterMutectCalls - ------------------------------------------------------------; 09:44:29.500 INFO FilterMutectCalls - The Genome Analysis Toolkit (GATK) v4.1.3.0; 09:44:29.500 INFO FilterMutectCalls - For support and documentation go to https://software.broadinstitute.org/gatk/; 09:44:29.500 INFO FilterMutectCalls - Executing as imp@imp-WorkStation on Linux v4.15.0-55-generic amd64; 09:44:29.500 INFO FilterMutectCalls - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_222-8u222-b10-1ubuntu1~16.04.1-b10; 09:44:29.501 INFO FilterMutectCalls - Start Date/Time: 2019年8月21日 上午09时44分27秒; 09:44:29.501 INFO FilterMutectCalls - ------------------------------------------------------------; 09:44:29.501 INFO FilterMutectCalls - ------------------------------------------------------------; 09:44:29.502 INFO FilterMutectCalls - HTSJDK Version: 2.20.1; 09:44:29.502 INFO FilterMutectCalls - Picard Version: 2.20.5; 09:44:29.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6102#issuecomment-523262338:1012,detect,detect,1012,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6102#issuecomment-523262338,1,['detect'],['detect']
Safety,"m.google.cloud.genomics.gatk.common. I've been working on this bam file issue, correcting errors in the files used for tests. Many of the errors involve reads with FLAGs that indicate that they are in pairs, but the mate is not extant in the file, causing the error. A way to fix this without deleting the offending reads is to set the FLAG to zero and also modify the RNEXT, PNEXT, and TLEN fields, if necessary, so that the read becomes single (provided that the values of all of these fields are not important for the tests). However, when I do this, I find that tests that write and then read bam files fail, because when the just-written file is read back, SAM validation complains that the mate unmapped FLAG is set for an unpaired read. It turns out that the copy of the file written by the test substitutes the value '8' for '0' as the FLAG for the modified reads. The relevant code in GenomicsConvertermakeSamRecord() (line 170) is:. flags += ((read.getNextMatePosition() == null || read.getNextMatePosition.getPosition() == null)) ? 8 : 0;. The effect of this line is that all reads which have null mate positions, even those which the FLAG specifies as unpaired, get the mate unmapped FLAG set, causing the validation errors that i'm seeing. The reason the tests have not failed before is apparently that the existing test files do not contain any reads with FLAGs that specify them as unpaired. A simple fix for this would be to convert the line above to:. flags += ( paired && (read.getNextMatePosition() == null || read.getNextMatePosition.getPosition() == null)) ? 8 : 0;. The redundant parens in the original code suggest that something like this may have been intended,but the google genomics documentation at http://google-genomics.readthedocs.org/en/latest/migrating_tips.html gives the following pseudocode:. flags += read.nextMatePosition.position == null ? 8 : 0 #mate_unmapped. so it looks like the doc supports the existing code. Should I submit this as an issue? . Thank you.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/569#issuecomment-114101033:1749,redund,redundant,1749,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/569#issuecomment-114101033,1,['redund'],['redundant']
Safety,"merging before the tests pass. Living on the edge, risking life and limb (and 3 growlers)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1061#issuecomment-151977150:51,risk,risking,51,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1061#issuecomment-151977150,1,['risk'],['risking']
Safety,"mline resource and without PON; - calling with PON only, without germline resource; - calling with germline resource only, without PON; - calling with both germline resource and PON. The results (please note that the labeling conventions are now different compared to WGS experiments, apologies for the inconvenience):. ![FD_1_T_tumor-normal_WES_muTect2_FD_1_tumor-normal_muTect2_PON_FD_1_tumor-normal_muTect2_gnomAD_FD_1_tumor-normal_muTect2_PON_gnomAD](https://user-images.githubusercontent.com/15612230/182358963-97f04d12-94c6-4f77-acef-c3ebcd78a98f.png). 2. The reference is GRCh38.primary_assembly.genome.fa; The benchmark (""gold standard"") call set contains only variants on chr1-chr22, which AFAIK are identical or almost identical between the different b38 versions. 3. In my minimal WES example, most of the new FPs (150/158) from v4.1.9.0 are not present in raw, unfiltered calls from v.4.1.8.1. This was found the following way.; Compare FPs by CHROM, POS, REF, ALT using the scratch output of som.py (ran with --keep-scratch):. `; comm -23 <(bcftools query -f ""%CHROM %POS %REF %ALT\n"" WES_FD_TN_4190_filter_som_py/fp.vcf.gz | sort) <(bcftools query -f ""%CHROM %POS %REF %ALT\n"" WES_FD_TN_4181_filter_som_py/fp.vcf.gz | sort) | sed 's/ /\t/g' > new_FPs.txt; `; `; wc -l new_FPs.txt ; `; `; 158 new_FPs.txt; `. Find new FPs present in the raw output of v.4.1.8.1, again matching by CHROM, POS, REF, ALT:. `; awk 'NR==FNR{a[$1""_""$2""_""$3""_""$4]=1; next;}{if(substr($0,1,1)!=""#"" && a[$1""_""$2""_""$4""_""$5]==1) print $0}' new_FPs.txt WES_FD_TN_4181.vcf | wc -l; `; `; 8; `. For the 8 new FPs that have been detected, but filtered in v.4.1.8.1 (all of which are SNVs), find FILTER classification:; `; awk 'NR==FNR{a[$1""_""$2""_""$3""_""$4]=1; next;}{if(substr($0,1,1)!=""#"" && a[$1""_""$2""_""$4""_""$5]==1) print $0}' new_FPs.txt WES_FD_TN_4181_filtered.vcf | cut -f7 | sort | uniq -c; `; `; 2 clustered_events; `; `; 1 normal_artifact; `; `; 1 strand_bias; `; `; 4 weak_evidence; `. Hope this helps,. Dmitriy",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1202344705:2994,detect,detected,2994,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1202344705,1,['detect'],['detected']
Safety,"more concretely the private method getReferenceBases(SAMSeqRecord) should be syncronized or avoid it calling directly to the syncronized getReferenceBases(SSR, boolean) and getReferenceBasesByRegions should not update the cache fields.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8139#issuecomment-1376313615:92,avoid,avoid,92,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8139#issuecomment-1376313615,1,['avoid'],['avoid']
Safety,"n't have an index, it returns a TribbleIndexedFeatureReader; > instead of a TabixFeatureReader, because methods.isTabix() returns; > false when an index is not present.; > - TribbleIndexedFeatureReader, in turn, opens a Java vanilla; > GZIPInputStream, instead of the BlockCompressedInputStream that gets; > opened when you create a TabixFeatureReader.; > - GZIPInputStream, in turn, has a *confirmed bug* filed against it in; > Oracle's bug tracker (see; > https://bugs.java.com/bugdatabase/view_bug.do?bug_id=7036144#), that; > it inappropriately relies on the available() method to detect; > end-of-file, which is never safe to do given the contract of; > available(); > - As the final piece in the ghastly puzzle, implementations of; > SeekableStream in htsjdk do not implement available() at all, instead; > using the default implementation which always returns 0.; >; > As a result of this combination of bugs in Java's GZIPInputStream itself; > and bugs in htsjdk's SeekableStream classes, end-of-file can be detected; > prematurely when within 26 bytes of the end of a block, due to the; > following code in GZIPInputStream.readTrailer():; >; > if (this.in.available() > 0 || n > 26) {; > ....; > }; > return true; // EOF; >; > Where n is the number of bytes left to inflate in the current block.; >; > The solution is to replace all usages of the bugged GZIPInputStream with; > BlockCompressedInputStream in tribble in htsjdk (at least, for points in; > the code where the input is known to be block-gzipped rather than regular; > gzipped). For due diligence we should also implement available(); > correctly for all implementations of SeekableStream in htsjdk.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/4224#issuecomment-360282461>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACnk0h8AF8wYzkbHSmAu4-8n5TE8GtOUks5tN6MfgaJpZM4RoUzm>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4224#issuecomment-360304725:1376,detect,detected,1376,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4224#issuecomment-360304725,1,['detect'],['detected']
Safety,"na-seq-gatk-variant-calling/.snakemake/conda/cd50d464/share/gatk4-4.2.4.1-0/gatk-package-4.2.4.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx4g -jar /home/eanderson/Documents/projects/yukon-chinookomes-dna-seq-gatk-variant-calling/.snakemake/conda/cd50d464/share/gatk4-4.2.4.1-0/gatk-package-4.2.4.1-local.jar GenotypeGVCFs -R resources/genome.fasta -V gendb://results/genomics_db/chromosomes/CM031199.1 -O results/vcf_parts/CM031199.1.vcf.gz; 22:17:18.737 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/eanderson/Documents/projects/yukon-chinookomes-dna-seq-gatk-variant-calling/.snakemake/conda/cd50d464/share/gatk4-4.2.4.1-0/gatk-package-4.2.4.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 16, 2022 10:17:18 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 22:17:18.863 INFO GenotypeGVCFs - ------------------------------------------------------------; 22:17:18.863 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.4.1; 22:17:18.863 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 22:17:18.864 INFO GenotypeGVCFs - Executing as eanderson@node34.cluster on Linux v4.18.0-193.28.1.el8_2.x86_64 amd64; 22:17:18.864 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v11.0.9.1-internal+0-adhoc..src; 22:17:18.864 INFO GenotypeGVCFs - Start Date/Time: January 16, 2022 at 10:17:18 PM PST; 22:17:18.864 INFO GenotypeGVCFs - ------------------------------------------------------------; 22:17:18.864 INFO GenotypeGVCFs - ------------------------------------------------------------; 22:17:18.864 INFO GenotypeGVCFs - HTSJDK Version: 2.24.1; 22:17:18.864 INFO GenotypeGVCFs - Picard Version: 2.25.4; 22:17:18.865 INFO GenotypeGVCF",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7639#issuecomment-1014180059:2183,detect,detect,2183,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7639#issuecomment-1014180059,1,['detect'],['detect']
Safety,"nd other key dependencies; - conda-forge::theano=1.0.4 # it is unlikely that new versions of theano will be released; # verify that this is using numpy compiled against MKL (e.g., by the presence of -lmkl_rt in theano.config.blas.ldflags); - defaults::tensorflow=1.15.0 # update only if absolutely necessary, as this may cause conflicts with other core dependencies; # verify that this is using numpy compiled against MKL (e.g., by checking tensorflow.pywrap_tensorflow.IsMklEnabled()); - conda-forge::scipy=1.0.0 # do not update, this will break a scipy.misc.logsumexp import (deprecated in scipy=1.0.0) in pymc3=3.1; - conda-forge::pymc3=3.1 # do not update, this will break gcnvkernel; - conda-forge::keras=2.2.4 # updated from pip-installed 2.2.0, which caused various conflicts/clobbers of conda-installed packages; # conda-installed 2.2.4 appears to be the most recent version with a consistent API and without conflicts/clobbers; # if you wish to update, note that versions of conda-forge::keras after 2.2.5; # undesirably set the environment variable KERAS_BACKEND = theano by default; - defaults::intel-openmp=2019.4; - conda-forge::scikit-learn=0.22.2; - conda-forge::matplotlib=3.2.1; - conda-forge::pandas=1.0.3. # core R dependencies; these should only be used for plotting and do not take precedence over core python dependencies!; - r-base=3.6.2; - r-data.table=1.12.8; - r-dplyr=0.8.5; - r-getopt=1.20.3; - r-ggplot2=3.3.0; - r-gplots=3.0.3; - r-gsalib=2.1; - r-optparse=1.6.4. # other python dependencies; these should be removed after functionality is moved into Java code; - biopython=1.76; - pyvcf=0.6.8; - bioconda::pysam=0.15.3 # using older conda-installed versions may result in libcrypto / openssl bugs. # pip installs should be avoided, as pip may not respect the dependencies found by the conda solver; - pip:; - gatkPythonPackageArchive.zip; ```. It seems to successfully create the environment. I'd still recommend updating the information on your README.md and the file.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6656#issuecomment-643526868:3717,avoid,avoided,3717,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6656#issuecomment-643526868,1,['avoid'],['avoided']
Safety,nds 11 --gvcf-gq-bands 12 --gvcf-gq-bands 13 --gvcf-gq-bands 14 --gvcf-gq-bands 15 --gvcf-gq-bands 16 --gvcf-gq-bands 17 --gvc; f-gq-bands 18 --gvcf-gq-bands 19 --gvcf-gq-bands 20 --gvcf-gq-bands 21 --gvcf-gq-bands 22 --gvcf-gq-bands 23 --gvcf-gq-bands 24 --gvcf-gq-bands 25 --gvcf-gq-bands 26 --gvcf-gq-bands 27 --g; vcf-gq-bands 28 --gvcf-gq-bands 29 --gvcf-gq-bands 30 --gvcf-gq-bands 31 --gvcf-gq-bands 32 --gvcf-gq-bands 33 --gvcf-gq-bands 34 --gvcf-gq-bands 35 --gvcf-gq-bands 36 --gvcf-gq-bands 37 -; -gvcf-gq-bands 38 --gvcf-gq-bands 39 --gvcf-gq-bands 40 --gvcf-gq-bands 41 --gvcf-gq-bands 42 --gvcf-gq-bands 43 --gvcf-gq-bands 44 --gvcf-gq-bands 45 --gvcf-gq-bands 46 --gvcf-gq-bands 47; --gvcf-gq-bands 48 --gvcf-gq-bands 49 --gvcf-gq-bands 50 --gvcf-gq-bands 51 --gvcf-gq-bands 52 --gvcf-gq-bands 53 --gvcf-gq-bands 54 --gvcf-gq-bands 55 --gvcf-gq-bands 56 --gvcf-gq-bands ; 57 --gvcf-gq-bands 58 --gvcf-gq-bands 59 --gvcf-gq-bands 60 --gvcf-gq-bands 70 --gvcf-gq-bands 80 --gvcf-gq-bands 90 --gvcf-gq-bands 99 --floor-blocks false --indel-size-to-eliminate-in-re; f-model 10 --disable-optimizations false --dragen-mode false --flow-mode NONE --apply-bqd false --apply-frd false --disable-spanning-event-genotyping false --transform-dragen-mapping-quali; ty false --mapping-quality-threshold-for-genotyping 20 --max-effective-depth-adjustment-for-frd 0 --just-determine-active-regions false --dont-genotype false --do-not-run-physical-phasing ; false --do-not-correct-overlapping-quality false --use-filtered-reads-for-annotations false --use-flow-aligner-for-stepwise-hc-filtering false --adaptive-pruning false --do-not-recover-dan; gling-branches false --recover-dangling-heads false --kmer-size 10 --kmer-size 25 --dont-increase-kmer-sizes-for-cycles false --allow-non-unique-kmers-in-ref false --num-pruning-samples 1 ; --min-dangling-branch-length 4 --recover-all-dangling-branches false --max-num-haplotypes-in-population 128 --min-pruning 2 --adaptive-pruning-initial-error-rate 0.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8574#issuecomment-1793390789:4877,recover,recover-dan,4877,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8574#issuecomment-1793390789,3,['recover'],"['recover-all-dangling-branches', 'recover-dan', 'recover-dangling-heads']"
Safety,"ner. As I'd think that all software dependencies and whatnot should be fine. However, I still get the same error message:. /gatk/./gatk --java-options ""-Xmx25g"" SplitNCigarReads \; > -R Homo_sapiens.GRCh38.dna.primary_assembly.fa -I subset_TINY_rehead.bam \; > --tmp-dir /gatk/my_data/temp -O thing.bam; Using GATK jar /gatk/gatk-package-4.1.3.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx25g -jar /gatk/gatk-package-4.1.3.0-local.jar SplitNCigarReads -R Homo_sapiens.GRCh38.dna.primary_assembly.fa -I subset_TINY_rehead.bam --tmp-dir /gatk/my_data/temp -O thing.bam. 21:12:14.158 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.3.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Mar 02, 2023 9:12:16 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 21:12:16.383 INFO SplitNCigarReads - ------------------------------------------------------------; 21:12:16.384 INFO SplitNCigarReads - The Genome Analysis Toolkit (GATK) v4.1.3.0; 21:12:16.384 INFO SplitNCigarReads - For support and documentation go to https://software.broadinstitute.org/gatk/; 21:12:16.384 INFO SplitNCigarReads - Executing as root@9d399eec0e24 on Linux v5.19.0-32-generic amd64; 21:12:16.384 INFO SplitNCigarReads - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_191-8u191-b12-0ubuntu0.16.04.1-b12; 21:12:16.384 INFO SplitNCigarReads - Start Date/Time: March 2, 2023 9:12:14 PM UTC; 21:12:16.385 INFO SplitNCigarReads - ------------------------------------------------------------; 21:12:16.385 INFO SplitNCigarReads - ------------------------------------------------------------; 21:12:16.385 INFO SplitNCigarReads - HTSJDK Version: 2.20.1; 21:12:16.385 INFO SplitNCigarReads - Picard Version: 2.20.5; 21:12:16.385 IN",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452564826:1183,detect,detect,1183,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452564826,1,['detect'],['detect']
Safety,nevermind. It crashes our tests. Too small benefit anyway for such a risk,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1734#issuecomment-212141789:69,risk,risk,69,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1734#issuecomment-212141789,1,['risk'],['risk']
Safety,"ng/.snakemake/conda/cd50d464/share/gatk4-4.2.4.1-0/gatk-package-4.2.4.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx4g -jar /home/eanderson/Documents/projects/yukon-chinookomes-dna-seq-gatk-variant-calling/.snakemake/conda/cd50d464/share/gatk4-4.2.4.1-0/gatk-package-4.2.4.1-local.jar GenotypeGVCFs -R resources/genome.fasta -V gendb://results/genomics_db/chromosomes/CM031199.1 --max-alternate-alleles 7 -O results/vcf_parts/CM031199.1.vcf.gz; 21:57:11.346 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/eanderson/Documents/projects/yukon-chinookomes-dna-seq-gatk-variant-calling/.snakemake/conda/cd50d464/share/gatk4-4.2.4.1-0/gatk-package-4.2.4.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 16, 2022 9:57:11 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 21:57:11.476 INFO GenotypeGVCFs - ------------------------------------------------------------; 21:57:11.477 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.4.1; 21:57:11.477 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 21:57:11.477 INFO GenotypeGVCFs - Executing as eanderson@node34.cluster on Linux v4.18.0-193.28.1.el8_2.x86_64 amd64; 21:57:11.477 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v11.0.9.1-internal+0-adhoc..src; 21:57:11.477 INFO GenotypeGVCFs - Start Date/Time: January 16, 2022 at 9:57:11 PM PST; 21:57:11.477 INFO GenotypeGVCFs - ------------------------------------------------------------; 21:57:11.477 INFO GenotypeGVCFs - ------------------------------------------------------------; 21:57:11.478 INFO GenotypeGVCFs - HTSJDK Version: 2.24.1; 21:57:11.478 INFO GenotypeGVCFs - Picard Version: 2.25.4; 21:57:11.478 INFO GenotypeGVCFs",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7639#issuecomment-1014180059:11500,detect,detect,11500,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7639#issuecomment-1014180059,1,['detect'],['detect']
Safety,"ng?. On Wed, Jan 24, 2018 at 4:39 PM droazen <notifications@github.com> wrote:. > @ldgauthier <https://github.com/ldgauthier> @yfarjoun; > <https://github.com/yfarjoun> We have an update on this! We've identified; > the bug:; >; > - When AbstractFeatureReader.getFeatureReader() tries to open a .vcf.gz; > that doesn't have an index, it returns a TribbleIndexedFeatureReader; > instead of a TabixFeatureReader, because methods.isTabix() returns; > false when an index is not present.; > - TribbleIndexedFeatureReader, in turn, opens a Java vanilla; > GZIPInputStream, instead of the BlockCompressedInputStream that gets; > opened when you create a TabixFeatureReader.; > - GZIPInputStream, in turn, has a *confirmed bug* filed against it in; > Oracle's bug tracker (see; > https://bugs.java.com/bugdatabase/view_bug.do?bug_id=7036144#), that; > it inappropriately relies on the available() method to detect; > end-of-file, which is never safe to do given the contract of; > available(); > - As the final piece in the ghastly puzzle, implementations of; > SeekableStream in htsjdk do not implement available() at all, instead; > using the default implementation which always returns 0.; >; > As a result of this combination of bugs in Java's GZIPInputStream itself; > and bugs in htsjdk's SeekableStream classes, end-of-file can be detected; > prematurely when within 26 bytes of the end of a block, due to the; > following code in GZIPInputStream.readTrailer():; >; > if (this.in.available() > 0 || n > 26) {; > ....; > }; > return true; // EOF; >; > Where n is the number of bytes left to inflate in the current block.; >; > The solution is to replace all usages of the bugged GZIPInputStream with; > BlockCompressedInputStream in tribble in htsjdk (at least, for points in; > the code where the input is known to be block-gzipped rather than regular; > gzipped). For due diligence we should also implement available(); > correctly for all implementations of SeekableStream in htsjdk.; >; > —; > You",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4224#issuecomment-360304725:945,detect,detect,945,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4224#issuecomment-360304725,2,"['detect', 'safe']","['detect', 'safe']"
Safety,"not necessary anymore for GATK-SV assembly-detected variants because the alt haplotype sequence is always output. still necessary for variants that are re-interpreted in #4189 , and other tools (but may never come true).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2386#issuecomment-361072987:43,detect,detected,43,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2386#issuecomment-361072987,1,['detect'],['detected']
Safety,"note to implementer: look into using `Throwable.getSuppressed()`. ```; /**; * Returns an array containing all of the exceptions that were; * suppressed, typically by the {@code try}-with-resources; * statement, in order to deliver this exception.; *; * If no exceptions were suppressed or {@linkplain; * #Throwable(String, Throwable, boolean, boolean) suppression is; * disabled}, an empty array is returned. This method is; * thread-safe. Writes to the returned array do not affect future; * calls to this method.; */; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/528#issuecomment-114228841:434,safe,safe,434,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/528#issuecomment-114228841,1,['safe'],['safe']
Safety,"nt bin* provides equal weight---rather than the counts themselves. As usual, modeling each bin as Poisson is close enough to modeling all bins as multinomial for our purposes. If we directly use the NB likelihood and simply weight the count likelihood by occurrences, occurrences in the peak will strongly affect the result, adversely so if the count likelihood is actually misspecified there. As an example, consider trying to fit a Poisson to data that is actually zero-inflated Poisson---fitting the histogram will actually result in a more robust estimate for the mean. Another benefit is that truncated data (as we have here) is straightforwardly handled in an unbiased way. In the special case of complete, trivially-binned data, the full, unbinned likelihood is recovered. I think this sort of histogram fitting is pretty standard in the astro/particle community. We can certainly change up the model to include strictly quantized + free-floating states as you describe (rather than the ""fuzzily quantized"" states I use here), but I just wanted to avoid having another level of mixtures/logsumexps for this quick prototype. However, note that modeling mosaicism on the autosomes is desirable, but there we also want the strong diploid prior to nail down the depth and per-contig bias. So we will have to be a little careful about how we introduce free-floating states. Also, since I was not using gcnvkernel, I had to integrate out all discrete parameters. It may be that we can write down a nice model with discrete parameters if we use your inference framework. Finally, I did not further bin the counts here (or rather, the bin size is 1), which already yields the maximum information, but I did use a maximum-count cutoff. If we use the same cutoff for all samples, this allows us to simply pass a non-ragged matrix from Java (with dimensions of samples x contigs x maximum count) as a TSV. However, we may run into trouble if we hit a case sample with very high depth. So some sort of spa",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-376307522:1159,avoid,avoid,1159,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-376307522,1,['avoid'],['avoid']
Safety,"nt context [C*, CT] can you maybe advise whats going on? . java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx30G -jar /run/media/riadh/One Touch1/Analysis/gatk-4.2.4.1/gatk-package-4.2.5.0-local.jar VariantAnnotator -V PE69_chr3.vcf -R /run/media/riadh/One Touch/Reference_data_b38/resources_broad_hg38_v0_Homo_sapiens_assembly38.fasta --resource:gnomad /run/media/riadh/One Touch/Reference_data_b38/gnomad.genomes.v3.1.2.sites.chr3.vcf.bgz -E gnomad.nhomalt -E gnomad.ALT -E gnomad.AF -O PE69_ch3_vep_cadd_gnomad.vcf --resource-allele-concordance; 10:58:19.715 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/run/media/riadh/One%20Touch1/Analysis/gatk-4.2.4.1/gatk-package-4.2.5.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Mar 17, 2022 10:58:19 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 10:58:19.796 INFO VariantAnnotator - ------------------------------------------------------------; 10:58:19.796 INFO VariantAnnotator - The Genome Analysis Toolkit (GATK) v4.2.5.0; 10:58:19.796 INFO VariantAnnotator - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:58:19.797 INFO VariantAnnotator - Executing as riadh@ikm-unix-1012.uio.no on Linux v5.16.12-200.fc35.x86_64 amd64; 10:58:19.797 INFO VariantAnnotator - Java runtime: OpenJDK 64-Bit Server VM v11.0.14.1+1; 10:58:19.797 INFO VariantAnnotator - Start Date/Time: March 17, 2022 at 10:58:19 AM CET; 10:58:19.797 INFO VariantAnnotator - ------------------------------------------------------------; 10:58:19.797 INFO VariantAnnotator - ------------------------------------------------------------; 10:58:19.797 INFO VariantAnnotator - HTSJDK Version: 2.24.1; 10:58:19.797 INFO VariantAnnotator - Picard Version: 2.25.4; 10:58:19.798 INFO Varian",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6689#issuecomment-1070784053:1183,detect,detect,1183,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6689#issuecomment-1070784053,1,['detect'],['detect']
Safety,nternal.tasks.execution.ExecuteActionsTaskExecuter$TaskExecution.execute(ExecuteActionsTaskExecuter.java:237); at org.gradle.internal.execution.steps.ExecuteStep.lambda$execute$1(ExecuteStep.java:33); at org.gradle.internal.execution.steps.ExecuteStep.execute(ExecuteStep.java:33); at org.gradle.internal.execution.steps.ExecuteStep.execute(ExecuteStep.java:26); at org.gradle.internal.execution.steps.CleanupOutputsStep.execute(CleanupOutputsStep.java:58); at org.gradle.internal.execution.steps.CleanupOutputsStep.execute(CleanupOutputsStep.java:35); at org.gradle.internal.execution.steps.ResolveInputChangesStep.execute(ResolveInputChangesStep.java:48); at org.gradle.internal.execution.steps.ResolveInputChangesStep.execute(ResolveInputChangesStep.java:33); at org.gradle.internal.execution.steps.CancelExecutionStep.execute(CancelExecutionStep.java:39); at org.gradle.internal.execution.steps.TimeoutStep.executeWithoutTimeout(TimeoutStep.java:73); at org.gradle.internal.execution.steps.TimeoutStep.execute(TimeoutStep.java:54); at org.gradle.internal.execution.steps.CatchExceptionStep.execute(CatchExceptionStep.java:35); at org.gradle.internal.execution.steps.CreateOutputsStep.execute(CreateOutputsStep.java:51); at org.gradle.internal.execution.steps.SnapshotOutputsStep.execute(SnapshotOutputsStep.java:45); at org.gradle.internal.execution.steps.SnapshotOutputsStep.execute(SnapshotOutputsStep.java:31); at org.gradle.internal.execution.steps.CacheStep.executeWithoutCache(CacheStep.java:208); at org.gradle.internal.execution.steps.CacheStep.execute(CacheStep.java:70); at org.gradle.internal.execution.steps.CacheStep.execute(CacheStep.java:45); at org.gradle.internal.execution.steps.BroadcastChangingOutputsStep.execute(BroadcastChangingOutputsStep.java:49); at org.gradle.internal.execution.steps.StoreSnapshotsStep.execute(StoreSnapshotsStep.java:43); at org.gradle.internal.execution.steps.StoreSnapshotsStep.execute(StoreSnapshotsStep.java:32); at org.gradle.internal.execution.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6466#issuecomment-590387973:8612,Timeout,TimeoutStep,8612,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6466#issuecomment-590387973,2,['Timeout'],['TimeoutStep']
Safety,"nts (up from 130 for the 100% tumor alone, as above). Using this joint segmentation for subsequent ModelSegments runs:. For the 100% normal, this yields 88 segments (up from 36):; ![N-SJS modeled](https://user-images.githubusercontent.com/11076296/76632024-ebecb300-6518-11ea-89ff-109c97970ef0.png). For the 100% tumor, this yields 166 segments (up from 130):; ![T-SJS modeled](https://user-images.githubusercontent.com/11076296/76632125-13dc1680-6519-11ea-9901-0c78809d08ba.png). I haven't performed detailed validations, but some spot checking suggests that this actually mitigate oversegmentation while still increasing sensitivity to shared events. For example, there is a small 13-bin deletion in chr19 that is found when running the 100% normal alone, but gets broken up into two adjacent deletions when running the 100% tumor alone (probably just due to statistical noise in the copy ratios). When running jointly, the deletion does not get broken up. However, as discussed over Slack, we should probably run some scenarios with simulated data to check behavior---for example, how robust is the joint segmentation to some of the samples being noisy/oversegmented?. There are lots of options for restructuring the workflow. We could potentially modify ModelSegments to take in the denoised copy ratios from the normal, when available, and add modeling of the normal and germline tagging to that tool. Or we could break things up into separate tools. @fleharty any opinions?. Note that another benefit of using this joint segmentation for germline tagging is that common breakpoints will be shared. This obviates the need for a lot of the idiosyncratic code (in the experimental postprocessing tools) that deals with reconciling segmentations and combining breakpoints. In general, I think such code is extremely prone to off-by-one errors and should be avoided, if possible. See https://github.com/broadinstitute/gatk/pull/5450 for a reminder of some of the remaining issues with that workflow.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6499#issuecomment-598764477:2278,avoid,avoided,2278,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6499#issuecomment-598764477,1,['avoid'],['avoided']
Safety,"o that. Sincerely,; Emily. From: ldgauthier ***@***.***>; Sent: Monday, March 28, 2022 2:39 PM; To: broadinstitute/gatk ***@***.***>; Cc: Emily Elizabeth Puckett (puckett3) ***@***.***>; Mention ***@***.***>; Subject: Re: [broadinstitute/gatk] CombineGVCFs: ERROR input alleles must contain <NON_REF> (Issue #7737). CAUTION: This email originated from outside of the organization. Do not click links or open attachments unless you recognize the sender and trust the content is safe. If I'm reading the process correctly, I don't actually think this should work. CombineGVCFs is specifically for combining GVCFs and it expects GVCFs to have <NON_REF> alleles. If you've already run the data through GenotypeGVCFs then you can't use CombineGVCFs again because the <NON_REF> likelihoods have been applied and those alleles are gone. The vcfcombine tool from bcftools is quite fast if all you want to do is join the samples together. -; Reply to this email directly, view it on GitHub<https://nam02.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fbroadinstitute%2Fgatk%2Fissues%2F7737%23issuecomment-1081062021&data=04%7C01%7CEmily.Puckett%40memphis.edu%7C51db6aa9f41b483e1ce408da10f2aa5d%7Cae145aeacdb2446ab05a7858dde5ddba%7C0%7C0%7C637840931685525269%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000&sdata=Pxg8joQfE51l5e3cUUbKA9bQEYDZjp0AxdX0aqDG1MY%3D&reserved=0>, or unsubscribe<https://nam02.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FALDFEHAXSKZ7YHSFGISLPUTVCIDGZANCNFSM5RZSK5PA&data=04%7C01%7CEmily.Puckett%40memphis.edu%7C51db6aa9f41b483e1ce408da10f2aa5d%7Cae145aeacdb2446ab05a7858dde5ddba%7C0%7C0%7C637840931685525269%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000&sdata=6Dkb6rbHDZpS05bYUHhlIRHJitgVtR%2FPB5rNHHFMg%2FQ%3D&reserved=0>.; You are receiving this because you were mentioned.Message ID: ***@***.******@***.***>",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7737#issuecomment-1082170127:1022,safe,safelinks,1022,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7737#issuecomment-1082170127,1,['safe'],['safelinks']
Safety,"o.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144); at java.lang.Thread.run(Thread.java:748); 18/04/24 17:42:11 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 18/04/24 17:42:11 INFO MemoryStore: MemoryStore cleared; 18/04/24 17:42:11 INFO BlockManager: BlockManager stopped; 18/04/24 17:42:11 INFO BlockManagerMaster: BlockManagerMaster stopped; 18/04/24 17:42:11 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/04/24 17:42:11 INFO SparkContext: Successfully stopped SparkContext; 17:42:11.053 INFO PathSeqPipelineSpark - Shutting down engine; [April 24, 2018 5:42:11 PM CEST] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 2.87 minutes.; Runtime.totalMemory()=866648064; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 8, xx.xx.xx.xx, executor 3): org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:39053,abort,aborted,39053,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,1,['abort'],['aborted']
Safety,"o_write_tribble=false -Dsamjdk.compression_level=2 -Xmx60g -jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar GenotypeGVCFs -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -G StandardAnnotation -G AS_StandardAnnotation -V gendb:///restricted/projectnb/kageproj/gatk/genomicsdb/genomicsDB.chr16 -L chr16:105582-211160 --use-new-qual-calculator --only-output-calls-starting-in-intervals TRUE --genomicsdb-shared-posixfs-optimizations TRUE --tmp-dir tmp -O chr16-105582-211160.vcf.gz; 07:46:18.893 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 07:46:18.944 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Aug 25, 2021 7:46:19 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 07:46:19.128 INFO GenotypeGVCFs - ------------------------------------------------------------; 07:46:19.128 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.0.0; 07:46:19.128 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 07:46:19.129 INFO GenotypeGVCFs - Executing as farrell@scc-hadoop.bu.edu on Linux v3.10.0-1160.36.2.el7.x86_64 amd64; 07:46:19.129 INFO GenotypeGVCFs - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 07:46:19.129 INFO GenotypeGVCFs - Start Date/Time: August 25, 2021 7:46:18 AM EDT; 07:46:19.129 INFO GenotypeGVCFs - ------------------------------------------------------------; 07:46:19.129 INFO GenotypeGVCFs - ------------------------------------------------------------; 07:46:19.130 INFO GenotypeGVCFs - HTSJDK Version: 2.24.0; 07:46:19.130 INFO GenotypeGVCFs - Picard Version: 2.25.0; 07:46:19.130 INFO GenotypeGVCFs - Built for",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7437#issuecomment-905431278:1604,detect,detect,1604,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7437#issuecomment-905431278,1,['detect'],['detect']
Safety,"ok, thanks. also - the travis build seems to have timed out. from the log i think that's just a travis CI issue, not due to my commit. is this travis timeout problem something you see frequently w/ GATK4?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4495#issuecomment-384687146:150,timeout,timeout,150,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4495#issuecomment-384687146,1,['timeout'],['timeout']
Safety,"ontent.com/812850/27811313-9000019c-6097-11e7-82ac-aac557be31db.PNG).; And the program failed eventually:; ```; 18:24:57.885 INFO BwaAndMarkDuplicatesPipelineSpark - Shutting down engine; [July 3, 2017 6:24:57 PM CST] org.broadinstitute.hellbender.tools.spark.pipelines.BwaAndMarkDuplicatesPipelineSpark done. Elapsed time: 269.29 minutes.; Runtime.totalMemory()=4172283904; org.apache.spark.SparkException: Job aborted due to stage failure: Task 607 in stage 3.0 failed 4 times, most recent failure: Lost task 607.13 in stage 3.0 (TID 14832, 12.9.68.0, executor 24): ExecutorLostFailure (executor 24 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 169939 ms; Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.s",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312758363:1371,abort,abortStage,1371,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312758363,1,['abort'],['abortStage']
Safety,ools.spark.pipelines.PrintVariantsSpark done. Elapsed time: 0.34 minutes.; Runtime.totalMemory()=1106771968; org.apache.spark.SparkException: Job aborted due to stage failure: Exception while getting task result: com.esotericsoftware.kryo.KryoException: java.lang.NullPointerException; Serialization trace:; genotypes (htsjdk.variant.variantcontext.VariantContext); interval (org.broadinstitute.hellbender.engine.spark.SparkSharder$PartitionLocatable); 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1872); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2055); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3840#issuecomment-464825765:9892,abort,abortStage,9892,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3840#issuecomment-464825765,1,['abort'],['abortStage']
Safety,or.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:38516,abort,abortStage,38516,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,2,['abort'],['abortStage']
Safety,"ore refinements to the method and have settled on the following procedure:. Assume we have _N_ data points:; ![1](https://user-images.githubusercontent.com/11076296/29580954-bea24562-8745-11e7-9c8c-d68504ba31da.png); To find segments, we:; 1) Select _C<sub>max</sub>_, the maximum number of changepoints to discover. In practice, _C<sub>max</sub> = 100_ per chromosome should more than suffice.; 2) Select a kernel (linear for sensitivity to changes in the distribution mean, Gaussian with a specified variance _σ<sup>2</sup>_ for multimodal data, etc.) and a subsample of _p_ points to approximate it using SVD.; 3) Select window sizes _w<sub>j</sub>_ for which to compute local costs at each point. To be precise, we compute the cost of a changepoint at the point with index _i_, assuming adjacent segments containing the points with indices _[i - w<sub>j</sub> + 1, i]_ and _[i + 1, i + w<sub>j</sub>]_. Selecting a minimum window size and then doubling up to relevant length scales (noting that longer window lengths allow for more subtle changepoints to be detected) works well in practice. For example, here are what the cost functions look like for window sizes of 8, 16, 32, and 64:. ![2](https://user-images.githubusercontent.com/11076296/29582011-210d37b8-8749-11e7-9383-0c657232347e.png). ![3](https://user-images.githubusercontent.com/11076296/29582016-23fbc6a6-8749-11e7-951e-f618e8489a0b.png). ![4](https://user-images.githubusercontent.com/11076296/29582044-3eb20a1e-8749-11e7-84a0-3734bad15e1f.png). ![5](https://user-images.githubusercontent.com/11076296/29582047-410ac490-8749-11e7-8a98-b2098cf1b5ea.png); 4) For each of these cost functions, find (up to) the _C<sub>max</sub>_ most significant local minima. The problem of finding local minima of a noisy function can be solved by using topological persistence (e.g., https://people.mpi-inf.mpg.de/~weinkauf/notes/persistence1d.html and http://www2.iap.fr/users/sousbie/web/html/indexd3dd.html?post/Persistence-and-simplification). ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-324125586:1082,detect,detected,1082,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-324125586,1,['detect'],['detected']
Safety,org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2114); at org,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6513#issuecomment-601702690:7589,abort,abortStage,7589,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6513#issuecomment-601702690,1,['abort'],['abortStage']
Safety,park.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.s,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:38613,abort,abortStage,38613,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,2,['abort'],['abortStage']
Safety,"pong. Are you planning to get rid of the USER ERROR classification? (not a problem, just curious re: what is the plan for error types names if any). Error output should include the website URL; if it's possible (not too painful) to include the specific tool doc URL, that would be even better (currently the url for tool doc pages is based on the classpath in a completely predictable way). Agree with @lbergelson that it would be way useful for the parsing system to run complete validation on the command line rather than bail out at the first error, so you'd get a list of everything that you did wrong in one go.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/418#issuecomment-146214892:373,predict,predictable,373,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/418#issuecomment-146214892,1,['predict'],['predictable']
Safety,"pool/src/gatk-4.2.5.0/gatk-package-4.2.5.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx1600g -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -jar /home/dan_vanderpool/src/gatk-4.2.5.0/gatk-package-4.2.5.0-local.jar GenotypeGVCFs -R /home/dan_vanderpool/Wolf_raw_reads/Wolf_genome/GCA_905319855.2_mCanLor1.2_genomic.fa -V gendb://Wolf_Genome_Variantsdb -O All_Wolf_Samples_Joint_Genotypes_Raw.vcf.gz -L /scratch/dan/Wolf_reads_raw/Wolf_GenCov300_Q20_Merged.interval_list -imr ALL --genomicsdb-max-alternate-alleles 10 --max-alternate-alleles 6; 17:49:29.781 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/dan_vanderpool/src/gatk-4.2.5.0/gatk-package-4.2.5.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 23, 2022 5:49:30 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 17:49:30.164 INFO GenotypeGVCFs - ------------------------------------------------------------; 17:49:30.165 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.5.0; 17:49:30.165 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:49:30.165 INFO GenotypeGVCFs - Executing as dan_vanderpool@0e07622619ad on Linux v4.4.0-210-generic amd64; 17:49:30.165 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v10.0.2+13; 17:49:30.166 INFO GenotypeGVCFs - Start Date/Time: February 23, 2022 at 5:49:29 PM UTC; 17:49:30.166 INFO GenotypeGVCFs - ------------------------------------------------------------; 17:49:30.166 INFO GenotypeGVCFs - ------------------------------------------------------------; 17:49:30.167 INFO GenotypeGVCFs - HTSJDK Version: 2.24.1; 17:49:30.167 INFO GenotypeGVCFs - Picard Version: 2.25.4; 17:49:30.167 INFO GenotypeGVCFs - Built for Spark Version: ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7639#issuecomment-1049112454:4475,detect,detect,4475,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7639#issuecomment-1049112454,1,['detect'],['detect']
Safety,"ptions.defaultCredentials(ServiceOptions.java:304); at com.google.cloud.ServiceOptions.<init>(ServiceOptions.java:278); at com.google.cloud.storage.StorageOptions.<init>(StorageOptions.java:83); at com.google.cloud.storage.StorageOptions.<init>(StorageOptions.java:31); at com.google.cloud.storage.StorageOptions$Builder.build(StorageOptions.java:78); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.setGlobalNIODefaultOptions(BucketUtils.java:382); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:183); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289). Nov 24, 2018 6:05:09 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; WARNING: Failed to detect whether we are running on Google Compute Engine.; java.net.NoRouteToHostException: No route to host (Host unreachable); at java.net.PlainSocketImpl.socketConnect(Native Method); at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350); at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206); at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188); at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392); at java.net.Socket.connect(Socket.java:589); at sun.net.NetworkClient.doConnect(NetworkClient.java:175); at sun.net.www.http.HttpClient.openServer(HttpClient.java:463); at sun.net.www.http.HttpClient.openServer(HttpClient.java:558); at sun.net.www.http.HttpClient.<init>(HttpClient.java:242); at sun.net.www.http.HttpClient.New(HttpClient.java:339); at sun.net.www.http.HttpClient.New(HttpClient.java:357); at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1220",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-441873417:3680,detect,detect,3680,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-441873417,2,['detect'],['detect']
Safety,"r appears to be related to the reblocking of the gvcfs. ```; gendb:///restricted/projectnb/kageproj/gatk/genomicsdb/genomicsDB.chr16; Using GATK jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx60g -jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar GenotypeGVCFs -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -G StandardAnnotation -G AS_StandardAnnotation -V gendb:///restricted/projectnb/kageproj/gatk/genomicsdb/genomicsDB.chr16 -L chr16:105582-211160 --use-new-qual-calculator --only-output-calls-starting-in-intervals TRUE --genomicsdb-shared-posixfs-optimizations TRUE --tmp-dir tmp -O chr16-105582-211160.vcf.gz; 07:46:18.893 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 07:46:18.944 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Aug 25, 2021 7:46:19 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 07:46:19.128 INFO GenotypeGVCFs - ------------------------------------------------------------; 07:46:19.128 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.0.0; 07:46:19.128 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 07:46:19.129 INFO GenotypeGVCFs - Executing as farrell@scc-hadoop.bu.edu on Linux v3.10.0-1160.36.2.el7.x86_64 amd64; 07:46:19.129 INFO GenotypeGVCFs - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 07:46:19.129 INFO GenotypeGVCFs - Start Date/Time",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7437#issuecomment-905431278:1184,Redund,Redundant,1184,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7437#issuecomment-905431278,1,['Redund'],['Redundant']
Safety,"r message. It silently fails. BTW, I'm dealing with WES data. This is the code I used:; # For GenomicDBImport, I randomly select 50 samples from our history samples(using the same probe set) along with the current batch.; time ${gatk} --java-options ""-Xmx8g -Xms2g"" GenomicsDBImport \; --tmp-dir /paedyl01/disk1/yangyxt/test_tmp \; --genomicsdb-update-workspace-path ${vcf_dir}/genomicdbimport_chr${1} \; -R ${ref_gen}/ucsc.hg19.fasta \; --batch-size 0 \; --sample-name-map ${gvcf}/batch_cohort.sample_map \; --reader-threads 5; check_return_code. # For GenotypeGVCFs; time ${gatk} --java-options ""-Xmx8g -Xms2g -DGATK_STACKTRACE_ON_USER_EXCEPTION=true"" GenotypeGVCFs \; -R ${ref_gen}/ucsc.hg19.fasta \; -V gendb://${vcf_dir}/genomicdbimport_chr${1} \; -G StandardAnnotation \; -G AS_StandardAnnotation \; -L chr${1} \; -O ${bgvcf}/all_${seq_type}_samples_plus_${sample_batch}.chr${1}.HC.vcf. # These are log records:; 02:07:51.286 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 02:07:51.321 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/yangyxt/software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl; Nov 06, 2020 2:07:56 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 02:07:56.529 INFO GenotypeGVCFs - ------------------------------------------------------------; 02:07:56.529 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.1.8.1; 02:07:56.530 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 02:08:01.543 INFO GenotypeGVCFs - Executing as yangyxt@paedyl01 on Linux v3.10.0-1062.18.1.el7.x86_64 amd64; 02:08:01.543 INFO GenotypeGVCFs - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_161-b12; 02:08:01.543 INFO GenotypeGVCFs - Start Date/Time: November 6, 2020 2:07:51",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3429#issuecomment-722764059:1124,Redund,Redundant,1124,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3429#issuecomment-722764059,1,['Redund'],['Redundant']
Safety,re-assembly-failure-bam false --num-matching-bases-in-dangling-end-to-recover -1 --error-; correction-log-odds -Infinity --error-correct-reads false --kmer-length-for-read-error-correction 25 --min-observations-for-kmer-to-be-solid 20 --likelihood-calculation-engine PairHMM --ba; se-quality-score-threshold 18 --dragstr-het-hom-ratio 2 --dont-use-dragstr-pair-hmm-scores false --pair-hmm-gap-continuation-penalty 10 --expected-mismatch-rate-for-read-disqualification 0; .02 --pair-hmm-implementation FASTEST_AVAILABLE --pcr-indel-model CONSERVATIVE --phred-scaled-global-read-mismapping-rate 45 --disable-symmetric-hmm-normalizing false --disable-cap-base-qu; alities-to-map-quality false --enable-dynamic-read-disqualification-for-genotyping false --dynamic-read-disqualification-threshold 1.0 --native-pair-hmm-threads 4 --native-pair-hmm-use-dou; ble-precision false --flow-hmm-engine-min-indel-adjust 6 --flow-hmm-engine-flat-insertion-penatly 45 --flow-hmm-engine-flat-deletion-penatly 45 --pileup-detection false --pileup-detection-; enable-indel-pileup-calling false --num-artificial-haplotypes-to-add-per-allele 5 --artifical-haplotype-filtering-kmer-size 10 --pileup-detection-snp-alt-threshold 0.1 --pileup-detection-i; ndel-alt-threshold 0.5 --pileup-detection-absolute-alt-depth 0.0 --pileup-detection-snp-adjacent-to-assembled-indel-range 5 --pileup-detection-bad-read-tolerance 0.0 --pileup-detection-pro; per-pair-read-badness true --pileup-detection-edit-distance-read-badness-threshold 0.08 --pileup-detection-chimeric-read-badness true --pileup-detection-template-mean-badness-threshold 0.0; --pileup-detection-template-std-badness-threshold 0.0 --bam-writer-type CALLED_HAPLOTYPES --dont-use-soft-clipped-bases false --override-fragment-softclip-check false --min-base-quality-s; core 10 --smith-waterman JAVA --max-mnp-distance 0 --force-call-filtered-alleles false --reference-model-deletion-quality 30 --soft-clip-low-quality-ends false --allele-informative-reads-o; verlap-marg,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8574#issuecomment-1793390789:6563,detect,detection,6563,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8574#issuecomment-1793390789,3,['detect'],"['detection', 'detection-snp-alt-threshold']"
Safety,rg.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:123); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.ap,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429:10890,abort,abortStage,10890,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429,1,['abort'],['abortStage']
Safety,"rk web interface showed errors in `sortByKey` steps:; ![sparkjob](https://user-images.githubusercontent.com/812850/27811313-9000019c-6097-11e7-82ac-aac557be31db.PNG).; And the program failed eventually:; ```; 18:24:57.885 INFO BwaAndMarkDuplicatesPipelineSpark - Shutting down engine; [July 3, 2017 6:24:57 PM CST] org.broadinstitute.hellbender.tools.spark.pipelines.BwaAndMarkDuplicatesPipelineSpark done. Elapsed time: 269.29 minutes.; Runtime.totalMemory()=4172283904; org.apache.spark.SparkException: Job aborted due to stage failure: Task 607 in stage 3.0 failed 4 times, most recent failure: Lost task 607.13 in stage 3.0 (TID 14832, 12.9.68.0, executor 24): ExecutorLostFailure (executor 24 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 169939 ms; Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312758363:1274,abort,abortStage,1274,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312758363,1,['abort'],['abortStage']
Safety,"rogram.java:31); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:163); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:149); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:190); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:27); at org.broadinstitute.hellbender.testutils.CommandLineProgramTester.runCommandLine(CommandLineProgramTester.java:107); at org.broadinstitute.hellbender.tools.HaplotypeCallerSparkIntegrationTest.testNonStrictVCFModeIsConsistentWithPastResults(HaplotypeCallerSparkIntegrationTest.java:109); Caused by:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 5.0 failed 1 times, most recent failure: Lost task 1.0 in stage 5.0 (TID 12, localhost, executor driver): java.util.ConcurrentModificationException; at java.util.ArrayList.sort(ArrayList.java:1464); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.<init>(ReadThreadingAssembler.java:81); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerReadThreadingAssemblerArgumentCollection.makeReadThreadingAssembler(HaplotypeCallerReadThreadingAssemblerArgumentCollection.java:37); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerArgumentCollection.createReadThreadingAssembler(AssemblyBasedCallerArgumentCollection.java:36); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.initialize(HaplotypeCallerEngine.java:231); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.<init>(HaplotypeCallerEng",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6513#issuecomment-601702690:4361,abort,aborted,4361,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6513#issuecomment-601702690,1,['abort'],['aborted']
Safety,"rt command used](https://github.com/Sydney-Informatics-Hub/Germline-ShortV/blob/master/gatk4_genomicsdbimport.sh) (university bioinformatics core facility's pipeline, not mine).; - 1 core and 4 GB RAM per task, but tasks seem to be using only about 1 GB RAM per task. 768 tasks (16 nodes) in total.; ```; %CPU WallTime Time Lim RSS mem memlim cpus; normal-exe = open&run; 105581211 R ds6924 hm82 genotype 4 00:18:25 02:00:00 1064GB 1064GB 3072GB 768; ```; - Jobs eventually finish if not running out of allocated time.; - Takes a long time to begin processing the first set of variants.; ```; 13:51:37.925 INFO GenotypeGVCFs - ------------------------------------------------------------; 13:51:39.736 INFO GenotypeGVCFs - Done initializing engine; 13:51:39.923 INFO ProgressMeter - Starting traversal; 13:51:39.923 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 14:23:57.323 WARN ReferenceConfidenceVariantContextMerger - Detected invalid annotations: When trying to merge variant contexts at location chr17:18363145 the annotation AS_RAW_MQ=64800.000|50400.000|0.000 was not a numerical value and was ignored; 14:23:57.346 WARN ReferenceConfidenceVariantContextMerger - Reducible annotation 'AS_RAW_MQ' detected, add -G Standard -G AS_Standard to the command to annotate in the final VC with this annotation.; 14:23:58.180 INFO ProgressMeter - chr17:18363854 32.3 1000 31.0; 14:24:13.258 INFO ProgressMeter - chr17:18376854 32.6 14000 430.0; 14:24:58.358 INFO ProgressMeter - chr17:18382854 33.3 20000 600.5; 14:32:49.287 INFO ProgressMeter - chr17:18393855 41.2 31000 753.2; 14:33:39.240 INFO ProgressMeter - chr17:18405856 42.0 43000 1024.1; 14:33:49.493 INFO ProgressMeter - chr17:18411856 42.2 49000 1162.3; 14:34:17.285 INFO ProgressMeter - chr17:18425856 42.6 63000 1478.1; ```. CPU utilisation does not improve after the variants begin processing after half an hour preparing traversal. ```; %CPU WallTime Time Lim RSS mem memlim cpus; normal-exe = ope",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8637#issuecomment-1879551089:1073,Detect,Detected,1073,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8637#issuecomment-1879551089,1,['Detect'],['Detected']
Safety,"s the available number of bins to represent the variations in the CDF in the most accurate way. Here's what I had written earlier:; ```; def get_counts_summary(counts, lo_cutoff=0.01, hi_cutoff=0.99, num_divisions=50):; sorted_counts = np.sort(counts); num_points = len(counts); lo_index = int(np.floor(lo_cutoff * num_points)); hi_index = min(int(np.ceil(hi_cutoff * num_points)), num_points - 1); lo_count = sorted_counts[lo_index]; hi_count = sorted_counts[hi_index]; abscissa_indices = np.round(np.linspace(lo_index, hi_index, num=num_divisions + 1)).astype(int); abscissa = np.asarray([sorted_counts[idx] for idx in abscissa_indices]); abscissa_counts = abscissa_indices[1:] - abscissa_indices[0:-1]; collapsed_abscissa, collapsed_abscissa_counts = collapse_abscissa_triplets(abscissa, abscissa_counts); return collapsed_abscissa, collapsed_abscissa_counts. def collapse_abscissa_triplets(abscissa, abscissa_counts):; if len(abscissa) < 3:; return abscissa, abscissa_counts; else:; pos = 0; collapsed_abscissa = [abscissa[pos]]; collapsed_abscissa_counts = []; while pos < len(abscissa) - 1:; first = abscissa[pos]; last = abscissa[pos + 1]; count = abscissa_counts[pos]; if first != last:; collapsed_abscissa += [last]; collapsed_abscissa_counts += [count]; pos += 1; else:; j = 1; while pos + j + 1 < len(abscissa):; if abscissa[pos + j + 1] == last:; count += abscissa_counts[pos + j]; j += 1; else:; break; collapsed_abscissa += [last]; collapsed_abscissa_counts += [count]; pos += j; return np.asarray(collapsed_abscissa), np.asarray(collapsed_abscissa_counts); ```; Here, `get_counts_summary` returns a tuple of `(abscissa, occurrences)`. The summary is interpreted as follows: there are `occurrences[m]` bins with counts >= `abscissa[m]` and <= `abscissa[m+1]`, for `m = 0, ...`, up to `num_divisions` (but could be smaller if some of the redundant abscissa are collapsed). In the PyMC3 code, we could evaluate the `NegativeBinomial` the the midpoint of `abscissa[m]` and `abscissa[m+1]`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-376291049:1996,redund,redundant,1996,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-376291049,1,['redund'],['redundant']
Safety,sanity check failed as expected,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8390#issuecomment-1613622866:0,sanity check,sanity check,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8390#issuecomment-1613622866,1,['sanity check'],['sanity check']
Safety,se --disable-cap-base-qu; alities-to-map-quality false --enable-dynamic-read-disqualification-for-genotyping false --dynamic-read-disqualification-threshold 1.0 --native-pair-hmm-threads 4 --native-pair-hmm-use-dou; ble-precision false --flow-hmm-engine-min-indel-adjust 6 --flow-hmm-engine-flat-insertion-penatly 45 --flow-hmm-engine-flat-deletion-penatly 45 --pileup-detection false --pileup-detection-; enable-indel-pileup-calling false --num-artificial-haplotypes-to-add-per-allele 5 --artifical-haplotype-filtering-kmer-size 10 --pileup-detection-snp-alt-threshold 0.1 --pileup-detection-i; ndel-alt-threshold 0.5 --pileup-detection-absolute-alt-depth 0.0 --pileup-detection-snp-adjacent-to-assembled-indel-range 5 --pileup-detection-bad-read-tolerance 0.0 --pileup-detection-pro; per-pair-read-badness true --pileup-detection-edit-distance-read-badness-threshold 0.08 --pileup-detection-chimeric-read-badness true --pileup-detection-template-mean-badness-threshold 0.0; --pileup-detection-template-std-badness-threshold 0.0 --bam-writer-type CALLED_HAPLOTYPES --dont-use-soft-clipped-bases false --override-fragment-softclip-check false --min-base-quality-s; core 10 --smith-waterman JAVA --max-mnp-distance 0 --force-call-filtered-alleles false --reference-model-deletion-quality 30 --soft-clip-low-quality-ends false --allele-informative-reads-o; verlap-margin 2 --smith-waterman-dangling-end-match-value 25 --smith-waterman-dangling-end-mismatch-penalty -50 --smith-waterman-dangling-end-gap-open-penalty -110 --smith-waterman-danglin; g-end-gap-extend-penalty -6 --smith-waterman-haplotype-to-reference-match-value 200 --smith-waterman-haplotype-to-reference-mismatch-penalty -150 --smith-waterman-haplotype-to-reference-ga; p-open-penalty -260 --smith-waterman-haplotype-to-reference-gap-extend-penalty -11 --smith-waterman-read-to-haplotype-match-value 10 --smith-waterman-read-to-haplotype-mismatch-penalty -15; --smith-waterman-read-to-haplotype-gap-open-penalty -30 --smith-waterman-rea,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8574#issuecomment-1793390789:7179,detect,detection-template-std-badness-threshold,7179,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8574#issuecomment-1793390789,1,['detect'],['detection-template-std-badness-threshold']
Safety,"sites annotated with PLs forced to true for reference-model confidence output; 22:42:22.734 WARN NativeLibraryLoader - Unable to find native library: native/libgkl_pairhmm_omp.dylib; 22:42:22.734 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; 22:42:22.734 INFO NativeLibraryLoader - Loading libgkl_pairhmm.dylib from jar:file:/Users/nhomer/miniconda3/envs/bfx/share/gatk4-4.1.8.1-0/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_pairhmm.dylib; 22:42:22.748 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 22:42:22.748 WARN IntelPairHmm - Ignoring request for 4 threads; not using OpenMP implementation; 22:42:22.748 INFO PairHMM - Using the AVX-accelerated native PairHMM implementation; 22:42:22.751 WARN GATKVariantContextUtils - Can't determine output variant file format from output file extension ""bam"". Defaulting to VCF.; 22:42:22.776 INFO ProgressMeter - Starting traversal; 22:42:22.777 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x000000010f47efd3, pid=96919, tid=0x0000000000002303; #; # JRE version: OpenJDK Runtime Environment (8.0_192-b01) (build 1.8.0_192-b01); # Java VM: OpenJDK 64-Bit Server VM (25.192-b01 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # C [libgkl_smithwaterman4496658849792952100.dylib+0x1fd3] _Z22smithWatermanBackTrackP10dnaSeqPairiiiiPii+0x3c3; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /private/tmp/hs_err_pid96919.log; #; # If you would like to submit a bug report, please visit:; # http://www.azulsystems.com/support/; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6733#issuecomment-667631737:4410,detect,detected,4410,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6733#issuecomment-667631737,1,['detect'],['detected']
Safety,spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429:11135,abort,abortStage,11135,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429,1,['abort'],['abortStage']
Safety,"st -L chr33.bed --genomicsdb-workspace-path chr33.db.The output log file is as follows，Using GATK jar /mnt/nvme1/opt/reseq_softwares/gatk-4.1.8.0/gatk-package-4.1.8.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx100g -Xms100g -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -jar /mnt/nvme1/opt/reseq_softwares/gatk-4.1.8.0/gatk-package-4.1.8.0-local.jar GenomicsDBImport -R /mnt/nvme1/reference/Gallus_gallus/Ensembl_g6a/Gallus_gallus.GRCg6a.dna.toplevel.fa --sample-name-map samplelist -L chr33.bed --genomicsdb-workspace-path chr33.db; 11:19:39.692 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/mnt/nvme1/opt/reseq_softwares/gatk-4.1.8.0/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jul 04, 2022 11:19:40 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 11:19:40.099 INFO GenomicsDBImport - ------------------------------------------------------------; 11:19:40.099 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.1.8.0; 11:19:40.100 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:19:40.100 INFO GenomicsDBImport - Executing as maosong@nygpu on Linux v3.10.0-1160.45.1.el7.x86_64 amd64; 11:19:40.100 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_312-b07; 11:19:40.100 INFO GenomicsDBImport - Start Date/Time: 2022年7月4日 上午11时19分39秒; 11:19:40.100 INFO GenomicsDBImport - ------------------------------------------------------------; 11:19:40.100 INFO GenomicsDBImport - ------------------------------------------------------------; 11:19:40.101 INFO GenomicsDBImport - HTSJDK Version: 2.22.0; 11:19:40.101 INFO GenomicsDBImport - Picard Version: 2.22.8; 11:19:40.101 INFO GenomicsDBImport - HTSJDK",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7952#issuecomment-1196217460:1292,detect,detect,1292,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7952#issuecomment-1196217460,1,['detect'],['detect']
Safety,"tes** total and produced an **11GB PoN** (this file includes all of the input read counts---which take up 20GB as a combined TSV file and a whopping 63GB as individual TSV files---as well as the eigenvectors, filtering results, etc.). The run can be found in /dsde/working/slee/wgs-pon-test/tieout/no-gc. It completed successfully with **-Xmx32G** (in comparison, CreatePanelOfNormals crashed after 40 minutes with -Xmx128G). The runtime breakdown was as follows:. - ~45 minutes simply from reading of the 90 TSV read-count files in serial. Hopefully #3349 should greatly speed this up. (In comparison, CombineReadCounts reading 10 files in parallel at a time took ~100 minutes to create the aforementioned 20GB combined TSV file, creating 25+GB of temp files along the way.). - ~5 minutes from the preprocessing and filtering steps. We could probably further optimize some of this code in terms of speed and heap usage. (I had to throw in a call to System.gc() to avoid an OOM with -Xmx32G, which I encountered in my first attempt at the run...). - ~5 minutes from performing the SVD of the post-filtering 8643028 x 86 matrix, maintaining 30 eigensamples. I could write a quick implementation of randomized SVD, which I think could bring this down a bit (the scikit-learn implementation takes <2 minutes on a 10M x 100 matrix), but this can probably wait. Clearly making I/O faster and more space efficient is the highest priority. Luckily it's also low hanging fruit. The 8643028 x 30 matrix of eigenvectors takes <2 minutes to read from HDF5 when the WGS PoN is used in DenoiseReadCounts, which gives us a rough idea of how long it should take to read in the original ~11.5M x 90 counts from HDF5. So once #3349 is in, then I think that a **~15 minute single-core WGS PoN could easily be viable**. I believe that a PoN on the order of this size will be all that is required for WGS denoising, if it is not already overkill. To go bigger by more than an order of magnitude, we'll have to go out of ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-317614503:1066,avoid,avoid,1066,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-317614503,1,['avoid'],['avoid']
Safety,thanks for the review @kcibul. I made some changes accordingly. re: PrepareCallset file of sample names. That would be nice! It would make this workflow simpler and it also simplifies the access requirements for PrepareCallset. re: Dockstore. We actually ruled this out because Terra says that the definition of a method configuration can change automatically if its updated in dockstore. Which can be useful but it adds a security risk since a compromised Dockstore can change the definition of the production AoU extraction WDL which runs with highly elevated permissions. We already have a script that creates method configurations from github so I can probably add something a little hacky to resolve relative imports to the raw github file that it refers to.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7242#issuecomment-846494686:432,risk,risk,432,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7242#issuecomment-846494686,1,['risk'],['risk']
Safety,this was made redundant by #3353,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2804#issuecomment-317848172:14,redund,redundant,14,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2804#issuecomment-317848172,1,['redund'],['redundant']
Safety,"u.edu, executor 2, partition 1, NODE_LOCAL, 7992 bytes); 2019-01-09 13:35:54 INFO TaskSetManager:54 - Lost task 4.2 in stage 0.0 (TID 9) on scc-q20.scc.bu.edu, executor 2: htsjdk.samtools.cram.CRAMException (Reference sequence MD5 mismatch for slice: sequence id 1, start 93925364, span 266689, expected MD5 54babf05a23e9e88a8738dfbb20a1683) [duplicate 2]; 2019-01-09 13:35:56 INFO TaskSetManager:54 - Starting task 4.3 in stage 0.0 (TID 12, scc-q20.scc.bu.edu, executor 2, partition 4, NODE_LOCAL, 7992 bytes); 2019-01-09 13:35:56 INFO TaskSetManager:54 - Lost task 1.3 in stage 0.0 (TID 11) on scc-q20.scc.bu.edu, executor 2: htsjdk.samtools.cram.CRAMException (Reference sequence MD5 mismatch for slice: sequence id 0, start 87545719, span 186383, expected MD5 492a29f6d7d6fcaf8dde06834861e7ae) [duplicate 3]; 2019-01-09 13:35:56 ERROR TaskSetManager:70 - Task 1 in stage 0.0 failed 4 times; aborting job; 2019-01-09 13:35:56 INFO YarnScheduler:54 - Cancelling stage 0; 2019-01-09 13:35:56 INFO YarnScheduler:54 - Stage 0 was cancelled; 2019-01-09 13:35:56 INFO DAGScheduler:54 - ResultStage 0 (count at CountReadsSpark.java:80) failed in 12.543 s due to Job aborted due to stage failure: Task 1 in stage 0.0 failed 4 times, most recent failure: Lost task 1.3 in stage 0.0 (TID 11, scc-q20.scc.bu.edu, executor 2): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 0, start 87545719, span 186383, expected MD5 492a29f6d7d6fcaf8dde06834861e7ae; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:32948,abort,aborting,32948,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,1,['abort'],['aborting']
Safety,"u.edu, executor 2, partition 9, NODE_LOCAL, 7992 bytes); 2019-01-07 11:34:11 INFO TaskSetManager:54 - Lost task 2.1 in stage 0.0 (TID 7) on scc-q12.scc.bu.edu, executor 2: htsjdk.samtools.cram.CRAMException (Reference sequence MD5 mismatch for slice: sequence id 0, start 160972515, span 170618, expected MD5 0cc5e1f5ec5c1b06d5a4bd5fff11b77f) [duplicate 1]; 2019-01-07 11:34:12 INFO TaskSetManager:54 - Starting task 3.3 in stage 0.0 (TID 11, scc-q21.scc.bu.edu, executor 1, partition 3, NODE_LOCAL, 7992 bytes); 2019-01-07 11:34:12 INFO TaskSetManager:54 - Lost task 1.3 in stage 0.0 (TID 9) on scc-q21.scc.bu.edu, executor 1: htsjdk.samtools.cram.CRAMException (Reference sequence MD5 mismatch for slice: sequence id 0, start 87545719, span 186383, expected MD5 492a29f6d7d6fcaf8dde06834861e7ae) [duplicate 3]; 2019-01-07 11:34:12 ERROR TaskSetManager:70 - Task 1 in stage 0.0 failed 4 times; aborting job; 2019-01-07 11:34:12 INFO YarnScheduler:54 - Cancelling stage 0; 2019-01-07 11:34:12 INFO YarnScheduler:54 - Stage 0 was cancelled; 2019-01-07 11:34:12 INFO DAGScheduler:54 - ResultStage 0 (count at CountReadsSpark.java:80) failed in 9.293 s due to Job aborted due to stage failure: Task 1 in stage 0.0 failed 4 times, most recent failure: Lost task 1.3 in stage 0.0 (TID 9, scc-q21.scc.bu.edu, executor 1): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 0, start 87545719, span 186383, expected MD5 492a29f6d7d6fcaf8dde06834861e7ae; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:4",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:33200,abort,aborting,33200,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,1,['abort'],['aborting']
Safety,"ually less expensive in typical (non-extreme) cases than reconstructing the full set of post-downsampling reads in an active region from multiple AlignmentContexts emitted by LIBS without any duplicates. I'll have to do some performance testing to see whether or not this is the case. Will try to get to this within the next few weeks, but the QC project has immediate priority. [...]. Discussed this with Ryan -- we agreed that the right thing to do is to move the enforcement of the hard cap on the total number of reads that can be in an active region from the HC walker to the engine, and have the size of the cap be controlled by a new argument (not dcov). That way you never pay the cost of storing the undownsampled reads for an active region in memory. We'd also have to educate users on exactly what the various downsampling arguments do for active region walkers. [...]. Making the hardcoded per-active-region cap settable from the command line is the easy part -- what seems hard is:; - Determining whether we can avoid storing all undownsampled reads in memory at once without affecting the quality of calls. Currently, as outlined in earlier comments on this ticket, we do a downsampling pass per locus which respects dcov (in LocusIteratorByState) but keep all undownsampled reads in memory anyway (defeating the main purpose of that first pass), then do a second downsampling pass per active region that does not respect dcov (uses the hardcoded per-region limit).; - If we find that we can't avoid storing all of the undownsampled reads in memory at once for some reason, then perhaps the right thing to do would be to completely disable the downsampling pass in LocusIteratorByState for active region traversals, and disallow the -dcov argument for active region walkers. Downsampling would then by controlled solely by the new argument to set the max # of reads per active region.; - Clarifying the meaning of the per-locus DP annotation for the HC given things like realignment of ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345:5690,avoid,avoid,5690,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345,1,['avoid'],['avoid']
Safety,ult.<init>; 0.0% 2 + 0 org.broadinstitute.hellbender.utils.read.markduplicates.ReadsKey.subkeyForFragment; 0.0% 2 + 0 htsjdk.samtools.BAMRecord.decodeBaseQualities; 0.8% 396 + 260 Total interpreted (including elided). Compiled + native Method ; 13.4% 10629 + 23 org.bdgenomics.adam.util.TwoBitFile$$anonfun$2.apply$mcZJ$sp; 8.6% 21 + 6794 org.broadinstitute.hellbender.utils.baq.BAQ.hmm_glocal; 5.7% 4492 + 0 org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine.updateRecalTablesForRead; 4.1% 3246 + 0 scala.collection.AbstractTraversable.genericBuilder; 2.4% 1932 + 0 scala.collection.AbstractSeq.size; 2.3% 1841 + 2 com.ning.compress.lzf.impl.UnsafeChunkEncoderLE.tryCompress; 2.0% 1560 + 0 org.broadinstitute.hellbender.transformers.BQSRReadTransformer.apply; 1.8% 1407 + 0 org.broadinstitute.hellbender.utils.collections.IntervalsSkipListOneContig.getOverlapping; 1.3% 1034 + 0 scala.collection.mutable.ArrayBuilder.sizeHint; 1.1% 898 + 0 com.ning.compress.lzf.impl.UnsafeChunkDecoder.decodeChunk; 1.1% 893 + 4 scala.collection.mutable.ArrayBuilder$.make; 1.1% 881 + 2 org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine.processRead; 1.0% 758 + 0 org.broadinstitute.hellbender.utils.recalibration.RecalUtils.computeCovariates; 0.9% 698 + 0 htsjdk.samtools.BinaryTagCodec.readTags; 0.7% 573 + 0 htsjdk.samtools.util.BlockCompressedOutputStream.write; 0.7% 535 + 0 org.broadinstitute.hellbender.utils.recalibration.covariates.ContextCovariate.recordValues; 0.6% 506 + 1 htsjdk.samtools.BinaryTagCodec.readSingleValue; 0.6% 459 + 1 scala.collection.Iterator$$anon$13.hasNext; 0.5% 400 + 0 org.broadinstitute.hellbender.relocated.com.google.common.collect.AbstractMapBasedMultimap.put; 0.5% 370 + 1 org.broadinstitute.hellbender.utils.baq.BAQ.calcBAQFromHMM; 0.5% 360 + 0 org.broadinstitute.hellbender.relocated.com.google.common.collect.ImmutableListMultimap.copyOf; 0.4% 348 + 5 scala.collection.IndexedSeqOptimized$class.zip; 0.4% 330 + 3 com.esotericsoft,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1657#issuecomment-208967490:2792,Unsafe,UnsafeChunkDecoder,2792,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1657#issuecomment-208967490,1,['Unsafe'],['UnsafeChunkDecoder']
Safety,un-physical-phasing ; false --do-not-correct-overlapping-quality false --use-filtered-reads-for-annotations false --use-flow-aligner-for-stepwise-hc-filtering false --adaptive-pruning false --do-not-recover-dan; gling-branches false --recover-dangling-heads false --kmer-size 10 --kmer-size 25 --dont-increase-kmer-sizes-for-cycles false --allow-non-unique-kmers-in-ref false --num-pruning-samples 1 ; --min-dangling-branch-length 4 --recover-all-dangling-branches false --max-num-haplotypes-in-population 128 --min-pruning 2 --adaptive-pruning-initial-error-rate 0.001 --pruning-lod-thresh; old 2.302585092994046 --pruning-seeding-lod-threshold 9.210340371976184 --max-unpruned-variants 100 --linked-de-bruijn-graph false --disable-artificial-haplotype-recovery false --enable-le; gacy-graph-cycle-detection false --debug-assembly false --debug-graph-transformations false --capture-assembly-failure-bam false --num-matching-bases-in-dangling-end-to-recover -1 --error-; correction-log-odds -Infinity --error-correct-reads false --kmer-length-for-read-error-correction 25 --min-observations-for-kmer-to-be-solid 20 --likelihood-calculation-engine PairHMM --ba; se-quality-score-threshold 18 --dragstr-het-hom-ratio 2 --dont-use-dragstr-pair-hmm-scores false --pair-hmm-gap-continuation-penalty 10 --expected-mismatch-rate-for-read-disqualification 0; .02 --pair-hmm-implementation FASTEST_AVAILABLE --pcr-indel-model CONSERVATIVE --phred-scaled-global-read-mismapping-rate 45 --disable-symmetric-hmm-normalizing false --disable-cap-base-qu; alities-to-map-quality false --enable-dynamic-read-disqualification-for-genotyping false --dynamic-read-disqualification-threshold 1.0 --native-pair-hmm-threads 4 --native-pair-hmm-use-dou; ble-precision false --flow-hmm-engine-min-indel-adjust 6 --flow-hmm-engine-flat-insertion-penatly 45 --flow-hmm-engine-flat-deletion-penatly 45 --pileup-detection false --pileup-detection-; enable-indel-pileup-calling false --num-artificial-haplotypes-to-add-per-allele,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8574#issuecomment-1793390789:5432,recover,recovery,5432,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8574#issuecomment-1793390789,3,"['detect', 'recover']","['detection', 'recover', 'recovery']"
Safety,use-dragstr-pair-hmm-scores false --pair-hmm-gap-continuation-penalty 10 --expected-mismatch-rate-for-read-disqualification 0; .02 --pair-hmm-implementation FASTEST_AVAILABLE --pcr-indel-model CONSERVATIVE --phred-scaled-global-read-mismapping-rate 45 --disable-symmetric-hmm-normalizing false --disable-cap-base-qu; alities-to-map-quality false --enable-dynamic-read-disqualification-for-genotyping false --dynamic-read-disqualification-threshold 1.0 --native-pair-hmm-threads 4 --native-pair-hmm-use-dou; ble-precision false --flow-hmm-engine-min-indel-adjust 6 --flow-hmm-engine-flat-insertion-penatly 45 --flow-hmm-engine-flat-deletion-penatly 45 --pileup-detection false --pileup-detection-; enable-indel-pileup-calling false --num-artificial-haplotypes-to-add-per-allele 5 --artifical-haplotype-filtering-kmer-size 10 --pileup-detection-snp-alt-threshold 0.1 --pileup-detection-i; ndel-alt-threshold 0.5 --pileup-detection-absolute-alt-depth 0.0 --pileup-detection-snp-adjacent-to-assembled-indel-range 5 --pileup-detection-bad-read-tolerance 0.0 --pileup-detection-pro; per-pair-read-badness true --pileup-detection-edit-distance-read-badness-threshold 0.08 --pileup-detection-chimeric-read-badness true --pileup-detection-template-mean-badness-threshold 0.0; --pileup-detection-template-std-badness-threshold 0.0 --bam-writer-type CALLED_HAPLOTYPES --dont-use-soft-clipped-bases false --override-fragment-softclip-check false --min-base-quality-s; core 10 --smith-waterman JAVA --max-mnp-distance 0 --force-call-filtered-alleles false --reference-model-deletion-quality 30 --soft-clip-low-quality-ends false --allele-informative-reads-o; verlap-margin 2 --smith-waterman-dangling-end-match-value 25 --smith-waterman-dangling-end-mismatch-penalty -50 --smith-waterman-dangling-end-gap-open-penalty -110 --smith-waterman-danglin; g-end-gap-extend-penalty -6 --smith-waterman-haplotype-to-reference-match-value 200 --smith-waterman-haplotype-to-reference-mismatch-penalty -150 --smith-waterman-ha,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8574#issuecomment-1793390789:6864,detect,detection-snp-adjacent-to-assembled-indel-range,6864,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8574#issuecomment-1793390789,2,['detect'],"['detection-bad-read-tolerance', 'detection-snp-adjacent-to-assembled-indel-range']"
Safety,"ut variants.funcotated.vcf --output-file-format VCF; Using GATK jar /home/deepak/software_library/gatk-4.1.7.0/gatk-package-4.1.7.0-local.jar. java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/deepak/software_library/gatk-4.1.7.0/gatk-package-4.1.7.0-local.jar Funcotator --variant /home/deepak/software_library/gatk-4.1.7.0/SAMPL3_VARIANTFIL.vcf --reference /media/deepak/EXTRA/Genomedir/hg38/hg38.fasta --ref-version hg38 --data-sources-path /media/deepak/EXTRA/FUNCOTATOR_DATA/DATA_SOURCES --output variants.funcotated.vcf --output-file-format VCF; 16:01:36.165 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/deepak/software_library/gatk-4.1.7.0/gatk-package-4.1.7.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; May 12, 2020 4:01:36 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 16:01:36.870 INFO Funcotator - ------------------------------------------------------------; 16:01:36.871 INFO Funcotator - The Genome Analysis Toolkit (GATK) v4.1.7.0; 16:01:36.871 INFO Funcotator - For support and documentation go to https://software.broadinstitute.org/gatk/; 16:01:36.871 INFO Funcotator - Executing as deepak@ngs on Linux v5.3.0-26-generic amd64; 16:01:36.871 INFO Funcotator - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_241-b07; 16:01:36.871 INFO Funcotator - Start Date/Time: 12 May, 2020 4:01:35 PM IST; 16:01:36.871 INFO Funcotator - ------------------------------------------------------------; 16:01:36.871 INFO Funcotator - ------------------------------------------------------------; 16:01:36.872 INFO Funcotator - HTSJDK Version: 2.21.2; 16:01:36.872 INFO Funcotator - Picard Version: 2.21.9; 16:01:36.872 INFO Funcotator - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 16:01:36.872 INFO Funcotator - H",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6598#issuecomment-664565036:1281,detect,detect,1281,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6598#issuecomment-664565036,1,['detect'],['detect']
Safety,"ut.; - For all workflows, we always collect integer read counts; for WGS, these are output as both HDF5 and TSV and the HDF5 is used for subsequent input.; - For the case workflow, we always collect allelic counts at all sites and output as TSV.; - [x] We should output all data files as HDF5 by default and as TSV optionally. EDIT: This is done for `CollectFragmentCounts`.; - [x] We will need to update the workflows when @MartonKN and @asmirnov239 get `PreprocessIntervals` and `CollectReadCounts` merged, respectively. These tools will remove the awkwardness required by `PadTargets` and `CalculateTargetCoverage`/`SparkGenomeReadCounts`. Denoising:; - All parameters are exposed in the PoN creation tool (#3356).; - Without a PoN, standardization and optional GC correction are performed (#3570).; - Other than the minor point about sample mean/median being used inconsistently noted above, the denoising process is essentially exactly the same mathematically as before (""superficial"" differences include the vastly improved memory and I/O optimizations, the ability to adjust number of principal components used, the removal of redundant SVDs, the enforcement of consistent GC-bias correction).; - [ ] That said, I'll carry over this TODO from above: Revisit standardization procedure by checking with simulated data. We should make sure that the centering of the data does not rescale the true copy ratio.; - The only major difference is we no longer make a QC PoN or check for large events. This was performed awkwardly in the old pipeline, so I'd rather not port it over. Eventually we will do all denoising with the gCNV coverage model anyway.; - Pre/tangent-normalization copy ratio are now referred to as standardized/denoised copy ratio.; - [x] Old code is still used for GC-bias correction in `CreateReadCountPanelOfNormals`, and we still use the `AnnotateTargets` tool. We should port this over (possibly as part of `PreprocessIntervals`) at some point (actually, I think we will be for",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:1895,redund,redundant,1895,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828,1,['redund'],['redundant']
Safety,uter.access$200(ExecuteActionsTaskExecuter.java:93); at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter$TaskExecution.execute(ExecuteActionsTaskExecuter.java:237); at org.gradle.internal.execution.steps.ExecuteStep.lambda$execute$1(ExecuteStep.java:33); at org.gradle.internal.execution.steps.ExecuteStep.execute(ExecuteStep.java:33); at org.gradle.internal.execution.steps.ExecuteStep.execute(ExecuteStep.java:26); at org.gradle.internal.execution.steps.CleanupOutputsStep.execute(CleanupOutputsStep.java:58); at org.gradle.internal.execution.steps.CleanupOutputsStep.execute(CleanupOutputsStep.java:35); at org.gradle.internal.execution.steps.ResolveInputChangesStep.execute(ResolveInputChangesStep.java:48); at org.gradle.internal.execution.steps.ResolveInputChangesStep.execute(ResolveInputChangesStep.java:33); at org.gradle.internal.execution.steps.CancelExecutionStep.execute(CancelExecutionStep.java:39); at org.gradle.internal.execution.steps.TimeoutStep.executeWithoutTimeout(TimeoutStep.java:73); at org.gradle.internal.execution.steps.TimeoutStep.execute(TimeoutStep.java:54); at org.gradle.internal.execution.steps.CatchExceptionStep.execute(CatchExceptionStep.java:35); at org.gradle.internal.execution.steps.CreateOutputsStep.execute(CreateOutputsStep.java:51); at org.gradle.internal.execution.steps.SnapshotOutputsStep.execute(SnapshotOutputsStep.java:45); at org.gradle.internal.execution.steps.SnapshotOutputsStep.execute(SnapshotOutputsStep.java:31); at org.gradle.internal.execution.steps.CacheStep.executeWithoutCache(CacheStep.java:208); at org.gradle.internal.execution.steps.CacheStep.execute(CacheStep.java:70); at org.gradle.internal.execution.steps.CacheStep.execute(CacheStep.java:45); at org.gradle.internal.execution.steps.BroadcastChangingOutputsStep.execute(BroadcastChangingOutputsStep.java:49); at org.gradle.internal.execution.steps.StoreSnapshotsStep.execute(StoreSnapshotsStep.java:43); at org.gradle.internal.execution.steps.StoreSnapshotsSte,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6466#issuecomment-590387973:8551,Timeout,TimeoutStep,8551,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6466#issuecomment-590387973,2,['Timeout'],['TimeoutStep']
Safety,"we need a test that has an actual gvcf file and compares to a stored file. On Fri, Jan 8, 2016 at 4:04 PM, Louis Bergelson notifications@github.com; wrote:. > Also, you had comment about a test being removed, but the comment got lost; > before I replied to it. The test that I removed was testing for a condition; > that got converted into an exception, and then avoided by restructuring the; > calling class to avoid having that condition come up. I added tests to make; > sure that it throws properly in those cases though.; > ; > —; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/gatk/pull/859#issuecomment-170123169.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/859#issuecomment-170123873:363,avoid,avoided,363,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/859#issuecomment-170123873,2,['avoid'],"['avoid', 'avoided']"
Safety,"weinkauf/notes/persistence1d.html and http://www2.iap.fr/users/sousbie/web/html/indexd3dd.html?post/Persistence-and-simplification). A straightforward watershed algorithm can sort all local minima by persistence in linear time after an initial sort of the data.; 5) These sets of local minima from all window sizes together provide the pool of candidate changepoints (some of which may overlap exactly or approximately). We perform backwards selection using the global segmentation cost. That is, we compute the global segmentation cost given all the candidate changepoints, calculate the cost change for removing each of the changepoints individually, remove the changepoint with the minimum cost change, and repeat. This gives the global cost as a function of the number of changepoints _C_.; 6) Add a penalty _a C + b C log(N / C)_ to the global cost and find the minimum to determine the number of changepoints. For the above simulated data, _a = 2_ and _b = 2_ works well, recovering all of the changepoints in the above example with no false positives:; ![6](https://user-images.githubusercontent.com/11076296/29582517-fbd604be-874a-11e7-8ef7-7bd727f65dcb.png). ![7](https://user-images.githubusercontent.com/11076296/29582518-fddf015c-874a-11e7-89e4-87250d2a52ab.png). In contrast, CBS produces two false positives (around the third and seventh of the true changepoints):. ![8](https://user-images.githubusercontent.com/11076296/29582545-18875126-874b-11e7-9166-9061bb120e43.png). We can change the penalty factor to smooth out less significant segments (which may be due to systematic noise, GC waves, etc.). Setting _a = 10, b = 10_ gives:; ![9](https://user-images.githubusercontent.com/11076296/29582598-515dffe0-874b-11e7-93c4-59422cd43b54.png); ![10](https://user-images.githubusercontent.com/11076296/29583130-0fe5316c-874d-11e7-8504-43618928cf68.png). (Note that the DNAcopy implementation of CBS does not allow for such simple control of the ""false-positive rate,"" as even setting the",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-324125586:2866,recover,recovering,2866,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-324125586,1,['recover'],['recovering']
Safety,"what are the decision criteria about that? how about some sanity check for the bams created - eg print reads on a 30GB file and then 'samtools view' to a samfile and compare MD5s (results in SAM format should be identical, down to a MD5) ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1597#issuecomment-201095496:58,sanity check,sanity check,58,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1597#issuecomment-201095496,1,['sanity check'],['sanity check']
Safety,"when trying to build GATK fully I get this error:; ```; > Task :gatkDoc FAILED; Execution optimizations have been disabled for task ':gatkDoc' to ensure correctness due to the following reasons:; - Gradle detected a problem with the following location: '/home/jeremie/GATK/build/classes/java/main'. Reason: Task ':gatkDoc' uses this output of task ':condaStandardEnvironmentDefinition' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem.; - Gradle detected a problem with the following location: '/home/jeremie/GATK/build/resources/main'. Reason: Task ':gatkDoc' uses this output of task ':condaStandardEnvironmentDefinition' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':gatkDoc'.; > Javadoc generation failed. Generated Javadoc options file (useful for troubleshooting): '/home/jeremie/GATK/build/tmp/gatkDoc/javadoc.options'. * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full insights. * Get more help at https://help.gradle.org. Deprecated Gradle features were used in this build, making it incompatible with Gradle 8.0. You can use '--warning-mode all' to show the individual deprecation warnings and determine if they come from your own scripts or plugins. See https://docs.gradle.org/7.3.2/userguide/command_line_interface.html#sec:command_line_warnings. Execution optimizations have been disabled for 1 invalid unit",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7936#issuecomment-1202544500:205,detect,detected,205,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7936#issuecomment-1202544500,2,['detect'],['detected']
Safety,y(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2027); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2048); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2067); at org,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:38855,abort,abortStage,38855,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,2,['abort'],['abortStage']
Safety,"{vcf_dir}/genomicdbimport_chr${1} \; -R ${ref_gen}/ucsc.hg19.fasta \; --batch-size 0 \; --sample-name-map ${gvcf}/batch_cohort.sample_map \; --reader-threads 5; check_return_code. # For GenotypeGVCFs; time ${gatk} --java-options ""-Xmx8g -Xms2g -DGATK_STACKTRACE_ON_USER_EXCEPTION=true"" GenotypeGVCFs \; -R ${ref_gen}/ucsc.hg19.fasta \; -V gendb://${vcf_dir}/genomicdbimport_chr${1} \; -G StandardAnnotation \; -G AS_StandardAnnotation \; -L chr${1} \; -O ${bgvcf}/all_${seq_type}_samples_plus_${sample_batch}.chr${1}.HC.vcf. # These are log records:; 02:07:51.286 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 02:07:51.321 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/yangyxt/software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl; Nov 06, 2020 2:07:56 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 02:07:56.529 INFO GenotypeGVCFs - ------------------------------------------------------------; 02:07:56.529 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.1.8.1; 02:07:56.530 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 02:08:01.543 INFO GenotypeGVCFs - Executing as yangyxt@paedyl01 on Linux v3.10.0-1062.18.1.el7.x86_64 amd64; 02:08:01.543 INFO GenotypeGVCFs - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_161-b12; 02:08:01.543 INFO GenotypeGVCFs - Start Date/Time: November 6, 2020 2:07:51 AM HKT; 02:08:01.543 INFO GenotypeGVCFs - ------------------------------------------------------------; 02:08:01.544 INFO GenotypeGVCFs - ------------------------------------------------------------; 02:08:01.544 INFO GenotypeGVCFs - HTSJDK Version: 2.23.0; 02:08:01.545 INFO GenotypeGVCFs - Picard Version: 2.22.8; 02:08:01.545 INFO GenotypeGVCFs - HTSJDK Defaults.C",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3429#issuecomment-722764059:1527,detect,detect,1527,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3429#issuecomment-722764059,1,['detect'],['detect']
Safety,"👍 Looks good to me. Did you want to try to switch to the release version, or should we merge this as is? . I didn't know even know we had redundant `hidden` / `hiddenOption` tags... both unused.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2293#issuecomment-264959626:138,redund,redundant,138,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2293#issuecomment-264959626,1,['redund'],['redundant']
Security,"	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at org.apache.spark.storage.BlockManager.reportAllBlocks(BlockManager.scala:217); 	at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:236); 	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:522); 	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:547); 	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547); 	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547); 	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953); 	at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:547); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308); 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180); 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); 17/10/18 17:35:58 INFO BlockManagerMaster: BlockManagerMaster stopped; 17/10/18 17:35:58 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker-1,5,main]; java.lang.OutOfMemoryError: Java heap space; 	at htsjdk.samtools.BAMRecordCodec.decode(BAMRecordCodec.java:208); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.getNextRecord(BAMFileReader.java:829); 	at htsjdk.samtools.BAMFileReader$BAMFileIndexIterator.getNextRecord(BAMFileReader.java:981); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.advance(BAMFileReader.java:803); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFil",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-337554749:5520,access,access,5520,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-337554749,1,['access'],['access']
Security,	at sun.net.www.http.KeepAliveStream.close(KeepAliveStream.java:85); 	at java.io.FilterInputStream.close(FilterInputStream.java:181); 	at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.close(HttpURLConnection.java:3448); 	at org.gradle.wrapper.Download.downloadInternal(Download.java:77); 	at org.gradle.wrapper.Download.download(Download.java:44); 	at org.gradle.wrapper.Install$1.call(Install.java:61); 	at org.gradle.wrapper.Install$1.call(Install.java:48); 	at org.gradle.wrapper.ExclusiveFileAccessManager.access(ExclusiveFileAccessManager.java:69); 	at org.gradle.wrapper.Install.createDist(Install.java:48); 	at org.gradle.wrapper.WrapperExecutor.execute(WrapperExecutor.java:107); 	at org.gradle.wrapper.GradleWrapperMain.main(GradleWrapperMain.java:61); Caused by: javax.net.ssl.SSLException: java.net.SocketException: Connection reset; 	at sun.security.ssl.Alerts.getSSLException(Alerts.java:208); 	at sun.security.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1949); 	at sun.security.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1906); 	at sun.security.ssl.SSLSocketImpl.handleException(SSLSocketImpl.java:1870); 	at sun.security.ssl.SSLSocketImpl.handleException(SSLSocketImpl.java:1815); 	at sun.security.ssl.AppInputStream.read(AppInputStream.java:116); 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:284); 	at java.io.BufferedInputStream.read(BufferedInputStream.java:345); 	at sun.net.www.MeteredStream.read(MeteredStream.java:134); 	at java.io.FilterInputStream.read(FilterInputStream.java:133); 	at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.read(HttpURLConnection.java:3375); 	at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.read(HttpURLConnection.java:3368); 	at org.gradle.wrapper.Download.downloadInternal(Download.java:62); 	... 7 more; Caused by: java.net.SocketException: Connection reset; 	at java.net.SocketInputStream.read(SocketInputStream.java:210); 	at java.net.SocketInputStream.read(SocketInputStream.java:141); ,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4194#issuecomment-358498401:1788,secur,security,1788,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4194#issuecomment-358498401,1,['secur'],['security']
Security, 0 htsjdk.samtools.BinaryCigarCodec.encode; 0.1% 21 + 0 htsjdk.samtools.SAMBinaryTagAndValue.remove; 25.5% 3799 + 84 Total compiled (including elided). Stub + native Method ; 23.3% 0 + 3554 java.util.zip.Deflater.deflateBytes; 20.8% 0 + 3164 java.lang.System.identityHashCode; 14.6% 0 + 2219 java.lang.String.intern; 4.5% 0 + 679 java.util.zip.Inflater.inflateBytes; 2.8% 0 + 426 java.io.FileOutputStream.writeBytes; 2.1% 0 + 318 java.net.SocketInputStream.socketRead0; 1.7% 0 + 266 sun.nio.ch.EPollArrayWrapper.epollWait; 0.7% 0 + 99 sun.nio.ch.NativeThread.current; 0.6% 0 + 96 java.util.zip.Deflater.reset; 0.6% 0 + 88 java.util.zip.Inflater.reset; 0.4% 0 + 58 org.apache.hadoop.util.NativeCrc32.nativeComputeChunkedSumsByteArray; 0.3% 0 + 47 java.io.FileInputStream.readBytes; 0.3% 0 + 41 sun.nio.ch.FileDispatcherImpl.read0; 0.2% 0 + 26 java.lang.Throwable.fillInStackTrace; 0.1% 0 + 19 java.io.UnixFileSystem.getLength; 0.1% 0 + 13 java.lang.Object.getClass; 0.1% 0 + 12 java.lang.Object.hashCode; 0.1% 11 + 0 java.lang.ClassLoader.defineClass1; 0.1% 3 + 7 java.lang.Class.forName0; 0.1% 0 + 9 sun.nio.ch.FileDispatcherImpl.size0; 0.1% 0 + 9 java.util.zip.ZipFile.getEntry; 0.1% 0 + 8 java.lang.Class.isArray; 0.0% 0 + 7 java.io.FileOutputStream.open0; 0.0% 0 + 6 java.lang.Class.isPrimitive; 0.0% 0 + 6 java.io.FileOutputStream.close0; 73.9% 16 + 11236 Total stub (including elided). Thread-local ticks:; 60.2% 23027 Blocked (of total); 0.0% 1 Class loader; 0.0% 1 Unknown: thread_state; ```. and on igzip:. ```; Flat profile of 425.43 secs (38916 total ticks): Executor task launch worker-4. Interpreted + native Method ; 0.1% 0 + 23 java.net.Inet6AddressImpl.lookupAllHostAddr; 0.1% 0 + 16 java.io.UnixFileSystem.delete0; 0.1% 14 + 0 org.apache.spark.util.collection.TimSort.sort; 0.1% 10 + 0 org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1$$anonfun$12$$anonfun$apply$4.apply$mcV$sp; 0.1% 10 + 0 htsjdk.samtools.BAMRecordCodec.encode; 0.0% 0 + 5 java.net.SocketInp,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1702#issuecomment-210127581:4961,hash,hashCode,4961,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1702#issuecomment-210127581,1,['hash'],['hashCode']
Security," 08:27:11.333 INFO Mutect2 - Done initializing engin; 08:27:11.381 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/gatk/gatk-package-4.1.1.0-local.jar!/com/intel/gkl/native/libgkl_utils.s; 08:27:11.383 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk/gatk-package-4.1.1.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.s; 08:27:11.426 INFO **IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHM**; 08:27:11.427 INFO IntelPairHmm - Available threads: 4; 08:27:11.428 INFO IntelPairHmm - Requested threads: 4; 08:27:11.428 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementatio; 08:27:11.432 INFO Mutect2 - Shutting down engin; [April 23, 2019 8:27:11 AM UTC] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.09 minutes.; Runtime.totalMemory()=190840832; java.lang.IllegalArgumentException: samples cannot be empt; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:724); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.ReferenceConfidenceModel.<init>(ReferenceConfidenceModel.java:116); 	at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticReferenceConfidenceModel.<init>(SomaticReferenceConfidenceModel.java:38); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2Engine.<init>(Mutect2Engine.java:149); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2.onTraversalStart(Mutect2.java:286); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:982); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4665#issuecomment-485729136:3312,validat,validateArg,3312,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4665#issuecomment-485729136,1,['validat'],['validateArg']
Security, 18607 +1212 ; ===============================================; Files 1168 1225 +57 ; Lines 62907 68654 +5747 ; Branches 9800 10837 +1037 ; ===============================================; + Hits 50604 55189 +4585 ; - Misses 8376 9267 +891 ; - Partials 3927 4198 +271; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/3331?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...titute/hellbender/tools/walkers/GenotypeGVCFs.java](https://codecov.io/gh/broadinstitute/gatk/pull/3331?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL0dlbm90eXBlR1ZDRnMuamF2YQ==) | `90% <ø> (ø)` | `47 <0> (ø)` | :arrow_down: |; | [...org/broadinstitute/hellbender/utils/GenomeLoc.java](https://codecov.io/gh/broadinstitute/gatk/pull/3331?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9HZW5vbWVMb2MuamF2YQ==) | `68.362% <ø> (ø)` | `85 <0> (ø)` | :arrow_down: |; | [...r/tools/walkers/variantutils/ValidateVariants.java](https://codecov.io/gh/broadinstitute/gatk/pull/3331?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhcmlhbnR1dGlscy9WYWxpZGF0ZVZhcmlhbnRzLmphdmE=) | `80% <81.25%> (-0.597%)` | `28 <17> (+10)` | |; | [...ools/spark/pathseq/PSFilterArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/3331?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9wYXRoc2VxL1BTRmlsdGVyQXJndW1lbnRDb2xsZWN0aW9uLmphdmE=) | `85% <0%> (-15%)` | `3% <0%> (+2%)` | |; | [...lbender/tools/spark/pathseq/PathSeqScoreSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/3331?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9wYXRoc2VxL1BhdGhTZXFTY29yZVNwYXJrLmphdmE=) | `72.84% <0%> (-7.16%)` | `13% <0%> (+5%)` | |; | [.../hellbender/tools/walkers/vqsr/TrancheManager.java](https://codecov.io/gh/broadinstitute/gatk/pull/3331?src=pr&el=tre,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3331#issuecomment-317128040:1530,Validat,ValidateVariants,1530,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3331#issuecomment-317128040,1,['Validat'],['ValidateVariants']
Security, <0%> (-30%)` | `1% <0%> (-2%)` | |; | [...er/tools/spark/sv/discovery/AlignmentInterval.java](https://codecov.io/gh/broadinstitute/gatk/pull/4066/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvQWxpZ25tZW50SW50ZXJ2YWwuamF2YQ==) | `91.064% <0%> (ø)` | `65% <0%> (-1%)` | :arrow_down: |; | [...nder/tools/spark/pipelines/PrintVariantsSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4066/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvUHJpbnRWYXJpYW50c1NwYXJrLmphdmE=) | `66.667% <0%> (ø)` | `4% <0%> (+2%)` | :arrow_up: |; | [...itute/hellbender/tools/walkers/SplitIntervals.java](https://codecov.io/gh/broadinstitute/gatk/pull/4066/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL1NwbGl0SW50ZXJ2YWxzLmphdmE=) | `88.889% <0%> (+0.654%)` | `10% <0%> (+4%)` | :arrow_up: |; | [...r/tools/walkers/validation/RemoveNearbyIndels.java](https://codecov.io/gh/broadinstitute/gatk/pull/4066/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vUmVtb3ZlTmVhcmJ5SW5kZWxzLmphdmE=) | `91.429% <0%> (+0.952%)` | `9% <0%> (+4%)` | :arrow_up: |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4066/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `80% <0%> (+1.29%)` | `39% <0%> (ø)` | :arrow_down: |; | [...adinstitute/hellbender/tools/IndexFeatureFile.java](https://codecov.io/gh/broadinstitute/gatk/pull/4066/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9JbmRleEZlYXR1cmVGaWxlLmphdmE=) | `91.667% <0%> (+1.344%)` | `17% <0%> (+5%)` | :arrow_up: |; | [...r/tools/walkers/variantutils/ValidateVariants.java](https://codecov.io/gh/broadinstitute/gatk/pull/4066/diff?src=pr&,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4066#issuecomment-355695318:2180,validat,validation,2180,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4066#issuecomment-355695318,1,['validat'],['validation']
Security," ApplyBQSR - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 23:41:16.953 INFO ApplyBQSR - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:41:16.953 INFO ApplyBQSR - Deflater: IntelDeflater; 23:41:16.953 INFO ApplyBQSR - Inflater: IntelInflater; 23:41:16.953 INFO ApplyBQSR - GCS max retries/reopens: 20; 23:41:16.953 INFO ApplyBQSR - Requester pays: disabled; 23:41:16.953 INFO ApplyBQSR - Initializing engine; 23:41:17.460 INFO ApplyBQSR - Done initializing engine; 23:41:17.527 INFO ProgressMeter - Starting traversal; 23:41:17.527 INFO ProgressMeter - Current Locus Elapsed Minutes Reads Processed Reads/Minute; 23:41:17.849 INFO ApplyBQSR - Shutting down engine; [February 26, 2020 11:41:17 PM EST] org.broadinstitute.hellbender.tools.walkers.bqsr.ApplyBQSR done. Elapsed time: 0.02 minutes.; Runtime.totalMemory()=2411724800; java.lang.IllegalStateException: The covariates table is missing ReadGroup S3_2 in RecalTable0; 	at org.broadinstitute.hellbender.utils.Utils.validate(Utils.java:752); 	at org.broadinstitute.hellbender.utils.recalibration.covariates.ReadGroupCovariate.keyForReadGroup(ReadGroupCovariate.java:81); 	at org.broadinstitute.hellbender.utils.recalibration.covariates.ReadGroupCovariate.recordValues(ReadGroupCovariate.java:53); 	at org.broadinstitute.hellbender.utils.recalibration.covariates.StandardCovariateList.recordAllValuesInStorage(StandardCovariateList.java:133); 	at org.broadinstitute.hellbender.utils.recalibration.RecalUtils.computeCovariates(RecalUtils.java:546); 	at org.broadinstitute.hellbender.utils.recalibration.RecalUtils.computeCovariates(RecalUtils.java:527); 	at org.broadinstitute.hellbender.transformers.BQSRReadTransformer.apply(BQSRReadTransformer.java:145); 	at org.broadinstitute.hellbender.transformers.BQSRReadTransformer.apply(BQSRReadTransformer.java:27); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); 	at jav",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6242#issuecomment-592005237:7645,validat,validate,7645,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6242#issuecomment-592005237,1,['validat'],['validate']
Security, Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...nder/tools/walkers/vqsr/FilterVariantTranches.java](https://codecov.io/gh/broadinstitute/gatk/pull/5175/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3IvRmlsdGVyVmFyaWFudFRyYW5jaGVzLmphdmE=) | `92.24% <ø> (ø)` | `42 <0> (ø)` | :arrow_down: |; | [...der/tools/walkers/vqsr/CNNVariantWriteTensors.java](https://codecov.io/gh/broadinstitute/gatk/pull/5175/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3IvQ05OVmFyaWFudFdyaXRlVGVuc29ycy5qYXZh) | `85.71% <100%> (+2.38%)` | `4 <0> (ø)` | :arrow_down: |; | [...hellbender/tools/walkers/vqsr/CNNVariantTrain.java](https://codecov.io/gh/broadinstitute/gatk/pull/5175/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3IvQ05OVmFyaWFudFRyYWluLmphdmE=) | `60% <46.66%> (-20.65%)` | `4 <0> (ø)` | |; | [...lkers/validation/EvaluateInfoFieldConcordance.java](https://codecov.io/gh/broadinstitute/gatk/pull/5175/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vRXZhbHVhdGVJbmZvRmllbGRDb25jb3JkYW5jZS5qYXZh) | `72.58% <72.58%> (ø)` | `14 <14> (?)` | |; | [...ellbender/tools/walkers/vqsr/CNNScoreVariants.java](https://codecov.io/gh/broadinstitute/gatk/pull/5175/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3IvQ05OU2NvcmVWYXJpYW50cy5qYXZh) | `73.68% <77.14%> (-1.32%)` | `41 <17> (+1)` | |; | [...ools/walkers/validation/InfoConcordanceRecord.java](https://codecov.io/gh/broadinstitute/gatk/pull/5175/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vSW5mb0NvbmNvcmRhbmNlUmVjb3JkLmphdmE=) | `93.93% <93.93%> (ø)` | `8 <8> (?)` | |; | [...n/EvaluateInfoFieldConcordanceIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pul,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5175#issuecomment-423756354:2067,validat,validation,2067,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5175#issuecomment-423756354,1,['validat'],['validation']
Security, Files 1781 1782 +1 ; Lines 132255 133311 +1056 ; Branches 14734 15003 +269 ; ===============================================; + Hits 114191 115180 +989 ; - Misses 12750 12781 +31 ; - Partials 5314 5350 +36; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/5031?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...ava/org/broadinstitute/hellbender/utils/Utils.java](https://codecov.io/gh/broadinstitute/gatk/pull/5031/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9VdGlscy5qYXZh) | `84.469% <ø> (+3.993%)` | `298 <0> (+153)` | :arrow_up: |; | [...aplotypecaller/HaplotypeCallerIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5031/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9IYXBsb3R5cGVDYWxsZXJJbnRlZ3JhdGlvblRlc3QuamF2YQ==) | `91.499% <100%> (ø)` | `85 <6> (ø)` | :arrow_down: |; | [...walkers/validation/ConcordanceIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5031/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vQ29uY29yZGFuY2VJbnRlZ3JhdGlvblRlc3QuamF2YQ==) | `100% <100%> (ø)` | `6 <0> (ø)` | :arrow_down: |; | [...ct/CreateSomaticPanelOfNormalsIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5031/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL211dGVjdC9DcmVhdGVTb21hdGljUGFuZWxPZk5vcm1hbHNJbnRlZ3JhdGlvblRlc3QuamF2YQ==) | `100% <100%> (ø)` | `3 <0> (ø)` | :arrow_down: |; | [...eVcfWithExpectedAlleleFractionIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5031/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vQW5ub3RhdGVWY2ZXaXRoRXhwZWN0ZWRBbGxlbGVGcmFjdGlvbkludGVncmF0aW9uVGVzdC5qYXZh) | `100% <100%> (ø)` | `7 <0> (ø)` | :arrow_down: |; | [...,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5031#issuecomment-406311079:1572,validat,validation,1572,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5031#issuecomment-406311079,1,['validat'],['validation']
Security," HaplotypeCaller - GCS max retries/reopens: 20 01:13:16.078 INFO HaplotypeCaller - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes 01:13:16.078 INFO HaplotypeCaller - Initializing engine 01:13:17.087 INFO HaplotypeCaller - Shutting down engine [January 18, 2020 1:13:17 AM IST] org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller done. Elapsed time: 0.02 minutes. Runtime.totalMemory()=2216689664 java.lang.NullPointerException at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.getContigNames(SequenceDictionaryUtils.java:463) at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.getCommonContigsByName(SequenceDictionaryUtils.java:457) at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.compareDictionaries(SequenceDictionaryUtils.java:234) at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.validateDictionaries(SequenceDictionaryUtils.java:150) at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.validateDictionaries(SequenceDictionaryUtils.java:98) at org.broadinstitute.hellbender.engine.GATKTool.validateSequenceDictionaries(GATKTool.java:621) at org.broadinstitute.hellbender.engine.GATKTool.onStartup(GATKTool.java:563) at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.onStartup(AssemblyRegionWalker.java:160) at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134) at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179) at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198) at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:152) at org.broadinstitute.hellbender.Main.mainEntry(Main.java:195) at org.broadinstitute.hellbender.Main.main(Main.java:275); > ; > Please suggest any solution. Thank you. hello，have you solved this problem？",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5947#issuecomment-1605272955:2760,validat,validateDictionaries,2760,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5947#issuecomment-1605272955,2,['validat'],"['validateDictionaries', 'validateSequenceDictionaries']"
Security," Here's some of the output:. ```; [March 9, 2017 7:03:42 PM EST] org.broadinstitute.hellbender.tools.picard.sam.ValidateSamFile --input src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam --use_jdk_deflater true --use_jdk_inflater true --MODE VERBOSE --MAX_OUTPUT 100 --IGNORE_WARNINGS false --VALIDATE_INDEX true --IS_BISULFITE_SEQUENCED false --MAX_OPEN_TEMP_FILES 8000 --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 1 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX false --CREATE_MD5_FILE false --help false --version false --verbosity INFO --QUIET false; [March 9, 2017 7:03:42 PM EST] Executing as gspowley@dna on Linux 3.10.0-514.10.2.el7.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_111-b14; Version: Version:4.alpha.2-170-g8d06823-SNAPSHOT; 19:03:42.998 INFO ValidateSamFile - Defaults.BUFFER_SIZE : 131072; 19:03:42.999 INFO ValidateSamFile - Defaults.COMPRESSION_LEVEL : 1; 19:03:42.999 INFO ValidateSamFile - Defaults.CREATE_INDEX : false; 19:03:42.999 INFO ValidateSamFile - Defaults.CREATE_MD5 : false; 19:03:42.999 INFO ValidateSamFile - Defaults.CUSTOM_READER_FACTORY : ; 19:03:42.999 INFO ValidateSamFile - Defaults.EBI_REFERENCE_SERVICE_URL_MASK : http://www.ebi.ac.uk/ena/cram/md5/%s; 19:03:42.999 INFO ValidateSamFile - Defaults.NON_ZERO_BUFFER_SIZE : 131072; 19:03:42.999 INFO ValidateSamFile - Defaults.REFERENCE_FASTA : null; 19:03:43.000 INFO ValidateSamFile - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 19:03:43.000 INFO ValidateSamFile - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 19:03:43.000 INFO ValidateSamFile - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 19:03:43.000 INFO ValidateSamFile - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 19:03:43.000 INFO ValidateSamFile - Defaults.USE_CRAM_REF_DOWNLOAD : false; 19:03:43.000 INFO ValidateSamFile - Deflater JdkDeflater; 19:03:43.000 INFO ValidateSamFile - Inflater JdkInflater; 19:03:43.000 INFO ValidateSamFile - Initializing engine; 19:03:43.000 INFO ValidateSamFile - Done initial",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2423#issuecomment-285513571:1086,Validat,ValidateSamFile,1086,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2423#issuecomment-285513571,1,['Validat'],['ValidateSamFile']
Security," Using codec VCFCodec to read file file:///run/media/riadh/My%20Book_From%20Eiklid/Analysis/gatk-4.2.4.1/ensembl-vep/PE69_chr3.vcf; 10:58:20.063 INFO VariantAnnotator - Done initializing engine; 10:58:20.091 WARN VariantAnnotatorEngine - The requested expression attribute ""gnomad.ALT"" is missing from the header in its resource file gnomad; 10:58:20.140 INFO ProgressMeter - Starting traversal; 10:58:20.140 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 10:58:42.160 INFO VariantAnnotator - Shutting down engine; [March 17, 2022 at 10:58:42 AM CET] org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotator done. Elapsed time: 0.37 minutes.; Runtime.totalMemory()=17158897664; java.lang.IllegalStateException: Allele in genotype C not in the variant context [C*, CT]; 	at htsjdk.variant.variantcontext.VariantContext$Validation.validateGenotypes(VariantContext.java:382); 	at htsjdk.variant.variantcontext.VariantContext$Validation.access$200(VariantContext.java:323); 	at htsjdk.variant.variantcontext.VariantContext$Validation$2.validate(VariantContext.java:331); 	at htsjdk.variant.variantcontext.VariantContext.lambda$validate$0(VariantContext.java:1384); 	at java.base/java.lang.Iterable.forEach(Iterable.java:75); 	at htsjdk.variant.variantcontext.VariantContext.validate(VariantContext.java:1384); 	at htsjdk.variant.variantcontext.VariantContext.<init>(VariantContext.java:489); 	at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:647); 	at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:638); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.trimAlleles(GATKVariantContextUtils.java:1464); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.trimAlleles(GATKVariantContextUtils.java:1420); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.getMinRepresentationBiallelics(VariantAnnotatorEn",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6689#issuecomment-1070784053:4076,access,access,4076,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6689#issuecomment-1070784053,1,['access'],['access']
Security, `35 <1> (ø)` | :arrow_down: |; | [...bender/utils/text/parsers/AbstractInputParser.java](https://codecov.io/gh/broadinstitute/gatk/pull/2543?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXh0L3BhcnNlcnMvQWJzdHJhY3RJbnB1dFBhcnNlci5qYXZh) | `88.679% <0%> (+1.642%)` | `31 <0> (ø)` | :arrow_down: |; | [...ollections/OptionalIntervalArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/2543?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL2FyZ3VtZW50Y29sbGVjdGlvbnMvT3B0aW9uYWxJbnRlcnZhbEFyZ3VtZW50Q29sbGVjdGlvbi5qYXZh) | `83.333% <0%> (+11.905%)` | `3 <1> (ø)` | :arrow_down: |; | [...stitute/hellbender/utils/pileup/PileupElement.java](https://codecov.io/gh/broadinstitute/gatk/pull/2543?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9waWxldXAvUGlsZXVwRWxlbWVudC5qYXZh) | `96.04% <0%> (+1.865%)` | `76 <0> (ø)` | :arrow_down: |; | [...bender/tools/exome/HashedListTargetCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/2543?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9IYXNoZWRMaXN0VGFyZ2V0Q29sbGVjdGlvbi5qYXZh) | `90.741% <0%> (+1.65%)` | `43 <0> (ø)` | :arrow_down: |; | [.../utils/read/markduplicates/DuplicationMetrics.java](https://codecov.io/gh/broadinstitute/gatk/pull/2543?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9yZWFkL21hcmtkdXBsaWNhdGVzL0R1cGxpY2F0aW9uTWV0cmljcy5qYXZh) | `85.366% <0%> (+2.033%)` | `13 <0> (ø)` | :arrow_down: |; | [...oadinstitute/hellbender/utils/read/CigarUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/2543?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9yZWFkL0NpZ2FyVXRpbHMuamF2YQ==) | `89.404% <0%> (+0.588%)` | `68 <0> (ø)` | :arrow_down: |; | [...der/utils/locusiterator/AlignmentStateMachine.java](https://codecov.io/gh/broadinstitute/gatk/pull/2543?src=pr&e,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2543#issuecomment-290171890:2742,Hash,HashedListTargetCollection,2742,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2543#issuecomment-290171890,1,['Hash'],['HashedListTargetCollection']
Security, `85.71% <100%> (+2.38%)` | `4 <0> (ø)` | :arrow_down: |; | [...hellbender/tools/walkers/vqsr/CNNVariantTrain.java](https://codecov.io/gh/broadinstitute/gatk/pull/5175/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3IvQ05OVmFyaWFudFRyYWluLmphdmE=) | `60% <46.66%> (-20.65%)` | `4 <0> (ø)` | |; | [...lkers/validation/EvaluateInfoFieldConcordance.java](https://codecov.io/gh/broadinstitute/gatk/pull/5175/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vRXZhbHVhdGVJbmZvRmllbGRDb25jb3JkYW5jZS5qYXZh) | `72.58% <72.58%> (ø)` | `14 <14> (?)` | |; | [...ellbender/tools/walkers/vqsr/CNNScoreVariants.java](https://codecov.io/gh/broadinstitute/gatk/pull/5175/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3IvQ05OU2NvcmVWYXJpYW50cy5qYXZh) | `73.68% <77.14%> (-1.32%)` | `41 <17> (+1)` | |; | [...ools/walkers/validation/InfoConcordanceRecord.java](https://codecov.io/gh/broadinstitute/gatk/pull/5175/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vSW5mb0NvbmNvcmRhbmNlUmVjb3JkLmphdmE=) | `93.93% <93.93%> (ø)` | `8 <8> (?)` | |; | [...n/EvaluateInfoFieldConcordanceIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5175/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vRXZhbHVhdGVJbmZvRmllbGRDb25jb3JkYW5jZUludGVncmF0aW9uVGVzdC5qYXZh) | `96% <96%> (ø)` | `3 <3> (?)` | |; | [...utils/smithwaterman/SmithWatermanIntelAligner.java](https://codecov.io/gh/broadinstitute/gatk/pull/5175/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9zbWl0aHdhdGVybWFuL1NtaXRoV2F0ZXJtYW5JbnRlbEFsaWduZXIuamF2YQ==) | `50% <0%> (-30%)` | `1% <0%> (-2%)` | |; | [...ithwaterman/SmithWatermanIntelAlignerUnitTest.java](https://codecov.io/gh/bro,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5175#issuecomment-423756354:2692,validat,validation,2692,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5175#issuecomment-423756354,1,['validat'],['validation']
Security," also removes by debugging code and comments. I think it is ready for a review. To some other questions you had above:. 1) The HashMap<FeatureInput<VariantContext>, HashMap<String, Collection<VariantContext>>> can be wrapped in a class with just a couple of methods, so we don't have to manifest that long type all over the place. I realize that's non-optimal, but this isnt anything I introduced here. I would really like to keep this PR as limited as we can, and address some larger refactoring in a different PR, once we've migrated to MultiVariantWalkerGroupedOnStart. 2) I know this PR still in an interim state, but passing the VariantWalker in as an argument to the comp methods doesn't seem like a step forward to me. If we can't solve that problem completely in this PR (which is fine, I'm all for trying to contain this), are those changes necessary ? Perhaps that part should just wait for the next round. As noted above, I'd like to propose this as iterative, with a second PR coming soon. I did this b/c it moved us toward not needing to pass around the walker. It minimizes the code that has access to the walker (as opposed to setting it after creating the instance of the Evaluator, etc. Yes, it exposes it for two methods, but those classes no longer hang on to it. I would like to ultimately remove this entirely. 3) To re-iterate testEvalTrackWithoutGenotypesWithSampleFields: the input file, noGenotypes.vcf, has a header dictionary with the full set of contigs, and a single variant from chr 1. Prior to this PR, the test executed and supplied b37_reference_20_21 as the reference FASTA. The variant in that VCF is from chr 1, not 20/21. The old code should have failed. It didnt, probably since it was preferentially taking the sequence dictionary from the VCF header and basically ignoring the FASTA. It didnt make much practical difference, but I believe my change here is right. I added the test case testEvalTrackWithoutGenotypesWithSampleFieldsWrongRef to cover that error.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-747619130:1619,access,access,1619,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-747619130,2,"['access', 'expose']","['access', 'exposes']"
Security," compare the result of MarkDuplicates and MarkDuplicatesSpark.; the same input SAM file and the default parameter, the MarkDuplicatesSpark have more data marked as duplicated.; Can you give me any suggest how to debug it, why the Spark version have more data marked?. READ_PAIR_DUPLICATES; **11933661 (MarkDuplicates); 11974162 (MarkDuplicatesSpark)**. Here is the metric file; ```. MarkDuplicatesSpark --output hdfs://wolfpass-aep:9000/user/test/spark_412.MarkDuplicates.bam --metrics-file hdfs://wolfpass-aep:9000/user/test/spark_412.MarkDuplicates-metrics.txt --input hdfs://wolfpass-aep:9000/user/test/spark_412.bowtie2.bam --spark-master yarn --duplicate-scoring-strategy SUM_OF_BASE_QUALITIES --do-not-mark-unmapped-mates false --read-name-regex <optimized capture of last three ':' separated fields as numeric values> --optical-duplicate-pixel-distance 100 --read-validation-stringency SILENT --interval-set-rule UNION --interval-padding 0 --interval-exclusion-padding 0 --interval-merging-rule ALL --bam-partition-size 0 --disable-sequence-dictionary-validation false --add-output-vcf-command-line true --sharded-output false --num-reducers 0 --help false --version false --showHidden false --verbosity INFO --QUIET false --use-jdk-deflater false --use-jdk-inflater false --gcs-max-retries 20 --gcs-project-for-requester-pays --disable-tool-default-read-filters false. METRICS CLASS	org.broadinstitute.hellbender.utils.read.markduplicates.GATKDuplicationMetrics LIBRARY	UNPAIRED_READS_EXAMINED	READ_PAIRS_EXAMINED	SECONDARY_OR_SUPPLEMENTARY_RDS	UNMAPPED_READS	UNPAIRED_READ_DUPLICATES READ_PAIR_DUPLICATES	READ_PAIR_OPTICAL_DUPLICATES	PERCENT_DUPLICATION ESTIMATED_LIBRARY_SIZE; lib1	173613	53799913	0	7610605	81003	11974162	585768	0.222961	05870713. MarkDuplicates --INPUT /home/test/WGS_pipeline/TEST/output/orig_412.bowtie2.bam --OUTPUT /home/test/WGS_pipeline/TEST/output/orig_412.MarkDuplicates.bam --METRICS_FILE /home/test/WGS_pipeline/TEST/output/orig_412.MarkDuplicates-metrics.txt -",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427229905:877,validat,validation-stringency,877,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427229905,2,['validat'],"['validation', 'validation-stringency']"
Security, easy just to upgrade the library version as we could end up with run time errors. I am adding this here so that its handy when ever you look at this further. Thanks again. . packageName | version | severity | language | module_id; -- | -- | -- | -- | --; com.google.protobuf:protobuf-java | 3.7.1 | high | java | [SNYK-JAVA-COMGOOGLEPROTOBUF-2331703 ](https://security.snyk.io/vuln/SNYK-JAVA-COMGOOGLEPROTOBUF-2331703 ); com.google.protobuf:protobuf-java | 3.7.1 | high | java | [SNYK-JAVA-COMGOOGLEPROTOBUF-3167772](https://security.snyk.io/vuln/SNYK-JAVA-COMGOOGLEPROTOBUF-3167772); io.netty:netty-codec-http2 | 4.1.96.Final | high | java | [SNYK-JAVA-IONETTY-5953332](https://security.snyk.io/vuln/SNYK-JAVA-IONETTY-5953332); log4j:log4j | 1.2.17 | high | java | [SNYK-JAVA-LOG4J-2342645](https://security.snyk.io/vuln/SNYK-JAVA-LOG4J-2342645); log4j:log4j | 1.2.17 | high | java | [SNYK-JAVA-LOG4J-2342646](https://security.snyk.io/vuln/SNYK-JAVA-LOG4J-2342646); log4j:log4j | 1.2.17 | high | java | [SNYK-JAVA-LOG4J-2342647](https://security.snyk.io/vuln/SNYK-JAVA-LOG4J-2342647); log4j:log4j | 1.2.17 | critical | java | [SNYK-JAVA-LOG4J-572732](https://security.snyk.io/vuln/SNYK-JAVA-LOG4J-572732); net.minidev:json-smart | 1.3.2 | high | java | [SNYK-JAVA-NETMINIDEV-3369748](https://security.snyk.io/vuln/SNYK-JAVA-NETMINIDEV-3369748); org.apache.zookeeper:zookeeper | 3.6.3 | high | java | [SNYK-JAVA-ORGAPACHEZOOKEEPER-5961102](https://security.snyk.io/vuln/SNYK-JAVA-ORGAPACHEZOOKEEPER-5961102); org.codehaus.jettison:jettison | 1.1 | high | java | [SNYK-JAVA-ORGCODEHAUSJETTISON-3168085](https://security.snyk.io/vuln/SNYK-JAVA-ORGCODEHAUSJETTISON-3168085); org.codehaus.jettison:jettison | 1.1 | high | java | [SNYK-JAVA-ORGCODEHAUSJETTISON-3367610](https://security.snyk.io/vuln/SNYK-JAVA-ORGCODEHAUSJETTISON-3367610); org.eclipse.jetty:jetty-http | 9.4.52.v20230823 | high | java | [SNYK-JAVA-ORGECLIPSEJETTY-5958847](https://security.snyk.io/vuln/SNYK-JAVA-ORGECLIPSEJETTY-5958847),MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8215#issuecomment-1890593067:1429,secur,security,1429,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8215#issuecomment-1890593067,7,['secur'],['security']
Security," engine; 10:58:20.091 WARN VariantAnnotatorEngine - The requested expression attribute ""gnomad.ALT"" is missing from the header in its resource file gnomad; 10:58:20.140 INFO ProgressMeter - Starting traversal; 10:58:20.140 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 10:58:42.160 INFO VariantAnnotator - Shutting down engine; [March 17, 2022 at 10:58:42 AM CET] org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotator done. Elapsed time: 0.37 minutes.; Runtime.totalMemory()=17158897664; java.lang.IllegalStateException: Allele in genotype C not in the variant context [C*, CT]; 	at htsjdk.variant.variantcontext.VariantContext$Validation.validateGenotypes(VariantContext.java:382); 	at htsjdk.variant.variantcontext.VariantContext$Validation.access$200(VariantContext.java:323); 	at htsjdk.variant.variantcontext.VariantContext$Validation$2.validate(VariantContext.java:331); 	at htsjdk.variant.variantcontext.VariantContext.lambda$validate$0(VariantContext.java:1384); 	at java.base/java.lang.Iterable.forEach(Iterable.java:75); 	at htsjdk.variant.variantcontext.VariantContext.validate(VariantContext.java:1384); 	at htsjdk.variant.variantcontext.VariantContext.<init>(VariantContext.java:489); 	at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:647); 	at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:638); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.trimAlleles(GATKVariantContextUtils.java:1464); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.trimAlleles(GATKVariantContextUtils.java:1420); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.getMinRepresentationBiallelics(VariantAnnotatorEngine.java:568); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateExpressions(VariantAnnotatorEngine.java:509); 	at org.broadinstitute.hellbender.to",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6689#issuecomment-1070784053:4266,validat,validate,4266,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6689#issuecomment-1070784053,1,['validat'],['validate']
Security," in the sampling of denoised copy ratios, fixes a memory leak by updating theano, and also adds some theano flags that typically yield a factor of ~2 speedup (notably, the OpenMP elemwise flag, although we also get a slight boost from using numpy MKL). This allows us to run, e.g.: . 2 shards of 50 samples by 100000 intervals on n1-standard-8s (8 CPU, 30GB memory, $0.08 / hr) each taking ~5 hours = ~1.6 cents / sample; 4 shards of 50 samples by 50000 intervals on n1-highmem-4s (4 CPU, 26GB memory, $0.05 / hr) each taking ~3.25 hours = ~1.3 cents / sample; 45 shards of 50 samples by 5000 intervals on *n1-standard-1s* (1CPU, 3.75GB memory, $0.01 / hr) each taking ~0.5 hours = ~0.5 cents / sample. For these runs, we used a slightly larger interval list and 1/4 the number of samples than in the first example, but because everything scales linearly, it's probably fair to compare the per-sample-and-interval costs. So we get a factor of ~8 savings if we keep the shard size the same. The cost was already satisfactory, but fixing the leak allows us to more easily run scatters that are not so wide, which may be crucial for running the megaWDL. Adding the OpenMP flag also lets CPU scalability work as intended. We can do a more systematic optimization for cost if desired, and we should also revalidate to make sure performance doesn't vary too much with shard size (from spot checking, it looks like marginal and/or single-bin calls may flicker on and off). Note that we have still not optimized inference for WES, although I believe @vruano has done some optimizations for WGS. @mwalker174 @vruano for WGS with 2kb bins, I would expect the cost of the gCNV step to be ~10 cents in cohort mode before inference optimizations, assuming we address #5716 to minimize disk costs. @asmirnov239 can you review? And maybe you can address dCR output in PostprocessGermlineCNVCalls and expose the number of samples in a separate PR? We can make some further changes to the dCR format there if we need.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5781#issuecomment-471570697:2172,expose,expose,2172,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5781#issuecomment-471570697,1,['expose'],['expose']
Security, org.broadinstitute.hellbender.testutils.CommandLineProgramTester.runCommandLine(CommandLineProgramTester.java:111); at org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReadsIntegrationTest.testLargeFileThatForcesSnappyUsage(SplitNCigarReadsIntegrationTest.java:85); Caused by:; htsjdk.samtools.util.RuntimeIOException: Write error; BinaryCodec in writemode; streamed file (filename not available); at htsjdk.samtools.util.BinaryCodec.writeBytes(BinaryCodec.java:222); at htsjdk.samtools.util.BlockCompressedOutputStream.writeGzipBlock(BlockCompressedOutputStream.java:444); at htsjdk.samtools.util.BlockCompressedOutputStream.deflateBlock(BlockCompressedOutputStream.java:408); at htsjdk.samtools.util.BlockCompressedOutputStream.write(BlockCompressedOutputStream.java:301); at htsjdk.samtools.util.BinaryCodec.writeBytes(BinaryCodec.java:220); at htsjdk.samtools.util.BinaryCodec.writeBytes(BinaryCodec.java:212); at htsjdk.samtools.BAMRecordCodec.encode(BAMRecordCodec.java:168); at htsjdk.samtools.BAMFileWriter.writeAlignment(BAMFileWriter.java:134); ... 15 more; Caused by:; java.io.IOException: No space left on device; at sun.nio.ch.FileDispatcherImpl.write0(Native Method); at sun.nio.ch.FileDispatcherImpl.write(FileDispatcherImpl.java:60); at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93); at sun.nio.ch.IOUtil.write(IOUtil.java:65); at sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:[211](https://github.com/broadinstitute/gatk/actions/runs/5547450688/jobs/10131043668#step:12:211)); at java.nio.channels.Channels.writeFullyImpl(Channels.java:78); at java.nio.channels.Channels.writeFully(Channels.java:101); at java.nio.channels.Channels.access$000(Channels.java:61); at java.nio.channels.Channels$1.write(Channels.java:174); at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82); at java.io.BufferedOutputStream.write(BufferedOutputStream.java:126); at htsjdk.samtools.util.BinaryCodec.writeBytes(BinaryCodec.java:220); ... 22 more; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8404#issuecomment-1635002002:3390,access,access,3390,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8404#issuecomment-1635002002,1,['access'],['access']
Security," pretty big drop in recall when optimizing LL. But we should also expect some discrepancy between LL and F1, according to one of the papers linked above. I would hope that with more variants or reliable training/truth (as in your data), things might stabilize or line up better. I'll try running with more malaria data, as well. The following trios x sites heatmap (top plot) for the validation set might better illustrate the arbitrariness in F1 (click to enlarge):. ![image](https://user-images.githubusercontent.com/11076296/158385585-1a0dfe8e-d4b7-4770-aed0-19ad81162c92.png). Here, yellow = het errors (since these are supposed to be clonal malaria samples), red = Mendelian errors, grey = no calls, green = Mendelian consistency, white = reference. The second plot shows the training/truth positives used to train the model and to calculate the LL score in the validation shard. The third plot shows the ""orthogonal truth"" positives/negatives used to calculate F1. So we can see that the difficulty in deriving F1 as a function of the score along the horizontal axis to give the third plot lies in collapsing the columns in the top plot into a single condition positive or condition negative status. Again, hard to do so without some arbitrariness; I simply came up with some rules to convert various amounts of red, yellow, green, etc. in each column to a red/white/green status. If you're using a single gold-standard sample, this should definitely be more straightforward. In any case, the optimal validation LL score at ~0.02 does appear to line up quite well visually with where one might manually set a threshold. It corresponds pretty well with the transition from the yellow/red/grey junk to the clean green/white sites in the top plot. Here's the same for the test set:. ![image](https://user-images.githubusercontent.com/11076296/158385662-6693a6c9-709c-482f-9a7e-5bb7030b3383.png). Happy to chat more about how you might implement this in your WDL---should be pretty straightforward!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1067396431:2782,validat,validation,2782,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1067396431,1,['validat'],['validation']
Security," redirect tqdm progress bar to python logger. commit 2e45bd30968b921fae225de3901fb97ece690b0c; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 19:45:49 2017 -0500. more arg related fixes. commit bb89a3bb338d88199881e8aca65f656f2acd7c0a; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 19:41:20 2017 -0500. arg related bugfixes in WDL, python, and java CLIs. commit 23569787ee2c8cc6c9227a44170cbbd02fe4427f; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 17:21:05 2017 -0500. fixed issue with python boolean argparse (they use weird semantics). commit ae841c9ed4cd9b2ca1ac0e9082d175ff8ea98298; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 16:44:02 2017 -0500. shorter gCNV WDL tests. commit 5466b806e36df16cad2d045be074e7f9afec0957; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 16:38:15 2017 -0500. fixed arg issues in somatic WDL; exposed all missing args to java side; major update to germline WDLs; all optional python args exposed to WDLs as optional args. commit 50cb6fd08de15469a9080cbb27ff30c8b7ee7e21; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 13:50:45 2017 -0500. missing serialVersionUID. commit 5f0f31eab63b0e6f6105708ded7f86c96c830781; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 13:35:33 2017 -0500. annotated intervals kebab case; updated germline WDL workflows. commit 29cc6234dbfb8db12559217a650c6ceb170c5797; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 13:15:28 2017 -0500. cleanup test files. commit 08a35bb4e65eceb735adcd41a91132e9a34d2b66; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 02:50:19 2017 -0500. update WDL scripts. commit 12bcfa192ee6fa6da21239ebf5b513633efe974f; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 02:47:33 2017 -0500. significant updates to GermlineCNVCaller; integration tests for GermlineCNVCaller w",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598:5916,expose,exposed,5916,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598,2,['expose'],['exposed']
Security," requested more than the maximum memory capability of the cluster (164726 MB per container); 17/10/13 18:11:36 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead; 17/10/13 18:11:36 INFO yarn.Client: Setting up container launch context for our AM; 17/10/13 18:11:36 INFO yarn.Client: Setting up the launch environment for our AM container; 17/10/13 18:11:36 INFO yarn.Client: Preparing resources for our AM container; 17/10/13 18:11:37 INFO yarn.Client: Uploading resource file:/tmp/hdfs/spark-c7e5eece-205e-4bce-a69b-4168c9b79045/__spark_conf__2918234914787361986.zip -> hdfs://mg:8020/user/hdfs/.sparkStaging/application_1507856833944_0003/__spark_conf__.zip; 17/10/13 18:11:37 INFO spark.SecurityManager: Changing view acls to: hdfs; 17/10/13 18:11:37 INFO spark.SecurityManager: Changing modify acls to: hdfs; 17/10/13 18:11:37 INFO spark.SecurityManager: Changing view acls groups to: ; 17/10/13 18:11:37 INFO spark.SecurityManager: Changing modify acls groups to: ; 17/10/13 18:11:37 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hdfs); groups with view permissions: Set(); users with modify permissions: Set(hdfs); groups with modify permissions: Set(); 17/10/13 18:11:37 INFO yarn.Client: Submitting application application_1507856833944_0003 to ResourceManager; 17/10/13 18:11:37 INFO impl.YarnClientImpl: Submitted application application_1507856833944_0003; 17/10/13 18:11:37 INFO cluster.SchedulerExtensionServices: Starting Yarn extension services with app application_1507856833944_0003 and attemptId None; 17/10/13 18:11:38 INFO yarn.Client: Application report for application_1507856833944_0003 (state: ACCEPTED); 17/10/13 18:11:38 INFO yarn.Client: ; 	 client token: N/A; 	 diagnostics: N/A; 	 ApplicationMaster host: N/A; 	 ApplicationMaster RPC port: -1; 	 queue: root.users.hdfs; 	 start time: 1507889497661; 	 final status: UNDEFINED; 	 tracking URL: http://mg:8088",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-336412775:10788,Secur,SecurityManager,10788,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-336412775,1,['Secur'],['SecurityManager']
Security, sun.net.www.MeteredStream.available(MeteredStream.java:170); 	at sun.net.www.http.KeepAliveStream.close(KeepAliveStream.java:85); 	at java.io.FilterInputStream.close(FilterInputStream.java:181); 	at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.close(HttpURLConnection.java:3448); 	at java.io.FilterInputStream.close(FilterInputStream.java:181); 	at shaded.cloud_nio.com.google.api.client.util.IOUtils.copy(IOUtils.java:97); 	at shaded.cloud_nio.com.google.api.client.util.IOUtils.copy(IOUtils.java:63); 	at shaded.cloud_nio.com.google.api.client.http.HttpResponse.download(HttpResponse.java:421); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:507); 	... 12 more; Caused by: javax.net.ssl.SSLException: java.net.SocketException: Connection reset; 	at sun.security.ssl.Alerts.getSSLException(Alerts.java:208); 	at sun.security.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1949); 	at sun.security.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1906); 	at sun.security.ssl.SSLSocketImpl.handleException(SSLSocketImpl.java:1870); 	at sun.security.ssl.SSLSocketImpl.handleException(SSLSocketImpl.java:1815); 	at sun.security.ssl.AppInputStream.read(AppInputStream.java:116); 	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286); 	at java.io.BufferedInputStream.read(BufferedInputStream.java:345); 	at sun.net.www.MeteredStream.read(MeteredStream.java:134); 	at java.io.FilterInputStream.read(FilterInputStream.java:133); 	at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.read(HttpURLConnection.java:3375); 	at shaded.cloud_nio.com.google.api.client.http.javanet.NetHttpResponse$SizeValidatingInputStream.read(NetHttpResponse.java:169); 	at java.io.FilterInputStream.read(FilterInputStream.java:107); 	at shaded.cloud_nio.com.google.api.client.util.ByteStreams.copy(ByteStreams.java:51); 	at shaded.cloud_nio.com.google.api.client.util.IOUtils.copy(IOUtils.java:94); 	...,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-301610931:8048,secur,security,8048,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-301610931,1,['secur'],['security']
Security, sun.net.www.MeteredStream.available(MeteredStream.java:170); 	at sun.net.www.http.KeepAliveStream.close(KeepAliveStream.java:85); 	at java.io.FilterInputStream.close(FilterInputStream.java:181); 	at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.close(HttpURLConnection.java:3448); 	at java.io.FilterInputStream.close(FilterInputStream.java:181); 	at shaded.cloud_nio.com.google.api.client.util.IOUtils.copy(IOUtils.java:97); 	at shaded.cloud_nio.com.google.api.client.util.IOUtils.copy(IOUtils.java:63); 	at shaded.cloud_nio.com.google.api.client.http.HttpResponse.download(HttpResponse.java:421); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:510); 	... 47 more; Caused by: javax.net.ssl.SSLException: java.net.SocketException: Connection reset; 	at sun.security.ssl.Alerts.getSSLException(Alerts.java:208); 	at sun.security.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1949); 	at sun.security.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1906); 	at sun.security.ssl.SSLSocketImpl.handleException(SSLSocketImpl.java:1870); 	at sun.security.ssl.SSLSocketImpl.handleException(SSLSocketImpl.java:1815); 	at sun.security.ssl.AppInputStream.read(AppInputStream.java:116); 	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286); 	at java.io.BufferedInputStream.read(BufferedInputStream.java:345); 	at sun.net.www.MeteredStream.read(MeteredStream.java:134); 	at java.io.FilterInputStream.read(FilterInputStream.java:133); 	at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.read(HttpURLConnection.java:3375); 	at shaded.cloud_nio.com.google.api.client.http.javanet.NetHttpResponse$SizeValidatingInputStream.read(NetHttpResponse.java:169); 	at java.io.FilterInputStream.read(FilterInputStream.java:107); 	at shaded.cloud_nio.com.google.api.client.util.ByteStreams.copy(ByteStreams.java:51); 	at shaded.cloud_nio.com.google.api.client.util.IOUtils.copy(IOUtils.java:94); 	...,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-308541727:6342,secur,security,6342,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-308541727,1,['secur'],['security']
Security, sun.net.www.MeteredStream.available(MeteredStream.java:170); 	at sun.net.www.http.KeepAliveStream.close(KeepAliveStream.java:85); 	at java.io.FilterInputStream.close(FilterInputStream.java:181); 	at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.close(HttpURLConnection.java:3448); 	at java.io.FilterInputStream.close(FilterInputStream.java:181); 	at shaded.cloud_nio.com.google.api.client.util.IOUtils.copy(IOUtils.java:97); 	at shaded.cloud_nio.com.google.api.client.util.IOUtils.copy(IOUtils.java:63); 	at shaded.cloud_nio.com.google.api.client.http.HttpResponse.download(HttpResponse.java:421); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:510); 	... 55 more; Caused by: javax.net.ssl.SSLException: java.net.SocketException: Connection reset; 	at sun.security.ssl.Alerts.getSSLException(Alerts.java:208); 	at sun.security.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1949); 	at sun.security.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1906); 	at sun.security.ssl.SSLSocketImpl.handleException(SSLSocketImpl.java:1870); 	at sun.security.ssl.SSLSocketImpl.handleException(SSLSocketImpl.java:1815); 	at sun.security.ssl.AppInputStream.read(AppInputStream.java:116); 	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286); 	at java.io.BufferedInputStream.read(BufferedInputStream.java:345); 	at sun.net.www.MeteredStream.read(MeteredStream.java:134); 	at java.io.FilterInputStream.read(FilterInputStream.java:133); 	at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.read(HttpURLConnection.java:3375); 	at shaded.cloud_nio.com.google.api.client.http.javanet.NetHttpResponse$SizeValidatingInputStream.read(NetHttpResponse.java:169); 	at java.io.FilterInputStream.read(FilterInputStream.java:107); 	at shaded.cloud_nio.com.google.api.client.util.ByteStreams.copy(ByteStreams.java:51); 	at shaded.cloud_nio.com.google.api.client.util.IOUtils.copy(IOUtils.java:94); 	...,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317549138:9830,secur,security,9830,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317549138,1,['secur'],['security']
Security," with 25.4 GB RAM, BlockManagerId(7, scc-q14.scc.bu.edu, 36726, None); 18/03/07 20:31:49 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(null) (192.168.18.195:47862) with ID 6; 18/03/07 20:31:49 INFO storage.BlockManagerMasterEndpoint: Registering block manager scc-q11.scc.bu.edu:46002 with 25.4 GB RAM, BlockManagerId(6, scc-q11.scc.bu.edu, 46002, None); 18/03/07 20:31:49 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 246.6 KB, free 8.4 GB); 18/03/07 20:31:50 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 25.3 KB, free 8.4 GB); 18/03/07 20:31:50 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.48.225.55:41567 (size: 25.3 KB, free: 8.4 GB); 18/03/07 20:31:50 INFO spark.SparkContext: Created broadcast 0 from newAPIHadoopFile at ReadsSparkSource.java:112; 18/03/07 20:31:50 INFO hdfs.DFSClient: Created HDFS_DELEGATION_TOKEN token 7175 for farrell on ha-hdfs:scc; 18/03/07 20:31:50 INFO security.TokenCache: Got dt for hdfs://scc; Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:scc, Ident: (HDFS_DELEGATION_TOKEN token 7175 for farrell); 18/03/07 20:31:50 INFO input.FileInputFormat: Total input paths to process : 1; 18/03/07 20:31:51 INFO spark.SparkContext: Starting job: aggregate at FlagStatSpark.java:73; 18/03/07 20:31:51 INFO scheduler.DAGScheduler: Got job 0 (aggregate at FlagStatSpark.java:73) with 629 output partitions; 18/03/07 20:31:51 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (aggregate at FlagStatSpark.java:73); 18/03/07 20:31:51 INFO scheduler.DAGScheduler: Parents of final stage: List(); 18/03/07 20:31:51 INFO scheduler.DAGScheduler: Missing parents: List(); 18/03/07 20:31:51 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at filter at GATKSparkTool.java:220), which has no missing parents; 18/03/07 20:31:51 INFO memory.MemoryStore: Block broadcast_1 stored as values in mem",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4506#issuecomment-371353888:5073,secur,security,5073,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4506#issuecomment-371353888,1,['secur'],['security']
Security,!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: CountReadsSpark is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 11:33:26.277 INFO CountReadsSpark - Initializing engine; 11:33:26.277 INFO CountReadsSpark - Done initializing engine; 2019-01-07 11:33:26 WARN SparkConf:66 - The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 2019-01-07 11:33:26 INFO SparkContext:54 - Running Spark version 2.3.0; 2019-01-07 11:33:26 INFO SparkContext:54 - Submitted application: CountReadsSpark; 2019-01-07 11:33:26 INFO SecurityManager:54 - Changing view acls to: farrell; 2019-01-07 11:33:26 INFO SecurityManager:54 - Changing modify acls to: farrell; 2019-01-07 11:33:26 INFO SecurityManager:54 - Changing view acls groups to:; 2019-01-07 11:33:26 INFO SecurityManager:54 - Changing modify acls groups to:; 2019-01-07 11:33:26 INFO SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(farrell); groups with view permissions: Set(); users with modify permissions: Set(farrell); groups with modify permissions: Set(); 2019-01-07 11:33:27 INFO Utils:54 - Successfully started service 'sparkDriver' on port 46828.; 2019-01-07 11:33:27 INFO SparkEnv:54 - Registering MapOutputTracker; 2019-01-07 11:33:27 INFO SparkEnv:54 - Registering BlockManagerMaster; 2019-01-07 11:33:27 INFO BlockManagerMasterEndpoint:54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 2019-01-07 11:33:27 INFO BlockManagerMasterEndpoint:54 - BlockManagerMasterEndpoint up; 2019-01-07 11:33:27 INFO DiskBlockManager:54 - Created local directory at /tmp/blockmgr-08460386-3abb-4431-ba8d-5b7d41a2a05c; 2019-01-07 11:33:27 INFO MemoryStore:54 - MemoryStore started with capacity 408.6 MB; 2019-01-07 11:33:27 INFO SparkEnv,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:4995,Secur,SecurityManager,4995,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,7,"['Secur', 'authenticat']","['SecurityManager', 'authentication']"
Security,!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: CountReadsSpark is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 13:35:11.512 INFO CountReadsSpark - Initializing engine; 13:35:11.512 INFO CountReadsSpark - Done initializing engine; 2019-01-09 13:35:11 WARN SparkConf:66 - The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 2019-01-09 13:35:11 INFO SparkContext:54 - Running Spark version 2.3.0; 2019-01-09 13:35:11 INFO SparkContext:54 - Submitted application: CountReadsSpark; 2019-01-09 13:35:11 INFO SecurityManager:54 - Changing view acls to: farrell; 2019-01-09 13:35:11 INFO SecurityManager:54 - Changing modify acls to: farrell; 2019-01-09 13:35:11 INFO SecurityManager:54 - Changing view acls groups to:; 2019-01-09 13:35:11 INFO SecurityManager:54 - Changing modify acls groups to:; 2019-01-09 13:35:11 INFO SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(farrell); groups with view permissions: Set(); users with modify permissions: Set(farrell); groups with modify permissions: Set(); 2019-01-09 13:35:12 INFO Utils:54 - Successfully started service 'sparkDriver' on port 42689.; 2019-01-09 13:35:12 INFO SparkEnv:54 - Registering MapOutputTracker; 2019-01-09 13:35:12 INFO SparkEnv:54 - Registering BlockManagerMaster; 2019-01-09 13:35:12 INFO BlockManagerMasterEndpoint:54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 2019-01-09 13:35:12 INFO BlockManagerMasterEndpoint:54 - BlockManagerMasterEndpoint up; 2019-01-09 13:35:12 INFO DiskBlockManager:54 - Created local directory at /tmp/blockmgr-dd94d6fb-7e3d-4def-a895-6e60f05d7a05; 2019-01-09 13:35:12 INFO MemoryStore:54 - MemoryStore started with capacity 372.6 MB; 2019-01-09 13:35:12 INFO SparkEnv,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:4734,Secur,SecurityManager,4734,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,7,"['Secur', 'authenticat']","['SecurityManager', 'authentication']"
Security,"""D:\Program Files\Java\jdk1.8.0_121\bin\java.exe"" -agentlib:jdwp=transport=dt_socket,address=127.0.0.1:62530,suspend=y,server=n -XX:TieredStopAtLevel=1 -noverify -Dspring.output.ansi.enabled=always -Dcom.sun.management.jmxremote -Dspring.jmx.enabled=true -Dspring.liveBeansView.mbeanDomain -Dspring.application.admin.enabled=true -javaagent:C:\Users\Sweet\AppData\Local\JetBrains\IntelliJIdea2020.1\captureAgent\debugger-agent.jar -Dfile.encoding=UTF-8 -classpath ""D:\Program Files\Java\jdk1.8.0_121\jre\lib\charsets.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\deploy.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\ext\access-bridge-64.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\ext\cldrdata.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\ext\dnsns.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\ext\jaccess.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\ext\jfxrt.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\ext\localedata.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\ext\nashorn.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\ext\sunec.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\ext\sunjce_provider.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\ext\sunmscapi.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\ext\sunpkcs11.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\ext\zipfs.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\javaws.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\jce.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\jfr.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\jfxswt.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\jsse.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\management-agent.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\plugin.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\resources.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\rt.jar;C:\project\push\target\classes;E:\repository\org\springframework\boot\spring-boot-starter-jdbc\2.3.0.RELEASE\spring-boot-starter-jdbc-2.3.0.RELEASE.jar;E:\repository\org\springframework\boot\spring-boot-starter\2.3",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-635805233:622,access,access-bridge-,622,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-635805233,1,['access'],['access-bridge-']
Security,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2649?src=pr&el=h1) Report; > Merging [#2649](https://codecov.io/gh/broadinstitute/gatk/pull/2649?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/32ea767879741f62889ee9118ac4f94608d563c3?src=pr&el=desc) will **decrease** coverage by `0.002%`.; > The diff coverage is `66.667%`. ```diff; @@ Coverage Diff @@; ## master #2649 +/- ##; ===============================================; - Coverage 76.126% 76.124% -0.002% ; - Complexity 11151 11153 +2 ; ===============================================; Files 769 769 ; Lines 40752 40753 +1 ; Branches 7110 7112 +2 ; ===============================================; Hits 31023 31023 ; Misses 7062 7062 ; - Partials 2667 2668 +1; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2649?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...r/tools/walkers/variantutils/ValidateVariants.java](https://codecov.io/gh/broadinstitute/gatk/pull/2649?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhcmlhbnR1dGlscy9WYWxpZGF0ZVZhcmlhbnRzLmphdmE=) | `80.597% <66.667%> (-1.221%)` | `18 <0> (+2)` | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2649#issuecomment-298987781:940,Validat,ValidateVariants,940,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2649#issuecomment-298987781,1,['Validat'],['ValidateVariants']
Security,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2759?src=pr&el=h1) Report; > Merging [#2759](https://codecov.io/gh/broadinstitute/gatk/pull/2759?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/5fd6c965741e590ed7c8d7ee820f0b72fb528187?src=pr&el=desc) will **not change** coverage.; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #2759 +/- ##; ===========================================; Coverage 80.131% 80.131% ; Complexity 16992 16992 ; ===========================================; Files 1144 1144 ; Lines 61630 61630 ; Branches 9605 9605 ; ===========================================; Hits 49385 49385 ; Misses 8422 8422 ; Partials 3823 3823; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2759?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...r/tools/walkers/variantutils/ValidateVariants.java](https://codecov.io/gh/broadinstitute/gatk/pull/2759?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhcmlhbnR1dGlscy9WYWxpZGF0ZVZhcmlhbnRzLmphdmE=) | `80.597% <ø> (ø)` | `18 <0> (ø)` | :arrow_down: |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2759#issuecomment-304135170:890,Validat,ValidateVariants,890,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2759#issuecomment-304135170,1,['Validat'],['ValidateVariants']
Security,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/3045?src=pr&el=h1) Report; > Merging [#3045](https://codecov.io/gh/broadinstitute/gatk/pull/3045?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/b01b71e2e84ceea6df6268594f0ee7aa4e7fe38f?src=pr&el=desc) will **increase** coverage by `0.017%`.; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #3045 +/- ##; ===============================================; + Coverage 80.131% 80.149% +0.017% ; - Complexity 16990 17003 +13 ; ===============================================; Files 1144 1144 ; Lines 61630 61673 +43 ; Branches 9605 9621 +16 ; ===============================================; + Hits 49385 49430 +45 ; + Misses 8422 8419 -3 ; - Partials 3823 3824 +1; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/3045?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...tools/walkers/haplotypecaller/HaplotypeCaller.java](https://codecov.io/gh/broadinstitute/gatk/pull/3045?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9IYXBsb3R5cGVDYWxsZXIuamF2YQ==) | `94.118% <ø> (ø)` | `18 <0> (ø)` | :arrow_down: |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/3045?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `73.649% <0%> (+2.027%)` | `34% <0%> (ø)` | :arrow_down: |; | [...llbender/tools/walkers/validation/Concordance.java](https://codecov.io/gh/broadinstitute/gatk/pull/3045?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vQ29uY29yZGFuY2UuamF2YQ==) | `91.367% <0%> (+2.825%)` | `39% <0%> (+11%)` | :arrow_up: |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3045#issuecomment-309612946:1549,validat,validation,1549,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3045#issuecomment-309612946,1,['validat'],['validation']
Security,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/3108?src=pr&el=h1) Report; > Merging [#3108](https://codecov.io/gh/broadinstitute/gatk/pull/3108?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/994b25194cb1a697903e8db749412cba9b422481?src=pr&el=desc) will **increase** coverage by `0.005%`.; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #3108 +/- ##; ===============================================; + Coverage 80.131% 80.136% +0.005% ; Complexity 16992 16992 ; ===============================================; Files 1144 1144 ; Lines 61630 61630 ; Branches 9605 9605 ; ===============================================; + Hits 49385 49388 +3 ; + Misses 8422 8419 -3 ; Partials 3823 3823; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/3108?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...llbender/tools/walkers/validation/Concordance.java](https://codecov.io/gh/broadinstitute/gatk/pull/3108?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vQ29uY29yZGFuY2UuamF2YQ==) | `88.542% <ø> (ø)` | `28 <0> (ø)` | :arrow_down: |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/3108?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `73.649% <0%> (+2.027%)` | `34% <0%> (ø)` | :arrow_down: |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3108#issuecomment-308209516:926,validat,validation,926,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3108#issuecomment-308209516,1,['validat'],['validation']
Security,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/3369?src=pr&el=h1) Report; > Merging [#3369](https://codecov.io/gh/broadinstitute/gatk/pull/3369?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/28c3b7d48a5b3c219a1057e96b1f728ab2f06b37?src=pr&el=desc) will **increase** coverage by `0.017%`.; > The diff coverage is `75%`. ```diff; @@ Coverage Diff @@; ## master #3369 +/- ##; ==============================================; + Coverage 79.923% 79.94% +0.017% ; - Complexity 17884 17897 +13 ; ==============================================; Files 1198 1198 ; Lines 64966 64980 +14 ; Branches 10114 10120 +6 ; ==============================================; + Hits 51923 51945 +22 ; + Misses 9010 9002 -8 ; Partials 4033 4033; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/3369?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...tools/spark/validation/CompareDuplicatesSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/3369?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay92YWxpZGF0aW9uL0NvbXBhcmVEdXBsaWNhdGVzU3BhcmsuamF2YQ==) | `82.927% <50%> (-1.883%)` | `24 <0> (ø)` | |; | [...der/engine/spark/datasources/ReadsSparkSource.java](https://codecov.io/gh/broadinstitute/gatk/pull/3369?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvZGF0YXNvdXJjZXMvUmVhZHNTcGFya1NvdXJjZS5qYXZh) | `80.198% <76.923%> (+10.724%)` | `38 <10> (+10)` | :arrow_up: |; | [...stitute/hellbender/engine/spark/GATKSparkTool.java](https://codecov.io/gh/broadinstitute/gatk/pull/3369?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvR0FUS1NwYXJrVG9vbC5qYXZh) | `85% <85.714%> (+0.789%)` | `55 <5> (+2)` | :arrow_up: |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/3369?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGU,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3369#issuecomment-325704642:927,validat,validation,927,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3369#issuecomment-325704642,1,['validat'],['validation']
Security,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/3445?src=pr&el=h1) Report; > Merging [#3445](https://codecov.io/gh/broadinstitute/gatk/pull/3445?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/69fd3791cf5dd975ce1cc820226fc4a4c8904b65?src=pr&el=desc) will **increase** coverage by `0.023%`.; > The diff coverage is `91.667%`. ```diff; @@ Coverage Diff @@; ## master #3445 +/- ##; ==============================================; + Coverage 80.287% 80.31% +0.023% ; - Complexity 17633 17647 +14 ; ==============================================; Files 1178 1178 ; Lines 63847 63854 +7 ; Branches 9928 9930 +2 ; ==============================================; + Hits 51261 51281 +20 ; + Misses 8631 8628 -3 ; + Partials 3955 3945 -10; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/3445?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...r/tools/walkers/variantutils/ValidateVariants.java](https://codecov.io/gh/broadinstitute/gatk/pull/3445?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhcmlhbnR1dGlscy9WYWxpZGF0ZVZhcmlhbnRzLmphdmE=) | `82.353% <91.667%> (+2.353%)` | `32 <0> (+4)` | :arrow_up: |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/3445?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `74.342% <0%> (-1.974%)` | `38% <0%> (ø)` | |; | [...bender/tools/spark/sv/evidence/ReadClassifier.java](https://codecov.io/gh/broadinstitute/gatk/pull/3445?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9ldmlkZW5jZS9SZWFkQ2xhc3NpZmllci5qYXZh) | `86.667% <0%> (+1.333%)` | `33% <0%> (+1%)` | :arrow_up: |; | [...er/tools/spark/sv/evidence/BreakpointEvidence.java](https://codecov.io/gh/broadinstitute/gatk/pull/3445?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVs,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3445#issuecomment-322886799:951,Validat,ValidateVariants,951,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3445#issuecomment-322886799,1,['Validat'],['ValidateVariants']
Security,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/3461?src=pr&el=h1) Report; > Merging [#3461](https://codecov.io/gh/broadinstitute/gatk/pull/3461?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/a1292eecb05bc4c61abc33d7b01c81b29383a150?src=pr&el=desc) will **increase** coverage by `0.001%`.; > The diff coverage is `75%`. ```diff; @@ Coverage Diff @@; ## master #3461 +/- ##; ===============================================; + Coverage 80.307% 80.307% +0.001% ; - Complexity 17645 17666 +21 ; ===============================================; Files 1178 1178 ; Lines 63854 63907 +53 ; Branches 9930 9947 +17 ; ===============================================; + Hits 51279 51322 +43 ; - Misses 8627 8632 +5 ; - Partials 3948 3953 +5; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/3461?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...ute/hellbender/utils/bwa/BwaMemAlignmentUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/3461?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9id2EvQndhTWVtQWxpZ25tZW50VXRpbHMuamF2YQ==) | `88.889% <75%> (-4.532%)` | `36 <26> (+5)` | |; | [...r/tools/walkers/variantutils/ValidateVariants.java](https://codecov.io/gh/broadinstitute/gatk/pull/3461?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhcmlhbnR1dGlscy9WYWxpZGF0ZVZhcmlhbnRzLmphdmE=) | `83.688% <0%> (+1.335%)` | `48% <0%> (+16%)` | :arrow_up: |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3461#issuecomment-323443004:1242,Validat,ValidateVariants,1242,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3461#issuecomment-323443004,1,['Validat'],['ValidateVariants']
Security,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/3530?src=pr&el=h1) Report; > Merging [#3530](https://codecov.io/gh/broadinstitute/gatk/pull/3530?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/bfa9af462f484c77597a9fdbdc46f66393afaff1?src=pr&el=desc) will **increase** coverage by `0.03%`.; > The diff coverage is `100%`. ```diff; @@ Coverage Diff @@; ## master #3530 +/- ##; ==============================================; + Coverage 80.079% 80.109% +0.03% ; - Complexity 17760 17791 +31 ; ==============================================; Files 1188 1188 ; Lines 64410 64543 +133 ; Branches 10004 10022 +18 ; ==============================================; + Hits 51579 51705 +126 ; Misses 8845 8845 ; - Partials 3986 3993 +7; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/3530?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...r/tools/walkers/variantutils/ValidateVariants.java](https://codecov.io/gh/broadinstitute/gatk/pull/3530?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhcmlhbnR1dGlscy9WYWxpZGF0ZVZhcmlhbnRzLmphdmE=) | `82.857% <100%> (+0.504%)` | `34 <0> (+2)` | :arrow_up: |; | [...er/tools/spark/sv/discovery/AlignmentInterval.java](https://codecov.io/gh/broadinstitute/gatk/pull/3530?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvQWxpZ25tZW50SW50ZXJ2YWwuamF2YQ==) | `89.831% <0%> (-0.847%)` | `23% <0%> (-1%)` | |; | [...nder/tools/spark/pipelines/ReadsPipelineSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/3530?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvUmVhZHNQaXBlbGluZVNwYXJrLmphdmE=) | `92.857% <0%> (-0.246%)` | `16% <0%> (+8%)` | |; | [...lotypecaller/readthreading/ReadThreadingGraph.java](https://codecov.io/gh/broadinstitute/gatk/pull/3530?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRp,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3530#issuecomment-325497752:947,Validat,ValidateVariants,947,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3530#issuecomment-325497752,1,['Validat'],['ValidateVariants']
Security,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/3880?src=pr&el=h1) Report; > Merging [#3880](https://codecov.io/gh/broadinstitute/gatk/pull/3880?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/bf18c26bde607497ab340e550ef94540ccd3bb1d?src=pr&el=desc) will **decrease** coverage by `0.024%`.; > The diff coverage is `0%`. ```diff; @@ Coverage Diff @@; ## master #3880 +/- ##; ===============================================; - Coverage 79.347% 79.322% -0.024% ; - Complexity 17919 18703 +784 ; ===============================================; Files 1172 1173 +1 ; Lines 64706 66787 +2081 ; Branches 9880 10565 +685 ; ===============================================; + Hits 51342 52977 +1635 ; - Misses 9446 9815 +369 ; - Partials 3918 3995 +77; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/3880?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...lbender/tools/validation/CompareBaseQualities.java](https://codecov.io/gh/broadinstitute/gatk/pull/3880?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy92YWxpZGF0aW9uL0NvbXBhcmVCYXNlUXVhbGl0aWVzLmphdmE=) | `75.676% <ø> (ø)` | `5 <0> (ø)` | :arrow_down: |; | [...titute/hellbender/tools/walkers/CountVariants.java](https://codecov.io/gh/broadinstitute/gatk/pull/3880?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL0NvdW50VmFyaWFudHMuamF2YQ==) | `100% <ø> (ø)` | `3 <0> (ø)` | :arrow_down: |; | [...ark/pipelines/metrics/MeanQualityByCycleSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/3880?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvbWV0cmljcy9NZWFuUXVhbGl0eUJ5Q3ljbGVTcGFyay5qYXZh) | `90.816% <ø> (ø)` | `11 <0> (ø)` | :arrow_down: |; | [...bender/tools/copynumber/CallCopyRatioSegments.java](https://codecov.io/gh/broadinstitute/gatk/pull/3880?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3880#issuecomment-347373155:949,validat,validation,949,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3880#issuecomment-347373155,1,['validat'],['validation']
Security,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/4894?src=pr&el=h1) Report; > Merging [#4894](https://codecov.io/gh/broadinstitute/gatk/pull/4894?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/41788f5de7711bf46222bb19c139d84946b40d70?src=pr&el=desc) will **increase** coverage by `0.191%`.; > The diff coverage is `94.231%`. ```diff; @@ Coverage Diff @@; ## master #4894 +/- ##; ===============================================; + Coverage 80.453% 80.644% +0.191% ; - Complexity 17837 18463 +626 ; ===============================================; Files 1092 1092 ; Lines 64231 65676 +1445 ; Branches 10348 10735 +387 ; ===============================================; + Hits 51676 52964 +1288 ; - Misses 8504 8601 +97 ; - Partials 4051 4111 +60; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/4894?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...tools/spark/validation/CompareDuplicatesSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4894/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay92YWxpZGF0aW9uL0NvbXBhcmVEdXBsaWNhdGVzU3BhcmsuamF2YQ==) | `89.63% <94.231%> (+4.683%)` | `40 <26> (+16)` | :arrow_up: |; | [...e/hellbender/tools/funcotator/FuncotatorUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4894/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9mdW5jb3RhdG9yL0Z1bmNvdGF0b3JVdGlscy5qYXZh) | `80.176% <0%> (-0.154%)` | `328% <0%> (+160%)` | |; | [...roadinstitute/hellbender/utils/read/ReadUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4894/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9yZWFkL1JlYWRVdGlscy5qYXZh) | `80% <0%> (ø)` | `200% <0%> (-2%)` | :arrow_down: |; | [...dataSources/gencode/GencodeFuncotationBuilder.java](https://codecov.io/gh/broadinstitute/gatk/pull/4894/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcm,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4894#issuecomment-397746010:949,validat,validation,949,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4894#issuecomment-397746010,1,['validat'],['validation']
Security,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/4982?src=pr&el=h1) Report; > Merging [#4982](https://codecov.io/gh/broadinstitute/gatk/pull/4982?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/ddf042a04ad02f2e3207ff71bdace1bd7cec9a49?src=pr&el=desc) will **increase** coverage by `0.003%`.; > The diff coverage is `88.889%`. ```diff; @@ Coverage Diff @@; ## master #4982 +/- ##; ===============================================; + Coverage 80.784% 80.787% +0.003% ; - Complexity 17960 17966 +6 ; ===============================================; Files 1095 1095 ; Lines 64592 64619 +27 ; Branches 10392 10400 +8 ; ===============================================; + Hits 52180 52204 +24 ; - Misses 8388 8389 +1 ; - Partials 4024 4026 +2; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/4982?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...tmutpileup/ValidateBasicSomaticShortMutations.java](https://codecov.io/gh/broadinstitute/gatk/pull/4982/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vYmFzaWNzaG9ydG11dHBpbGV1cC9WYWxpZGF0ZUJhc2ljU29tYXRpY1Nob3J0TXV0YXRpb25zLmphdmE=) | `86.905% <88.889%> (+0.94%)` | `13 <0> (+6)` | :arrow_up: |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4982#issuecomment-402356444:938,Validat,ValidateBasicSomaticShortMutations,938,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4982#issuecomment-402356444,1,['Validat'],['ValidateBasicSomaticShortMutations']
Security,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/5007?src=pr&el=h1) Report; > Merging [#5007](https://codecov.io/gh/broadinstitute/gatk/pull/5007?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/1977c537b209d25fea504d2f601af7a9731debcf?src=pr&el=desc) will **increase** coverage by `0.015%`.; > The diff coverage is `80%`. ```diff; @@ Coverage Diff @@; ## master #5007 +/- ##; ===============================================; + Coverage 60.162% 60.177% +0.015% ; - Complexity 12772 12785 +13 ; ===============================================; Files 1095 1096 +1 ; Lines 64616 64666 +50 ; Branches 10394 10397 +3 ; ===============================================; + Hits 38874 38914 +40 ; - Misses 21504 21510 +6 ; - Partials 4238 4242 +4; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/5007?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...e/hellbender/engine/AbstractConcordanceWalker.java](https://codecov.io/gh/broadinstitute/gatk/pull/5007/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvQWJzdHJhY3RDb25jb3JkYW5jZVdhbGtlci5qYXZh) | `84.706% <50%> (-0.836%)` | `13 <0> (ø)` | |; | [...s/walkers/validation/MergeMutect2CallsWithMC3.java](https://codecov.io/gh/broadinstitute/gatk/pull/5007/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vTWVyZ2VNdXRlY3QyQ2FsbHNXaXRoTUMzLmphdmE=) | `81.25% <81.25%> (ø)` | `13 <13> (?)` | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5007#issuecomment-404375834:1232,validat,validation,1232,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5007#issuecomment-404375834,1,['validat'],['validation']
Security,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/5322?src=pr&el=h1) Report; > Merging [#5322](https://codecov.io/gh/broadinstitute/gatk/pull/5322?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/63d864067e74463eb72f58fe60a46603224cf8d2?src=pr&el=desc) will **decrease** coverage by `0.001%`.; > The diff coverage is `25%`. ```diff; @@ Coverage Diff @@; ## master #5322 +/- ##; ===============================================; - Coverage 86.793% 86.792% -0.001% ; Complexity 30107 30107 ; ===============================================; Files 1842 1842 ; Lines 139393 139395 +2 ; Branches 15369 15370 +1 ; ===============================================; Hits 120984 120984 ; - Misses 12823 12824 +1 ; - Partials 5586 5587 +1; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/5322?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...tmutpileup/ValidateBasicSomaticShortMutations.java](https://codecov.io/gh/broadinstitute/gatk/pull/5322/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vYmFzaWNzaG9ydG11dHBpbGV1cC9WYWxpZGF0ZUJhc2ljU29tYXRpY1Nob3J0TXV0YXRpb25zLmphdmE=) | `80.172% <25%> (-0.529%)` | `19 <0> (ø)` | |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/5322/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `80.488% <0%> (-0.61%)` | `42% <0%> (ø)` | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5322#issuecomment-431054487:928,Validat,ValidateBasicSomaticShortMutations,928,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5322#issuecomment-431054487,1,['Validat'],['ValidateBasicSomaticShortMutations']
Security,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/5718?src=pr&el=h1) Report; > Merging [#5718](https://codecov.io/gh/broadinstitute/gatk/pull/5718?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/226f6d70a9c09318d45506c726f6744e2379d60c?src=pr&el=desc) will **decrease** coverage by `0.002%`.; > The diff coverage is `86.667%`. ```diff; @@ Coverage Diff @@; ## master #5718 +/- ##; ===============================================; - Coverage 87.069% 87.067% -0.003% ; - Complexity 31875 31880 +5 ; ===============================================; Files 1940 1940 ; Lines 146738 146756 +18 ; Branches 16226 16229 +3 ; ===============================================; + Hits 127764 127776 +12 ; - Misses 13061 13065 +4 ; - Partials 5913 5915 +2; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/5718?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...walkers/validation/ConcordanceIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5718/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vQ29uY29yZGFuY2VJbnRlZ3JhdGlvblRlc3QuamF2YQ==) | `98.601% <100%> (-1.399%)` | `8 <6> (+2)` | |; | [...llbender/tools/walkers/validation/Concordance.java](https://codecov.io/gh/broadinstitute/gatk/pull/5718/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vQ29uY29yZGFuY2UuamF2YQ==) | `87.179% <50%> (-1.417%)` | `41 <0> (+2)` | |; | [...oadinstitute/hellbender/utils/pairhmm/PairHMM.java](https://codecov.io/gh/broadinstitute/gatk/pull/5718/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9wYWlyaG1tL1BhaXJITU0uamF2YQ==) | `74.82% <0%> (-3.597%)` | `24% <0%> (ø)` | |; | [...hellbender/utils/pairhmm/VectorLoglessPairHMM.java](https://codecov.io/gh/broadinstitute/gatk/pull/5718/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbn,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5718#issuecomment-467164762:941,validat,validation,941,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5718#issuecomment-467164762,1,['validat'],['validation']
Security,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/5984?src=pr&el=h1) Report; > Merging [#5984](https://codecov.io/gh/broadinstitute/gatk/pull/5984?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/730164d3c5ddaead665ce0750e3e4558cef074e7?src=pr&el=desc) will **increase** coverage by `6.783%`.; > The diff coverage is `80%`. ```diff; @@ Coverage Diff @@; ## master #5984 +/- ##; ===============================================; + Coverage 80.142% 86.924% +6.783% ; - Complexity 31040 32741 +1701 ; ===============================================; Files 2014 2014 ; Lines 151333 151367 +34 ; Branches 16612 16617 +5 ; ===============================================; + Hits 121281 131575 +10294 ; + Misses 24197 13728 -10469 ; - Partials 5855 6064 +209; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/5984?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...tools/walkers/ValidateVariantsIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5984/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL1ZhbGlkYXRlVmFyaWFudHNJbnRlZ3JhdGlvblRlc3QuamF2YQ==) | `98.701% <100%> (+97.252%)` | `44 <4> (+42)` | :arrow_up: |; | [...r/tools/walkers/variantutils/ValidateVariants.java](https://codecov.io/gh/broadinstitute/gatk/pull/5984/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhcmlhbnR1dGlscy9WYWxpZGF0ZVZhcmlhbnRzLmphdmE=) | `80.62% <66.667%> (-2.263%)` | `42 <7> (+6)` | |; | [...nder/utils/runtime/StreamingProcessController.java](https://codecov.io/gh/broadinstitute/gatk/pull/5984/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9ydW50aW1lL1N0cmVhbWluZ1Byb2Nlc3NDb250cm9sbGVyLmphdmE=) | `67.299% <0%> (-0.474%)` | `33% <0%> (ø)` | |; | [.../walkers/vqsr/CNNScoreVariantsIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5984/diff?src=,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5984#issuecomment-498803022:955,Validat,ValidateVariantsIntegrationTest,955,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5984#issuecomment-498803022,1,['Validat'],['ValidateVariantsIntegrationTest']
Security,"############################################ | 100%; Preparing transaction: done; Verifying transaction: done; Executing transaction: done; Requirement 'build/gatkPythonPackageArchive.zip' looks like a filename, but the file does not exist; Processing ./build/gatkPythonPackageArchive.zip; Exception:; Traceback (most recent call last):; File ""/BioinfSoftware/Anaconda/envs/gatk/lib/python3.6/site-packages/pip/basecommand.py"", line 215, in main; status = self.run(options, args); File ""/BioinfSoftware/Anaconda/envs/gatk/lib/python3.6/site-packages/pip/commands/install.py"", line 335, in run; wb.build(autobuilding=True); File ""/BioinfSoftware/Anaconda/envs/gatk/lib/python3.6/site-packages/pip/wheel.py"", line 749, in build; self.requirement_set.prepare_files(self.finder); File ""/BioinfSoftware/Anaconda/envs/gatk/lib/python3.6/site-packages/pip/req/req_set.py"", line 380, in prepare_files; ignore_dependencies=self.ignore_dependencies)); File ""/BioinfSoftware/Anaconda/envs/gatk/lib/python3.6/site-packages/pip/req/req_set.py"", line 620, in _prepare_file; session=self.session, hashes=hashes); File ""/BioinfSoftware/Anaconda/envs/gatk/lib/python3.6/site-packages/pip/download.py"", line 809, in unpack_url; unpack_file_url(link, location, download_dir, hashes=hashes); File ""/BioinfSoftware/Anaconda/envs/gatk/lib/python3.6/site-packages/pip/download.py"", line 715, in unpack_file_url; unpack_file(from_path, location, content_type, link); File ""/BioinfSoftware/Anaconda/envs/gatk/lib/python3.6/site-packages/pip/utils/__init__.py"", line 599, in unpack_file; flatten=not filename.endswith('.whl'); File ""/BioinfSoftware/Anaconda/envs/gatk/lib/python3.6/site-packages/pip/utils/__init__.py"", line 482, in unzip_file; zipfp = open(filename, 'rb'); FileNotFoundError: [Errno 2] Datei oder Verzeichnis nicht gefunden: '/BioinfSoftware/build/gatkPythonPackageArchive.zip'. CondaValueError: pip returned an error; ```. Thanks also to all the other ones for their good hints for using germlineCNV caller.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4125#issuecomment-357188460:2652,hash,hashes,2652,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4125#issuecomment-357188460,4,['hash'],['hashes']
Security,$13.hasNext; 0.4% 68 + 0 htsjdk.samtools.BinaryTagCodec.readSingleValue; 0.4% 66 + 0 org.broadinstitute.hellbender.utils.read.markduplicates.OpticalDuplicateFinder.getRapidDefaultReadNameRegexSplit; 0.4% 63 + 1 java.util.stream.ReferencePipeline.collect; 0.4% 61 + 0 org.broadinstitute.hellbender.relocated.com.google.common.collect.Multimaps.index; 0.4% 59 + 1 org.apache.spark.util.collection.TimSort$SortState.mergeHi; 0.3% 49 + 0 scala.collection.Iterator$$anon$11.next; 0.3% 48 + 0 htsjdk.samtools.util.BlockCompressedInputStream.read; 0.3% 47 + 0 org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSparkUtils.lambda$markPairedEnds$66146993$1; 0.3% 45 + 0 scala.collection.Iterator$$anon$13.next; 0.3% 1 + 42 org.apache.spark.util.collection.PartitionedSerializedPairBuffer.insert; 0.3% 42 + 0 org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink$$Lambda$113.call; 0.3% 42 + 0 java.util.stream.AbstractPipeline.evaluate; 0.2% 38 + 0 java.util.HashMap.putVal; 0.2% 37 + 0 htsjdk.samtools.BinaryCigarCodec.decode; 33.6% 5071 + 112 Total compiled (including elided). Stub + native Method ; 28.5% 0 + 4400 java.lang.System.identityHashCode; 17.0% 0 + 2627 java.lang.String.intern; 5.8% 0 + 891 java.util.zip.Inflater.inflateBytes; 4.0% 0 + 615 java.io.FileOutputStream.writeBytes; 3.9% 0 + 602 htsjdk.samtools.util.zip.IntelDeflater.deflateBytes; 1.3% 0 + 200 java.io.FileOutputStream.open0; 0.8% 0 + 119 sun.nio.ch.NativeThread.current; 0.7% 0 + 103 java.util.zip.Inflater.reset; 0.6% 0 + 92 org.apache.hadoop.util.NativeCrc32.nativeComputeChunkedSumsByteArray; 0.6% 0 + 91 htsjdk.samtools.util.zip.IntelDeflater.reset; 0.4% 0 + 56 java.io.FileInputStream.readBytes; 0.3% 0 + 41 sun.nio.ch.FileDispatcherImpl.read0; 0.2% 0 + 33 java.lang.Class.isPrimitive; 0.2% 0 + 27 java.lang.Throwable.fillInStackTrace; 0.1% 1 + 19 java.io.UnixFileSystem.getLength; 0.1% 0 + 17 sun.nio.ch.FileDispatcherImpl.size0; 0.1% 0 + 14 org.apache.hadoop.util.NativeCrc32.n,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1702#issuecomment-210127581:8977,Hash,HashMap,8977,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1702#issuecomment-210127581,1,['Hash'],['HashMap']
Security,"'Passing' workflow at: https://app.terra.bio/#workspaces/allofus-drc-wgs-dev/AoU_DRC_WGS_12-6-21_beta_ingest/job_history/e51afc46-ef55-4c59-b4dd-6ab7d3de0ca8 . Note that this workflow fails because one of the actual validations fails, so the report that is generated indicates this",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7937#issuecomment-1182284459:216,validat,validations,216,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7937#issuecomment-1182284459,1,['validat'],['validations']
Security,"()` method below, and it implies that either the `perAlleleValues` for one of the alleles is itself null, or the `perAlleleValues` for one of the alleles contains a null `Integer`. In `computeSBAnnotation()` we have `perAlleleValues.values().removeIf(Objects::isNull)`, which seems to rule out the former option (perAlleleValues for a particular allele itself being null), and implies that instead one of the individual Integers in the `List<Integer>` perAlleleValues for a particular allele is null. Any ideas on how that could happen?. ```; public static String encode(List<Integer> alleleValues) {; return String.join("","", alleleValues.stream().map(i -> i.toString()).collect(Collectors.toList()));; }. protected static String makeRawAnnotationString(final List<Allele> vcAlleles, final Map<Allele, List<Integer>> perAlleleValues) {; final List<String> alleleStrings = vcAlleles.stream(); // does not replace a null value with zero list - only if the key is not in the map; .map(a -> perAlleleValues.getOrDefault(a, ZERO_LIST)); .map(StrandBiasUtils::encode); .collect(Collectors.toList());; return String.join(AnnotationUtils.ALLELE_SPECIFIC_RAW_DELIM, alleleStrings);. }. public static Map<String, Object> computeSBAnnotation(VariantContext vc, AlleleLikelihoods<GATKRead, Allele> likelihoods, String key) {; // calculate the annotation from the likelihoods; // likelihoods can come from HaplotypeCaller or Mutect2 call to VariantAnnotatorEngine; final Map<String, Object> annotations = new HashMap<>();; final ReducibleAnnotationData<List<Integer>> myData = new AlleleSpecificAnnotationData<>(vc.getAlleles(),null);; getStrandCountsFromLikelihoodMap(vc, likelihoods, myData, MIN_COUNT);; Map<Allele, List<Integer>> perAlleleValues = new LinkedHashMap<>(myData.getAttributeMap());; perAlleleValues.values().removeIf(Objects::isNull);; final String annotationString = makeRawAnnotationString(vc.getAlleles(), perAlleleValues);; annotations.put(key, annotationString);; return annotations;; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6766#issuecomment-697902360:1674,Hash,HashMap,1674,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6766#issuecomment-697902360,1,['Hash'],['HashMap']
Security,(+2)` | :arrow_up: |; | [...r/tools/walkers/mutect/Mutect2IntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5317/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL211dGVjdC9NdXRlY3QySW50ZWdyYXRpb25UZXN0LmphdmE=) | `94.369% <100%> (+0.588%)` | `71 <3> (+3)` | :arrow_up: |; | [...te/hellbender/tools/walkers/annotator/CountNs.java](https://codecov.io/gh/broadinstitute/gatk/pull/5317/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Fubm90YXRvci9Db3VudE5zLmphdmE=) | `75% <75%> (ø)` | `8 <8> (?)` | |; | [...r/tools/walkers/mutect/Mutect2FilteringEngine.java](https://codecov.io/gh/broadinstitute/gatk/pull/5317/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL211dGVjdC9NdXRlY3QyRmlsdGVyaW5nRW5naW5lLmphdmE=) | `85.232% <76%> (-1.089%)` | `63 <4> (+4)` | |; | [...nstitute/hellbender/utils/gcs/BucketUtilsTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5317/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHNUZXN0LmphdmE=) | `56.303% <0%> (-3.103%)` | `13% <0%> (+1%)` | |; | [...ls/genomicsdb/GenomicsDBImportIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5317/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9nZW5vbWljc2RiL0dlbm9taWNzREJJbXBvcnRJbnRlZ3JhdGlvblRlc3QuamF2YQ==) | `86.682% <0%> (-2.388%)` | `77% <0%> (ø)` | |; | [...tmutpileup/ValidateBasicSomaticShortMutations.java](https://codecov.io/gh/broadinstitute/gatk/pull/5317/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vYmFzaWNzaG9ydG11dHBpbGV1cC9WYWxpZGF0ZUJhc2ljU29tYXRpY1Nob3J0TXV0YXRpb25zLmphdmE=) | `80.172% <0%> (-0.529%)` | `19% <0%> (ø)` | |; | ... and [9 more](https://codecov.io/gh/broadinstitute/gatk/pull/5317/diff?src=pr&el=tree-more) | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5317#issuecomment-430675873:3714,Validat,ValidateBasicSomaticShortMutations,3714,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5317#issuecomment-430675873,1,['Validat'],['ValidateBasicSomaticShortMutations']
Security,(You can also permanently turn the validation off for your tool by overriding the `getSequenceDictionaryValidationArgumentCollection()` method),MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6589#issuecomment-625367361:35,validat,validation,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6589#issuecomment-625367361,1,['validat'],['validation']
Security,) | `88.889% <0%> (+0.654%)` | `10% <0%> (+4%)` | :arrow_up: |; | [...r/tools/walkers/validation/RemoveNearbyIndels.java](https://codecov.io/gh/broadinstitute/gatk/pull/4066/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vUmVtb3ZlTmVhcmJ5SW5kZWxzLmphdmE=) | `91.429% <0%> (+0.952%)` | `9% <0%> (+4%)` | :arrow_up: |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4066/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `80% <0%> (+1.29%)` | `39% <0%> (ø)` | :arrow_down: |; | [...adinstitute/hellbender/tools/IndexFeatureFile.java](https://codecov.io/gh/broadinstitute/gatk/pull/4066/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9JbmRleEZlYXR1cmVGaWxlLmphdmE=) | `91.667% <0%> (+1.344%)` | `17% <0%> (+5%)` | :arrow_up: |; | [...r/tools/walkers/variantutils/ValidateVariants.java](https://codecov.io/gh/broadinstitute/gatk/pull/4066/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhcmlhbnR1dGlscy9WYWxpZGF0ZVZhcmlhbnRzLmphdmE=) | `85.465% <0%> (+2.608%)` | `58% <0%> (+24%)` | :arrow_up: |; | [...kers/variantutils/UpdateVCFSequenceDictionary.java](https://codecov.io/gh/broadinstitute/gatk/pull/4066/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhcmlhbnR1dGlscy9VcGRhdGVWQ0ZTZXF1ZW5jZURpY3Rpb25hcnkuamF2YQ==) | `89.873% <0%> (+2.917%)` | `23% <0%> (+9%)` | :arrow_up: |; | [...oadinstitute/hellbender/tools/GatherVcfsCloud.java](https://codecov.io/gh/broadinstitute/gatk/pull/4066/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9HYXRoZXJWY2ZzQ2xvdWQuamF2YQ==) | `77.656% <0%> (+6.845%)` | `54% <0%> (+14%)` | :arrow_up: |; | ... and [1 more](https://codecov.io/gh/broadinstitute/gatk/pull/4066,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4066#issuecomment-355695318:3100,Validat,ValidateVariants,3100,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4066#issuecomment-355695318,1,['Validat'],['ValidateVariants']
Security,"- HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 18:11:33.871 INFO PrintReadsSpark - Deflater: IntelDeflater; 18:11:33.871 INFO PrintReadsSpark - Inflater: IntelInflater; 18:11:33.871 INFO PrintReadsSpark - GCS max retries/reopens: 20; 18:11:33.871 INFO PrintReadsSpark - Using google-cloud-java patch c035098b5e62cb4fe9155eff07ce88449a361f5d from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 18:11:33.871 INFO PrintReadsSpark - Initializing engine; 18:11:33.871 INFO PrintReadsSpark - Done initializing engine; 17/10/13 18:11:33 INFO spark.SparkContext: Running Spark version 2.2.0.cloudera1; 17/10/13 18:11:34 WARN spark.SparkConf: spark.master yarn-client is deprecated in Spark 2.0+, please instead use ""yarn"" with specified deploy mode.; 17/10/13 18:11:34 INFO spark.SparkContext: Submitted application: PrintReadsSpark; 17/10/13 18:11:34 INFO spark.SecurityManager: Changing view acls to: hdfs; 17/10/13 18:11:34 INFO spark.SecurityManager: Changing modify acls to: hdfs; 17/10/13 18:11:34 INFO spark.SecurityManager: Changing view acls groups to: ; 17/10/13 18:11:34 INFO spark.SecurityManager: Changing modify acls groups to: ; 17/10/13 18:11:34 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hdfs); groups with view permissions: Set(); users with modify permissions: Set(hdfs); groups with modify permissions: Set(); 17/10/13 18:11:34 INFO util.Utils: Successfully started service 'sparkDriver' on port 45754.; 17/10/13 18:11:34 INFO spark.SparkEnv: Registering MapOutputTracker; 17/10/13 18:11:34 INFO spark.SparkEnv: Registering BlockManagerMaster; 17/10/13 18:11:34 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 17/10/13 18:11:34 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up; 17/10/13 18:11:34 INFO storage.DiskBlockManager: Created local directory at /tmp/hdfs/blockmgr-ea0e",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-336412775:4031,Secur,SecurityManager,4031,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-336412775,1,['Secur'],['SecurityManager']
Security,"- Initializing engine; 08:37:21.698 INFO GermlineCNVCaller - Done initializing engine; 08:37:22.015 INFO GermlineCNVCaller - Retrieving intervals from read-count file (results/200219_X008378.counts.tsv)...; 08:37:22.119 INFO GermlineCNVCaller - No annotated intervals were provided...; 08:37:22.120 INFO GermlineCNVCaller - No GC-content annotations for intervals found; explicit GC-bias correction will not be performed...; 08:37:22.194 INFO GermlineCNVCaller - Running the tool in the COHORT mode...; 08:37:22.195 INFO GermlineCNVCaller - Shutting down engine; [February 26, 2019 8:37:22 AM GMT] org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller done. Elapsed time: 0.29 minutes.; Runtime.totalMemory()=330301440; java.lang.IllegalArgumentException: Output directory results/190226.181217_K00178.CNVCaller does not exist.; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:724); at org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller.validateArguments(GermlineCNVCaller.java:361); at org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller.doWork(GermlineCNVCaller.java:281); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); at org.broadinstitute.hellbender.Main.main(Main.java:291); Using GATK jar /mnt/storage/apps/software/gatk/4.1.0.0/gatk-package-4.1.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /mnt/storage/apps/software/gatk/4.1.0.0/gatk-package-4.1.0.0-loc`. I'm running these comm",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4825#issuecomment-467406398:15954,validat,validateArguments,15954,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4825#issuecomment-467406398,1,['validat'],['validateArguments']
Security,-----------------------------------------; 16:01:36.872 INFO Funcotator - HTSJDK Version: 2.21.2; 16:01:36.872 INFO Funcotator - Picard Version: 2.21.9; 16:01:36.872 INFO Funcotator - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 16:01:36.872 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:01:36.872 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:01:36.872 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:01:36.872 INFO Funcotator - Deflater: IntelDeflater; 16:01:36.872 INFO Funcotator - Inflater: IntelInflater; 16:01:36.872 INFO Funcotator - GCS max retries/reopens: 20; 16:01:36.872 INFO Funcotator - Requester pays: disabled; 16:01:36.872 INFO Funcotator - Initializing engine; 16:01:37.316 INFO FeatureManager - Using codec VCFCodec to read file file:///home/deepak/software_library/gatk-4.1.7.0/SAMPL3_VARIANTFIL.vcf; 16:01:37.360 INFO Funcotator - Done initializing engine; 16:01:37.360 INFO Funcotator - Validating Sequence Dictionaries...; 16:01:37.366 INFO Funcotator - Processing user transcripts/defaults/overrides...; 16:01:37.366 INFO Funcotator - Initializing data sources...; 16:01:37.368 INFO DataSourceUtils - Initializing data sources from directory: /media/deepak/EXTRA/FUNCOTATOR_DATA/DATA_SOURCES; 16:01:37.369 WARN DataSourceUtils - Could not read MANIFEST.txt: unable to log data sources version information.; 16:01:37.375 INFO DataSourceUtils - Resolved data source file path: file:///home/deepak/software_library/gatk-4.1.7.0/simple_uniprot_Dec012014.tsv -> file:///media/deepak/EXTRA/FUNCOTATOR_DATA/DATA_SOURCES/data_source_14/hg38/simple_uniprot_Dec012014.tsv; 16:01:37.391 INFO DataSourceUtils - Resolved data source file path: file:///home/deepak/software_library/gatk-4.1.7.0/hgnc_download_Nov302017.tsv -> file:///media/deepak/EXTRA/FUNCOTATOR_DATA/DATA_SOURCES/data_source_13/hg38/hgnc_download_Nov302017.tsv; 16:01:37.393 INFO DataSourceUtils - Resolved data source file path: file:/,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6598#issuecomment-664565036:3016,Validat,Validating,3016,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6598#issuecomment-664565036,1,['Validat'],['Validating']
Security,---------------------------; > 12:28:16.542 INFO Funcotator - HTSJDK Version: 2.22.0; > 12:28:16.543 INFO Funcotator - Picard Version: 2.22.8; > 12:28:16.543 INFO Funcotator - HTSJDK Defaults.COMPRESSION_LEVEL : 2; > 12:28:16.543 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; > 12:28:16.543 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; > 12:28:16.543 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; > 12:28:16.543 INFO Funcotator - Deflater: IntelDeflater; > 12:28:16.543 INFO Funcotator - Inflater: IntelInflater; > 12:28:16.543 INFO Funcotator - GCS max retries/reopens: 20; > 12:28:16.543 INFO Funcotator - Requester pays: disabled; > 12:28:16.543 INFO Funcotator - Initializing engine; > 12:28:17.254 INFO FeatureManager - Using codec VCFCodec to read file file:///home/pkus/mutect_test/filtered_variants/P1.vcf.gz; > 12:28:17.687 INFO Funcotator - Done initializing engine; > 12:28:17.688 INFO Funcotator - Validating Sequence Dictionaries...; > 12:28:17.755 INFO Funcotator - Processing user transcripts/defaults/overrides...; > 12:28:17.756 INFO Funcotator - Initializing data sources...; > 12:28:17.759 INFO DataSourceUtils - Initializing data sources from directory: /home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s; > 12:28:17.775 INFO DataSourceUtils - Data sources version: 1.6.2019124s; > 12:28:17.776 INFO DataSourceUtils - Data sources source: ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/funcotator/funcotator_dataSources.v1.6.20190124s.tar.gz; > 12:28:17.776 INFO DataSourceUtils - Data sources alternate source: gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.6.20190124s.tar.gz; > 12:28:17.795 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/chr1_b_bed.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/chr1_b_bed/hg38/chr1_b_bed.tsv; > 12:28:17.805 INFO DataSourceUtils,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6708#issuecomment-661776975:3145,Validat,Validating,3145,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6708#issuecomment-661776975,1,['Validat'],['Validating']
Security,"-IS_BISULFITE_SEQUENCED false --MAX_OPEN_TEMP_FILES 8000 --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 1 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX false --CREATE_MD5_FILE false --help false --version false --verbosity INFO --QUIET false; [March 9, 2017 7:03:42 PM EST] Executing as gspowley@dna on Linux 3.10.0-514.10.2.el7.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_111-b14; Version: Version:4.alpha.2-170-g8d06823-SNAPSHOT; 19:03:42.998 INFO ValidateSamFile - Defaults.BUFFER_SIZE : 131072; 19:03:42.999 INFO ValidateSamFile - Defaults.COMPRESSION_LEVEL : 1; 19:03:42.999 INFO ValidateSamFile - Defaults.CREATE_INDEX : false; 19:03:42.999 INFO ValidateSamFile - Defaults.CREATE_MD5 : false; 19:03:42.999 INFO ValidateSamFile - Defaults.CUSTOM_READER_FACTORY : ; 19:03:42.999 INFO ValidateSamFile - Defaults.EBI_REFERENCE_SERVICE_URL_MASK : http://www.ebi.ac.uk/ena/cram/md5/%s; 19:03:42.999 INFO ValidateSamFile - Defaults.NON_ZERO_BUFFER_SIZE : 131072; 19:03:42.999 INFO ValidateSamFile - Defaults.REFERENCE_FASTA : null; 19:03:43.000 INFO ValidateSamFile - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 19:03:43.000 INFO ValidateSamFile - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 19:03:43.000 INFO ValidateSamFile - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 19:03:43.000 INFO ValidateSamFile - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 19:03:43.000 INFO ValidateSamFile - Defaults.USE_CRAM_REF_DOWNLOAD : false; 19:03:43.000 INFO ValidateSamFile - Deflater JdkDeflater; 19:03:43.000 INFO ValidateSamFile - Inflater JdkInflater; 19:03:43.000 INFO ValidateSamFile - Initializing engine; 19:03:43.000 INFO ValidateSamFile - Done initializing engine; ERROR: Record 9762, Read name 20GAVAAXX100126:7:2:8126:115177, bin field of BAM record does not equal value computed based on alignment start and end, and length of sequence to which read is aligned; ERROR: Record 24466, Read name 20FUKAAXX100202:7:46:13035:77621, bin field of BAM record does not equal value comp",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2423#issuecomment-285513571:1414,Validat,ValidateSamFile,1414,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2423#issuecomment-285513571,1,['Validat'],['ValidateSamFile']
Security,"-hadoop2; 17/11/27 20:39:45 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at droazen-test-cluster-m/10.240.0.10:8032; 17/11/27 20:39:47 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1511814592376_0002; 17/11/27 20:39:52 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@7fbe38a{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 20:39:52.363 INFO CountReadsSpark - Shutting down engine; [November 27, 2017 8:39:52 PM UTC] org.broadinstitute.hellbender.tools.spark.pipelines.CountReadsSpark done. Elapsed time: 0.16 minutes.; Runtime.totalMemory()=630718464; code: 0; message: Error code 404 trying to get security access token from Compute Engine metadata for the default service account. This may be because the virtual machine instance does not have permission scopes specified.; reason: null; location: null; retryable: false; com.google.cloud.storage.StorageException: Error code 404 trying to get security access token from Compute Engine metadata for the default service account. This may be because the virtual machine instance does not have permission scopes specified.; 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:189); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:340); 	at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:197); 	at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:194); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:89); 	at com.google.cloud.RetryHelper.run(RetryHelper.java:74); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:51); 	at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:194); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:614); 	at java.nio.file.Files.exists(Files.java:2385); 	at htsjdk.samtools.util.IOUtil.assertFileIsRea",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3855#issuecomment-347320994:6007,secur,security,6007,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3855#issuecomment-347320994,2,"['access', 'secur']","['access', 'security']"
Security,-haplotype-to-reference-mismatch-penalty -150 --smith-waterman-haplotype-to-reference-ga; p-open-penalty -260 --smith-waterman-haplotype-to-reference-gap-extend-penalty -11 --smith-waterman-read-to-haplotype-match-value 10 --smith-waterman-read-to-haplotype-mismatch-penalty -15; --smith-waterman-read-to-haplotype-gap-open-penalty -30 --smith-waterman-read-to-haplotype-gap-extend-penalty -5 --flow-assembly-collapse-hmer-size 0 --flow-assembly-collapse-partial-mode; false --flow-filter-alleles false --flow-filter-alleles-qual-threshold 30.0 --flow-filter-alleles-sor-threshold 3.0 --flow-filter-lone-alleles false --flow-filter-alleles-debug-graphs fal; se --min-assembly-region-size 50 --max-assembly-region-size 300 --active-probability-threshold 0.002 --max-prob-propagation-distance 50 --force-active false --assembly-region-padding 100 -; -padding-around-indels 75 --padding-around-snps 20 --padding-around-strs 75 --max-extension-into-assembly-region-padding-legacy 25 --max-reads-per-alignment-start 50 --enable-legacy-assemb; ly-region-trimming false --interval-set-rule UNION --interval-padding 0 --interval-exclusion-padding 0 --interval-merging-rule ALL --read-validation-stringency SILENT --seconds-between-pro; gress-updates 10.0 --disable-sequence-dictionary-validation false --create-output-bam-index true --create-output-bam-md5 false --create-output-variant-index true --create-output-variant-md; 5 false --max-variants-per-shard 0 --lenient false --add-output-sam-program-record true --add-output-vcf-command-line true --cloud-prefetch-buffer 40 --cloud-index-prefetch-buffer -1 --dis; able-bam-index-caching false --sites-only-vcf-output false --help false --version false --showHidden false --verbosity INFO --QUIET false --use-jdk-deflater false --use-jdk-inflater false ; --gcs-max-retries 20 --gcs-project-for-requester-pays --disable-tool-default-read-filters false --minimum-mapping-quality 20 --disable-tool-default-annotations false --enable-all-annotati; ons false --a,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8574#issuecomment-1793390789:9015,validat,validation-stringency,9015,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8574#issuecomment-1793390789,1,['validat'],['validation-stringency']
Security,"-length read without FZ, CS or CQ tag; ERROR: Record 966616, Read name UMI-ACG-TGG-5, Zero-length read without FZ, CS or CQ tag; ERROR: Record 966618, Read name UMI-ACT-GGG-11, Zero-length read without FZ, CS or CQ tag; ERROR: Record 966620, Read name UMI-ACT-GGG-12, Zero-length read without FZ, CS or CQ tag; ERROR: Record 966627, Read name UMI-GGC-TGT-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 966674, Read name UMI-CCT-GTC-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 966699, Read name UMI-CCG-TGA-4, Zero-length read without FZ, CS or CQ tag; ERROR: Record 966722, Read name UMI-AGG-TGT-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 966742, Read name UMI-CCG-TCA-5, Zero-length read without FZ, CS or CQ tag; ERROR: Record 966752, Read name UMI-GAA-GAT-5, Zero-length read without FZ, CS or CQ tag; ERROR: Record 966784, Read name UMI-CCT-TAT-12, Zero-length read without FZ, CS or CQ tag; ERROR: Record 966875, Read name UMI-AGG-GGG-10, Zero-length read without FZ, CS or CQ tag; ERROR: Record 966887, Read name UMI-AGG-CCG-5, Zero-length read without FZ, CS or CQ tag; ERROR: Record 966916, Read name UMI-GCT-TCG-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 966939, Read name UMI-CAA-TGT-8, Zero-length read without FZ, CS or CQ tag; ERROR: Record 966989, Read name UMI-GAA-TCA-7, Zero-length read without FZ, CS or CQ tag; ERROR: Record 966991, Read name UMI-TAG-TGT-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 967245, Read name UMI-AAG-ATT-8, Zero-length read without FZ, CS or CQ tag; ERROR: Record 975151, Read name UMI-ACT-CCC-4, Zero-length read without FZ, CS or CQ tag; ERROR: Record 1064783, Read name UMI-GGA-GGT-6, Zero-length read without FZ, CS or CQ tag; Maximum output of [100] errors reached.; [Tue Jul 14 11:25:59 EDT 2020] picard.sam.ValidateSamFile done. Elapsed time: 0.12 minutes.; Runtime.totalMemory()=1450180608; To get help, see http://broadinstitute.github.io/picard/index.html#GettingHelp",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6695#issuecomment-658247132:11065,Validat,ValidateSamFile,11065,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6695#issuecomment-658247132,1,['Validat'],['ValidateSamFile']
Security,"-length; ```; WMCF9-CB5:shlee$ ./gatk LeftAlignAndTrimVariants -R ~/Documents/ref/hg38/Homo_sapiens_assembly38.fasta -V ~/Downloads/zeta_snippet_shlee/zeta_snippet.vcf.gz --max-indel-length 250 -O zeta_snippet_leftalign_250_96branch.vcf.gz; Using GATK wrapper script /Users/shlee/Documents/branches/hellbender/build/install/gatk/bin/gatk; Running:; /Users/shlee/Documents/branches/hellbender/build/install/gatk/bin/gatk LeftAlignAndTrimVariants -R /Users/shlee/Documents/ref/hg38/Homo_sapiens_assembly38.fasta -V /Users/shlee/Downloads/zeta_snippet_shlee/zeta_snippet.vcf.gz --max-indel-length 250 -O zeta_snippet_leftalign_250_96branch.vcf.gz; 14:03:44.243 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/shlee/Documents/branches/hellbender/build/install/gatk/lib/gkl-0.8.5.jar!/com/intel/gkl/native/libgkl_compression.dylib; Sep 06, 2018 2:03:44 PM shaded.cloud_nio.com.google.auth.oauth2.DefaultCredentialsProvider warnAboutProblematicCredentials; WARNING: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a ""quota exceeded"" or ""API not enabled"" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/.; 14:03:44.358 INFO LeftAlignAndTrimVariants - ------------------------------------------------------------; 14:03:44.358 INFO LeftAlignAndTrimVariants - The Genome Analysis Toolkit (GATK) v4.0.8.1-25-g0c6f06f-SNAPSHOT; 14:03:44.359 INFO LeftAlignAndTrimVariants - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:03:44.359 INFO LeftAlignAndTrimVariants - Executing as shlee@WMCF9-CB5 on Mac OS X v10.13.6 x86_64; 14:03:44.359 INFO LeftAlignAndTrimVariants - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_111-b14; 14:03:44.359 INFO LeftAlignAndTrimVariants - Start Date/Time",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3487#issuecomment-419190326:7097,authenticat,authenticated,7097,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3487#issuecomment-419190326,1,['authenticat'],['authenticated']
Security,".....................; Exception in thread ""main"" javax.net.ssl.SSLException: Connection has been shutdown: javax.net.ssl.SSLException: java.net.SocketException: Connection reset; 	at sun.security.ssl.SSLSocketImpl.checkEOF(SSLSocketImpl.java:1541); 	at sun.security.ssl.AppInputStream.available(AppInputStream.java:60); 	at java.io.BufferedInputStream.available(BufferedInputStream.java:410); 	at sun.net.www.MeteredStream.available(MeteredStream.java:170); 	at sun.net.www.http.KeepAliveStream.close(KeepAliveStream.java:85); 	at java.io.FilterInputStream.close(FilterInputStream.java:181); 	at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.close(HttpURLConnection.java:3448); 	at org.gradle.wrapper.Download.downloadInternal(Download.java:77); 	at org.gradle.wrapper.Download.download(Download.java:44); 	at org.gradle.wrapper.Install$1.call(Install.java:61); 	at org.gradle.wrapper.Install$1.call(Install.java:48); 	at org.gradle.wrapper.ExclusiveFileAccessManager.access(ExclusiveFileAccessManager.java:69); 	at org.gradle.wrapper.Install.createDist(Install.java:48); 	at org.gradle.wrapper.WrapperExecutor.execute(WrapperExecutor.java:107); 	at org.gradle.wrapper.GradleWrapperMain.main(GradleWrapperMain.java:61); Caused by: javax.net.ssl.SSLException: java.net.SocketException: Connection reset; 	at sun.security.ssl.Alerts.getSSLException(Alerts.java:208); 	at sun.security.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1949); 	at sun.security.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1906); 	at sun.security.ssl.SSLSocketImpl.handleException(SSLSocketImpl.java:1870); 	at sun.security.ssl.SSLSocketImpl.handleException(SSLSocketImpl.java:1815); 	at sun.security.ssl.AppInputStream.read(AppInputStream.java:116); 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:284); 	at java.io.BufferedInputStream.read(BufferedInputStream.java:345); 	at sun.net.www.MeteredStream.read(MeteredStream.java:134); 	at java.io.FilterInputStream.read(FilterInputStream.java:133); 	at ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4194#issuecomment-358498401:1316,access,access,1316,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4194#issuecomment-358498401,1,['access'],['access']
Security,".2.4.1/ensembl-vep/PE69_chr3.vcf; 10:58:20.063 INFO VariantAnnotator - Done initializing engine; 10:58:20.091 WARN VariantAnnotatorEngine - The requested expression attribute ""gnomad.ALT"" is missing from the header in its resource file gnomad; 10:58:20.140 INFO ProgressMeter - Starting traversal; 10:58:20.140 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 10:58:42.160 INFO VariantAnnotator - Shutting down engine; [March 17, 2022 at 10:58:42 AM CET] org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotator done. Elapsed time: 0.37 minutes.; Runtime.totalMemory()=17158897664; java.lang.IllegalStateException: Allele in genotype C not in the variant context [C*, CT]; 	at htsjdk.variant.variantcontext.VariantContext$Validation.validateGenotypes(VariantContext.java:382); 	at htsjdk.variant.variantcontext.VariantContext$Validation.access$200(VariantContext.java:323); 	at htsjdk.variant.variantcontext.VariantContext$Validation$2.validate(VariantContext.java:331); 	at htsjdk.variant.variantcontext.VariantContext.lambda$validate$0(VariantContext.java:1384); 	at java.base/java.lang.Iterable.forEach(Iterable.java:75); 	at htsjdk.variant.variantcontext.VariantContext.validate(VariantContext.java:1384); 	at htsjdk.variant.variantcontext.VariantContext.<init>(VariantContext.java:489); 	at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:647); 	at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:638); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.trimAlleles(GATKVariantContextUtils.java:1464); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.trimAlleles(GATKVariantContextUtils.java:1420); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.getMinRepresentationBiallelics(VariantAnnotatorEngine.java:568); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.a",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6689#issuecomment-1070784053:4175,validat,validate,4175,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6689#issuecomment-1070784053,1,['validat'],['validate']
Security,".971 INFO FeatureManager - Using codec VCFCodec to read file file:///run/media/riadh/My%20Book_From%20Eiklid/Analysis/gatk-4.2.4.1/ensembl-vep/PE69_chr3.vcf; 10:58:20.063 INFO VariantAnnotator - Done initializing engine; 10:58:20.091 WARN VariantAnnotatorEngine - The requested expression attribute ""gnomad.ALT"" is missing from the header in its resource file gnomad; 10:58:20.140 INFO ProgressMeter - Starting traversal; 10:58:20.140 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 10:58:42.160 INFO VariantAnnotator - Shutting down engine; [March 17, 2022 at 10:58:42 AM CET] org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotator done. Elapsed time: 0.37 minutes.; Runtime.totalMemory()=17158897664; java.lang.IllegalStateException: Allele in genotype C not in the variant context [C*, CT]; 	at htsjdk.variant.variantcontext.VariantContext$Validation.validateGenotypes(VariantContext.java:382); 	at htsjdk.variant.variantcontext.VariantContext$Validation.access$200(VariantContext.java:323); 	at htsjdk.variant.variantcontext.VariantContext$Validation$2.validate(VariantContext.java:331); 	at htsjdk.variant.variantcontext.VariantContext.lambda$validate$0(VariantContext.java:1384); 	at java.base/java.lang.Iterable.forEach(Iterable.java:75); 	at htsjdk.variant.variantcontext.VariantContext.validate(VariantContext.java:1384); 	at htsjdk.variant.variantcontext.VariantContext.<init>(VariantContext.java:489); 	at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:647); 	at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:638); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.trimAlleles(GATKVariantContextUtils.java:1464); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.trimAlleles(GATKVariantContextUtils.java:1420); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.getMinRepresentationBia",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6689#issuecomment-1070784053:4065,Validat,Validation,4065,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6689#issuecomment-1070784053,1,['Validat'],['Validation']
Security,".USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 18:11:33.871 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 18:11:33.871 INFO PrintReadsSpark - Deflater: IntelDeflater; 18:11:33.871 INFO PrintReadsSpark - Inflater: IntelInflater; 18:11:33.871 INFO PrintReadsSpark - GCS max retries/reopens: 20; 18:11:33.871 INFO PrintReadsSpark - Using google-cloud-java patch c035098b5e62cb4fe9155eff07ce88449a361f5d from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 18:11:33.871 INFO PrintReadsSpark - Initializing engine; 18:11:33.871 INFO PrintReadsSpark - Done initializing engine; 17/10/13 18:11:33 INFO spark.SparkContext: Running Spark version 2.2.0.cloudera1; 17/10/13 18:11:34 WARN spark.SparkConf: spark.master yarn-client is deprecated in Spark 2.0+, please instead use ""yarn"" with specified deploy mode.; 17/10/13 18:11:34 INFO spark.SparkContext: Submitted application: PrintReadsSpark; 17/10/13 18:11:34 INFO spark.SecurityManager: Changing view acls to: hdfs; 17/10/13 18:11:34 INFO spark.SecurityManager: Changing modify acls to: hdfs; 17/10/13 18:11:34 INFO spark.SecurityManager: Changing view acls groups to: ; 17/10/13 18:11:34 INFO spark.SecurityManager: Changing modify acls groups to: ; 17/10/13 18:11:34 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hdfs); groups with view permissions: Set(); users with modify permissions: Set(hdfs); groups with modify permissions: Set(); 17/10/13 18:11:34 INFO util.Utils: Successfully started service 'sparkDriver' on port 45754.; 17/10/13 18:11:34 INFO spark.SparkEnv: Registering MapOutputTracker; 17/10/13 18:11:34 INFO spark.SparkEnv: Registering BlockManagerMaster; 17/10/13 18:11:34 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 17/10/13 18:11:34 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up; 17/10/13 18:11:34 INFO ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-336412775:3956,Secur,SecurityManager,3956,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-336412775,1,['Secur'],['SecurityManager']
Security,.getBlockLocations(FSDirStatAndListingOp.java:152); 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1819); 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:692); 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:381); 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java); 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447); 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793); 	at java.security.AccessController.doPrivileged(Native Method); 	at javax.security.auth.Subject.doAs(Subject.java:422); 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1840); 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489). 	at org.apache.hadoop.ipc.Client.call(Client.java:1475); 	at org.apache.hadoop.ipc.Client.call(Client.java:1412); 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229); 	at com.sun.proxy.$Proxy10.getBlockLocations(Unknown Source); 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getBlockLocations(ClientNamenodeProtocolTranslatorPB.java:255); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191); 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHa,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294:9819,secur,security,9819,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294,1,['secur'],['security']
Security,.getBlockLocations(FSDirStatAndListingOp.java:152); 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1819); 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:692); 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:381); 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java); 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447); 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793); 	at java.security.AccessController.doPrivileged(Native Method); 	at javax.security.auth.Subject.doAs(Subject.java:422); 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1840); 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489). 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:237); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReads(GATKSparkTool.java:488); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:468); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:458); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitu,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294:3661,secur,security,3661,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294,1,['secur'],['security']
Security,.getBlockLocations(FSDirStatAndListingOp.java:152); 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1819); 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:692); 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:381); 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java); 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447); 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793); 	at java.security.AccessController.doPrivileged(Native Method); 	at javax.security.auth.Subject.doAs(Subject.java:422); 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1840); 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489). 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423); 	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106); 	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73); 	at org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1228); 	at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1213); 	at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1201); 	at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:306); 	at ,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294:6578,secur,security,6578,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294,1,['secur'],['security']
Security,.getBlockLocations(FSDirStatAndListingOp.java:152); 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1819); 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:692); 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:381); 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java); 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447); 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793); 	at java.security.AccessController.doPrivileged(Native Method); 	at javax.security.auth.Subject.doAs(Subject.java:422); 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1840); 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489). ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException: Failed to read bam header from /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; Caused by:File does not exist: /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:72); 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:62); 	at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:152); 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1819); 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:692); 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslato,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294:1998,secur,security,1998,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294,1,['secur'],['security']
Security,.google.cloud.storage.StorageException: Connection has been shutdown: javax.net.ssl.SSLException: java.net.SocketException: Connection reset; 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:189); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:515); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:127); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:124); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:93); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:49); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:124); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:113); 	... 41 more; Caused by: javax.net.ssl.SSLException: Connection has been shutdown: javax.net.ssl.SSLException: java.net.SocketException: Connection reset; 	at sun.security.ssl.SSLSocketImpl.checkEOF(SSLSocketImpl.java:1541); 	at sun.security.ssl.AppInputStream.available(AppInputStream.java:60); 	at java.io.BufferedInputStream.available(BufferedInputStream.java:410); 	at sun.net.www.MeteredStream.available(MeteredStream.java:170); 	at sun.net.www.http.KeepAliveStream.close(KeepAliveStream.java:85); 	at java.io.FilterInputStream.close(FilterInputStream.java:181); 	at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.close(HttpURLConnection.java:3448); 	at java.io.FilterInputStream.close(FilterInputStream.java:181); 	at shaded.cloud_nio.com.google.api.client.util.IOUtils.copy(IOUtils.java:97); 	at shaded.cloud_nio.com.google.api.client.util.IOUtils.copy(IOUtils.java:63); 	at shaded.cloud_nio.com.google.api.client.http.HttpResponse.download(HttpResponse.java:421); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:510); 	... 47 more; Caused by: javax.net.ssl.SSLException: java.net.SocketException: Connection reset; 	at ,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-308541727:5137,secur,security,5137,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-308541727,1,['secur'],['security']
Security,.google.cloud.storage.StorageException: Connection has been shutdown: javax.net.ssl.SSLException: java.net.SocketException: Connection reset; 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:189); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:515); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:127); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:124); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:93); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:49); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:124); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:113); 	... 49 more; Caused by: javax.net.ssl.SSLException: Connection has been shutdown: javax.net.ssl.SSLException: java.net.SocketException: Connection reset; 	at sun.security.ssl.SSLSocketImpl.checkEOF(SSLSocketImpl.java:1541); 	at sun.security.ssl.AppInputStream.available(AppInputStream.java:60); 	at java.io.BufferedInputStream.available(BufferedInputStream.java:410); 	at sun.net.www.MeteredStream.available(MeteredStream.java:170); 	at sun.net.www.http.KeepAliveStream.close(KeepAliveStream.java:85); 	at java.io.FilterInputStream.close(FilterInputStream.java:181); 	at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.close(HttpURLConnection.java:3448); 	at java.io.FilterInputStream.close(FilterInputStream.java:181); 	at shaded.cloud_nio.com.google.api.client.util.IOUtils.copy(IOUtils.java:97); 	at shaded.cloud_nio.com.google.api.client.util.IOUtils.copy(IOUtils.java:63); 	at shaded.cloud_nio.com.google.api.client.http.HttpResponse.download(HttpResponse.java:421); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:510); 	... 55 more; Caused by: javax.net.ssl.SSLException: java.net.SocketException: Connection reset; 	at ,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317549138:8625,secur,security,8625,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317549138,1,['secur'],['security']
Security,.http.KeepAliveStream.close(KeepAliveStream.java:85); 	at java.io.FilterInputStream.close(FilterInputStream.java:181); 	at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.close(HttpURLConnection.java:3448); 	at java.io.FilterInputStream.close(FilterInputStream.java:181); 	at shaded.cloud_nio.com.google.api.client.util.IOUtils.copy(IOUtils.java:97); 	at shaded.cloud_nio.com.google.api.client.util.IOUtils.copy(IOUtils.java:63); 	at shaded.cloud_nio.com.google.api.client.http.HttpResponse.download(HttpResponse.java:421); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:507); 	... 12 more; Caused by: javax.net.ssl.SSLException: java.net.SocketException: Connection reset; 	at sun.security.ssl.Alerts.getSSLException(Alerts.java:208); 	at sun.security.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1949); 	at sun.security.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1906); 	at sun.security.ssl.SSLSocketImpl.handleException(SSLSocketImpl.java:1870); 	at sun.security.ssl.SSLSocketImpl.handleException(SSLSocketImpl.java:1815); 	at sun.security.ssl.AppInputStream.read(AppInputStream.java:116); 	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286); 	at java.io.BufferedInputStream.read(BufferedInputStream.java:345); 	at sun.net.www.MeteredStream.read(MeteredStream.java:134); 	at java.io.FilterInputStream.read(FilterInputStream.java:133); 	at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.read(HttpURLConnection.java:3375); 	at shaded.cloud_nio.com.google.api.client.http.javanet.NetHttpResponse$SizeValidatingInputStream.read(NetHttpResponse.java:169); 	at java.io.FilterInputStream.read(FilterInputStream.java:107); 	at shaded.cloud_nio.com.google.api.client.util.ByteStreams.copy(ByteStreams.java:51); 	at shaded.cloud_nio.com.google.api.client.util.IOUtils.copy(IOUtils.java:94); 	... 15 more; Caused by: java.net.SocketException: Connection reset; 	at java.net,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-301610931:8125,secur,security,8125,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-301610931,1,['secur'],['security']
Security,.http.KeepAliveStream.close(KeepAliveStream.java:85); 	at java.io.FilterInputStream.close(FilterInputStream.java:181); 	at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.close(HttpURLConnection.java:3448); 	at java.io.FilterInputStream.close(FilterInputStream.java:181); 	at shaded.cloud_nio.com.google.api.client.util.IOUtils.copy(IOUtils.java:97); 	at shaded.cloud_nio.com.google.api.client.util.IOUtils.copy(IOUtils.java:63); 	at shaded.cloud_nio.com.google.api.client.http.HttpResponse.download(HttpResponse.java:421); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:510); 	... 47 more; Caused by: javax.net.ssl.SSLException: java.net.SocketException: Connection reset; 	at sun.security.ssl.Alerts.getSSLException(Alerts.java:208); 	at sun.security.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1949); 	at sun.security.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1906); 	at sun.security.ssl.SSLSocketImpl.handleException(SSLSocketImpl.java:1870); 	at sun.security.ssl.SSLSocketImpl.handleException(SSLSocketImpl.java:1815); 	at sun.security.ssl.AppInputStream.read(AppInputStream.java:116); 	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286); 	at java.io.BufferedInputStream.read(BufferedInputStream.java:345); 	at sun.net.www.MeteredStream.read(MeteredStream.java:134); 	at java.io.FilterInputStream.read(FilterInputStream.java:133); 	at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.read(HttpURLConnection.java:3375); 	at shaded.cloud_nio.com.google.api.client.http.javanet.NetHttpResponse$SizeValidatingInputStream.read(NetHttpResponse.java:169); 	at java.io.FilterInputStream.read(FilterInputStream.java:107); 	at shaded.cloud_nio.com.google.api.client.util.ByteStreams.copy(ByteStreams.java:51); 	at shaded.cloud_nio.com.google.api.client.util.IOUtils.copy(IOUtils.java:94); 	... 50 more; Caused by: java.net.SocketException: Connection reset; 	at java.net,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-308541727:6419,secur,security,6419,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-308541727,1,['secur'],['security']
Security,.http.KeepAliveStream.close(KeepAliveStream.java:85); 	at java.io.FilterInputStream.close(FilterInputStream.java:181); 	at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.close(HttpURLConnection.java:3448); 	at java.io.FilterInputStream.close(FilterInputStream.java:181); 	at shaded.cloud_nio.com.google.api.client.util.IOUtils.copy(IOUtils.java:97); 	at shaded.cloud_nio.com.google.api.client.util.IOUtils.copy(IOUtils.java:63); 	at shaded.cloud_nio.com.google.api.client.http.HttpResponse.download(HttpResponse.java:421); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:510); 	... 55 more; Caused by: javax.net.ssl.SSLException: java.net.SocketException: Connection reset; 	at sun.security.ssl.Alerts.getSSLException(Alerts.java:208); 	at sun.security.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1949); 	at sun.security.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1906); 	at sun.security.ssl.SSLSocketImpl.handleException(SSLSocketImpl.java:1870); 	at sun.security.ssl.SSLSocketImpl.handleException(SSLSocketImpl.java:1815); 	at sun.security.ssl.AppInputStream.read(AppInputStream.java:116); 	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286); 	at java.io.BufferedInputStream.read(BufferedInputStream.java:345); 	at sun.net.www.MeteredStream.read(MeteredStream.java:134); 	at java.io.FilterInputStream.read(FilterInputStream.java:133); 	at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.read(HttpURLConnection.java:3375); 	at shaded.cloud_nio.com.google.api.client.http.javanet.NetHttpResponse$SizeValidatingInputStream.read(NetHttpResponse.java:169); 	at java.io.FilterInputStream.read(FilterInputStream.java:107); 	at shaded.cloud_nio.com.google.api.client.util.ByteStreams.copy(ByteStreams.java:51); 	at shaded.cloud_nio.com.google.api.client.util.IOUtils.copy(IOUtils.java:94); 	... 58 more; Caused by: java.net.SocketException: Connection reset; 	at java.net,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317549138:9907,secur,security,9907,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317549138,1,['secur'],['security']
Security,.java:3448); 	at org.gradle.wrapper.Download.downloadInternal(Download.java:77); 	at org.gradle.wrapper.Download.download(Download.java:44); 	at org.gradle.wrapper.Install$1.call(Install.java:61); 	at org.gradle.wrapper.Install$1.call(Install.java:48); 	at org.gradle.wrapper.ExclusiveFileAccessManager.access(ExclusiveFileAccessManager.java:69); 	at org.gradle.wrapper.Install.createDist(Install.java:48); 	at org.gradle.wrapper.WrapperExecutor.execute(WrapperExecutor.java:107); 	at org.gradle.wrapper.GradleWrapperMain.main(GradleWrapperMain.java:61); Caused by: javax.net.ssl.SSLException: java.net.SocketException: Connection reset; 	at sun.security.ssl.Alerts.getSSLException(Alerts.java:208); 	at sun.security.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1949); 	at sun.security.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1906); 	at sun.security.ssl.SSLSocketImpl.handleException(SSLSocketImpl.java:1870); 	at sun.security.ssl.SSLSocketImpl.handleException(SSLSocketImpl.java:1815); 	at sun.security.ssl.AppInputStream.read(AppInputStream.java:116); 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:284); 	at java.io.BufferedInputStream.read(BufferedInputStream.java:345); 	at sun.net.www.MeteredStream.read(MeteredStream.java:134); 	at java.io.FilterInputStream.read(FilterInputStream.java:133); 	at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.read(HttpURLConnection.java:3375); 	at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.read(HttpURLConnection.java:3368); 	at org.gradle.wrapper.Download.downloadInternal(Download.java:62); 	... 7 more; Caused by: java.net.SocketException: Connection reset; 	at java.net.SocketInputStream.read(SocketInputStream.java:210); 	at java.net.SocketInputStream.read(SocketInputStream.java:141); 	at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); 	at sun.security.ssl.InputRecord.readV3Record(InputRecord.java:593); 	at sun.security.ssl.InputRecord.read(InputRecord.java:532); 	at sun.security.ssl.SSLS,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4194#issuecomment-358498401:2009,secur,security,2009,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4194#issuecomment-358498401,1,['secur'],['security']
Security,.java:38); at org.broadinstitute.hellbender.engine.dataflow.coders.GATKReadCoder.encode(GATKReadCoder.java:27); at com.google.cloud.dataflow.sdk.transforms.join.UnionCoder.encode(UnionCoder.java:82); at com.google.cloud.dataflow.sdk.transforms.join.UnionCoder.encode(UnionCoder.java:37); at com.google.cloud.dataflow.sdk.util.WindowedValue$FullWindowedValueCoder.encode(WindowedValue.java:528); at com.google.cloud.dataflow.sdk.util.WindowedValue$FullWindowedValueCoder.encode(WindowedValue.java:472); at com.google.cloud.dataflow.sdk.coders.IterableLikeCoder.encode(IterableLikeCoder.java:96); at com.google.cloud.dataflow.sdk.coders.IterableLikeCoder.encode(IterableLikeCoder.java:42); at com.google.cloud.dataflow.sdk.coders.KvCoder.encode(KvCoder.java:92); at com.google.cloud.dataflow.sdk.coders.KvCoder.encode(KvCoder.java:42); at com.google.cloud.dataflow.sdk.util.CoderUtils.encodeToByteArray(CoderUtils.java:86); at com.google.cloud.dataflow.sdk.util.CoderUtils.encodeToByteArray(CoderUtils.java:73); at com.google.cloud.dataflow.sdk.util.SerializableUtils.ensureSerializableByCoder(SerializableUtils.java:135); at com.google.cloud.dataflow.sdk.runners.DirectPipelineRunner$Evaluator.ensureSerializableByCoder(DirectPipelineRunner.java:887); at com.google.cloud.dataflow.sdk.runners.DirectPipelineRunner$Evaluator.ensureElementEncodable(DirectPipelineRunner.java:852); at com.google.cloud.dataflow.sdk.runners.DirectPipelineRunner$Evaluator.ensurePCollectionEncodable(DirectPipelineRunner.java:844); at com.google.cloud.dataflow.sdk.runners.DirectPipelineRunner$Evaluator.setPCollectionValuesWithMetadata(DirectPipelineRunner.java:767); at com.google.cloud.dataflow.sdk.transforms.GroupByKey.evaluateHelper(GroupByKey.java:512); at com.google.cloud.dataflow.sdk.transforms.GroupByKey.access$000(GroupByKey.java:132); at com.google.cloud.dataflow.sdk.transforms.GroupByKey$1.evaluate(GroupByKey.java:460); at com.google.cloud.dataflow.sdk.transforms.GroupByKey$1.evaluate(GroupByKey.java:455),MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/668#issuecomment-122949556:3084,access,access,3084,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/668#issuecomment-122949556,1,['access'],['access']
Security,.next; 0.3% 48 + 0 htsjdk.samtools.util.BlockCompressedInputStream.read; 0.3% 47 + 0 org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSparkUtils.lambda$markPairedEnds$66146993$1; 0.3% 45 + 0 scala.collection.Iterator$$anon$13.next; 0.3% 1 + 42 org.apache.spark.util.collection.PartitionedSerializedPairBuffer.insert; 0.3% 42 + 0 org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink$$Lambda$113.call; 0.3% 42 + 0 java.util.stream.AbstractPipeline.evaluate; 0.2% 38 + 0 java.util.HashMap.putVal; 0.2% 37 + 0 htsjdk.samtools.BinaryCigarCodec.decode; 33.6% 5071 + 112 Total compiled (including elided). Stub + native Method ; 28.5% 0 + 4400 java.lang.System.identityHashCode; 17.0% 0 + 2627 java.lang.String.intern; 5.8% 0 + 891 java.util.zip.Inflater.inflateBytes; 4.0% 0 + 615 java.io.FileOutputStream.writeBytes; 3.9% 0 + 602 htsjdk.samtools.util.zip.IntelDeflater.deflateBytes; 1.3% 0 + 200 java.io.FileOutputStream.open0; 0.8% 0 + 119 sun.nio.ch.NativeThread.current; 0.7% 0 + 103 java.util.zip.Inflater.reset; 0.6% 0 + 92 org.apache.hadoop.util.NativeCrc32.nativeComputeChunkedSumsByteArray; 0.6% 0 + 91 htsjdk.samtools.util.zip.IntelDeflater.reset; 0.4% 0 + 56 java.io.FileInputStream.readBytes; 0.3% 0 + 41 sun.nio.ch.FileDispatcherImpl.read0; 0.2% 0 + 33 java.lang.Class.isPrimitive; 0.2% 0 + 27 java.lang.Throwable.fillInStackTrace; 0.1% 1 + 19 java.io.UnixFileSystem.getLength; 0.1% 0 + 17 sun.nio.ch.FileDispatcherImpl.size0; 0.1% 0 + 14 org.apache.hadoop.util.NativeCrc32.nativeComputeChunkedSums; 0.1% 13 + 0 java.lang.ClassLoader.defineClass1; 0.1% 0 + 11 java.lang.Class.isArray; 0.1% 0 + 9 java.io.FileInputStream.available; 0.1% 0 + 8 java.lang.Object.hashCode; 0.0% 0 + 7 java.lang.System.arraycopy; 0.0% 0 + 7 sun.nio.ch.EPollArrayWrapper.epollWait; 0.0% 0 + 6 java.lang.Class.forName0; 0.0% 0 + 5 java.util.zip.ZipFile.getEntry; 65.3% 14 + 10056 Total stub (including elided). Thread-local ticks:; 60.4% 23502 Blocked (of total); ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1702#issuecomment-210127581:10165,hash,hashCode,10165,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1702#issuecomment-210127581,1,['hash'],['hashCode']
Security,.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:109); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:131); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:104); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: java.net.SocketTimeoutException: Read timed out; 	at java.net.SocketInputStream.socketRead0(Native Method); 	at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); 	at java.net.SocketInputStream.read(SocketInputStream.java:171); 	at java.net.SocketInputStream.read(SocketInputStream.java:141); 	at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); 	at sun.security.ssl.InputRecord.read(InputRecord.java:503); 	at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973); 	at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:930); 	at sun.security.ssl.AppInputStream.read(AppInputStream.java:105); 	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286); 	at java.io.BufferedInputStream.read(BufferedInputStream.java:345); 	at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:704); 	at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:647); 	at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1569); 	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1474); 	at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480); 	at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:338); 	at shaded.cloud_nio.com.google.a,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300298180:6585,secur,security,6585,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300298180,1,['secur'],['security']
Security,".samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:36); 	at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:16); 	at htsjdk.samtools.util.AbstractAsyncWriter$WriterRunnable.run(AbstractAsyncWriter.java:124); 	at java.lang.Thread.run(Thread.java:748); Caused by: htsjdk.samtools.SAMException: Exception creating BAM index for record i 1/2 76b aligned read.; 	at htsjdk.samtools.BAMIndexer.processAlignment(BAMIndexer.java:119); 	at htsjdk.samtools.BAMFileWriter.writeAlignment(BAMFileWriter.java:139); 	... 5 more; Caused by: htsjdk.samtools.SAMException: IOException in BinaryBAMIndexWriter reference 0; 	at htsjdk.samtools.BinaryBAMIndexWriter.writeReference(BinaryBAMIndexWriter.java:151); 	at htsjdk.samtools.BAMIndexer.advanceToReference(BAMIndexer.java:138); 	at htsjdk.samtools.BAMIndexer.processAlignment(BAMIndexer.java:115); 	... 6 more; Caused by: java.nio.channels.ClosedByInterruptException; 	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202); 	at sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:216); 	at java.nio.channels.Channels.writeFullyImpl(Channels.java:78); 	at java.nio.channels.Channels.writeFully(Channels.java:101); 	at java.nio.channels.Channels.access$000(Channels.java:61); 	at java.nio.channels.Channels$1.write(Channels.java:174); 	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82); 	at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140); 	at htsjdk.samtools.BinaryBAMIndexWriter.writeReference(BinaryBAMIndexWriter.java:149); 	... 8 more; ```. One of the logs is here: https://storage.googleapis.com/hellbender-test-logs/build_reports/12617.7/tests/test/index.html. We saw a whole host of those cigar validation errors you're seeing when we updated htsdjk the last time, it's a new validation that wasn't previously checked in htsjdk, so a lot of the test files had errors in them that no one had ever noticed/bothered fixing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2558#issuecomment-329967977:1648,access,access,1648,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2558#issuecomment-329967977,3,"['access', 'validat']","['access', 'validation']"
Security,"/; > CreateBinningIntervals.java; > <https://github.com/broadinstitute/gatk/pull/3597#discussion_r140646119>:; >; > > + createBins();; > + }; > +; > + /**; > + * Generates binning coverage in the intervals given by the user.; > + * The width of bins, the intervals and the output file's path are given by the user.; > + */; > + public void createBins() {; > + // check if the output directory exists; > + if (!outputFile.exists() && !outputFile.mkdir()) {; > + throw new RuntimeException(""Unable to create file: "" + outputFile.getAbsolutePath());; > + }; > +; > + // check if the bin widths are set appropriately; > + if(widthOfBins <= 0) {; > + throw new IllegalArgumentException(""Width of bins "" + Integer.toString(widthOfBins) + "" should be >= 0."");; >; > @asmirnov <https://github.com/asmirnov> and @samuelklee; > <https://github.com/samuelklee> are both correct, but for the future in; > cases where you *would* want an IllegalArgumentException you should use; > Utils.validateArg to render this sort of thing a one-liner.; > ------------------------------; >; > In src/main/java/org/broadinstitute/hellbender/tools/copynumber/; > CreateBinningIntervals.java; > <https://github.com/broadinstitute/gatk/pull/3597#discussion_r140646132>:; >; > > + doc = ""width of the bins"",; > + fullName = WIDTH_OF_BINS_LONG_NAME,; > + shortName = WIDTH_OF_BINS_SHORT_NAME,; > + optional = true,; > + minValue = 1; > + ); > + private int widthOfBins = 1;; > +; > + @Argument(; > + doc = ""width of the padding regions"",; > + fullName = PADDING_LONG_NAME,; > + shortName = PADDING_SHORT_NAME,; > + optional = true,; > + minValue = 0; > + ); > + private int padding = 0;; >; > . . . and if this padding is different from the inherited padding then; > this demands a comment to avoid confusion.; > ------------------------------; >; > In src/main/java/org/broadinstitute/hellbender/tools/copynumber/; > CreateBinningIntervals.java; > <https://github.com/broadinstitute/gatk/pull/3597#discussion_r140646146>:; >; > > +",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3597#issuecomment-331744211:4257,validat,validateArg,4257,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3597#issuecomment-331744211,1,['validat'],['validateArg']
Security,"/FindBreakpointEvidenceSpark.java; - input coverage-scaled thresholds, convert to absolute internally. Allow thresholds to be double instead of int; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilter.java; - getter functions added to calculate properties for XGBoostEvidenceFilter. Also fromStringRep() and helper constructors added for testing; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointEvidence.java; - updates to tests reflecting changes to these interfaces; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilterTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/FindBreakpointEvidenceSparkUnitTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/integration/FindBreakpointEvidenceSparkIntegrationTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/integration/SVIntegrationTestDataProvider.java. 4. Added code; - factory to call appropriate BreakpointEvidence filter; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointFilterFactory.java; - simple helper class to hold feature vectors for classifier; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/EvidenceFeatures.java; - implement classifier-based BreakpointEvidence filter; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/XGBoostEvidenceFilter.java; - unit test for classifier filter; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/XGBoostEvidenceFilterUnitTest.java. 5. Added resources; - Genome tracts; src/main/resources/large/hg38_centromeres.txt.gz; src/main/resources/large/hg38_gaps.txt.gz; src/main/resources/large/hg38_umap_s100.txt.gz; - Classifier binary file; src/main/resources/large/sv_evidence_classifier.bin; - Data used for validation of performance in unit tests; src/test/resources/sv_classifier_test_data.json; src/test/resources/sv_features_test_data.json",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477:3079,validat,validation,3079,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477,1,['validat'],['validation']
Security,"/branches/hellbender/build/install/gatk/bin/gatk LeftAlignAndTrimVariants -R /Users/shlee/Documents/ref/hg38/Homo_sapiens_assembly38.fasta -V /Users/shlee/Downloads/zeta_snippet_shlee/zeta_snippet.vcf.gz --maxIndelSize 250 -O zeta_snippet_leftalign_maxindelsize250.vcf.gz; 17:24:16.345 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/shlee/Documents/branches/hellbender/build/install/gatk/lib/gkl-0.8.5.jar!/com/intel/gkl/native/libgkl_compression.dylib; Sep 05, 2018 5:24:16 PM shaded.cloud_nio.com.google.auth.oauth2.DefaultCredentialsProvider warnAboutProblematicCredentials; WARNING: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a ""quota exceeded"" or ""API not enabled"" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/.; 17:24:16.502 INFO LeftAlignAndTrimVariants - ------------------------------------------------------------; 17:24:16.502 INFO LeftAlignAndTrimVariants - The Genome Analysis Toolkit (GATK) v4.0.8.1-24-gb43bc27-SNAPSHOT; 17:24:16.502 INFO LeftAlignAndTrimVariants - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:24:16.502 INFO LeftAlignAndTrimVariants - Executing as shlee@WMCF9-CB5 on Mac OS X v10.13.6 x86_64; 17:24:16.502 INFO LeftAlignAndTrimVariants - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_111-b14; 17:24:16.503 INFO LeftAlignAndTrimVariants - Start Date/Time: September 5, 2018 5:24:16 PM EDT; 17:24:16.503 INFO LeftAlignAndTrimVariants - ------------------------------------------------------------; 17:24:16.503 INFO LeftAlignAndTrimVariants - ------------------------------------------------------------; 17:24:16.503 INFO LeftAlignAndTrimVariants - HTSJDK Version: 2.16.0; 17:24:16.503 INFO LeftAlignAndTrimVariants - Picard V",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3487#issuecomment-418887543:1394,authenticat,authentication,1394,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3487#issuecomment-418887543,1,['authenticat'],['authentication']
Security,"0:00.685 INFO PostprocessGermlineCNVCalls - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:00:00.685 INFO PostprocessGermlineCNVCalls - Deflater: IntelDeflater; 17:00:00.685 INFO PostprocessGermlineCNVCalls - Inflater: IntelInflater; 17:00:00.685 INFO PostprocessGermlineCNVCalls - GCS max retries/reopens: 20; 17:00:00.685 INFO PostprocessGermlineCNVCalls - Requester pays: disabled; 17:00:00.685 INFO PostprocessGermlineCNVCalls - Initializing engine; 17:00:04.480 INFO PostprocessGermlineCNVCalls - Done initializing engine; 17:00:07.582 INFO PostprocessGermlineCNVCalls - Shutting down engine; [October 29, 2020 5:00:07 PM MSK] org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls done. Elapsed time: 0.12 minutes.; Runtime.totalMemory()=2468347904; java.lang.IllegalArgumentException: Records were not strictly sorted in dictionary order.; 	at org.broadinstitute.hellbender.tools.copynumber.arguments.CopyNumberArgumentValidationUtils.validateIntervals(CopyNumberArgumentValidationUtils.java:60); 	at org.broadinstitute.hellbender.tools.copynumber.formats.collections.AbstractLocatableCollection.getShardedCollectionSortOrder(AbstractLocatableCollection.java:142); 	at org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls.onTraversalStart(PostprocessGermlineCNVCalls.java:297); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1047); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); Using GATK jar /home/lmbs02/bio/biosoft/gatk/gatk-4.1.9.0/gat",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6924#issuecomment-718787427:2939,validat,validateIntervals,2939,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6924#issuecomment-718787427,1,['validat'],['validateIntervals']
Security,0> (ø)` | :arrow_down: |; | [...tools/funcotator/dataSources/TableFuncotation.java](https://codecov.io/gh/broadinstitute/gatk/pull/4276/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9mdW5jb3RhdG9yL2RhdGFTb3VyY2VzL1RhYmxlRnVuY290YXRpb24uamF2YQ==) | `60% <100%> (ø)` | `20 <0> (ø)` | :arrow_down: |; | [.../tools/copynumber/utils/MergeAnnotatedRegions.java](https://codecov.io/gh/broadinstitute/gatk/pull/4276/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL3V0aWxzL01lcmdlQW5ub3RhdGVkUmVnaW9ucy5qYXZh) | `100% <100%> (ø)` | `3 <3> (?)` | |; | [...ils/annotatedinterval/AnnotatedIntervalHeader.java](https://codecov.io/gh/broadinstitute/gatk/pull/4276/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL3V0aWxzL2Fubm90YXRlZGludGVydmFsL0Fubm90YXRlZEludGVydmFsSGVhZGVyLmphdmE=) | `100% <100%> (ø)` | `6 <6> (?)` | |; | [...tmutpileup/ValidateBasicSomaticShortMutations.java](https://codecov.io/gh/broadinstitute/gatk/pull/4276/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vYmFzaWNzaG9ydG11dHBpbGV1cC9WYWxpZGF0ZUJhc2ljU29tYXRpY1Nob3J0TXV0YXRpb25zLmphdmE=) | `85.965% <100%> (ø)` | `7 <0> (ø)` | :arrow_down: |; | [...nder/tools/copynumber/utils/TagGermlineEvents.java](https://codecov.io/gh/broadinstitute/gatk/pull/4276/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL3V0aWxzL1RhZ0dlcm1saW5lRXZlbnRzLmphdmE=) | `100% <100%> (ø)` | `3 <3> (?)` | |; | [...ataSources/xsv/LocatableXsvFuncotationFactory.java](https://codecov.io/gh/broadinstitute/gatk/pull/4276/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9mdW5jb3RhdG9yL2RhdGFTb3VyY2VzL3hzdi9Mb2NhdGFibGVYc3ZGdW5jb3RhdGlvbkZhY3RvcnkuamF2YQ==) | `85.185% <61.538%> (+0.491%)` | `24 <0> (-4)` | :arrow_down: |; | [...g/broadi,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4276#issuecomment-361355746:2779,Validat,ValidateBasicSomaticShortMutations,2779,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4276#issuecomment-361355746,1,['Validat'],['ValidateBasicSomaticShortMutations']
Security,2); 	at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:152); 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1819); 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:692); 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:381); 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java); 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447); 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793); 	at java.security.AccessController.doPrivileged(Native Method); 	at javax.security.auth.Subject.doAs(Subject.java:422); 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1840); 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489). 	at org.apache.hadoop.ipc.Client.call(Client.java:1475); 	at org.apache.hadoop.ipc.Client.call(Client.java:1412); 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229); 	at com.sun.proxy.$Proxy10.getBlockLocations(Unknown Source); 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getBlockLocations(ClientNamenodeProtocolTranslatorPB.java:255); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191); 	at org.ap,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294:9751,secur,security,9751,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294,1,['secur'],['security']
Security,2); 	at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:152); 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1819); 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:692); 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:381); 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java); 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447); 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793); 	at java.security.AccessController.doPrivileged(Native Method); 	at javax.security.auth.Subject.doAs(Subject.java:422); 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1840); 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489). 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:237); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReads(GATKSparkTool.java:488); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:468); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:458); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLinePro,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294:3593,secur,security,3593,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294,1,['secur'],['security']
Security,2); 	at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:152); 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1819); 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:692); 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:381); 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java); 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447); 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793); 	at java.security.AccessController.doPrivileged(Native Method); 	at javax.security.auth.Subject.doAs(Subject.java:422); 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1840); 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489). 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423); 	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106); 	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73); 	at org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1228); 	at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1213); 	at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1201); 	at org.apache.hadoop.hdfs.DFSInputStream.fe,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294:6510,secur,security,6510,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294,1,['secur'],['security']
Security,2); 	at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:152); 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1819); 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:692); 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:381); 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java); 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447); 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793); 	at java.security.AccessController.doPrivileged(Native Method); 	at javax.security.auth.Subject.doAs(Subject.java:422); 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1840); 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489). ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException: Failed to read bam header from /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; Caused by:File does not exist: /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:72); 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:62); 	at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:152); 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1819); 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:692); 	at org.apa,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294:1930,secur,security,1930,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294,1,['secur'],['security']
Security,2.936 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 15:47:53.565 INFO ProgressMeter - ENA|LVXK01000001|LVXK01000001.1:19555 0.2 90 508.0; 15:48:05.962 INFO ProgressMeter - ENA|LVXK01000001|LVXK01000001.1:136820 0.4 600 1563.5; 15:48:16.023 INFO ProgressMeter - ENA|LVXK01000001|LVXK01000001.1:360783 0.6 1560 2828.9; 15:48:19.342 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 0.010346494000000001; 15:48:19.342 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 6.453042841; 15:48:19.347 INFO SmithWatermanAligner - Total compute time in java Smith-Waterman : 10.39 sec; 15:48:19.348 INFO Mutect2 - Shutting down engine; [28 novembre 2019 15:48:19 CET] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.72 minutes.; Runtime.totalMemory()=3822583808; java.lang.IllegalArgumentException: Cannot construct fragment from more than two reads; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:725); 	at org.broadinstitute.hellbender.utils.read.Fragment.create(Fragment.java:36); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.utils.genotyper.AlleleLikelihoods.groupEvidence(AlleleLikelihoods.java:595); 	at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticGenotypingEngine.callMutations(SomaticGenotypingEngine.java:93); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2Engine.callRegion(Mutect2Engine.java:251); 	at or,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6271#issuecomment-559553558:3831,validat,validateArg,3831,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6271#issuecomment-559553558,1,['validat'],['validateArg']
Security,2@cb2-VirtualBox:~/gatk$ ./gradlew bundle --stacktrace; > Task :gatkDoc FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':gatkDoc'.; > Javadoc generation failed. Generated Javadoc options file (useful for troubleshooting): '/home/cb2/gatk/build/tmp/gatkDoc/javadoc.options'. * Try:; Run with --info or --debug option to get more log output. Run with --scan to get full insights. * Exception is:; org.gradle.api.tasks.TaskExecutionException: Execution failed for task ':gatkDoc'.; at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter$3.accept(ExecuteActionsTaskExecuter.java:166); at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter$3.accept(ExecuteActionsTaskExecuter.java:163); at org.gradle.internal.Try$Failure.ifSuccessfulOrElse(Try.java:191); at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(ExecuteActionsTaskExecuter.java:156); at org.gradle.api.internal.tasks.execution.ValidatingTaskExecuter.execute(ValidatingTaskExecuter.java:62); at org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecuter.execute(SkipEmptySourceFilesTaskExecuter.java:108); at org.gradle.api.internal.tasks.execution.ResolveBeforeExecutionOutputsTaskExecuter.execute(ResolveBeforeExecutionOutputsTaskExecuter.java:67); at org.gradle.api.internal.tasks.execution.ResolveAfterPreviousExecutionStateTaskExecuter.execute(ResolveAfterPreviousExecutionStateTaskExecuter.java:46); at org.gradle.api.internal.tasks.execution.CleanupStaleOutputsExecuter.execute(CleanupStaleOutputsExecuter.java:94); at org.gradle.api.internal.tasks.execution.FinalizePropertiesTaskExecuter.execute(FinalizePropertiesTaskExecuter.java:46); at org.gradle.api.internal.tasks.execution.ResolveTaskExecutionModeExecuter.execute(ResolveTaskExecutionModeExecuter.java:95); at org.gradle.api.internal.tasks.execution.SkipTaskWithNoActionsExecuter.execute(SkipTaskWithNoActionsExecuter.java:57); at org.gradle.api.internal.tasks,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4155#issuecomment-566796716:2077,Validat,ValidatingTaskExecuter,2077,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4155#issuecomment-566796716,1,['Validat'],['ValidatingTaskExecuter']
Security,2xvdWQuamF2YQ==) | `70.811% <0%> (ø)` | `40 <0> (ø)` | :arrow_down: |; | [...rg/broadinstitute/hellbender/utils/IndexUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/3989/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9JbmRleFV0aWxzLmphdmE=) | `80.702% <100%> (ø)` | `16 <2> (ø)` | :arrow_down: |; | [...kers/variantutils/UpdateVCFSequenceDictionary.java](https://codecov.io/gh/broadinstitute/gatk/pull/3989/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhcmlhbnR1dGlscy9VcGRhdGVWQ0ZTZXF1ZW5jZURpY3Rpb25hcnkuamF2YQ==) | `86.957% <100%> (ø)` | `14 <0> (ø)` | :arrow_down: |; | [...itute/hellbender/tools/walkers/SplitIntervals.java](https://codecov.io/gh/broadinstitute/gatk/pull/3989/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL1NwbGl0SW50ZXJ2YWxzLmphdmE=) | `88.235% <100%> (ø)` | `6 <2> (ø)` | :arrow_down: |; | [...der/tools/walkers/variantutils/SelectVariants.java](https://codecov.io/gh/broadinstitute/gatk/pull/3989/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhcmlhbnR1dGlscy9TZWxlY3RWYXJpYW50cy5qYXZh) | `80.663% <100%> (ø)` | `125 <0> (ø)` | :arrow_down: |; | [...broadinstitute/hellbender/engine/FeatureInput.java](https://codecov.io/gh/broadinstitute/gatk/pull/3989/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvRmVhdHVyZUlucHV0LmphdmE=) | `94.203% <100%> (ø)` | `16 <0> (ø)` | :arrow_down: |; | [...r/tools/walkers/variantutils/ValidateVariants.java](https://codecov.io/gh/broadinstitute/gatk/pull/3989/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhcmlhbnR1dGlscy9WYWxpZGF0ZVZhcmlhbnRzLmphdmE=) | `82.857% <66.667%> (ø)` | `34 <0> (ø)` | :arrow_down: |; | ... and [3 more](https://codecov.io/gh/broadinstitute/gatk/pull/3989/diff?src=pr&el=tree-more) | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3989#issuecomment-352845785:3650,Validat,ValidateVariants,3650,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3989#issuecomment-352845785,1,['Validat'],['ValidateVariants']
Security,35% <ø> (ø)` | `6 <0> (ø)` | :arrow_down: |; | [...nder/tools/spark/BaseRecalibratorSparkSharded.java](https://codecov.io/gh/broadinstitute/gatk/pull/4070/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9CYXNlUmVjYWxpYnJhdG9yU3BhcmtTaGFyZGVkLmphdmE=) | `22.807% <ø> (ø)` | `2 <0> (ø)` | :arrow_down: |; | [...nder/tools/spark/pipelines/PrintVariantsSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4070/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvUHJpbnRWYXJpYW50c1NwYXJrLmphdmE=) | `66.667% <ø> (ø)` | `2 <0> (ø)` | :arrow_down: |; | [...k/pipelines/BwaAndMarkDuplicatesPipelineSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4070/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvQndhQW5kTWFya0R1cGxpY2F0ZXNQaXBlbGluZVNwYXJrLmphdmE=) | `78.947% <ø> (ø)` | `4 <0> (ø)` | :arrow_down: |; | [...er/tools/walkers/variantutils/VariantsToTable.java](https://codecov.io/gh/broadinstitute/gatk/pull/4070/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhcmlhbnR1dGlscy9WYXJpYW50c1RvVGFibGUuamF2YQ==) | `93.182% <ø> (ø)` | `75 <0> (ø)` | :arrow_down: |; | [...r/tools/walkers/variantutils/ValidateVariants.java](https://codecov.io/gh/broadinstitute/gatk/pull/4070/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhcmlhbnR1dGlscy9WYWxpZGF0ZVZhcmlhbnRzLmphdmE=) | `82.857% <ø> (ø)` | `34 <0> (ø)` | :arrow_down: |; | [...broadinstitute/hellbender/tools/GetSampleName.java](https://codecov.io/gh/broadinstitute/gatk/pull/4070/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9HZXRTYW1wbGVOYW1lLmphdmE=) | `62.5% <ø> (ø)` | `6 <0> (ø)` | :arrow_down: |; | ... and [36 more](https://codecov.io/gh/broadinstitute/gatk/pull/4070/diff?src=pr&el=tree-more) | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4070#issuecomment-355848388:3461,Validat,ValidateVariants,3461,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4070#issuecomment-355848388,1,['Validat'],['ValidateVariants']
Security,"3; 13:15:05.204 INFO FuncotatorDataSourceDownloader - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 13:15:05.204 INFO FuncotatorDataSourceDownloader - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 13:15:05.204 INFO FuncotatorDataSourceDownloader - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 13:15:05.204 INFO FuncotatorDataSourceDownloader - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 13:15:05.204 INFO FuncotatorDataSourceDownloader - Deflater: IntelDeflater; 13:15:05.204 INFO FuncotatorDataSourceDownloader - Inflater: IntelInflater; 13:15:05.204 INFO FuncotatorDataSourceDownloader - GCS max retries/reopens: 20; 13:15:05.204 INFO FuncotatorDataSourceDownloader - Requester pays: disabled; 13:15:05.204 INFO FuncotatorDataSourceDownloader - Initializing engine; 13:15:05.205 INFO FuncotatorDataSourceDownloader - Done initializing engine; 13:15:05.205 INFO FuncotatorDataSourceDownloader - Germline data sources selected.; 13:15:05.207 INFO FuncotatorDataSourceDownloader - Collecting expected checksum...; 13:19:33.264 INFO FuncotatorDataSourceDownloader - Shutting down engine; [November 18, 2023 1:19:33 PM CST] org.broadinstitute.hellbender.tools.funcotator.FuncotatorDataSourceDownloader done. Elapsed time: 4.48 minutes.; Runtime.totalMemory()=1967128576; code: 0; message: All 3 retries failed. Waited a total of 14000 ms between attempts; reason: null; location: null; retryable: false; com.google.cloud.storage.StorageException: All 3 retries failed. Waited a total of 14000 ms between attempts; 	at com.google.cloud.storage.contrib.nio.CloudStorageRetryHandler.handleRetryForStorageException(CloudStorageRetryHandler.java:135); 	at com.google.cloud.storage.contrib.nio.CloudStorageRetryHandler.handleStorageException(CloudStorageRetryHandler.java:115); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.fetchSize(CloudStorageReadChannel.java:253); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.<init>(CloudStorageReadChannel",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8275#issuecomment-1817434417:2600,checksum,checksum,2600,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8275#issuecomment-1817434417,1,['checksum'],['checksum']
Security,"47:51.534 INFO IntelPairHmm - Requested threads: 4; 11:47:51.534 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 11:47:51.557 INFO ProgressMeter - Starting traversal; 11:47:51.557 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 11:47:52.683 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 0.0; 11:47:52.683 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 0.0; 11:47:52.683 INFO SmithWatermanAligner - Total compute time in java Smith-Waterman : 0.24 sec; 11:47:52.684 INFO Mutect2 - Shutting down engine; [July 2, 2020 11:47:52 AM CEST] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.03 minutes.; Runtime.totalMemory()=2511863808; java.lang.IllegalArgumentException: Read bases and read quality arrays aren't the same size: Bases: 38 vs Base Q's: 38 vs Insert Q's: 146 vs Delete Q's: 146.; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:734); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.createQualityModifiedRead(PairHMMLikelihoodCalculationEngine.java:205); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.modifyReadQualities(PairHMMLikelihoodCalculationEngine.java:268); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngine.java:236); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngine.java:164); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2Engine.callRegion(Mutect2Engine.java:246); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2.apply(Mutect2.java:299); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:200); 	at ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-652912482:5242,validat,validateArg,5242,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-652912482,1,['validat'],['validateArg']
Security,5062/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vYmFzaWNzaG9ydG11dHBpbGV1cC9Qb3dlckNhbGN1bGF0aW9uVXRpbHMuamF2YQ==) | `96.667% <100%> (+1.429%)` | `18 <7> (+3)` | :arrow_up: |; | [...tmutpileup/BasicSomaticShortMutationValidator.java](https://codecov.io/gh/broadinstitute/gatk/pull/5062/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vYmFzaWNzaG9ydG11dHBpbGV1cC9CYXNpY1NvbWF0aWNTaG9ydE11dGF0aW9uVmFsaWRhdG9yLmphdmE=) | `62.5% <100%> (+1.974%)` | `5 <0> (ø)` | :arrow_down: |; | [...ion/basicshortmutpileup/BasicValidationResult.java](https://codecov.io/gh/broadinstitute/gatk/pull/5062/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vYmFzaWNzaG9ydG11dHBpbGV1cC9CYXNpY1ZhbGlkYXRpb25SZXN1bHQuamF2YQ==) | `96.774% <100%> (+0.222%)` | `16 <2> (+1)` | :arrow_up: |; | [...tmutpileup/ValidateBasicSomaticShortMutations.java](https://codecov.io/gh/broadinstitute/gatk/pull/5062/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vYmFzaWNzaG9ydG11dHBpbGV1cC9WYWxpZGF0ZUJhc2ljU29tYXRpY1Nob3J0TXV0YXRpb25zLmphdmE=) | `83.636% <100%> (+0.15%)` | `19 <0> (ø)` | :arrow_down: |; | [...dateBasicSomaticShortMutationsIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5062/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vYmFzaWNzaG9ydG11dHBpbGV1cC9WYWxpZGF0ZUJhc2ljU29tYXRpY1Nob3J0TXV0YXRpb25zSW50ZWdyYXRpb25UZXN0LmphdmE=) | `100% <100%> (ø)` | `5 <0> (ø)` | :arrow_down: |; | [...utils/smithwaterman/SmithWatermanIntelAligner.java](https://codecov.io/gh/broadinstitute/gatk/pull/5062/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9zbWl0aHdhdGVybWFuL1NtaXRoV2F0ZXJtYW5JbnRlbEFsaWduZXIuamF2YQ==) | `50% <0%> (,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5062#issuecomment-408490831:2311,Validat,ValidateBasicSomaticShortMutations,2311,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5062#issuecomment-408490831,1,['Validat'],['ValidateBasicSomaticShortMutations']
Security,"514.10.2.el7.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_111-b14; Version: Version:4.alpha.2-170-g8d06823-SNAPSHOT; 19:03:42.998 INFO ValidateSamFile - Defaults.BUFFER_SIZE : 131072; 19:03:42.999 INFO ValidateSamFile - Defaults.COMPRESSION_LEVEL : 1; 19:03:42.999 INFO ValidateSamFile - Defaults.CREATE_INDEX : false; 19:03:42.999 INFO ValidateSamFile - Defaults.CREATE_MD5 : false; 19:03:42.999 INFO ValidateSamFile - Defaults.CUSTOM_READER_FACTORY : ; 19:03:42.999 INFO ValidateSamFile - Defaults.EBI_REFERENCE_SERVICE_URL_MASK : http://www.ebi.ac.uk/ena/cram/md5/%s; 19:03:42.999 INFO ValidateSamFile - Defaults.NON_ZERO_BUFFER_SIZE : 131072; 19:03:42.999 INFO ValidateSamFile - Defaults.REFERENCE_FASTA : null; 19:03:43.000 INFO ValidateSamFile - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 19:03:43.000 INFO ValidateSamFile - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 19:03:43.000 INFO ValidateSamFile - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 19:03:43.000 INFO ValidateSamFile - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 19:03:43.000 INFO ValidateSamFile - Defaults.USE_CRAM_REF_DOWNLOAD : false; 19:03:43.000 INFO ValidateSamFile - Deflater JdkDeflater; 19:03:43.000 INFO ValidateSamFile - Inflater JdkInflater; 19:03:43.000 INFO ValidateSamFile - Initializing engine; 19:03:43.000 INFO ValidateSamFile - Done initializing engine; ERROR: Record 9762, Read name 20GAVAAXX100126:7:2:8126:115177, bin field of BAM record does not equal value computed based on alignment start and end, and length of sequence to which read is aligned; ERROR: Record 24466, Read name 20FUKAAXX100202:7:46:13035:77621, bin field of BAM record does not equal value computed based on alignment start and end, and length of sequence to which read is aligned; ERROR: Record 97940, Read name 20FUKAAXX100202:5:7:21464:86224, bin field of BAM record does not equal value computed based on alignment start and end, and length of sequence to which read is aligned; ERROR: Record 97955, Read na",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2423#issuecomment-285513571:1731,Validat,ValidateSamFile,1731,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2423#issuecomment-285513571,1,['Validat'],['ValidateSamFile']
Security,52%)` | |; | [...aplotypecaller/HaplotypeCallerIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5710/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9IYXBsb3R5cGVDYWxsZXJJbnRlZ3JhdGlvblRlc3QuamF2YQ==) | `57.339% <0%> (-30.767%)` | `89% <0%> (+4%)` | |; | [...utils/smithwaterman/SmithWatermanIntelAligner.java](https://codecov.io/gh/broadinstitute/gatk/pull/5710/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9zbWl0aHdhdGVybWFuL1NtaXRoV2F0ZXJtYW5JbnRlbEFsaWduZXIuamF2YQ==) | `50% <0%> (-30%)` | `1% <0%> (-2%)` | |; | [...utils/variant/GATKVariantContextUtilsUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5710/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy92YXJpYW50L0dBVEtWYXJpYW50Q29udGV4dFV0aWxzVW5pdFRlc3QuamF2YQ==) | `61.598% <0%> (-24.25%)` | `160% <0%> (ø)` | |; | [...walkers/validation/ConcordanceIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5710/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vQ29uY29yZGFuY2VJbnRlZ3JhdGlvblRlc3QuamF2YQ==) | `79.444% <0%> (-20.556%)` | `3% <0%> (-3%)` | |; | [...kers/vqsr/VariantGaussianMixtureModelUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5710/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3IvVmFyaWFudEdhdXNzaWFuTWl4dHVyZU1vZGVsVW5pdFRlc3QuamF2YQ==) | `62.857% <0%> (-20.162%)` | `13% <0%> (ø)` | |; | [.../walkers/vqsr/TruthSensitivityTrancheUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5710/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3IvVHJ1dGhTZW5zaXRpdml0eVRyYW5jaGVVbml0VGVzdC5qYXZh) | `66.667% <0%> (-19.048%)` | `12% <0%> (ø)` | |; | [...stitute/hellbender/utils/nio/PathLineIterator.java](h,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5710#issuecomment-466524542:2858,validat,validation,2858,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5710#issuecomment-466524542,1,['validat'],['validation']
Security,6e7635897e1a6a773af5684511e2358d369af94?src=pr&el=desc) will **increase** coverage by `0.002%`.; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #4310 +/- ##; ===============================================; + Coverage 79.065% 79.067% +0.002% ; - Complexity 16582 16583 +1 ; ===============================================; Files 1048 1048 ; Lines 59504 59504 ; Branches 9717 9717 ; ===============================================; + Hits 47047 47048 +1 ; Misses 8682 8682 ; + Partials 3775 3774 -1; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/4310?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...bender/tools/walkers/mutect/FilterMutectCalls.java](https://codecov.io/gh/broadinstitute/gatk/pull/4310/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL211dGVjdC9GaWx0ZXJNdXRlY3RDYWxscy5qYXZh) | `95.833% <ø> (ø)` | `7 <0> (ø)` | :arrow_down: |; | [...llbender/tools/walkers/validation/Concordance.java](https://codecov.io/gh/broadinstitute/gatk/pull/4310/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vQ29uY29yZGFuY2UuamF2YQ==) | `88.542% <ø> (ø)` | `28 <0> (ø)` | :arrow_down: |; | [...itute/hellbender/tools/walkers/mutect/Mutect2.java](https://codecov.io/gh/broadinstitute/gatk/pull/4310/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL211dGVjdC9NdXRlY3QyLmphdmE=) | `92% <ø> (ø)` | `15 <0> (ø)` | :arrow_down: |; | [...ls/walkers/mutect/CreateSomaticPanelOfNormals.java](https://codecov.io/gh/broadinstitute/gatk/pull/4310/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL211dGVjdC9DcmVhdGVTb21hdGljUGFuZWxPZk5vcm1hbHMuamF2YQ==) | `87.273% <ø> (ø)` | `8 <0> (ø)` | :arrow_down: |; | [...ellbender/tools/exome/FilterByOrientationBias.java](https://codecov.io/gh/broadinstitute/gatk/pull/4310/di,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4310#issuecomment-361770954:1236,validat,validation,1236,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4310#issuecomment-361770954,1,['validat'],['validation']
Security,"78 INFO HaplotypeCaller - GCS max retries/reopens: 20; 01:13:16.078 INFO HaplotypeCaller - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 01:13:16.078 INFO HaplotypeCaller - Initializing engine; 01:13:17.087 INFO HaplotypeCaller - Shutting down engine; [January 18, 2020 1:13:17 AM IST] org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller done. Elapsed time: 0.02 minutes.; Runtime.totalMemory()=2216689664; java.lang.NullPointerException; 	at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.getContigNames(SequenceDictionaryUtils.java:463); 	at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.getCommonContigsByName(SequenceDictionaryUtils.java:457); 	at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.compareDictionaries(SequenceDictionaryUtils.java:234); 	at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.validateDictionaries(SequenceDictionaryUtils.java:150); 	at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.validateDictionaries(SequenceDictionaryUtils.java:98); 	at org.broadinstitute.hellbender.engine.GATKTool.validateSequenceDictionaries(GATKTool.java:621); 	at org.broadinstitute.hellbender.engine.GATKTool.onStartup(GATKTool.java:563); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.onStartup(AssemblyRegionWalker.java:160); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:152); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:195); 	at org.broadinstitute.hellbender.Main.main(Main.java:275). Please suggest any solution.; Thank you.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5947#issuecomment-575601220:2673,validat,validateDictionaries,2673,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5947#issuecomment-575601220,3,['validat'],"['validateDictionaries', 'validateSequenceDictionaries']"
Security,"8,26:0.261:13:10:2100,793:37:29; ```. **output GatherVcfs to .vcf.gz allows for duplicate records**; ```; WMCF9-CB5:precomputed_results shlee$ java -jar $PICARD GatherVcfs I=split3_8.vcf.gz I=split2_8.vcf.gz O=../test_gathervcf_split8_overlap.vcf.gz; [Wed Jun 07 14:51:32 EDT 2017] picard.vcf.GatherVcfs INPUT=[split3_8.vcf.gz, split2_8.vcf.gz] OUTPUT=../test_gathervcf_split8_overlap.vcf.gz VERBOSITY=INFO QUIET=false VALIDATION_STRINGENCY=STRICT COMPRESSION_LEVEL=5 MAX_RECORDS_IN_RAM=500000 CREATE_INDEX=true CREATE_MD5_FILE=false GA4GH_CLIENT_SECRETS=client_secrets.json; [Wed Jun 07 14:51:32 EDT 2017] Executing as shlee@WMCF9-CB5 on Mac OS X 10.11.6 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_111-b14; Picard version: 2.9.2-SNAPSHOT; INFO	2017-06-07 14:51:32	GatherVcfs	Checking inputs.; INFO	2017-06-07 14:51:32	GatherVcfs	Checking file headers and first records to ensure compatibility.; INFO	2017-06-07 14:51:32	GatherVcfs	Gathering by copying gzip blocks. Will not be able to validate position non-overlap of files.; WARNING	2017-06-07 14:51:32	GatherVcfs	Index creation not currently supported when gathering block compressed VCFs.; INFO	2017-06-07 14:51:32	GatherVcfs	Gathering /Users/shlee/Documents/workshop_materials/mutect2_tutorial/mutect2_handson/precomputed_results/split3_8.vcf.gz; INFO	2017-06-07 14:51:32	GatherVcfs	Gathering /Users/shlee/Documents/workshop_materials/mutect2_tutorial/mutect2_handson/precomputed_results/split2_8.vcf.gz; [Wed Jun 07 14:51:32 EDT 2017] picard.vcf.GatherVcfs done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=257425408; WMCF9-CB5:precomputed_results shlee$ gzcat ../test_gathervcf_split8_overlap.vcf.gz | grep -v '##' ; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	NORMAL	TUMOR; chr6	33414233	.	GT	G	.	PASS	ECNT=1;HCNT=1;MAX_ED=.;MIN_ED=.;NLOD=28.24;RPA=5,4;RU=T;STR;TLOD=154.53	GT:AD:AF:ALT_F1R2:ALT_F2R1:QSS:REF_F1R2:REF_F2R1	0/0:112,0:0.00:0:0:3730,0:62:50	0/1:66,70:0.534:25:41:2209,2350:26:40; chr6	33442919	.	A	C	.	alt_allele_i",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3061#issuecomment-306889518:2414,validat,validate,2414,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3061#issuecomment-306889518,1,['validat'],['validate']
Security,"80k is what I had easy access to and what I'm the most invested in; benchmarking right now. Master does fine with the same params. It's slow,; but no failures. We decided to split into 1000 shards (Eric is convinced; that there's a substantial startup cost per shard so we do better in total; cpu-hours on fewer shards) and each of those takes about 24 hours. On Thu, May 10, 2018, 11:09 AM Louis Bergelson <notifications@github.com>; wrote:. > @ldgauthier <https://github.com/ldgauthier> You're running 80k? Does that; > run using the current master version of GATK? I assumed you were rerunning; > a 20k shard with the same settings we had used for the 20k.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388082988>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AGRhdLdlQoWlC8kjRvJJermDYEjltVUFks5txFgigaJpZM4TOtSm>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388183296:23,access,access,23,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388183296,1,['access'],['access']
Security,"90 4.1 67150 16425.7; 09:53:22.294 INFO ProgressMeter - 1:20576686 4.3 71380 16776.4; 09:53:32.681 INFO ProgressMeter - 1:21106727 4.4 73230 16538.3; 09:53:44.258 INFO ProgressMeter - 1:21270052 4.6 73820 15975.4; 09:53:54.757 INFO ProgressMeter - 1:21754504 4.8 75500 15742.8; 09:54:04.928 INFO ProgressMeter - 1:23419224 5.0 81370 16387.6; 09:54:15.956 INFO ProgressMeter - 1:23812728 5.1 82750 16070.6; 09:54:31.008 INFO ProgressMeter - 1:24023237 5.4 83470 15457.4; 09:54:33.610 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 58.921665822; 09:54:33.611 INFO SmithWatermanAligner - Total compute time in java Smith-Waterman : 72.92 sec; 09:54:33.612 INFO Mutect2 - Shutting down engine; [March 7, 2019 9:54:33 AM EST] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 5.51 minutes.; Runtime.totalMemory()=193003520; java.lang.IllegalArgumentException: readMaxLength must be > 0 but got 0; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:730); 	at org.broadinstitute.hellbender.utils.pairhmm.PairHMM.initialize(PairHMM.java:152); 	at org.broadinstitute.hellbender.utils.pairhmm.N2MemoryPairHMM.initialize(N2MemoryPairHMM.java:28); 	at org.broadinstitute.hellbender.utils.pairhmm.LoglessPairHMM.initialize(LoglessPairHMM.java:7); 	at org.broadinstitute.hellbender.utils.pairhmm.PairHMM.initialize(PairHMM.java:177); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.initializePairHMM(PairHMMLikelihoodCalculationEngine.java:242); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngine.java:177); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2Engine.callRegion(Mutect2Engine.java:229); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2.apply(Mutect2.java:232); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(Assemb",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5543#issuecomment-470579844:4754,validat,validateArg,4754,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5543#issuecomment-470579844,1,['validat'],['validateArg']
Security,"9155eff07ce88449a361f5d from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 18:11:33.871 INFO PrintReadsSpark - Initializing engine; 18:11:33.871 INFO PrintReadsSpark - Done initializing engine; 17/10/13 18:11:33 INFO spark.SparkContext: Running Spark version 2.2.0.cloudera1; 17/10/13 18:11:34 WARN spark.SparkConf: spark.master yarn-client is deprecated in Spark 2.0+, please instead use ""yarn"" with specified deploy mode.; 17/10/13 18:11:34 INFO spark.SparkContext: Submitted application: PrintReadsSpark; 17/10/13 18:11:34 INFO spark.SecurityManager: Changing view acls to: hdfs; 17/10/13 18:11:34 INFO spark.SecurityManager: Changing modify acls to: hdfs; 17/10/13 18:11:34 INFO spark.SecurityManager: Changing view acls groups to: ; 17/10/13 18:11:34 INFO spark.SecurityManager: Changing modify acls groups to: ; 17/10/13 18:11:34 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hdfs); groups with view permissions: Set(); users with modify permissions: Set(hdfs); groups with modify permissions: Set(); 17/10/13 18:11:34 INFO util.Utils: Successfully started service 'sparkDriver' on port 45754.; 17/10/13 18:11:34 INFO spark.SparkEnv: Registering MapOutputTracker; 17/10/13 18:11:34 INFO spark.SparkEnv: Registering BlockManagerMaster; 17/10/13 18:11:34 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 17/10/13 18:11:34 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up; 17/10/13 18:11:34 INFO storage.DiskBlockManager: Created local directory at /tmp/hdfs/blockmgr-ea0e0669-2981-4277-80a0-a67eddf1001d; 17/10/13 18:11:34 INFO memory.MemoryStore: MemoryStore started with capacity 366.3 MB; 17/10/13 18:11:34 INFO spark.SparkEnv: Registering OutputCommitCoordinator; 17/10/13 18:11:34 INFO util.log: Logging initialized @3816ms; 17/10/13 18:11:34 INFO server.Server: jetty-9.3.z-SNAPSHOT; 17/10/13 ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-336412775:4266,Secur,SecurityManager,4266,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-336412775,3,"['Secur', 'authenticat']","['SecurityManager', 'authentication']"
Security,": 20; 08:37:16.409 INFO GermlineCNVCaller - Requester pays: disabled; 08:37:16.409 INFO GermlineCNVCaller - Initializing engine; 08:37:21.698 INFO GermlineCNVCaller - Done initializing engine; 08:37:22.015 INFO GermlineCNVCaller - Retrieving intervals from read-count file (results/200219_X008378.counts.tsv)...; 08:37:22.119 INFO GermlineCNVCaller - No annotated intervals were provided...; 08:37:22.120 INFO GermlineCNVCaller - No GC-content annotations for intervals found; explicit GC-bias correction will not be performed...; 08:37:22.194 INFO GermlineCNVCaller - Running the tool in the COHORT mode...; 08:37:22.195 INFO GermlineCNVCaller - Shutting down engine; [February 26, 2019 8:37:22 AM GMT] org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller done. Elapsed time: 0.29 minutes.; Runtime.totalMemory()=330301440; java.lang.IllegalArgumentException: Output directory results/190226.181217_K00178.CNVCaller does not exist.; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:724); at org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller.validateArguments(GermlineCNVCaller.java:361); at org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller.doWork(GermlineCNVCaller.java:281); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); at org.broadinstitute.hellbender.Main.main(Main.java:291); Using GATK jar /mnt/storage/apps/software/gatk/4.1.0.0/gatk-package-4.1.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compress",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4825#issuecomment-467406398:15857,validat,validateArg,15857,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4825#issuecomment-467406398,1,['validat'],['validateArg']
Security,":///restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -- --spark-runner SPARK --spark-master yarn --executor-memory 48G --driver-memory 16g --driver-cores 2 --executor-cores 8 --num-executors 8. ```; 18/03/07 13:24:26 INFO storage.BlockManagerMasterEndpoint: Registering block manager scc-q14.scc.bu.edu:42456 with 25.4 GB RAM, BlockManagerId(2, scc-q14.scc.bu.edu, 42456, None); 18/03/07 13:24:27 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 247.0 KB, free 8.4 GB); 18/03/07 13:24:28 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 25.5 KB, free 8.4 GB); 18/03/07 13:24:28 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.48.225.55:32895 (size: 25.5 KB, free: 8.4 GB); 18/03/07 13:24:28 INFO spark.SparkContext: Created broadcast 0 from newAPIHadoopFile at ReadsSparkSource.java:112; 18/03/07 13:24:28 INFO hdfs.DFSClient: Created HDFS_DELEGATION_TOKEN token 7164 for farrell on ha-hdfs:scc; 18/03/07 13:24:28 INFO security.TokenCache: Got dt for hdfs://scc; Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:scc, Ident: (HDFS_DELEGATION_TOKEN token 7164 for farrell); 18/03/07 13:24:28 INFO input.FileInputFormat: Total input paths to process : 1; 18/03/07 13:59:26 INFO spark.SparkContext: Starting job: aggregate at FlagStatSpark.java:73; 18/03/07 13:59:26 INFO scheduler.DAGScheduler: Got job 0 (aggregate at FlagStatSpark.java:73) with 252 output partitions; 18/03/07 13:59:26 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (aggregate at FlagStatSpark.java:73); 18/03/07 13:59:26 INFO scheduler.DAGScheduler: Parents of final stage: List(); 18/03/07 13:59:26 INFO scheduler.DAGScheduler: Missing parents: List(); 18/03/07 13:59:26 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at filter at GATKSparkTool.java:220), which has no missing parents; 18/03/07 13:59:26 INFO memory.MemoryStore: Block broadcast_1 stored as values in mem",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4506#issuecomment-371280304:1499,secur,security,1499,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4506#issuecomment-371280304,1,['secur'],['security']
Security,":11:36 INFO yarn.Client: Requesting a new application from cluster with 3 NodeManagers; 17/10/13 18:11:36 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (164726 MB per container); 17/10/13 18:11:36 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead; 17/10/13 18:11:36 INFO yarn.Client: Setting up container launch context for our AM; 17/10/13 18:11:36 INFO yarn.Client: Setting up the launch environment for our AM container; 17/10/13 18:11:36 INFO yarn.Client: Preparing resources for our AM container; 17/10/13 18:11:37 INFO yarn.Client: Uploading resource file:/tmp/hdfs/spark-c7e5eece-205e-4bce-a69b-4168c9b79045/__spark_conf__2918234914787361986.zip -> hdfs://mg:8020/user/hdfs/.sparkStaging/application_1507856833944_0003/__spark_conf__.zip; 17/10/13 18:11:37 INFO spark.SecurityManager: Changing view acls to: hdfs; 17/10/13 18:11:37 INFO spark.SecurityManager: Changing modify acls to: hdfs; 17/10/13 18:11:37 INFO spark.SecurityManager: Changing view acls groups to: ; 17/10/13 18:11:37 INFO spark.SecurityManager: Changing modify acls groups to: ; 17/10/13 18:11:37 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hdfs); groups with view permissions: Set(); users with modify permissions: Set(hdfs); groups with modify permissions: Set(); 17/10/13 18:11:37 INFO yarn.Client: Submitting application application_1507856833944_0003 to ResourceManager; 17/10/13 18:11:37 INFO impl.YarnClientImpl: Submitted application application_1507856833944_0003; 17/10/13 18:11:37 INFO cluster.SchedulerExtensionServices: Starting Yarn extension services with app application_1507856833944_0003 and attemptId None; 17/10/13 18:11:38 INFO yarn.Client: Application report for application_1507856833944_0003 (state: ACCEPTED); 17/10/13 18:11:38 INFO yarn.Client: ; 	 client token: N/A; 	 diagnostics: N/A; 	 ApplicationMaster h",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-336412775:10633,Secur,SecurityManager,10633,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-336412775,1,['Secur'],['SecurityManager']
Security,":39:44 INFO org.spark_project.jetty.server.Server: Started @3988ms; 17/11/27 20:39:44 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@7fbe38a{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 17/11/27 20:39:44 INFO com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase: GHFS version: 1.6.1-hadoop2; 17/11/27 20:39:45 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at droazen-test-cluster-m/10.240.0.10:8032; 17/11/27 20:39:47 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1511814592376_0002; 17/11/27 20:39:52 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@7fbe38a{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 20:39:52.363 INFO CountReadsSpark - Shutting down engine; [November 27, 2017 8:39:52 PM UTC] org.broadinstitute.hellbender.tools.spark.pipelines.CountReadsSpark done. Elapsed time: 0.16 minutes.; Runtime.totalMemory()=630718464; code: 0; message: Error code 404 trying to get security access token from Compute Engine metadata for the default service account. This may be because the virtual machine instance does not have permission scopes specified.; reason: null; location: null; retryable: false; com.google.cloud.storage.StorageException: Error code 404 trying to get security access token from Compute Engine metadata for the default service account. This may be because the virtual machine instance does not have permission scopes specified.; 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:189); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:340); 	at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:197); 	at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:194); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:89); 	at com.google.cloud.RetryHelper.run(RetryHelper.java:74); 	at com.google.cloud.RetryHelper.runWithRetries(Ret",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3855#issuecomment-347320994:5710,secur,security,5710,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3855#issuecomment-347320994,2,"['access', 'secur']","['access', 'security']"
Security,":; /Users/shlee/Documents/branches/hellbender/build/install/gatk/bin/gatk LeftAlignAndTrimVariants -R /Users/shlee/Documents/ref/hg38/Homo_sapiens_assembly38.fasta -V /Users/shlee/Downloads/zeta_snippet_shlee/zeta_snippet.vcf.gz -O zeta_snippet_leftalign_96branch.vcf.gz; 12:55:31.964 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/shlee/Documents/branches/hellbender/build/install/gatk/lib/gkl-0.8.5.jar!/com/intel/gkl/native/libgkl_compression.dylib; Sep 06, 2018 12:55:32 PM shaded.cloud_nio.com.google.auth.oauth2.DefaultCredentialsProvider warnAboutProblematicCredentials; WARNING: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a ""quota exceeded"" or ""API not enabled"" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/.; 12:55:32.083 INFO LeftAlignAndTrimVariants - ------------------------------------------------------------; 12:55:32.083 INFO LeftAlignAndTrimVariants - The Genome Analysis Toolkit (GATK) v4.0.8.1-25-g0c6f06f-SNAPSHOT; 12:55:32.083 INFO LeftAlignAndTrimVariants - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:55:32.083 INFO LeftAlignAndTrimVariants - Executing as shlee@WMCF9-CB5 on Mac OS X v10.13.6 x86_64; 12:55:32.083 INFO LeftAlignAndTrimVariants - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_111-b14; 12:55:32.083 INFO LeftAlignAndTrimVariants - Start Date/Time: September 6, 2018 12:55:31 PM EDT; 12:55:32.083 INFO LeftAlignAndTrimVariants - ------------------------------------------------------------; 12:55:32.084 INFO LeftAlignAndTrimVariants - ------------------------------------------------------------; 12:55:32.084 INFO LeftAlignAndTrimVariants - HTSJDK Version: 2.16.0; 12:55:32.084 INFO LeftAlignAndTrimVariants - Picard ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3487#issuecomment-419190326:1356,authenticat,authentication,1356,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3487#issuecomment-419190326,1,['authenticat'],['authentication']
Security,; 	at java.io.FilterInputStream.close(FilterInputStream.java:181); 	at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.close(HttpURLConnection.java:3448); 	at org.gradle.wrapper.Download.downloadInternal(Download.java:77); 	at org.gradle.wrapper.Download.download(Download.java:44); 	at org.gradle.wrapper.Install$1.call(Install.java:61); 	at org.gradle.wrapper.Install$1.call(Install.java:48); 	at org.gradle.wrapper.ExclusiveFileAccessManager.access(ExclusiveFileAccessManager.java:69); 	at org.gradle.wrapper.Install.createDist(Install.java:48); 	at org.gradle.wrapper.WrapperExecutor.execute(WrapperExecutor.java:107); 	at org.gradle.wrapper.GradleWrapperMain.main(GradleWrapperMain.java:61); Caused by: javax.net.ssl.SSLException: java.net.SocketException: Connection reset; 	at sun.security.ssl.Alerts.getSSLException(Alerts.java:208); 	at sun.security.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1949); 	at sun.security.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1906); 	at sun.security.ssl.SSLSocketImpl.handleException(SSLSocketImpl.java:1870); 	at sun.security.ssl.SSLSocketImpl.handleException(SSLSocketImpl.java:1815); 	at sun.security.ssl.AppInputStream.read(AppInputStream.java:116); 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:284); 	at java.io.BufferedInputStream.read(BufferedInputStream.java:345); 	at sun.net.www.MeteredStream.read(MeteredStream.java:134); 	at java.io.FilterInputStream.read(FilterInputStream.java:133); 	at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.read(HttpURLConnection.java:3375); 	at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.read(HttpURLConnection.java:3368); 	at org.gradle.wrapper.Download.downloadInternal(Download.java:62); 	... 7 more; Caused by: java.net.SocketException: Connection reset; 	at java.net.SocketInputStream.read(SocketInputStream.java:210); 	at java.net.SocketInputStream.read(SocketInputStream.java:141); 	at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); 	,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4194#issuecomment-358498401:1855,secur,security,1855,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4194#issuecomment-358498401,1,['secur'],['security']
Security,; 	at sun.net.www.MeteredStream.available(MeteredStream.java:170); 	at sun.net.www.http.KeepAliveStream.close(KeepAliveStream.java:85); 	at java.io.FilterInputStream.close(FilterInputStream.java:181); 	at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.close(HttpURLConnection.java:3448); 	at org.gradle.wrapper.Download.downloadInternal(Download.java:77); 	at org.gradle.wrapper.Download.download(Download.java:44); 	at org.gradle.wrapper.Install$1.call(Install.java:61); 	at org.gradle.wrapper.Install$1.call(Install.java:48); 	at org.gradle.wrapper.ExclusiveFileAccessManager.access(ExclusiveFileAccessManager.java:69); 	at org.gradle.wrapper.Install.createDist(Install.java:48); 	at org.gradle.wrapper.WrapperExecutor.execute(WrapperExecutor.java:107); 	at org.gradle.wrapper.GradleWrapperMain.main(GradleWrapperMain.java:61); Caused by: javax.net.ssl.SSLException: java.net.SocketException: Connection reset; 	at sun.security.ssl.Alerts.getSSLException(Alerts.java:208); 	at sun.security.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1949); 	at sun.security.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1906); 	at sun.security.ssl.SSLSocketImpl.handleException(SSLSocketImpl.java:1870); 	at sun.security.ssl.SSLSocketImpl.handleException(SSLSocketImpl.java:1815); 	at sun.security.ssl.AppInputStream.read(AppInputStream.java:116); 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:284); 	at java.io.BufferedInputStream.read(BufferedInputStream.java:345); 	at sun.net.www.MeteredStream.read(MeteredStream.java:134); 	at java.io.FilterInputStream.read(FilterInputStream.java:133); 	at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.read(HttpURLConnection.java:3375); 	at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.read(HttpURLConnection.java:3368); 	at org.gradle.wrapper.Download.downloadInternal(Download.java:62); 	... 7 more; Caused by: java.net.SocketException: Connection reset; 	at java.net.SocketInputStream.read(SocketInputStream.java:210),MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4194#issuecomment-358498401:1721,secur,security,1721,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4194#issuecomment-358498401,1,['secur'],['security']
Security,; ## master #3228 +/- ##; ===============================================; + Coverage 80.419% 80.427% +0.008% ; Complexity 17290 17290 ; ===============================================; Files 1165 1165 ; Lines 62596 62597 +1 ; Branches 9768 9768 ; ===============================================; + Hits 50339 50345 +6 ; + Misses 8352 8347 -5 ; Partials 3905 3905; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/3228?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...broadinstitute/hellbender/utils/test/BaseTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/3228?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0Jhc2VUZXN0LmphdmE=) | `83.704% <100%> (+0.122%)` | `36 <0> (ø)` | :arrow_down: |; | [.../tools/walkers/validation/CountFalsePositives.java](https://codecov.io/gh/broadinstitute/gatk/pull/3228?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vQ291bnRGYWxzZVBvc2l0aXZlcy5qYXZh) | `93.548% <100%> (ø)` | `7 <1> (ø)` | :arrow_down: |; | [.../tools/walkers/validation/FalsePositiveRecord.java](https://codecov.io/gh/broadinstitute/gatk/pull/3228?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vRmFsc2VQb3NpdGl2ZVJlY29yZC5qYXZh) | `100% <100%> (ø)` | `7 <2> (ø)` | :arrow_down: |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/3228?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `73.649% <0%> (+2.027%)` | `34% <0%> (ø)` | :arrow_down: |; | [...e/hellbender/engine/spark/SparkContextFactory.java](https://codecov.io/gh/broadinstitute/gatk/pull/3228?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvU3BhcmtDb250ZXh0RmFjdG9yeS5qYXZh) | `70% <0%> (+3.333%)` | `10% <0%> (ø)` | :arrow_down: |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3228#issuecomment-314209891:1519,validat,validation,1519,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3228#issuecomment-314209891,1,['validat'],['validation']
Security,; ===============================================; Files 1065 1065 ; Lines 58788 58788 ; Branches 9578 9578 ; ===============================================; + Hits 46310 46315 +5 ; + Misses 8752 8746 -6 ; - Partials 3726 3727 +1; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/3989?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...nder/tools/spark/pipelines/PrintVariantsSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/3989/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvUHJpbnRWYXJpYW50c1NwYXJrLmphdmE=) | `66.667% <ø> (ø)` | `2 <0> (ø)` | :arrow_down: |; | [...adinstitute/hellbender/tools/IndexFeatureFile.java](https://codecov.io/gh/broadinstitute/gatk/pull/3989/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9JbmRleEZlYXR1cmVGaWxlLmphdmE=) | `90.323% <ø> (ø)` | `12 <0> (ø)` | :arrow_down: |; | [...r/tools/walkers/validation/RemoveNearbyIndels.java](https://codecov.io/gh/broadinstitute/gatk/pull/3989/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vUmVtb3ZlTmVhcmJ5SW5kZWxzLmphdmE=) | `90.476% <ø> (ø)` | `5 <0> (ø)` | :arrow_down: |; | [...oadinstitute/hellbender/tools/GatherVcfsCloud.java](https://codecov.io/gh/broadinstitute/gatk/pull/3989/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9HYXRoZXJWY2ZzQ2xvdWQuamF2YQ==) | `70.811% <0%> (ø)` | `40 <0> (ø)` | :arrow_down: |; | [...rg/broadinstitute/hellbender/utils/IndexUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/3989/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9JbmRleFV0aWxzLmphdmE=) | `80.702% <100%> (ø)` | `16 <2> (ø)` | :arrow_down: |; | [...kers/variantutils/UpdateVCFSequenceDictionary.java](https://codecov.io/gh/broadinstitute/gatk/pull/3989/diff?src=pr&el=tree#diff-c3JjL21haW4va,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3989#issuecomment-352845785:1528,validat,validation,1528,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3989#issuecomment-352845785,1,['validat'],['validation']
Security,=) | `56.522% <0%> (-36.335%)` | `2 <0> (ø)` | |; | [...der/engine/spark/datasources/ReadsSparkSource.java](https://codecov.io/gh/broadinstitute/gatk/pull/6010/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvZGF0YXNvdXJjZXMvUmVhZHNTcGFya1NvdXJjZS5qYXZh) | `63.158% <100%> (+1.825%)` | `18 <1> (+1)` | :arrow_up: |; | [...stitute/hellbender/engine/spark/GATKSparkTool.java](https://codecov.io/gh/broadinstitute/gatk/pull/6010/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvR0FUS1NwYXJrVG9vbC5qYXZh) | `82.873% <100%> (+0.095%)` | `78 <0> (ø)` | :arrow_down: |; | [...ellbender/tools/spark/pathseq/PathSeqBwaSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/6010/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9wYXRoc2VxL1BhdGhTZXFCd2FTcGFyay5qYXZh) | `67.391% <100%> (ø)` | `7 <0> (ø)` | :arrow_down: |; | [...tools/spark/validation/CompareDuplicatesSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/6010/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay92YWxpZGF0aW9uL0NvbXBhcmVEdXBsaWNhdGVzU3BhcmsuamF2YQ==) | `89.63% <100%> (ø)` | `40 <0> (ø)` | :arrow_down: |; | [...lbender/tools/spark/pathseq/PathSeqScoreSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/6010/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9wYXRoc2VxL1BhdGhTZXFTY29yZVNwYXJrLmphdmE=) | `58.491% <100%> (+1.083%)` | `7 <0> (ø)` | :arrow_down: |; | [...lkers/varianteval/evaluators/VariantEvaluator.java](https://codecov.io/gh/broadinstitute/gatk/pull/6010/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhcmlhbnRldmFsL2V2YWx1YXRvcnMvVmFyaWFudEV2YWx1YXRvci5qYXZh) | `70% <0%> (-12.353%)` | `12% <0%> (ø)` | |; | [...ls/walkers/varianteval/util/EvaluationContext.java](https://codecov.io,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6010#issuecomment-503050294:2196,validat,validation,2196,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6010#issuecomment-503050294,1,['validat'],['validation']
Security,===================; Files 1059 1056 -3 ; Lines 59177 59149 -28 ; Branches 9616 9615 -1 ; ==============================================; - Hits 46750 46744 -6 ; + Misses 8689 8667 -22 ; Partials 3738 3738; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/4094?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...kers/variantutils/CalculateGenotypePosteriors.java](https://codecov.io/gh/broadinstitute/gatk/pull/4094/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhcmlhbnR1dGlscy9DYWxjdWxhdGVHZW5vdHlwZVBvc3RlcmlvcnMuamF2YQ==) | `85.915% <ø> (ø)` | `13 <0> (ø)` | :arrow_down: |; | [...stitute/hellbender/tools/HaplotypeCallerSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4094/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9IYXBsb3R5cGVDYWxsZXJTcGFyay5qYXZh) | `82.022% <ø> (ø)` | `23 <0> (ø)` | :arrow_down: |; | [...tmutpileup/ValidateBasicSomaticShortMutations.java](https://codecov.io/gh/broadinstitute/gatk/pull/4094/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vYmFzaWNzaG9ydG11dHBpbGV1cC9WYWxpZGF0ZUJhc2ljU29tYXRpY1Nob3J0TXV0YXRpb25zLmphdmE=) | `85.965% <ø> (ø)` | `7 <0> (ø)` | :arrow_down: |; | [...pipelines/metrics/CollectMultipleMetricsSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4094/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvbWV0cmljcy9Db2xsZWN0TXVsdGlwbGVNZXRyaWNzU3BhcmsuamF2YQ==) | `92.593% <ø> (ø)` | `9 <0> (ø)` | :arrow_down: |; | [...s/metrics/CollectBaseDistributionByCycleSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4094/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvbWV0cmljcy9Db2xsZWN0QmFzZURpc3RyaWJ1dGlvbkJ5Q3ljbGVTcGFyay5qYXZh) | `87.037% <ø> (ø)` | `9 <0> (ø)` | :arrow_dow,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4094#issuecomment-356113187:1542,Validat,ValidateBasicSomaticShortMutations,1542,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4094#issuecomment-356113187,1,['Validat'],['ValidateBasicSomaticShortMutations']
Security,====================; Files 1089 1090 +1 ; Lines 64159 64937 +778 ; Branches 10344 10510 +166 ; ===============================================; + Hits 51600 52279 +679 ; - Misses 8498 8569 +71 ; - Partials 4061 4089 +28; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/4878?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...s/read/markduplicates/sparkrecords/PairedEnds.java](https://codecov.io/gh/broadinstitute/gatk/pull/4878/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9yZWFkL21hcmtkdXBsaWNhdGVzL3NwYXJrcmVjb3Jkcy9QYWlyZWRFbmRzLmphdmE=) | `100% <ø> (ø)` | `1 <0> (ø)` | :arrow_down: |; | [...roadinstitute/hellbender/utils/read/ReadUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4878/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9yZWFkL1JlYWRVdGlscy5qYXZh) | `80% <100%> (+0.142%)` | `202 <3> (+3)` | :arrow_up: |; | [...tools/spark/validation/CompareDuplicatesSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4878/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay92YWxpZGF0aW9uL0NvbXBhcmVEdXBsaWNhdGVzU3BhcmsuamF2YQ==) | `84.946% <100%> (+0.502%)` | `24 <3> (ø)` | :arrow_down: |; | [...itute/hellbender/engine/spark/GATKRegistrator.java](https://codecov.io/gh/broadinstitute/gatk/pull/4878/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvR0FUS1JlZ2lzdHJhdG9yLmphdmE=) | `100% <100%> (ø)` | `3 <0> (ø)` | :arrow_down: |; | [...icates/sparkrecords/MarkDuplicatesSparkRecord.java](https://codecov.io/gh/broadinstitute/gatk/pull/4878/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9yZWFkL21hcmtkdXBsaWNhdGVzL3NwYXJrcmVjb3Jkcy9NYXJrRHVwbGljYXRlc1NwYXJrUmVjb3JkLmphdmE=) | `100% <100%> (ø)` | `7 <3> (ø)` | :arrow_down: |; | [...ils/read/markduplicates/sparkrecords/Fragment.java](https://co,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4878#issuecomment-396338920:1555,validat,validation,1555,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4878#issuecomment-396338920,1,['validat'],['validation']
Security,====================================; + Hits 38890 52186 +13296 ; + Misses 21482 8384 -13098 ; + Partials 4232 4024 -208; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/4970?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...nce/SegmentedCpxVariantSimpleVariantExtractor.java](https://codecov.io/gh/broadinstitute/gatk/pull/4970/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvaW5mZXJlbmNlL1NlZ21lbnRlZENweFZhcmlhbnRTaW1wbGVWYXJpYW50RXh0cmFjdG9yLmphdmE=) | `93.96% <100%> (+8.949%)` | `71 <0> (+5)` | :arrow_up: |; | [.../sv/discovery/inference/CpxVariantInterpreter.java](https://codecov.io/gh/broadinstitute/gatk/pull/4970/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvaW5mZXJlbmNlL0NweFZhcmlhbnRJbnRlcnByZXRlci5qYXZh) | `79.839% <100%> (+74.921%)` | `26 <0> (+25)` | :arrow_up: |; | [...tmutpileup/ValidateBasicSomaticShortMutations.java](https://codecov.io/gh/broadinstitute/gatk/pull/4970/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vYmFzaWNzaG9ydG11dHBpbGV1cC9WYWxpZGF0ZUJhc2ljU29tYXRpY1Nob3J0TXV0YXRpb25zLmphdmE=) | `85.965% <0%> (-0.94%)` | `7% <0%> (-6%)` | |; | [.../hellbender/tools/genomicsdb/GenomicsDBImport.java](https://codecov.io/gh/broadinstitute/gatk/pull/4970/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9nZW5vbWljc2RiL0dlbm9taWNzREJJbXBvcnQuamF2YQ==) | `75.758% <0%> (-0.591%)` | `53% <0%> (+7%)` | |; | [...llbender/tools/genomicsdb/GenomicsDBConstants.java](https://codecov.io/gh/broadinstitute/gatk/pull/4970/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9nZW5vbWljc2RiL0dlbm9taWNzREJDb25zdGFudHMuamF2YQ==) | `0% <0%> (ø)` | `0% <0%> (ø)` | :arrow_down: |; | [...r/tools/walkers/annotator/ClippingRankSumTest.java](https://co,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4970#issuecomment-401503431:1658,Validat,ValidateBasicSomaticShortMutations,1658,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4970#issuecomment-401503431,1,['Validat'],['ValidateBasicSomaticShortMutations']
Security,"> @colinhercus I was able to re-run your command successfully on the latest master branch (not in a release yet). I believe PR #6240 fixed the issue. @Rohit-Satyam @danielecook there's a good chance the errors you encountered are also fixed. If not, please let me know. In reference to your reply, I wish to inform you the problem still stands. > java.lang.IllegalArgumentException: Cannot construct fragment from more than two reads; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:725); at org.broadinstitute.hellbender.utils.read.Fragment.create(Fragment.java:36); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1376); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); at org.broadinstitute.hellbender.utils.genotyper.AlleleLikelihoods.groupEvidence(AlleleLikelihoods.java:595); at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticGenotypingEngine.callMutations(SomaticGenotypingEngine.java:93); at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2Engine.callRegion(Mutect2Engine.java:251); at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2.apply(Mutect2.java:320); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:308); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:281); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1048); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLinePro",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6230#issuecomment-595805643:480,validat,validateArg,480,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6230#issuecomment-595805643,1,['validat'],['validateArg']
Security,> @lbergelson @gokalpcelik any chance of giving me access to the workspace for the 330 whole exomes?. Hi @nalinigans ; Unfortunately this is on my private company server but I may be able to conduct tests if you need me to. I can generate a fork of gatk and update GenomicsDB to test it.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8989#issuecomment-2434590689:51,access,access,51,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8989#issuecomment-2434590689,1,['access'],['access']
Security,"> Another potential solution is to audit every Dataflow (and spark) code that can receive SAMRecords as input, and make sure they call some utility ""putHeadersBack"" function. I agree. I think this is the most practical solution. (Having the coder/serializer do it is difficult, as it's hard to get the header to the serializer, unless it is passed statically, which I think we'd rather avoid.). This would look something like `reads.map(read -> ReadUtils.addHeader(header, read))`, and would be added after every shuffle. Since we should be very aware where every shuffle is happening (and we want to minimize their number) it shouldn't be too onerous. The other related point that Uri has touched on is the need for an efficient encoding of reads (even without the header), which is critical to making the computations run in a reasonable amount of time. The approach I've taken in https://github.com/broadinstitute/hellbender/pull/899 is to use the htsjdk BAMEncoder to serialize reads (Hadoop-BAM does something very similar), and it works very well in my tests of sorting large BAMs. Does https://github.com/broadinstitute/hellbender/pull/899 plus ""putHeadersBack"" sound like a reasonable solution?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141004828:35,audit,audit,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141004828,1,['audit'],['audit']
Security,"> Hi @icemduru Looks like your slurm workload manager was configured to have a limit of 48GBs of maximum process memory size per execution. Your java instance is set with -Xmx45G which will cover most of this limit and leaves only a handful of memory space for the native GenomicsDB library. Native libraries work above the heapsize so it is better for you to set your -Xmx to a more sensible size of 8~12GB and leave rest of the memory space to the native library to use.; > ; > Keep in mind that this memory limit on slurm could be set per user not per task therefore you may need to run a single contig at a time or maybe 2 of them simultaneously. Otherwise slurm may interefere with all the tasks and cancel all your jobs.; > ; > One final reminder. We strongly recommend users to set th; [slurm-22680938.out_text.txt](https://github.com/user-attachments/files/16608314/slurm-22680938.out_text.txt); e temporary directory to somewhere else other than /tmp. Slurm workload manager interferes with this preference and sometimes results in premature termination of the gatk processes due to deletion of extracted native library and accessory files.; > ; > I hope this helps. Thank you for your help, but unfortunately it didn't resolve the issue. I've already tried allocating 10GB of memory using the -Xmx10g flag and redirecting the temporary directory away from /tmp. However, GATK is still attempting to consume more than 48GB of RAM, resulting in the termination of my run.; [slurm-22680938.out_text.txt](https://github.com/user-attachments/files/16608325/slurm-22680938.out_text.txt)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8918#issuecomment-2287941632:1133,access,accessory,1133,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8918#issuecomment-2287941632,1,['access'],['accessory']
Security,"> Hm - I don't think we can take that last change. Theres not much use in validating args after they've been used by the constructors. Let me see if there is an alternative. Well in the prior commit I just caught CommandLineException (https://github.com/broadinstitute/gatk/pull/6973/commits/814839f498cda8ce627a47229d77fb6cac7ca6e0), but this seemed hacky. . Why cant there be a separate method to create the class and validate args? . There is VariantEvalEngine.validateAndInitialize(), where some classes do analogous checks.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-827857883:74,validat,validating,74,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-827857883,3,['validat'],"['validate', 'validateAndInitialize', 'validating']"
Security,"> How do you know ""without affecting sensitivity"" ?. I ran our validations.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3988#issuecomment-352884724:63,validat,validations,63,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3988#issuecomment-352884724,1,['validat'],['validations']
Security,"> I don't think we've made any guarantees about the thread safety of Funcotator or the associated datasource classes.; > ; > Also, this account seems to be a bot and I can't access its listed home page…; > ; > I can audit the class at some point. https://codesafe.qianxin.com/#/home",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7376#issuecomment-894740783:174,access,access,174,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7376#issuecomment-894740783,2,"['access', 'audit']","['access', 'audit']"
Security,"> It gives us the ability to easily aggregate records across multiple FeatureInputs, and (potentially, if we wanted) to retrieve records by type rather than by source. Regardless of doing full injection FeatureContext seems rather unnecessary to me; the query ability that you mention is provided by FeatureManger on which the FeatureContext delegates on given the genomeLoc of the locus or read.... I don't see why the tool cannot be exposed to FeatureManager directly. Is this done to make sure that walkers are of the right type? I mean if you are working on aread walker and you need to look for information beyond the read's genomeLoc that means that you must consider a different walker type or develop a new one?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/242#issuecomment-76763116:193,inject,injection,193,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/242#issuecomment-76763116,2,"['expose', 'inject']","['exposed', 'injection']"
Security,"> It seems like the patch in 4.1.6 didn't go far enough and that exception needs to be replaced with a continue in all cases. That would work, but I see where I caused the regression upstream. I chopped leading and trailing deletions from haplotype cigars, same as for read cigars, but for haplotypes we want to keep these deletions because the start and end positions need to remain pegged to the reference start and end. I have a fix + regression test branch, which is running on every M2 validation.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6533#issuecomment-609115892:491,validat,validation,491,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6533#issuecomment-609115892,1,['validat'],['validation']
Security,> [...] you need to build within a full git clone of the GATK repository [...]. I build as part of the FreeBSD package build. Package builders can never build from git clones because git clones don't preserve fingerprints and fingerprints are needed to maintain security (repeatability of builds). Could you please consider adding an alternative way of determining of version through a build option?. Packages on all OSes would benefit from this. Thank you.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7143#issuecomment-799722491:262,secur,security,262,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7143#issuecomment-799722491,1,['secur'],['security']
Security,@Bowen1992 I'm not sure I understand exactly what the problem is. . Is this a problem specific to running tools with genomicsDB? ; What is ParaStor? It sounds like some sort of enterprise file system? Is other software fast when writing to ParaStor? I'm sure we'll be able to debug this since I don't think we have access to a similar system.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8546#issuecomment-1758332891:315,access,access,315,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8546#issuecomment-1758332891,1,['access'],['access']
Security,"@DanishIntizar Hello! Thank you for this pr. This is great to see an official plugin from amazon available. I appreciate that you took the time to make it an optional include. I think if we're going to include it we might as well just add it as one of our normal dependencies though. Assuming there aren't any dependency conflicts it **should** (always a risky statement) be independent from everything else. . Thanks also for identifying the different issues you mentioned. It's expected that it won't work with most picard tools as you discovered, but we're actively in the process of updating more of them too support Paths instead of Files so that will slowly improve. The second issue is more worrisome. We regularly use an equivalent provider with google to read reference files through the exact same code, so I suspect there is either some sort of mismatched assumptions in the way they are handling things. Maybe something strange with the Path.resolve methods or the like. (Or in in the much worse potential case a bug in their look ahead caching.). I'd like to look into that before we'd merge this. Ideally we would have tests for this. Are there any public AWS paths we could read from without any secret authentication?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8672#issuecomment-1930094721:1218,authenticat,authentication,1218,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8672#issuecomment-1930094721,1,['authenticat'],['authentication']
Security,"@EdwardDixon Sure, here's my suggested repair process:. 1. If you haven't already, add an ""upstream"" remote to your git clone via `git remote add upstream git@github.com:broadinstitute/gatk.git` (or `https://github.com/broadinstitute/gatk.git` if you don't have ssh authentication set up with github). 2. `git fetch upstream`. 3. Copy the files you actually intended to change in this PR into a temp directory somewhere. 4. Create a new temporary branch off of `upstream/master`: `git checkout -b avxcheck_repaired upstream/master`. 5. Copy the files you saved in step 3 back into their original locations in the working tree. 6. `git commit -a`. 7. Examine the diff against upstream/master via `git diff upstream/master HEAD`. Verify that the diff is what you expect. 8. Run `git rev-parse HEAD` and save the commit ID it outputs. 9. Switch back to the broken version of the branch: `git checkout avxcheck`. 10. Run `git reset --hard commit_id_from_step_8`. This will force the branch to point to the repaired commit we created in step 6. 11. Run `git push -f origin avxcheck:avxcheck` to force-push the repaired version of the branch into your fork. Then check that it looks ok on github. For avoiding this sort of thing in the future, here's a few tips:. * Never run `git merge` or `git pull`. Always update your branch with changes from the latest gatk master branch via the command: `git fetch upstream && git rebase -i upstream/master`, followed by `git push -f` to push the rebased branch into your fork. * If you've never run `git rebase` before, read a tutorial on it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-437415495:266,authenticat,authentication,266,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-437415495,1,['authenticat'],['authentication']
Security,"@Emmalynchen I wouldn't worry about the `log4j:WARN` messages discussed in this thread---they're just harmless annoyances that pop up because we haven't gotten around to making sure the HDF5Library dependency uses the same logger as the rest of the GATK. Looking at your initial post (before you edited it), it looks like DenoiseReadCounts is failing because the panel of normals contains different intervals than those in the read-count collection you are trying to denoise:. ```; 22:50:58.635 INFO SVDDenoisingUtils - Validating sample intervals against original intervals used to build panel of normals...; 22:50:59.487 INFO DenoiseReadCounts - Shutting down engine; [May 7, 2019 10:50:59 PM UTC] org.broadinstitute.hellbender.tools.copynumber.DenoiseReadCounts done. Elapsed time: 0.06 minutes.; Runtime.totalMemory()=894959616; java.lang.IllegalArgumentException: Sample intervals must be identical to the original intervals used to build the panel of normals.; ```. You might try asking for more pointers over in the GATK Forums (https://gatkforums.broadinstitute.org/gatk), if you need them. Good luck!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3763#issuecomment-491473550:520,Validat,Validating,520,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3763#issuecomment-491473550,1,['Validat'],['Validating']
Security,"@Horneth Ideally we'd just check up-front whether the bucket has requester pays enabled, and specify the user's default project as the billing project if it is. . It would also be good, I think, if we included a toggle that allows the client to tell the library to throw and refuse to proceed if an attempt is made to access requester-pays data, so that users who don't want to incur GCS access charges can get a hard guarantee that they won't.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4828#issuecomment-394839598:318,access,access,318,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4828#issuecomment-394839598,2,['access'],['access']
Security,"@J-Moravec GATK and picard should both handle the same reference files. It typically requires not a gzipped reference, but a bgzipped reference to enable random access ( as I think samtools does as well.). You will need several auxiliary files with the reference. You need the .fai index as well as the .gzi index. . Could you post the stack trace you're receiving as well as additional information about your reference file to help debug the problem you're seeing?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6590#issuecomment-625875092:161,access,access,161,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6590#issuecomment-625875092,1,['access'],['access']
Security,"@LeeTL1220 @droazen This is ready for review. It modestly improves all of our validations except Dream challenge 4, which I suspect is because the synthetic data doesn't respect mate pairing. To account for that I added an advanced option to turn off mate-awareness. @kachulis Thanks for catching the error in finding fragments.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5831#issuecomment-490599827:78,validat,validations,78,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5831#issuecomment-490599827,1,['validat'],['validations']
Security,"@LeeTL1220 @katevoss @ruchim I started exposing all optional task-level parameters in the somatic workflows so that they could be specified via json when the workflows are used as subworkflows. E.g., `CNVSomaticPanelWorkflow.PreprocessIntervals.bin_length` is an optional task-level parameter that can be specified properly via json when `CNVSomaticPanelWorkflow` is the top-level workflow, but not when `CNVSomaticPanelWorkflow` is used as a subworkflow. This is because `MetaWorkflow.CNVSomaticPanelWorkflow.PreprocessIntervals.bin_length` cannot be set, correct?. However, things quickly became very messy. For example, alongside parameters like `bin_length` which are unique to the PreprocessIntervals task, we also have a lot of optional runtime parameters that are named generic things like `mem` which are not. So to expose these, we'd have to have workflow-level parameters with names like `preprocess_intervals_mem`, etc. It seems like this is exactly the problem the expected functionality would solve, if only it worked past the subworkflow level and the namespace is propagated as one would expect. Requiring that these be exposed also partially obviates the reason for having optional task-level arguments in the first place---what's the point of having them be optional if I have to add lines of code to expose all of them at the workflow level?. So again, I'm strongly against exposing all inputs for a particular workflow on the off-chance that that workflow might be used as a subworkflow. This adds a lot of unnecessary boilerplate that quickly gets very messy. I think that this problem should instead be solved by dynamically bubbling up all inputs, optional or required, at all levels. Anyway, I'm not going to try to tackle this before release, which I think was OK with @LeeTL1220. However, after release, I'd be happy to sit down and discuss how we want to do this sort of thing going forward.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3980#issuecomment-355830670:824,expose,expose,824,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3980#issuecomment-355830670,3,['expose'],"['expose', 'exposed']"
Security,"@LeeTL1220 Having started to implement this. I have a number of design questions that would be informed by your usecases. . Firstly, is there a reason to preserve symbolic alleles? It seems as though spanning deletions could/should be dropped as in most cases there is another variant context representing that deletion elsewhere in your file? Should there be validation around dropping spanning deletion symbolic alleles to ensure we aren't dropping a spanning deletion that isn't represented anywhere else? What about nocalls? . Your example suggests that we rely on the header line counts for subsetting annotations, if there is a disagreement in the header do you want any more sophisticated behavior than just throwing? My understanding is that we are lenient with splitting in htsjdk and there have been some mislabeled header lines in the past that would make this an expected state. Furthermore, most allele specific annotators are of type string because there is no standard for ""|"" delimiters which makes them hard to handle properly. @ldgauthier do you have any suggestions as to how to detect and handle allele specific annotations?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4976#issuecomment-404949363:360,validat,validation,360,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4976#issuecomment-404949363,1,['validat'],['validation']
Security,"@LeeTL1220 I have a fast python implementation of the above. It'll take a little bit of additional code to make it output segment files. I can add that and start running some validation data, or I can just go ahead and start coding up the Java implementation, depending on how long you think it'll take to put together some validation runs up through DenoiseReadCounts. Do we want to improve the ReCapSegCaller behind CallSegments while we're at it? @davidbenjamin perhaps you can briefly remind me of the idea behind your initial caller and of the issues it had.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-324128827:175,validat,validation,175,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-324128827,2,['validat'],['validation']
Security,"@LeeTL1220 I went ahead and exposed more mem_gb parameters, which is convenient when we want to go below 250bp and the pair WDL is used as a subworkflow. Please review carefully!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4364#issuecomment-363787268:28,expose,exposed,28,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4364#issuecomment-363787268,1,['expose'],['exposed']
Security,"@LeeTL1220 Latest commit includes the rollback. I will create a separate branch for you that is rebased on sl_preprocess. Looking at it again, I initially described the change to you incorrectly. I thought it was ""similar CR || similar AF -> merge"" to ""similar CR && similar AF -> merge"", but that's not actually the case; it's instead ""similar according to credible interval 1 || similar according to credible interval 2 -> merge"" to ""similar according to credible interval 1 && similar according to credible interval 2 -> merge"". Probably the `&&` behavior is way too conservative, so I think rolling back to the `||` behavior would be fine for release. Let's double check with the validation just to be sure.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4046#issuecomment-355467989:684,validat,validation,684,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4046#issuecomment-355467989,1,['validat'],['validation']
Security,"@LeeTL1220 Not sure if you will absolutely need this for HCC1143 WES validation, but just be aware that this change is coming soon.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3981#issuecomment-352128732:69,validat,validation,69,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3981#issuecomment-352128732,1,['validat'],['validation']
Security,"@LeeTL1220 Note that I've validated with womtool, but as we've seen (#4281), changes of this sort (which deal with optional parameters, etc.) may slip through even if tests pass. You should take a careful look to make sure everything is in order!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4288#issuecomment-361293506:26,validat,validated,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4288#issuecomment-361293506,1,['validat'],['validated']
Security,"@LeeTL1220 OK, see the sl_change_model_segments_defaults_rebased branch. For validation, I'd say that sweeping the following should suffice:. Array[Float] kernel_variance_allele_fractions = [0.025, 0.05, 0.25]; Array[Float] smoothing_thresholds_allele_fraction = [2.0, 10.0, 50.0]; Array[Float] smoothing_thresholds_copy_ratio = [2.0, 10.0, 50.0]",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4046#issuecomment-355468401:77,validat,validation,77,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4046#issuecomment-355468401,1,['validat'],['validation']
Security,"@LeeTL1220 OK, tweaked the message a bit. I think I'm OK with this going in for the next point release. This is the sort of thing for which it will be nice to have the automatic validations, as a sanity check.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4292#issuecomment-363828979:178,validat,validations,178,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4292#issuecomment-363828979,1,['validat'],['validations']
Security,@LeeTL1220 The criteria in my opinion are being the best Mutect and being stable. Were you suggesting waiting for some validation like MC3?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4384#issuecomment-365660852:119,validat,validation,119,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4384#issuecomment-365660852,1,['validat'],['validation']
Security,"@Ning-310 The error you're getting here (""Did not inflate expected amount"") implies that your input file is likely corrupt. Can you try running the tool `PrintBGZFBlockInformation` to validate the compressed blocks in your `.vcf.gz`?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7114#issuecomment-793015796:184,validat,validate,184,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7114#issuecomment-793015796,1,['validat'],['validate']
Security,@SHuang-Broad what is PipelineOptions needed for ... does one need it to access the reference if it stored in something that is not a ordinary file? (e.g. GS bucket?),MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3476#issuecomment-325025607:73,access,access,73,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3476#issuecomment-325025607,1,['access'],['access']
Security,"@Sun-shan Hi, could you try running with the `--disable-sequence-dictionary-validation` command?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4112#issuecomment-357043845:76,validat,validation,76,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4112#issuecomment-357043845,1,['validat'],['validation']
Security,"@Tintest Sorry for the slow reply. I'm not sure exactly what the issue is. I've never seen this exact error before. . I have two guesses, one is that there's something really weird going on in spark that's causing that null pointer exception which is killing the heartbeat. I'm not sure how to debug that without your access to your input data . The other theory which I think is more likely, is that you're running out of memory and it's causing weird errors to occur. How much memory is available to your job? You can set the java -Xmx value with `--java-options ""-Xmx120g""` as a GATK option. I would check that you're not running out of memory on your machine, or giving the job too little. I think for BaseRecalibratorSpark you want at least 2-4g per core, but haven't tested it in a long time so I might be wrong about that.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4515#issuecomment-373250134:318,access,access,318,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4515#issuecomment-373250134,1,['access'],['access']
Security,"@WanessaMGoes It looks like most/all of your reads are getting filtered out by GATK's `WellformedReadFilter`:. ```; 17:07:23.141 INFO DepthOfCoverage - 1031666 read(s) filtered by: WellformedReadFilter; ```. Could you try running `ValidateSamFile` on your bam, and paste the result here?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7332#issuecomment-882767886:231,Validat,ValidateSamFile,231,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7332#issuecomment-882767886,1,['Validat'],['ValidateSamFile']
Security,@Zepeng-Mu There seems to be an unsupported character in your reference fasta. Can you verify the integrity hash of your reference? Could you also try re-generating the fasta index using `samtools faidx`?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6911#issuecomment-716737017:98,integrity,integrity,98,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6911#issuecomment-716737017,2,"['hash', 'integrity']","['hash', 'integrity']"
Security,"@akiezun @pgrosu I'd be very hesitant to make such a radical change to the internals of the tool at the same instant we're trying to validate BQSR for production use (https://github.com/broadinstitute/gatk/issues/1413). BAQ doesn't use `NestedIntegerArray`, anyway -- `NestedIntegerArray` is used for the recal tables themselves.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1460#issuecomment-180664239:133,validat,validate,133,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1460#issuecomment-180664239,1,['validat'],['validate']
Security,@akiezun Done. Could you give the user **coveralls** pull access to the repo so it can write into the pull requests?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/79#issuecomment-68935893:58,access,access,58,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/79#issuecomment-68935893,1,['access'],['access']
Security,"@akiezun In the settings I think you can give individual users access to the repo. I don't it ever got ticked for recapseg, so it's _not_ making comments on the pull requests.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/79#issuecomment-69066246:63,access,access,63,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/79#issuecomment-69066246,1,['access'],['access']
Security,"@akiezun back to you with a comment. <!-- Reviewable comment -K2OCrEDRCLfJof_Cdgp:-1160907455 -->. ---. Reviewed 2 of 2 files at r1.; Review status: all files reviewed at latest revision, 2 unresolved discussions. ---. <sup>**[src/main/java/org/broadinstitute/hellbender/engine/datasources/ReferenceAPISource.java, line 155 [r1]](https://reviewable.io:443/reviews/broadinstitute/gatk/1058#-K2OAVk5uR5BT7DB8ujU)** ([raw file](https://github.com/broadinstitute/gatk/blob/a5c03b9e93125bf1acd9e9969a81f236a128ee6d/src/main/java/org/broadinstitute/hellbender/engine/datasources/ReferenceAPISource.java#L155)):</sup>; Should this be a `UserException`? It seems like it's probably a bug in the calling code if it's going off the end of a contig, unless you think the likely path is that they've specified the wrong reference. In either case, I think the message should mention that fewer bases were retrieved than requested in addtion to displaying the interval. It might be worth making this a more specific exception subclass too, I can imagine potential situations where people would want to catch and handle this differently. . <!-- Reviewable comment -K2OAVk5uR5BT7DB8ujV:-2111080668 -->. ---. <sup>**[src/test/java/org/broadinstitute/hellbender/engine/datasources/ReferenceAPISourceUnitTest.java, line 170 [r1]](https://reviewable.io:443/reviews/broadinstitute/gatk/1058#-K2OCQKoj0hZv86WbYXu)** ([raw file](https://github.com/broadinstitute/gatk/blob/a5c03b9e93125bf1acd9e9969a81f236a128ee6d/src/test/java/org/broadinstitute/hellbender/engine/datasources/ReferenceAPISourceUnitTest.java#L170)):</sup>; It would be nice if this were more specific so we can be sure the test isn't passing accidentally due to a failed authentication or downed webserver. <!-- Reviewable comment -K2OCQKpe3Ja-0PPUKjV:775526476 -->. ---. Comments from the [review on Reviewable.io](https://reviewable.io:443/reviews/broadinstitute/gatk/1058). <!-- Sent from Reviewable.io -->",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1058#issuecomment-154172680:1715,authenticat,authentication,1715,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1058#issuecomment-154172680,1,['authenticat'],['authentication']
Security,"@akiezun so my approach here is to stream through to build a histogram on bins 0, ..., 250. I can spin my own median function on a histogram, but it seems that Tim has already written a histogram class below in htsjdk.samtools.util that computes percentiles, mean, etc... I think it'd make sense to use something like this in general rather than locally replicating methods for common statistics on histograms. How to proceed? His class expects the histogram as a sortedMap (I was just using an array from 0 to 250). package htsjdk.samtools.util;. import htsjdk.samtools.util.Histogram.Bin;. /**; - Class for computing and accessing histogram type data. Stored internally in; - a sorted Map so that keys can be iterated in order.; *; - @author Tim Fennell; */; public class Histogram",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/540#issuecomment-115278716:623,access,accessing,623,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/540#issuecomment-115278716,1,['access'],['accessing']
Security,@akiezun this will sound dumb because you'd assume i have access to the internal infrastructure but i've been All Cloud since I got here. Are these files at these locations on the Isilon or do they reside elsewhere?. ```; bamIn=/dsde/working/akiezun/data/CEUTrio.HiSeq.WEx.b37.NA12892.bam; vcfIn=/dsde/working/akiezun/data/dbsnp_138.b37.excluding_sites_after_129.vcf; refIn=/dsde/working/akiezun/data/human_g1k_v37.fasta; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1609#issuecomment-227226948:58,access,access,58,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1609#issuecomment-227226948,1,['access'],['access']
Security,@alanhoyle Can you tell us whether the 400 Bad Request error is repeatable -- did you see it more than once? Oftentimes when accessing cloud data we encounter transient errors like this that go away on their own.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6926#issuecomment-724225557:125,access,accessing,125,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6926#issuecomment-724225557,1,['access'],['accessing']
Security,"@ashwini06 . This bam appears to be malformed and it fails Picard ValidateSamFile. I think you'll need to examine the earlier stages of your pipeline that produce your bam to ensure you get a correctly formed bam. I'm going to close this ticket now since this doesn't appear to be an issue with Mutect2. (base) wm462-624:Downloads fleharty$ java -jar $PICARD ValidateSamFile I=concatenated_ACC5611A1_XXXXXX_consensusalign_ds.bam ; INFO	2020-07-14 11:25:52	ValidateSamFile	. ********** NOTE: Picard's command line syntax is changing.; **********; ********** For more information, please see:; ********** https://github.com/broadinstitute/picard/wiki/Command-Line-Syntax-Transition-For-Users-(Pre-Transition); **********; ********** The command line looks like this in the new syntax:; **********; ********** ValidateSamFile -I concatenated_ACC5611A1_XXXXXX_consensusalign_ds.bam; **********. 11:25:52.673 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/fleharty/resources/picard.jar!/com/intel/gkl/native/libgkl_compression.dylib; [Tue Jul 14 11:25:52 EDT 2020] ValidateSamFile INPUT=concatenated_ACC5611A1_XXXXXX_consensusalign_ds.bam MODE=VERBOSE MAX_OUTPUT=100 IGNORE_WARNINGS=false VALIDATE_INDEX=true INDEX_VALIDATION_STRINGENCY=EXHAUSTIVE IS_BISULFITE_SEQUENCED=false MAX_OPEN_TEMP_FILES=8000 SKIP_MATE_VALIDATION=false VERBOSITY=INFO QUIET=false VALIDATION_STRINGENCY=STRICT COMPRESSION_LEVEL=5 MAX_RECORDS_IN_RAM=500000 CREATE_INDEX=false CREATE_MD5_FILE=false GA4GH_CLIENT_SECRETS=client_secrets.json USE_JDK_DEFLATER=false USE_JDK_INFLATER=false; [Tue Jul 14 11:25:52 EDT 2020] Executing as fleharty@wm462-624 on Mac OS X 10.15.5 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_191-b12; Deflater: Intel; Inflater: Intel; Provider GCS is not available; Picard version: 2.20.4-SNAPSHOT; WARNING	2020-07-14 11:25:52	ValidateSamFile	NM validation cannot be performed without the reference. All other validations will still occur.; ERROR: Record 18321, Read name U",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6695#issuecomment-658247132:66,Validat,ValidateSamFile,66,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6695#issuecomment-658247132,4,['Validat'],['ValidateSamFile']
Security,"@asmirnov239 looks like you are getting an NPE---remember that intervals are resolved after argument validation, so you need to do the check later. Also, good point, I think you can get a singleton after scattering if you get unlucky with your shards. ; Perhaps change the check to a filtering step in GermlineCNVCaller?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6559#issuecomment-617310137:101,validat,validation,101,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6559#issuecomment-617310137,1,['validat'],['validation']
Security,"@bbimber - here some answers:. * The `LocusWalker` in the GATK4 framework provides the traversal over each locus, and provides the information for the reference (`ReferenceContext`) and the pileup (`AlignmentContext`). I guess that this is similar to a rod-walker in the previous framework if a `FeatureInput` is present (accessed by the `FeatureContext`); * The apply function params are never null, but they may be empty. The reference context could be tested if it is empty by using `hasBackingDataSource()`. Other contexts can be tested with other methods to check if they are empty.; * For auto-discovery of feature inputs, mark the field with `FeatureInput<>` for the type of feature that you want; if you require more than one, a list of `FeatureInput` can be provided. Then, the features for the feature class can be retrieved from the `FeatureContext` on the apply function.; * For argument exceptions, use the ones in `UserException` or `CommandLineProgramException`, depending on which one fits better on your use case... If you wanna check an example of a locus walker with features, see [`ExampleLocusWalker`](https://github.com/broadinstitute/gatk/blob/4ef87ca10c3b57af76d829995e532c279b17cff2/src/main/java/org/broadinstitute/hellbender/tools/examples/ExampleLocusWalker.java). I hope that it helps!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/616#issuecomment-337598527:322,access,accessed,322,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/616#issuecomment-337598527,1,['access'],['accessed']
Security,"@bbimber Hmn, yeah, think it needs someone who has direct write access. I'll get a thumb from a teammate. Thanks for looking at it and for the pr!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8752#issuecomment-2023092567:64,access,access,64,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8752#issuecomment-2023092567,1,['access'],['access']
Security,"@bbimber I was hoping this could be reduced to a single new `File, PedigreeValidationType` constructor overload, and the new `File` getter (and without any changes to the existing subclasses). Its also not a perfect solution, but I'd prefer to minimize addition of any new methods that expose founderIDs or SampleDB, since we aspire to factor out the existing code that uses those from this class completely. As for the failed test, it looks like the tests timed out for some reason, hopefully transient, but it I'm guessing its unrelated.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7277#issuecomment-855914785:286,expose,expose,286,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7277#issuecomment-855914785,1,['expose'],['expose']
Security,"@bbimber Thanks -- I think we'll upload the files to a public Google storage bucket instead, to avoid the need for sharing passwords out in the open like this. We should be able to get to this after the GATK point release goes out tomorrow. I'll make a post here once they're uploaded.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/616#issuecomment-360580547:123,password,passwords,123,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/616#issuecomment-360580547,1,['password'],['passwords']
Security,"@bbimber We just added full versions of the B37 and HG38 references to the repo a couple of days ago. You'll have to rebase this branch on current master to access them, but it might make it easier to port some of the GATK3 tests that use b37.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-430743842:157,access,access,157,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-430743842,1,['access'],['access']
Security,@beginner984 Could you explain what is happening in greater detail. I assume you mean that the .bam file was totally empty without even a header? Are you sure the task ran to completion? Do you have to command line output from the tool anywhere? What version of gatk are you running off of? Are you sure that the input file involved passes validation? . I would suggest that you direct this and other questions of this nature to the [gatk forums](https://gatkforums.broadinstitute.org/gatk/categories/gatk-support-forum). There are people on there who can help you get your tools up an running with the gatk and its tools and might have answers to your problem. Thank You.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1394#issuecomment-474026929:340,validat,validation,340,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1394#issuecomment-474026929,1,['validat'],['validation']
Security,"@bensprung So I thought this would be a trivial change. It turns out that encoding the Genotype as something like `1/1` is done way down in the depths of the VCF encoder and isn't exposed in an accessible way. It's going to need a (hopefully simple) change to the underlying htsjdk library to expose that machinery. It shouldn't be hard, it just means it will take a bit longer to get to than I expected.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8160#issuecomment-1397695685:180,expose,exposed,180,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8160#issuecomment-1397695685,3,"['access', 'expose']","['accessible', 'expose', 'exposed']"
Security,"@bhanugandham To get around this issue, the user can run `ValidateVariants` with the `--validation-type-to-exclude ALLELES` argument.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6630#issuecomment-640713793:58,Validat,ValidateVariants,58,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6630#issuecomment-640713793,2,"['Validat', 'validat']","['ValidateVariants', 'validation-type-to-exclude']"
Security,"@bshifaw related to what Sam was saying - we also have a few standard resources needed to run the workflows that we would like to share with users. What is the standard procedure for doing so? Ideally they would be bundled with featured workspaces, but also accessible from outside of Terra",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6017#issuecomment-506504159:258,access,accessible,258,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6017#issuecomment-506504159,1,['access'],['accessible']
Security,"@byoo The easiest thing would be if you can upload it to google cloud and make it publicly visible. Then we can copy it over and you can delete it. Or if you can share your google account name I can grant you upload permission on a bucket we own. (If you want to not publish it to the world you can email it to me louisb@broadinstitute.org ) . Alternatively, if you can't use google cloud, you could upload it to the gatk ftp site. See this article here about how to connect to upload: https://gatkforums.broadinstitute.org/gatk/discussion/1215/how-can-i-access-the-gsa-public-ftp-server.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-465237693:555,access,access-the-gsa-public-ftp-server,555,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-465237693,1,['access'],['access-the-gsa-public-ftp-server']
Security,"@chandrans I don't have access to dsde-docs so I can't see the ticket/test files (we asked @vdauwera to give me access last week for a different issue, but I don't have it yet).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4525#issuecomment-377963524:24,access,access,24,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4525#issuecomment-377963524,2,['access'],['access']
Security,"@chapmanb We were able to reproduce a failure with your command line. This looks like an issue related to JNI and garbage collection that is exposed by setting `-Xmx46965m` and `-XX:+UseSerialGC`, but it needs further debugging. To confirm, can you please try running without specifying these javaOptions? Something like this:; ```; ./gatk-launch --javaOptions '-Djava.io.tmpdir=$TEMP_DIR' \; ApplyBQSRSpark \; --sparkMaster local[16] \; --input $BAM_IN \; --output $BAM_OUT \; --bqsr_recal_file $BQSR_RECAL \; -- \; --conf spark.local.dir=$SPARK_LOCAL_DIR; ```. FYI, we see better performance from Spark when using an SSD for spark.local.dir. The `--conf ` option above shows how to set the spark.local.dir.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3605#issuecomment-332370070:141,expose,exposed,141,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3605#issuecomment-332370070,1,['expose'],['exposed']
Security,"@cmnbroad : first - would it be possible to kick off travis tests? i refactored this and dont seem to be able to do that. Second, yes, I was trying to reorder and condense the commits but clearly didnt work. I think the problem was trying to put your GATK3 commit first (which would seem to make sense). in any case, I just recreated this, putting a pristine GATK3 first, following a consolidated set of my commits with 1) the limited core changes, 2) the meat of the VariantEval port, and 3) A separate commit with a port of GATK3 VariantEvalIntegrationTest which is useful for validation but should not be merged. To your points:. 1) I substantially cut down the incoming large files, mostly by limiting the intervals of new large VCFs. 2) On the plugin: this was discussed above, and I initially also pointed out this should ultimately go into Barclay. You are actually the one who proposed staging it in GATK. I am not entirely sure I understand the reticence on plugins; however, my goal is to get VariantEval ported by touching as little of it as possible. This is already sucking up a ton of time. I flipped VariantEvalUtils to gather a list of classes from the appropriate package instead of a full-on plugin. That should satisfy that concern?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-431913735:579,validat,validation,579,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-431913735,1,['validat'],['validation']
Security,@cmnbroad @SHuang-Broad . The cluster uses Kerberos for authentication. This style of pathname works for reading the cram file which is on the hdfs file system. . Using the hadoop shell works fine.... ; hadoop fs -ls hdfs:///project/casa/gcad/adsp.cc; Found 2 items; drwxrwxr-x - zhucc casa 0 2018-04-27 14:59 hdfs:///project/casa/gcad/adsp.cc/cram; drwxrwxr-x - farrell casa 0 2018-05-08 15:21 hdfs:///project/casa/gcad/adsp.cc/sv. When I change this to a local file a similar error occurs. The program runs for 40 plus minutes and then gets the following error. . ```; 2019-05-19 19:09:41 INFO TaskSetManager:54 - Finished task 92.0 in stage 13.0 (TID 68093) in 1108 ms on scc-q01.scc.bu.edu (executor 24) (101/116); 2019-05-19 19:09:41 INFO TaskSetManager:54 - Finished task 101.0 in stage 13.0 (TID 68102) in 1061 ms on scc-q01.scc.bu.edu (executor 6) (102/116); 2019-05-19 19:09:41 INFO TaskSetManager:54 - Finished task 34.0 in stage 13.0 (TID 68035) in 1653 ms on scc-q01.scc.bu.edu (executor 24) (103/116); 2019-05-19 19:09:41 INFO TaskSetManager:54 - Finished task 44.0 in stage 13.0 (TID 68045) in 1553 ms on scc-q07.scc.bu.edu (executor 7) (104/116); 2019-05-19 19:09:41 INFO TaskSetManager:54 - Finished task 63.0 in stage 13.0 (TID 68064) in 1362 ms on scc-q01.scc.bu.edu (executor 24) (105/116); 2019-05-19 19:09:41 INFO TaskSetManager:54 - Finished task 102.0 in stage 13.0 (TID 68103) in 1057 ms on scc-q07.scc.bu.edu (executor 7) (106/116); 2019-05-19 19:09:41 INFO TaskSetManager:54 - Finished task 39.0 in stage 13.0 (TID 68040) in 1604 ms on scc-q06.scc.bu.edu (executor 23) (107/116); 2019-05-19 19:09:41 INFO TaskSetManager:54 - Finished task 5.0 in stage 13.0 (TID 68006) in 2015 ms on scc-q01.scc.bu.edu (executor 24) (108/116); 2019-05-19 19:09:41 INFO TaskSetManager:54 - Finished task 10.0 in stage 13.0 (TID 68011) in 1928 ms on scc-q06.scc.bu.edu (executor 23) (109/116); 2019-05-19 19:09:41 INFO TaskSetManager:54 - Finished task 15.0 in stage 13.0 (TID 68016) in 1865 ms,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5942#issuecomment-494014590:56,authenticat,authentication,56,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5942#issuecomment-494014590,1,['authenticat'],['authentication']
Security,"@cmnbroad @lbergelson The cram index looks like it has all the info required to generate the splits without using the CramContainerIterator to look at the cram file directly. . Could using the crai index for splits be a potential solution to the glacially slow cram split generation? ; ; CRAM index. A CRAM index is a gzipped tab delimited file containing the following columns:; 1. Sequence id; 2. Alignment start; 3. Alignment span; 4. **Container start byte offset in the file**; 5. Slice start byte offset in the container data (‘blocks’); 6. Slice bytes; Each line represents a slice in the CRAM file. Please note that all slices must be listed in index file. In Hadoop-bam this code could read the crai instead of the cram to find the container boundaries. public List<InputSplit> getSplits(List<InputSplit> splits, Configuration conf); throws IOException {; // update splits to align with CRAM container boundaries; List<InputSplit> newSplits = new ArrayList<InputSplit>();; Map<Path, List<Long>> fileToOffsets = new HashMap<Path, List<Long>>();; for (InputSplit split : splits) {; FileSplit fileSplit = (FileSplit) split;; Path path = fileSplit.getPath();; List<Long> containerOffsets = fileToOffsets.get(path);; if (containerOffsets == null) {; containerOffsets = getContainerOffsets(conf, path);; fileToOffsets.put(path, containerOffsets);; }; long newStart = nextContainerOffset(containerOffsets, fileSplit.getStart());; long newEnd = nextContainerOffset(containerOffsets, fileSplit.getStart() +; fileSplit.getLength());; long newLength = newEnd - newStart;; if (newLength == 0) { // split is wholly within a container; continue;; }; FileSplit newSplit = new FileSplit(fileSplit.getPath(), newStart, newLength,; fileSplit.getLocations());; newSplits.add(newSplit);; }; return newSplits;; }",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4506#issuecomment-373078699:1024,Hash,HashMap,1024,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4506#issuecomment-373078699,1,['Hash'],['HashMap']
Security,"@cmnbroad After thinking about this I went ahead and created VariantEvalEngine. Doing this in one PR will simplify some of the sticking points around what is a final change vs. what it expected to be fixed later. With this change, the goal is to strip most logic from VariantEval into the engine. This engine can be constructed with a VariantEvalArgumentCollection, and any kind of GATKTool as the owner. I tried to minimize the amount of context the VariantEvalEngine needed to hang on to. This means all the child classes have visibility on the VariantEvalEngine, but are no longer directly exposed to either the walker class or the argument collection. . All the logic around gathering the arguments to form DrivingVariants is moved to a static method in VariantEvalEngine. . I also rebased and fixed conflicts.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-750428516:593,expose,exposed,593,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-750428516,1,['expose'],['exposed']
Security,"@cmnbroad Could you take a quick look at this again when you get a chance? I changed a few things in the untested methods to respond to @magicDGS's comments, but since they're so important and basically untested I think it would be good for someone to scan them. I change the behavior of failing customCommandLineValidation to always throw, so it's consistent failing regular command line validation as well. I also fixed some comment formatting.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2340#issuecomment-275150232:389,validat,validation,389,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2340#issuecomment-275150232,1,['validat'],['validation']
Security,"@cmnbroad I cleaned up some of the hashes and was able to create the conda environment locally. Can you try on your mac? We'll see if tests pass on Travis as well, then merge if all is good.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4061#issuecomment-355647907:35,hash,hashes,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4061#issuecomment-355647907,1,['hash'],['hashes']
Security,@cmnbroad I have added a validation/warning step to the pedigree annotations. It suffers from the issue where specifying both possibleDenovo and one of the other ped annotations will not affect warning between annotations. Since I'm choosing to only spit out warnings to the user this should probably be acceptable.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5663#issuecomment-466118658:25,validat,validation,25,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5663#issuecomment-466118658,1,['validat'],['validation']
Security,"@cmnbroad I saw your comment in https://github.com/broadinstitute/gatk/pull/7822. I'm trying to add WDL validation here, using the infrastructure in gradle. I was interested here in just validating the WDLs in the scripts directory.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7826#issuecomment-1116666702:104,validat,validation,104,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7826#issuecomment-1116666702,2,['validat'],"['validating', 'validation']"
Security,"@cmnbroad I started down this road. i wanted to make sure i follow your reasoning on some of this. I think you propose to change the tool arguments such that each input VCF is tagged by name (like -V:eval vcf1.vcf.gz -V:comp vcf2.vcf.gz), instead of different argument names. This is paired with a change to set the 'source' on each VariantContext to match the name of the source feature context. Unless I'm missing something, this basically makes everything identified by strings, with no direct FeatureInput <-> VariantContext reference, right? . Presumably, MultiVariantWalkerGroupedOnStart could implement something like:. protected Map<FeatureInput<VariantContext>, List<VariantContext>> groupVariantsByFeatureInput(List<VariantContext> variants) {; Map<String, FeatureInput<VariantContext>> sourceMap = new HashMap<>();; getDrivingVariantsFeatureInputs().forEach(x -> sourceMap.put(x.getName(), x));; ; Map<FeatureInput<VariantContext>, List<VariantContext>> ret = new HashMap<>();; variants.forEach(vc -> {; FeatureInput<VariantContext> fi = sourceMap.get(vc.getSource());; if (fi == null) {; //possibly throw? ; }; ; List<VariantContext> l = ret.getOrDefault(fi, new ArrayList<>());; l.add(vc);; ret.put(fi, l);; }); ; ; return ret;; }",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5439#issuecomment-730532600:813,Hash,HashMap,813,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5439#issuecomment-730532600,2,['Hash'],['HashMap']
Security,@cmnbroad I would harmonize the `PicardCommandLineProgram` option name with the `GATKTool` name. Possibly it should be extracted into a Validation stringency argument collection that can be used in both places.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1439#issuecomment-175158837:136,Validat,Validation,136,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1439#issuecomment-175158837,1,['Validat'],['Validation']
Security,"@cmnbroad OK - what about this proposal? I just added a protected getter and fixed the typo in 'annotation'? We could expose a constructor based way to set PedigreeValidationType, but if you dont really want to expose more of the guts of PedigreeAnnotation to subclasses prior to splitting apart founderIds and pedigree, what about keeping this as simple and minimal as possible?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7277#issuecomment-855970929:118,expose,expose,118,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7277#issuecomment-855970929,2,['expose'],['expose']
Security,"@cmnbroad OK, considerable progress here. I was able to adjust behavior such that only two tests have changed behavior from GATK4/master. I think this is now correct. One instance of changed behavior is the Snpeff/overlap one we discussed above. The second is the one where we now provide the full genome as REF, not the truncated genome. I think this difference is justified since the tool now requires a reference, and the prior version was arguably too lenient on validation of contigs. Anyway, this branch now also removes by debugging code and comments. I think it is ready for a review. To some other questions you had above:. 1) The HashMap<FeatureInput<VariantContext>, HashMap<String, Collection<VariantContext>>> can be wrapped in a class with just a couple of methods, so we don't have to manifest that long type all over the place. I realize that's non-optimal, but this isnt anything I introduced here. I would really like to keep this PR as limited as we can, and address some larger refactoring in a different PR, once we've migrated to MultiVariantWalkerGroupedOnStart. 2) I know this PR still in an interim state, but passing the VariantWalker in as an argument to the comp methods doesn't seem like a step forward to me. If we can't solve that problem completely in this PR (which is fine, I'm all for trying to contain this), are those changes necessary ? Perhaps that part should just wait for the next round. As noted above, I'd like to propose this as iterative, with a second PR coming soon. I did this b/c it moved us toward not needing to pass around the walker. It minimizes the code that has access to the walker (as opposed to setting it after creating the instance of the Evaluator, etc. Yes, it exposes it for two methods, but those classes no longer hang on to it. I would like to ultimately remove this entirely. 3) To re-iterate testEvalTrackWithoutGenotypesWithSampleFields: the input file, noGenotypes.vcf, has a header dictionary with the full set of contigs, and a",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-747619130:467,validat,validation,467,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-747619130,3,"['Hash', 'validat']","['HashMap', 'validation']"
Security,"@cmnbroad Sorry to bug here, but I am wondering if it would be possible for someone to review. This is a limited change that basically consolidates some internal code in PedigreeAnnotation and exposes a couple protected getters. Tests are passing. Thanks in advance.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7277#issuecomment-853234849:193,expose,exposes,193,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7277#issuecomment-853234849,1,['expose'],['exposes']
Security,"@cmnbroad Thank you for pointing out those build failures and even digging down to the apparent cause! I investigated and the issue wasn't inability to decompress gzip files (or at least wasn't only that), but XReadLines trims the lines by default and my code doesn't. The ""expected"" files have an extra tab at the end of some lines (the CHROM line for example) that this was picking up. What I've done is updated XReadLines so it can take Paths as input, so we get good matching behavior without having to duplicate code. While I was at it I also exposed XReadLines' ability to strip out comments, so assertEqualTextFiles didn't need to re-implement it anymore. Assuming Travis passes, this should be ready to review. I have the feeling we're getting close!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5378#issuecomment-456919065:548,expose,exposed,548,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5378#issuecomment-456919065,1,['expose'],['exposed']
Security,"@cmnbroad Thank you so much for the reply. I don't have a small test case for you, but I can provide some other information.; It is RNA seq data and passes validation check (`java -jar picard.jar ValidateSamFile I=S3_2.unmapped.split.bam MODE=SUMMARY`).; BaseRecalibrator cmd:; `gatk BaseRecalibrator -R Homo_sapiens_assembly38.fasta -I S3_2.unmapped.split.bam --use-original-qualities -O S3_2.unmapped.recal_data.csv -known-sites Homo_sapiens_assembly38.dbsnp138.vcf -known-sites Mills_and_1000G_gold_standard.indels.hg38.vcf.gz --known-sites Homo_sapiens_assembly38.known_indels.vcf.gz`; ApplyBQSR cmd:; `gatk ApplyBQSR -R Homo_sapiens_assembly38.fasta -I S3_2.unmapped.split.bam -O S3_2.unmapped.aligned.duplicates_marked.recalibrated.bam -bqsr S3_2.unmapped.recal_data.csv --add-output-sam-program-record --use-original-qualities`; RecalTables in S3_2.unmapped.recal_data.csv are empty. Here is the screen dump of BaseRecalibrator and ApplyBQSR.; BaseRecalibrator; ```; Using GATK jar <XXX>/gatk-4.1.4.1-83-g031c407-SNAPSHOT/gatk-package-4.1.4.1-83-g031c407-SNAPSHOT-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar <XXX>/gatk-4.1.4.1-83-g031c407-SNAPSHOT/gatk-package-4.1.4.1-83-g031c407-SNAPSHOT-local.jar BaseRecalibrator -R Homo_sapiens_assembly38.fasta -I S3_2.unmapped.split.bam --use-original-qualities -O S3_2.unmapped.recal_data.csv -known-sites Homo_sapiens_assembly38.dbsnp138.vcf -known-sites Mills_and_1000G_gold_standard.indels.hg38.vcf.gz --known-sites Homo_sapiens_assembly38.known_indels.vcf.gz; 23:39:34.668 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:<XXX>/gatk-4.1.4.1-83-g031c407-SNAPSHOT/gatk-package-4.1.4.1-83-g031c407-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 26, 2020 11:39:34 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6242#issuecomment-592005237:156,validat,validation,156,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6242#issuecomment-592005237,2,"['Validat', 'validat']","['ValidateSamFile', 'validation']"
Security,"@cmnbroad rebasing is done. To summarize changes since your last review:. - I backed out the earlier changes to FeatureInput/FeatureDataSource in favor of those from #7219 ; - I dont entirely know why this didnt hit before, but I made an update to VariantStratifiers to make tests pass. See: https://github.com/broadinstitute/gatk/pull/6973/commits/1569a909d3dc2301337e46441cc0cd969843c8d1. The gist is that we now instantiate those classes and pass VariantEvalEngine. Two of these classes had validation in their constructors, and could throw a CommandLineException if the tool was executed with bad arguments. This exception was getting caught and re-thrown as GATKException with the misleading message ""Problem making an instance of ...."". This proposal is to make a separate VariantStratifier.validateArgs() method, with a default no-op validation, and to call this only after instantiation. This was already exercised under the tests, such as testMultipleEvalTracksAlleleCountWithoutMerge(). VariantEval tests pass locally for me. With luck, tests will pass here.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-827805993:494,validat,validation,494,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-827805993,3,['validat'],"['validateArgs', 'validation']"
Security,"@cmnbroad 👍 to adding an advanced command line option for it. . @magicDGS Our goal is to make it unnecessary for normal users to ever need to see a stacktrace. We're definitely not at the point yet where every UserException produces either a) the complete information necessary to debug, or even b) the correct information. We're trying to fix all those cases, but there's a lot of possible failure modes between cloud access, spark, filesystem plugins, etc, so it's going to be an issue for a while. . I don't think it will hurt the user experience to have an extra commandline argument for it. Printing the stacktrace when debug is on isn't a bad idea, but I think it's better to have finer grained control over it. . The other issue is that it's easier to explain to an unsophisticated user how to set an extra commmand line argument rather than trying to get them to set the environment variable which well be helpful for our support team when they're trying to debug someone's problem. (especially since setting the environment variables may be different on a spark cluster than on a local run).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2443#issuecomment-285390394:419,access,access,419,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2443#issuecomment-285390394,1,['access'],['access']
Security,"@cmnbroad, I'm thinking about a use case for no filters allowed for the user of the tool. Imagine that you want to pull down some evidence for discordant read pairs (for instance, for SV), and the user provides a read filter which removes this kind of signatures. Could it be possible to generate an interface for the `ReadFilterArgumentCollection` and implementations for no-exposed/exposed to the final user?. This will be similar to the optional/required arguments for reads, reference and so on...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1900#issuecomment-225941325:376,expose,exposed,376,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1900#issuecomment-225941325,2,['expose'],['exposed']
Security,"@cmnbroad, sorry for the delayed response. I was in Taiwan giving a workshop last week and then I fell ill (run of the mill cold virus). The Comms team has been migrating issue tracking to a new system on Monday.com, which I am just now familiarizing myself to as I have been occupied with Taiwan workshop preparation. Forum questions are tracked in a separate system, Zendesk. The SOPs towards handling work for the two different systems are still under development so the best way to ensure you are up to date with the progress of work is to contact Robert @rcmajovski. The previous GitHub board that we used at https://github.com/broadinstitute/dsde-docs is still up and I still have my issue tickets here as I haven't had a chance to migrate these. . I do not know if you have access, but here is the link to track the issue ticket on Monday.com:; https://dsp-comms.monday.com/boards/145112271/. Just to orient you, if you click on the issue, a sideboard slides out from the right and you can comment on the work:; ![screenshot 2018-12-13 22 51 00](https://user-images.githubusercontent.com/11543866/49982483-c0040b80-ff2a-11e8-99a5-b6aae33d0311.png). It seems I've been assigned to update these documents. I'm unfamiliar with JEXL itself. I will survey the work that needs to be done and let you know.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5422#issuecomment-447206700:781,access,access,781,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5422#issuecomment-447206700,1,['access'],['access']
Security,"@cwhelan I was actually debating with myself about whether to include the initialization script here, as it was living in the bucket referred to in the creation script.; So we could do this:; always store the initialization script locally with the creation script instead of referring to a script living remotely, and makes that a required argument. The good: this makes it easier to track changes; The bad: initialization script must be removed from the bucket to avoid tracking possible different versions. A non-technical issue: we are ""delivering"" SGA in the initialization script, if that comes in to this repo, legal might have a problem with it. On the other hand, if the initialization script lives in a place only we can access, we are ""installing SGA for our own use"", which is not a problem with the GPL license.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2435#issuecomment-285093289:730,access,access,730,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2435#issuecomment-285093289,1,['access'],['access']
Security,"@cwhelan I've having problems with the non-Spark JAR though:. ``` bash; $ gradle clean installDist; $ java -jar build/libs/gatk-4.pre-alpha-*-SNAPSHOT.jar; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/logging/log4j/LogManager; at org.broadinstitute.hellbender.cmdline.ClassFinder.<clinit>(ClassFinder.java:29); at org.broadinstitute.hellbender.Main.extractCommandLineProgram(Main.java:108); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.main(Main.java:86); Caused by: java.lang.ClassNotFoundException: org.apache.logging.log4j.LogManager; at java.net.URLClassLoader$1.run(URLClassLoader.java:372); at java.net.URLClassLoader$1.run(URLClassLoader.java:361); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:360); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); ... 4 more; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1213#issuecomment-162013287:748,secur,security,748,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1213#issuecomment-162013287,2,"['Access', 'secur']","['AccessController', 'security']"
Security,@cwhelan could you give andrei and sam access to the FC dsde-methods-sv-dev workspace can investigate the bug. I and Steve had tried without success.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5217#issuecomment-424435081:39,access,access,39,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5217#issuecomment-424435081,1,['access'],['access']
Security,"@davidadamsphd Is this still an issue? Will it be resolved as part of your validation efforts, or do we have to address it separately?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1042#issuecomment-157482285:75,validat,validation,75,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1042#issuecomment-157482285,1,['validat'],['validation']
Security,"@davidbenjamin How's the patch coming? Did the M2 validation tests pass on your branch? We'll definitely try to expedite the code review, but I'll think we'll want some additional heavy-duty testing prior to release.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6533#issuecomment-610995045:50,validat,validation,50,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6533#issuecomment-610995045,1,['validat'],['validation']
Security,@davidbenjamin I have asked the user that reported this issue to share their file and will let you know as soon as I can get access.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6098#issuecomment-530197163:125,access,access,125,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6098#issuecomment-530197163,1,['access'],['access']
Security,"@davidbenjamin I made the requested changes and submitted novaseq validation jobs. They haven't failed yet, but I'll monitor the jobs and make changes to the wdl as needed. Will let you know when they finish.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4895#issuecomment-408528614:66,validat,validation,66,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4895#issuecomment-408528614,1,['validat'],['validation']
Security,"@davidbenjamin I've been looking at this with @nh13 and I think what's going on here is a little different that @nh13 described. Specifically when I try to reproduce this I get results very similar to those shown above, but with the MNP calling turned on I also get a second variant at `chr2:241815307`. I don't have access to the original calls @nh13 was looking at, but I suspect they may contain this call too. So I get the following with MNP support on (I'm not sure why I'm not getting them phased):. ```; chr2 241815307 . CA TG 962.73 . GT:AD:DP:GQ:PL 0/1:26,34:60:99:1000,0,758; chr2 241815308 . A G 2214.77 . GT:AD:DP:GQ:PL 1/1:1,60:61:99:1243,136,0; ```. This makes it look a lot like the issue described in #5523. I'm attaching ; ![an IGV screenshot of the region](https://user-images.githubusercontent.com/1609210/53673861-55b85880-3c47-11e9-9338-43c5ba40b5b6.png) and a SAM file, [NA24694.chr2.241815307.sam.gz](https://github.com/broadinstitute/gatk/files/2921535/NA24694.chr2.241815307.sam.gz), for the region shown that reproduces this issue.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5696#issuecomment-468859167:317,access,access,317,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5696#issuecomment-468859167,1,['access'],['access']
Security,"@davidbenjamin I've got a munrosa_bams_bugreport.tar.gz (2.1 MB) ready for you -- I'm trying to upload to the ftp side via the instructions [here](https://gatkforums.broadinstitute.org/gatk/discussion/1894/how-do-i-submit-a-detailed-bug-report), but I haven't been able to get access this morning due to the 20 user limit. Is there any other way I can send it over to you? I'd prefer not to post here.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3697#issuecomment-402257370:277,access,access,277,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3697#issuecomment-402257370,1,['access'],['access']
Security,"@davidbenjamin I've significantly refactored the production code, see the last commit. Most of this refactoring was to done make the code for the accounting of different modes (SNP/INDEL/both x BGMM/python x non/allele-specific) more minimal and straightforward. I've also combined the score/apply steps using the TwoPassVariantWalker. There's still lots of documentation, cleanup, and hardening/validation to be done, but most of the key methods and design choices have been documented, so I think it could be worth a quick review at this stage. Again, no need to nitpick code-style details, etc. (unless you really want to!) In the meantime, I'm going to do some more testing/tieout to make sure the refactor didn't break anything. This covers ~1800 LOC, which is roughly 50% of the equivalent VQSR code. Even modulo the remaining work just mentioned, which may add a few hundred LOC, I think this is a decent improvement---additional functionality, stability, etc. notwithstanding!. There's stubs for adding the truth-sensitivity conversion you proposed---should be pretty straightforward. I think it should also still be pretty easy for future pushes to add features like extraction/downsampling of unlabeled data, etc., but please do keep an eye out for design choices that may ultimately be constraining.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7659#issuecomment-1044836946:396,validat,validation,396,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7659#issuecomment-1044836946,1,['validat'],['validation']
Security,"@davidbenjamin Note that this is not exposed in CollectAllelicCounts either and is set to 30 by default. Our default set of read filters is also less stringent. However, we do expose a threshold on minimum base quality. Just a few more things to consider when we unify the two tools!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4011#issuecomment-354334621:37,expose,exposed,37,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4011#issuecomment-354334621,2,['expose'],"['expose', 'exposed']"
Security,"@davidbenjamin again, sorry to keep bugging on this thread, but it's been a while and we're really hoping to push these changes through since they're blocking a project. I believe I addressed everything in your review. I did identify another (arguably pre-existing) issue in GenotypGVCFs that would be exposed whenever it runs in all-sites mode or in force-output mode - the lack of allele trimming. This PR addresses that, including test cases, though I havent heard back about this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6406#issuecomment-581418093:302,expose,exposed,302,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6406#issuecomment-581418093,1,['expose'],['exposed']
Security,"@davidbenjamin at long last, back to you. I updated the nio wdl too, and it passes the Firecloud M2 wdl validation with the HCC sample, but not with the cram. But that's because that cram file is aligned to hg38, whereas the workspace uses hg19. I didn't touch anything related to the CramToBam task in the nio wdl so I think we're OK.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5599#issuecomment-474900012:104,validat,validation,104,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5599#issuecomment-474900012,1,['validat'],['validation']
Security,"@davidbenjamin mutect2_pon.wdl and mutect2.wdl worked great without docker installed. Thanks!. @samuelklee @sooheelee As a user, I found a json template useful for two reasons, though it may be up to how a wdl is written.; 1) womtool generates inputs from all the dependent wdls including unnecessary ones for the workflow. (e.g. mutect2_pon.wdl); 2) womtool didn't provide default values. Looking at mutect_resources.wdl, I wondered what the good value for minimum_allele_frequency is (or what GATK team used for creating the resource bundle). In addition, I thought a test to validate every wdl would be helpful. (womtool validate [a wdl])",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4188#issuecomment-360544256:578,validat,validate,578,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4188#issuecomment-360544256,2,['validat'],['validate']
Security,"@davidbenjamin, et al. I have two recommendations:. 1) Though I prefer to work symbolically and through proofs, it might be nice to first expand on the validation by proof in the JavaDoc - including for the specific function's header - and anywhere else where necessary across the GATK code, just for sanity's sake, and for tying things together neatly and properly. This process of always going through the mathematical steps alerts me when I code that I have not missed anything. . 2) When dealing with multiple levels of transformations, it probably would be good to formulate a collection of complete set of simple tests. Since like you mentioned {phased} is a subset (⊂) of {unphased}, then the paths of phased genotypes one works with would also be ideal to test on. Does this function have any validation tests confirming the correct likelihoods, which would be performed for both phased and unphased genotypes? These can be generated tests, if original files do not exists. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2019#issuecomment-236759221:152,validat,validation,152,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2019#issuecomment-236759221,2,['validat'],['validation']
Security,"@davidbernick thanks!. I noticed that the existing jobs are now failing with a GCS error (see https://gatk-jenkins.broadinstitute.org/job/gatk-perf-test-spark-markeddupe/436/console):. ```; Error code 404 trying to get security access token from Compute Engine metadata for the default service account. This may be because the virtual machine instance does not have permission scopes specified.; ```. There has been a change to the GCS library (https://github.com/broadinstitute/gatk/commit/b47838c9a5fa172ed6669ed4872b04d91c962a85), but when I ran a GCS pipeline manually on my machine it worked fine, even with this change. Any thoughts?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3573#issuecomment-329446325:219,secur,security,219,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3573#issuecomment-329446325,2,"['access', 'secur']","['access', 'security']"
Security,"@doazen I need to re-review this myself, and see what more validation I can do. I hate to miss the release, but I won't be able to do that today.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7402#issuecomment-952203649:59,validat,validation,59,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7402#issuecomment-952203649,1,['validat'],['validation']
Security,"@droazen +1 for being affected by this issue in production. As this is in production (same as @schelhorn , with big pharma which have very strict security requirements), and as 4.1.8.0 contains critical security vulnerabilities that were mitigated in subsequent releases, we are in a serious pickle here. @jhl667 how did you conclude that 4.1.8.0 performs better than newer versions? What we see is that it emits more variants, but after filtering and intersecting with other callers (i.e. Strelka), we get more variants and a ""better"" result (we can't really define ""better"" - it's merely an observation) with 4.2.4.1.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1407439000:146,secur,security,146,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1407439000,2,['secur'],['security']
Security,"@droazen , I was able to reproduce your result. I tried to isolate what made it work or not. I tried with two kinds of inputs:; - on the hellbender bucket, or; - on my own bucket. I tried with two choices for `GOOGLE_APPLICATION_CREDENTIALS`:; - default credentials, or; - my own. I tried with two different clusters:; - one created in the Broad project, or; - one created in my own project. With every one of those eight combinations, I got the same result: the dreaded ""Error code 404 trying to get security access token from Compute Engine metadata for the default service account."". ```; ./gatk-launch CountReadsSpark -I gs://hellbender/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -- --sparkRunner GCS --cluster jp-test-cluster --executor-cores 2 --num-executors 2; com.google.cloud.storage.StorageException: Error code 404 trying to get security access token from Compute Engine metadata for the default service account. This may be because the virtual machine instance does not have permission scopes specified.; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3855#issuecomment-352147413:501,secur,security,501,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3855#issuecomment-352147413,4,"['access', 'secur']","['access', 'security']"
Security,"@droazen - That won't be solved by the current #3447, because there is no way of fine-tune the codecs: I require to being able to add/remove concrete classes, and exclude codecs from a concrete package. An example is a custom codec implementation for some feature, to provide extra-validation for the downstream toolkit. This would be even more useful if HTSJDK is moving to an interface-based library...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2139#issuecomment-337622596:282,validat,validation,282,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2139#issuecomment-337622596,1,['validat'],['validation']
Security,@droazen : Thanks a lot for prioritizing and attending to this. The security posture has greatly improved from where we started. Community greatly benefits from your effort. I have migrated to using the 4.5 release after some regression testing. Below is a list of critical and high findings with 4.5 release. There are links to snyk version update recommendations. I know sometimes its not easy just to upgrade the library version as we could end up with run time errors. I am adding this here so that its handy when ever you look at this further. Thanks again. . packageName | version | severity | language | module_id; -- | -- | -- | -- | --; com.google.protobuf:protobuf-java | 3.7.1 | high | java | [SNYK-JAVA-COMGOOGLEPROTOBUF-2331703 ](https://security.snyk.io/vuln/SNYK-JAVA-COMGOOGLEPROTOBUF-2331703 ); com.google.protobuf:protobuf-java | 3.7.1 | high | java | [SNYK-JAVA-COMGOOGLEPROTOBUF-3167772](https://security.snyk.io/vuln/SNYK-JAVA-COMGOOGLEPROTOBUF-3167772); io.netty:netty-codec-http2 | 4.1.96.Final | high | java | [SNYK-JAVA-IONETTY-5953332](https://security.snyk.io/vuln/SNYK-JAVA-IONETTY-5953332); log4j:log4j | 1.2.17 | high | java | [SNYK-JAVA-LOG4J-2342645](https://security.snyk.io/vuln/SNYK-JAVA-LOG4J-2342645); log4j:log4j | 1.2.17 | high | java | [SNYK-JAVA-LOG4J-2342646](https://security.snyk.io/vuln/SNYK-JAVA-LOG4J-2342646); log4j:log4j | 1.2.17 | high | java | [SNYK-JAVA-LOG4J-2342647](https://security.snyk.io/vuln/SNYK-JAVA-LOG4J-2342647); log4j:log4j | 1.2.17 | critical | java | [SNYK-JAVA-LOG4J-572732](https://security.snyk.io/vuln/SNYK-JAVA-LOG4J-572732); net.minidev:json-smart | 1.3.2 | high | java | [SNYK-JAVA-NETMINIDEV-3369748](https://security.snyk.io/vuln/SNYK-JAVA-NETMINIDEV-3369748); org.apache.zookeeper:zookeeper | 3.6.3 | high | java | [SNYK-JAVA-ORGAPACHEZOOKEEPER-5961102](https://security.snyk.io/vuln/SNYK-JAVA-ORGAPACHEZOOKEEPER-5961102); org.codehaus.jettison:jettison | 1.1 | high | java | [SNYK-JAVA-ORGCODEHAUSJETTISON-3168085](https://,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8215#issuecomment-1890593067:68,secur,security,68,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8215#issuecomment-1890593067,3,['secur'],['security']
Security,"@droazen @cmnbroad @mbabadi I generally agree with the sentiments expressed in #4127, except that I think it's OK to require a conda environment (or even use of the Docker) for these particular tools. How we should validate this requirement is another question. We can discuss more with @vdauwera. @stefandiederich Hopefully once you get the conda environment set up you will be able to run the tools. We would definitely appreciate any feedback you might be able to provide. Note that the gCNV model is relatively sophisticated, so there may be some parameters (which control the priors for the model as well as how inference is performed) that you will need to adjust for your data. Depending on the number of intervals/bins you are using and your memory constraints, you may also need to scatter across multiple GermlineCNVCaller runs; see how things are done in the WDLs here: https://github.com/broadinstitute/gatk/tree/master/scripts/cnv_wdl/germline. As you noted, this pipeline is still in beta. We are currently running several evaluations and hope to soon release some Best Practices recommendations for the aforementioned parameter values that should work well for various data types generated at the Broad. We will also have some blog or forum posts that explain the new CNV pipelines in more detail coming soon---stay tuned!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4125#issuecomment-357034364:215,validat,validate,215,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4125#issuecomment-357034364,1,['validat'],['validate']
Security,"@droazen @davidbenjamin any thoughts regarding the last bullet above in https://github.com/broadinstitute/gatk/pull/6885#issuecomment-891907471 on possible integration tests? Started looking at this today and was wondering if you might have any suggestions. Ideally, we'd want to test that the exposure was done correctly through the 3 affected tools: HaplotypeCaller, Mutect2, and FilterAlignmentArtifacts. I can certainly take the approach outlined above and 1) on master, pick one or more integration tests for each tool, then generate results by changing the original unexposed constants and running on the relevant test data, 2) on this branch, commit those new results, then add corresponding versions of the integration tests that change the exposed inputs and check against the results. However, not sure if we'll want to clutter the repo with more test files just for this sort of exposing of constants, and such tests don't really feel complete anyway. So alternatively, I could probably write a script to do essentially the same thing and just check consistency between the branches for a bunch of randomly generated SW parameter values, perhaps also running on more substantial test files for each tool. I can document this process and then we can move on without committing any new tests or test files once we're satisfied that the exposure was done correctly. Or if you guys have additional suggestions, would be glad to hear them!. Finally, it looks like FilterAlignmentArtifacts doesn't have any integration tests for correctness---let me know if there are auxiliary tests we'd want to run there. Anyway, probably overthinking things, but the exposure was enough of a headache that I want to make sure I did it right. But would also rather fully hash out what to do beforehand, so I don't end up having to redo things after review.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6885#issuecomment-896314077:749,expose,exposed,749,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6885#issuecomment-896314077,2,"['expose', 'hash']","['exposed', 'hash']"
Security,@droazen @lbergelson -- this is blocking the JG run. I can get you access to the underlying data if necessary but hoping the stack trace will point to something obvious,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2713#issuecomment-301329590:67,access,access,67,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2713#issuecomment-301329590,1,['access'],['access']
Security,"@droazen @lbergelson I'm not really sure how to do this. It's easy in SVN, but not in git. We would need to insert the version info in the labels of the dockerfile when we cut a release -- the release version (e.g. 4.beta.4) not the git hash. How is GotC doing it?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3645#issuecomment-333543566:237,hash,hash,237,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3645#issuecomment-333543566,1,['hash'],['hash']
Security,"@droazen Applied your steps, I hope correctly - the pull request looks clean now. Your steps were a huge help. . The travis build looks like failing now, for reasons not obviously connected with our commit:. `Error: (converted from warning) unable to access index for repository http://cran.mtu.edu/src/contrib; Execution halted; The command ""if [[ $TEST_DOCKER != true ]]; then sudo mkdir -p /usr/local/lib/R/; sudo mkdir -p site-library; sudo ln -sFv ~/site-library /usr/local/lib/R/site-library; sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys E298A3A825C0D65DFD57CBB651716619E084DAB9; sudo add-apt-repository ""deb http://cran.rstudio.com/bin/linux/ubuntu trusty/""; sudo apt-get update; sudo apt-get install -y --force-yes r-base-dev=3.1.3-1trusty; sudo apt-get install -y --force-yes r-base-core=3.1.3-1trusty; sudo Rscript scripts/docker/gatkbase/install_R_packages.R; fi;"" failed and exited with 1 during .`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-437922888:251,access,access,251,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-437922888,1,['access'],['access']
Security,@droazen Didn't she validate the input bam though?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317047601:20,validat,validate,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317047601,1,['validat'],['validate']
Security,@droazen Hadoop-BAM has been released now so this is ready for review now whenever you can get to it. There are two other changes included that are unrelated to the tickets listed above:; - ValidateSAMFile has a change because the setValidateIndex API has been deprecated in htsjdk; - Includes a centralized DEFAULT_READ_VALIDATION_STRINGENCY value per our discussion.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1469#issuecomment-184224336:190,Validat,ValidateSAMFile,190,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1469#issuecomment-184224336,1,['Validat'],['ValidateSAMFile']
Security,"@droazen I am more confident that this method was not ported (or deleted, I suppose). It is used only once in gsa-unstable. The GATK 3 method `HaplotypeCallerGenotypingEngine::prepareReadAlleleLikelihoodsForAnnotation` is identical to the GATK 4 method `AssemblyBasedCallerGenotypingEngine::prepareReadAlleleLikelihoodsForAnnotation` except for the following lines that are missing in GATK 4:. ```java; if (call.getAlleles().size() != readAlleleLikelihoodsForAnnotations.alleleCount()) {; readAlleleLikelihoodsForAnnotations.updateNonRefAlleleLikelihoods(new IndexedAlleleList<>(new HashSet<>(call.getAlleles())));; }; ```. I can't think of a reason for this code to be removed, so why don't we restore those lines to `prepareReadAlleleLikelihoodsForAnnotation` and copy the GATK 3 code for `ReadLikelihoods. updateNonRefAlleleLikelihoods()`, which doesn't seem to exist or have any equivalent in GATK 4?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1950#issuecomment-300901327:583,Hash,HashSet,583,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1950#issuecomment-300901327,1,['Hash'],['HashSet']
Security,"@droazen I have a PR for gatk-bwa-mem that adds a footer to the image file so that we can test integrity. I also added code to test every (I think) call that returns an error indication, and pass this info up the chain. Could you review the PR or delegate, please?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3209#issuecomment-313504079:95,integrity,integrity,95,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3209#issuecomment-313504079,1,['integrity'],['integrity']
Security,"@droazen I have forward that question to GP and will get back to you once I get their answer. (At least, to my knowledge, I can't determine this without their help since I only have access to the output but not the pipeline.)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7755#issuecomment-1099616963:182,access,access,182,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7755#issuecomment-1099616963,1,['access'],['access']
Security,@droazen I just looked and it seems that the only other big one I added was `--disable-artificial-haplotype-recovery` and that one is very esoteric indeed and doesn't need to be exposed I don't think.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6737#issuecomment-668197410:178,expose,exposed,178,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6737#issuecomment-668197410,1,['expose'],['exposed']
Security,"@droazen I thought I had updated this months ago saying that I was finished validating, and that it was ready for review, but it appears that I didn't. But anyway its ready.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7402#issuecomment-1108948701:76,validat,validating,76,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7402#issuecomment-1108948701,1,['validat'],['validating']
Security,@droazen I would rather not get into why exactly the CNV tests are relying on bogus intervals that don't pass validation and what to do about it on this branch. I would rather get this version of things in now to help in at least the -L interval file use case,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7295#issuecomment-860948476:110,validat,validation,110,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7295#issuecomment-860948476,1,['validat'],['validation']
Security,@droazen I'm not authorized either.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3033#issuecomment-306542782:17,authoriz,authorized,17,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3033#issuecomment-306542782,1,['authoriz'],['authorized']
Security,@droazen No objection here. It may be that my changing the db access to read only fixed the issue.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4413#issuecomment-413247125:62,access,access,62,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4413#issuecomment-413247125,1,['access'],['access']
Security,"@droazen Per our discussion about whether the default stringency should be SILENT or STRICT, I re-ran the BaseRecalibrator integration tests (remember these were the only tests that failed with STRINGENCY=STRICT, at least non-Spark tests which is all that are affected by this PR) to see why they failed. There are about 15 tests, totalling about 10 bams used there that fail validation. Based on Picard validation, some have >100 errors, some have a handful; the most common errors are:. -Mate not found for paired read; -Mate Alignment start should be 0 because reference name = *; -Mapped mate should have mate reference name. There are also a handful of missing NM tags. I have the file-by-file breakdown if you want to see it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1439#issuecomment-174663312:376,validat,validation,376,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1439#issuecomment-174663312,2,['validat'],['validation']
Security,@droazen The PR is #6544. James has reviewed and requested a few more tests. It's working fine on validations including ~30 exomes and ~15 genomes.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6533#issuecomment-611223334:98,validat,validations,98,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6533#issuecomment-611223334,1,['validat'],['validations']
Security,"@droazen We could probably force it too, but it's easier to just have it build with an incomplete version number which is what the new commit does. If there's no tag it just uses the short-hash, and if there is no git describe --always --long (which is true with jgit for instance..) then it will build with ""version-unknown"".",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/196#issuecomment-77919740:189,hash,hash,189,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/196#issuecomment-77919740,1,['hash'],['hash']
Security,"@droazen Yes, please. Sorry for not catching this! Turns out that unpaired reads that pass all the M2 read filters and show evidence of a SNV are rare enough that they don't show up in any of the M2 validations.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5121#issuecomment-413611150:199,validat,validations,199,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5121#issuecomment-413611150,1,['validat'],['validations']
Security,"@droazen any thoughts how we should proceed here, if at all? @ldgauthier reminded me that this story was unfinished and is getting a little stale. @fleharty take note if we want to report progress on this front to our MalariaGEN collaborators. On my end, there are a couple of things to do:; - [x] rebase and resolve conflicts; - [x] change TSV input as discussed above; - [x] add doc strings for new arguments; - [x] add integration tests to make absolutely sure exposure was done correctly, perhaps? I'm open to discussion about how this should be done. Complete coverage here will be difficult and perhaps not worth the effort, but I can probably put in a few tests that make sure changing the hard-coded values in master and doing the same via the exposed parameters in this branch have the same effect on a few existing test cases. However, while I'm doing the last three, I wonder if we could run whatever canonical evaluations/optimizations we have to see whether it's worth consolidating some of the parameter sets at this stage? I think there's an argument for having at least two sets (haplotype-to-reference + read-to-haplotype), but I'm not sure how to justify having a separate set for dangling heads/tails. But also not sure which set the latter should be consolidated with---@jamesemery thoughts? Again, let me reiterate that it seems that many of these parameter values were chosen arbitrarily (or, if not, that the procedure for choosing them has been lost). As a start, you can see the results of some optimizations I did on the CHM mix on slide 15 at https://docs.google.com/presentation/d/1zGuquAZWSUQ-wNxp8D6HhGNjIaFcV0_X9WAS4LODbEo/edit?usp=sharing Here, I optimized over haplotype-to-reference + read-to-haplotype SW parameters on various metrics after variant normalization using vcfeval. These optimizations were done using the Bayesian optimization framework I prototyped long ago (see https://github.com/broadinstitute/gatk-evaluation/tree/master/pipeline-optimizer and http",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6885#issuecomment-891907471:752,expose,exposed,752,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6885#issuecomment-891907471,1,['expose'],['exposed']
Security,@droazen can you run it on GSC or maybe ask @jean-philippe-martin ? I put the bam file in the hellbender-validation/test-input/NA12878 bucket,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/995#issuecomment-170034661:105,validat,validation,105,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/995#issuecomment-170034661,1,['validat'],['validation']
Security,"@droazen it looked like it was going to work but then—. ```java; @Override; final protected ReferenceDataSource directlyAccessEngineReferenceDataSource() {; throw new GATKException(""Should never directly access the engine ReferenceDataSource in walker tool classes "" +; ""outside of the engine package. Walker tools should get their data via apply() instead."");; }; ```. Also, this is really not the responsibility of the tool class—it should be handled as part of the walker.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6512#issuecomment-618429411:204,access,access,204,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6512#issuecomment-618429411,1,['access'],['access']
Security,"@droazen looking into the ripping out a bit more, I think it's too disruptive for alpha. The recalibration table will change and it will require a more thorough validation. As this is a potentially results-changing change, I vote to move this past alpha. I have removed the indel calculations from the ApplyBQSR because that does not change any semantics. ; (i propose instead to nominate https://github.com/broadinstitute/gatk/issues/1078 for alpha - I can put that in very quickly)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1056#issuecomment-157051241:161,validat,validation,161,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1056#issuecomment-157051241,1,['validat'],['validation']
Security,@droazen sorry for a late response. I agree moving to java 17 would help. I do see that GATK itself is using the newer version of log4j but then its the transitive dependencies for the libraries used that bring in the older version of log4j. . this creates situations that the final compiled jar has both version of the log4j and this could create problems. . Gatk being a very useful tool gets integrated in multiple other tools and pipelines so in a way affecting the security posture of where its being used. The risk might be low being a standalone cli tool but its a very hard conversation with info security :) . May I ask for a ballpark ETA for the new version? Appreciate the work thats gone into this tool.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8215#issuecomment-1448897264:470,secur,security,470,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8215#issuecomment-1448897264,2,['secur'],['security']
Security,@droazen still giving error:. com.google.cloud.storage.StorageException: Error code 404 trying to get security access token from Compute Engine metadata for the default service account. This may be because the virtual machine instance does not have permission scopes specified.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3592#issuecomment-330918132:102,secur,security,102,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3592#issuecomment-330918132,2,"['access', 'secur']","['access', 'security']"
Security,"@droazen thanks for the quick response! Just to be clear, my concerns were about testing that I didn't somehow screw up the original behavior through the exposure, not just testing that *some* behavior was exposed. But message received---will keep things on the simple side!. Also, please see the plots in #5564 to get an idea of the effect on outputs, if you haven't already. Would appreciate any thoughts you might have on that thread!. Will try to get this done in the next day or two, thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6885#issuecomment-896328697:206,expose,exposed,206,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6885#issuecomment-896328697,1,['expose'],['exposed']
Security,"@droazen yes, at least to a point we have this worked out. we really needed something besides CombineGVCFs in order to scale, but GenomicsDB definitely is a new format and I hope the tool keeps getting fleshed out. One final question: is there a way to check the integrity of a GenomicsDB instance? With a VCF one could at least iterate it, or check that it's a valid (non-truncated) gzip file. . Thanks for the help - we can close the issue as well.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6688#issuecomment-684864223:263,integrity,integrity,263,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6688#issuecomment-684864223,1,['integrity'],['integrity']
Security,"@droazen, I think I have pushed most of the changes requested -. * Moved out `appendPathToDir` from BucketUtils to IOUtils; * `appendPathToDir` now uses Path.resolve() to append a given path to dir; * If a workspace already exists and `overExistingWorkspace` is false, a `UnableToCreateGenomicsDBWorkspace` exception is thrown while creating a GenomicsDB workspace.; * Made sure all paths passed to GenomicsDB are absolute.; * Introduced `gendb.hdfs:` and `gendb.gs:` URI schemes in addition to the existing `gendb:` scheme for identifying Cloud paths in GenomicsDB with unit testing for these new schemes.; * Added unit tests to test writing to GenomicsDB workspace/arrays to GCS and then reading/querying from the same GenomicsDB instance from GCS with validation.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5017#issuecomment-415611303:755,validat,validation,755,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5017#issuecomment-415611303,1,['validat'],['validation']
Security,"@droazen, the SAMRecord interface exposes getReferenceIndex and getMateReferenceIndex, so there is no question that the index has to be there. . What I was talking about what the optimization where when serializing, the [BAMRecordCodec only saves the index](https://github.com/samtools/htsjdk/blob/master/src/java/htsjdk/samtools/BAMRecordCodec.java#L131), and not the name. That is because if we have the header then we can go from the index to the name. If there is no header, then we can't do that optimization anymore and instead have to save those two fields for every read.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141251365:34,expose,exposes,34,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141251365,1,['expose'],['exposes']
Security,"@droazen, we have a free GCS account, so it is possible that Hadoop requires extra configuration for authenticating/connecting with the HELLBENDER travis service account. Can anyone help here? This the code we have for connecting to GCS via Hadoop. ```; hdfsFS gcs_connect(struct hdfsBuilder *builder, const std::string& working_dir) {; char *gcs_creds = getenv(""GOOGLE_APPLICATION_CREDENTIALS"");; if (gcs_creds) {; value = parse_json(gcs_creds, ""project_id""); // free value after hdfsBuilderConnect as it is shallow copied.; if (value) {; hdfsBuilderConfSetStr(builder, ""google.cloud.auth.service.account.enable"", ""true"");; hdfsBuilderConfSetStr(builder, ""google.cloud.auth.service.account.json.keyfile"", gcs_creds);; hdfsBuilderConfSetStr(builder, ""fs.gs.project.id"", value);; }; }. if (working_dir.empty()) {; hdfsBuilderConfSetStr(builder, ""fs.gs.working.dir"", ""/"");; } else {; hdfsBuilderConfSetStr(builder, ""fs.gs.working.dir"", working_dir.c_str());; }. // Default buffer sizes are huge in the GCS connector. GenomicsDB reads/writes in smaller chunks,; // so the buffer size can be made a little smaller.; hdfsBuilderConfSetStr(builder, ""fs.gs.io.buffersize.write"", ""262144"");. hdfsFS hdfs_handle = hdfsBuilderConnect(builder);; free(value);; return hdfs_handle;; }; ```. This is the error from Travis logs-; ```; Running Test: Test method testWriteToAndQueryFromGCS(org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImportIntegrationTest); hdfsBuilderConnect(forceNewInstance=1, nn=gs://hellbender-test-logs, port=0, kerbTicketCachePath=(NULL), userName=(NULL)) error:; java.io.IOException: Error getting access token from metadata server at: http://metadata/computeMetadata/v1/instance/service-accounts/default/token; at com.google.cloud.hadoop.util.CredentialFactory.getCredentialFromMetadataServiceAccount(CredentialFactory.java:210); at com.google.cloud.hadoop.util.CredentialConfiguration.getCredential(CredentialConfiguration.java:75); at com.google.cloud.hadoop.fs.gcs.GoogleHadoo",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5197#issuecomment-422915888:101,authenticat,authenticating,101,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5197#issuecomment-422915888,1,['authenticat'],['authenticating']
Security,"@droazen, will put some debug print statements in the two tests that are failing while authenticating with GCS and issue another pull request to _nalinigans_genomicsdb_uri_support_ branch. Hope that is OK. Thanks.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5197#issuecomment-422843915:87,authenticat,authenticating,87,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5197#issuecomment-422843915,1,['authenticat'],['authenticating']
Security,"@droazen,. Apologies for the delay in getting back to you. Given the nature of our work, it's essential that we address and remove any high and critical vulnerabilities, regardless of their real-world threat level. Ensuring our system remains secure is our top priority. Here is the pull request with the modifications to address the high and critical vulnerabilities: [#8950](https://github.com/broadinstitute/gatk/pull/8950). Please review and let me know if you have any feedback.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8215#issuecomment-2285999993:201,threat,threat,201,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8215#issuecomment-2285999993,2,"['secur', 'threat']","['secure', 'threat']"
Security,"@fleharty @avalind ; Sorry, something happened with my previous message.; But what I wrote previously was that I couldn't reproduce the same error message using Picard ValidateSamFile. I tried validating my bam file and I don't see any errors. Even the samtools flagstat option works fine on my bam file.; Please find the attached screenshots,. <img width=""704"" alt=""picard"" src=""https://user-images.githubusercontent.com/6302819/88064375-8023f200-cb6b-11ea-960e-bab93f79ff22.png"">. <img width=""289"" alt=""flagstst"" src=""https://user-images.githubusercontent.com/6302819/88064447-9631b280-cb6b-11ea-86ee-6c49f9111507.png"">. Do you still think my bam file is malformatted?. PS: @fleharty used Picard version (2.20.4-SNAPSHOT), whereas I used v.2.23.2; for running Picard ValidateSamFile.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6695#issuecomment-661884614:168,Validat,ValidateSamFile,168,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6695#issuecomment-661884614,3,"['Validat', 'validat']","['ValidateSamFile', 'validating']"
Security,"@fleharty It's line 810 in that class (https://github.com/samtools/htsjdk/blob/f15bc9d2c0297a1bde6b89aa95cf2dc45dfc567f/src/main/java/htsjdk/variant/vcf/AbstractVCFCodec.java#L810). We need to switch from calling `decodeInts()` to calling a method that tolerates and preserves missing values. A decision will need to be made about whether, for AD specifically, missing values should be replaced with 0 (which @ldgauthier said she'd be ok with), or passed through to the caller as '.' or null. If we choose to propagate the missing values back to the caller, we may need to do downstream work in GATK/Picard to modify tools to handle them, and also modify the HTSJDK accessor for the AD field to return list of `Integer` instead of array of `int`. If we replace the missing values with 0, we likely wouldn't have to patch any downstream code at all.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6744#issuecomment-682016997:666,access,accessor,666,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6744#issuecomment-682016997,1,['access'],['accessor']
Security,"@frank-y-liu Thanks for the pull request! Looks good to me except for a very minor nitpick about tabs. In general we always use spaces. You should be able to set your IDE to autoconvert them. . I'm happy to merge without the cloud tests. It's a security hazard to let pull request from forks have access to those tokens, so they don't get passed to builds from forks. We separate the clouds tests explicitly so they can be skipped without breaking the rest of the tests when this happens. Your code should not effect any of the cloud functionality so I'm not worried if those tests didn't run. We can give you direct push access as well if that's more convenient for you. Then the tests will all run.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1761#issuecomment-213575978:245,secur,security,245,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1761#issuecomment-213575978,3,"['access', 'secur']","['access', 'security']"
Security,@gbrandt6 @bhanugandham do we have access to gvcfs that reproduce this problem? just want something real to test a fix with.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6766#issuecomment-689720463:35,access,access,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6766#issuecomment-689720463,1,['access'],['access']
Security,@gmagoon I agree that it does look a lot like an off-by-one error. The genotype validator is complaining about seeing a reference allele that just happens to be the same as the previous VC. . @cwhelan can you take a look when you get a chance?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5336#issuecomment-431855929:80,validat,validator,80,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5336#issuecomment-431855929,1,['validat'],['validator']
Security,"@gokalpcelik I see that the bug exists in the updated code too. We can fix it, but would be good to have some dataset that can be used to validate. Any chance to ask the user to generate a small example?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8788#issuecomment-2073371655:138,validat,validate,138,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8788#issuecomment-2073371655,1,['validat'],['validate']
Security,"@gspowley I have neglected this for a while, to say the least. Here is a command line using publicly available data with paths on the Broad servers. Everyone at the Broad has read access to these files, FWIW. What should I do with the data?. ```bash; wgs_intervals=/seq/references/Homo_sapiens_assembly19/v1/variant_calling/wgs_calling_regions.v1.interval_list; hg19=/seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta; tumor_bam=/dsde/working/davidben/dream/synthetic/original_bams_symlinks/tumor_4.bam; tumor_sample=synthetic.challenge.set4.tumour; normal_bam=/dsde/working/davidben/dream/synthetic/original_bams_symlinks/normal_4.bam; normal_sample=synthetic.challenge.set4.normal; java -jar $gatk Mutect2 \; -R $hg19 \; -L $wgs_intervals \; -I $tumor_bam -tumor $tumor_sample \; -I $normal_bam -normal $normal_sample \; -O output.vcf; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2562#issuecomment-307436212:180,access,access,180,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2562#issuecomment-307436212,1,['access'],['access']
Security,"@gudeqing Thanks for reporting. `GatherBamFiles` is a bit of a tricky tool and it's easy to do the wrong thing with it accidentally. It's possible that there's an error in either how it was run or the input files but you definitely also could have discovered a bug. Samtools indexing it correctly would be point towards a bug, but I'd like you to check a few things first to be sure. . GatherBamFiles is dumb and just concatenates bam files so it's very picky about inputs. If the bam files in the input are not disjoint or they are specified out of order than `GatherBamFiles` will produce an invalid output which might manifest in indexing errors. It's also possible that the header specified didn't include all the contigs present in the collection of bams which could have a similar error result. Could you run ValidateSamFile on the result and report back what it says? If the file is out of order in some way or the header doesn't match it should report that.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6379#issuecomment-575680528:815,Validat,ValidateSamFile,815,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6379#issuecomment-575680528,1,['Validat'],['ValidateSamFile']
Security,"@ilyasoifer Is there any way I can access the original cram (or better yet, a small subset thereof consisting of just MT) that illustrates this issue) and the reference ? It might be hard to debug without that. If thats not possible, a few suggestions: can you try using PrintReads to write the original cram (I would try just MT) first to a cram, then to a sam, and also the original cram to a sam, and see how those compare? It would also be useful to see what that read looks like if you use samtools view on the ORIGINAL cram. Do you know what software/version was used to write the original cram ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8768#issuecomment-2045130095:35,access,access,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8768#issuecomment-2045130095,1,['access'],['access']
Security,@ilyasoifer cnorman@broadinstitute.org. And don't worry about doing the PrintReads conversions I requested - if I have access to the original file and the reference I can debug this directly.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8768#issuecomment-2045185628:119,access,access,119,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8768#issuecomment-2045185628,1,['access'],['access']
Security,"@jamesemery - i will also look into whether this file is somehow created by subsetting another during tests. in the meantime, would it be possible to also get these? i'm pretty confident this is the final list:. gsa-hpprojects\GATK\data\Comparisons\Validated\HapMap\3.3\genotypes_r27_nr.b37_fwd.vcf. and the following are in privateTestDir:. overlapTest.bed; PhaseByTransmission/PhaseByTransmission.IntegrationTest.goodFamilies.ped; PhaseByTransmission/PhaseByTransmission.IntegrationTest.TP.vcf; NA12878.HiSeq.WGS.b37_decoy.indel.recalibrated.vcf; yri.trio.gatk.ug.head.vcf; NA12878.HiSeq.WGS.b37_decoy.indel.recalibrated.vcf; Mills_and_1000G_gold_standard.indels.b37.sites.vcf; validationReportComp.vcf; validationReportComp.noGenotypes.vcf",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/616#issuecomment-366799537:249,Validat,Validated,249,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/616#issuecomment-366799537,3,"['Validat', 'validat']","['Validated', 'validationReportComp']"
Security,"@jamesemery : i have run all integration tests, and I think this is a complete list of the remaining files I can hopefully get:. /private/gatk-tools-private/src/test/resources/withSymbolic.b37.vcf; /private/gatk-tools-private/src/test/resources/PhaseByTransmission/PhaseByTransmission.IntegrationTest.TP.vcf ; /private/gatk-tools-private/src/test/resources/yri.trio.gatk_glftrio.intersection.annotated.filtered.chr1.vcf ; /private/gatk-tools-private/src/test/resources/NA12878.HiSeq.WGS.b37_decoy.indel.recalibrated.vcf ; /private/gatk-tools-private/src/test/resources/validationReportEval.noGenotypes.vcf ; /private/gatk-tools-private/src/test/resources/validationReportEval.vcf ; /private/gatk-tools-private/src/test/resources/ac0.vcf ; /humgen/gsa-hpprojects/GATK/data/Comparisons/Validated/HapMap/3.3/genotypes_r27_nr.b37_fwd.vcf; /humgen/gsa-hpprojects/GATK/data/Validation_Data/snpEff2.0.5.AFR.unfiltered.VariantAnnotator.output.vcf. Thanks.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/616#issuecomment-366146129:569,validat,validationReportEval,569,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/616#issuecomment-366146129,3,"['Validat', 'validat']","['Validated', 'validationReportEval']"
Security,"@jamesemery @droazen I've updated this branch to ensure all read and write paths to shared state in `GenotypeLikelihoodCalculators` is synchronized. I then wrote a little [test](https://github.com/broadinstitute/gatk/commit/3bb178746b1dd286f55ba77e6939e2104ced98d0) using `AlleleSubsettingUtils` to access `GenotypeLikelihoodCalculators` 10^6 times to see the effect of adding synchronization. R session (times are in millis):; ```; > without_sync = c(10166, 10049, 10306, 10059, 10165); > with_sync = c(10700, 10384, 9923, 10097, 10190); > t.test(without_sync, with_sync, paired=TRUE). 	Paired t-test. data: without_sync and with_sync; t = -0.70447, df = 4, p-value = 0.52; alternative hypothesis: true difference in means is not equal to 0; 95 percent confidence interval:; -542.5421 322.9421; sample estimates:; mean of the differences ; -109.8 ; ```. The p-value is not less than 0.05, so we can't reject the null hypothesis (that the mean times are the same). So adding synchronization doesn't seem to make any difference in this test. BTW, I noticed that `GenotypeLikelihoods` has synchronization, so there is some precedent for thread-safety using this means.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5071#issuecomment-426338479:299,access,access,299,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5071#issuecomment-426338479,1,['access'],['access']
Security,"@jamesemery Back to you, at long last. I adopted your suggestion of a proper search that doesn't revisit already-seen vertices and came up with a better way of seeding the ""good"" subgraph that is safe from your STR concern. As far as code is concerned it's a total rewrite — you can pretend the first PR commit doesn't exist. The new criterion for seeding the search is chains with good log odds on both ends and which are incident on a vertex with multiple good out-edges or multiple good in-edges. The rationale is that the adjacency of two bad edges may have good log odds (Suppose a bad edge comes in and two bad edges come out. One is a new error on top of the original error and one is the continuation of the original error) but two have two outgoing edges with good log odds requires an actual real variant. On our M2 validations this essentially no effect on sensitivity and a mild reduction in false positives. I will leave it to you (or to me when I don't have to work like a vampire) to investigate how well it interacts with junction trees. As a first step I wrote a basic unit test for the basic pathology of the old method.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6520#issuecomment-624265441:826,validat,validations,826,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6520#issuecomment-624265441,1,['validat'],['validations']
Security,"@jamesemery Can you rebase this branch onto latest master to resolve the conflicts? Recommend doing a local squash first given the number of commits here to make it less painful (by ""local squash"" I mean first `rebase -i` onto the hash of the first commit in the `git log` history that's not your own, and then rebase onto `origin/master`).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3803#issuecomment-359036049:231,hash,hash,231,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3803#issuecomment-359036049,1,['hash'],['hash']
Security,@jamesemery Could I get a 👍 on this from you. Anders reviewed it but it's not counting him since he doesn't have write access.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6668#issuecomment-646861071:119,access,access,119,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6668#issuecomment-646861071,1,['access'],['access']
Security,"@jamesemery I agree - all access (read and write) to `GenotypeLikelihoodCalculators` instance variables needs to be synchronized to make it safe. I think it would be sufficient to make `getInstance()` and `calculateGenotypeCountUsingTables()` synchronized. @droazen, are you concerned about performance for the Spark case? For the walker version, presumably the access is single-threaded, and hence [uncontended, which is very cheap](https://books.google.co.uk/books?id=mzgFCAAAQBAJ&pg=PA230&lpg=PA230&dq=java+uncontended+synchronization+goetz&source=bl&ots=7W4J807faW&sig=YALE1qdWoAUELPqLRhIedz-bZ20&hl=en&sa=X&ved=2ahUKEwj4jJeko8zdAhXVFsAKHazkBrcQ6AEwB3oECAIQAQ#v=onepage&q=java%20uncontended%20synchronization%20goetz&f=false). Another option would be to maintain a separate instance of `GenotypeLikelihoodCalculators` per genotyping engine. The size of the table is ploidy * alleles, so not too large?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5071#issuecomment-423546586:26,access,access,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5071#issuecomment-423546586,2,['access'],['access']
Security,@jamesemery Still some test failures https://storage.googleapis.com/hellbender-test-logs/build_reports/master_19811.2/tests/test/classes/org.broadinstitute.hellbender.tools.spark.validation.CompareDuplicatesSparkIntegrationTest.html#testOutputFile,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4894#issuecomment-397343622:179,validat,validation,179,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4894#issuecomment-397343622,1,['validat'],['validation']
Security,"@jamesemery What about supporting an initialize() method on VariantAnnotation? This is GATK3-like, and would be non-disruptive to existing code, since the interfaces could have a default no-op implementation? . /**; * Provides an opportunity to set up context; */; public void initialize(VariantAnnotatorEngine engine) {. }. Then we could address whether any context is appropriate to expose via methods on VariantAnnotatorEngine?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6930#issuecomment-754274008:385,expose,expose,385,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6930#issuecomment-754274008,1,['expose'],['expose']
Security,"@jamesemery and now the overview of the more complex changes:. - `AssemblyResultSet`: the code for adding and removing haplotypes based on pileup alleles has become a `void` method of this class, where it belongs. Here and elsewhere I introduce snappy variable and function named referring to ""good"" and ""bad"" alleles, which I find visually much clearer. The code is basically the same as before but somewhat streamified. I extracted a `makeHaplotypeWithInsertedEvent` method to eliminate some code duplication between GGA and pileup force-calling.; - `HaplotypeCallerEngine` and `Mutect2Engine`: Force-calling alleles are split into biallelic `Events`. Duplicated code for finding all pileup events, then sifting them into good event to force-call and bad events to remove is extracted as `PileupBasedAlleles.goodAndBadPileupEvents`. Computing `allVariationEvents` is much simpler because 1) it now uses `Event` instead of `VariantContext` and 2) `Event` overrides `equals` and `hashCode`.; - `PileupBasedAlleles`: `getPileupVariantContexts` and sorting into good and bad pileup variants has been unified into `goodAndBadPileupEvents()`. It has additionally been somewhat rewritten for conciseness. Also, instead of the somewhat kludgy method of making `VariantContext` with four temporary attributes, then filtering based on those attributes, it calculates the filtering status immediately and uses `Events`. Also fixed the somewhat-misleading use of the word `alt` to mean `SNP`.; - `AssemblyBasedCallerUtils`: `applyPileupEventsAsForcedAlleles`, along with several helper methods that it calls, has been moved into `AssemblyResultResult`, where it is now a void member method.; - `GATKVariantContextUtils` mainly just using `Event` instead of `VariantContext`, which simplifies the code for splitting a `VariantContext` into biallelics. After going through this exercise I realize that it's not actually so much. The diff's bark is worse than its bite. The overwhelming majority of changes are eit",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8332#issuecomment-1574175702:980,hash,hashCode,980,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8332#issuecomment-1574175702,1,['hash'],['hashCode']
Security,"@jamesemery sorry to bug on this topic, but I'm hoping to make a push early this year to fully migrate my lab off GATK3 . I looked more closely at the specific annotations we need to migrate. I decided that I will implement our walker, 'DiscvrVariantAnnotator', which is basically a light wrapper around VariantAnnotation. This will make it easier to spike in custom annotations. In that walker, I will override makeVariantAnnotations(). I will make a new marker interface for EngineAwareAnnotation, and test that on all the Annotation classes, and use this to inject FeatureManager. So no core GATK changes needed. I did find one thing I'd like to propose. You probably know PedigreeAnnotation is special-cased in GATK. Annotations that use it have automatic argument validation and have the SampleDB injected. Currently, PedigreeAnnotation is a subclass of InfoFieldAnnotation, so isnt available to GenotypeAnnotations. There doesnt appear to be a solid reason why. I tried to fix that and my best idea is the proposal here: #7041 . The core idea is to convert InfoFieldAnnotation and GenotypeAnnotation to interfaces. This is generally a trivial switch in existing code. With that, it becomes possible for classes that currently extend PedigreeAnnotation (which I switched to no longer extend InfoFieldAnnotation) to simply PedigreeAnnotation and implement InfoFieldAnnotation. This makes it possible for future classes to extend PedigreeAnnotation and implement GenotypeAnnotation.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6930#issuecomment-760424063:561,inject,inject,561,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6930#issuecomment-760424063,3,"['inject', 'validat']","['inject', 'injected', 'validation']"
Security,"@jamesemery: thank you. however, I am probably missing something obvious here. i tried to access this using 'gsutil cp'; however, i get an AccessDeniedException: bbimber@gmail.com does not have storage.objects.list access to variant-eval-test-data. Should I, and/or is there another way to access this?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/616#issuecomment-363220456:90,access,access,90,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/616#issuecomment-363220456,4,"['Access', 'access']","['AccessDeniedException', 'access']"
Security,"@jason-weirather Interesting. I have no trouble accessing the FTP site from outside the Broad. What kind of error message are you getting?. There is a new version from 3/29 that has several fixes in it, in addition you'll need to make sure you have the latest GATK code (you may need to pull the source code rather than a release - I'm not sure when the last release was and some fixes required both data source changes and code changes). If you can wait a few days we're planning on doing another minor / bugfix release this week. In general it's probably not worth trying to fix errors in the data sources - you may find yourself going down a rabbit hole. That said, one of the things that was fixed was that data source line in the gencode file. There shouldn't be very many __UNKNOWN__ fields (if any at all) - at least there aren't when running with the latest version of everything.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4521#issuecomment-383404016:48,access,accessing,48,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4521#issuecomment-383404016,1,['access'],['accessing']
Security,"@jean-philippe-martin ; Thanks for the input. I've checked both `gs://broad-dsde-methods-shuang/pb/bams/NA12892/` and `gs://broad-dsde-methods-sv/samples/G94797_CHM_MIX/WGS1/tmp`, they return something like this. ```; gs://broad-dsde-methods-shuang/pb/bams/NA12892/:; Creation time: Mon, 22 Apr 2019 16:14:50 GMT; Update time: Mon, 22 Apr 2019 16:14:50 GMT; Storage class: STANDARD; Content-Length: 11; Content-Type: text/plain; Hash (crc32c): XkI+Dw==; Hash (md5): apnFdauH+MfR7R5S5+NJzg==; ETag: CJekwKSM5OECEAE=; Generation: 1555949690032663; Metageneration: 1; ACL: [; {; ""entity"": ""project-owners-222581509023"",; ""projectTeam"": {; ""projectNumber"": ""222581509023"",; ""team"": ""owners""; },; ""role"": ""OWNER""; },; {; ""entity"": ""project-editors-222581509023"",; ""projectTeam"": {; ""projectNumber"": ""222581509023"",; ""team"": ""editors""; },; ""role"": ""OWNER""; },; {; ""entity"": ""project-viewers-222581509023"",; ""projectTeam"": {; ""projectNumber"": ""222581509023"",; ""team"": ""viewers""; },; ""role"": ""READER""; },; {; ""email"": ""shuang@broadinstitute.org"",; ""entity"": ""user-shuang@broadinstitute.org"",; ""role"": ""OWNER""; }; ]; ......; ......; ```. The line the `Content-Length: 11` seems to suggest you are right.; And if I run `gsutil ls -lh gs://broad-dsde-methods-shuang/pb/bams/NA12892/`, I get; ```; 11 B 2019-04-22T16:14:50Z gs://broad-dsde-methods-shuang/pb/bams/NA12892/; ......; ......; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5935#issuecomment-492762131:429,Hash,Hash,429,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5935#issuecomment-492762131,2,['Hash'],['Hash']
Security,"@jean-philippe-martin Can you comment on this error with your thoughts? Despite now doing a channel reopen on `UnknownHostException` in our fork of the NIO library, all reopens are failing, which implies that this error can't be recovered from via a simple retry. Could there be something wrong in our authentication setup?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-412906931:302,authenticat,authentication,302,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-412906931,1,['authenticat'],['authentication']
Security,"@jean-philippe-martin Can you comment on this one? It looks like `google-cloud-java` recently bumped their `google-auth-library-credentials` and `google-auth-library-oauth2-http` dependencies to `0.8.0` -- was there some change that would require us to modify our authentication-related code in GATK, and/or the permissions setup in our Google Cloud project, that could explain the error:. ```; Error code 404 trying to get security access token from Compute Engine metadata for the default service account. This may be because the virtual machine instance does not have permission scopes specified.; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3591#issuecomment-330936001:264,authenticat,authentication-related,264,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3591#issuecomment-330936001,3,"['access', 'authenticat', 'secur']","['access', 'authentication-related', 'security']"
Security,"@jean-philippe-martin I like your counter proposal in general for testing path integration. I think writing to GCS over NIO is an important enough feature that we should have at least 1 test in gatk that actually writes to a real GCS bucket in case there's ever an issue specifically with GCS (authentication issues are one potential problem I can imagine). . It seems like we should be able to design in a way that avoids collisions. What does `Files.createTempFile()` do with gcs? My guess is that it probably doesn't do the right thing, but maybe we could fix it so it would? Or use some sort of scheme with random UUID's like the methods in BucketUtils that we have already.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2558#issuecomment-332235140:294,authenticat,authentication,294,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2558#issuecomment-332235140,1,['authenticat'],['authentication']
Security,@jean-philippe-martin I think we can set up a repro by creating a new github project with a simple travis build that just does an NIO access. I don't think we can reproduce it locally since I'm pretty sure it's a bad interaction with the environment.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5928#issuecomment-516890466:134,access,access,134,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5928#issuecomment-516890466,1,['access'],['access']
Security,"@jean-philippe-martin Review complete. Looks good modulo the ReadSource stuff. I'm concerned about the reinvention of ReadsSource, so I think we should probably patch the validation stringency issue before pulling this into main. I see that you have a patch for it already, so that means we should probably review that quickly.. It does seem like you're doing a bunch of work which is very close to the existing `DataflowReadsPipeline`. It would be good to be able to unify those in the future so that future tools wont have to deal with getting reads and headers and the like. Have you considered filtering out unconvertible Reads with a filter applied before the ReadTransform step? It would decouple the issue of ""our conversion is broken"" from the implementation of BQSR at some cost in speed.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/556#issuecomment-110877031:171,validat,validation,171,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/556#issuecomment-110877031,1,['validat'],['validation']
Security,"@jean-philippe-martin Thank you for putting this together, but performing authentication and reading the first byte might be too small a test for running on the Cloud. Could you run a profile test using `gcloud-java-nio` with 100 GB, 500 GB, 1 TB, 10 TB of data and process it?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2013#issuecomment-233417619:74,authenticat,authentication,74,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2013#issuecomment-233417619,1,['authenticat'],['authentication']
Security,"@jean-philippe-martin The bug is in a piece of code that ISN'T using NIO, but is using some old code from the dataflow days to access the bam. I think that we can replace that code now that NIO is working and we should no longer need these special cases for GCS files.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2394#issuecomment-277832993:127,access,access,127,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2394#issuecomment-277832993,1,['access'],['access']
Security,@jean-philippe-martin What happens if someone tries to read a `gs:` path but they don't have the correct/any authentication?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2013#issuecomment-233445289:109,authenticat,authentication,109,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2013#issuecomment-233445289,1,['authenticat'],['authentication']
Security,"@jean-philippe-martin When you patch this one, could you also audit the rest of `CloudStorageReadChannel` for any other methods that could trigger a GCS access and require retries?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3253#issuecomment-314549425:62,audit,audit,62,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3253#issuecomment-314549425,2,"['access', 'audit']","['access', 'audit']"
Security,@jean-philippe-martin since you have push access to this repo you should be able to force travis to rerun a branch if you're logged in,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1662#issuecomment-207072471:42,access,access,42,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1662#issuecomment-207072471,1,['access'],['access']
Security,"@jean-philippe-martin, we want to expose a walker-like interface, but we also care about the general ease of writing programs using the tools we and the user wrote (in native Dataflow/Spark). This is a point that @droazen, has emphasized to me several times. I'll let him add more detail on this if needed. I agree that the static approach won't work for Dataflow when workers are added, but I think I have a solution for Spark that works even when workers are added.; We'd create a new class (like I suggested above), but this would have a `Broadcast<SAMFileHeader>`, which is basically a lazy-loader for headers. We'd only load the header when needed.; I think this may be the best of all solution as it could also support several headers.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141177047:34,expose,expose,34,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141177047,1,['expose'],['expose']
Security,"@jhl667 I'm looking into this. It looks like I neglected to set the sqlite connection to read only mode when connecting to the db file. I'm going to update it to do so. I'm not sure this applies when a read-only connection is created, but it looks like sqlite has some issues with NFS / distributed file systems:; - https://stackoverflow.com/questions/9907429/locking-sqlite-file-on-nfs-filesystem-possible ; - https://github.com/CGATOxford/CGATPipelines/issues/. One post in the github thread above mentions using `-o flock` when mounting Lustre partitions so that they all have concurrent locks. This _may_ be a workaround in the meantime. . I'll try to look at it on our NFS mounts - I don't have access to a Lustre fs.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4413#issuecomment-366009015:700,access,access,700,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4413#issuecomment-366009015,1,['access'],['access']
Security,"@jhl667, yep, I understand. Still, this issue is too big to be left alone. I think the Broad has to act here, since patient's lives are at stake and we didn't receive any actionable response for half a year. Mutect2 has a good reputation, and the Broad profits from that, but it is also medical software and it should be treated as such. Mutect2 will continue to be used for some time, and this has to be fixed. @droazen, is there anything else you people can do? Is the wider GATK dev team aware of the issue? Is this something I should escalate to someone else so that you get support from your management to fix it? We have pharma collabs with the Broad in place, I could try doing it that way, or via the genomics/pharma community. I'm not trying to be threatening or anything, just thinking out loud how we can help you to get the resources to solve this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1404845998:757,threat,threatening,757,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1404845998,1,['threat'],['threatening']
Security,"@jkobject Actually, we just noticed that your error is triggered by the file `gs://fc-secure-bd7b8bc9-f665-4269-997e-5a402088a369/5c2db926-3b1c-479c-9ed3-a99ce518de91/omics_mutect2/60955825-7723-4bc9-8202-bdd9975bb5c0/call-mutect2/Mutect2/7d737efc-c8be-4a6d-8803-4f786129521a/call-SplitIntervals/glob-0fc990c5ca95eebc97c4c204e3e303e1/0000-scattered.interval_list`, not the bam. Could you attempt the `gsutil` test on that file instead, and let us know what happens? Eg.,. ```; gsutil -u broad-firecloud-ccle cp gs://fc-secure-bd7b8bc9-f665-4269-997e-5a402088a369/5c2db926-3b1c-479c-9ed3-a99ce518de91/omics_mutect2/60955825-7723-4bc9-8202-bdd9975bb5c0/call-mutect2/Mutect2/7d737efc-c8be-4a6d-8803-4f786129521a/call-SplitIntervals/glob-0fc990c5ca95eebc97c4c204e3e303e1/0000-scattered.interval_list .; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7700#issuecomment-1064482965:86,secur,secure-,86,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7700#issuecomment-1064482965,2,['secur'],['secure-']
Security,"@jkobject Our testing of the nightly image with our own service account and billing project suggests that the requester pays access issue is resolved, so we're not sure what's causing it to continue to fail for you. As an experiment, could you try this: within the gatk-nightly image, try to access your requester pays file `gs://cclebams/wgs_hg38/CDS-0b4jFH.wgs_ccle.bam` using the `gsutil` command with the `-u` option. Eg., `gsutil -u broad-firecloud-ccle cp gs://cclebams/wgs_hg38/CDS-0b4jFH.wgs_ccle.bam .`, and report whether that succeeds. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7700#issuecomment-1064477058:125,access,access,125,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7700#issuecomment-1064477058,2,['access'],['access']
Security,"@jkobject That appears to be a different error: ""User project specified in the request is invalid"" instead of ""Bucket is a requester pays bucket but no user project provided"", which was the error this patch fixed. Can you confirm that the `broad-firecloud-ccle` project exists and is authorized under your service account? . @lbergelson Can you comment further on this?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7700#issuecomment-1064417440:284,authoriz,authorized,284,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7700#issuecomment-1064417440,1,['authoriz'],['authorized']
Security,"@john-alexander Just to be sure, you've checked that file is accessible to singularity?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6446#issuecomment-685813946:61,access,accessible,61,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6446#issuecomment-685813946,1,['access'],['accessible']
Security,"@jonn-smith I have sucessfully built GATK by these commands. git clone https://github.com/broadinstitute/gatk; gradlew bundle. I've got this zip file in folder ""build"".; gatk-4.0.4.0-34-g2cc7abd-SNAPSHOT.zip. So I unzipped and used this to replace GATK-4.0.4.0 that I downloaded from https://github.com/broadinstitute/gatk/releases/download/4.0.4.0/gatk-4.0.4.0.zip. I still found errors. 21:09:33.811 INFO ProgressMeter - Starting traversal; 21:09:33.811 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 21:09:46.267 INFO ProgressMeter - chr1:24929636 0.2 3000 14453.2; 21:09:59.072 INFO ProgressMeter - chr1:64681324 0.4 6000 14251.2; 21:10:09.456 INFO ProgressMeter - chr1:156245393 0.6 9000 15149.8; 21:10:21.510 INFO ProgressMeter - chr1:206965947 0.8 12000 15094.7; 21:10:26.132 INFO Funcotator - Shutting down engine; [May 23, 2018 9:10:26 PM ICT] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 1.36 minutes.; Runtime.totalMemory()=11500781568; java.lang.IllegalArgumentException: Genomic positions must be > 0.; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:722). What should I do? Can you send me one that is ready-to-use? Thanks.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-391371859:1134,validat,validateArg,1134,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-391371859,1,['validat'],['validateArg']
Security,"@jonn-smith Is there a forum post (or other docs) on how to setup a datasource for remote, NIO access? Do we make it clear that this only supports what the GATK supports?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5425#issuecomment-439976455:95,access,access,95,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5425#issuecomment-439976455,1,['access'],['access']
Security,"@kachulis Thanks for the report. The fix will be slightly complicated by the fact that there is also a (GATK-tool) level arg called `masterSequenceDictionary`. So in the override, we'll want to validate/resolve that argument against the others as well. If you need a short term workaround, you can try disabling on-they-fly indexing (`--create-output-variant-index false`, and then index the output separately using `IndexFeatureFile`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5087#issuecomment-411051092:194,validat,validate,194,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5087#issuecomment-411051092,1,['validat'],['validate']
Security,"@kcibul Does @jean-philippe-martin's suggestion above work for you, or are you still having authentication issues when running using a service account?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2425#issuecomment-284507997:92,authenticat,authentication,92,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2425#issuecomment-284507997,1,['authenticat'],['authentication']
Security,"@kcibul My reasoning for doing it in WDL is to better integrate with the process that creates the VAT table, and therefore make sure that the person running it has access (and knows the location of) not only to the VAT table but also the intermediary steps (e.g. the annotation JSON files that are output from NIRVANA). Not all of the validation steps need to be all bash; the first one was because it's literally just a call to make sure a table exists and has rows with `vid` values in it. Other rules (e.g. [rule #2](https://github.com/broadinstitute/dsp-spec-ops/issues/365)) will most likely need either python or jq to run.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7352#issuecomment-883457790:164,access,access,164,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7352#issuecomment-883457790,2,"['access', 'validat']","['access', 'validation']"
Security,@kdatta Why not use some kind of globally-unique identifier for the arrays if name collision is an issue (such as a hash or UUID)? Would that solve the problem?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3411#issuecomment-320325990:116,hash,hash,116,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3411#issuecomment-320325990,1,['hash'],['hash']
Security,"@kgururaj . So I found out that there are other fields that are also throwing a similar error with `vcf-validator`:. Below you will find this: `INFO tag [an_adj_exac_oth=16,16] expected different number of values (1)`. ```; # from the vcf-validator; INFO field at 4:2044128 .. INFO tag [af_exac_all=0] expected different number of values (expected 3, found 1),INFO tag [af_adj_exac_fin=0] expected different number of values (expected 3, found 1),INFO tag [an_adj_exac_oth=16,16] expected different number of values (1),INFO tag [an_adj_exac_nfe=202,202] expected different number of values (1),INFO tag [an_adj_exac_afr=20,20] expected different number of values (1),INFO tag [af_adj_exac_amr=0] expected different number of values (expected 3, found 1),INFO tag [an_exac_all=1246,1246] expected different number of values (1),INFO tag [an_adj_exac_amr=10,10] expected different number of values (1),INFO tag [af_adj_exac_oth=0] expected different number of values (expected 3, found 1),INFO tag [an_adj_exac_eas=30,30] expected different number of values (1),INFO tag [an_adj_exac_fin=2,2] expected different number of values (1),INFO tag [an_adj_exac_sas=966,966] expected different number of values (1),INFO tag [af_adj_exac_sas=0] expected different number of values (expected 3, found 1),INFO tag [af_adj_exac_afr=0] expected different number of values (expected 3, found 1),INFO tag [af_adj_exac_nfe=0] expected different number of values (expected 3, found 1),INFO tag [max_aaf_all=1] expected different number of values (expected 3, found 1),INFO tag [af_adj_exac_eas=0] expected different number of values (expected 3, found 1); ```. In this case, `an_adj_exac_oth` has > 1 values and only 1 is allowed:. ```; grep ; ##INFO=<ID=an_adj_exac_oth,Number=1,Type=Integer,Description=""Other Chromosome Count (from /mnt/isilon/cbmi/variome/bin/bcbio-nextgen/bcbio/gemini_data/ExAC.r0.3.sites.vep.tidy.vcf.gz)"">. # the corresponding variant in the vcf ; 4	2044128	.	C	T,CGCT,<NON_REF>	3476.7	.	DP=97",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5045#issuecomment-407497476:104,validat,validator,104,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5045#issuecomment-407497476,2,['validat'],['validator']
Security,"@kgururaj I ran the commands you suggested. . > [user@cedar5 bin]$ bash -x TestGenomicsDBJar/run_checks.sh; > + [[ hB != hxB ]]; > + XTRACE_STATE=-x; > + [[ hxB != hxB ]]; > + VERBOSE_STATE=+v; > + set +xv; > + unset XTRACE_STATE VERBOSE_STATE; > ++ uname -s; > + osname=Linux; > + jar xf genomicsdb--jar-with-dependencies.jar libtiledbgenomicsdb.so; > java.io.FileNotFoundException: genomicsdb--jar-with-dependencies.jar (No such file or directory); > at java.util.zip.ZipFile.open(Native Method); > at java.util.zip.ZipFile.<init>(ZipFile.java:219); > at java.util.zip.ZipFile.<init>(ZipFile.java:149); > at java.util.zip.ZipFile.<init>(ZipFile.java:120); > at sun.tools.jar.Main.extract(Main.java:1004); > at sun.tools.jar.Main.run(Main.java:305); > at sun.tools.jar.Main.main(Main.java:1288); > + jar xf genomicsdb--jar-with-dependencies.jar libtiledbgenomicsdb.dylib; > java.io.FileNotFoundException: genomicsdb--jar-with-dependencies.jar (No such file or directory); > at java.util.zip.ZipFile.open(Native Method); > at java.util.zip.ZipFile.<init>(ZipFile.java:219); > at java.util.zip.ZipFile.<init>(ZipFile.java:149); > at java.util.zip.ZipFile.<init>(ZipFile.java:120); > at sun.tools.jar.Main.extract(Main.java:1004); > at sun.tools.jar.Main.run(Main.java:305); > at sun.tools.jar.Main.main(Main.java:1288); > + '[' Linux == Darwin ']'; > + LIBRARY_SUFFIX=so; > + ldd libtiledbgenomicsdb.so; > ldd: ./libtiledbgenomicsdb.so: No such file or directory; > + md5sum libtiledbgenomicsdb.so; > md5sum: libtiledbgenomicsdb.so: No such file or directory. I'm using a compute canada server, so I don't have root access. The version of gatk4 I'm using was installed by their support team, and I load it using 'module load gatk'. I had that module loaded when I ran this test.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4124#issuecomment-357005071:1615,access,access,1615,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4124#issuecomment-357005071,1,['access'],['access']
Security,@knight2015 I'm sorry to say we don't support spark 3.0.0 a the moment. If you have access to a spark 2.4.x cluster I would try that.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6644#issuecomment-640701152:84,access,access,84,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6644#issuecomment-640701152,1,['access'],['access']
Security,@ksw9 can you run the vcf validator tool suggested by @komalsrathi in #5045 [here](https://github.com/broadinstitute/gatk/issues/5045#issuecomment-407476343)? The issue is primarily caused by mismatch in the field description in the VCF header and the data lines. @droazen can you comment on the [sanity check that I suggested here](https://github.com/broadinstitute/gatk/issues/5045#issuecomment-407501684)?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5113#issuecomment-413282882:26,validat,validator,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5113#issuecomment-413282882,1,['validat'],['validator']
Security,"@lbergelson @droazen Both of you committed changes to the Dockerfile recently, but as far as I can tell they are not security related. Should I keep this PR at [4.2.4.1](https://hub.docker.com/layers/broadinstitute/gatk/4.2.4.1/images/sha256-421d2fb2cc869249cef3f4d7a77289256d295b04ba623096228e0e5fd42939e9?context=explore)?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7611#issuecomment-1048281865:117,secur,security,117,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7611#issuecomment-1048281865,1,['secur'],['security']
Security,@lbergelson @gokalpcelik any chance of giving me access to the workspace for the 330 whole exomes?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8989#issuecomment-2433346908:49,access,access,49,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8989#issuecomment-2433346908,1,['access'],['access']
Security,"@lbergelson For the `isinf` error, let's try this change in common_data_structure.h. ```; - if (isinf(small) == -1 || isinf(big) == -1); + if (std::isinf(small) == -1 || std::isinf(big) == -1); ```. The gettime related errors are because mac doesn't support clock_gettime ([discussion here](http://stackoverflow.com/questions/5167269/clock-gettime-alternative-in-mac-os-x)). The code in util.cc is part of some unused sandbox code, which can be removed. Try this command to check for AVX support on mac. ```; sysctl -a | grep machdep.cpu.features; ```. I'll try to get access to a mac to help the build debug go faster.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1504#issuecomment-187840330:569,access,access,569,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1504#issuecomment-187840330,1,['access'],['access']
Security,"@lbergelson Hi, thank you for your kind reply. Following is the report of ValidateSamFile:; Tool returned: 3; ERROR::RECORD_OUT_OF_ORDER:Record 86076959, Read name A00583:183:HLCWVDMXX:1:1114:30897:16579, The record is out of [coordinate] order, prior read name [ST-E00159:680:H57NGCCX2:6:2201:21765:57301], prior coodinates [22:51244173]; ERROR::MATE_NOT_FOUND:Read name ST-E00159:680:H57NGCCX2:6:2208:30289:5300, Mate not found for paired read; ERROR::MATE_NOT_FOUND:Read name A00583:183:HLCWVDMXX:1:1236:31611:36793, Mate not found for paired read; ...... The Result really supprised me! However, mutect2 and haplotypecaller are ok with the input of the resulted bam indexed with samtools. I also found that the conclusion of ""Mate not found for paired read"" is True. The bam was generated by: fastq ->mergeBam( bwabam + (fastq->ubam) ) -> markdup+sortAndFixTags -> applyBQSR -> gatherBam. Steps of Bwa and BQSR were paralleled with Intervals. And, I am sure that all my inputs are in consistent order. Example of interval specified: ""-L chr21:1+ -L chr22:1+"". Looking foward to your reply.; Best Regards!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6379#issuecomment-575985383:74,Validat,ValidateSamFile,74,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6379#issuecomment-575985383,1,['Validat'],['ValidateSamFile']
Security,@lbergelson I added a second argument per your request. I still disagree that this is the right argument to expose because it is very dangerous.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5974#issuecomment-497383571:108,expose,expose,108,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5974#issuecomment-497383571,1,['expose'],['expose']
Security,"@lbergelson I added an integration test that writes to GCS... it doesn't work for me (""com.google.cloud.storage.StorageException (...) does not have storage.objects.get access to (...)""). This may be due to a misconfiguration on my end. I wonder if it'll work with Travis.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2558#issuecomment-334877523:169,access,access,169,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2558#issuecomment-334877523,1,['access'],['access']
Security,"@lbergelson I agree with you about moving tests up, though I would make the point that there were many tests that got pushed down in the first place because they would involve fixing bugs in MarkDuplicatesGATK that were already fixed in MarkDuplicatesSpark, I will audit the ones I did and didn't push up so i'm more confident there is a reason to have tests pushed down or not.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5166#issuecomment-419550902:265,audit,audit,265,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5166#issuecomment-419550902,1,['audit'],['audit']
Security,"@lbergelson I disagree -- it's very clear to me that those tests will trigger Google authentication, just by tracing through the code.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2706#issuecomment-300806909:85,authenticat,authentication,85,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2706#issuecomment-300806909,1,['authenticat'],['authentication']
Security,"@lbergelson I updated to the latest 2.x Mockito and that fixed the problem. The only failing test now is FuncotatorIntegrationTest#nonTrivialLargeDataValidationTest. The output VCF differs in the `FUNCOTATION` annotation, and it’s to do with ordering of the fields. E.g. it will be. ```; 1_%7C_1|false_%7C_false|false_%7C_false; ```; not; ```; false_%7C_false|1_%7C_1|false_%7C_false; ``` . It looks like it could be a case of using HashSet not LinkedHashSet, or HashMap not LinkedHashMap - but a quick replace throughout the GATK and HTSJDK codebase didn’t fix the problem.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6119#issuecomment-532593427:433,Hash,HashSet,433,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6119#issuecomment-532593427,2,['Hash'],"['HashMap', 'HashSet']"
Security,"@lbergelson I was referring more to the middle part of the StackOverflow by Daniel Chapman - specifically the [4.1 The ObjectStreamClass Class](http://docs.oracle.com/javase/8/docs/platform/serialization/spec/class.html#a5082) and [4.6 Stream Unique Identifiers](http://docs.oracle.com/javase/8/docs/platform/serialization/spec/class.html#a4100):. _If not specified by the class, the value returned is a hash computed from the class's name, interfaces, methods, and fields using the Secure Hash Algorithm (SHA) as defined by the National Institute of Standards._. Now when I look at the `java.io.ObjectStreamClass.java` file for 64-bit JDK7 and JDK8 - from src.zip - both have the same code for the following parts after performing a `diff` - I didn't list all of the lines of code since they are quite long:. ```; public long getSerialVersionUID() {; // REMIND: synchronize instead of relying on volatile?; if (suid == null) {; suid = AccessController.doPrivileged(; new PrivilegedAction<Long>() {; public Long run() {; return computeDefaultSUID(cl);; }; }; );; }; return suid.longValue();; }; ... private static long computeDefaultSUID(Class<?> cl) {; ...very long code which can be inspected via the src.zip file...; }; ```. So looking at the code portions of `computeDefaultSUID()` and I notice in our instance `ReadFilter` is a interface, which gets defined later via [ReadFilterLibrary.java](https://github.com/broadinstitute/hellbender/blob/62ef76ba60951c562a0d4c39189aa3f01f27f8d3/src/main/java/org/broadinstitute/hellbender/engine/filters/ReadFilterLibrary.java) or via `new ReadsFilter(readFilter, header)`, in either of these instances the fields would be different, based on this portion of `computeDefaultSUID` when looking at declared fields:. ```; Field[] fields = cl.getDeclaredFields();; MemberSignature[] fieldSigs = new MemberSignature[fields.length];; for (int i = 0; i < fields.length; i++) {; fieldSigs[i] = new MemberSignature(fields[i]);; }; ```. Therefore the `SUID` would be ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499:404,hash,hash,404,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499,4,"['Access', 'Hash', 'Secur', 'hash']","['AccessController', 'Hash', 'Secure', 'hash']"
Security,"@lbergelson Louis, I run into the similar trouble when I enable GATK to access oss fs of Aliyun, our oss of hadoop-fs impl (org.apache.hadoop.fs.{FileSystem, Path}) work well with spark perfectly but oss provider (java.nio.file.spi.FileSystemProvider) is not available today. (Not included in gatk package gatk-4.beta.6. . After I researched on [SparkContext](; https://github.com/apache/spark/blob/1c9f95cb771ac78775a77edd1abfeb2d8ae2a124/core/src/main/scala/org/apache/spark/SparkContext.scala) impl and [GATK rg.seqdoop.hadoop_bam.AnySAMInputFormat](https://github.com/broadinstitute/gatk/search?utf8=%E2%9C%93&q=java.nio.file.FileSystem&type=Code) impl. [IOUtils](https://github.com/broadinstitute/gatk/blob/94ac626218e073b77156a3eff076003d26be318c/src/main/java/org/broadinstitute/hellbender/utils/io/IOUtils.java#L535). Today, org.apache.hadoop.fs.{FileSystem, Path} is much broadly used in the Big Data world, and most of vendors of distribution storage provider already provide impl of org.apache.hadoop.fs.{FileSystem, Path} include AWS, Google and Alibaba. There are huge customers of Hadoop already work on hadoop.fs for years, if GAKT on spark could rely on org.apache.hadoop.fs.{FileSystem, Path} , I guess GAKT could acquire more existing customers of Hadoop on Cloud much faster . . Maybe we could consider migrating java.nio.file.FileSystem impl to org.apache.hadoop.fs.{FileSystem, Path} impl in [SparkContextFacto]r(https://github.com/broadinstitute/gatk/blob/73f2a62bee52518b57a985717770ed3a64d83243/src/main/java/org/broadinstitute/hellbender/engine/spark/SparkContextFactory.java), otherwise we could support both nio and hadoop thru env variable, Let me know your thought!. ```; scala> stringRdd.saveAsTextFile(""oss://eric-new/testwrite10""). scala> val stringRdd = sc.parallelize(Seq(""Test String"")); stringRdd: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[4] at parallelize at <console>:24. scala> stringRdd.saveAsTextFile(""oss://eric-new/testwrite11""); ```. ``` oss",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3936#issuecomment-354989381:72,access,access,72,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3936#issuecomment-354989381,1,['access'],['access']
Security,"@lbergelson You may recall that we've encountered things like malformed block-compressed input that validates and can be read without error, and yet appears to have fewer records than it should.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317048503:100,validat,validates,100,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317048503,1,['validat'],['validates']
Security,@lbergelson changed it to `validate`. Anything else?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2543#issuecomment-290780224:27,validat,validate,27,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2543#issuecomment-290780224,1,['validat'],['validate']
Security,@lbergelson here is the new version of the tool with all of the integration tests hand-verified verses the old version of the tool. The output now consistently passes validation and there is now an option to turn on (it is off by default) the N-splitting of secondary alignments.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2021#issuecomment-239503282:167,validat,validation,167,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2021#issuecomment-239503282,1,['validat'],['validation']
Security,@lbergelson i didn't have you on the whitelist for that command. now i do. we want to explicitly authorize who can run that because it's essentially allowing pull-requests to run on our servers so we need a human thumbs-up.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1975#issuecomment-233026831:97,authoriz,authorize,97,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1975#issuecomment-233026831,1,['authoriz'],['authorize']
Security,@lbergelson thank you for the comment and sorry for my bit late response. I excluded the dependency to the jsr203-s3a and tested that both local- and spark-gatk can access s3a files by dynamically loading it. I also added a new directory `scripts/s3a` for documentation and simple tests for s3a demonstration.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6698#issuecomment-665484597:165,access,access,165,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6698#issuecomment-665484597,1,['access'],['access']
Security,"@lbergelson thanks for following up. To the first part:. ```; jar tvf GenomeAnalysisTK4.jar; echo $?; ```; returns 0. and this is:; ```; jar tvf ../bin/GenomeAnalysisTK4.jar | grep -i FileTruncatedException; 765 Wed Mar 17 12:09:12 PDT 2021 htsjdk/samtools/FileTruncatedException.class; ```; so seems ok. I will talk to the group that manages the cluster. one out there possibility is that this is based on a lustre filesystem, and there could be some cryptic cluster-specific access issue. i have no specific reason to believe this, but weird things have happened.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7675#issuecomment-1042022576:477,access,access,477,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7675#issuecomment-1042022576,1,['access'],['access']
Security,"@lbergelson the NIO access would fail with a ""permission denied"" error.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2013#issuecomment-233445717:20,access,access,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2013#issuecomment-233445717,1,['access'],['access']
Security,"@lbergelson, I don't think that this solution will help in this case, because another error when trying to use `CommandLineProgramTest`is that it extends `BaseTest`, which loads directly a `GenomeLocParser` for a reference that is not present and it blows up in every test. Regarding the `Main` class, because you point it out here, I would like to have some control over `Main` and how it manages things like errors or logging header. Basically all the things that I'm facing at the moment are, apart of this error using the testing framework, is that the framework have tons of mentions to the GATK itself (error messages pointing to the GATK manual page or bundle tools), and little control over which of them should be expose to the final user. Only as an example, I would like to output a line with the name and version of my software and a short notice about the usage of the GATK framework and which version I'm using (for easier maintenance, and contribution if a bug is found).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2122#issuecomment-242802278:723,expose,expose,723,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2122#issuecomment-242802278,1,['expose'],['expose']
Security,"@lbergson's instructions above are good, but somehow they did not work for me. I was able to follow the [application default credentials](https://developers.google.com/identity/protocols/application-default-credentials) instructions, though. Here are the steps I took:. 1. create a new service account on the Google Cloud web page and download the JSON key file.; 2. gcloud auth activate-service-account --key-file ""$PATH_TO_THE_KEY_FILE""; 3. export GOOGLE_APPLICATION_CREDENTIALS=""$PATH_TO_THE_KEY_FILE"". I cleared my credentials first to make sure that the access worked because of the above steps, not because of other credentials. After those steps, gatk was able to run from my desktop and access files using the service account credentials. `gsutil ls` worked as well.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2425#issuecomment-282900470:559,access,access,559,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2425#issuecomment-282900470,2,['access'],['access']
Security,"@ldgauthier Concordance with XHMM would definitely be useful for validating calls on clusters for which we do not have WGS data. ; We need to modify code for having a fixed common CNV regions, but that should be straightforward.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4738#issuecomment-387828741:65,validat,validating,65,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4738#issuecomment-387828741,1,['validat'],['validating']
Security,"@ldgauthier Feel free to open a ticket describing your dream sequence dictionary compatibility check -- we can make the existing check stricter if you think it's too permissive, since users are always free to run with `--disable-sequence-dictionary-validation`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3754#issuecomment-495336889:249,validat,validation,249,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3754#issuecomment-495336889,1,['validat'],['validation']
Security,"@ldgauthier If you feel that you need some validation, but less strict/expensive than the default, then I'd suggest turning off the default validation, writing your own scaled-down dictionary validation routine, and calling it from `onTraversalStart()` in your tool. Then if it seems like the scaled-down validation might be generally useful, we could hook it up to the `SequenceDictionaryValidationArgumentCollection` as a third engine-level option.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6589#issuecomment-625438251:43,validat,validation,43,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6589#issuecomment-625438251,4,['validat'],['validation']
Security,"@ldgauthier This is why the `--disable-sequence-dictionary-validation` argument exists in `GATKTool`. If you're confident in the compatibility of your inputs, and the checks are too expensive, you can run with that option and (optionally) perform some less strict validation of your own in your `onTraversalStart()` method.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6589#issuecomment-625366653:59,validat,validation,59,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6589#issuecomment-625366653,2,['validat'],['validation']
Security,"@ldgauthier and @jonn-smith . As discussed during the gatk office hours, this error traces back to ValidateVariants in GVCF mode being unable to handle variants with a lower start position than the previous contig.; Example:; Super-Scaffold_1 9238114 . T <NON_REF> . . END=9238123 GT:DP:GQ:MIN_DP:PL 0/0:12:0:11:0,0,0; Super-Scaffold_2 1 . G <NON_REF> . . END=4 GT:DP:GQ:MIN_DP:PL 0/0:31:93:31:0,93,1141",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6023#issuecomment-507376213:99,Validat,ValidateVariants,99,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6023#issuecomment-507376213,1,['Validat'],['ValidateVariants']
Security,@ldgauthier has volunteered to open a PR to expose this parameter -- should be part of the next GATK release,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1662611627:44,expose,expose,44,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1662611627,1,['expose'],['expose']
Security,"@lucidtronix Are the environment variables that you added to the Docker env essential to realize the speed 2x improvement ? I'm reluctant to just add them to the Docker env without understanding what they're doing and whether/how they impact other components. i.e., changing OPEN_MP thread affinity/pinning params etc. might impact the native Intel PairHMM implementation (also @samuelklee will these impact CNV) ? Another option is reduce the scope of them and set them only for the specific tool(s), possibly exposed as command line arguments. The ScriptExecutor has control over the python process' environment and could easily propagate them to the so they only affect the particular Python process. But the values would have to be provided somehow.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5725#issuecomment-475614790:511,expose,exposed,511,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5725#issuecomment-475614790,1,['expose'],['exposed']
Security,"@magiDGS My preference would be to do a single PR with all of the fixes for the enable/disable validation rules and allowed values changes, and only those changes. Then we can do a second one with the extensibility changes. You can decide if you want to close this PR, or use it as the basis for the second one - either way is fine with me. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2355#issuecomment-278328215:95,validat,validation,95,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2355#issuecomment-278328215,1,['validat'],['validation']
Security,"@magicDGS Args like the config file that are truly optional (have no default value at all) do not show up in the command line or headers unless they're populated with some value. It should be pretty easy for ReadTools (which I think already has a common base class for its tools), to ensure a config file is never accepted by just precluding it via custom command line validation, or arg preprocessing. BTW, all tools built with GATK already have numerous common args that may or may not apply in a given tool context. For example, all of the ReadWalkers have a `--lenientVCFProcessing` arg. So I'm not even sure we need to make this hidden, since it will hide it from gatk users. My 2 cents. Others may feel differently.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4474#issuecomment-371819413:369,validat,validation,369,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4474#issuecomment-371819413,1,['validat'],['validation']
Security,"@magicDGS Good question! The main requirement is that the APIs need to allow you to *optionally* pass in URIs/Paths for all of the ""companion files"" for a particular input. For example, the `fai` and `dict` files for a fasta, or the `bai` file for a bam. When using signed URIs for authentication, these would all have separate signed URIs that would need to be provided explicitly. Of course, there should also be API methods that don't require you to pass in all of the companion files, and instead infer them automatically from the Path to the primary input, as htsjdk currently does. We would use these whenever possible (eg., when using account-level authentication rather than signed URIs, or when no authentication is necessary).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5269#issuecomment-429064032:282,authenticat,authentication,282,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5269#issuecomment-429064032,3,['authenticat'],['authentication']
Security,"@magicDGS I glanced at the tests and it appears there may be a scientific validation component, which I am unfamiliar with (I'm in an intro to Java course currently). Is this the case? If scientific validation is needed, then it is best to involve someone familiar with validation, e.g. Laura or Yossi. If all you need is data that can be run through these commands, I can put this together. Let me know. I'm late to these efforts, but I'd like to check one thing. Because of the way GRCh38 contigs are parsed, e.g. the HLAs that contain colons in their names, I believe we now prefer Picard-style intervals lists that tab-separate values instead of the `10:96000399-96000421` format that RealignerTargetCreator produces. I'm not certain of the status of GATK-style intervals lists, but I do know that the CNV developers have swiched to Picard-style. Is this what is produced by the new RealignerTargetCreator?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-371506127:74,validat,validation,74,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-371506127,3,['validat'],['validation']
Security,"@magicDGS I'd strongly prefer not to introduce a read filter descriptor hierarchy if we can avoid it, as it will be tricky to get right, and add complexity. We definitely need to be able to extend the package list used by the descriptor to find plugins, but as you point out we'll be able to use the configuration mechanism for that. For before/after-analysis filters, I expect that we'll just add that directly to the existing plugin once we resolve https://github.com/broadinstitute/gatk/pull/2085 (which I hope to get to this week). I think the rest of the cases can be addressed by overriding makeReadFilter and providing custom behavior of filter merging. If this turns out to be something truly common, we could consider allowing the tool to inject an argument collection into the plugin.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2353#issuecomment-274970451:748,inject,inject,748,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2353#issuecomment-274970451,1,['inject'],['inject']
Security,"@magicDGS My apologies for the long delay on this. It looks like there are still quite a few standard porting issues that need to be addressed in this PR (brackets, finals, multiple top-level public classes, outdated GATK3 usage example, remove references to RODs, kebabify, etc., etc.). There is also the bigger issue of testing and validation - ideally at a minimum we'd reproduce the existing GATK3 tests, but since these are dependent on large, private files, those tests will have to be replaced with new tests, and validated/compared against GATK3. These same issues will come up with IndelRealigner. @vdauwera @sooheelee Even with @magicDGS graciously volunteering to do the work of porting the code, retaining the indel realignment tools will require internal review, helping with test development and validation, and support. Before we commit to that, I guess I want to make sure that this is indeed a high priority, and that you think porting this to GATK4 is a better option than relying on GATK3, or letting @magicDGS port it to ReadTools ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-363451643:334,validat,validation,334,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-363451643,3,['validat'],"['validated', 'validation']"
Security,"@magicDGS Sorry for the delay on these AssemblyRegion-related PRs. There is an effort at the Broad right now to validate the GATK4 `HaplotypeCaller` against the GATK3 version. Until this is complete, we're not accepting even minor changes to code on the critical path for the `HaplotypeCaller`, except for bug fixes that arise from the validation work. It's still possible that as a result of this validation work `AssemblyRegionWalker` may get refactored/altered to address problems discovered, so until we have a final version that produces acceptable results for `HaplotypeCaller` (and we're not quite there yet) other changes to that part of the codebase will have to wait. Sorry for the inconvenience -- once GATK4's `HaplotypeCaller` gets the official stamp of approval we will certainly find a way to get all of your changes in. In the mean time we have to ask you to be patient a little longer!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2371#issuecomment-287447471:112,validat,validate,112,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2371#issuecomment-287447471,3,['validat'],"['validate', 'validation']"
Security,"@magicDGS The problem with exposing the datasources to walkers is that they would be able to invalidate the entire traversal. For example, a `ReadWalker` could alter the traversal intervals on the reads datasource mid-way through traversal from within `apply()`, or it could cause the reads iterator used by the engine to get closed by issuing a separate `iterator()` call on the datasource, which would cause the rest of the traversal to fail. This is why I feel strongly that the datasource objects should not be directly accessible to walker-based tools. Note that it's still possible for walkers to create their own, separate datasources without reaching into the ones used by the engine, or a tool author can extend `GATKTool` directly rather than one of the walker base classes and have the freedom to access everything (which was not possible before this PR).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4964#issuecomment-401423305:524,access,accessible,524,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4964#issuecomment-401423305,2,['access'],"['access', 'accessible']"
Security,"@magicDGS There isn't a specific master branch, but each push to master gets it's own artifact with a name like 4.alpha.1-208-g702c9e3-SNAPSHOT. You can search for the short commit hash of the build you want and use that artifact. . you'll need to add the artifactory repository to your build file. ```; repositories {; maven {; url ""https://artifactory.broadinstitute.org/artifactory/libs-snapshot/"" ; }; ```. There seems to be some bug right now where it hasn't uploaded the last 2 commits to master, I'm not sure what's causing that, but in general there should always be the most recent commits to master available. (As well as a snapshot of every pull request branch). It wouldn't be a bad idea for us to have master-SNAPSHOT as well, maybe we should do that.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1985#issuecomment-231406928:181,hash,hash,181,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1985#issuecomment-231406928,1,['hash'],['hash']
Security,"@magicDGS Yes, I think it would be much simpler if we had one PR with all of the fixes for the validation rules (and related help issues). The extensibility changes we've been discussing should be a separate PR.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2377#issuecomment-278330318:95,validat,validation,95,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2377#issuecomment-278330318,1,['validat'],['validation']
Security,"@magicDGS, after much discussion with @droazen, we will make this test set even smaller. Also, the current test set will be temporary, to unimpede you, until updates to the main test set can be made (discussion to start next week). @cmnbroad says we need to keep an eye out for coverage in the tests, given the need to make this dataset extremely small. So I removed chr17 from the mini-reference, such that only four small snippets of reference from various chromosomes remained. However, it appears there are only two usable indel realignments going on with GATK3. Note thate these are targeted exome samples with comparatively low coverage. [for_magicDGS.zip](https://github.com/broadinstitute/gatk/files/1820785/for_magicDGS.zip). There are two samples, so as to enable testing nWayOut, and an artificial reference `hg38_Shl01`. The two sites to hone in on are chr11:177568 and chr11:207134. Note also that the BAMs are in an invalid state according to ValidateSamFile. However, GATK3 RealignerTargetCreator and IndelRealigner did not seem to mind. I think these tools should allow processing of BAMs in any validation state. Apologies for the meager state of the data. On the bright side, the data set including the ref is only 23MB and will meet with @droazen's approval in terms of size. . Good luck @magicDGS.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-373845600:957,Validat,ValidateSamFile,957,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-373845600,2,"['Validat', 'validat']","['ValidateSamFile', 'validation']"
Security,"@marcopessoa, the fixes for this issue are not in master yet - here is the pending PR #6305. You could use the [genomicsdb_120_1](https://github.com/broadinstitute/gatk/tree/genomicsdb_120_1) branch to test out your scenario and post what you find to provide further validation for the changes in the PR. Thanks.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6275#issuecomment-573447804:267,validat,validation,267,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6275#issuecomment-573447804,1,['validat'],['validation']
Security,@mbabadi Can you let me know about the validation tools today?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3887#issuecomment-348487741:39,validat,validation,39,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3887#issuecomment-348487741,1,['validat'],['validation']
Security,"@mbabadi I know for the hg38 runs you are using CalculateTargetCoverage, but in case you used SparkGenomeReadCounts for any other WGS runs you're looking at, here's another thing to be aware of when considering the duplicates issue. I'm also running into frequent errors when running SparkGenomeReadCounts for the WGS CNV validation that seem to be related to hadoop bam or binning errors. Let's move to @asmirnov239's new tool ASAP.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3367#issuecomment-324935273:322,validat,validation,322,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3367#issuecomment-324935273,1,['validat'],['validation']
Security,"@mbabadi I've updated my PR to use miniconda3. @mbabadi @lucidtronix @samuelklee I think we should aim for tools that at least run out-of-box, without depending on any out-of-band configuration other than the conda env. On top of that we can provide guidance/configs for users on how to enable further optimizations, like g++. Does that sound like an achievable goal ?. As for the docker, we're going to have strike the right balance between image bloat and performance(including test performance). I think we're around 4+ gig now, and counting. Before the Python integration we were at 1.9G, and trying to find ways to reduce it. So lets see where we wind up but keep that in mind. Finally, we need to find a way to install the (GATK) python package(s) without depending on access to the GATK repo. Right now I think the gCNV branch has a ""pip install from source"" added to the conda env .yml. That will work on the docker at the moment (and thus on travis), but that won't work for non-docker users how don't have source/repo access. Also, one of the proposals to reduce the size of the docker is to remove the repo clone that is currently there. My proposal is that we change the gradle build to create an archive/zip of the python source (this would include the VQSR-CNN package code as well as gCNV kernel). We can then copy that on to the docker image, and pip-install it from the copy. That would retain the ability to always run travis tests based on the code in the repo, and also keep the nightly docker image in sync. We'll also have deliver the archive as an artifact somehow (perhaps including PyPi) for non-docker users.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3912#issuecomment-350303277:775,access,access,775,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3912#issuecomment-350303277,2,['access'],['access']
Security,@mcovarr @RoriCremer I have modified this now to fail outright if one of the validations fail. It now calls a GenerateFinalReport task as its last task and that will summarize the results in a way that (hopefully) makes it easier to understand the validation failure.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7850#issuecomment-1131987266:77,validat,validations,77,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7850#issuecomment-1131987266,2,['validat'],"['validation', 'validations']"
Security,"@mcovarr Hi Miguel - I just took a quick look at this branch, and it seems that it nicely addresses most of our needs for PGEN extract - specifically it would allow us to make an `ExtractCohortToPgen` that is just a slight variation of `ExtractCohortToVcf`. . There is one other thing we need though, which is a way to determine how many variants will be traversed *before* we traverse them. PGEN needs that up front, so we'd need it when we create the PGEN writer (currently it looks like that would be in the `ExtractCohortToPgen` `onStart` method). Is there any way to do that, and/or does it require access to the ExtractCohortEngine, which currently looks to private in the `ExtractTool` base class ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8344#issuecomment-1570383483:604,access,access,604,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8344#issuecomment-1570383483,1,['access'],['access']
Security,"@meganshand Here's a quick example:. ![image](https://user-images.githubusercontent.com/11076296/158385742-20a3303b-d8ce-4335-b42f-622da9bfa8d3.png); ![image](https://user-images.githubusercontent.com/11076296/158385777-6174f8b8-7abb-4b31-92d1-11cc8064854b.png). Note that the malaria data used was pretty small: chr1-2 training (~20k positive training/truth variants, ~50k negative training variants; note also that the threshold for determining negative training was not tuned---a threshold corresponding to a 98% truth sensitivity was arbitrarily chosen), chr3 validation (~50k variants), and chr4-6 test (~150k variants). The LL score is calculated from a validation set held out from the training/truth positives used to train the model, while the F1 score is calculated using ""orthogonal truth"" positives/negatives determined using 3 families of ~30 trios each. However, there's some arbitrariness in how we define the boundary for the latter positives/negatives, and hence some arbitrariness in the F1 score itself. But I'd expect using gold-standard GIAB truth would be more straightforward. Not sure how much we can conclude, but that the validation and test F1s are similar and that the validation LL score isn't *too* far off are encouraging. That said, there is a pretty big drop in recall when optimizing LL. But we should also expect some discrepancy between LL and F1, according to one of the papers linked above. I would hope that with more variants or reliable training/truth (as in your data), things might stabilize or line up better. I'll try running with more malaria data, as well. The following trios x sites heatmap (top plot) for the validation set might better illustrate the arbitrariness in F1 (click to enlarge):. ![image](https://user-images.githubusercontent.com/11076296/158385585-1a0dfe8e-d4b7-4770-aed0-19ad81162c92.png). Here, yellow = het errors (since these are supposed to be clonal malaria samples), red = Mendelian errors, grey = no calls, green = Mendelian con",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1067396431:564,validat,validation,564,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1067396431,2,['validat'],['validation']
Security,"@meganshand I ran the ""Full Pipeline"" workflows in a clone of your FC workspace: https://portal.firecloud.org/#workspaces/broad-firecloud-dsde/copy-of-megans-m2-mito-validations. I did not run any of the things that generate graphs because they were harder for me to understand. To compare the new results to your previous ones, I took all variants that were either PASS or had only the contamination filter applied, extracted just the locus and alleles columns, then manually inspected the diff. For the 5% and 50% spike-ins there were usually no differences at all, while for the 1% spike-in the difference was usually 2-5 variants that straddled the LOD threshold.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5473#issuecomment-443745103:166,validat,validations,166,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5473#issuecomment-443745103,1,['validat'],['validations']
Security,"@meganshand There is a warning in the docs for `ReadCoordinateComparator` that it should not be used for bam file output that needs to match the ordering of `SAMRecordCoordinateComparator` exactly, since it sorts all unmapped reads after all mapped reads. `ReadCoordinateComparator` is a comparator for `GATKRead`, and that interface does not allow unmapped reads to have a position. Ie., even if an unmapped `SAMRecord` is assigned the position of its mapped mate, calling `getContig()`/`getStart()` on the unmapped read via the `GATKRead` interface will return null/0. This was done mainly for consistency reasons and to simplify client code. Whenever we need bam file order for reads in GATK4, we operate on SAMRecords directly and use either the `SAMRecordCoordinateComparator` from htsjdk or the `HeaderlessSAMRecordCoordinateComparator` (for headerless Spark reads) that produces the same ordering. I recommend addressing this for this tool via `presorted = false` for now, since the GATK3 version has it set to false as well with the comment: ""**we don't want to assume that reads will be written in order by the manager because in deep, deep pileups it won't work**"". This suggests that even if you were to change the comparator used by this tool to behave like `SAMRecordCoordinateComparator`, you'd still have ordering issues in deep coverage areas. It's worthwhile, though, to open a separate ticket to explore whether `ReadCoordinateComparator` could be changed to exactly match bam file order. Eg., perhaps we could add `getAssignedContig()`, `getAssignedStart()`, etc. methods to `GATKRead` to expose the positions that unmapped reads with mapped mates get assigned for sorting purposes.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1853#issuecomment-224668518:1608,expose,expose,1608,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1853#issuecomment-224668518,1,['expose'],['expose']
Security,@mepowers Nice to meet you. . This issue isn't resolved. What was resolved was uploading a core dump that exhibits the problem. Is it possible for you to take a look into what's the causing the invalid pointer? Let us know what additional information we can provide. The core dump is located at `gs://hellbender/bugs/5690/core.tar.gz` and should be publicly accessible.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-465272533:358,access,accessible,358,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-465272533,1,['access'],['accessible']
Security,"@michaelgatzen ; in the meantime, if you need a samtools docker that can read from the bucket, you can ; ```; docker pull us.gcr.io/broad-dsde-methods/samtoolscloud:bucket.access. docker run us.gcr.io/broad-dsde-methods/samtoolscloud:bucket.access \; /bin/bash -c \; ""export GCS_OAUTH_TOKEN=`gcloud auth application-default print-access-token`; samtools view -H gs://your_bucket""; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6148#issuecomment-531017117:172,access,access,172,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6148#issuecomment-531017117,3,['access'],"['access', 'access-token']"
Security,"@micknudsen The splitting-bai is a different index from the bai. It's used to determine where spark should split the bam into shards when it's distributing work across the cluster. It doesn't provide random access support to the file, so it's a supplement to the bai instead of a replacement. Ideally we'd also output a normal bai as well, but due to the way the work is sharded it's not trivial to do so. . You can create the bai with `samtools index` if you use samtools, or `CreateHadoopBamSplittingIndex` has an option to output a bai. . The spark tools really should be creating it on the fly but we haven't gotten a chance to implement it yet. I opened a new ticket to track that since I didn't see one anywhere #4226",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4219#issuecomment-359544765:207,access,access,207,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4219#issuecomment-359544765,1,['access'],['access']
Security,"@mohitmathew Thanks for the report! We are currently in the process of updating GATK to Java 17, which necessarily involves updating many of our dependencies. We are also updating our docker image to be based off of the latest Ubuntu LTS release. This should greatly reduce the number of critical vulnerabilities in our release image. After the Java 17 switchover we can revisit this and see what security issues remain.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8215#issuecomment-1442245408:397,secur,security,397,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8215#issuecomment-1442245408,1,['secur'],['security']
Security,@mrizkypw I've reported these abusive PRs to github as well as the Broad's security team.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6194#issuecomment-537530173:75,secur,security,75,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6194#issuecomment-537530173,1,['secur'],['security']
Security,"@munrosa @ldgauthier Possible breakthrough. . First, what's definitely true about the het at 169510380 in 55_55003_F5region.bam when I reproduce the bug with `-L chr1:169510380 -ip 100`:. * The variant is considered active and triggers assembly, as it should.; * For every kmer size there are non-unique kmers in the reference, so it increases up to k = 85, the last attempt at which the engine relaxes the unique kmers requirement. (See `ReadThreadingAssembler` line 425).; * Once it reaches this kmer size, there are cycles in the graph and so no assembly is returned. (See `ReadThreadingAssembler` line 464). Thus no alt haplotype is discovered and the variant is missed. I believe there are two possible solutions.; * The assembly engine looks for cycles before pruning, but this order could be switched with no ill effects. In the case of this het there are no cycles after pruning because the apparent cycle was a poorly-supported path due to sequencing error. Here regular pruning works but the new `--adaptive-pruning` option would give a bit more security against false cycles.; * We don't actually have to check for cycles, especially in the last, desperate kmer attempt. Well, we do with the current recursive implementation of `KBestHaplotypeFinder`, but we *don't* in the Dijkstra's algorithm implementation currently under review: #5462. (Technical note: @ldgauthier I know I promised that this PR gives entirely equivalent results to the existing implementation, but technically this is only true if the existing implementation finishes in finite time. Due to the greedy -- but optimal -- nature of Dijkstra's algorithm, cycles do not cause issues). Personally, I am in favor of *both* solutions -- looking for cycles after pruning, and waiving the no-cycle requirement on the last attempt. They are complementary.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3697#issuecomment-446465913:1056,secur,security,1056,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3697#issuecomment-446465913,1,['secur'],['security']
Security,"@mwalker174 Ok, I've asked our Google collaborator @jean-philippe-martin to comment on https://github.com/broadinstitute/gatk/issues/3591. It looks like there were some authentication-related changes in the newer gcloud releases that could explain the error. It may be that we just need to update our client code and/or project IAM settings.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3592#issuecomment-330937174:169,authenticat,authentication-related,169,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3592#issuecomment-330937174,1,['authenticat'],['authentication-related']
Security,"@mwalker174 This is just FYI... you do not need to do anything. You cannot assume that the user running the test doesn't have root access. Also, I cleaned up the test a bit to use standard TestNG conventions (note the ``@ Test``):; ```; @Test(expectedExceptions = UserException.CouldNotCreateOutputFile.class); @SuppressWarnings(""unchecked""); public void testWriteTwoKryo() throws Exception {; final File tempFile = createTempFile(""test"", "".dat"");; final Integer int_in = 29382;; final String str_in = ""test string"";; PSUtils.writeKryoTwo(tempFile.getPath(), int_in, str_in);. final Kryo kryo = new Kryo();; kryo.setReferences(false);; final Input input = new Input(BucketUtils.openFile(tempFile.getPath(), null));; final Integer int_out = (Integer) kryo.readClassAndObject(input);; final String str_out = (String) kryo.readClassAndObject(input);; input.close();. Assert.assertEquals(int_in, int_out);; Assert.assertEquals(str_in, str_out);. // Point to a subdir that does not exist, so that we get a FNF exception; PSUtils.writeKryoTwo(tempFile.getAbsolutePath() + ""/bad_dir/bad_subdir/"", int_out, str_out);; }; ```. Please note that this is not in ``master`` at the time of this writing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2708#issuecomment-300832489:131,access,access,131,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2708#issuecomment-300832489,1,['access'],['access']
Security,@nalinigans @mlathara Is there an integrity checker tool for GenomicsDB (or a programmatic way to check the integrity of a GenomicsDB instance)?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6688#issuecomment-684910131:34,integrity,integrity,34,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6688#issuecomment-684910131,2,['integrity'],['integrity']
Security,@nenewell I took a brief look yesterday and it looks like there are many bams that fail to validate due to errors with mate pairs. I'm not sure how concerned we are about those errors. It's the weirder ones like . ```; htsjdk.samtools.SAMException: SAMFormatException on record 01; at htsjdk.samtools.SamFileValidator.validateSamRecordsAndQualityFormat(SamFileValidator.java:308); at htsjdk.samtools.SamFileValidator.validateSamFile(SamFileValidator.java:199); at htsjdk.samtools.SamFileValidator.validateSamFileVerbose(SamFileValidator.java:159); at org.broadinstitute.hellbender.tools.picard.sam.ValidateSamFile.doWork(ValidateSamFile.java:129); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:97); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:150); at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgram.instanceMain(PicardCommandLineProgram.java:51); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:71); at org.broadinstitute.hellbender.Main.main(Main.java:86); Caused by: htsjdk.samtools.SAMFormatException: Error parsing text SAM file. Not enough fields; File src/test/resources/org/broadinstitute/hellbender/tools/picard/analysis/CompareMetrics/colqualyldmtcs.bam; Line 1; ```. That seem particularly concerning.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/569#issuecomment-112826383:91,validat,validate,91,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/569#issuecomment-112826383,6,"['Validat', 'validat']","['ValidateSamFile', 'validate', 'validateSamFile', 'validateSamFileVerbose', 'validateSamRecordsAndQualityFormat']"
Security,"@nenewell If I understand what is going on correctly I think this may be a bug in the validator. Sam spec says . • If 0x1 is unset, no assumptions can be made about 0x2, 0x8, 0x20, 0x40 and 0x80. | Bit | Description |; | --- | --- |; | 1 0x1 | template having multiple segments in sequencing |; | 2 0x2 | each segment properly aligned according to the aligner |; | 4 0x4 | segment unmapped |; | 8 0x8 | next segment in the template unmapped |; | 16 0x10 | SEQ being reverse complemented |; | 32 0x20 | SEQ of the next segment in the template being reverse complemented |; | 64 0x40 | the first segment in the template |; | 128 0x80 | the last segment in the template |; | 256 0x100 | secondary alignment |; | 512 0x200 | not passing quality controls |; | 1024 0x400 | PCR or optical duplicate |; | 2048 0x800 | supplementary alignment |. So it shouldn't matter what google is setting the mate flags to if the read is a singleton. I know there are some other issues with things like this, I think `SAMRecord.equals()` isn't properly agnostic to certain flag combinations and will incorrectly evaluate as not equal even in some cases where the read is logically identical. Does that make sense or have I missed something?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/569#issuecomment-114255696:86,validat,validator,86,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/569#issuecomment-114255696,1,['validat'],['validator']
Security,"@nh13 Thank you for clarifying. As of today GKL does not auto-detect when there's a read that's ""too long,"" ie a read length we haven't validated with GKL. We should be able to build that into our pending release. I agree we should also make sure that if the GKL pairHMM fails, the JAVA version is called instead. @Kmannth @droazen let's discuss this in our next sync.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6733#issuecomment-674250782:136,validat,validated,136,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6733#issuecomment-674250782,1,['validat'],['validated']
Security,@nh13 The tool as it currently stands does not appear to do any validation on the VCF header. Agree that this is something the tool ought to do. . @ldgauthier I heard that you had a branch that improves this tool -- did you by any chance add header validation in that branch?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6762#issuecomment-679294356:64,validat,validation,64,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6762#issuecomment-679294356,2,['validat'],['validation']
Security,"@niyomiw That's not a `NullPointerException` -- it's a `NumberFormatException`, and is different from the issue reported in this ticket. It's possible that one or more of your VCFs are malformed in some way. Could you try running `ValidateVariants` on the inputs?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6766#issuecomment-716675461:231,Validat,ValidateVariants,231,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6766#issuecomment-716675461,1,['Validat'],['ValidateVariants']
Security,"@niyomiw That's not a `NullPointerException` -- it's a `NumberFormatException`, and is different from the issue resolved in https://github.com/broadinstitute/gatk/issues/6766. It's possible that one or more of your VCFs are malformed in some way. Could you try running `ValidateVariants` on the inputs?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6913#issuecomment-716675049:270,Validat,ValidateVariants,270,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6913#issuecomment-716675049,1,['Validat'],['ValidateVariants']
Security,"@pettyalex I don't think the Spark tools such as `MarkDuplicateSpark` are a likely candidate for stdin/stdout support. As you point out, they achieve parallelism by partitioning and then randomly accessing serialized input files. Even if it they could read from stdin, the benefits would be minimal, since they can't begin processing until they've seen the entire input stream, and they can't begin assembling the output until all of the worker nodes have finished processing their individual shards. So it would still require serializing the input, andI think the coarse grained process-parallelism you usually get from pipelining would be pretty minimal.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6749#issuecomment-1096818715:196,access,accessing,196,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6749#issuecomment-1096818715,1,['access'],['accessing']
Security,"@pgrosu I'm not sure that actually does explain what's happening. If I read it correctly it's saying that some objects were serialized, then the class was changed, and the old saved objects were no longer loadable. . Our current situation is that we serialized some objects, and deserializing them with the exact same class failed. The first situation is expected, the second one should not happen. . Is it possible that we are using different jvms on our local machine vs on the google cluster? So classes are serialized locally and then a jvm dependent hashcode is different at the other end?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107699331:555,hash,hashcode,555,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107699331,1,['hash'],['hashcode']
Security,"@rahulg603 This last time this was reported, we were unable to reproduce on our end, and the issue mysteriously ""went away"" on its own for @ldgauthier. Could you please report whether you're still getting the same error today? Are you able to access the same bucket using `gsutil` ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6179#issuecomment-1048012280:243,access,access,243,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6179#issuecomment-1048012280,1,['access'],['access']
Security,"@rdbremel for ""mystery 1"" see issue #5447. This should be an innocuous warning that it can't initialize the Google Cloud Storage code and shouldn't cause a failure unless you try to access paths that start with ""gs://"". Going through the Cloud initialization steps described in the README should remove the warning (though again, this isn't required if you don't need to read files from the cloud). Mystery 2: For what it's worth, ""GC overhead limit exceeded"" indicates that the VM was spending too much time in GC. Running low on memory is a possible cause but generating too many small objects or being stuck in an infinite loop of allocation/deallocation are others. In the past these have been caused by inputs that were malformed in some way. This isn't the place for this discussion though, please file a separate issue since it's a separate bug.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6182#issuecomment-548955879:182,access,access,182,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6182#issuecomment-548955879,1,['access'],['access']
Security,@ruchim would you be able to run the centaur tests on an arbitrary hash? That way we don't release bad WDL.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4319#issuecomment-362086043:67,hash,hash,67,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4319#issuecomment-362086043,1,['hash'],['hash']
Security,"@samuelklee @davidbenjamin I'm not authorized to merge PRs, so one of yous will have to.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3033#issuecomment-306541526:35,authoriz,authorized,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3033#issuecomment-306541526,1,['authoriz'],['authorized']
Security,@samuelklee Can we just expose `bin_length` since it determines what is WGS vs. Exome? Punt the rest?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3980#issuecomment-355997554:24,expose,expose,24,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3980#issuecomment-355997554,1,['expose'],['expose']
Security,@samuelklee Feel free to merge regardless of your decision. This PR is actually blocking a validation.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3550#issuecomment-331029649:91,validat,validation,91,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3550#issuecomment-331029649,1,['validat'],['validation']
Security,"@samuelklee I am inclined toward dropping all nd4j-related things. Given that we have access to tf, theano and numpy, I personally do not intend to do any heavy lifting in Java in the foreseeable future. Feel free to clean up and issue PR.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2929#issuecomment-358097583:86,access,access,86,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2929#issuecomment-358097583,1,['access'],['access']
Security,"@samuelklee I support exposing these parameters via the command line, but I'd be opposed to any consolidation of parameters that changes the HaplotypeCaller output prior to the initial DRAGEN-GATK release in November, as the evaluations in that project are difficult enough as it is. If you want to do an evaluation to find the best set of SW parameters now, that's fine of course -- but we wouldn't be able to actually merge any breaking HaplotypeCaller changes until after the November DRAGEN-GATK release, and we'd also have to check whether the proposed changes affect the functional equivalence of GATK and DRAGEN (we're developing tests now that can check this). If you want to expose the SW parameters on the CLI now, I think 12 arguments is fine. Just give each argument a clear prefix indicating what it applies to (eg., `--read-to-haplotype-mismatch-penalty`). If a user has gotten to the point where they feel the need to mess with the SW parameters, their command line is probably already long and complex as it is, so adding a few additional arguments won't ruin their day.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6863#issuecomment-705081291:684,expose,expose,684,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6863#issuecomment-705081291,1,['expose'],['expose']
Security,"@samuelklee I wrote some benchmarks for the exact combinatorics and you were right, my optimization was pointless. Although the `CombinatoricsUtils` method does explicitly multiply out instead of using cached factorials 1) the number of multiplications is only min(ploidy, (allele count - 1)), and 2) it actually takes quite a while (much larger than reasonable ploidy and allele count) for multiplication to take longer than the memory access of stored factorials. . I have removed this error in judgment.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1066873168:437,access,access,437,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1066873168,1,['access'],['access']
Security,"@samuelklee If there are CNV tools that can't comfortably extend `GATKTool` as things stand now, then I think that we should adjust `GATKTool` to be more flexible until they can do so. This would help with certain long-term goals that the engine team has (such as all tools supporting NIO for all inputs, consistent sequence dictionary validation, etc.)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2471#issuecomment-358040921:336,validat,validation,336,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2471#issuecomment-358040921,1,['validat'],['validation']
Security,@samuelklee We can easily expose the `IntervalMergingRule` in `IntervalArgumentCollection` (which is where `-L` is defined) as an argument to prevent the merging of adjacent intervals. We could also add a way for individual tools to set a default value for `IntervalMergingRule` themselves.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3246#issuecomment-314496762:26,expose,expose,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3246#issuecomment-314496762,1,['expose'],['expose']
Security,"@samuelklee Yes, this looks similar to what happened in master yesterday. In that case it looked like a transient remote access issue during the docker build. Having said that, its strange that the two data points we have were both on M2 WDL build.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4118#issuecomment-356761950:121,access,access,121,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4118#issuecomment-356761950,1,['access'],['access']
Security,@samuelklee the problem is that I need something pretty quickly here. I'm guessing that changing the GMM algorithm is going to require a ton of validation...,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3230#issuecomment-313891523:144,validat,validation,144,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3230#issuecomment-313891523,1,['validat'],['validation']
Security,"@schaluva Could I get access to a bam for that chromosome or some smaller interval that exhibits the bug and the original, non-simplified germline resource VCF? I think the error in filtering has been fixed, so I'm focusing on the first error.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6098#issuecomment-530090255:22,access,access,22,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6098#issuecomment-530090255,1,['access'],['access']
Security,"@schelhorn We have a large clinical ""truth"" set we utilize during workflow validations. We also utilize spike-in samples from SeraCare and perform dilutions using a couple of the common Coriell cell lines. We noticed the Mutect2 calling inconsistency while validating a small targeted panel.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1171672027:75,validat,validations,75,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1171672027,2,['validat'],"['validating', 'validations']"
Security,"@skwalker -- you've validated GATK 4 HaplotypeCaller pretty thoroughly, have you seen this issue? If not, let's close it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1858#issuecomment-443235374:20,validat,validated,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1858#issuecomment-443235374,1,['validat'],['validated']
Security,"@slzhao This is definitely a reasonable thing to do. Please feel free to issue a PR to do this. That said, I have to give you a warning. Some of the datasources rely on `sqlite3` and therefore have issues on some distributed filesystems (see this [post on Lustre/NFS errors](https://github.com/CGATOxford/CGATPipelines/issues/39)). There are a few posts in the GATK forums about this as well. So you may want to do some testing before using one centralized copy of the data sources. . As a heads-up - I do not have access to a Lustre filesystem so I am unable to do any debugging with it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6731#issuecomment-671508424:515,access,access,515,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6731#issuecomment-671508424,1,['access'],['access']
Security,"@sooheelee I can't speak for CNV, but there isn't any general reason to prefer Picard interval lists in GATK. There was previously an issue with parsing interval queries that used contig names that contained "":"", but thats fixed now. The only time we prefer a Picard list is the theoretical case were you use a query interval against a sequence dictionary that contains contigs that make that query ambiguous (hg38 is not one of those). GATK will detect and reject such a query and suggest using a Picard interval file to disambiguate it. @magicDGS I'm not sure how/if writing tests against existing files in the repository will be useful. I want to restate that we don't want to take ports of these tools if they're marked `@Experimental `or `@Beta` because they haven't been validated, or don't have good test coverage. We need to find a way need to have valid tests so they'll be production ready.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-371528161:777,validat,validated,777,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-371528161,1,['validat'],['validated']
Security,"@sooheelee I'm not sure I understand this issue. Is someone running the gatk docker image and then having people use that as a shared server? If they are, then it seems like it's their responsibility to control ssh access to the server, set up permissions, etc. That's outside of the scope of what we can do. If someone is running docker and starting up their own instance of the gatk container, then they have root access to that container by definition. . Could you explain the exact use case you want to support? I read the thread you pointed at but I also didn't really follow EADG's reasoning.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3644#issuecomment-333650137:215,access,access,215,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3644#issuecomment-333650137,2,['access'],['access']
Security,"@t-ogasawara Thanks for pointing us to the OSU service -- that looks like just what we need! I'll see if I can obtain access for our project. If we can get access and set up automated continuous tests for PPC, then I think we can use a single repo for the two architectures as you propose. Without continuous testing, though, I'm not convinced that `if the files under common are changed, the changes should work on PPC if the tests don't fail on x86_64.` -- at least, we would not be comfortable making changes to common code without tests in place.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1748#issuecomment-215460975:118,access,access,118,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1748#issuecomment-215460975,2,['access'],['access']
Security,"@takeshi-yoshimura Native access to s3a::// seems useful. In order to include this though we need some tests to show that it works/ demonstrate how to use it. I'm a bit concerned that it's version 0.0.1 (although it looks like there is a [0.0.2](https://search.maven.org/artifact/net.fnothaft/jsr203-s3a) out, should that be the one incorporated instead?) and there doesn't seem to be any activity on the library's [github](https://github.com/fnothaft/jsr203-s3a) in the last two years-ish. I'm wondering how stable/supported it is. . Maybe @fnothaft can comment on it. What's the status of this library? Do you recommend incorporating it or is there a different solution you've moved on to?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6698#issuecomment-658863506:26,access,access,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6698#issuecomment-658863506,1,['access'],['access']
Security,"@takutosato Based on all of our validations I added a commit to make this the default for M2. Because M2 shares a nested argument collection with HaplotypeCaller, this was pretty awkward. Louis told me this was the best among bad options.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5473#issuecomment-444360492:32,validat,validations,32,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5473#issuecomment-444360492,1,['validat'],['validations']
Security,@takutosato We don't need to do this. The read orientation model is sufficiently well-validated.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5654#issuecomment-572203804:86,validat,validated,86,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5654#issuecomment-572203804,1,['validat'],['validated']
Security,"@tedsharpe This looks good to me. In general partition sizes can be much larger than 100kb without problems, so I suspect it's is something funny to do with the task serialization of ctx.paralellize(). . If this is a performance critical tool it would probably be better to rewrite it in a way that it can load the reference in parallel. Since I assume this is something you run essentially once per reference it may not be worth it. . If you're worried about small machines running out of memory, I would expose the parameter that lets you configure how much memory each partition gets. I would expect in any spark configuration each core will have no less than ~1gb to work with and likely 2 -4 on any machines used for biology work.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1505#issuecomment-187387150:506,expose,expose,506,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1505#issuecomment-187387150,1,['expose'],['expose']
Security,"@theisaacwong Thanks for reporting this. it might be hard to debug this without access to the input file. It looks like the issue might be with the cram file itself - do you know how the file was created (GATK, etc. ?). Also, it would be interesting to know if GATK is able to consume the file without specifying query intervals (without -L).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6865#issuecomment-704956160:80,access,access,80,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6865#issuecomment-704956160,1,['access'],['access']
Security,"@tomwhite After spending some time searching for this feature for my testing purposes, it would be helpful to expose the NIO adapter toggle directly from the command line in this branch.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5138#issuecomment-418494235:110,expose,expose,110,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5138#issuecomment-418494235,1,['expose'],['expose']
Security,@tomwhite I think this might have been from the first run of the jenkins tests since the disq change over - not sure though. You should have access to the `broad-gatk-test-jenkins-robust` bucket now if you need it.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5545#issuecomment-449440744:141,access,access,141,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5545#issuecomment-449440744,1,['access'],['access']
Security,"@tomwhite I'm told you've done some work in the past with variant stores. Since we're now moving into working on the variant part of the pipeline, we'd like to assess our options for storing/accessing variants efficiently. Perhaps you could present some of your past work, and/or prototype some technologies you think are promising? We're going to look at TileDB (https://github.com/broadinstitute/gatk/issues/1647), but we'd like to get your suggestions/thoughts on this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1659#issuecomment-202576944:191,access,accessing,191,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1659#issuecomment-202576944,1,['access'],['accessing']
Security,"@tomwhite The problem is that `GATKRead` does not expose the ""positions"" of reads that are marked as unmapped (this was a conscious design choice so that client code doesn't have to check 5 different fields to determine whether a read is unmapped or not). So we could write a new comparator, but it would have to convert to `SAMRecord` internally and would only be used when writing -- this is why we initially preferred to patch `ReadsSparkSink` rather than write a new comparator. But it looks like we have no choice...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1249#issuecomment-162026651:50,expose,expose,50,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1249#issuecomment-162026651,1,['expose'],['expose']
Security,"@vdauwera @droazen @lbergelson @cmnbroad Hey, this pull request will fail as long as the docker hub repo for broadinstitute/gatk has restricted read access. Any objections to making reading of the gatk (not gatk-protected) dockerhub repo public?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2726#issuecomment-302212757:149,access,access,149,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2726#issuecomment-302212757,1,['access'],['access']
Security,"@vdauwera Currently it looks like many causes of failure in the existing MalformedReadFilter do explode with an exception instead of filtering.; - A read has a bad or undefined read group; - read.getReadLength() == read.getBaseQualities().length; - read.getReadBases() != SAMRecord.NULL_SEQUENCE; - containsNOperator(read). These cases all throw exceptions by default or in any case. Should these be changed to filter out criteria? Or should these be moved to a ReadValidator like @jmthibault79 suggests. Alternatively, are these checked for by the HTSJDK validation / should they be?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/193#issuecomment-75647767:556,validat,validation,556,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/193#issuecomment-75647767,1,['validat'],['validation']
Security,"@vdauwera I think we're planning on keeping a distinction between a bug and a user error. What the actual messages say is up for debate though. . Are you going to be offering support for alpha hellbender? What website do you want to link to? There aren't any doc urls at the moment so there's no where to direct people to right now. . Doing complete validation is a bit tricky, since an invalid option will screw up parsing for subsequent options. We should output a list of all the missing required options though instead of bailing at the first one.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/418#issuecomment-146240164:350,validat,validation,350,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/418#issuecomment-146240164,1,['validat'],['validation']
Security,"@vdauwera Yes, he needs to be added to the appropriate github team (one that gives him write access to the repo).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1382#issuecomment-169178877:93,access,access,93,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1382#issuecomment-169178877,1,['access'],['access']
Security,"@vdauwera, examples of long arguments from the `StandardArgumentDefinitions`:. * `--disable-tool-default-read-filters`; * `--disable-sequence-dictionary-validation`; * `--add-output-sam-program-record`; * `--add-output-vcf-command-line`. This arguments are long anyway, but from my point of view it is more readable in the camel-case format; in addition, for the two last I have a question: is the upper-case format extension (SAM/VCF) going to be in lower-case? If not, there is still a mixture of upper/lower-case that may be confusing (and difficult to enforce).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2596#issuecomment-324038691:153,validat,validation,153,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2596#issuecomment-324038691,1,['validat'],['validation']
Security,"@vidprijatelj , I can't reproduce the issue on `MacOS` and `Centos 7`. Can you provide us with more information with respect to the system you are on? What is the OS? Are there any [access control lists](https://man7.org/linux/man-pages/man5/acl.5.html) setup?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8233#issuecomment-1470386063:182,access,access,182,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8233#issuecomment-1470386063,1,['access'],['access']
Security,@vilay-nference Thank you for your pull request. I've incorporated your suggestions and closed out many vulnerabilities from our transitive dependencies. Hadoop/spark have finally stopped incorporating log4j1 so that one is closed out for good. . I've also rebuilt our base docker to incorporate recent patches from ubuntu. We've implemented some additional security scanning into our build process which will help keep us more up to date going forward.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8215#issuecomment-2432287402:358,secur,security,358,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8215#issuecomment-2432287402,1,['secur'],['security']
Security,"@vilay-nference Were you able to get this configuration to pass tests on your end? I've attempted to incorporate your changes into https://github.com/broadinstitute/gatk/pull/8998, but I'm running into issues with hadoop and protobuf incompatibilities. I see the same problem with your branch when I try to run tests on it. (I also can't run tests without disabling -Werror on your branch since there are still some unresolved deprecation and other minor issues). Errors look like this:. ```; Caused by: java.lang.ExceptionInInitializerError: Exception java.lang.NoClassDefFoundError: Could not initialize class org.apache.hadoop.security.proto.SecurityProtos [in thread ""IPC Server handler 1 on default port 64812""]; 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos.<clinit>(ClientNamenodeProtocolProtos.java); ```. and you can easily trigger one by running `ParallelCopyGCSDirectoryIntoHDFSSparkIntegrationTest`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8950#issuecomment-2412827680:630,secur,security,630,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8950#issuecomment-2412827680,2,"['Secur', 'secur']","['SecurityProtos', 'security']"
Security,"@vilay-nference You are always very welcome to submit a pull request on github with any proposed changes to GATK!. Most of the remaining vulnerabilities are in dependencies-of-dependencies which can be difficult to update, but we are slowly chipping away at them. For example, log4j 1.x is a dependency of the latest release of Apache Spark 3.x, and 4.x is still in preview (and note again that the log4j 1.x vulnerabilities are not the same as the infamous and very serious vulnerability that affected log4j 2.x some years ago). We don't believe that any of the remaining library vulnerabilities pose a real-world threat to GATK in practice, but it would still be good to eliminate them.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8215#issuecomment-2223132298:615,threat,threat,615,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8215#issuecomment-2223132298,1,['threat'],['threat']
Security,"@vruano ; Tests are failing, could you resolve this?; It appears to be in:; org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorIntegrationTest > testAgainstMutect2 FAILED; java.lang.IllegalArgumentException: Dirichlet parameters may not be negative; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:798); at org.broadinstitute.hellbender.utils.Dirichlet.<init>(Dirichlet.java:26); at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticLikelihoodsEngine.getEffectiveCounts(SomaticLikelihoodsEngine.java:56)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7303#issuecomment-859204971:314,validat,validateArg,314,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7303#issuecomment-859204971,1,['validat'],['validateArg']
Security,@vruano Should `ExomeToolsTestUtils` be exposed? What about `ReadClipperTestUtils`?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/525#issuecomment-103626555:40,expose,exposed,40,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/525#issuecomment-103626555,1,['expose'],['exposed']
Security,"@yfarjoun @vdauwera I've refined the tool categorization based on feedback on the tentative categorization. Thank you @yfarjoun for the review and feedback. The refinement is reflected in the new tabbed sheet in the shared Google Spreadsheet:`1217Changes_categorization-and-assignments`. I've separated out GATK vs Picard tools for each of the categories. . Here is a summary of the changes. 1. New 11 to `Diagnostics and QC`:; AnalyzeCovariates (from Alignment, Duplicate flagging and BQSR); GatherBQSRReports (from Alignment, Duplicate flagging and BQSR); FlagStat (from Read Data Manipulation); FlagStatSpark (from Read Data Manipulation); GetSampleName (from Read Data Manipulation); Picard BamIndexStats (from Read Data Manipulation); Picard CalculateReadGroupChecksum (from Read Data Manipulation); Picard CheckTerminatorBlock (from Read Data Manipulation); Picard CompareSAMs (from Read Data Manipulation); Picard ValidateSamFile (from Read Data Manipulation); Picard ViewSam (from Read Data Manipulation). 2. Merge 14 tools remaining in `Alignment, Duplicate flagging and BQSR` with 37 tools in `Read Data Manipulation`. Keep latter name. 	51 tools. 3. Move these out of `Read Data Manipulation`:; CompareDuplicatesSpark (to DxQC); ConvertHeaderlessHadoopBamShardToBam (to Other); CreateHadoopBamSplittingIndex (to Other). 4. Move ValidateVariants into `Variant Evaluation`. Also:; `Variant Evaluation and Refinement` --> `Variant Evaluation`; `VCF Manipulation` --> `Variant Manipulation` . Let us know your thoughts. Thank you.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-352313248:921,Validat,ValidateSamFile,921,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-352313248,2,['Validat'],"['ValidateSamFile', 'ValidateVariants']"
Security,"@yfarjoun How do you want to proceed with this PR, given that there are downstream issues even after this fix? Is the strategy going to be to keep developing patches like this, or throw in the towel and sanitize the problematic bases early on in the pipeline?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6625#issuecomment-642045845:203,sanitiz,sanitize,203,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6625#issuecomment-642045845,1,['sanitiz'],['sanitize']
Security,"@yfarjoun I believe you always have opinions about validation, wdyt?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2129#issuecomment-243281529:51,validat,validation,51,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2129#issuecomment-243281529,1,['validat'],['validation']
Security,"@yfarjoun Right, the intention of this ticket was to implement the codec in htsjdk, then add a GATK integration test proving that we can now access interval_list files as tribble features.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5788#issuecomment-472566997:141,access,access,141,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5788#issuecomment-472566997,1,['access'],['access']
Security,"@yfarjoun Well, as I said in person I believe that there are benefits to running with asynchronous prefetching turned on even in the random-access case. @jean-philippe-martin can confirm.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5882#issuecomment-482681722:140,access,access,140,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5882#issuecomment-482681722,1,['access'],['access']
Security,"@yl-h I have created a new branch [genomicsdb_6744](https://github.com/broadinstitute/gatk/tree/genomicsdb_6744) that exposes GenomicsDBArgument Collection to CreateSomaticPanelOfNormals. Can you please run the following to help us narrow down the issues?. 1. The default for GenomicsDB exports/queries changed from BCFCodec streaming in 4.1.7.0 to VCFCodec in 4.1.8.0. Run `gatk CreateSomaticPanelOfNormal` with `--genomicsdb-use-bcf-codec true` to override this default. If the expected PoN records is still missing variants, can you also run (2)?; 2. Run `gatk SelectVariants -O out.vcf -V gendb://...` on a small region with this branch to verify the number of variants is the same as from 4.1.7.0. If not, would you be able to distill and post any line that is missing now?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6744#issuecomment-672322785:118,expose,exposes,118,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6744#issuecomment-672322785,1,['expose'],['exposes']
Security,"@zhanyinx Just merged a fix to this, which exposes a new CLI parameter. Tonight's nightly build will have the new option for you to try.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8276#issuecomment-1674597532:43,expose,exposes,43,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8276#issuecomment-1674597532,1,['expose'],['exposes']
Security,"A bit of a side note, but which reference do we want specifically?; On Apr 19, 2015 8:57 PM, ""Adam Kiezun"" notifications@github.com wrote:. > a requirement is that we need to store and access the human reference file; > (~3GB, uncompressed) and a few other files < 1 GB each. The files will have; > public access so no additional security is required.; > ; > —; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/hellbender/issues/388#issuecomment-94330634; > .",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/388#issuecomment-94349276:185,access,access,185,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/388#issuecomment-94349276,3,"['access', 'secur']","['access', 'security']"
Security,"A few recent bugs, which are all entirely my fault, came about because the liftover of gnomAD to hg38 (there is no official hg38 gnomAD yet) exposed some new edge cases, such as `AF=.` and `AF=0`, that caused errors. I suspect that you are seeing one that we hadn't found yet. Unfortunately, we do not have nearly validation on hg38. Here's what I will do: 1) correct our hg38 gnomAD to fix liftover artifacts and put this new resource in the GATK bucket. 2) Create a Firecloud workspace with a few hg38 samples in order to reproduce the error and to make sure future changes don't create new problems 3) try to fix the error because even if 1) works it's sloppy to rely on the fact that gnomAD won't have these edge cases. I hope 1) succeeds because it will be available immediately without waiting for the next release.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5821#issuecomment-478305154:141,expose,exposed,141,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5821#issuecomment-478305154,2,"['expose', 'validat']","['exposed', 'validation']"
Security,"According to @jean-philippe-martin, the following workaround is still necessary in `GoogleGenomicsReadToGATKReadAdapter`:. ```; @Override; public GATKRead copy() {; // workaround until https://github.com/google/google-http-java-client/issues/293 is released; genomicsRead.setAlignedQuality(new ArrayList<>(genomicsRead.getAlignedQuality()));; Map<String, List<String>> infoCopy = new HashMap<>();; for (Map.Entry<String, List<String>> entry : genomicsRead.getInfo().entrySet()) {; infoCopy.put(entry.getKey(), new ArrayList<>(entry.getValue()));; }; genomicsRead.setInfo(infoCopy);; genomicsRead.getAlignment().setCigar(new ArrayList<>(genomicsRead.getAlignment().getCigar()));; // end workaround. // clone() actually makes a deep copy of all fields here (via GenericData.clone()); return new GoogleGenomicsReadToGATKReadAdapter(genomicsRead.clone());; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/650#issuecomment-132634761:384,Hash,HashMap,384,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/650#issuecomment-132634761,1,['Hash'],['HashMap']
Security,"Actually, I'm going to go ahead and add some exact match tests to guard against this sort of thing. Behavior for key somatic CNV tool modules (i.e., kernel segmentation and MCMC) is unit tested to within statistical noise (so, not exact match) on simulated data, but most integration tests just check for plumbing and not correctness. The idea was always that this sort of thing would be covered by what eventually became CARROT, since such tests would probably have to be long running and require more resources than are available in the repo to be useful. See the high priority but long dormant issues https://github.com/broadinstitute/gatk/issues/4122 and https://github.com/broadinstitute/gatk/issues/4123, as well as https://github.com/broadinstitute/gatk/issues/4630. In fact, I think the original idea was that Lee's validation would be the first to go into CARROT. Note also that I did some work to set up transition of all existing CNV tests (also including the somatic CNV validation against TCGA SNP calls that I put together on Terra) before going on leave and moving off CNVs, but during all that, we managed to 1) lose TCGA access, 2) delete the test files on which Lee based his validation after he left, and 3) reassign at least one of the people that was going to help with the transition. Again, the resulting differences here are minor and it's unlikely that future non-CNV code changes will have similar effects, since the CNV code is relatively well encapsulated, but the exact-match checks will hopefully give us some peace of mind until CARROT tests are ready. @jonn-smith @KevinCLydon looping you in just in case you're not aware of all of this history. Would love to chat about where CARROT is at and where you'd like it to go---feel free to ping me anytime!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7649#issuecomment-1023354175:824,validat,validation,824,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7649#issuecomment-1023354175,4,"['access', 'validat']","['access', 'validation']"
Security,"Actually, I'm noticing that while using NIO for the BAM for read/allelic-count collection is usually much more efficient, using NIO for the reference in other tasks can be much slower. Perhaps the access patterns for the reference (hitting ~10^5 intervals for WES PreprocessIntervals/AnnotateIntervals and ~10^6 sites for WES/WGS CollectAllelicCounts, respectively) make localization a better strategy? @droazen does that sound right to you?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4806#issuecomment-392046938:197,access,access,197,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4806#issuecomment-392046938,1,['access'],['access']
Security,"Actually, Jessica Hekman just confirmed she ran into an issue when running the GATK SV WDLs on-prem with Singularity, namely being unable to access the GATK jar in the root directory.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6525#issuecomment-1006929885:141,access,access,141,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6525#issuecomment-1006929885,1,['access'],['access']
Security,"Added a `PrintReads` based test (`IntelInflaterIntegrationTest.testIntelInflaterDeflaterWithPrintReads`) to test integration of `IntelInflater` and `IntelDeflater`. Removed the previous `IntelInflater` and `IntelDeflater` integration tests, which were basically copies of GKL unit tests. . **Note:** ; The `PrintReads` test is using `CEUTrio.HiSeq.WGS.b37.NA12878.20.21.tiny.md.bam` for input. When using `CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam` for input, we see the exception below when comparing `PrintReads` input and output. **This is true when using `IntelInflater`/`IntelDeflater`, as well as JDK `Inflater`/`Deflater`.** Is this a problem?. ```; htsjdk.samtools.SAMFormatException: SAM validation error: ERROR: Record 9762, Read name 20GAVAAXX100126:7:2:8126:115177, bin field of BAM record does not equal value computed based on alignment start and end, and length of sequence to which read is aligned; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2423#issuecomment-284551832:695,validat,validation,695,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2423#issuecomment-284551832,1,['validat'],['validation']
Security,"Added a few comments of my own -- requested that you refactor to check the index modification time in the `FeatureDataSource` constructors, rather than in the sequence dictionary validation routines.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3063#issuecomment-320948324:179,validat,validation,179,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3063#issuecomment-320948324,1,['validat'],['validation']
Security,"Added a unit test. To do so I had to make `BaseRecalibrationEngine.calculateKnownSites()` static. This wasn't a problem because I don't think it accesses any instance attributes but if there's some reason it shouldn't be static, I can do an integration test instead.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6389#issuecomment-576898683:145,access,accesses,145,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6389#issuecomment-576898683,1,['access'],['accesses']
Security,"Adressed all comments, @cmnbroad. Still the question on how to allow the user to provide custom validation. Back to you and thanks for reviewing!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2355#issuecomment-289361595:96,validat,validation,96,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2355#issuecomment-289361595,1,['validat'],['validation']
Security,"Agree that a separate README for the binary distribution is a good idea, but the document linked to above lacks basic instructions on things like running on a cluster and setting up GCS authentication. I think the doc should be based on the repo README instead with some sections omitted. It would actually be best if it could be generated automatically somehow from the repo README, so that we don't have to edit two documents whenever we make a change.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3199#issuecomment-311986024:186,authenticat,authentication,186,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3199#issuecomment-311986024,1,['authenticat'],['authentication']
Security,Ah no we should just give them direct access to our dev repo. Will do now. Thanks though @ronlevine,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2308#issuecomment-291033418:38,access,access,38,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2308#issuecomment-291033418,1,['access'],['access']
Security,"Ah, that's a good reason not to use it. How about just plain old `validate`?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2543#issuecomment-290773651:66,validat,validate,66,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2543#issuecomment-290773651,1,['validat'],['validate']
Security,"Alright, I think I am in agreement with you @lbergelson about this behavior. Furthermore, all I needed out of this branch was an exposed mechanism for getting back the un-merged intervals so that I can track them myself in subtools. To that end I think I'm going to keep this branch and its tests and get rid of the merging rule argument in favor of leaving the logic for merging in place to be accessed by tools.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5887#issuecomment-567666730:129,expose,exposed,129,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5887#issuecomment-567666730,2,"['access', 'expose']","['accessed', 'exposed']"
Security,"Also it seems that one can access the type arguments of fields thru reflection, thus the default behavior could constrain to try the codecs whose declared feature type is compatible with the FeatureInput parameter type. This would require less effort from the programmers perspective.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1184#issuecomment-163297024:27,access,access,27,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1184#issuecomment-163297024,1,['access'],['access']
Security,"Also just realized there’s *yet another* implementation in htsjdk, HardyWeinbergCalculation at https://github.com/samtools/htsjdk/blob/master/src/main/java/htsjdk/tribble/util/popgen/HardyWeinbergCalculation.java, so just a reminder to myself to check against that. Looks like a two-sided p-value of sorts is calculated there—I think this is P_{2\alpha} from Wigginton, although I need to double check. EDIT: Yup, it is, and furthermore the implementation appears to be correct. Phew! Added one more test to guard against a possible overflow issue that came up with that implementation, although it doesn't appear we have the same issue here. Will also note that 1) tests for the htsjdk implementation are pretty slim and don't actually cover very much, and 2) I don't see why we need to have two copies of this implementation, when all that essentially differs is the choice of p-value returned---we could certainly consolidate and expose the option of which p-value to return. Finally, I will also note that there is an implementation in bcftools. I have not checked it for correctness, but it appears to allow the calculation of both the one-sided p-value intended by ExcessHet, as well as what Wigginton calls P_{HWE}. So with that, the aforementioned implementations have covered every p-value discussed by that paper—and then one!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7394#issuecomment-892893837:933,expose,expose,933,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7394#issuecomment-892893837,1,['expose'],['expose']
Security,"Also note that I decided to append `_for_oncotator` to `additional_args`, since this is sufficiently vague without the suffix. However, analogous suffixes were not appended to exposed optional arguments for other tasks, since their names were less ambiguous. This is the sort of grossness we can do away with once Cromwell handles this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4324#issuecomment-362128625:176,expose,exposed,176,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4324#issuecomment-362128625,1,['expose'],['exposed']
Security,Also when I validated the bam I got no errors. . I'll try rerunning this BQSR step a bunch of times to see if I can get the error again and see if it happens on the same shard.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317000181:12,validat,validated,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317000181,1,['validat'],['validated']
Security,"Also, MQ filtering results in stochastic coverage dropout. It is likely that low MQ regions significantly overlap across samples, in which case, downstream CNV can learn such biases and correct the coverage. Will test this in validations.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4551#issuecomment-375722179:226,validat,validations,226,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4551#issuecomment-375722179,1,['validat'],['validations']
Security,"Also, do you want to run the ""GATK protected test suite"" against ""GATK Public""? Explain the workflow you envision. . 1. A PR comes in on GATK Public. ; 2. Job runs with the HEAD of GATK Public PR. (The commit hash); 3. Job grabs GATK Protected and...? that's where Im not sure what you want. ""against a snapshot of GATK public built from the current branch"". Is GATK Protected regularly taken from a GATK snapshot? Do you have a scripted mechanism for that or just using GIT tools?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1758#issuecomment-287539278:209,hash,hash,209,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1758#issuecomment-287539278,1,['hash'],['hash']
Security,"Also, it may be prudent for me to run the data through the commands the tests use, as the data I will make comes from an external source and may not validate in its current state, depending on the tool.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-371500028:149,validat,validate,149,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-371500028,1,['validat'],['validate']
Security,"Also, the sooner Cromwell handles exposure of subworkflow task-level parameters, the better. I had to make these changes to bubble up and expose all optional parameters for all subworkflows, a process which adds an enormous amount of boilerplate and is (obviously) prone to errors. I plan to revert the changes when Cromwell is ready (#4287), so let me know!. I did not file an issue yet. @LeeTL1220 may have?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4319#issuecomment-362072883:138,expose,expose,138,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4319#issuecomment-362072883,1,['expose'],['expose']
Security,"Although there is a workaround, ideally we'd remove the assumption from our docker that it can access the root user's home dir.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-434049374:95,access,access,95,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-434049374,1,['access'],['access']
Security,"And for cosmic:. /home/robby/Tools/NGS/gatk-4.2.6.1-src/scripts/funcotator/data_sources/cosmic/; getCosmicDataSources.sh; This script creates the cosmic data sources for the Funcotator GATK tool. For usage information run with the '-h' option. To retrieve the COSMIC data sources you must have a COSMIC account.; Please enter your COSMIC account credentials:; Enter your email address: ***@***.***; Enter your password: ; Creating folders: ...; mkdir: created directory 'cosmic'; mkdir: created directory 'cosmic/hg19'; mkdir: created directory 'cosmic/hg38'; mkdir: created directory 'cosmic_fusion'; mkdir: created directory 'cosmic_fusion/hg19'; mkdir: created directory 'cosmic_fusion/hg38'; mkdir: created directory 'cosmic_tissue'; mkdir: created directory 'cosmic_tissue/hg19'; mkdir: created directory 'cosmic_tissue/hg38'; Getting files ... ; get: cosmic/grch37/cosmic/v84/; CosmicCompleteTargetedScreensMutantExport.tsv.gz: ssh: Could not resolve ; hostname sftp-cancer.sanger.ac.uk: Name or service not known; get: cosmic/grch37/cosmic/v84/CosmicFusionExport.tsv.gz: ssh: Could not ; resolve hostname sftp-cancer.sanger.ac.uk: Name or service not known; get: cosmic/grch38/cosmic/v84/; CosmicCompleteTargetedScreensMutantExport.tsv.gz: ssh: Could not resolve ; hostname sftp-cancer.sanger.ac.uk: Name or service not known; get: cosmic/grch38/cosmic/v84/CosmicFusionExport.tsv.gz: ssh: Could not ; resolve hostname sftp-cancer.sanger.ac.uk: Name or service not known; ***@***.***:~/Tools/NGS> . -- ; ; r-engelmann.de - Ihre Seite für die Auswertung und Visualisierung von Daten ; aus den Bereichen Biomedizin, Finanzen, Sozioökonomie und weitere. On Dienstag, 30. August 2022 16:36:24 CEST Jonn Smith wrote:; > @robby81 Which scripts are you running and what are the errors you see? The ; data sources scripts; > are unsupported, but should work out of the box (they did last time I tried ; them).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7427#issuecomment-1274207002:410,password,password,410,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7427#issuecomment-1274207002,1,['password'],['password']
Security,"Annotations in VCF are a nightmare due to format requirements. I'd recommend against using VCF to store annotations unless it's absolutely necessary. The mechanism to do so is too unwieldy - either you add annotations by name per allele as real annotations (i.e. accounted for in the header and then added to the INFO field as applicable with per-allele annotations separated by commas), or you add them to the INFO field as a pipe-delimited single annotation field, with commas separating this long annotation for each allele (this is currently what Funcotator does). Both are kind of gross, with the former taking a LOT of extra space and the latter being basically unreadable by eye. You also need to make sure annotations are sanitized for illegal characters (such as commas). Funcotator has an open issue for this. A tabular format for annotations makes more sense, and, as much as it pains me to suggest it, MAF may be a quick answer here.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-386121869:730,sanitiz,sanitized,730,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-386121869,1,['sanitiz'],['sanitized']
Security,Another argument against this: the map function of a tool should clearly articulate its inputs in its signature. A map() that takes no parameters and relies on reflection/injection into members for its inputs would be supremely bad design.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/242#issuecomment-76739266:171,inject,injection,171,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/242#issuecomment-76739266,1,['inject'],['injection']
Security,Another option would be to add back the constructor that doesn't have ValidationStringency in CRAMIterator. (It looks like https://github.com/samtools/htsjdk/commit/efb11267ceea800d221a36d4cb9756dde4c7a984 introduced the incompatible change.) ValidationStringency can still be set using the setValidationStringency method. This would mean that no Hadoop-BAM JDK change or release is needed.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1247#issuecomment-161945126:70,Validat,ValidationStringency,70,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1247#issuecomment-161945126,2,['Validat'],['ValidationStringency']
Security,"Another question: is there a way to get the test data used for GATK3 and VariantEval? Even if they're not small enough to reasonably check in, it would be at least useful to use that integration test as a way to validate it's working as expected in GATK4.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/616#issuecomment-337340876:212,validat,validate,212,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/616#issuecomment-337340876,1,['validat'],['validate']
Security,"Any chance we could break off legacy CNV tools into their own group? There are *many* more of them than there will be in the new pipelines---and many of them are experimental, deprecated, unsupported, or for validation only---that I think it makes sense to hide them and perhaps be less stringent about their documentation requirements. Anything we can do to reduce the support burden before release would be great.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-346125341:208,validat,validation,208,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-346125341,1,['validat'],['validation']
Security,"Are the errors below part of this, when starting BwaSpark with spark-submit?; I activated ""--disable-sequence-dictionary-validation true"", but that doesn't help. It is very unclear, why a BAM is not recognized as a BAM file. I have tried all kinds of ways to make sure that it is a BAM and not a SAM file.; The documentation for BwaSpark also says ""BAM/SAM/CRAM file containing reads"", so if SAM files are really not possible, that should probably be changed.; ...; Even on verbosity DEBUG, the comments are not at all helpful to get at the problem.; E.g. ""Cannot retrieve file pointers within SAM text files.""; Is that a general statement about SAM files? Or does it only say, that in this specific SAM file (which is actually a BAM file), file pointers cannot be found?; What pointers are meant exactly?; How could this be fixed?. ```; ""SamReaderFactory	Unable to detect file format from input URL or stream, assuming SAM format.""; Which URL?; Which stream?; Why would this happen? What could be the error?; The SAM/BAM distinction seems very unclear. It would be more helpful, if some specific missing aspect (e.g. not queryname sorted) would be clearly declared as the culprit.; ...; 00:29 DEBUG: [kryo] Write: SAMFileHeader{VN=1.5, SO=queryname}; ...; WARNING	2018-01-16 02:11:25	SamReaderFactory	Unable to detect file format from input URL or stream, assuming SAM format.; ...; java.lang.UnsupportedOperationException: Cannot retrieve file pointers within SAM text files.; 	at htsjdk.samtools.SAMTextReader.getFilePointerSpanningReads(SAMTextReader.java:185); ...; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4131#issuecomment-357838062:121,validat,validation,121,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4131#issuecomment-357838062,1,['validat'],['validation']
Security,Are these functions exposed to jexl?. https://github.com/samtools/htsjdk/blob/335f2c1d70fe922c1bedfcb2d7d7751d5adb723c/src/main/java/htsjdk/variant/variantcontext/VariantContext.java#L737,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5916#issuecomment-489349581:20,expose,exposed,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5916#issuecomment-489349581,1,['expose'],['exposed']
Security,"As @cmnbroad pointed out, moving the function would need to expose the map, :+1: to just making it package protected and keeping it in place.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/787#issuecomment-128727780:60,expose,expose,60,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/787#issuecomment-128727780,1,['expose'],['expose']
Security,"As an addendum to this task, we would also want to improve the test coverage to the underlying methods by hitting the newly exposed API.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5910#issuecomment-499224455:124,expose,exposed,124,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5910#issuecomment-499224455,1,['expose'],['exposed']
Security,"As discussed during GATK office hrs, we need to improve ValidateVariants message to say this is not a invalid vcf. ; **Solution**: separate validation argument that goes beyond vcf specs.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6630#issuecomment-637245840:56,Validat,ValidateVariants,56,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6630#issuecomment-637245840,2,"['Validat', 'validat']","['ValidateVariants', 'validation']"
Security,As it turns out some of the `createGenomeLoc()` instances are being used by the GermlineCNVPipeline tests (evidently in ValidateVariants) and will fail if genome validation is enabled. This should be investigated.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7300#issuecomment-861713884:120,Validat,ValidateVariants,120,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7300#issuecomment-861713884,2,"['Validat', 'validat']","['ValidateVariants', 'validation']"
Security,"As per the linked discussion, we are going to try to get access to a hosted PPC service that we can run automated tests on. If we're successful in setting up PPC automated tests, then the two native PairHmm implementations could live in the same repo -- if not, then we might prefer two separate repos.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1788#issuecomment-215710485:57,access,access,57,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1788#issuecomment-215710485,1,['access'],['access']
Security,"At the Helsinki workshop someone explained to me they couldn't use Dockers on the server because folks don't typically have root access. I cannot say I understand the details, but I can get you in touch with someone who does if this is something you want to follow up on.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3644#issuecomment-333936510:129,access,access,129,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3644#issuecomment-333936510,1,['access'],['access']
Security,"At the moment, ReadTools only documents and use `ReadFilters` but I am planning to probably add pack a couple of tools from the GATK/Picard tools at some point. In addition, I am working on another toolkit based on the GATK code, and it will include also annotations for variants (and probably some VCF tools). I just thought that it will be useful to been able to pull out the super-category map to re-use the GATK docgen code. If a downstream project with extra-categories wants to use the GATK templates and DocGen code might get into troubles without being able to access that. I am still working on how to document better my toolkits, but it is not a problem yet. Anyway, I don't really have any strong feeling about this; I just wanted to reduce a bit the complexity of the GATK code and do the same with my downstream projects. If it is something that you anticipate that it is going to change the contract often, feel free to close (the RNA Strings can be removed in other PR).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4247#issuecomment-360856661:569,access,access,569,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4247#issuecomment-360856661,1,['access'],['access']
Security,"Audit results, there are two (very related issues).; Issue (1); `setPosition` from `SAMRecordToGATKReadAdapter` calls `setReferenceName` from `SAMRecord`. ```; public void setReferenceName(final String value) {; /* String.intern() is surprisingly expensive, so avoid it by looking up in sequence dictionary if possible */; if (NO_ALIGNMENT_REFERENCE_NAME.equals(value)) {; mReferenceName = NO_ALIGNMENT_REFERENCE_NAME;; mReferenceIndex = NO_ALIGNMENT_REFERENCE_INDEX;; return;; } else if (mHeader != null) {; final int referenceIndex = mHeader.getSequenceIndex(value);; if (referenceIndex != -1) {; setReferenceIndex(referenceIndex);; return;; }; }; // Drop through from above if nothing done.; mReferenceName = value.intern();; mReferenceIndex = null;; }; ```. Issue (2); `setMatePosition` from `SAMRecordToGATKReadAdapter` calls `setMateReferenceName` from `SAMRecord`. ```; public void setMateReferenceName(final String mateReferenceName) {; /* String.intern() is surprisingly expensive, so avoid it by looking up in sequence dictionary if possible */; if (NO_ALIGNMENT_REFERENCE_NAME.equals(mateReferenceName)) {; mMateReferenceName = NO_ALIGNMENT_REFERENCE_NAME;; mMateReferenceIndex = NO_ALIGNMENT_REFERENCE_INDEX;; return;; } else if (mHeader != null) {; final int referenceIndex = mHeader.getSequenceIndex(mateReferenceName);; if (referenceIndex != -1) {; setMateReferenceIndex(referenceIndex);; return;; }; }; // Drop through from above if nothing done.; this.mMateReferenceName = mateReferenceName.intern();; mMateReferenceIndex = null;; }; ```. @lbergelson, what's the fix?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141122913:0,Audit,Audit,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141122913,1,['Audit'],['Audit']
Security,"Authorization settings for the connector are described here: https://github.com/GoogleCloudPlatform/bigdata-interop/blob/master/gcs/conf/gcs-core-default.xml#L50. @jamesemery have you been able to get the connector working?. @droazen what configuration improvements did you have in mind?. Also, I'm not sure what the difference between `google.cloud.auth.service.account.json.keyfile` and `fs.gs.auth.service.account.json.keyfile` is (if any).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5996#issuecomment-500755373:0,Authoriz,Authorization,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5996#issuecomment-500755373,1,['Authoriz'],['Authorization']
Security,"Back to @droazen. Also in the last round, I didn't actually add the code in PositionalDownSampler to reject `submit` after `signalEndOfInput` was called (I added the state to keep track of it, but left out the actual validate call in submit). Letting tests run then, back to you.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5594#issuecomment-457759988:217,validat,validate,217,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5594#issuecomment-457759988,1,['validat'],['validate']
Security,"Back to @meganshand. I put in a simple mitochondrial integration test. Given that our MC3 validation already covers this particular bug I actually don't think it needs a new test for mitochondria. Also, for later, are any of your spike-in bams public (or rather, public + public)? I noticed that the NA12878 truth doesn't have very low AFs.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5057#issuecomment-408649991:90,validat,validation,90,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5057#issuecomment-408649991,1,['validat'],['validation']
Security,Back to you @cmnbroad. Your commit and some minor changes are included. There are still two questions:. * Why not using a `LinkedHashMap` instead of a `HashMap`/`ArrayList` for the default filters?; * Maybe it is better to make the fields final to do not override them in future changes or being aware of the change of state.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2401#issuecomment-283311142:152,Hash,HashMap,152,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2401#issuecomment-283311142,1,['Hash'],['HashMap']
Security,"Back to you @takutosato. You caught a couple of whoppers. Fortunately, the validations still look good after fixing them.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4832#issuecomment-394917958:75,validat,validations,75,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4832#issuecomment-394917958,1,['validat'],['validations']
Security,"Barclay now has implementations for min/max and minRecommended/maxRecommended values for integer/float args (and these are integrated with the help doclet). Once GATK is upgraded to the next Barclay version, we can start using them. i.e., ApplyBQSRArgumentCollection. Also, there are a ton of places in gatk-protected where there is custom validation code that could be replaced with annotations.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/143#issuecomment-273214207:340,validat,validation,340,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/143#issuecomment-273214207,1,['validat'],['validation']
Security,"Better sample name validation is done on the java side of plotting in #2858, but otherwise I don't really see a way around hardcoding column names in the R code that isn't more trouble than it's worth.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2862#issuecomment-335623002:19,validat,validation,19,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2862#issuecomment-335623002,1,['validat'],['validation']
Security,Btw the GATK3 version of SNCR doesn't generate a valid bam file according to ValidateSAMFile. Would be lovely to fix that here (and backport the fix if possible). v4 issue is here: https://github.com/broadinstitute/gatk/issues/1394,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1864#issuecomment-222232015:77,Validat,ValidateSAMFile,77,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1864#issuecomment-222232015,1,['Validat'],['ValidateSAMFile']
Security,"Bumping this since I ran into the same error as I was helping QC a colleagues data, running GATK 4.1.8.1 produces the following:. https://www.dropbox.com/s/2uleabl53dmg9y3/Screenshot%202020-07-28%2000.35.45.png. And this is on targeted capture data (Twist custom capture) ran through our core facility's sentieon pipeline, using the 'consensus' reads mapped to 1kg_grch37, using the raw reads works fine. Im not very familiar with sentieons pipelines but the steps to generate the UMI consensus reads are described at https://support.sentieon.com/appnotes/umi/. At first I though that discrepancy between @fleharty's ValidateSam and yours @ashwini06, could be that in the the newer version of Picard uses an updated version of htsjdk (v 2.23.0), but it's the same version of htsjdk that's included in GATK 4.1.8.1, so it seems unlikely. Walking through the commits between Picard 2.22.8 (the one bundled with GATK 4.1.8.1) and 2.23.2 doesn't (at least at first glance for me) show any commits changing code that could explain the differences in behaviour.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6695#issuecomment-664683562:617,Validat,ValidateSam,617,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6695#issuecomment-664683562,1,['Validat'],['ValidateSam']
Security,"By ""currently"" you mean in classic GATK or in Hellbender? In classic GATK; I'm pretty sure none of these causes an exit. On Mon, Feb 23, 2015 at 5:08 PM, Louis Bergelson notifications@github.com; wrote:. > @vdauwera https://github.com/vdauwera Currently it looks like many; > causes of failure in the existing MalformedReadFilter do explode with an; > exception instead of filtering.; > - A read has a bad or undefined read group; > - read.getReadLength() == read.getBaseQualities().length; > - read.getReadBases() != SAMRecord.NULL_SEQUENCE; > - containsNOperator(read); > ; > These cases all throw exceptions by default or in any case. Should these; > be changed to filter out criteria? Or should these be moved to a; > ReadValidator like @jmthibault79 https://github.com/jmthibault79; > suggests. Alternatively, are these checked for by the HTSJDK validation /; > should they be?; > ; > —; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/hellbender/issues/193#issuecomment-75647767; > . ## . Geraldine A. Van der Auwera, Ph.D.; Bioinformatics Scientist II; GATK Support & Outreach; Broad Institute",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/193#issuecomment-75648317:851,validat,validation,851,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/193#issuecomment-75648317,1,['validat'],['validation']
Security,"By the way, I thought @vdauwera was opposed to using optional inputs in this way at some point (see #3657). Was that question ever decided? (I'm still of the opinion that they *should* be used in this way, but this is one of the reasons I didn't for this iteration of the WDL.). To be clear, the pair WDL right now does not allow all of the workflow paths (tumor-only, no PoN, etc.) that the new tools make possible. It only allows the one that we will most likely run in production (matched-normal + PoN). We should probably make the WDL a little more flexible to cover the most common use cases, but I'm fine if it doesn't completely expose all of the possible workflow paths---this would probably just make the WDL harder to maintain. Users can write their own WDLs in this case.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3983#issuecomment-362696132:636,expose,expose,636,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3983#issuecomment-362696132,1,['expose'],['expose']
Security,C6C62656E6465722F746F6F6C732F77616C6B6572732F67656E6F74797065722F47656E6F747970696E67456E67696E652E6A617661) |; | 0% | [...nstitute/hellbender/engine/AssemblyRegionWalker.java](https://codecov.io/gh/broadinstitute/gatk/pull/2293/compare#diff-7372632F6D61696E2F6A6176612F6F72672F62726F6164696E737469747574652F68656C6C62656E6465722F656E67696E652F417373656D626C79526567696F6E57616C6B65722E6A617661) |; | 0% | [...titute/hellbender/engine/spark/LocusWalkerSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/2293/compare#diff-7372632F6D61696E2F6A6176612F6F72672F62726F6164696E737469747574652F68656C6C62656E6465722F656E67696E652F737061726B2F4C6F63757357616C6B6572537061726B2E6A617661) |; | 0% | [...broadinstitute/hellbender/utils/GenomeLocParser.java](https://codecov.io/gh/broadinstitute/gatk/pull/2293/compare#diff-7372632F6D61696E2F6A6176612F6F72672F62726F6164696E737469747574652F68656C6C62656E6465722F7574696C732F47656E6F6D654C6F635061727365722E6A617661) |; | 0% | [...ellbender/tools/validation/CompareBaseQualities.java](https://codecov.io/gh/broadinstitute/gatk/pull/2293/compare#diff-7372632F6D61696E2F6A6176612F6F72672F62726F6164696E737469747574652F68656C6C62656E6465722F746F6F6C732F76616C69646174696F6E2F436F6D70617265426173655175616C69746965732E6A617661) |; | 0% | [...hellbender/tools/walkers/bqsr/AnalyzeCovariates.java](https://codecov.io/gh/broadinstitute/gatk/pull/2293/compare#diff-7372632F6D61696E2F6A6176612F6F72672F62726F6164696E737469747574652F68656C6C62656E6465722F746F6F6C732F77616C6B6572732F627173722F416E616C797A65436F76617269617465732E6A617661) |; | 0% | [...lections/RequiredVariantInputArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/2293/compare#diff-7372632F6D61696E2F6A6176612F6F72672F62726F6164696E737469747574652F68656C6C62656E6465722F636D646C696E652F617267756D656E74636F6C6C656374696F6E732F526571756972656456617269616E74496E707574417267756D656E74436F6C6C656374696F6E2E6A617661) |; > [Review all 203 files changed](https://codecov.io/gh/bro,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2293#issuecomment-265198632:2834,validat,validation,2834,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2293#issuecomment-265198632,1,['validat'],['validation']
Security,"Can someone here take inventory of the various Picard tools that you want to consider moving over? These are NOT represented in the GATK best practices document, for example `MergeBamAlignment` or `ValidateSamFile`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1#issuecomment-66663056:198,Validat,ValidateSamFile,198,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1#issuecomment-66663056,1,['Validat'],['ValidateSamFile']
Security,Can you gain access to the original gvcfs for those samples? if not I don't think there is a standard way to do such a thing in GATK following best practices. Perhaps there is some general VCF merging tool that would do the trick sort-of but it won't be the same as if you had run that sample with the rest thru the single sample + joint-genotyping pipeline.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7292#issuecomment-855333445:13,access,access,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7292#issuecomment-855333445,1,['access'],['access']
Security,"Can you point out where in the log you see that? I'm looking at it but I don't see anything about memory in the log you provided. (Except the Runtime.totalMemory()=4523032576 which is just standard output spam from gatk when it shutsdown) Sequence dictionary validation usually happens first, it's strange that a failure in the middle of a run would be effected by it. I'm no very curious what weird thing is happening that's causing this...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6182#issuecomment-548114772:259,validat,validation,259,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6182#issuecomment-548114772,1,['validat'],['validation']
Security,"Can you run the GATK tool `ValidateSamFile` on `CEUTrio.HiSeq.WGS.b37.NA12878.20.21.tiny.md.bam`, and see if you get the same validation error as you do after roundtripping through the inflater/deflater, @gspowley?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2423#issuecomment-285104828:27,Validat,ValidateSamFile,27,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2423#issuecomment-285104828,2,"['Validat', 'validat']","['ValidateSamFile', 'validation']"
Security,Can you setup your temporary folder to a location where you have read and write access? Slurm might interfere with temporary files. ; [How to setup and use temporary folder for GATK local execution](https://gatk.broadinstitute.org/hc/en-us/articles/18965297287067-How-to-setup-and-use-temporary-folder-for-GATK-local-execution),MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8873#issuecomment-2168101103:80,access,access,80,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8873#issuecomment-2168101103,1,['access'],['access']
Security,"Can you try a few more cache sizes like 200000 and 500000? Also, when you do the PR, could you create `VariantWalker.FEATURE_CACHE_SIZE = 100000` (or whatever the final value is) and pass it into the `FeatureManager` in `initializeFeatures()`? `FeatureDataSource.DEFAULT_QUERY_LOOKAHEAD_BASES` should stay at 1000 (for ReadWalkers, which have a lot more overlap in their query access patterns)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1092#issuecomment-155505129:377,access,access,377,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1092#issuecomment-155505129,1,['access'],['access']
Security,"Can't seem to do git clone https://github.com/broadinstitute/hellbender/. This is my console output:. ```; wmd16-c9e:codespace vdauwera$ git clone http://github.com/broadinstitute/hellbender/; Cloning into 'hellbender'...; remote: Counting objects: 22221, done.; remote: Compressing objects: 100% (142/142), done.; remote: Total 22221 (delta 47), reused 4 (delta 4), pack-reused 22046; Receiving objects: 100% (22221/22221), 36.63 MiB | 3.58 MiB/s, done.; Resolving deltas: 100% (9903/9903), done.; Checking connectivity... done.; Downloading src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam (76.16 MB); Username for 'http://github.com': vdauwera; Password for 'http://vdauwera@github.com': ; Error accessing media: src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam (6b1304800e60c0ac0358df137bdad48b7857a36465b04fef3fbbb09380f04746). Errors logged to /Users/vdauwera/codespace/hellbender/.git/lfs/objects/logs/20151005T220016.510795175.log.; Use `git lfs logs last` to view the log.; Downloading src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam.bai (11.25 KB); Username for 'http://github.com': ; ```. Looks like I'm failing to download large test files. Do I need to be on VPN for this to work? Or is it expected and I should ignore it?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/950#issuecomment-145722061:664,Password,Password,664,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/950#issuecomment-145722061,2,"['Password', 'access']","['Password', 'accessing']"
Security,"Closing this ancient PR -- this is hard to test/would take a lot of work to get in, and it's a somewhat exotic use case. We do already have tests covering access to private files with default credentials, which seems sufficient.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2879#issuecomment-453564368:155,access,access,155,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2879#issuecomment-453564368,1,['access'],['access']
Security,CommandLineProgram.java:195); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:137); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:158); 	at org.broadinstitute.hellbender.Main.main(Main.java:239); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:755); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.io.IOException: Error code 404 trying to get security access token from Compute Engine metadata for the default service account. This may be because the virtual machine instance does not have permission scopes specified.; 	at shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials.refreshAccessToken(ComputeEngineCredentials.java:152); 	at shaded.cloud_nio.com.google.auth.oauth2.OAuth2Credentials.refresh(OAuth2Credentials.java:175); 	at shaded.cloud_nio.com.google.auth.oauth2.OAuth2Credentials.getRequestMetadata(OAuth2Credentials.java:161); 	at shaded.cloud_nio.com.google.auth.http.HttpCredentialsAdapter.initialize(HttpCredentialsAdapter.java:96); 	at com.google.cloud.http.HttpTransportOptions$1.initialize(HttpTransportOptions.java:157); 	at shaded.cloud_nio.com.google.api.client.http.HttpRequestFactory.buildRequest(HttpRequestFactory.java:93); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:300); 	at shaded.cloud_nio.com.google.api.client.googleapis.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3855#issuecomment-347320994:9215,secur,security,9215,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3855#issuecomment-347320994,2,"['access', 'secur']","['access', 'security']"
Security,"Completing https://github.com/broadinstitute/hellbender/issues/673 will take care of the case of ""missing reference for CRAM"", but we also need to make sure we're handling the case of ""wrong reference"" elegantly (where elegantly means ""throw a `UserException` with a descriptive error message). We want a test case with a reference that is the wrong reference for a CRAM, but has a compatible sequence dictionary (so that it won't be caught by the sequence dictionary validation). Both the wrong and missing reference cases should have a simple integration test that runs, eg., `PrintReads` with `expectedExceptions = UserException.class`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/677#issuecomment-126031374:468,validat,validation,468,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/677#issuecomment-126031374,1,['validat'],['validation']
Security,"Could it be possible to read the file from a `java.nio.Path` in the [gatk-bwamem-jni](https://github.com/broadinstitute/gatk-bwamem-jni), @SHuang-Broad? It looks that it's a constraint of the native code, but it will be nice to be able to have just one index image in HDFS accessible for all the nodes...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-311908959:273,access,accessible,273,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-311908959,1,['access'],['accessible']
Security,"Could this be related to having sliced objects in the gsutils buckets but not using a code path that goes through a native CRC implementation? I ask because I noticed that when I try to download the file. ```; gs://hellbender/test/resources/benchmark/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam; ```. with gsutil, I get this error:. ```; CommandException: ; Downloading this composite object requires integrity checking with CRC32c,; but your crcmod installation isn't using the module's C extension, so the; hash computation will likely throttle download performance. For help; installing the extension, please see:. $ gsutil help crcmod. To download regardless of crcmod performance or to skip slow integrity; checks, see the ""check_hashes"" option in your boto config file.; ```. Could the GATK command path be computing all of the CRC hashes in Java code, slowing it down?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1755#issuecomment-223982882:404,integrity,integrity,404,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1755#issuecomment-223982882,4,"['hash', 'integrity']","['hash', 'hashes', 'integrity']"
Security,Could you also run `docker images --digests` and paste the sha256 hash value for the image you're running? For `broadinstitute/gatk:4.4.0.0` it should be `044112d3d70603732d4a654ecaee33919cf9d45332d47268f5f1697b6ed558ed`,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8402#issuecomment-1648498439:66,hash,hash,66,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8402#issuecomment-1648498439,1,['hash'],['hash']
Security,Create a checksum to validate individual data sources based on the size of the data source files and some other meta attributes. Calculate the checksum for each data source in each data source package (somatic/germline). Store these checksums in the GATK jar and validate the data sources at runtime. Default to an exception if the checksums do not match. Have a command-line option to reduce the exception to a warning.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4380#issuecomment-415085087:9,checksum,checksum,9,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4380#issuecomment-415085087,6,"['checksum', 'validat']","['checksum', 'checksums', 'validate']"
Security,"Cromwell switched to Java 11 starting with version 60 so the Java 8 sub-builds are probably going to error like. ```; Caused by: java.lang.UnsupportedClassVersionError: wdl/draft3/parser/WdlParser$Ast has been compiled by a more recent version of the Java Runtime (class file version 55.0), this version of the Java Runtime only recognizes class file versions up to 52.0; 	at java.lang.ClassLoader.defineClass1(Native Method); 	at java.lang.ClassLoader.defineClass(ClassLoader.java:756); 	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); 	at java.net.URLClassLoader.defineClass(URLClassLoader.java:473); ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7824#issuecomment-1116073144:497,secur,security,497,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7824#issuecomment-1116073144,3,"['Secur', 'secur']","['SecureClassLoader', 'security']"
Security,"DATA_SOURCES/data_source_8/hg38/dnaRepairGenes.20180524T145835.csv; 16:01:43.979 INFO Funcotator - Initializing Funcotator Engine...; 16:01:43.983 INFO Funcotator - Creating a VCF file for output: file:/home/deepak/software_library/gatk-4.1.7.0/variants.funcotated.vcf; 16:01:44.020 INFO ProgressMeter - Starting traversal; 16:01:44.020 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 16:01:44.068 WARN GencodeFuncotationFactory - Cannot create complete funcotation for variant at chr1:1-10454 due to alternate allele: <NON_REF>; 16:01:44.116 INFO VcfFuncotationFactory - dbSNP 9606_b150 cache hits/total: 0/0; 16:01:44.121 INFO Funcotator - Shutting down engine; [12 May, 2020 4:01:44 PM IST] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 0.14 minutes.; Runtime.totalMemory()=2889875456; java.lang.IllegalArgumentException: Invalid interval. Contig:chr1 start:-9 end:10464; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:733); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:59); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:35); at org.broadinstitute.hellbender.tools.funcotator.FuncotatorUtils.createReferenceSnippet(FuncotatorUtils.java:1439); at org.broadinstitute.hellbender.tools.funcotator.FuncotatorUtils.getBasesInWindowAroundReferenceAllele(FuncotatorUtils.java:1468); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createFuncotationForSymbolicAltAllele(GencodeFuncotationFactory.java:2560); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createFlankFuncotation(GencodeFuncotationFactory.java:2465); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createGencodeFuncotationOnSingleTranscript(GencodeFuncotationFactory.java:953); at org.broadinstitute.hellbender.tools.func",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6598#issuecomment-664565036:6401,validat,validateArg,6401,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6598#issuecomment-664565036,1,['validat'],['validateArg']
Security,"Dear @davidbenjamin and @droazen,. please find [here](https://github.com/broadinstitute/gatk/files/12196051/2023-06-21.ISMB.Poster.on.XOP.Variant.Calling.Workflow.and.Mutect2.SES.v3.pdf) the poster that we presented this Monday at ISMB 2023 in Lyon. It sheds some additional light on this issue and explains why we believe that fine-tuning of the error model introduced in Mutect2 4.1.9.0 led to overcalling of variants in samples with increased (but not pathologic) DNA degradation. Presentation of the poster led to several interesting discussions with other users of Mutect2 at the conference. Given these results, I would propose that you could expose the parameter you newly set in commit [a304725](https://github.com/broadinstitute/gatk/commit/a304725a60f5000ec6381040137043a557fc3dc1) `private static final int ONE_THIRD_QUAL_CORRECTION = 5;` as a user-facing command line parameter. In this manner, you could leave the parameter at `5` by default but allow users who work on clinical (especially FFPE) samples to change it to `0` instead, thus effectively restoring the behaviour shown (and benchmarked) prior to 4.1.9.0. Would that be an acceptable compromise?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1655902557:649,expose,expose,649,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1655902557,1,['expose'],['expose']
Security,Dear GATK staff. We are now forbidden access to any webpages starting with the address https://gatkforums.broadinstitute.org/gatk/discussion/ by cloudflare. We can still access pages starting with https://gatk.broadinstitute.org/hc/en-us/ . We are connecting from various computers and netwrk in France using Firefox but it seems that it is the server itself that is blocked by Cloudflare. Have you already been notified of this problem and do you think you can solve it with cloudflare?. Thanks for your help. Best regards. Thierry Grange,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5869#issuecomment-577016491:38,access,access,38,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5869#issuecomment-577016491,2,['access'],['access']
Security,"Dear all, thank you for the quick reply! Yes, I tried this on the latest release (4.0.8.0), and it is still an issue. vcf-validator validates the input VCFs, however, in the output VCF, I get errors associated with the field descriptions for the VAF annotation (which I believe may be specific to DeepVariant), a few of which are pasted below:. The header tag 'reference' not present. (Not required but highly recommended.). column CDC1551_clean_mutated_9.fasta_1_1 at NC_000962.3:580772; \ .. FORMAT tag [VAF] expected different number of values (expected 1, found 2); column CDC1551_clean_mutated_2.fasta_1_1 at NC_000962.3:580772 .. FORMAT tag [VAF] expected different number of values (expected 1, found 2). I tried the above after changing the VAF field description label from Number=R to Number=A in my unmerged gVCF files. This did not solve the problem. I'm attaching two example VCF files which I am attempting to merge. My commands are below: . ```. # Create GenomicsDBImport; gatk GenomicsDBImport \; -R ${REF_DIR}${ref} \; -V CDC1551_clean_mutated_1.fasta_1_1.bwa_deep.g.vcf.gz \; -V CDC1551_clean_mutated_2.fasta_1_1.bwa_deep.g.vcf.gz \; --reader-threads 8 \; --genomicsdb-workspace-path ${mapper}_${caller}_comb \; --intervals 'NC_000962.3' \; --java-options '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true' \; --overwrite-existing-genomicsdb-workspace true . ## Joint Genotype VCF; gatk GenotypeGVCFs \; -R ${REF_DIR}${ref} \; -V gendb://${mapper}_${caller}_comb \; -O ${mapper}_${caller}_joint.vcf; ```. [test_vcfs.zip](https://github.com/broadinstitute/gatk/files/2295977/test_vcfs.zip). Thank you for your support on this problem!; Best,",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5113#issuecomment-413755772:122,validat,validator,122,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5113#issuecomment-413755772,2,['validat'],"['validates', 'validator']"
Security,"Default changed from 250 -> 20 in #5699 and exposed in #7450. Unfortunately, I don't recall if we did any benchmarking to spot check runtime/output changes, but I would expect 20 samples to be sufficient for estimating posterior means and standard deviations to the level needed for most downstream use cases.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5754#issuecomment-910220751:44,expose,exposed,44,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5754#issuecomment-910220751,1,['expose'],['exposed']
Security,"Did you clone the GATK repo ?; **Yes**; What JDK/version are you using ?; **openjdk version ""11.0.6""**. Execution failed for task ':gatkDoc'.; > Javadoc generation failed. Generated Javadoc options file (useful for troubleshooting): '/usr/bin/gatk/build/tmp/gatkDoc/javadoc.options'. * Try:; Run with --info or --debug option to get more log output. Run with --scan to get full insights. * Exception is:; org.gradle.api.tasks.TaskExecutionException: Execution failed for task ':gatkDoc'.; at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter$3.accept(ExecuteActionsTaskExecuter.java:166); at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter$3.accept(ExecuteActionsTaskExecuter.java:163); at org.gradle.internal.Try$Failure.ifSuccessfulOrElse(Try.java:191); at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(ExecuteActionsTaskExecuter.java:156); at org.gradle.api.internal.tasks.execution.ValidatingTaskExecuter.execute(ValidatingTaskExecuter.java:62); at org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecuter.execute(SkipEmptySourceFilesTaskExecuter.java:108); at org.gradle.api.internal.tasks.execution.ResolveBeforeExecutionOutputsTaskExecuter.execute(ResolveBeforeExecutionOutputsTaskExecuter.java:67); at org.gradle.api.internal.tasks.execution.ResolveAfterPreviousExecutionStateTaskExecuter.execute(ResolveAfterPreviousExecutionStateTaskExecuter.java:46); at org.gradle.api.internal.tasks.execution.CleanupStaleOutputsExecuter.execute(CleanupStaleOutputsExecuter.java:94); at org.gradle.api.internal.tasks.execution.FinalizePropertiesTaskExecuter.execute(FinalizePropertiesTaskExecuter.java:46); at org.gradle.api.internal.tasks.execution.ResolveTaskExecutionModeExecuter.execute(ResolveTaskExecutionModeExecuter.java:95); at org.gradle.api.internal.tasks.execution.SkipTaskWithNoActionsExecuter.execute(SkipTaskWithNoActionsExecuter.java:57); at org.gradle.api.internal.tasks.execution.SkipOnlyIfTaskExecuter.exec",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6466#issuecomment-590387973:951,Validat,ValidatingTaskExecuter,951,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6466#issuecomment-590387973,2,['Validat'],['ValidatingTaskExecuter']
Security,"Discussed with @kcibul (and others) offline. Conclusion: the majority of the validation code might end up being non-WDL with just one or two WDL tasks that call pthyon....etc., but it will be divided up into separate WDL tasks for now for development, and once it's done (or mostly) done, it can always be restructured to have the code in whatever place/structure makes the most sense going forward.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7352#issuecomment-883526280:77,validat,validation,77,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7352#issuecomment-883526280,1,['validat'],['validation']
Security,"Disq uses different code for finding container offsets compared to Hadoop-BAM, so that might be where the problem is coming from. However, I need access to the file that it's failing with to diagnose the issue. @jjfarrell how can I get a copy?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451880631:146,access,access,146,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451880631,1,['access'],['access']
Security,Do all those BAMs pass ValidateSamFile?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/541#issuecomment-112965444:23,Validat,ValidateSamFile,23,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/541#issuecomment-112965444,1,['Validat'],['ValidateSamFile']
Security,Does the bam pass `ValidateSamFile`?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6065#issuecomment-516480503:19,Validat,ValidateSamFile,19,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6065#issuecomment-516480503,1,['Validat'],['ValidateSamFile']
Security,"Does the problem go away if you use an output path with the 'hdfs://' scheme? E.g. _hdfs://namenode:8020/user/yaron/output.bam_ (where _namenode_ is the hostname of the namenode). There are two libraries being used internally for accessing the filesystem - the Hadoop filesystem API, and the NIO API - and they have slightly different behaviour if no scheme is provided. So to avoid problems it's best to give full paths with URI schemes for all input and output paths.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3066#issuecomment-407791242:230,access,accessing,230,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3066#issuecomment-407791242,1,['access'],['accessing']
Security,Don't forget to expose the file in the WDL.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5114#issuecomment-413203475:16,expose,expose,16,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5114#issuecomment-413203475,1,['expose'],['expose']
Security,"Done. Full results are on the internal presentation slides. The summary table follows:. | speedup vs async on a 1-core machine | slice | whole | intervals |; |-------------|----------|---------|----------|; | vcf | 0.91| 1.40| 0.57|; | bam (exome)| 40.42| 1.06| 1.02|; | bam (wgs)| 111.18| 1.21| 0.99|. This table compares the execution time of a single machine running PrintReads or SelectVariants, getting its input either directly from the Google bucket (NIO), or by first copying it with gsutil and then running off the local disk (with the async option turned on, allowing eager decompression of the stream - a feature the NIO code does not have). Each experiment is run four times, and each number here represents the ratio of two such experiments. Numbers larger than 1 indicate that NIO was faster. Each row is a different input file: vcf, small bam (exome), large bam (whole genome). Each column is a selection of what to read from the file (via the `-L` argument): a megabase slice, the whole file, or a long list of intervals. The NIO code relies heavily on prefetching, so it doesn't perform well with the many disjoint accesses of the right column. When processing only a small part of the (already small) vcf file, NIO loses out to copy + local processing. Everywhere else the direct-to-bucket ""NIO"" code performs quite well, up to 111x faster than the ""copy then process"" approach. I also ran the full set with async disabled. It makes a difference but NIO still wins and loses at the same places by similar margins (in particular the 111x win remains).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2424#issuecomment-284056956:1132,access,accesses,1132,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2424#issuecomment-284056956,1,['access'],['accesses']
Security,"Dsamjdk.compression_level=2 -jar /Applications/genomicstools/gatk/gatk-4.0.11.0/gatk-package-4.0.11.0-local.jar LearnReadOrientationModel -alt-table 13_tumor-alt.tsv -ref-hist 13_tumor-ref.metrics -alt-hist 13_tumor-alt-depth1.metrics -O 13_tumor-artifact-prior-table.tsv; 12:16:19.960 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Applications/genomicstools/gatk/gatk-4.0.11.0/gatk-package-4.0.11.0-local.jar!/com/intel/gkl/native/libgkl_compression.dylib; Nov 26, 2018 12:16:20 PM shaded.cloud_nio.com.google.auth.oauth2.DefaultCredentialsProvider warnAboutProblematicCredentials; WARNING: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a ""quota exceeded"" or ""API not enabled"" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/.; 12:16:20.176 INFO LearnReadOrientationModel - ------------------------------------------------------------; 12:16:20.177 INFO LearnReadOrientationModel - The Genome Analysis Toolkit (GATK) v4.0.11.0; 12:16:20.177 INFO LearnReadOrientationModel - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:16:20.177 INFO LearnReadOrientationModel - Executing as shlee@WMCF9-CB5 on Mac OS X v10.13.6 x86_64; 12:16:20.177 INFO LearnReadOrientationModel - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_111-b14; 12:16:20.177 INFO LearnReadOrientationModel - Start Date/Time: November 26, 2018 12:16:19 PM EST; 12:16:20.177 INFO LearnReadOrientationModel - ------------------------------------------------------------; 12:16:20.177 INFO LearnReadOrientationModel - ------------------------------------------------------------; 12:16:20.178 INFO LearnReadOrientationModel - HTSJDK Version: 2.16.1; 12:16:20.178 INFO LearnReadOrientationModel - Picard Version: 2",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-441721615:1500,authenticat,authentication,1500,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-441721615,1,['authenticat'],['authentication']
Security,"EDT] Executing as louisb@wm1b0-8ab on Mac OS X 10.9.5 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_45-b14; Version: Version:GATK.4.alpha-556-g73c02ea-SNAPSHOT JdkDeflater; 17:44:44.990 [main] INFO org.broadinstitute.hellbender.tools.PrintReads - Initializing engine; 17:44:45.124 [main] INFO org.broadinstitute.hellbender.tools.PrintReads - Done initializing engine; 17:44:45.151 [main] INFO org.broadinstitute.hellbender.engine.ReadsDataSource - Preparing readers for traversal; 17:44:45.153 [main] INFO org.broadinstitute.hellbender.engine.ReadsDataSource - Done preparing readers for traversal; 17:44:45.178 [main] INFO org.broadinstitute.hellbender.tools.PrintReads - Shutting down engine; [August 17, 2015 5:44:45 PM EDT] org.broadinstitute.hellbender.tools.PrintReads done. Elapsed time: 0.00 minutes.; Runtime.totalMemory()=257425408; java.lang.IllegalArgumentException: end must be >= start. start:2801961 end:2801960; at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:33); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:45); at org.broadinstitute.hellbender.engine.ReadWalker.lambda$traverse$25(ReadWalker.java:64); at org.broadinstitute.hellbender.engine.ReadWalker$$Lambda$43/308889081.accept(Unknown Source); at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); at java.util.Iterator.forEachRemaining(Iterator.java:116); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:512); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:502); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/828#issuecomment-131972247:1424,validat,validatePositions,1424,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/828#issuecomment-131972247,1,['validat'],['validatePositions']
Security,"Evaluation of THCA/STAD/LUAD TCGA WGS/WES CR concordance with SNP arrays was implemented on FC last summer and showed good performance. For WES, comparisons against GATK CNV and CODEX showed comparable to highly improved performance, respectively, with minimal parameter tuning. WGS comparisons were unavailable due to limitations of competing tools. This evaluation will be expanded to include CR/MAF concordance against PanCanAtlas ABSOLUTE results. Some curation of the samples could be performed; some batch effects were observed in some LC WGS LUAD samples. Comparisons to other tools will probably be removed for ease of maintenance. Will be adapted to fit into whatever framework arises from #4630; same goes for HCC1143 and CRSP validations.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4122#issuecomment-459833697:737,validat,validations,737,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4122#issuecomment-459833697,1,['validat'],['validations']
Security,FO PathSeqPipelineSpark - GCS max retries/reopens: 20; 17:39:19.246 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 17:39:19.246 INFO PathSeqPipelineSpark - Initializing engine; 17:39:19.246 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 18/04/24 17:39:19 INFO SparkContext: Running Spark version 2.2.0; 18/04/24 17:39:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/04/24 17:39:19 INFO SparkContext: Submitted application: PathSeqPipelineSpark; 18/04/24 17:39:20 INFO SecurityManager: Changing view acls to: username; 18/04/24 17:39:20 INFO SecurityManager: Changing modify acls to: username; 18/04/24 17:39:20 INFO SecurityManager: Changing view acls groups to:; 18/04/24 17:39:20 INFO SecurityManager: Changing modify acls groups to:; 18/04/24 17:39:20 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(username); groups with view permissions: Set(); users with modify permissions: Set(username); groups with modify permissions: Set(); 18/04/24 17:39:20 INFO Utils: Successfully started service 'sparkDriver' on port 46576.; 18/04/24 17:39:20 INFO SparkEnv: Registering MapOutputTracker; 18/04/24 17:39:20 INFO SparkEnv: Registering BlockManagerMaster; 18/04/24 17:39:20 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 18/04/24 17:39:20 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up; 18/04/24 17:39:20 INFO DiskBlockManager: Created local directory at /tmp/username/blockmgr-24b23f43-effa-45a6-8539-b9de234fa346; 18/04/24 17:39:20 INFO MemoryStore: MemoryStore started with capacity 366.3 MB; 18/04/24 17:39:20 INFO SparkEnv: Registering OutputComm,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:7276,Secur,SecurityManager,7276,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,7,"['Secur', 'authenticat']","['SecurityManager', 'authentication']"
Security,FeatureManager is an internal engine class that should not be exposed to the tool -- that is why FeatureContext exists as a wrapper. It is to prevent the client from doing bad things like closing important engine resources. See the comments at the top of the FeatureContext class:. ```; * Wrapper around FeatureManager that presents Feature data from a particular interval to a client tool; * without improperly exposing engine internals.; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/242#issuecomment-76763857:62,expose,exposed,62,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/242#issuecomment-76763857,1,['expose'],['exposed']
Security,"Finished refactoring the production code as detailed above, just need to add some tests. Results are exactly the same as before in single-sample mode---except for allele-fraction-only mode. This is because I refactored all existing segmenters (there were separate ones for copy-ratio-only, allele-fraction-only, and copy-ratio + allele-fraction) as special cases of a single multisample segmenter; however, the Gaussian kernel in the old allele-fraction-only segmenter was missing a normalization factor that is now present in the new multisample segmenter. Thus, users who previously ran in allele-fraction-only mode will have to retune parameters to achieve the same level of sensitivity. I expect that this will be a very small number of users (if any---note that allele-fraction-only mode isn't even exposed in the WDL), but we can probably mention it in the release notes. Might need to update a figure, etc. as well in the tutorial.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6499#issuecomment-611833344:804,expose,exposed,804,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6499#issuecomment-611833344,1,['expose'],['exposed']
Security,"Finished, [2D CNN inference](https://github.com/broadinstitute/gatk/blob/master/src/main/java/org/broadinstitute/hellbender/tools/walkers/vqsr/CNNScoreVariants.java) and training merged, many new issues spawned.:; - [X] A cool(?) name CNNScoreVariants; - [x] Model training script (in Python, eventually in Java); - [x] Pretrained model for WGS; - [x] Pretrained model for WEx (still being validated and was only trained on NA12878); - [x] Model inference and VCF annotation (in Java); - [x] Solution for applying filters based on CNN score cutoff (tranches.py script); - [x] Currently just re-filtering. Still no joint calling solution...; - [x] Hyperparameters optimized for small 2d model similar performance but .25 params.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4225#issuecomment-377623706:390,validat,validated,390,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4225#issuecomment-377623706,1,['validat'],['validated']
Security,"For @davidadamsphd when you get tired of validation tickets (if you don't have the bandwidth for this one, feel free to re-assign).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1047#issuecomment-150586732:41,validat,validation,41,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1047#issuecomment-150586732,1,['validat'],['validation']
Security,For @lbergelson. The general plan for HaplotypeCaller work in Q2 is:. -I will work on validation/testing of the walker version; -Louis will take the Spark version; -Adam will work on further HC performance optimizations (potentially for both walker and spark),MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1639#issuecomment-202447009:86,validat,validation,86,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1639#issuecomment-202447009,1,['validat'],['validation']
Security,"For Travis, the best way might be to put the test input files in a publicly-accessible location and use the API key.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/529#issuecomment-106015822:76,access,accessible,76,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/529#issuecomment-106015822,1,['access'],['accessible']
Security,"For germline CNV:; It would be interesting to know which details (e.g. read group tags such as platform, model or also average insert size) are important for choosing additional Fastq/BAM files for creating a Panel of Normals (PON), when one does not have access to a batch. Which results from the GATK quality metrics would be most useful for choosing such datasets?. I'm thinking especially about e.g. choosing other Fastq/BAM files from the Personal Genomes Project. It would also be interesting to know if something like a ""Panel of Unnormals"" could be created, e.g. by choosing datasets from similar patients according to participant survey results from the Personal Genomes Project (https://my.pgp-hms.org/google_surveys). If a Panel of Normals can be used to reject spurious read counts, then a Panel of Unnormals could help to not reject rare read counts. (Hypermobile) Ehlers-Danlos-Syndrome as an unsolved and probably multi-gene case might be a perfect example, because it is already known that many genes exist with similar effects on connective tissue, i.e. hypermobility.; There are 115 hits grepping the above surveys for Ehlers-Danlos and at least a few of these might offer full fastq/bam files.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2881#issuecomment-358092692:256,access,access,256,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2881#issuecomment-358092692,1,['access'],['access']
Security,"For now, the tool will just supply the count and not make any decision as to whether the variant validated.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5059#issuecomment-408433167:97,validat,validated,97,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5059#issuecomment-408433167,1,['validat'],['validated']
Security,"For repeated operators (whether xIyI or xMyM), I think GATK3 has/had a function to simplify cigars to ""sanitize"" that situation. In terms of desired behavior, we don't want to filter out the reads, we want to transform them to be processable without difficulty. I think xIyD should be considered valid.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/428#issuecomment-95056320:103,sanitiz,sanitize,103,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/428#issuecomment-95056320,1,['sanitiz'],['sanitize']
Security,"For testing #3069, creating a Java test that shows that the GKL messages are controlled by the [log4j logLevel](https://www.tutorialspoint.com/log4j/log4j_logging_levels.htm).; Also, this branch should be rebased. A fix for the test failures, due to an issue with 2 factor authentication for [GIt Large File Storage](https://git-lfs.github.com/) of test data was recently merged.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3177#issuecomment-312404141:273,authenticat,authentication,273,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3177#issuecomment-312404141,1,['authenticat'],['authentication']
Security,"For the record, without this validation step in place this test fails with the following out of memory error (which we don't fully understand yet, but it's basically GroupByKey going crazy). @jean-philippe-martin do you think this deserves a separate ticket to investigate further? . java.lang.OutOfMemoryError: Java heap space; at java.util.Arrays.copyOf(Arrays.java:3236); at java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:113); at java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93); at java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:140); at com.google.cloud.dataflow.sdk.util.ExposedByteArrayOutputStream.write(ExposedByteArrayOutputStream.java:82); at java.io.DataOutputStream.write(DataOutputStream.java:107); at java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1877); at java.io.ObjectOutputStream$BlockDataOutputStream.setBlockDataMode(ObjectOutputStream.java:1786); at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1189); at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348); at com.google.cloud.dataflow.sdk.coders.SerializableCoder.encode(SerializableCoder.java:113); at org.broadinstitute.hellbender.engine.dataflow.coders.GATKReadCoder.encode(GATKReadCoder.java:38); at org.broadinstitute.hellbender.engine.dataflow.coders.GATKReadCoder.encode(GATKReadCoder.java:27); at com.google.cloud.dataflow.sdk.transforms.join.UnionCoder.encode(UnionCoder.java:82); at com.google.cloud.dataflow.sdk.transforms.join.UnionCoder.encode(UnionCoder.java:37); at com.google.cloud.dataflow.sdk.util.WindowedValue$FullWindowedValueCoder.encode(WindowedValue.java:528); at com.google.cloud.dataflow.sdk.util.WindowedValue$FullWindowedValueCoder.encode(WindowedValue.java:472); at com.google.cloud.dataflow.sdk.coders.IterableLikeCoder.encode(IterableLikeCoder.java:96); at com.google.cloud.dataflow.sdk.coders.IterableLikeCoder.encode(IterableLikeCoder.java:42); at com.google.cloud.da",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/668#issuecomment-122949556:29,validat,validation,29,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/668#issuecomment-122949556,3,"['Expose', 'validat']","['ExposedByteArrayOutputStream', 'validation']"
Security,"For what it's worth, this works but I should improve the error message. If you don't have the right permissions, currently you get an error that's something like this:. ```; htsjdk.samtools.util.RuntimeIOException: Error opening file: file:///home/jpmartin//gatk/gs:/bobs-bucket-you-cant-write-to/test-output/printtogcs.bam; ```. This is both; - confusing: it makes it look like the path is wrong even though the code actually tried the correct path; - improvable: it'd be nice if we gave the reason there was an error (403 Forbidden: user jpmartin doesn't have write access)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2422#issuecomment-342255531:568,access,access,568,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2422#issuecomment-342255531,1,['access'],['access']
Security,"GATK consumes hdf5, but doesn't expose an API for it. https://github.com/broadinstitute/hdf5-java-bindings/issues/15 is a more appropriate place for that question.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5953#issuecomment-494358138:32,expose,expose,32,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5953#issuecomment-494358138,1,['expose'],['expose']
Security,"GCS NIO provider is public already, accessible at:; https://github.com/GoogleCloudPlatform/gcloud-java/tree/gcs-nio. I'm working on getting the [integration tests](https://github.com/jean-philippe-martin/gcloud-java/blob/integration-tests/gcloud-java-contrib/gcloud-java-nio/src/test/java/com/google/gcloud/storage/contrib/nio/ITGcsNio.java) and examples ready, and later on getting the NIO provider onto the main branch so it's part of the default SDK download. The integration tests pass, showing it's now possible to read/write GCS files via the NIO interface. So technically you could start using it now, though it's probably easier to wait until those tests and the examples have passed the pull request gauntlet.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1547#issuecomment-191974649:36,access,accessible,36,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1547#issuecomment-191974649,1,['access'],['accessible']
Security,GVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9HZW5vdHlwaW5nRW5naW5lLmphdmE=) | `49.123% <0%> (-0.442%)` | `45 <0> (+1)` | |; | [...ute/hellbender/utils/variant/GATKVCFConstants.java](https://codecov.io/gh/broadinstitute/gatk/pull/2655?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy92YXJpYW50L0dBVEtWQ0ZDb25zdGFudHMuamF2YQ==) | `50% <0%> (-16.667%)` | `3 <2> (+2)` | |; | [...itute/hellbender/tools/walkers/vqsr/ApplyVQSR.java](https://codecov.io/gh/broadinstitute/gatk/pull/2655?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3IvQXBwbHlWUVNSLmphdmE=) | `75% <100%> (ø)` | `55 <0> (ø)` | :arrow_down: |; | [...roadinstitute/hellbender/engine/ProgressMeter.java](https://codecov.io/gh/broadinstitute/gatk/pull/2655?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUHJvZ3Jlc3NNZXRlci5qYXZh) | `93.75% <0%> (-1.563%)` | `21% <0%> (-1%)` | |; | [...r/tools/walkers/variantutils/ValidateVariants.java](https://codecov.io/gh/broadinstitute/gatk/pull/2655?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhcmlhbnR1dGlscy9WYWxpZGF0ZVZhcmlhbnRzLmphdmE=) | `81.081% <0%> (+0.484%)` | `24% <0%> (+6%)` | :arrow_up: |; | [...org/broadinstitute/hellbender/utils/MathUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/2655?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9NYXRoVXRpbHMuamF2YQ==) | `81.009% <0%> (+1.711%)` | `170% <0%> (+30%)` | :arrow_up: |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/2655?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `74.026% <0%> (+1.948%)` | `35% <0%> (ø)` | :arrow_down: |; | [...e/hellbender/engine/spark/SparkContextFactory.java](https://codecov.io/gh/broadinstitute/gatk/pull/2655?src=pr&el=tree#diff-c3JjL21,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2655#issuecomment-299088185:2403,Validat,ValidateVariants,2403,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2655#issuecomment-299088185,1,['Validat'],['ValidateVariants']
Security,"Glad to hear you were able to make progress. We're open to suggestions around improving the tooling for this. For instance, you mentioned wanting to redo samples -- we already have support in GenomicsDB for querying by sample. We should be able to expose that at the GATK level. As long as you're okay with renaming the sample when you re-generate the gVCFs that should work. . Technically we could expose support to modify existing samples, but that get's a bit hairy because of the way data is retrieved. . I'm not sure why the queries for intact chromosomes take so much longer. Since you were able to replicate with a single sample, ~7m interval is there any chance you can share just that bit (workspace, or even better that portion of the gvcf) and we can take a deeper look?. To your question about whether GenomicsDBImport includes variants that span the specified import interval: it will definitely include variants that start in those intervals, but it won't always store variants that start before the import interval. For deletions, we have some special handling for variants that start before the interval - they should show up represented by the star allele, but I don't think this is the case for insertions starting before the import interval.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1221066848:248,expose,expose,248,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1221066848,2,['expose'],['expose']
Security,"Good catch @EdwardDixon. From @cmnbroad 's Oct 12 post:; >We may need to provide a fallback environment for those (I'll try to get resolution on that). If it turns out we do, I'm actually not suggesting the fallback be automatic (3 in your list), just that we have a **graceful failure mode and an instructive error message.**. We have this now, right? Just want to be sure. If there's more work to be done, let's hash it out on another PR.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-438735712:414,hash,hash,414,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-438735712,1,['hash'],['hash']
Security,"Good catch. I think hc7b2577_8 might be specific for linux64. We can try removing that hash, but I don't have a mac, so I can't test this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4061#issuecomment-355645250:87,hash,hash,87,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4061#issuecomment-355645250,1,['hash'],['hash']
Security,Got the go-ahead and the access has been changed.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2726#issuecomment-302518914:25,access,access,25,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2726#issuecomment-302518914,1,['access'],['access']
Security,"Great addition to the list. [`htsjdk.samtools.metrics.MetricsFile`](https://github.com/samtools/htsjdk/tree/master/src/java/htsjdk/samtools/metrics) are a nice way of generating summary outputs, and later generating summary reports. For those without access to the above repo, among other features, `ComparePipelineTestOutput` generates HTML reports on directories of metrics files. Still, I believe one of the biggest complaints by comp-bios was that after improving an algorithm (ex: PairHMM), one is penalized by having to update lines of expected output spread across dozens of files.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/69#issuecomment-67238583:251,access,access,251,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/69#issuecomment-67238583,1,['access'],['access']
Security,"Great! And yes, LL will be optimized separately for SNPs and INDELs. How about this for a first workflow to target?. 1) Run ExtractVariantAnnotations on a training set of chromosomes. You can keep training/truth labels as in Best Practices, for now.; 2) Run TrainVariantAnnotationsModel on that. We'll use the truth scores generated here for any sensitivity conversions---i.e., we'll be calibrating scores only to the truth sites that are contained in the training set of chromosomes.; 3) Use the trained model to run a single shard of ScoreVariantAnnotations on a validation set of chromosomes.; 4) Run some variation of the above script on the resulting outputs to determine SNP and INDEL score thresholds for optimizing the corresponding LL scores. We can also add some code to the script to use the truth scores from step 2 to convert these score thresholds into truth-sensitivity thresholds.; 5) Provide these truth-sensitivity thresholds to ScoreVariantAnnotations and use them to hard filter. Evaluate on a test set of chromosomes. If all looks good, we can later move steps 3-4 into the train tool and automate the passing of sensitivities in 5 via outputs in the model directory. This will let us keep the basic interface of ScoreVariantAnnotations the same, but we'll have to add a few basic parameters to TrainVariantAnnotationsModel to control the train/validation split. So I think all this branch is missing is step 5---we'll simply need to add command-line parameters for the SNP/INDEL sensitivity thresholds and then do the hard filtering in the VCF writing method highlighted above. Do you think you can handle implementing that in this branch, and then the rest at the WDL level? I can help with the python script for the LL stuff (or anything else), if needed. Not sure if you got a chance to check out what your collaborators are doing in the methods you're looking to compare against, but it would be good to understand if this basic scheme for train/validation/test splitting can",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1068345084:565,validat,validation,565,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1068345084,1,['validat'],['validation']
Security,"Great! Now that we know it works, it can wait to be reviewed. It's a small change but this feature had stayed untested for too long. In fact an even better way of doing this, if we have the energy & desire, would be to set up a separate bucket with a separate project. I *think* if we do it right we may then be able to have a fully automated test of the explicit credentials, as the default credentials would have access to the ""normal"" test data but we'd make sure the default user is not authorized to access the separate bucket (so we have to use the explicit credentials).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2879#issuecomment-306295687:415,access,access,415,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2879#issuecomment-306295687,3,"['access', 'authoriz']","['access', 'authorized']"
Security,"Great, thanks!. On Mon, Dec 3, 2018 at 10:15 AM David Benjamin <notifications@github.com>; wrote:. > @meganshand <https://github.com/meganshand> I ran the ""Full Pipeline""; > workflows in a clone of your FC workspace:; > https://portal.firecloud.org/#workspaces/broad-firecloud-dsde/copy-of-megans-m2-mito-validations.; > I did not run any of the things that generate graphs because they were; > harder for me to understand. To compare the new results to your previous; > ones, I took all variants that were either PASS or had only the; > contamination filter applied, extracted just the locus and alleles columns,; > then manually inspected the diff. For the 5% and 50% spike-ins there were; > usually no differences at all, while for the 1% spike-in the difference was; > usually 2-5 variants that straddled the LOD threshold.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/pull/5473#issuecomment-443745103>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AGRhdMwCcuQyzMweZjxWrXBODTCBaOSIks5u1T_-gaJpZM4Y9STI>; > .; >. -- ; Laura Doyle Gauthier, Ph.D.; Associate Director, Germline Methods; Data Sciences Platform; gauthier@broadinstitute.org; Broad Institute of MIT & Harvard; 320 Charles St.; Cambridge MA 0214",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5473#issuecomment-443751026:305,validat,validations,305,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5473#issuecomment-443751026,1,['validat'],['validations']
Security,"Grr, looks like this slipped by us in #4288. See my comments there---why does this validate in womtool? We got bit by a similar issue in #4281.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4319#issuecomment-362071370:83,validat,validate,83,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4319#issuecomment-362071370,1,['validat'],['validate']
Security,"HG00731 fails with a similar error:. ```; 18/04/11 16:08:40 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 98.0 in stage 42.0 (TID 49620, cwhelan-hg00731-cram-samtools-bam-feature-w-4.c.broad-dsde-methods.internal, executor 43): java.lang.IllegalArgumentException: Invalid interval. Contig:chr6 start:34662153 end:34662143; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.ContigAlignmentsModifier.computeNewRefSpanAndCigar(ContigAlignmentsModifier.java:163); at org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.ContigAlignmentsModifier.clipAlignmentInterval(ContigAlignmentsModifier.java:42); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.removeOverlap(CpxVariantInterpreter.java:179); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.deOverlapAlignments(CpxVariantInterpreter.java:122); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.furtherPreprocess(CpxVariantInterpreter.java:79); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.lambda$inferCpxVariant$bdd686a3$1(CpxVariantInterpreter.java:51); at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTa",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4648#issuecomment-380510575:380,validat,validateArg,380,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4648#issuecomment-380510575,2,['validat'],"['validateArg', 'validatePositions']"
Security,HI @droazen I see you were on this issue and generated a PR but could not merge because test case failures. I wanted to check if you were able to make progress on this. Within my organization infosec independently reviewed and have denied use of GATK :( . Let me know if you have an ETA for security fix update. Thank you!,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8215#issuecomment-1678986791:291,secur,security,291,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8215#issuecomment-1678986791,1,['secur'],['security']
Security,"HI @jonn-smith ; Creating this issue ticket as discussed. ; Issue: The header of that particular file has a hash mark # before it, because of that the parser wont work properly. A bug seems to have been introduced in the tribble indexing code for TSV files.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6223#issuecomment-545117559:108,hash,hash,108,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6223#issuecomment-545117559,1,['hash'],['hash']
Security,"Hello @dagsbio and thank you for your question. It sounds like there was an issue with your bam file, possibly with the header version line. We support up through bam v1.6. You can also try running ValidateSamFile on the bam. I would recommend however that you direct your questions/troubleshooting requests to the new gatk support forum here: https://gatk.broadinstitute.org/hc/en-us/community/topics",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6350#issuecomment-571636329:198,Validat,ValidateSamFile,198,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6350#issuecomment-571636329,1,['Validat'],['ValidateSamFile']
Security,"Hello @jackycsie. I would request that you direct this sort of question to the GATK Support forums https://gatkforums.broadinstitute.org/gatk/categories/gatk-support-forum. Regarding the BAM size there are a number of reasons the file could have shrunk, the most likely answer is that the GATK applied a different level of compression to the output bam than was applied to your inputs. I would recommend you run CountReads and calling ValidateSamFile on the outputs for your run and if there appears to be an issue with the output you post a question about it on the Support Forums.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6236#issuecomment-547452764:435,Validat,ValidateSamFile,435,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6236#issuecomment-547452764,1,['Validat'],['ValidateSamFile']
Security,"Hello @liuyihhha, I'm sorry to hear that. We should probably include a more prominent warning that something like this could happen if you disable all of the readfilters for the tool some of them are there just to make sure everything is formatted correctly and doesn't crash. If you `--disable-tool-default-read-filters` it disables all of them including the basic ""can the GATK even parse this read"" style readfilters. I would recommend adding back in some of the more basic read filters and seeing if you still get this error. Specifically I would recommend trying with the following arguments:; ```; --disable-tool-default-read-filters true; -RF WellformedReadFilter; -RF GoodCigarReadFilter; ```; As for what about SplitNCigarReads could be causing the reads to be invalid I would like to have some idea of what the issue is. You could try running `ValidateSamFile` on your input to Mutect2 and hopefully that will spit back some readnames that are invalid which should make it easier to diagnose. Given the operations in SplitNCigarReads and the nature of your stacktrace it wouldn't surprise me if there is something wrong/messy about the cigars output by that tool in some cases.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8224#issuecomment-1450266657:854,Validat,ValidateSamFile,854,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8224#issuecomment-1450266657,1,['Validat'],['ValidateSamFile']
Security,"Hello, Could you tell me the exact source websites of funcotator_dataSources.v1.7.20200521g? I did not find it in your Google Cloud (genomics-public-data).; Besides, I used this code to download`./gatk-4.1.9.0/gatk FuncotatorDataSourceDownloader --germline --validate-integrity --extract-after-download; `, but the error appeared as following:`Nov 18, 2023 1:15:05 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 13:15:05.202 INFO FuncotatorDataSourceDownloader - ------------------------------------------------------------; 13:15:05.203 INFO FuncotatorDataSourceDownloader - The Genome Analysis Toolkit (GATK) v4.1.9.0; 13:15:05.203 INFO FuncotatorDataSourceDownloader - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:15:05.203 INFO FuncotatorDataSourceDownloader - Executing as yaoxq@mu01 on Linux v3.10.0-693.el7.x86_64 amd64; 13:15:05.203 INFO FuncotatorDataSourceDownloader - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_151-b12; 13:15:05.203 INFO FuncotatorDataSourceDownloader - Start Date/Time: November 18, 2023 1:15:04 PM CST; 13:15:05.203 INFO FuncotatorDataSourceDownloader - ------------------------------------------------------------; 13:15:05.203 INFO FuncotatorDataSourceDownloader - ------------------------------------------------------------; 13:15:05.204 INFO FuncotatorDataSourceDownloader - HTSJDK Version: 2.23.0; 13:15:05.204 INFO FuncotatorDataSourceDownloader - Picard Version: 2.23.3; 13:15:05.204 INFO FuncotatorDataSourceDownloader - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 13:15:05.204 INFO FuncotatorDataSourceDownloader - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 13:15:05.204 INFO FuncotatorDataSourceDownloader - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 13:15:05.204 INFO FuncotatorDataSourceDownloader - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 13:15:05.204 I",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8275#issuecomment-1817434417:259,validat,validate-integrity,259,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8275#issuecomment-1817434417,1,['validat'],['validate-integrity']
Security,"Hello,. I think we're pretty close on porting this. However, i wanted to ask again if I could get access to the test data used in GATK3 VariantEvalWalkerUnitTest and VariantEvalIntegrationTest. We dont need to port those files to GATK4 unless some of them already exist; however, having these files would help confirm the ported tool is behaving exactly as GATK3. Second, I am hoping to confirm a behavior for FeaureContext: many GATK3 stratifiers follow roughly this pattern (VariantEval / knownCNVsFile being an example). The pattern is: 1) some VCF/BED file provided as an argument, 2) in initialize(), the walker reads this file in memory into a List<GenomeLoc> or IntervalTree<GenomeLoc>. This is so each locus can quickly find overlapping intervals. In GATK4, there is no longer a point in loading the whole file into memory, right? If I define an argument as FeatureInput<Feature>, it will automatically be initialized. FeatureContext.getValues() seems to give me overlapping intervals (including partial overlaps). Therefore there is no reason to try to replicate that GATK3 pattern of reading intervals into memory. Can someone confirm this is correct?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/616#issuecomment-343563884:98,access,access,98,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/616#issuecomment-343563884,1,['access'],['access']
Security,"Here is a recap of what we discussed today during the CNV meeting:. For the first round of evaluations we decided to run Germline CNV pipeline on TCGA exomes using a range of key hyperparameters (namely psi-t-scale and p-alt) and establish the base level performance metrics using output of GenomeSTRiP on matched WGS samples as ground truth. . @mbabadi could you come up with a good range of hyperparameter values that you think should be cross-validated?. In particular we need to:. - Dockerize tools we will be evaluating against (XHMM, CODEX2, CLAMMS, GenomeSTRiP); - Write a WDL that runs Germline CNV that scatters across range of key hyperparameters and outputs array of VCFs; - Write VCF processors for output of CLAMMS and CODEX2 ; - Write WDLs for running XHMM, CODEX2, CLAMMS and GenomeSTRiP that output VCFs; - Write WDL that takes results of the above and uses @mbabadi 's gCNV evaluation python modules(located here /dsde/working/mehrtash/gCNV_theano_eval) to output performance metrics; - Decide on an automatic evaluation framework. For the next round of evaluations we need to:. - Decide on appropriate metrics for evaluating performance on trios and write scripts that implement them; - Expand the range of hyperparameters in search space (possibly include different bin sizes, GC vs no GC correction, and fragment mid point coverage collection vs largest fragment overlap coverage collection); - Use gnomAD subset of matched WES/WGS pairs for validation",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4123#issuecomment-362075071:446,validat,validated,446,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4123#issuecomment-362075071,2,['validat'],"['validated', 'validation']"
Security,"Here's an idea. It may be too much work, for now, and I'm fine with this code as it stands, but perhaps others are not. Anyway, the idea is this:. The constructor gets one extra argument: A lambda that serves as a validation function of the form; ```; void validate( int contig, int start, int end );; ```; It either throws an exception or returns silently.; Every client of SVInterval thus documents its intended conventions, and makes certain that those conventions are being adhered to.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5157#issuecomment-418868380:214,validat,validation,214,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5157#issuecomment-418868380,2,['validat'],"['validate', 'validation']"
Security,"Here's how these scripts are organized and why they take the form it is now:. How to run; * `manage/project.sh` is the ""executable""; * paths for VCF files (zipped or not) from PacBio callsets on CHM haploids, and Manta's VCF on the mixture should be provided to `manage/project.sh`, and; * paths for two versions of GATK-SV callsets; one is fine, but scripts in the sub-directory `manage` must be modified. Two GATK-SV vcf files are requested because this would allow one to compare if a supposedly improvement would make our raw sensitivity/specificity better or worse, that was the use case [here](https://github.com/SHuang-Broad/GATK-SV-callset-regressionTest), and; * paths to where results are to be stored, one for each GATK-SV callset must be given and ; * path to where to store the results of comparing the two callsets; * several GNU bash utilities are expected, `guniq` and `gsort`, when run on a Mac, as well as `bedtools`. and what to expect; * the scripts checks the VCF files, prints to screen a slew of information that one can pipe, or simplely browse through.; * the scripts also outputs the ID's of variants from each of the two GATK callsets that are ""validated"" by PacBio haploid calls. Misc points:; * watch out for ""duplicated"" records, as sometimes different assembly contigs mapped to the same locations have slightly different alleles (SNP, for example) hence both would be output, but there aren't many such records based on experience; * there are also some variants that we output to the VCFs having size <50 or >50K, both of which are filtered upfront and saved separately.; * The scripts started when we first call insertions, deletions, inversions and small duplications, and back then PacBio call sets on the CHM haploids were not available, so Manta's calls were used as ""reference"", that explains why they are referred to throughout the project",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4406#issuecomment-365730030:1172,validat,validated,1172,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4406#issuecomment-365730030,1,['validat'],['validated']
Security,"Here's some old code that uses SamLocusIterator (from tfennel) that AllelicCapseg can adapt for now. From Tim:; ""They key to making this nice and simple is the SamLocusIterator class, which given a BAM file and a list of intervals, will give you pileups at each position in the intervals, filtered how you want them, and even provide convenience methods to access the exact base per read that is piled up at the site etc. The really nice things about doing it this way is that the constructor to SamLocusIterator takes a simple parameter to tell it whether to use an index/query mechanism (similar to what you're doing now) or to just read the BAM serially up until the last interval is reached and output the loci of interest. Running the below with ~100k sites on a standard exome (15GB or so) without using the index took only about 15 minutes."". ```; public void pileup(final File bam, final IntervalList intervals, final int minQ, final File outputFile) {; final int MAX_INTERVALS_FOR_INDEX = 25000; // just a guess, not sure what the right number is. final SamLocusIterator iterator = new SamLocusIterator(new SAMFileReader(bam), intervals, intervals.size() < MAX_INTERVALS_FOR_INDEX);; iterator.setEmitUncoveredLoci(false);; iterator.setQualityScoreCutoff(minQ);. final BufferedWriter out = IoUtil.openFileForBufferedWriting(outputFile); // will automatically gzip if filename ends with .gz; try {; while (iterator.hasNext()) {; final SamLocusIterator.LocusInfo locus = iterator.next();; int a=0, c=0, g=0, t=0;. for (final SamLocusIterator.RecordAndOffset rec : locus.getRecordAndPositions()) {; switch (rec.getReadBase()) {; case 'A' : ++a; break;; case 'C' : ++c; break;; case 'G' : ++g; break;; case 'T' : ++t; break;; }; }. out.append(locus.getSequenceName() + ""\t"" + locus.getPosition() + ""\t"" + a + ""\t"" + c + ""\t"" + g + ""\t"" + t + ""\t"");; }. out.close(); ; }; catch (IOException ioe) { throw new RuntimeIOException(ioe); }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/335#issuecomment-88102420:357,access,access,357,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/335#issuecomment-88102420,1,['access'],['access']
Security,Hey @droazen can you please give me authorization to merge PRs? Or can you please merge this PR? Thanks.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2812#issuecomment-306374281:36,authoriz,authorization,36,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2812#issuecomment-306374281,1,['authoriz'],['authorization']
Security,"Hey @lbergelson, @droazen or @jamesemery: I'm hoping we could finalize this issue. I am unable to commit directly to this branch. To respond to the suggestions from @droazen I made this PR which i am hoping someone can merge into this feature branch (https://github.com/broadinstitute/gatk/pull/8871). . To recap:; - This PR is driven by a request from some of DISCVRseq's users to restore a GATK3 behavior where all source IDs are saved for variants when multiple VCFs are merged.; - This PR would expose this as an opt-in feature, and should not change any default GATK4 behavior. Therefore existing variant merge code should be unchanged.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8752#issuecomment-2206804137:499,expose,expose,499,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8752#issuecomment-2206804137,1,['expose'],['expose']
Security,"Hey @rsasch -- Just thinking out loud here… but does WDL seem like the right ""language"" to implement these sort of checks? It just feels like there is a ton of boilerplate, the WDL/bash constructs are a bit hard to follow, and I'm sure the development cycles were pretty slow waiting for cromwell to spin up all those VMs, etc. An alternative would be to write all these validations as a python script, which could make better use of structure (ie authenticating once, etc) and I bet would be quite a bit more readable. You could also run it locally if needed for development, debugging and then ultimately wrap it into a single-task WDL so it could be run easily from Terra. Happy to chat more",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7352#issuecomment-883435010:371,validat,validations,371,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7352#issuecomment-883435010,2,"['authenticat', 'validat']","['authenticating', 'validations']"
Security,"Hi @aderzelle. Thanks for your interest in NeuralNetInference. The tool currently has pre-beta`Experimental` status, so for now it should be used for evaluation purposes only. We expect to release another version of GATK, probably within the next week, that will include an updated version of the tool that will include a default architecture file, along with some additional tools for things like training. The tools will still be `Experimental`, but should be a bit easier to use. In the meantime, there is a bit more information about how to access the existing hd5 file [here](https://github.com/broadinstitute/gatk/issues/4511). Note that in the next release, the name of the tool will have changed to CNNScoreVariants. @lucidtronix anything else to add here?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4559#issuecomment-375647411:545,access,access,545,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4559#issuecomment-375647411,1,['access'],['access']
Security,Hi @cmnbroad - I removed a space in the path and re-ran ValidateVariants. It went through throwing no errors. Many thanks.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4657#issuecomment-380989518:56,Validat,ValidateVariants,56,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4657#issuecomment-380989518,1,['Validat'],['ValidateVariants']
Security,"Hi @dislek, this type of question might be more appropriate for the GATK forum (and has actually been previously asked there, see https://gatkforums.broadinstitute.org/gatk/discussion/12543/determinegermlinecontigploidy and the thread linked in the answer). The priors file is a TSV file that is described in the tool documentation for DetermineGermlineContigPloidy. You should manually construct this TSV file, with contig names appropriate to the reference being used and prior probabilities appropriate for the quality of your data set---probably the suggestions in the example in the tool documentation are a reasonable place to start. However, you may want to validate the ploidy calls output by DetermineGermlineContigPloidy using samples where the truth is known to ensure that the model is trained correctly using your choice of priors and other model parameters.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5719#issuecomment-467252385:665,validat,validate,665,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5719#issuecomment-467252385,1,['validat'],['validate']
Security,Hi @icemduru ; Looks like your slurm workload manager was configured to have a limit of 48GBs of maximum process memory size per execution. Your java instance is set with -Xmx45G which will cover most of this limit and leaves only a handful of memory space for the native GenomicsDB library. Native libraries work above the heapsize so it is better for you to set your -Xmx to a more sensible size of 8~12GB and leave rest of the memory space to the native library to use. . Keep in mind that this memory limit on slurm could be set per user not per task therefore you may need to run a single contig at a time or maybe 2 of them simultaneously. Otherwise slurm may interefere with all the tasks and cancel all your jobs. . One final reminder. We strongly recommend users to set the temporary directory to somewhere else other than /tmp. Slurm workload manager interferes with this preference and sometimes results in premature termination of the gatk processes due to deletion of extracted native library and accessory files. . I hope this helps.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8918#issuecomment-2283694332:1010,access,accessory,1010,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8918#issuecomment-2283694332,1,['access'],['accessory']
Security,Hi @jonn-smith I had some success getting outputs with this. However ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/funcotator/ is no longer accessible. The most recent version of I accessed on 3/19/2018 still contained at least one error. I'm trying to correct them myself as I go using a more recent build of GATK but it would be helpful if the data files required by this program were available. The one I found is in `gencode_xrefseq.config` where it references a source that doesn't exist and I fixed that. After that I was able to get outputs with hg38. Thanks for your work on this!. I'd also point out there are a lot of fields in the MAF with `__UNKNOWN__` as the entry,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4521#issuecomment-383387645:150,access,accessible,150,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4521#issuecomment-383387645,2,['access'],"['accessed', 'accessible']"
Security,"Hi @praneetha92 ,. Where did this file come from? GATK doesn't produce GLs anymore. It looks like there are the wrong number of likelihoods for one of your genotypes. Unfortunately our validation tool doesn't check this, but can you try vcf-validator? You can find it here: https://vcftools.github.io/perl_module.html#vcf-validator",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6897#issuecomment-710135263:185,validat,validation,185,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6897#issuecomment-710135263,3,['validat'],"['validation', 'validator']"
Security,"Hi @tushu1232. The index image is a new feature that's not fully exposed in a friendly. It's the 5 indexes that bwa requires baked into a single file for easy distribution. ( .amb, .ann, .bwt, .pac, .sa ) We're going to add an official tool to generate it in the near future, but at present the only way to do so is to write a tool yourself that calls into org.broadinstitute.hellbender.utils.bwa.BwaMemIndex.createIndexImage();",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2414#issuecomment-282128828:65,expose,exposed,65,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2414#issuecomment-282128828,1,['expose'],['exposed']
Security,"Hi Adam,. Maybe I'm thinking naively here - and I don't have access to a complete and proper Spark cluster for rigorous testing - but just as a simple test of loading a VCF via Spark, I took [PrintReadsSpark.java](https://github.com/broadinstitute/gatk/blob/030858bc08328200b9df287db2571b907189ec66/src/main/java/org/broadinstitute/hellbender/tools/spark/pipelines/PrintReadsSpark.java) and performed following updates:; - Copied `./src/test/resources/org/broadinstitute/hellbender/utils/SequenceDictionaryUtils/test.vcf` into the local directory.; - Renamed the copy of `PrintReadsSpark.java` as `PrintVCFSpark.java`; - Added `import org.broadinstitute.hellbender.utils.variant.Variant;`; - Added `import org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSource;`; - As a test, I changed to the `runTool` method with the following to print the information in the first element in the RDD:. ``` Java; JavaRDD<Variant> rddParallelVariants =; variantsSparkSource.getParallelVariants(output);. System.out.println( rddParallelVariants.first().toString() );; ```. And after re-compiling GATK and running `PrintVCFSpark`, I got the following to print the first element of the RDD:. ``` Bash; $ ./gatk-launch PrintVCFSpark --input test.vcf --output test.vcf. Running:; /home/pgrosu/me/hellbender_broad_institute/gatk/build/install/gatk/bin/gatk PrintVCFSpark --input test.vcf --output test.vcf; [February 14, 2016 7:04:16 PM EST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintVCFSpark --output test.vcf --input test.vcf --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false; [February 14, 2016 7:04:16 PM EST] Executing as pgrosu on Linux 2.6.32-358.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_05-b13; Version: Version:4.alpha-86-g154d0a8-SNAPSHOT JdkD",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857:61,access,access,61,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857,1,['access'],['access']
Security,"Hi Adam,. Thank you for this amazing work, and would like to suggest something small without disturbing the test-flow. So unless I'm not mistaken, the class that is consuming the memory is [NestedIntegerArray.java](https://github.com/broadgsa/gatk/blob/master/public/gatk-utils/src/main/java/org/broadinstitute/gatk/utils/collections/NestedIntegerArray.java). Why not try an experiment where you implement something similar to [Google's SSTable](https://www.igvita.com/2012/02/06/sstable-and-log-structured-storage-leveldb/) that got release as [leveldb](https://github.com/google/leveldb). For instance the key-index lookup for you can be hash or a hash+offset. Basically these structures borrow from the concept of a [Log-Structured Merge-Tree (LSM-Tree) - link is a PDF paper](http://paperhub.s3.amazonaws.com/18e91eb4db2114a06ea614f0384f2784.pdf). You can push most of these to disk instead of memory, and they can be compressed as well. The indices can remain in memory for fast-lookup with the data staying on disk. So for instance, you can generate an key-index digest of specific values via `hash_function(numReadGroups.id, qualDimension.index, eventDimension.type)`, whose result you then use to quickly lookup the value for that key - which would reside on disk. That can be further improved with a priority queue cache for the most accessed values. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1460#issuecomment-180626838:640,hash,hash,640,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1460#issuecomment-180626838,3,"['access', 'hash']","['accessed', 'hash']"
Security,"Hi James, that definitely shouldn't be happening. Any chance I could get access to that cram file or a 10 kB chunk around chr1:1914706 for debugging?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6473#issuecomment-593049062:73,access,access,73,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6473#issuecomment-593049062,1,['access'],['access']
Security,"Hi Jose,; Your system ulimit setting is too low. Please do this with root at; /etc/security/limits.conf; * soft memlock unlimited; * hard memlock unlimited; * hard nofile 20480; * soft nofile 20480; * hard nproc 40960; * soft nproc 40960; * soft stack unlimited; * hard stack unlimited. Ruzhu; -------------------------------------------; Ruzhu Chen, PhD (845) 433-8426(T/L 293-8426); Email: ruzhuchen@us.ibm.com, Mobile: (845) 337-7238; Sr. Technical Solution Architect, HPC / Genomics & Life Sciences; IBM Systems, 2455 South Road, Poughkeepsie, NY 12601. From:	Jose Sergio Hleap <notifications@github.com>; To:	broadinstitute/gatk <gatk@noreply.github.com>; Cc:	ruzhuchen <ruzhuchen@us.ibm.com>, Mention; <mention@noreply.github.com>; Date:	03/12/2020 11:37 AM; Subject:	[EXTERNAL] Re: [broadinstitute/gatk] Got ""Too many open files""; when use BaseRecalibratorSpark (#5316). Apologies on the poor report. There are no other users in these compute; nodes (I am the tester) and for all intents and purposes the ulimit is; pretty high (hard limit of 8192 max files). I am using GATK version; 4.1.4.1, although it might be the one that has been optimised for IBM; power9 systems by @ruzhuchen. Currently I am waiting for the sys admin to; increase the max files further, but I believe that this is far from ideal.; Here is the (simplified) command:. gatk --java-options ""-Xmx40g; -Djava.library.path=/bio/apps/gatk_4.1.4/gatk-4.1.4.1/libs; -DGATK_STACKTRACE_ON_USER_EXCEPTION=true"" Mutect2 -R; Homo_sapiens_assembly38.fa -I illuminaN_hg38.br.recal.bam; --max-mnp-distance 0 -O illuminaN.vcf.gz. May be I am running it wrong?. —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub, or unsubscribe.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5316#issuecomment-598269062:83,secur,security,83,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5316#issuecomment-598269062,1,['secur'],['security']
Security,"Hi Paul, that means you don't have access to our internal repositories. Let me see if I can get you access.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/269#issuecomment-183014915:35,access,access,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/269#issuecomment-183014915,2,['access'],['access']
Security,"Hi guys,. In case it helps, here's an upload site that could be used for these GATK3 test data, either uploading through the browser:. https://prime-seq.ohsu.edu/project/Internal/Bimber/GATK/begin.view?. or command line (-T for file name(s)), note the user/password:. curl --basic -T foo.txt -u ""gatk_upload@gatk.com:genomes"" https://prime-seq.ohsu.edu/_webdav/Internal/Bimber/GATK/@files/. I'll delete that temporary user and dropbox once finished, so i'm not that worried about posting credential in this public forum. I'm also happy to download from somewhere if that's an option. The post above summarizes the files we're hoping to get. Thanks again for the help,; Ben",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/616#issuecomment-360578779:257,password,password,257,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/616#issuecomment-360578779,1,['password'],['password']
Security,"Hi, ; for those looking to run containers within a multi-user HPC environment, running a container with default root privileges presents a potential data security risk. Adding something like :. RUN useradd -ms /bin/bash gatk; WORKDIR /home/gatk; USER gatk. to the Docker file would greatly reduce the risk and bring the current containers in line with general best practice, e.g https://medium.com/@mccode/processes-in-containers-should-not-run-as-root-2feae3f0df3b. There should be no downsides to running in this manner. Singularity could help but the current configuration will be picked up and prevented from running by any site using a container security scanner, e.g. Aqua.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3644#issuecomment-494457377:154,secur,security,154,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3644#issuecomment-494457377,2,['secur'],['security']
Security,"Hi, since there is DOS (Denial of Service) threat for log4j 2.16.0(https://logging.apache.org/log4j/2.x/security.html),; is that possible to update GATK with log4j_2.17.0? Thanks.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7603#issuecomment-998102555:43,threat,threat,43,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7603#issuecomment-998102555,2,"['secur', 'threat']","['security', 'threat']"
Security,"Hi,. @gbrandt6 can you let me know how to access the bug report files that the user pushed to [ftp](https://gatk.broadinstitute.org/hc/en-us/community/posts/360072797951/comments/360012763332)? Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6793#issuecomment-686806185:42,access,access,42,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6793#issuecomment-686806185,1,['access'],['access']
Security,"Hi,. Not sure if it is applicable but I just ran vcf-validator on the input gvcf file (also not sure if you can run vcf-validator on a gvcf): . ```; vcf-validator Sample_1__CDL-164-04P-gatk-haplotype-annotated-rnaedit-annotated-gemini.vcf.gz; ```. Here are the first few lines from the logs:. ```; The header tag 'reference' not present. (Not required but highly recommended.); INFO field at 1:14599 .. INFO tag [max_aaf_all=0.2096] expected different number of values (expected 2, found 1); INFO field at 1:14604 .. INFO tag [max_aaf_all=0.2096] expected different number of values (expected 2, found 1); INFO field at 1:14930 .. INFO tag [max_aaf_all=0.5231] expected different number of values (expected 2, found 1); INFO field at 1:15211 .. INFO tag [max_aaf_all=0.7316] expected different number of values (expected 2, found 1); INFO field at 1:15274 .. INFO tag [max_aaf_all=0.7205] expected different number of values (expected 3, found 1); INFO field at 1:16949 .. INFO tag [max_aaf_all=0.0227] expected different number of values (expected 2, found 1); INFO field at 1:17365 .. INFO tag [max_aaf_all=0.3841] expected different number of values (expected 2, found 1),INFO tag [af_adj_exac_afr=; 0.1603] expected different number of values (expected 2, found 1),INFO tag [af_exac_all=0.2553] expected different number of values (expect; ed 2, found 1),INFO tag [af_adj_exac_nfe=0.2715] expected different number of values (expected 2, found 1),INFO tag [af_adj_exac_sas=0.2883; ] expected different number of values (expected 2, found 1),INFO tag [af_adj_exac_oth=0.2581] expected different number of values (expected; 2, found 1),INFO tag [af_adj_exac_eas=0.3841] expected different number of values (expected 2, found 1),INFO tag [af_adj_exac_amr=0.221] e; xpected different number of values (expected 2, found 1),INFO tag [af_adj_exac_fin=0.2245] expected different number of values (expected 2,; found 1); INFO field at 1:17373 .. INFO tag [af_adj_exac_amr=0.028] expected different number ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5045#issuecomment-407476343:53,validat,validator,53,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5045#issuecomment-407476343,3,['validat'],['validator']
Security,"Hi,; I update GATK today.; After 158 minutes variant calling on the same bam files, I have another issue :. ```; [3 décembre 2019 13:57:42 CET] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 158.34 minutes.; Runtime.totalMemory()=28647096320; Exception in thread ""main"" java.lang.OutOfMemoryError: GC overhead limit exceeded; 	at java.util.LinkedHashMap$LinkedKeySet.iterator(LinkedHashMap.java:543); 	at java.util.HashSet.iterator(HashSet.java:173); 	at java.util.AbstractCollection.toArray(AbstractCollection.java:137); 	at java.util.LinkedList.addAll(LinkedList.java:408); 	at java.util.LinkedList.addAll(LinkedList.java:387); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.graphs.BaseGraph$BaseGraphIterator.next(BaseGraph.java:774); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.graphs.BaseGraph$BaseGraphIterator.next(BaseGraph.java:723); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.graphs.BaseGraph.removePathsNotConnectedToRef(BaseGraph.java:505); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.getAssemblyResult(ReadThreadingAssembler.java:514); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.createGraph(ReadThreadingAssembler.java:492); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.assemble(ReadThreadingAssembler.java:401); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.runLocalAssembly(ReadThreadingAssembler.java:148); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.assembleReads(AssemblyBasedCallerUtils.java:290); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2Engine.callRegion(Mutect2Engine.java:224); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2.apply(Mutect2.java:320); 	at org.broadinstitute.hellben",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6271#issuecomment-561188674:447,Hash,HashSet,447,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6271#issuecomment-561188674,2,['Hash'],['HashSet']
Security,Hm - I don't think we can take that last change. Theres not much use in validating args after they've been used by the constructors. Let me see if there is an alternative.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-827854652:72,validat,validating,72,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-827854652,1,['validat'],['validating']
Security,"Hmm, I don't have access to dsde-docs. I thought by default haplotypeCaller doesn't use supplementary reads in GATK3? If I'm wrong about that assumption, then I don't see any reason why it would be different in GATK4.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2043#issuecomment-235071588:18,access,access,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2043#issuecomment-235071588,1,['access'],['access']
Security,"Hmm, I started taking a stab at the LL score implementation, but I think it's going to complicate the code quite a bit and add some branching options to the tool interfaces. Compounding this with a change in the use of ""truth"" and ""validation"" terminology, I fear that the resulting differences from the legacy strategy might be a bit much for users to digest!. So I'd want to better understand the cost/benefit before we proceed. How critical is automatic tuning of the hard threshold? And what's the relative importance to method changes that increase AUC (i.e., as opposed to figuring out where on the curve to hard threshold)? Is there a clear path forward for evaluating such a tuning process? @meganshand would be glad to chat more!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1065524909:232,validat,validation,232,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1065524909,1,['validat'],['validation']
Security,"Hmm, as we discussed, I'm not sure this will be so straightforward, given that we only have easy access to scores for positive truth---and hence, no false positives, which precludes calculation of precision and F1. I *think* we could pass a VCF for a sample with gold-standard positives and negatives and use the existing code for extracting labels, but this will require a bit of engineering and be more trouble than it's worth. There are other options---see https://ir.cwi.nl/pub/30479, for example. We might want to experiment with the LL score discussed there (see https://www.aaai.org/Papers/ICML/2003/ICML03-060.pdf for the original paper---although note that despite the paper's high citation count, I'm not sure what the canonical name for this metric actually is, but it doesn't appear to be ""LL score""---perhaps someone else knows or has better Google-fu and can figure it out) before moving on to their methods for estimating F1. Doing a literature search for other discussions of optimizing F1 or other metrics in the context of positive-unlabeled learning might be worthwhile, but I think most methods will probably involve some sort of estimation of the base rate in unlabeled data. I think we may have to add some mechanism for holding out a validation set during training if we want to automatically tune thresholds in a rigorous fashion. Shouldn't be too bad---we can just have the training tool randomly mask out a set of the truth and pass the mask to the scoring tool (or maybe just determine the threshold in the training tool, if we are running in positive/negative mode and have access to unlabeled data)---but does add a couple of parameters to the tool interfaces. This also adds additional dependence on the quality of the truth resources. I think an implicit assumption in any use of the truth---even just thresholding/calibrating by sensitivity---is that it is a random sample; however, I'm not sure how true this is in actual use. For example, in malaria, it looks like we",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1062931241:97,access,access,97,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1062931241,1,['access'],['access']
Security,"Hmmm, rather than making assumptions, the aligner could expose its SW parameters and then it comes down to a line of arithmetic.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5466#issuecomment-443324822:56,expose,expose,56,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5466#issuecomment-443324822,1,['expose'],['expose']
Security,Hmn. This is failing with 403 unauthorized errors. Seems like something about authentication changed.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6042#issuecomment-511979513:78,authenticat,authentication,78,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6042#issuecomment-511979513,1,['authenticat'],['authentication']
Security,"How about doing this everywhere except in Picard, instead of abandoning it completely? Non-linked hash sets/maps have caused MANY problems in GATK over the years.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1844#issuecomment-220622306:98,hash,hash,98,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1844#issuecomment-220622306,1,['hash'],['hash']
Security,How do I access that? I thought that the GATK resources were located here: https://console.cloud.google.com/storage/browser/genomics-public-data/resources/broad/hg38/v0/. Is there a reason this is not in the GATK resource bundle?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5821#issuecomment-481902753:9,access,access,9,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5821#issuecomment-481902753,1,['access'],['access']
Security,"How to reproduce:. Modify the code in ```AlignmentIntervalUnitTest.testConstructionFromSAMRecord``` to perform a validation of the read returned by ```applyAlignment```:. ```; final SAMRecord samRecord = BwaMemAlignmentUtils.applyAlignment(""whatever"", SVDiscoveryTestDataProvider.makeDummySequence(expectedContigLength, (byte)'A'), null, null, bwaMemAlignment, refNames, hg19Header, false, false);; if (samRecord.isValid() != null) {; throw new IllegalStateException(samRecord.isValid().stream().map(s -> s.getMessage()).collect(Collectors.joining("", "")));; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3459#issuecomment-323218384:113,validat,validation,113,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3459#issuecomment-323218384,1,['validat'],['validation']
Security,"Huh, I wonder why gradle doesn't like a lustre system. I didn't now that was an issue. Git-lfs is an annoyance to install if you don't have access to a package manager. I *think* it can be installed without sudo but I get why that's a pain. I've spent a non-zero amount of time fighting with git-lfs installation before. Thank you for elaborating!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7675#issuecomment-1042057009:140,access,access,140,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7675#issuecomment-1042057009,1,['access'],['access']
Security,"I added one unrelated bugfix. FuncotatorUtils.createReferenceSnippet tries to expand the reference window. When doing this, it should never allow a start less than 1. The last commit addresses that. . Note: I did not see an easy way for createReferenceSnippet() to identify the length of the contig (such as access to the SequenceDictionary), but it would in theory be useful to also check contig size and not exceed it. @droazen or @jonn-smith: it would be helpful if you could approve the test run. ```; 22 Jun 2023 14:54:27,152 DEBUG: 	java.lang.IllegalArgumentException: Invalid interval. Contig:MT start:0 end:20; 22 Jun 2023 14:54:27,154 DEBUG: 		at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:804); 22 Jun 2023 14:54:27,155 DEBUG: 		at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:59); 22 Jun 2023 14:54:27,156 DEBUG: 		at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:35); 22 Jun 2023 14:54:27,158 DEBUG: 		at org.broadinstitute.hellbender.tools.funcotator.FuncotatorUtils.createReferenceSnippet(FuncotatorUtils.java:1461); 22 Jun 2023 14:54:27,159 DEBUG: 		at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createIgrFuncotation(GencodeFuncotationFactory.java:2481); 22 Jun 2023 14:54:27,160 DEBUG: 		at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createIgrFuncotations(GencodeFuncotationFactory.java:2407); 22 Jun 2023 14:54:27,162 DEBUG: 		at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createDefaultFuncotationsOnVariant(GencodeFuncotationFactory.java:499); 22 Jun 2023 14:54:27,163 DEBUG: 		at org.broadinstitute.hellbender.tools.funcotator.DataSourceFuncotationFactory.createFuncotations(DataSourceFuncotationFactory.java:217); 22 Jun 2023 14:54:27,164 DEBUG: 		at org.broadinstitute.hellbender.tools.funcotator.DataSourceFuncotationFactory.create",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8363#issuecomment-1603412226:308,access,access,308,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8363#issuecomment-1603412226,3,"['access', 'validat']","['access', 'validateArg', 'validatePositions']"
Security,"I added you as collaborator, you should have access now.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/269#issuecomment-183027964:45,access,access,45,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/269#issuecomment-183027964,1,['access'],['access']
Security,"I addressed partially your comments (and fixed a compilation error due to the tests using the previous arguments). One of the major points of discussion are the following:. * `Collection` instead of `List`: I think that the first is more flexible, because a client maybe wants to have a `LinkedHashSet` as the argument to avoid repetition of the same filter. I agree that the abstract class should discourage not honoring the user order.; * Access to methods/fields: I think that the plugin could be used outside GATK in a different way by extending it. I explained some of my usage cases in one of the comments in the code, but just by overriding a simple method the whole plugin could be used very nicely in some of them. I would prefer to do that than copy your code and re-implement the bits that I would like to change. Back to you for your ideas on this, @cmnbroad!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2355#issuecomment-275359208:441,Access,Access,441,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2355#issuecomment-275359208,1,['Access'],['Access']
Security,I agree that it is a good security measure to use fixed signed dependencies for repeatable builds. GATK depends on gradle 3.1.: [download](https://services.gradle.org/distributions/gradle-3.1-bin.zip) [shaw256](https://services.gradle.org/distributions/gradle-3.1-bin.zip.sha256),MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5483#issuecomment-444208372:26,secur,security,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5483#issuecomment-444208372,1,['secur'],['security']
Security,"I agree with everything @magicDGS said. In general, the native GATK tool benefits from all of the automatic framework management and I/O support (like honoring md5, lenient args, etc.). #2234 also has the validation checking mentioned, and has more tests. (It is true that when #2223 goes in, we'll have to override the default sequence dictionary validation behavior, maybe via disableSequenceDictionaryValidation). So I think we should take the native one, but if we do choose to keep this one for any reason, then I'll still want to do a line-by-line CR before we merge.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2232#issuecomment-257306629:205,validat,validation,205,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2232#issuecomment-257306629,2,['validat'],['validation']
Security,I also can't access those files. Who can grant access to them?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2658#issuecomment-299493615:13,access,access,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2658#issuecomment-299493615,2,['access'],['access']
Security,"I am aware that those methods should be definetively implemented in the abstract class - but it could be recommended *NOT TO OVERRIDE* unless you know what you are doing. I know the problems of having them exposed, but also I can see some potential to be accessible from an implemented walker when the user knows what is doing...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4964#issuecomment-404134765:206,expose,exposed,206,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4964#issuecomment-404134765,2,"['access', 'expose']","['accessible', 'exposed']"
Security,"I am going to be able to identify samples for a new panel of normals this week, after which generating and validating the panel will take another few days.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1289162161:107,validat,validating,107,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1289162161,1,['validat'],['validating']
Security,"I am having a similar issue with GATK `4.1.4.1` that persists after updating to `4.2.0.0`:; ```; 00:33:06.768 INFO BaseRecalibrationEngine - The covariates being used here:; 00:33:06.768 INFO BaseRecalibrationEngine - ReadGroupCovariate; 00:33:06.768 INFO BaseRecalibrationEngine - QualityScoreCovariate; 00:33:06.768 INFO BaseRecalibrationEngine - ContextCovariate; 00:33:06.768 INFO BaseRecalibrationEngine - CycleCovariate; 21/03/28 00:33:07 INFO BlockManagerInfo: Removed broadcast_0_piece0 on fend04.cluster:42128 in memory (size: 35.5 KB, free: 5.2 GB); 21/03/28 00:33:14 ERROR Executor: Exception in task 1.0 in stage 0.0 (TID 1); java.lang.IllegalArgumentException: Table1 1,3 not equal to 2,3; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:798); at org.broadinstitute.hellbender.utils.recalibration.RecalUtils.combineTables(RecalUtils.java:560); at org.broadinstitute.hellbender.utils.recalibration.RecalibrationTables.combine(RecalibrationTables.java:144); at org.broadinstitute.hellbender.utils.recalibration.RecalibrationTables.inPlaceCombine(RecalibrationTables.java:178); at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction2$1.apply(JavaPairRDD.scala:1037); at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157); at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157); at scala.collection.Iterator$class.foreach(Iterator.scala:891); at scala.collection.AbstractIterator.foreach(Iterator.scala:1334); at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1334); at scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214); at scala.collection.AbstractIterator.aggregate(Iterator.scala:1334); at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$26.apply(RDD.scala:1190); at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$26.apply(RDD.scala:1190); at org.apa",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5854#issuecomment-808817724:748,validat,validateArg,748,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5854#issuecomment-808817724,1,['validat'],['validateArg']
Security,"I am still receiving security warnings about GATK 4.4.0.0:. Detected by File Paths: gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar; Detected by Library: pkg:java/log4j:log4j; CPE: cpe:/a:apache:log4j:1.2.17; Version End of Life Date: August 4th, 2015 at 7:00 PM",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8215#issuecomment-1513816621:21,secur,security,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8215#issuecomment-1513816621,1,['secur'],['security']
Security,"I appreciate the desire for minimal changes, but I would point out that tying the VariantContext to the source FeatureInput is likely to be a somewhat common need for MultiVariantWalkers. I realize you have a lot of existing example that dont need this capability. While I started this using VariantEval/VariantQC, I more recently tried to port CombineVariants to GATK4 and hit a similar roadblock. I have one or two other lab-specific walkers that would benefit from using the iteration pattern of MultiVariantWalkerGroupedOnStart, but also need some ability to retain the FeatureInput->VariantContext link. I believe GATK3 retained this. It would be nice to at least make this a capability available to all MultiVariantWalkerGroupedOnStart walkers. My suggestion above about making a FeatureInputAwareVariantContext wasnt necessarily meant to be introduced into every class. Would something conceptually like this pass GATK if I could introduce it more surgically, perhaps injected into MultiVariantDataSource alone?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-823594870:975,inject,injected,975,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-823594870,1,['inject'],['injected']
Security,"I can access the artefactory web site. I tried again, and the build worked! Must have been a transient issue. Thanks for checking!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2579#issuecomment-292596069:6,access,access,6,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2579#issuecomment-292596069,1,['access'],['access']
Security,"I can confirm it was due to ""gs://gcp-public-data--gnomad"" not giving the correct authorization.. I had to copy the file in my own workspace. . It seems pretty problematic as it is the recommended file to run the workflow with...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7492#issuecomment-934925641:82,authoriz,authorization,82,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7492#issuecomment-934925641,1,['authoriz'],['authorization']
Security,"I can expose validation stringency, but there is a TODO in this file that says ""allow SamReader settings to be customized by client"". So my question is similar to the question in issue 419: should I allow the caller to optionally provide their own SamReaderFactory so they can customize all of the options, or do we really want to just limit it to validation stringency ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/181#issuecomment-111532924:6,expose,expose,6,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/181#issuecomment-111532924,3,"['expose', 'validat']","['expose', 'validation']"
Security,"I checked only the tools that are marked `no` in the `correct category in gatk --list` in the 0107Check_category_&_doc tab of https://docs.google.com/spreadsheets/d/19SvP6DHyXewm8Cd47WsM3NUku_czP2rkh4L_6fd-Nac/edit?usp=sharing. ## The following eight tools need fixing:. tool | category philosophically ok? | 2nd check, correct category in gatkDoc? | categorization fixed by https://github.com/broadinstitute/gatk/pull/4094?; -- | -- | -- | --; IndexFeatureFile | NO | y | no; still in Variant Manipulation but should be in Other.; CreateHadoopBamSplittingIndex | NO | y | no; still in Other but no longer with ConvertHeaderlessHadoopBamShardToBam; VariantAnnotator | y | DOES NOT SHOW UP | no; does not show up; FixCallsetSampleOrdering | MISSING | no | no; does not show up; DepthOfCoverage | y | DOES NOT SHOW UP | no; DiagnoseTargets | y | DOES NOT SHOW UP | no; GatherTranches | y | y | DOES NOT SHOW UP IN GATKDOC; shows up in correct category in gatk --list; ValidateBasicSomaticShortMutations | MISSING | no | DOES NOT SHOW UP IN GATKDOC; shows up in correct category in gatk --list. ## The following tools appear fixed by this PR:. tool | category philosophically ok? | 2nd check, correct category in gatkDoc? | categorization fixed by https://github.com/broadinstitute/gatk/pull/4094?; -- | -- | -- | --; CollectBaseDistributionByCycleSpark | y | y | y; ASEReadCounter | y | y | y; CountBases | y | y | y; CountBasesSpark | y | y | y; CountReads | y | y | y; CountReadsSpark | y | y | y; PileupSpark | y | y | y; CollectInsertSizeMetricsSpark | y | y | y; CollectMultipleMetricsSpark | y | y | y; CollectQualityYieldMetricsSpark | y | y | y; CompareBaseQualities | y | y | y; EstimateLibraryComplexityGATK | y | y | y; FilterLongReadAlignmentsSAMSpark | y | y | y; FlagStat | y | y | y; FlagStatSpark | y | y | y; MeanQualityByCycleSpark | y | y | y; QualityScoreDistributionSpark | y | y | y; GatherBQSRReports | y | y | y; ApplyBQSR | y | y | y; ApplyBQSRSpark | y | y | y; BaseRecalibrato",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4094#issuecomment-356110640:966,Validat,ValidateBasicSomaticShortMutations,966,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4094#issuecomment-356110640,1,['Validat'],['ValidateBasicSomaticShortMutations']
Security,"I cleaned up the class a bit - removed all specialized filtering methods in favor of generic predicate-based access. If we find repeated uses in client code, we can move those here. . Regarding tests, can you list the conditions that you think are inadequately tested?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/661#issuecomment-128042589:109,access,access,109,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/661#issuecomment-128042589,1,['access'],['access']
Security,I cleaned up the mutect2 wdl and added multi-sample support. I also optimized resource usage and exposed the memory parameters: https://github.com/phylyc/gatk4-somatic-snvs-indels,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7532#issuecomment-1125321665:97,expose,exposed,97,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7532#issuecomment-1125321665,1,['expose'],['exposed']
Security,"I created a branch in gatk-protected to address this issue. @lbergelson, @droazen -- could I get push access to the gatk-protected repo so I can push and submit a PR? Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2717#issuecomment-302442385:102,access,access,102,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2717#issuecomment-302442385,1,['access'],['access']
Security,"I did this yesterday but didn't look into all the output. ```; find src/test/resources -name ""*.bam"" > bams. for x in $(cat bams):; do build/install/hellbender/bin/hellbender ValidateSamFile -I $x >> validateBamOutput 2>&1; done; ```. output is here:; https://gist.github.com/lbergelson/002289ce94b0eace183c",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/569#issuecomment-112827447:175,Validat,ValidateSamFile,175,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/569#issuecomment-112827447,2,"['Validat', 'validat']","['ValidateSamFile', 'validateBamOutput']"
Security,"I did, but then I rethought it and decided that I think it's too fragile to do it that way: You have to guarantee that all classes that might want to be registered are loaded and initialized before you instantiate the SparkConf. The problem is that it varies among JVM implementations exactly when that (class loading and initialization) happens. Some JVMs do the whole mess, chasing all references down from main recursively at the beginning, others are as lazy as possible and initialize only when actually traversing a reference for the first time. We could use that technique to ""inject"" a set of registrations into the GATKRegistrator from each main class. But since it's unreliable to do it in arbitrary classes, it seemed more straightforward to just let the normal object oriented method of overrides handle the problem, since it will need to be done only by direct subclasses, anyway. But I'm certainly open to other solutions, so long as we provide the functionality.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1767#issuecomment-214473964:584,inject,inject,584,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1767#issuecomment-214473964,1,['inject'],['inject']
Security,"I didn't realize before that there is no ""include"" (opt-in) validation type arg, only ""exclude"". So I'm not sure what the purpose of having ""ALL"" is in the first place, if the only thing you can usefully do with it is exclude it. I think the best longer term fix would be to add an ""--validation-type-to-include"" arg, and have it default to the everything except for IDs, and then construct the actual types based on merging include/exclude args. But thats a bigger change then just fixing the current (silent do-nothing) default behavior, and requires more error checking for conflicting args. Lets start with changing it so that in the default (no args) case, we log a warning message saying that IDs will be left out since no IDS were provided, and proceed with the remaining validations. Then if we want to get more ambitious we can talk about making the bigger change. Also, as part of the initial fix, it might be a good idea to change `calculateValidationTypesToApply` so that it doesn't modify the `excludeTypes` list directly, since this is the list provided by the user, and instead uses it's own temporary list.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5862#issuecomment-498685279:60,validat,validation,60,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5862#issuecomment-498685279,3,['validat'],"['validation', 'validation-type-to-include', 'validations']"
Security,"I discovered that it actually is possible to load unmapped reads with the existing google code code. I had thought it didn't include the ability to do so, but after further digging, there's a way to do it that we just haven't exposed.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/560#issuecomment-114228839:226,expose,exposed,226,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/560#issuecomment-114228839,1,['expose'],['exposed']
Security,"I discovered that one of the 345 input gvcfs failed VCF validation. When I removed that file and reran with no other changes, I did not get the ""terminate called without an active exception"" error. However, ImportGvcfs still fails; the failure seems to occur immediately after GenomicsDBImport logs success in importing all batches, in each shard. From all the Cromwell logs it looks like everything is working, but the top level workflow execution fails. I've been trying various configurations of memory, scatter count, and #nodes, so I don't have those log files around still. I can rerun with -DGATK_STACKTRACE_ON_USER_EXCEPTION=true and see if I get anything useful.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8076#issuecomment-1295310651:56,validat,validation,56,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8076#issuecomment-1295310651,1,['validat'],['validation']
Security,I don't know if it's the same issue but we have recently started seeing random 403 errors running dataproc jobs that appear to be internal to GCS dataproc services:. ```; ERROR: (gcloud.dataproc.jobs.submit.spark) HTTPError 403: cwhelan@broadinstitute.org does not have storage.objects.get access to dataproc-ed605f51-8ceb-44f7-b48c-a87bc588c1a6-us/google-cloud-dataproc-metainfo/dbfb2df5-060a-425e-ac31-77484354f264/jobs/0a5c53e9-f935-48f8-a39e-8a46d20b5ec9/driveroutput.000000010.; ```. For us the job keeps running on the dataproc cluster but the error crashes the client program that submitted the job.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3735#issuecomment-339034362:290,access,access,290,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3735#issuecomment-339034362,1,['access'],['access']
Security,I don't think so -- using the API key introduces all sorts of security issues with sanitizing command lines. I think we just want to rely on the default Google credentials.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2402#issuecomment-288549958:62,secur,security,62,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2402#issuecomment-288549958,2,"['sanitiz', 'secur']","['sanitizing', 'security']"
Security,"I don't think that hiding/disable arguments would work in every case: sometimes, an argument shouldn't be exposed but still available to set programmatically, or maybe just reduce visibility making it `@Hidden` and/or `@Advance`. What is the problem of making an interface for the top-level argument to the GATK? Changing the interface or the `CommadnLineProgram` has the same effect, but the API user can still behave the same as before. It is much more extensible and downstream-friendly. What's about making the `CLPConfigurationArgumentCollection` an interface always returning defaults to be able to change it in a proper way? The cycle of development of a new argument will be: 1) add a new method to the interface with a default returning what will be expected from the previous behaviour, 2) add and return by the argument in the GATK implementation, 3) use the getter in the CLP for perform the operation. This only adds the first point, and operating in 3 classes instead of 3. For API user it is really easy to maintain the previous behavior when upgrading the dependency by just using their own implementation of the class, or include the top-level new arguments by using the GATK implementation. It is much more flexible and extensible (I always think about GATK also as a library). In addition, I think that this approach is also important for evolving GATK. For example, if a new top-level argument is tagged as experimental (still not supported but requested in Barclay), removing it would allow to keep the interface (no version bump) the same and final users can still operate with the experimental argument. The same applies to the `GATKTool` base class (https://github.com/broadinstitute/gatk/issues/4341), and for downstream projects the aim should be to be able to extend safely the `CommandLineProgram` directly to implement their own toolkit using the powerful GATK framework.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3998#issuecomment-366185003:106,expose,exposed,106,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3998#issuecomment-366185003,1,['expose'],['exposed']
Security,"I don't think that's the elusive bug we're looking for, but a bit more argument validation certainly cannot hurt. The gradle files look like they're auto-generated, I certainly didn't change them myself. Let me know if you think we should delete them instead.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2643#issuecomment-298450921:80,validat,validation,80,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2643#issuecomment-298450921,1,['validat'],['validation']
Security,"I don't think we've made any guarantees about the thread safety of Funcotator or the associated datasource classes. . Also, this account seems to be a bot and I can't access its listed home page…. I can audit the class at some point.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7376#issuecomment-891860172:167,access,access,167,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7376#issuecomment-891860172,2,"['access', 'audit']","['access', 'audit']"
Security,"I get tarballs from github, and then download dependencies and generate the intermediate tarball internally. We never clone git repositories as you might think due to security issues.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6395#issuecomment-584408522:167,secur,security,167,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6395#issuecomment-584408522,1,['secur'],['security']
Security,I got another report of something similar in the non-spark HaplotypeCaller; stack trace below. ````; java.lang.IllegalStateException: Duplicate key [B@42515a2f; at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133); at java.util.HashMap.merge(HashMap.java:1253); at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.buildGapContinuationPenalties(PairHMMLikelihoodCalculationEngine.java:304); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngine.java:253); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngine.java:187); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:520); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller.apply(HaplotypeCaller.java:239); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:244); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:217); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:779); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(Com,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3018#issuecomment-310805959:251,Hash,HashMap,251,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018#issuecomment-310805959,2,['Hash'],['HashMap']
Security,"I got it to work by using the runtime switch --disable-sequence-dictionary-validation . . If that is not used it crashes. . . Docker commandline. . /gatk Funcotator --disable-sequence-dictionary-validation \. -R mydata/refs/Homo_sapiens_assembly19.fasta \. -V mydata/P50513_mutect2_filtered.vcf \. -O mydata/P50513_mutect2_funcotator.maf \. --output-file-format MAF \. --data-sources-path mydata/dataSourcesFolder/funcotator_dataSources.v1.6.20190124s/ --ref-version hg19. . . . From: Louis Bergelson <notifications@github.com> ; Sent: Wednesday, October 30, 2019 10:26 AM; To: broadinstitute/gatk <gatk@noreply.github.com>; Cc: rdbremel <rdbremel017@gmail.com>; Mention <mention@noreply.github.com>; Subject: Re: [broadinstitute/gatk] Funcotator shuts down (#6182). . @rdbremel <https://github.com/rdbremel> This got missed in the churn of issues. Does this happen repeatedly or is it a 1 time occurrence? We've seen similar issues in the past and tried to wrap them all in layers of retries, but sometimes things slip through. —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub <https://github.com/broadinstitute/gatk/issues/6182?email_source=notifications&email_token=ANCR2VB4ZCHMAJUHBKE2SP3QRGRQFA5CNFSM4I2MRFQKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOECUTZZI#issuecomment-547962085> , or unsubscribe <https://github.com/notifications/unsubscribe-auth/ANCR2VHRV5JESZYAYX55YHTQRGRQFANCNFSM4I2MRFQA> . <https://github.com/notifications/beacon/ANCR2VAS2WE5TDCUC6G5LETQRGRQFA5CNFSM4I2MRFQKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOECUTZZI.gif>",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6182#issuecomment-548102382:75,validat,validation,75,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6182#issuecomment-548102382,2,['validat'],['validation']
Security,"I guess we need to do some work on the doclet code, abstract out example code and use templates to transform it into the appropriate format/syntax depending what project is generating the documentation. Alternatively and only if documentation html is well formed (xhtml like) then in theory we could use XSLT transformation style sheets to convert embedded code example encoded with xml/xhtml into the concrete syntax. Most major browsers support XSLT. EDIT: The XSLT solution won't probably work since even if we try to change the output from html to xhtml, the fact that we are injecting the javadoc's html would probably break the xhtml.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-349724551:580,inject,injecting,580,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-349724551,1,['inject'],['injecting']
Security,"I have a run going now. On Jan 4, 2018 15:32, ""samuelklee"" <notifications@github.com> wrote:. > Placeholders for now. We can tweak the actual values once @LeeTL1220; > <https://github.com/leetl1220> checks effect on validation.; >; > Closes #4032 <https://github.com/broadinstitute/gatk/issues/4032>.; > ------------------------------; > You can view, comment on, or merge this pull request online at:; >; > https://github.com/broadinstitute/gatk/pull/4046; > Commit Summary; >; > - Changed default values for ModelSegments segmentation parameters.; >; > File Changes; >; > - *M* src/main/java/org/broadinstitute/hellbender/tools/copynumber/; > ModelSegments.java; > <https://github.com/broadinstitute/gatk/pull/4046/files#diff-0> (6); >; > Patch Links:; >; > - https://github.com/broadinstitute/gatk/pull/4046.patch; > - https://github.com/broadinstitute/gatk/pull/4046.diff; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/pull/4046>, or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk8coObtbYN125S1_BMBx1VnnmbF4ks5tHTVzgaJpZM4RTh_B>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4046#issuecomment-355405908:216,validat,validation,216,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4046#issuecomment-355405908,1,['validat'],['validation']
Security,"I have been able to get the connector working on GCP VMs where I have manually authenticated locally with my own account. I have not successfully gotten it working on a cromwell VM or ortherwise using manually supplied keyfiles. Anecdotal evidence, but its worth mentioning that both: `fs.gs.impl`; `fs.AbstractFileSystem.gs.impl`; seem to be optional for getting a run to work. It seems to have defaulted to the right things in the trials I've tested (though thats not to say the default will always work). I have put in a question on the issue tracker asking about available authentication inside a pipelines API VM.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5996#issuecomment-500846568:79,authenticat,authenticated,79,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5996#issuecomment-500846568,2,['authenticat'],"['authenticated', 'authentication']"
Security,"I have just experienced the same problem of stack overflow with my first attempt to use gatk HaplotypeCallerSpark with the option --spark-master local[*]. My GATK version is 4.1.2.0,that was installed via bioconda. ; Should I wait for the corrected version or is there a way to circumvent the problem with extra install or by using options like --java-options '-XssOptimalValue'?; When is the corrected version expected? Is Q2 (end of June?) still an option? Will it be readily on bioconda then?; Thanks",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5869#issuecomment-499414227:362,Xss,XssOptimalValue,362,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5869#issuecomment-499414227,1,['Xss'],['XssOptimalValue']
Security,"I have tested this with a fresh `gcloud` client and have not been able to reproduce the error. I did find an article from someone else who got the `400: invalid_grant` error: https://blog.timekit.io/google-oauth-invalid-grant-nightmare-and-how-to-fix-it-9f4efaf1da35. The long and short of it is that it's an authentication issue. Can you verify that the authentication you're using on the terminal is valid? That is, can you get at other public resources on gcloud?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6926#issuecomment-725097056:309,authenticat,authentication,309,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6926#issuecomment-725097056,2,['authenticat'],['authentication']
Security,"I haven't been able to reproduce @vdauwera error, but there are issues with the https checkout at the moment. ; There's one annoying issue witwhere it will prompt for a password before every individual file download. This will be fixed in https://github.com/github/git-lfs/issues/755. It can be worked around by using `git config credential.helper cache` but the easiest thing to do at the moment is just using the ssh checkout. . I need to investigate what happens with ssh checkout if you don't have a key set up.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/952#issuecomment-150376513:169,password,password,169,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/952#issuecomment-150376513,1,['password'],['password']
Security,"I implemented @lbergelson's suggestion with a new method: `validateArg(final boolean condition, final Supplier<String> msg)` invoked as eg `Utils.validateArg(n > 0, () -> String.format(""You have chosen the worst value ever, name %d,"", n))`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1979#issuecomment-232122503:59,validat,validateArg,59,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1979#issuecomment-232122503,2,['validat'],['validateArg']
Security,I just added you so you should have access now.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2043#issuecomment-235077255:36,access,access,36,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2043#issuecomment-235077255,1,['access'],['access']
Security,"I ran across a weird case the other day that I wanted to document here.; ` final Set<VCFHeaderLine> headerInfo = new HashSet<>();; headerInfo.addAll(getHeaderForVariants().getMetaDataInInputOrder());; headerInfo.add(GATKVCFHeaderLines.getFilterLine(GATKVCFConstants.LOW_HET_FILTER_NAME));; vcfWriter = createVCFWriter(new File(outputVcf));; vcfWriter.writeHeader(new VCFHeader(headerInfo));; `; Setting up the header this way end up with no genotypes in the output vcf. This is because the sample line is not included in the headerInfo, it is maintained as a separate field in the VCFHeader object. To resolve this issue the last line needed to be:; ` vcfWriter.writeHeader(new VCFHeader(headerInfo, getHeaderForVariants().getSampleNamesInOrder()));; `",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6048#issuecomment-582452360:117,Hash,HashSet,117,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6048#issuecomment-582452360,1,['Hash'],['HashSet']
Security,I recommend we error out when provided with `--validation-type-to-exclude ALL`. It doesn't make sense - why call the validator if you're not going to validate?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5862#issuecomment-485714312:47,validat,validation-type-to-exclude,47,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5862#issuecomment-485714312,3,['validat'],"['validate', 'validation-type-to-exclude', 'validator']"
Security,"I reproduced various out of memory errors in a Linux VM with 4G of RAM, both with the `IntelInflaterDeflaterIntegrationTest` enabled and disabled. Most resulted in the kernel killing the Java process, like this one (from `dmesg`):; ```; [38425.759992] Out of memory: Kill process 10295 (java) score 747 or sacrifice child; [38425.759998] Killed process 10295 (java) total-vm:7885212kB, anon-rss:3250892kB, file-rss:0kB; ```. Some were caught by the JVM, like this one:; ```; #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 90177536 bytes for committing reserved memory.; # Possible reasons:; # The system is out of physical RAM or swap space; # In 32 bit mode, the process size limit was hit; # Possible solutions:; # Reduce memory load on the system; # Increase physical memory or swap space; # Check if swap backing store is full; # Use 64 bit Java on a 64 bit OS; # Decrease Java heap size (-Xmx/-Xms); # Decrease number of Java threads; # Decrease Java thread stack sizes (-Xss); # Set larger code cache with -XX:ReservedCodeCacheSize=; # This output file may be truncated or incomplete.; #; # Out of Memory Error (os_linux.cpp:2627), pid=20484, tid=139679452493568; #; # JRE version: Java(TM) SE Runtime Environment (8.0_72-b15) (build 1.8.0_72-b15); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.72-b15 mixed mode linux-amd64 compressed oops); # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; ```. Here's my theory of what's happening. The `maxHeapSize` for test JVMs is set to 4G in `build.gradle`:; ```; maxHeapSize = ""4G""; ```. A 4G max heap size is too high for systems with 4G of RAM, because the Java heap grows until the system runs out of memory. If we decrease `maxHeapSize`, the GC should prevent the Java heap from growing too large, with the trade-off of more GC calls. I changed the `maxHeapSize` to `2G` a",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2490#issuecomment-288423316:1059,Xss,Xss,1059,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2490#issuecomment-288423316,1,['Xss'],['Xss']
Security,"I reran the workflow, this time allocating 200GB RAM to tmp in the slurm job, everything else exactly the same, and got the ""terminate called without an active exception"" failure again, so that error was not due to the gvcf that failed VCF validation as it was not included. This time, shard 9 succeeded in ImportGvcfs and also successfully completed GenotypeGVCFs. I have attached the top level stdout and stderr logs for the slurm job, and the stdout.background and stderr.background logs from shard 3 of ImportGvcfs. No java error log was present on any of the ImportGvcfs shards' execution directories, and all except shard 9 had rc=250.; [ImportGvcfsWithTmpError.tar.gz](https://github.com/broadinstitute/gatk/files/9911311/ImportGvcfsWithTmpError.tar.gz)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8076#issuecomment-1298661994:240,validat,validation,240,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8076#issuecomment-1298661994,1,['validat'],['validation']
Security,I run like this:. ```; ./src/main/java/org/broadinstitute/hellbender/tools/validation/validate-reads-spark-pipeline.sh src/test/resources/org/broadinstitute/hellbender/tools/validation/CEUTrio.HiSeq.WGS.b37.ch20.1m-1m1k.NA12878.bam src/test/resources/large/human_g1k_v37.20.21.fasta src/test/resources/large/human_g1k_v37.20.21.2bit src/test/resources/large/dbsnp_138.b37.20.21.vcf throwOnDiff saveIntermediateFiles; ```. And it fails like this:. ```; -----------CompareMatrix summary------------; diff count %total; -24 101 0.0662; -23 39 0.0256; -22 10 0.0066; 0 152210 99.8033; 22 10 0.0066; 23 39 0.0256; 24 101 0.0662. ---------CompareMatrix full matrix (non-zero entries) ----------; QRead1 QRead2 diff count; 2 2 0 15922; 2 24 -22 10; 2 25 -23 39; 2 26 -24 101; 4 4 0 41; 5 5 0 35; 21 21 0 16; 22 22 0 7443; 23 23 0 7881; 24 2 22 10; 24 24 0 37073; 25 2 23 39; 25 25 0 20772; 26 2 24 101; 26 26 0 43841; 27 27 0 16358; 28 28 0 2828; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1922#issuecomment-226819228:75,validat,validation,75,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1922#issuecomment-226819228,3,['validat'],"['validate-reads-spark-pipeline', 'validation']"
Security,"I see that this is occurring in the mitochondrial chromosome. The model was trained mostly with the autosomes and so scores on the mitochondrial DNA have not been extensively validated. That said, this looks like a bug and we hope to have a fix in soon.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4727#issuecomment-387737569:175,validat,validated,175,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4727#issuecomment-387737569,1,['validat'],['validated']
Security,"I see the exception on most chromosomes, here's the one for chr1:. java.lang.IllegalArgumentException: Invalid interval. Contig:chr1 start:79293873 end:79293872; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:49); at org.broadinstitute.hellbender.engine.AssemblyRegion.add(AssemblyRegion.java:335); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.fillNextAssemblyRegionWithReads(AssemblyRegionIterator.java:230); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.loadNextAssemblyRegion(AssemblyRegionIterator.java:194); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.next(AssemblyRegionIterator.java:135); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.next(AssemblyRegionIterator.java:34); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:290); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:271); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:893); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:136); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:152); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:195); at org.broadinstitute.hellbender.Main.main(Main.java:275)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4120#issuecomment-356718095:207,validat,validateArg,207,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4120#issuecomment-356718095,2,['validat'],"['validateArg', 'validatePositions']"
Security,"I see, thanks for pointing this out. We need to run; ```; git archive --format tar.gz --prefix gatk-{hash} -o gatk-{hash}.tar.gz {hash}; ```; to retrieve and archive the files.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6395#issuecomment-584477699:101,hash,hash,101,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6395#issuecomment-584477699,3,['hash'],['hash']
Security,"I set TEST_TYPE to ""all"" and was able to run this test without failure. The command I used is:; ```; ./gradlew test --tests org.broadinstitute.hellbender.utils.nio.GcsNioIntegrationTest.openPublicFile; ```; I ran it 10 times and got the same result every time:; `BUILD SUCCESSFUL`. It looks like this was a transient problem: either the internet connection was stalled or the authentication server was down temporarily. As this happens at the very beginning of the execution, it's probably not a very big deal: the user can just retry. Incidentally, PR #2506 that is under review lengthens the connection timeouts, if I am not mistaken. This will probably make the problem less likely to reoccur.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2514#issuecomment-288852260:376,authenticat,authentication,376,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2514#issuecomment-288852260,1,['authenticat'],['authentication']
Security,"I spoke too soon. No matter how I define the type, whether String or File, when I run womtools validate I get the error:. ```; ERROR: Value for this attribute is expected to be a string:. bam: {; ```. I added the following parameter_meta field to the task:. ![screenshot 2018-11-08 18 02 03](https://user-images.githubusercontent.com/11543866/48232693-7569ff00-e380-11e8-9dad-2eed3ca68118.png). How to correct this @cjllanwarne? Here's the relevant WDL: https://github.com/broadinstitute/gatk/blob/4.0.11.0/scripts/cnv_wdl/cnv_common_tasks.wdl",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4806#issuecomment-437188313:95,validat,validate,95,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4806#issuecomment-437188313,1,['validat'],['validate']
Security,"I suggest copying the test file. Shared test files can lead to trouble. On Sunday, June 14, 2015, cmnbroad notifications@github.com wrote:. > Based on the TODO that was in ReadsDataSource.java, I exposed a; > SamReaderFactory parameter for ReadsDataSource rather than limit it to just; > validation stringency.; > ; > Whats the right protocol for adding a test that uses a test file from; > another package (I'm reaching into the picard test data for a data file for; > ; > ## an engine test). Alternatively, is there a better way to test this change ?; > ; > You can view, comment on, or merge this pull request online at:; > ; > https://github.com/broadinstitute/hellbender/pull/565; > Commit Summary; > - Enable setting validation stringency in ReadsDataSource.; > ; > File Changes; > - _M_; > src/main/java/org/broadinstitute/hellbender/engine/ReadsDataSource.java; > https://github.com/broadinstitute/hellbender/pull/565/files#diff-0; > (38); > - _M_; > src/test/java/org/broadinstitute/hellbender/engine/ReadsDataSourceUnitTest.java; > https://github.com/broadinstitute/hellbender/pull/565/files#diff-1; > (25); > ; > Patch Links:; > - https://github.com/broadinstitute/hellbender/pull/565.patch; > - https://github.com/broadinstitute/hellbender/pull/565.diff; > ; > —; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/hellbender/pull/565. ## . Sent from Gmail Mobile",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/565#issuecomment-111864608:196,expose,exposed,196,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/565#issuecomment-111864608,3,"['expose', 'validat']","['exposed', 'validation']"
Security,"I suppose that you are going to try to set up the automated tests on PPC. If the test environment is set up, the native libraries can live in the same repo.; @droazen @akiezun Did you not get access a PPC service?. It is useful to keep the source files under a single tree (with the common and cpu-specific parts as I explained) from the view point of maintenance and another porting.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1788#issuecomment-217069817:192,access,access,192,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1788#issuecomment-217069817,1,['access'],['access']
Security,"I terms of comparing implementations in this particular instance, the entire BAM file is also being scanned, so there is no additional limitation in using Parquet. In general, though, there are some additional complications for typical Hadoop data sets due to their distributed nature. There are multiple methods for accelerating lookups, depending on the particular application and what kind of latency you need. A few of them:; - Data set partitioning in the style of Hive, where you split your data set into a directory hierarchy that's based on the values of one of the columns. This is like building an index, and should be done on cols that feature in lot of predicates. In the quince repo that we're using for ingest, we are partitioning the data based on genome locus. This makes it easy to access the data only from the locus of interest.; - Parquet supports predicate pushdown and column stats on its row chunks, so the more you homogenize the data (e.g., by sorting), the more likely it is you can skip large blocks of data.; - The parquet file format supports the concept of indexing (though I don't think it is implemented yet in any of the packages that read/write it); - For particular applications, you can also use a different storage backend like HBase or Kudu that allow very rapid point/range queries at scale. I believe GEL is planning on trying out HBase for some of their applications; - The Hammerbacher lab and the ADAM folks are also working on tools for BAM file indexing and visualization that scales on Hadoop. I think the projects are cycledash/pileup.js and mango, respectively.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1067#issuecomment-152446541:799,access,access,799,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1067#issuecomment-152446541,1,['access'],['access']
Security,"I think all of our dataflow / spark code is at least almost entirely using `GATKRead`. GATKRead is designed to not provide access to the header because it's not available from a google `Read` backed `GATKRead`. It sounds like there is some information that `Read` includes that is missing from a headerless `SAMRecord`. I think we could audit the `SAMRecordToGATKReadAdaptor` to find any places it touches the header and then cache that information in the adaptor before stripping the header. We don't need to add back in the headers at any point, because we provide library functions to perform any header related operation with a provided header.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141107659:123,access,access,123,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141107659,2,"['access', 'audit']","['access', 'audit']"
Security,"I think even more important is that they share the reference genome that they are using. Get Outlook for Android<https://aka.ms/AAb9ysg>; ________________________________; From: Gökalp Çelik ***@***.***>; Sent: Wednesday, April 24, 2024 12:28:39 AM; To: broadinstitute/gatk ***@***.***>; Cc: Ilya Soifer ***@***.***>; Assign ***@***.***>; Subject: Re: [broadinstitute/gatk] Prevent users enabling annotations with mismatching data type (flow etc) (Issue #8788). Assigned #8788<https://github.com/broadinstitute/gatk/issues/8788> to @ilyasoifer<https://github.com/ilyasoifer>. —; Reply to this email directly, view it on GitHub<https://github.com/broadinstitute/gatk/issues/8788#event-12581899218>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AGDPRCP66IKPOBF2GPENP6LY63HAPAVCNFSM6AAAAABGTGMBPWVHI2DSMVQWIX3LMV45UABCJFZXG5LFIV3GK3TUJZXXI2LGNFRWC5DJN5XDWMJSGU4DCOBZHEZDCOA>.; You are receiving this because you were assigned.Message ID: ***@***.***>. ________________________________. CONFIDENTIALITY NOTICE: This message (including any attachments) should be presumed to contain confidential, proprietary, privileged and/or private information. Information contained in this message is intended only for the recipient(s) named above. Any disclosure, reproduction, distribution or other use of this message or any attachments by any unauthorized individual or entity is strictly prohibited. If you have received this message in error, please notify the sender immediately, and delete the message and any attachments. Learn more about Ultima Genomics’ Privacy Policy<https://www.ultimagenomics.com/privacy-policy>.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8788#issuecomment-2074020625:1003,CONFIDENTIAL,CONFIDENTIALITY,1003,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8788#issuecomment-2074020625,2,"['CONFIDENTIAL', 'confidential']","['CONFIDENTIALITY', 'confidential']"
Security,"I think for SQ you could limit the lines you write out to the contigs that are covered in your intervals list, ignore the rest. So you can access that info as soon as you've parsed the command line. Or if you're running without an intervals list you could supply a seq dict file through a separate arg. But the former seems safer. . For other modes: EXTREME would hardcode what we consider required. Then you could potentially do ARBITRARY to allow passing strings for specific attributes that you want to drop. . None of these would have to depend on what's in the calls, I think.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2233#issuecomment-266059687:139,access,access,139,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2233#issuecomment-266059687,1,['access'],['access']
Security,"I think it still needs to be run through an QA process independent from the dev team . There may be options, details etc that we don't do exactly how production does. Things coming to mind (there's surely much more):; - test splitting input in in many ways (the same exact way as production does); - generation of md5 (we're not doing it yet); - the PG tag in the header (we're not writing one yet); - validate that the original qualities are preserved exactly (i;ve not done it as part of this validation); - testing memory consumption and resource consumption - that's something that only someone close to production can measure/answer in the same environment in which production runs. I'm running on very different env. etc.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1413#issuecomment-188597920:402,validat,validate,402,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1413#issuecomment-188597920,2,['validat'],"['validate', 'validation']"
Security,"I think it triggers in certain situations where a firewall is blocking the connection. If the internet is simply unreachable it doesn't happen, so I don't know what the exact error case is. It happened consistently for people inside Intel's firewall or vpn. . An option to disable gcs support isn't a bad idea, it's kind of a hack though, it would be better if we could understand and avoid triggering the problem. If we could only initialize GCS support when we are sure that we actually are accessing files from google that could be a useful, but it doesn't seem like there's any single point we can plug into to detect that, it would have to be spread over everything that uses paths.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3491#issuecomment-427432141:50,firewall,firewall,50,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3491#issuecomment-427432141,3,"['access', 'firewall']","['accessing', 'firewall']"
Security,"I think that `DIRECT` should only give access to walker impls. Anything Spark should use spark-submit or gcs. So if you're running a local Spark tool, you would use `--runner SPARK --master local[3]`. Then you never need the additional sparkMaster option.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1329#issuecomment-164065760:39,access,access,39,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1329#issuecomment-164065760,1,['access'],['access']
Security,"I think that each tool should either emit proper error messages, or deal with the data it's given. currently BQSR emits a cryptic error message so I think that this PR is an improvement regardless of whether the pipeline is sanitized. . That said, I think that it might be a good idea to make ValidateSamFile break on non-ACGTN bases.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6625#issuecomment-642719900:224,sanitiz,sanitized,224,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6625#issuecomment-642719900,2,"['Validat', 'sanitiz']","['ValidateSamFile', 'sanitized']"
Security,"I think that it is necessary to have a way for downstream projects to override some of the top-level arguments in the base CLP class. For example, the config file is for documentation purposes, but I don't want to expose users to that argument because I will set the defaults programmatically. Another example is the GCS retries, which might not be useful for a software that is not planning to support GCS even if it is already implemented (or does not want to expose). As a downstream developer, for me it is important to being able to configure arguments and expose/hide them to my final users; with the current implementation, my main issue is to have an argument that are irrelevant for the toolkit user and that I get questions about why and how to use them (the most clear example, the config file). If the main problem is to change an interface, a default value for new methods can be added to keep the same behavior.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3998#issuecomment-361876183:214,expose,expose,214,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3998#issuecomment-361876183,3,['expose'],['expose']
Security,"I think that it would be fine to have both legacy and modern styles in the; examples, but I think having gatk in the picard docs will not pass review. On Tue, Dec 5, 2017 at 10:41 PM, sooheelee <notifications@github.com> wrote:. > It would be nice to have a few Picard tools that we feature in BPWs (e.g.; > MarkDuplicates) show both syntaxes:; >; > 1. java -jar picard.jar ValidateSamFile I=reads.bam MODE=SUMMARY; > 2. gatk ValidateSamFile -I reads.bam -MODE=SUMMARY; >; > I assume MODE gets one dash and not two, but really I don't know. So; > having such examples scattered throughout the tool chest for most often; > used tools would be great.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/3853#issuecomment-349522774>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACnk0ht41UsGHu_2TgrbKKNJwDepEdMZks5s9gz4gaJpZM4QitCF>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-349642831:374,Validat,ValidateSamFile,374,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-349642831,2,['Validat'],['ValidateSamFile']
Security,"I think that this is a nice feature (at least for me) and not a bug. For example, if in GATK someone runs a tool with `-RF read_filters.args`, then the pipeline cannot be reproduced in a different dataset unless the file is accessible. I can understand that it could be also nice to preserve the `-RF read_filters.args` to be able to modify the file an re-run the tool with different parameters, but for me the purpose of storing the command line in the header or other places is keep track of the exact params that I used: if a file is modified, then it is impossible to trace the params. For input files, this is expected (if the input has changed, it is expected that the result change), but for arguments it shouldn't be the case (independently of the file changing, the tool was running with exactly that parameters). I vote for solve this in Barclay in a configurable way, to allow users to decide which kind of verbosity of the command line they want (I definitely prefer to expand as currently).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3797#issuecomment-342798092:224,access,accessible,224,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3797#issuecomment-342798092,1,['access'],['accessible']
Security,"I think that this is part of wider need to for dependency-injection in tools; often the initialize() method might be loaded with instantiation of components that themselves require some user argument inputs.. can this be done in a more declarative fashion? . For example... HC, UG or GenotypeGVCFs the have annotationEngine or genotypingEngine components that are explicitly initialized in initialize() what if the engine is responsible to instantiate them and add the appropriate arguments to the command line which are declared in the corresponding classes rather than in the tool (or a synthetic argument collection class)?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/96#issuecomment-69810912:58,inject,injection,58,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/96#issuecomment-69810912,1,['inject'],['injection']
Security,"I think the `Optimized` version deals with avoiding replicating reads in a more nuanced manner, but if I understand correctly, it doesn't seem to me to avoid the shuffle. It looks like it essentially ignores the concept of data locality entirely, and potentially transfers a lot of data over the network. (Equivalent to performing a shuffle on a sorted file in HDFS.). IMO, the current ""shuffle"" implementation is already a ""Spark-y"" way to do it, but with multiple inefficiencies:; - Reads/variants are keyed to their corresponding shards, replicating reads if they cross over shard boundaries. This necessitates an `aggregateByKey` operation that potentially reshuffles the entire data set at the end to deal with neighboring shards that could be hashed to different machines.; - The impl uses `cogroup` and `groupByKey`, which require materializing all values for a key in memory (which could be large). Best to avoid these if possible.; - And related to the previous issue, the join strategy for reads and variants is basically a cross-product-and-filter, which is not very efficient, especially considering that the data can be ordered. I think the best implementation here would steal JP's method of sharding the reads/variants, but make use of `repartitionAndSortWithinPartition`, which lets you specify what partition to use and also sorts all the values within a given partition. This means that we could employ a sort-merge on each partition, and only scan through the datasets once after shuffling them. Do you already have an impl for doing a sort-merge of `Locatable`s? These can be a bit tricky. I wrote one for the `ShuffleRegionJoin` impl in ADAM, but there are semantic differences that would make it less efficient to use. (Specifically, it would require the `aggregateByKey` operation and also creating `SimpleInterval`-style objects from a separate model.). Finally, I would also add the ability to specify which join strategy to use separately for the reference bases and the vari",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1007#issuecomment-151721602:749,hash,hashed,749,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1007#issuecomment-151721602,1,['hash'],['hashed']
Security,"I think the issue might be that you need a ` -- ` between the gatk options on the left, and the spark specific options on the right. This is a confusing artifact of how our arg parsing works and the fact that the gatk-launch script needs a way of finding the spark options but doesn't have access to our java parser. (we're planning on fixing that in the near future, but no good time line) . Could you try:; ```; /home/axverdier/Tools/GATK4/gatk-4.beta.6/gatk-launch CountReadsSpark --programName gatk4-testing --input hdfs://spark01:7222/user/axverdier/data/710-PE-G1.bam --output hdfs://spark01:7222/user/axverdier/testOutGATK_CountReadsSpark --javaOptions -Dmapr.library.flatclass -- --sparkRunner SPARK --sparkMaster yarn --deploy-mode cluster; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3933#issuecomment-350064538:290,access,access,290,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3933#issuecomment-350064538,1,['access'],['access']
Security,"I think the piece we need is just the parser, though -- we can keep the Picard code that injects values into instance variables & parses the annotations, etc., we just need something to go from the raw args array into a parsed set of names + values.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/72#issuecomment-69632895:89,inject,injects,89,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/72#issuecomment-69632895,1,['inject'],['injects']
Security,"I think there are two points to this issue: 1. validation or no-validation and 2. need to pass GenomeLocParser around... . Is 1. about performance? otherwise we prefer to have validated locations, right? and If it is about performance I think it should be shown that it really makes a difference to remove those checks. About, 2., can be solved by being able to recover the sequence-dictonary/reference object (called Reference from this point on) from a genome loc and then you can ask it for a new genome-loc if the appropriate methods are added instead of depending on that annoying middle man called GenomeLocParser. I would say that is unlikely to be in the situation where you want to create a GenomeLoc out of the blur without having already a reference to another GenomeLoc or Reference object available. It is an issue if GenomeLoc holds on to a reference to the Reference object? (or the instantiating class is a inner class of the Reference?)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/100#issuecomment-69802695:47,validat,validation,47,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/100#issuecomment-69802695,3,['validat'],"['validated', 'validation']"
Security,"I tried clearing my caches and rebuilding, but I resolve everything. I noticed that our artifactory website looks much different today than it did yesterday. I wonder if it was down temporarily for an update. Maybe try again now? Unless they put it behind the firewall which would be a disaster... can you access https://artifactory.broadinstitute.org/?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2579#issuecomment-292587641:260,firewall,firewall,260,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2579#issuecomment-292587641,2,"['access', 'firewall']","['access', 'firewall']"
Security,"I tried this branch out and got the dreaded 404 error, unfortunately:. ```; $ ./gatk-launch CountReadsSpark -I gs://hellbender/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -- --sparkRunner GCS --cluster droazen-test-cluster --executor-cores 2 --num-executors 2; Using GATK jar /Users/droazen/src/hellbender/build/libs/gatk-package-4.beta.6-54-g0ee99da-SNAPSHOT-spark.jar; jar caching is disabled because GATK_GCS_STAGING is not set. please set GATK_GCS_STAGING to a bucket you have write access too in order to enable jar caching; add the following line to you .bashrc or equivalent startup script. export GATK_GCS_STAGING=gs://<my_bucket>/. Replacing spark-submit style args with dataproc style args. --cluster droazen-test-cluster --executor-cores 2 --num-executors 2 -> --cluster droazen-test-cluster --properties spark.driver.userClassPathFirst=true,spark.io.compression.codec=lzf,spark.driver.maxResultSize=0,spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 ,spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 ,spark.kryoserializer.buffer.max=512m,spark.yarn.executor.memoryOverhead=600,spark.executor.cores=2,spark.executor.instances=2. Running:; gcloud dataproc jobs submit spark --cluster droazen-test-cluster --properties spark.driver.userClassPathFirst=true,spark.io.compression.codec=lzf,spark.driver.maxResultSize=0,spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 ,spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3855#issuecomment-347320994:504,access,access,504,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3855#issuecomment-347320994,1,['access'],['access']
Security,"I was able to download the gnomAD VCFs with a gsutil cp command within a; couple hours of the remote funcotator failures. --; Alan Hoyle - alan@alanhoyle.com - alanhoyle.com; ------------------------------; *From:* Jonn Smith <notifications@github.com>; *Sent:* Tuesday, November 10, 2020 9:59:06 PM; *To:* broadinstitute/gatk <gatk@noreply.github.com>; *Cc:* Alan Hoyle <alan@alanhoyle.com>; Mention <mention@noreply.github.com>; *Subject:* Re: [broadinstitute/gatk] Funcotator with gnomAD enabled crashes; with Bad Request (#6926). I have tested this with a fresh gcloud client and have not been able to; reproduce the error. I did find an article from someone else who got the 400: invalid_grant; error:; https://blog.timekit.io/google-oauth-invalid-grant-nightmare-and-how-to-fix-it-9f4efaf1da35; <https://www.google.com/url?q=https://blog.timekit.io/google-oauth-invalid-grant-nightmare-and-how-to-fix-it-9f4efaf1da35&source=gmail-imap&ust=1605668351000000&usg=AOvVaw03ZXI9QiPy1AFI5zfsFIjB>. The long and short of it is that it's an authentication issue. Can you; verify that the authentication you're using on the terminal is valid? That; is, can you get at other public resources on gcloud?. —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub; <https://www.google.com/url?q=https://github.com/broadinstitute/gatk/issues/6926%23issuecomment-725097056&source=gmail-imap&ust=1605668351000000&usg=AOvVaw0sZboplCCqclv3xBdQk3Fb>,; or unsubscribe; <https://www.google.com/url?q=https://github.com/notifications/unsubscribe-auth/AACGX433BU42UPHZTKQLTBTSPH4XVANCNFSM4TD2FDGA&source=gmail-imap&ust=1605668351000000&usg=AOvVaw0n415Vb2d-0gOnEk9wramu>; .",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6926#issuecomment-728261716:1038,authenticat,authentication,1038,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6926#issuecomment-728261716,2,['authenticat'],['authentication']
Security,I wasn't able to reproduce this exact issue. However at least I can make the error message a bit more user-friendly (submitting #2417 for review). Crucially this will now give the name of the file we're having issues with (thus removing the uncertainty about whether it's the data or the index file we cannot access).,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2415#issuecomment-281515286:309,access,access,309,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2415#issuecomment-281515286,1,['access'],['access']
Security,"I would like to specify what passing a `ReadFilter` to some of my tools means, so maybe passing an `ArgumentCollection` will be simpler than this one, I agree. Although #2085 may solve the issue regarding the `ReadTransformer`/`ReadFilter` ordering, I would like to have in the plugin a way to specify different parameters (maybe some of then hidden before expose to users or advanced in the case of disabling). I will open a new PR for that change, but I will really appreciate if I can get something like that in this and other plugins (if implemented).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2353#issuecomment-275082983:357,expose,expose,357,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2353#issuecomment-275082983,1,['expose'],['expose']
Security,"I'd also be hesitant to break the previous expectation that IntervalArgumentCollection contains a non-empty list of intervals. If I understand correctly (and apologies if not, I'm glancing at the repo between paternity-leave duties and am quite sleep deprived!), all calling code would have to add an explicit check that the new option isn't enabled or risk failing ungracefully downstream. For CNV code, this might be as simple as changing the validation method `CopyNumberArgumentValidationUtils.validateIntervalArgumentCollection`, but I wouldn't generally expect it to be so straightforward to add such checks throughout the codebase. I also agree with @lbergelson that the expected behavior might not be immediately clear and that perhaps this could be addressed in the scattering step---seems like shards could just be limited to regions that cover the resource at the outset. Consider also an older comment at https://github.com/broadinstitute/gatk/pull/5392#issuecomment-435588845 about whether or not we should just use the equivalent Picard tool (horrible glob aside).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6209#issuecomment-540740687:445,validat,validation,445,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6209#issuecomment-540740687,2,['validat'],"['validateIntervalArgumentCollection', 'validation']"
Security,I'll add: I suspect that this could get done pretty quickly/painlessly if @lbergelson (our travis expert) and yourself (who has set up tests like this before many times) got together in a room and hashed it out. Recommend setting up a meeting with @lbergelson at a time that's convenient in the next week or two. I'll post command lines here by early next week.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2298#issuecomment-287478818:197,hash,hashed,197,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2298#issuecomment-287478818,1,['hash'],['hashed']
Security,I'm actually a little surprised that it's not emitting a 1/* at the second location (despite that perhaps not being compatible with the spec; I agree with @bhandsaker 's view of what `*` should mean) given the changes I made in https://github.com/broadinstitute/gatk/pull/4963 to support spanning deletions. Those changes were not made with MNPs in mind but reading the code I'm surprised the `*` allele is not injected into the variant context. What version of HaplotypeCaller are you using @tfenne?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5523#issuecomment-447369490:411,inject,injected,411,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5523#issuecomment-447369490,1,['inject'],['injected']
Security,"I'm also seeing this more often during the Docker build, not sure if it is related:. ````; Step 5/27 : RUN /gatk/gradlew clean compileTestJava installAll localJar createPythonPackageArchive -Drelease=$DRELEASE; ---> Running in d08cd7336c45; Downloading https://services.gradle.org/distributions/gradle-3.1-bin.zip; .......................................; Exception in thread ""main"" javax.net.ssl.SSLException: Connection has been shutdown: javax.net.ssl.SSLException: java.net.SocketException: Connection reset; 	at sun.security.ssl.SSLSocketImpl.checkEOF(SSLSocketImpl.java:1541); 	at sun.security.ssl.AppInputStream.available(AppInputStream.java:60); 	at java.io.BufferedInputStream.available(BufferedInputStream.java:410); 	at sun.net.www.MeteredStream.available(MeteredStream.java:170); 	at sun.net.www.http.KeepAliveStream.close(KeepAliveStream.java:85); 	at java.io.FilterInputStream.close(FilterInputStream.java:181); 	at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.close(HttpURLConnection.java:3448); 	at org.gradle.wrapper.Download.downloadInternal(Download.java:77); 	at org.gradle.wrapper.Download.download(Download.java:44); 	at org.gradle.wrapper.Install$1.call(Install.java:61); 	at org.gradle.wrapper.Install$1.call(Install.java:48); 	at org.gradle.wrapper.ExclusiveFileAccessManager.access(ExclusiveFileAccessManager.java:69); 	at org.gradle.wrapper.Install.createDist(Install.java:48); 	at org.gradle.wrapper.WrapperExecutor.execute(WrapperExecutor.java:107); 	at org.gradle.wrapper.GradleWrapperMain.main(GradleWrapperMain.java:61); Caused by: javax.net.ssl.SSLException: java.net.SocketException: Connection reset; 	at sun.security.ssl.Alerts.getSSLException(Alerts.java:208); 	at sun.security.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1949); 	at sun.security.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1906); 	at sun.security.ssl.SSLSocketImpl.handleException(SSLSocketImpl.java:1870); 	at sun.security.ssl.SSLSocketImpl.handleException(SSLSocketImpl.java:1815); ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4194#issuecomment-358498401:521,secur,security,521,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4194#issuecomment-358498401,2,['secur'],['security']
Security,"I'm getting the same issue on GATK 4.1.9.0 FilterAlignmentArtifacts. This bug has been present for 1 year. Has this been fixed?; Note: There is no work-around because FilterAlignmentArtifacts does not have a --smith-waterman option. Here is my error:; ```; 20:12:42.724 WARN FilterAlignmentArtifacts - . [1m[31m !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: FilterAlignmentArtifacts is an EXPERIMENTAL tool and should not be used for production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!![0m. 20:12:42.725 INFO FilterAlignmentArtifacts - Initializing engine; 20:12:48.403 INFO FeatureManager - Using codec VCFCodec to read file gs://fc-secure-024a1aae-a4f9-4025-aa93-f759f93a8203/50383670-4607-4e59-9bfc-4db970980f0e/Mutect2/773a91ea-25be-4d49-b97c-16527076250c/call-Filter/cacheCopy/TN-20-36-filtered.vcf; 20:12:50.117 INFO FilterAlignmentArtifacts - Done initializing engine; 20:12:51.042 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk/gatk-package-4.1.9.0-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 20:12:51.099 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 20:12:51.100 INFO IntelPairHmm - Available threads: 14; 20:12:51.100 INFO IntelPairHmm - Requested threads: 4; 20:12:51.100 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 20:12:51.100 INFO ProgressMeter - Starting traversal; 20:12:51.100 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 20:20:25.766 INFO ProgressMeter - chr3:104142090 7.6 1000 132.0; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007efc9818177e, pid=24, tid=0x00007f13b3c76700; #; # JRE version: OpenJDK Runtime Environment (8.0_242-b08) (build 1.8.0_242-8u242-b08-0ubuntu3~18.04-b08); # Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 ); # Problematic frame:; # C [libgkl_smithwaterman18",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-781673098:682,secur,secure-,682,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-781673098,1,['secur'],['secure-']
Security,"I'm glad it's working now, but a PS since you asked about `-independent-mates`: several months ago we made Mutect2 force paired reads to share the latent random variable indicating which haplotype they are derived from in the somatic genotyping model. This is correct because paired reads come from the same molecule of DNA. `-independent-mates` disables this and tells Mutect2 to forget about pairing. We only created the option because some synthetic validation data is generated by spiking in variation without regard to pairing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6230#issuecomment-596165833:453,validat,validation,453,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6230#issuecomment-596165833,1,['validat'],['validation']
Security,"I'm going to abandon this - Picard tests depend on the order in contrived ways, eg:. ```; private DetailPair getWorstMetrics(final List<DetailPair> metrics) {; PreAdapterDetailMetrics worstPreAdapterMetrics = null;; BaitBiasDetailMetrics worstBaitBiasMetrics = null;; for (final DetailPair m : metrics) {; if (worstPreAdapterMetrics == null || m.preAdapterMetrics.QSCORE < worstPreAdapterMetrics.QSCORE) worstPreAdapterMetrics = m.preAdapterMetrics;; if (worstBaitBiasMetrics == null || m.baitBiasMetrics.QSCORE < worstBaitBiasMetrics.QSCORE) worstBaitBiasMetrics = m.baitBiasMetrics;; }; return new DetailPair(worstPreAdapterMetrics, worstBaitBiasMetrics);; }; ```. if all `QSCORE` are the same this arbitrarily pick the first one. That backfires when I switch to linked hashmaps and linkedhashsets. Specifically `CollectSequencingArtifactMetricsTest` fails. Given that #1210 (fate of Picard) is unresolved I'm going to not put time into it yet.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1844#issuecomment-220611498:772,hash,hashmaps,772,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1844#issuecomment-220611498,1,['hash'],['hashmaps']
Security,I'm informed that I have access to the broad-firecloud-dsde billing account and it appears whatever error we encountered the other day was something transient.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4806#issuecomment-437409513:25,access,access,25,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4806#issuecomment-437409513,1,['access'],['access']
Security,I'm kind of glad that 4.1 exposed this because previously unmatched pairs were silently giving wildly-inflated contamination estimates.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5880#issuecomment-483083398:26,expose,exposed,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5880#issuecomment-483083398,1,['expose'],['exposed']
Security,"I'm new at this, so I may be missing something, but it looks to me like there is an issue with the conversion code in GenomicsConverter.makeSAMRecord() in com.google.cloud.genomics.gatk.common. I've been working on this bam file issue, correcting errors in the files used for tests. Many of the errors involve reads with FLAGs that indicate that they are in pairs, but the mate is not extant in the file, causing the error. A way to fix this without deleting the offending reads is to set the FLAG to zero and also modify the RNEXT, PNEXT, and TLEN fields, if necessary, so that the read becomes single (provided that the values of all of these fields are not important for the tests). However, when I do this, I find that tests that write and then read bam files fail, because when the just-written file is read back, SAM validation complains that the mate unmapped FLAG is set for an unpaired read. It turns out that the copy of the file written by the test substitutes the value '8' for '0' as the FLAG for the modified reads. The relevant code in GenomicsConvertermakeSamRecord() (line 170) is:. flags += ((read.getNextMatePosition() == null || read.getNextMatePosition.getPosition() == null)) ? 8 : 0;. The effect of this line is that all reads which have null mate positions, even those which the FLAG specifies as unpaired, get the mate unmapped FLAG set, causing the validation errors that i'm seeing. The reason the tests have not failed before is apparently that the existing test files do not contain any reads with FLAGs that specify them as unpaired. A simple fix for this would be to convert the line above to:. flags += ( paired && (read.getNextMatePosition() == null || read.getNextMatePosition.getPosition() == null)) ? 8 : 0;. The redundant parens in the original code suggest that something like this may have been intended,but the google genomics documentation at http://google-genomics.readthedocs.org/en/latest/migrating_tips.html gives the following pseudocode:. flags += read.n",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/569#issuecomment-114101033:823,validat,validation,823,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/569#issuecomment-114101033,1,['validat'],['validation']
Security,"I'm not able to access a cluster to test this on right now, however I wonder if it's something to do with the sort order declared in the BAM header. MarkDuplicatesSpark uses the same header for the output as the input, and doesn't change the sort order attribute, unlike SortBamSpark. Does setting it to unsorted help?. Have you been able to reproduce this on smaller inputs? I noticed that we don't have an integration test for MD.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1005#issuecomment-148753478:16,access,access,16,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1005#issuecomment-148753478,1,['access'],['access']
Security,I'm not sure I have access to that bucket -- which project is it associated with?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2658#issuecomment-299480244:20,access,access,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2658#issuecomment-299480244,1,['access'],['access']
Security,"I'm not sure at this point. ; Is it possible the issue would also occur if the bam passes ValidateSamFile, and the intervals file is sorted, but the 2 have different contig orderings?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6065#issuecomment-591651451:90,Validat,ValidateSamFile,90,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6065#issuecomment-591651451,1,['Validat'],['ValidateSamFile']
Security,"I'm not sure exactly what's happening but I suspect it has something to do with the way the files are mounted. My guess is that there is some sort of transient interruption happening in the connection between the EC2 instance and the file server, and it's causing an error in gatk. When reading from a local file GATK does not expect any errors since errors in local files are usually fatal problems caused by a broken disk. Its probably some sort of bug in amazon's fuse implementation which isn't properly hiding network problems from the software. . I expect that your output is truncated at the point the error occured, and you probably need to rerun those shards. Instead of mounting them with amazon's fuse, you could try to either copy the files to a local disc, or access them using an NIO filesystem plugins like this plugin https://github.com/awslabs/aws-java-nio-spi-for-s3 or as signed URLs using https://github.com/broadinstitute/http-nio/ (included in gatk 4.6).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8735#issuecomment-2214915942:773,access,access,773,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8735#issuecomment-2214915942,1,['access'],['access']
Security,"I'm not sure what went wrong exactly, I might have messed up something when I did the travis encrypt command. I've deleted the previous one, regenerated a new-new key, and reencrypted it. Hopefully this time it will work.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5308#issuecomment-430319851:93,encrypt,encrypt,93,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5308#issuecomment-430319851,1,['encrypt'],['encrypt']
Security,"I've gotten this to work with an internal GCE project (I haven't successfully tested with broad-dsde-dev because of firewall rules). However, it should look like this using bdutil. ```; ./bdutil -P cluster-1 -z us-central1-a -p broad-dsde-dev -b dataproc-8cbe9d51-94fb-4ad4-9c34-a283212c2ae6-us -e hadoop2 socksproxy 12340; ```. which is just a wrapper around. ```; gcloud --project=broad-dsde-dev --quiet --verbosity=info compute ssh cluster-1-m --command= --ssh-flag=-N --ssh-flag=-D12340 --ssh-flag=-oServerAliveInterval=60 --ssh-flag=-oServerAliveCountMax=3 --ssh-flag=-oConnectTimeout=30 --zone=us-central1-a; ```. and a few other commands. After the proxy has started, you need to open up Chrome with the proxy:. ```; /Applications/Google\ Chrome.app/Contents/MacOS/Google\ Chrome --proxy-server='socks5://localhost:12340' --host-resolver-rules='MAP * 0.0.0.0, EXCLUDE localhost' --user-data-dir=/tmp/bdutil-socksproxy/cluster-1-m http://cluster-1-m:8088 http://cluster-1-m:50070; ```. That'll open up the master page. If you have any funky IP address/proxy extensions (like I do), you'll need to turn those off. @droazen and @lbergelson, where do you want this documented?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/975#issuecomment-148457456:116,firewall,firewall,116,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/975#issuecomment-148457456,1,['firewall'],['firewall']
Security,"I've removed some in #2786. The rest are all related to the `ReferenceAPISource` (so not part of GCS authentication, part of Genomics authentication). So one may argue that after this change lands, we can close this bug. Or we can push further and try to use `authHolder` instead of the `PipelineOptions` everywhere.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/963#issuecomment-305275228:101,authenticat,authentication,101,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/963#issuecomment-305275228,2,['authenticat'],['authentication']
Security,"INDEL informative reads based on the reference confidence model"">; ##FORMAT=<ID=MB,Number=4,Type=Integer,Description=""Per-sample component statistics to detect mate bias"">; ##FORMAT=<ID=MIN_DP,Number=1,Type=Integer,Description=""Minimum DP observed within the GVCF block"">; ##FORMAT=<ID=PL,Number=G,Type=Integer,Description=""Normalized, Phred-scaled likelihoods for genotypes as defined in the VCF specification"">; ##FORMAT=<ID=PRI,Number=G,Type=Float,Description=""Phred-scaled prior probabilities for genotypes"">; ##FORMAT=<ID=PS,Number=1,Type=Integer,Description=""Physical phasing ID information, where each unique ID within a given sample (but not across samples) connects records within a phasing group"">; ##FORMAT=<ID=SB,Number=4,Type=Integer,Description=""Per-sample component statistics which comprise the Fisher's Exact Test to detect strand bias"">; ##FORMAT=<ID=SPL,Number=.,Type=Integer,Description=""Normalized, Phred-scaled likelihoods for SNPs based on the reference confidence model"">; ##FORMAT=<ID=SQ,Number=A,Type=Float,Description=""Somatic quality"">; ##DRAGENCommandLine=<ID=HashTableBuild,Version=""SW: 01.003.044.3.8.4, HashTableVersion: 8"",CommandLineOptions=""/opt/edico/bin/dragen --build-hash-table true --enable-cnv true --ht-alt-aware-validate true; ##DRAGENCommandLine=<ID=dragen,Version=""SW: 05.021.609.3.9.5, HW: 05.021.609"",Date=""Wed Feb 09 21:30:31 UTC 2022"",CommandLineOptions=""--bam-input s3://cromwell-dragen-us-east-1/samples/validacaoDRAGEN/ba; ```. About chrM, yeah... all of them have this info... like below. ```; grep ""^chrM"" <name>.hard-filtered.gvcf | head ; chrM	1	.	G	<NON_REF>	.	weak_evidence	END=1	GT:AD:DP:SQ:MIN_DP	0/0:112,1579:1691:0:1691; chrM	2	.	A	<NON_REF>	.	PASS	END=72	GT:AD:DP:SQ:MIN_DP	0/0:2504,2:2506:99:1712; chrM	73	.	A	G,<NON_REF>	.	PASS	DP=2017;MQ=208.10;FractionInformativeReads=0.948	GT:SQ:AD:AF:F1R2:F2R1:DP:SB:MB	1/1:98.13,0.00:0,1912,0:1.000,0.000:0,980,0:0,932,0:1912:0,0,756,1156:0,0,932,980; ```. As I replied to you, in another section,",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7797#issuecomment-1112612397:2359,Hash,HashTableBuild,2359,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7797#issuecomment-1112612397,1,['Hash'],['HashTableBuild']
Security,"INDEX false --CREATE_MD5_FILE false --help false --version false --verbosity INFO --QUIET false; [March 9, 2017 7:03:42 PM EST] Executing as gspowley@dna on Linux 3.10.0-514.10.2.el7.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_111-b14; Version: Version:4.alpha.2-170-g8d06823-SNAPSHOT; 19:03:42.998 INFO ValidateSamFile - Defaults.BUFFER_SIZE : 131072; 19:03:42.999 INFO ValidateSamFile - Defaults.COMPRESSION_LEVEL : 1; 19:03:42.999 INFO ValidateSamFile - Defaults.CREATE_INDEX : false; 19:03:42.999 INFO ValidateSamFile - Defaults.CREATE_MD5 : false; 19:03:42.999 INFO ValidateSamFile - Defaults.CUSTOM_READER_FACTORY : ; 19:03:42.999 INFO ValidateSamFile - Defaults.EBI_REFERENCE_SERVICE_URL_MASK : http://www.ebi.ac.uk/ena/cram/md5/%s; 19:03:42.999 INFO ValidateSamFile - Defaults.NON_ZERO_BUFFER_SIZE : 131072; 19:03:42.999 INFO ValidateSamFile - Defaults.REFERENCE_FASTA : null; 19:03:43.000 INFO ValidateSamFile - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 19:03:43.000 INFO ValidateSamFile - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 19:03:43.000 INFO ValidateSamFile - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 19:03:43.000 INFO ValidateSamFile - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 19:03:43.000 INFO ValidateSamFile - Defaults.USE_CRAM_REF_DOWNLOAD : false; 19:03:43.000 INFO ValidateSamFile - Deflater JdkDeflater; 19:03:43.000 INFO ValidateSamFile - Inflater JdkInflater; 19:03:43.000 INFO ValidateSamFile - Initializing engine; 19:03:43.000 INFO ValidateSamFile - Done initializing engine; ERROR: Record 9762, Read name 20GAVAAXX100126:7:2:8126:115177, bin field of BAM record does not equal value computed based on alignment start and end, and length of sequence to which read is aligned; ERROR: Record 24466, Read name 20FUKAAXX100202:7:46:13035:77621, bin field of BAM record does not equal value computed based on alignment start and end, and length of sequence to which read is aligned; ERROR: Record 97940, Read name 20FUKAAXX100202:5:7:21464:86",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2423#issuecomment-285513571:1561,Validat,ValidateSamFile,1561,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2423#issuecomment-285513571,1,['Validat'],['ValidateSamFile']
Security,"Ideally we'd minimize changes to core engine classes, and instead create new specialized subclasses (like a subclass of MultiVariantWalkerGroupedOnStart) that implement the desired behavior. The main thing is that we need something that only affects `VariantEval` - anything that affects other tools would run into the same issues as #4571 - specifically that tests that use VariantContext equality to validate results will start to fail because of the presence of source name in the actual values.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-823578133:402,validat,validate,402,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-823578133,1,['validat'],['validate']
Security,"If I understand what you're proposing, the tool code would have to use `instanceof` checks and typecasts to take advantage of this feature (unless `FeatureInputAwareVariantContext` was exposed in the `apply` method). Other reviewers may feel differently, but using the source field (which I don't view as ""repurposing"" since I think its aligned with the intent of that field) seems cleaner. Up to you if you want to submit a PR with your proposed change to see if its viable.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-824047890:185,expose,exposed,185,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-824047890,1,['expose'],['exposed']
Security,"If a tool exists and is runnable, but is not documented, it should be accessible via tab-completion. Otherwise people can't depend on tab-completion to give a complete list of all tools that can be run.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3596#issuecomment-330936329:70,access,accessible,70,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3596#issuecomment-330936329,1,['access'],['accessible']
Security,"If only some of the accesses failed, then it's likely what we're seeing here is GCS refusing to serve requests because it feels it's getting too many. How many machines are trying to access the file? How many threads per machine? What storage class is the bucket?. The lower storage classes support fewer parallel accesses, and NIO's prefetching (if enabled) results in two threads per reader.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5631#issuecomment-459842021:20,access,accesses,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5631#issuecomment-459842021,3,['access'],"['access', 'accesses']"
Security,"If we find one that does everything we need, sure -- but if there isn't such a beast, I'd advocate a hybrid approach of a NIH library to parse the args according to POSIX conventions, and an ""IH"" solution (which already exists, of course) to handle the annotation parsing and value injection.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/72#issuecomment-69635412:282,inject,injection,282,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/72#issuecomment-69635412,1,['inject'],['injection']
Security,"If we like this -- we also need to . - [x] build a jar; - [x] update the WDL to use this tool (and the Jar); - [ ] Put the BED files someplace public/widely accessible (likely just the 1kb version); - [x] Run an E2E on QuickStart, merge the VCFs and compare (and see no differences); - [x] If we want to validate evenness we need to run with a lot of shards and enough data that they are interesting. Maybe Stroke 10k; - [x] In parallel if we could turn some of the above script into integration tests that would be awesome",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7643#issuecomment-1017113500:157,access,accessible,157,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7643#issuecomment-1017113500,2,"['access', 'validat']","['accessible', 'validate']"
Security,"If you need files that will pass, we have validation passing versions of many of our test files in https://github.com/broadinstitute/gatk/pull/809. It's possible you could pull some of those in. We haven't figured out a scheme for evaluating if they are equivalent for testing purposes, but if you wanted to look at the specific failing files it might be a good place to start. . I'm not sure STRICT is the right default validation since traditionally gatk ran it's own orthogonal validation from picard, and many GATK valid bams will not pass picard validation. I wasn't involved in the talk with @droazen though, so I assume you've already discussed this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1439#issuecomment-175160857:42,validat,validation,42,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1439#issuecomment-175160857,4,['validat'],['validation']
Security,"If you would like the GenomicsDB for chromosome `CM031199.1` (which, by the way, was created with GATK 4.2.4.1) that I used in the above two examples, for your own debugging purposes, you can download it as a .tar archive from:. [https://drive.google.com/file/d/1LzZCkWfmNb8IcZpdreaNIxtJ8GQQ-b7g/view?usp=sharing](https://drive.google.com/file/d/1LzZCkWfmNb8IcZpdreaNIxtJ8GQQ-b7g/view?usp=sharing). It is 385 Mb, and it has an SHA1 hash (from the Unix `shasum` utility) of `d330a28120713fb05c95d1bf54342944f5d741c9`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7639#issuecomment-1014196799:432,hash,hash,432,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7639#issuecomment-1014196799,1,['hash'],['hash']
Security,"In ADAM, we have well maintained Hadoop InputFormats for both normal and interleaved FASTQ. Additionally, we wrap these InputFormats in Spark-friendly APIs (exposed in Scala, Java, Python, and R) that add validation and standard transformations. GATK already has a dependency on ADAM, so...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4612#issuecomment-405114805:157,expose,exposed,157,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4612#issuecomment-405114805,2,"['expose', 'validat']","['exposed', 'validation']"
Security,"In fact, setting the deploy-mode works with manual jobs as we get logs in our Hadoop monitor ( the tool to monitor the jobs on the spark cluster ) and directly on our console if deploy-mode is not set / set to client. Both `--deploy-mode` and `--conf 'spark.submit.deployMode=cluster'`. But with GATK, logs appear directly on my console and not in the Hadoop monitor even if we set with `--conf 'spark.submit.deployMode=cluster`. The other methods `--deploy-mode` and `-- --deploy-mode` having the said problems.; About the `-- --deploy-mode` and the JNI linkage error, I'm currently checking this.; All our Spark nodes have access to the mapr libraries from `/opt/mapr/...`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3933#issuecomment-350676916:625,access,access,625,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3933#issuecomment-350676916,1,['access'],['access']
Security,"In gatk3 we had mechanism for multithreading in gatk, but they made the tools very complicated and didn't provide enough speed up to be worthwhile in most cases. In gatk 4 we made the decision to write the tools as single threaded. We have a separate implementation of some tools using spark which is a parallelization library. . If you want to parallelize the HaplotypeCaller you can shard the input and run multiple copies of the tool on different input shards. That's how we do it in production. Alternatively you can try out the beta tool HaplotypeCallerSpark which is natively parallel but is still being developed. It hasn't been validated and may not produce results that are as accurate as the regular HaplotypeCaller.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6117#issuecomment-524554040:636,validat,validated,636,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6117#issuecomment-524554040,1,['validat'],['validated']
Security,"In looking at his further, the container header contains a stream offset, and each slice header also contains a global record counter. Both of these need to be updated. Its not clear if its possible to repair these without re-encoding the entire container stream, but if so that should probably be done in a method exposed by htsjdk.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2201#issuecomment-324756574:315,expose,exposed,315,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2201#issuecomment-324756574,1,['expose'],['exposed']
Security,"Interesting, it's definitely possible it's coming from one of the other buckets. I don't think we have fine grained control over WHICH bucket we attempt to read requester pays status from, so it's possible if it's enabled it's necessary to have that permission on every bucket. It's annoying that the error message doesn't say which reader is performing the access. Is there a longer stack trace available?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7492#issuecomment-934908586:358,access,access,358,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7492#issuecomment-934908586,1,['access'],['access']
Security,"Interesting, when I run locally I get:. ```; Gradle suite > Gradle test > org.broadinstitute.hellbender.tools.funcotator.FuncotatorIntegrationTest > nonTrivialLargeDataValidationTest[3](/Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/funcotator/validationTestData/regressionTestHg19Large.vcf, /Users/louisb/Workspace/gatk/src/test/resources/large/Homo_sapiens_assembly19.fasta.gz, hg19, /Users/louisb/Workspace/gatk/src/test/resources/large/funcotator/funcotator_dataSources/, /Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/funcotator/validationTestData/regressionTestHg19Large_expected.vcf) FAILED; java.lang.UnsatisfiedLinkError: 'void org.sqlite.core.NativeDB._open_utf8(byte[], int)'; at org.sqlite.core.NativeDB._open_utf8(Native Method); at org.sqlite.core.NativeDB._open(NativeDB.java:71); at org.sqlite.core.DB.open(DB.java:174); at org.sqlite.core.CoreConnection.open(CoreConnection.java:220); at org.sqlite.core.CoreConnection.<init>(CoreConnection.java:76); at org.sqlite.jdbc3.JDBC3Connection.<init>(JDBC3Connection.java:25); at org.sqlite.jdbc4.JDBC4Connection.<init>(JDBC4Connection.java:24); at org.sqlite.SQLiteConnection.<init>(SQLiteConnection.java:45); at org.sqlite.JDBC.createConnection(JDBC.java:114); at org.sqlite.JDBC.connect(JDBC.java:88); at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:677); at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:189); at org.broadinstitute.hellbender.tools.funcotator.dataSources.cosmic.CosmicFuncotationFactory.<init>(CosmicFuncotationFactory.java:161); at org.broadinstitute.hellbender.tools.funcotator.dataSources.DataSourceUtils.createCosmicDataSource(DataSourceUtils.java:469); at org.broadinstitute.hellbender.tools.funcotator.dataSources.DataSourceUtils.createDataSourceFuncotationFactoriesForDataSources(DataSourceUtils.java:289); at org.broadinstitute.hellbender.tools.funcotator.Funcotator.onTraversalStart(Funcotator.java",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6119#issuecomment-532715444:281,validat,validationTestData,281,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6119#issuecomment-532715444,2,['validat'],['validationTestData']
Security,Is there any update for this issue? I'm asking as AWS moved to NIO v2 late 2018 and htslib supports direct S3 access.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3708#issuecomment-628392999:110,access,access,110,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3708#issuecomment-628392999,1,['access'],['access']
Security,"Is there some discussion about why this is necessary? This doesn't seem like the right place to expose this to me. Wouldn't this make more sense being set, for example, on the CLI with the `spark-submit` command?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1066#issuecomment-152359731:96,expose,expose,96,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1066#issuecomment-152359731,1,['expose'],['expose']
Security,"It happened multiple times over the course of a couple days. Since I; downloaded the full gnomad exome data locally, I haven't tested again. --; - Alan Hoyle - alan@alanhoyle.com - http://www.alanhoyle.com/ -. On Mon, Nov 9, 2020 at 2:23 PM droazen <notifications@github.com> wrote:. > @alanhoyle <https://github.com/alanhoyle> Can you tell us whether the 400; > Bad Request error is repeatable -- did you see it more than once?; > Oftentimes when accessing cloud data we encounter transient errors like; > this that go away on their own.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/6926#issuecomment-724225557>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AACGX43OY2CFF5KFZXZOH4LSPA6T3ANCNFSM4TD2FDGA>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6926#issuecomment-724267156:448,access,accessing,448,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6926#issuecomment-724267156,1,['access'],['accessing']
Security,"It looks like the noGenotypes.vcf in GATK3 was derived from the v37 ref with decoy. In GATK4, the sequence dictionary validation requirement is that the input and reference have a common subset of contigs, so given your description of the difference I would expect it to pass that test. Feel free to reopen if you think I've missed something.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4453#issuecomment-410724975:118,validat,validation,118,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4453#issuecomment-410724975,1,['validat'],['validation']
Security,It looks like we need to update mockito. ; https://storage.googleapis.com/hellbender-test-logs/build_reports/master_27538.13/tests/test/index.html. ```. java.lang.IllegalArgumentException: Unknown Java version: 11; 	at net.bytebuddy.ClassFileVersion.ofJavaVersion(ClassFileVersion.java:135); 	at net.bytebuddy.ClassFileVersion$VersionLocator$ForJava9CapableVm.locate(ClassFileVersion.java:357); 	at net.bytebuddy.ClassFileVersion.ofThisVm(ClassFileVersion.java:147); 	at net.bytebuddy.dynamic.loading.ClassInjector$UsingReflection$Dispatcher$CreationAction.run(ClassInjector.java:301); 	at net.bytebuddy.dynamic.loading.ClassInjector$UsingReflection$Dispatcher$CreationAction.run(ClassInjector.java:290); 	at java.base/java.security.AccessController.doPrivileged(Native Method); 	at net.bytebuddy.dynamic.loading.ClassInjector$UsingReflection.<clinit>(ClassInjector.java:70); 	at net.bytebuddy.dynamic.loading.ClassLoadingStrategy$Default$InjectionDispatcher.load(ClassLoadingStrategy.java:184); 	at net.bytebuddy.dynamic.TypeResolutionStrategy$Passive.initialize(TypeResolutionStrategy.java:79); 	at net.bytebuddy.dynamic.DynamicType$Default$Unloaded.load(DynamicType.java:4456); 	at org.mockito.internal.creation.bytebuddy.SubclassBytecodeGenerator.mockClass(SubclassBytecodeGenerator.java:115); 	at org.mockito.internal.creation.bytebuddy.TypeCachingBytecodeGenerator$1.call(TypeCachingBytecodeGenerator.java:37); 	at org.mockito.internal.creation.bytebuddy.TypeCachingBytecodeGenerator$1.call(TypeCachingBytecodeGenerator.java:34); 	at net.bytebuddy.TypeCache.findOrInsert(TypeCache.java:138); 	at net.bytebuddy.TypeCache$WithInlineExpunction.findOrInsert(TypeCache.java:346); 	at net.bytebuddy.TypeCache.findOrInsert(TypeCache.java:161); 	at net.bytebuddy.TypeCache$WithInlineExpunction.findOrInsert(TypeCache.java:355); 	at org.mockito.internal.creation.bytebuddy.TypeCachingBytecodeGenerator.mockClass(TypeCachingBytecodeGenerator.java:32); 	at org.mockito.internal.creation.bytebuddy.SubclassB,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6119#issuecomment-532377836:724,secur,security,724,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6119#issuecomment-532377836,3,"['Access', 'Inject', 'secur']","['AccessController', 'InjectionDispatcher', 'security']"
Security,"It seems like travis isn't really testing this properly, it's hard to prove that it's not using the default credentials. Would it make sense to do the following?; 1. create a new gcloud project; 2. create a private file in a bucket only accessible by that account ; 3. add a service account json for that account to travis, and use it in this test.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2879#issuecomment-330339002:237,access,accessible,237,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2879#issuecomment-330339002,1,['access'],['accessible']
Security,"It shouldn't take any extra work to merge most external requests once things are setup properly. The issue is that certain dataflow operations a key/password to access our google account and we can't publish those as part of the repo, so we use travis.ci's encrypt mechanism to make them available to the build without making them public. On external pull requests, travis doesn't make the keys available, because it would possible to access them with a malicious pull request. . Currently the things that rely on authentication aren't running anyway in travis, but the key was still being decrypted and failing on external requests I've removed that step for now, and will be more careful to ensure that builds will continue even if the key is missing when we re-enable it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/478#issuecomment-98215051:149,password,password,149,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/478#issuecomment-98215051,5,"['access', 'authenticat', 'encrypt', 'password']","['access', 'authentication', 'encrypt', 'password']"
Security,"It turns out we were wrong and there was already a knob exposed for this. I gave the user some directions on increasing the correct buffer size, and they have accepted that answer. . We can probably go ahead and close this issue.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6150#issuecomment-532297743:56,expose,exposed,56,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6150#issuecomment-532297743,1,['expose'],['exposed']
Security,"It would be great if the jbwa also exposes a `bwa index` option, as we might need to map reads back to the assembled contigs (there will be many of them) for the genotyping step in SV pipeline, which in turn requires the contigs themselves to be indexed.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1857#issuecomment-227504606:35,expose,exposes,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1857#issuecomment-227504606,1,['expose'],['exposes']
Security,"It would be nice to have a StrandedInterval class, and then the distal targets could be stored as one of these in the various BreakpointEvidence subclasses that have distal targets, and the PairedStrandedIntervals class could just have two of them. This would also let you eliminate the methods hasDistalTargets and getDistalTargetsUpstreamOfBreakpoints -- getDistalTargets would do the job of all three.; I assume that PairedStrandedIntervals shouldn't ever have null intervals for source or target, so you could just validate in the constructor and then you wouldn't have to test nullness in equals and hashCode.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3469#issuecomment-328935711:519,validat,validate,519,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3469#issuecomment-328935711,2,"['hash', 'validat']","['hashCode', 'validate']"
Security,"It would be nice to have a few Picard tools that we feature in BPWs (e.g. MarkDuplicates) show both syntaxes:. 1. java -jar picard.jar ValidateSamFile I=reads.bam MODE=SUMMARY; 2. gatk ValidateSamFile -I reads.bam -MODE SUMMARY. I assume MODE gets one dash and not two, but really I don't know. So having such examples scattered throughout the tool chest for most often used tools would be great. The second example could be prefaced with, e.g.; > Picard tools are callable from GATK4. The equivalent command to the above invoked via the GATK launch script would be:",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-349522774:135,Validat,ValidateSamFile,135,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-349522774,2,['Validat'],['ValidateSamFile']
Security,"It's a feature, we're so committed to open source that we don't allow it to be made private through encryption.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4718#issuecomment-385800585:100,encrypt,encryption,100,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4718#issuecomment-385800585,1,['encrypt'],['encryption']
Security,"It's hard to say what is going on here exactly...but a couple of clarifying questions here. You stated that importing to EBS was roughly 2x faster than Lustre (~1hr vs ~2hrs). Is that right? Are the vcfs on Lustre in both cases?. We're not sure exactly what is going on here...generally we do see some slowdown on shared filesystems compared to just private mounted disks. Do you have access to any CloudWatch metrics or the like? I'm not entirely sure what to look for there, but looking at IOPs over time, or transactions per second, read/write transaction size, etc might help show something interesting.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7646#issuecomment-1044815768:385,access,access,385,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7646#issuecomment-1044815768,1,['access'],['access']
Security,"It's not a surprise that a local disk is faster than a remote one, but the; magnitude of the difference is a lot more than I would expect. I remember; at the time using direct GCS access to get the best possible performance in; the bit I was working on, but I don't remember exactly how much of a; difference it made. From my desktop it takes 2m25s to download the whole file, so the ~6min; difference seems really excessive, something is broken. One thing to look; into is whether the sharding is working correctly (are we getting the; correct number of parallel downloads?). Presumably this code is using the HDFS adapter. It'll be interesting to; compare vs the NIO version (and then the optimized NIO version once I write; it).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1755#issuecomment-213094600:180,access,access,180,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1755#issuecomment-213094600,1,['access'],['access']
Security,"It's not clear to me that we want these tools in Gatk4. We deliberately didn't port them because we felt they were unnecessary going forward. . I understand that there are some legitimate use cases that require them: ex low coverage naive variant calling from high ploidy pools which haplotype caller would do poorly on. (Also, do we know that haplotype caller doesn't do well on those sorts of things? Maybe we should consider modifications there if it doesn't?) I'm not sure that supporting that use case is worth the added complexity of maintaining and supporting these tools. Especially since we don't provide a pileup based variant caller as part of gatk4... . @vdauwera Can you comment? . @sooheelee I'm not sure I agree with you that supporting this for mutect 1 is useful. ; A) We don't want to support the use of mutect 1 anymore and would like to encourage people to switch to mutect 2 which I think we now believe is a better variant caller for both snps and indels. ; B) Mutect 1 users are already using gatk3, so they have access to these tools already. Mutect 1 also requires co-cleaning which I believe is a different but related tool to indel realignment. . For the variant review issue, we have thoughts on implementing a much better solution for variant review by creating an assembly plugin for igv.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3104#issuecomment-308451988:1036,access,access,1036,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3104#issuecomment-308451988,1,['access'],['access']
Security,"It's true that our measurements have shown some improvement even in a random access case. Surely we should be able to fabricate a more extreme case where prefetching doesn't help, so it still makes sense to offer a choice.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5882#issuecomment-482689496:77,access,access,77,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5882#issuecomment-482689496,1,['access'],['access']
Security,It's weird that it worked before though if roles aren't set up right. It seems like security issues shouldn't be solved by asking people to upgrade their client software so that it can deny them permission.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3591#issuecomment-330940018:84,secur,security,84,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3591#issuecomment-330940018,1,['secur'],['security']
Security,Its use in `ValidateBasicSomaticShortMutations` seems limited to the integration test. Can I rewrite the test to do without `AnnotatedInterval` and call it a day?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3884#issuecomment-526876913:12,Validat,ValidateBasicSomaticShortMutations,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3884#issuecomment-526876913,1,['Validat'],['ValidateBasicSomaticShortMutations']
Security,"I’ll take a look at the somatic tests. They should be OK, probably just something related to kebab case changes. EDIT: Or hmm, maybe they weren't passing before. Something to do with annotated-interval validation, I think. I think the WDL tests should be using the Docker, which has g++. Travis machines might be slower?. Integration tests will need to be in the python test group. Take a look at the python tests.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-350160424:202,validat,validation,202,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-350160424,1,['validat'],['validation']
Security,"I’m sorry I wasn’t really keeping a good trail of the errors and it was a few weeks ago and I have been doing other stuff. . . I have a notation in my log about the run time switch and I seem to have gotten the notion about using it from reading stuff in one of the tutorials or on the forum. I gotta admit I am clueless about what the dictionary validation is actually doing so it wouldn’t have been anything that I conjured up on my own. . . From: Louis Bergelson <notifications@github.com> ; Sent: Wednesday, October 30, 2019 4:13 PM; To: broadinstitute/gatk <gatk@noreply.github.com>; Cc: rdbremel <rdbremel017@gmail.com>; Mention <mention@noreply.github.com>; Subject: Re: [broadinstitute/gatk] Funcotator shuts down (#6182). . Can you point out where in the log you see that? I'm looking at it but I don't see anything about memory in the log you provided. (Except the Runtime.totalMemory()=4523032576 which is just standard output spam from gatk when it shutsdown) Sequence dictionary validation usually happens first, it's strange that a failure in the middle of a run would be effected by it. I'm no very curious what weird thing is happening that's causing this... —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub <https://github.com/broadinstitute/gatk/issues/6182?email_source=notifications&email_token=ANCR2VFFO5775FSQO6EHFX3QRH2HHA5CNFSM4I2MRFQKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOECVZCVA#issuecomment-548114772> , or unsubscribe <https://github.com/notifications/unsubscribe-auth/ANCR2VA3C2XW4BZEI5YNGITQRH2HHANCNFSM4I2MRFQA> . <https://github.com/notifications/beacon/ANCR2VFOYRDVUGP66IIIVHDQRH2HHA5CNFSM4I2MRFQKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOECVZCVA.gif>",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6182#issuecomment-548119939:347,validat,validation,347,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6182#issuecomment-548119939,2,['validat'],['validation']
Security,JlbmRlci90b29scy9zcGFyay9BcHBseUJRU1JTcGFyay5qYXZh) | `50% <0%> (-50%)` | `3% <0%> (ø)` | |; | [...ender/tools/spark/pipelines/BQSRPipelineSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4087/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvQlFTUlBpcGVsaW5lU3BhcmsuamF2YQ==) | `55% <0%> (-45%)` | `5% <0%> (-3%)` | |; | [...ute/hellbender/tools/walkers/UnmarkDuplicates.java](https://codecov.io/gh/broadinstitute/gatk/pull/4087/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL1VubWFya0R1cGxpY2F0ZXMuamF2YQ==) | `45% <0%> (-45%)` | `5% <0%> (ø)` | |; | [...itute/hellbender/tools/walkers/bqsr/ApplyBQSR.java](https://codecov.io/gh/broadinstitute/gatk/pull/4087/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Jxc3IvQXBwbHlCUVNSLmphdmE=) | `52.381% <0%> (-39.286%)` | `5% <0%> (-1%)` | |; | [.../tools/walkers/validation/CountFalsePositives.java](https://codecov.io/gh/broadinstitute/gatk/pull/4087/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vQ291bnRGYWxzZVBvc2l0aXZlcy5qYXZh) | `56.863% <0%> (-36.686%)` | `4% <0%> (-3%)` | |; | [...ender/engine/MultiVariantWalkerGroupedOnStart.java](https://codecov.io/gh/broadinstitute/gatk/pull/4087/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvTXVsdGlWYXJpYW50V2Fsa2VyR3JvdXBlZE9uU3RhcnQuamF2YQ==) | `62% <0%> (-34.875%)` | `14% <0%> (-6%)` | |; | [...nder/tools/spark/pipelines/ReadsPipelineSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4087/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvUmVhZHNQaXBlbGluZVNwYXJrLmphdmE=) | `54.545% <0%> (-34.816%)` | `10% <0%> (-2%)` | |; | [...lbender/tools/spark/pipelines/CountReadsSpark.java](https://codecov.io/gh/broadinstitute/gatk/pul,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4087#issuecomment-356051662:2695,validat,validation,2695,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4087#issuecomment-356051662,1,['validat'],['validation']
Security,"Just it case, I want to add that ""origin/master)"" kind of outcome is just a possibility ... sometimes you may get a line like ```* (HEAD detached at 18ed19)``` and then it would be ```18ed19)```. So please refrain of trying to handle the detached ```origin/whatever``` case and rather provide a solution that works with any ""garbage"" that the ```git batch --contains HASH``` command may come out with. I would say that unless the output lines is a standard looking branch name (e.g perl regex: ```/\s*([a-zA-Z_0-9]*)\s*/```) then just omit this part of the name or fill it with something constant like a ""UNKNOWN"" or ""DETACHED"" or ""HEAD"" or ""SNAPSHOT""....",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3642#issuecomment-333904259:367,HASH,HASH,367,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3642#issuecomment-333904259,1,['HASH'],['HASH']
Security,"Just noting, as we discussed, this could allow us to condense coverage files, etc. We'd want to make sure that bins are always validated and that we don't introduce use cases that corrupt the bins (e.g., filtering bins). Also not really sure how hard indexing would be. Should be in conjunction with #4717. We are probably OK shuffling around relatively large interval and count files for the time being, but we can adjust priority when it makes sense (also depending on where htsjdk development is on these issues).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5702#issuecomment-466203330:127,validat,validated,127,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5702#issuecomment-466203330,1,['validat'],['validated']
Security,"Just to add - GenomicsDB relies on the filesystem for integrity checks just like any other file on the filesystem. We could add integrity checks at a micro chunk-level, but that would be at the expense of performance.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6688#issuecomment-713055866:54,integrity,integrity,54,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6688#issuecomment-713055866,2,['integrity'],['integrity']
Security,"Just to make sure I understand the issue---will this cause technical problems in the Firecloud environment, or is it more of a style issue?. If the latter, one reason I prefer the use of optional file inputs to trigger tool-level ""modes"" when possible is that it propagates more naturally from the tool level. For example, let's consider a tool that can operate in either tumor-only or matched-pair mode. It is natural at the tool level to make the tumor a required input and the normal optional. The other options are quite awkward: 1) make both inputs required and switch between using the normal or not with a flag (in which case it is very easy for the user to shoot themselves in the foot if they forget to set the flag right, and we'd have to pass a dummy normal every time we want to run tumor only if we don't actually have a pair), 2) leave the normal as optional but add a flag anyway, which would be redundant and require an additional validation (i.e., if the flag is set to matched mode but we don't have a normal, we should fail early), or 3) write separate tools for each mode with the corresponding required inputs. If we accept that optional file input is the way to handle such a scenario at the tool level but not at the workflow level, then we will simply run into the same problems at the workflow level. I'm sure there are more complex scenarios when triggering on file presence/absence doesn't uniquely specify a workflow, in which case flags are a must. But for simple scenarios, I'm not sure why we shouldn't take advantage of the ability to specify optional file inputs in WDL (actually, I'm not sure how else we are supposed to use them?). However, if this is a problem for Firecloud, then I'd like to understand why---and what possible solutions there might be.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3657#issuecomment-334046444:947,validat,validation,947,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3657#issuecomment-334046444,1,['validat'],['validation']
Security,Kryo issue tracking this here is here https://github.com/EsotericSoftware/kryo/issues/382. Temporary fix is to run with the following set:. ```; spark.executor.extraJavaOptions –XX:hashCode=0; spark.driver.extraJavaOptions –XX:hashCode=0; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1524#issuecomment-189368808:181,hash,hashCode,181,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1524#issuecomment-189368808,2,['hash'],['hashCode']
Security,"Let me know if you want more methods exposed, or need any help in general.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2072#issuecomment-236905536:37,expose,exposed,37,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2072#issuecomment-236905536,1,['expose'],['exposed']
Security,"Let's discuss further before you get too far along. The design of the Collections code was intended to ensure that very strict file formats are adhered to within the CNV pipeline. Making it more flexible to accommodate TSVs with arbitrary column headers, relax requirements for sequence dictionaries, etc. undermines that goal. There are also two other issues to consider:. 1) It looks like @jonn-smith has also been putting considerable effort into building a TSV framework for Funcotator. Perhaps CombineSegmentBreakpoints should consider using that framework instead, if it is more appropriate. We can also discuss bringing the CNV pipeline over into that framework, but this should definitely wait until after release. The end goal is for CNV team to spend as little time as possible writing or maintaining any code related to TSV parsing. 2) @mbabadi has put together some python evaluation code for the new gCNV, which makes use of the IntervalTree python package and PyVCF to accomplish some things that are very similar to what CombineSegmentBreakpoints is doing. Perhaps we could implement a similar approach purely in Java by making use of the IntervalTree implementation in htsjdk. I think for now we should treat CombineSegmentBreakpoints as a one-off tool to be used for internal validations. After release, we should design a more generic evaluation tool. This tool could take as input multiple collections of annotated locatables, with a few rigidly defined formats allowed (e.g., VCF, CNV Collection TSVs, perhaps some TSVs from other tools, etc.), with one designated as ground truth. The regions for evaluation could also be specified via -L (since it is possible this might not completely specified by the ground-truth collection). The appropriate intersections and lookups could then be performed.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3995#issuecomment-352860616:1293,validat,validations,1293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3995#issuecomment-352860616,1,['validat'],['validations']
Security,"Let's talk about groupReadPairs, pairedReads, and unpairedReads. The groupBy method called by groupReadPairs is very expensive both in time and in memory. It's a full hash shuffle of GATKReads (time expensive), that results in a gazillion 1- and 2-element Lists (memory expensive). So you certainly don't want to do it twice. But the way pairedReads and unpairedReads is set up, you *will* do it twice if you want to process both paired and unpaired reads. (And even if you aren't, someone else might try to use this code to do so.). So my first suggestion is that you remove the call to groupReadPairs from pairedReads and unpairedReads, and let a user groupReadPairs once, and reuse the resulting JavaPairRDD to process paired and unpaired reads. My second suggestion is quite a bit more complicated, but I think it would result in far better performance. I'll sketch it out here, and then I can explain it further in person, if it's a direction you'd like to pursue. The first step is to create a JavaRDD<GATKRead> in which all pairs sharing a template name are in the same partition (but without grouping them). To do that, you temporarily boost the input JavaRDD into a JavaPairRDD<String,GATKRead> by extracting the read name as a key. Then you repartition (to do the shuffle). Then you map back to an ordinary JavaRDD<GATKRead> by keeping just the value. (Note: if the BAM has queryname sort order, you can just skip this step entirely.); Now you can do a mapPartition operation to filter for paired or unpaired reads: Iterate over the reads in the partition, and keep a hash map of [name -> read] of reads that have not yet found mates. To filter for paired reads, whenever you find the name of the current read already in the table, just emit the current read and the read in the map as a pair, and delete the read from the map (you're done with that name -- this keeps the table smaller). To filter for unpaired reads, just delete any map entry that you successfully look up, and insert any ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2664#issuecomment-299955039:167,hash,hash,167,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2664#issuecomment-299955039,1,['hash'],['hash']
Security,"Looks like the problem occurs when the cram dictionary is ""compatible"" with the reference dictionary according to our validation routines, but the reference dictionary does not contain all contigs from the cram dictionary. We are going to make our dictionary validation stricter to prevent this (https://github.com/broadinstitute/hellbender/issues/802). We should also patch htsjdk to throw a nicer error than `NullPointerException` in this case -- ie., insert a check after `CRAMIterator` line 158 that throws a `RuntimeException` with a nice ""contig from cram header not found in reference"" message if refs == null. Once the above patch goes into htsjdk we can close this ticket and focus on the validation issue in https://github.com/broadinstitute/hellbender/issues/802",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/793#issuecomment-130344138:118,validat,validation,118,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/793#issuecomment-130344138,3,['validat'],['validation']
Security,"M 1.8.0_25-b17; Version: Version:4.pre-alpha-210-gd7179f7-SNAPSHOT JdkDeflater; 07:07:56.668 INFO PrintReads - Initializing engine; 07:07:56.815 INFO PrintReads - Shutting down engine; [December 5, 2015 7:07:56 AM EST] org.broadinstitute.hellbender.tools.PrintReads done. Elapsed time: 0.00 minutes.; Runtime.totalMemory()=257425408; ***********************************************************************. A USER ERROR has occurred: Input files reference and reads have incompatible contigs: Found contigs with the same name but different lengths: ; contig reference = 1 / 1000000; contig reads = 1 / 249250621.; reference contigs = [1]; reads contigs = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, X, Y, MT, GL000207.1, GL000226.1, GL000229.1, GL000231.1, GL000210.1, GL000239.1, GL000235.1, GL000201.1, GL000247.1, GL000245.1, GL000197.1, GL000203.1, GL000246.1, GL000249.1, GL000196.1, GL000248.1, GL000244.1, GL000238.1, GL000202.1, GL000234.1, GL000232.1, GL000206.1, GL000240.1, GL000236.1, GL000241.1, GL000243.1, GL000242.1, GL000230.1, GL000237.1, GL000233.1, GL000204.1, GL000198.1, GL000208.1, GL000191.1, GL000227.1, GL000228.1, GL000214.1, GL000221.1, GL000209.1, GL000218.1, GL000220.1, GL000213.1, GL000211.1, GL000199.1, GL000217.1, GL000216.1, GL000215.1, GL000205.1, GL000219.1, GL000224.1, GL000223.1, GL000195.1, GL000212.1, GL000222.1, GL000200.1, GL000193.1, GL000194.1, GL000225.1, GL000192.1, NC_007605]. ***********************************************************************; ```. I think you meant ""the Picard version of this metric does not complain"", which is expected given that Picard tools do not do the same automatic sequence dictionary validation as walkers and spark tools. If you think they should, you should create a different ticket ""Multi-input Picard tools and metrics should perform sequence dictionary validation on their inputs"" or file a specific bug against the Picard version of `CollectQualityYieldMetrics`. Closing",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1262#issuecomment-162176558:2773,validat,validation,2773,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1262#issuecomment-162176558,2,['validat'],['validation']
Security,"MProxy: Connecting to ResourceManager at mg/10.131.101.159:8032; 17/10/13 18:11:36 INFO yarn.Client: Requesting a new application from cluster with 3 NodeManagers; 17/10/13 18:11:36 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (164726 MB per container); 17/10/13 18:11:36 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead; 17/10/13 18:11:36 INFO yarn.Client: Setting up container launch context for our AM; 17/10/13 18:11:36 INFO yarn.Client: Setting up the launch environment for our AM container; 17/10/13 18:11:36 INFO yarn.Client: Preparing resources for our AM container; 17/10/13 18:11:37 INFO yarn.Client: Uploading resource file:/tmp/hdfs/spark-c7e5eece-205e-4bce-a69b-4168c9b79045/__spark_conf__2918234914787361986.zip -> hdfs://mg:8020/user/hdfs/.sparkStaging/application_1507856833944_0003/__spark_conf__.zip; 17/10/13 18:11:37 INFO spark.SecurityManager: Changing view acls to: hdfs; 17/10/13 18:11:37 INFO spark.SecurityManager: Changing modify acls to: hdfs; 17/10/13 18:11:37 INFO spark.SecurityManager: Changing view acls groups to: ; 17/10/13 18:11:37 INFO spark.SecurityManager: Changing modify acls groups to: ; 17/10/13 18:11:37 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hdfs); groups with view permissions: Set(); users with modify permissions: Set(hdfs); groups with modify permissions: Set(); 17/10/13 18:11:37 INFO yarn.Client: Submitting application application_1507856833944_0003 to ResourceManager; 17/10/13 18:11:37 INFO impl.YarnClientImpl: Submitted application application_1507856833944_0003; 17/10/13 18:11:37 INFO cluster.SchedulerExtensionServices: Starting Yarn extension services with app application_1507856833944_0003 and attemptId None; 17/10/13 18:11:38 INFO yarn.Client: Application report for application_1507856833944_0003 (state: ACCEPTED); 17/10/13 18:11:38 INFO y",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-336412775:10558,Secur,SecurityManager,10558,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-336412775,1,['Secur'],['SecurityManager']
Security,"Managers; 17/10/13 18:11:36 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (164726 MB per container); 17/10/13 18:11:36 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead; 17/10/13 18:11:36 INFO yarn.Client: Setting up container launch context for our AM; 17/10/13 18:11:36 INFO yarn.Client: Setting up the launch environment for our AM container; 17/10/13 18:11:36 INFO yarn.Client: Preparing resources for our AM container; 17/10/13 18:11:37 INFO yarn.Client: Uploading resource file:/tmp/hdfs/spark-c7e5eece-205e-4bce-a69b-4168c9b79045/__spark_conf__2918234914787361986.zip -> hdfs://mg:8020/user/hdfs/.sparkStaging/application_1507856833944_0003/__spark_conf__.zip; 17/10/13 18:11:37 INFO spark.SecurityManager: Changing view acls to: hdfs; 17/10/13 18:11:37 INFO spark.SecurityManager: Changing modify acls to: hdfs; 17/10/13 18:11:37 INFO spark.SecurityManager: Changing view acls groups to: ; 17/10/13 18:11:37 INFO spark.SecurityManager: Changing modify acls groups to: ; 17/10/13 18:11:37 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hdfs); groups with view permissions: Set(); users with modify permissions: Set(hdfs); groups with modify permissions: Set(); 17/10/13 18:11:37 INFO yarn.Client: Submitting application application_1507856833944_0003 to ResourceManager; 17/10/13 18:11:37 INFO impl.YarnClientImpl: Submitted application application_1507856833944_0003; 17/10/13 18:11:37 INFO cluster.SchedulerExtensionServices: Starting Yarn extension services with app application_1507856833944_0003 and attemptId None; 17/10/13 18:11:38 INFO yarn.Client: Application report for application_1507856833944_0003 (state: ACCEPTED); 17/10/13 18:11:38 INFO yarn.Client: ; 	 client token: N/A; 	 diagnostics: N/A; 	 ApplicationMaster host: N/A; 	 ApplicationMaster RPC port: -1; 	 queue: root.users.hdfs; 	 start",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-336412775:10710,Secur,SecurityManager,10710,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-336412775,1,['Secur'],['SecurityManager']
Security,"Maybe I misunderstand the underlying model, but if some Pedigree annotations only need to know which samples are founders (ExcessHet ?) , and some need to know the full relationships (PossibleDeNovo), then I'm suggesting we change the class hierarchy to reflect that:. PedigreeAnnotation; |--TrioAnnotation; |----PossibleDeNovo; |--ExcessHet (assuming ExcessHet only needs founders...); ... Then the plugin could deterministically validate whether the user has provided sufficient args for the set of requested annotations; and if so, propagate them accordingly. A TrioAnnotation could only be populated (from the command line at least) from a file, whereas the others could be populated from either a file or just a set of IDs. I think it would simplify the annotations.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5663#issuecomment-463372550:431,validat,validate,431,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5663#issuecomment-463372550,1,['validat'],['validate']
Security,"Merging this according to discussion with @LeeTL1220. We still need to investigate some possible regressions in the CRSP validation, but we should be good for prerelease on the CNV somatic side. There is one last branch (#4061) on the germline side, but I think we didn't plan on having gCNV ready for prerelease.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4046#issuecomment-355665197:121,validat,validation,121,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4046#issuecomment-355665197,1,['validat'],['validation']
Security,"Meta-comments for reviewers: the new program groups in this PR are based on @sooheelee 's spreadsheet, including some that are placeholders for things that will soon live in Picard, but aren't accessible from there yet. I've left the old program groups intact because they're still being referenced by tools. As the doc PRs are merged in, eventually these will be left dangling with no references, and then we'll remove them. In the meantime we'll have some redundancies (ReadProgramGroup will be replaced by ReadDataProgramGroup, or whatever we settle on).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3924#issuecomment-349722389:193,access,accessible,193,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3924#issuecomment-349722389,1,['access'],['accessible']
Security,"Model (BGMM) backend vs. python sklearn IsolationForest backend; (BGMM tests to be added once PR for the backend goes in.); - [x] Tool-level docs. Minor TODOs:. - [x] Parameter-level docs.; - [x] Parameter/mode validation.; - [x] Refactor main code block for model training; it's a bit monolithic and procedural now.; - [x] Decide on behavior for ill-behaved annotations. E.g., all missing, zero variance. Future work:. - [ ] We could allow subsetting of annotations here, which might allow for easier treatment of ill-behaved annotations. However, I'd say enabling workflows where the set of annotations is fixed is the priority.; - [ ] We could do positive-unlabeled training more rigorously or iteratively. Right now, we essentially do a single iteration to determine negative data. This could perhaps be preceded by a round of refactoring to clean up model training and make it less procedural.; - [ ] Automatic threshold tuning could be built into the tool, see #7711. We'd probably have to introduce a ""validation"" label. Perhaps it makes sense to keep this sort of thing at the workflow level?; - [ ] In the positive-negative framework enforced by the Java code in this tool, a ""model"" is anything that assigns a score, we fit two models to different subsets of the data, and then take the difference of the two scores. While the python backend does give some freedom to specify a model, future developers may want to go beyond the framework itself. For example, more traditional classification frameworks, etc. could be explored. As an intermediate step, one could perhaps use the positive/negative scores from the current framework in a more sophisticated way (e.g., using them as features), rather than just taking their difference. This sort of future work could be developed completely independently of the codebase associated with the current training tool (or done externally in python), but should still be able to make use of the extract and score tools, since the contracts should be",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7724#issuecomment-1067948369:1589,validat,validation,1589,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7724#issuecomment-1067948369,1,['validat'],['validation']
Security,Moot because @samuelklee replaced uses of `HashedListTargetCollection` with `OverlapDetector`.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1754#issuecomment-357482326:43,Hash,HashedListTargetCollection,43,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1754#issuecomment-357482326,1,['Hash'],['HashedListTargetCollection']
Security,"Most likely not. We've validated that we don't need it for DNA, it's hard to imagine that would be different for RNA.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1394#issuecomment-231498501:23,validat,validated,23,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1394#issuecomment-231498501,1,['validat'],['validated']
Security,"Most spark tools use the one in GATKSparkTool, but some have some special requirements that make it not work for them. They have to specify different sequence dictionaries or something like that in a way that isn't exposed. Maybe something could be refactored there, but they needed manually adjusting to match the new behavior because of their special handling of the writing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6458#issuecomment-594167389:215,expose,exposed,215,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6458#issuecomment-594167389,1,['expose'],['exposed']
Security,"Multiple causes can cause closed connections when reading from GCS, almost all of which are outside of our control. This will never be ""completely fixed"" in the sense that even if the code is perfect it's completely possible to send too many requests to GCS, and it'll respond by closing connections. The main factors that I know of are:. - number of concurrent accesses to the GCS bucket in question; - number of concurrent accesses to the GCP project in question; - storage class of the GCS bucket in question (the more expensive ones have more replicas, thus can handle a higher load). If you're running into those difficulties I would suggest trying to reduce the load (reduce the number of concurrent workers or threads) and making sure it's not a single-region storage bucket. If that fails, perhaps try using a different bucket that no one else is also using (to reduce other sources of load). If I understand correctly that you didn't change the version you're using but are suddenly seeing more issues than before, then perhaps the cause is a server-side change from GCS (outside of our control), a change in configuration (are you reading from a bucket of a different class from before), or perhaps just an increase of other activity on the same bucket/project. The current code is very persistent in its retries: as you can see from the messages it spent a whole half hour waiting. If it's an overload situation then you may get better performance by reducing the worker count (as they will have to retry less).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5631#issuecomment-526270716:362,access,accesses,362,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5631#issuecomment-526270716,2,['access'],['accesses']
Security,My branch only validates the lengths of some annotations of type list -- I didn't add any checks with respect to the spec.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6762#issuecomment-705185296:15,validat,validates,15,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6762#issuecomment-705185296,1,['validat'],['validates']
Security,My hypothesis is that the many accesses exhausts something along the path and that something as a result rejects all connections. Perhaps to serve a client they need a file handle and there's a limit of < 1e6 handles? . If we really need to open this many perhaps we should try to put together a minimal program that reproduces the problem and then submit that to cloud support.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-308586876:31,access,accesses,31,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-308586876,1,['access'],['accesses']
Security,"My problem is fixed with the first commit, because if `customCommandLineValidation()` throws an `UserException.CommandLineException` it is catched and printed (otherwise, it is ignore in `Main`, except for the returning status). Anyway, I added a commit with the implementation of the void method for all the tools. I guess that it is not a good idea after all, because it could help to print several errors. My first commit deal with the two situations, printing one/several errors if the `String[]` is not null, and if the validation throws an error catching it and handle in the same way as the ones in `CommandLineProgram`. I will rather go for the first and changing the printing of the errors in the array to the same place as the catched exceptions, and decorate it in the same way for not confusing the software user. What do you thing, @lbergelson?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2226#issuecomment-255856456:525,validat,validation,525,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2226#issuecomment-255856456,1,['validat'],['validation']
Security,"My recollection is that this is a use case we never put any priority on so there's no test in the GATK suite for access to private files. There should be, of course. The feature is there and (at least locally) it worked when I tried it. NIO does not use the API_KEY, it uses default credentials. Those are environment variables that are set by the `gcloud` command or pre-set for you in the case of virtual machines on Google. There are two cases: local execution and Spark. . I just tested local execution and it worked fine for me:. ```; $ ./gatk-launch PrintReads -L Broad.human.exome.b37.small.interval_list -I gs://jpmartin-private/bench/WGS-G94982-NA12878.bam -O t_gcs.bam; ```. this command worked even though (unless I'm mistaken) neither the bucket nor the file are public. One challenge however is that the way to set default credentials has changed recently. Calling `gcloud auth login` used to be enough but now we have to call (IIRC) `gcloud auth application-default login`. For Spark, the default credentials are set as whoever owns the dataproc environment that's used to run the show. So it should be set so it has access to the buckets necessary. NIO has mechanisms for accessing buckets that belong to someone other than who is running the Spark job, but they are not hooked into GATK yet.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2394#issuecomment-277832658:113,access,access,113,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2394#issuecomment-277832658,3,['access'],"['access', 'accessing']"
Security,"My suspicion was wrong. We also include a safety check which cause us to correctly reject most accidental matches. If we detect 2 chromosomes with the same name but different lengths we fail even if we detect otherwise matching chromosomes. I've run all the dictionaries I could find in the gatk bundle against each and only b37 and b37_decoy are compatible with each other which is the desired behavior I believe. | | hg18 | hg19 | b37 | b37_decoy | hg38 |; | -- |------|-----|------|-----------|-------|; | hg18 | ✅ | | | | |; | hg19 | | ✅ | | | |; | b37 | | | ✅ | ✅ | |; | b37_decoy | | | ✅ | ✅ | |; | hg38 | | | | | ✅ |. ```; @DataProvider; public Iterator<Object[]> getComparisons(){; final ArrayList<Object[]> comparisons = new ArrayList<>();; final List<String> dicts = Arrays.asList(""Homo_sapiens_assembly18.dict"",; ""ucsc.hg19.dict"",; ""human_b36_both.dict"",; ""human_g1k_v37.dict"",; ""human_g1k_v37_decoy.dict"",; ""Homo_sapiens_assembly38.dict"");; for( String left : dicts) {; for (String right: dicts){; Path leftDict =Paths.get(""/Users/louisb/Downloads/dicts"", left);; Path rightDict = Paths.get(""/Users/louisb/Downloads/dicts"", right);. comparisons.add( new Object[] {leftDict, rightDict});; }; }; return comparisons.iterator();; }. @Test(dataProvider = ""getComparisons""); public void testSequenceDictionariesAgainstEachother(Path left, Path right){; String leftName = left.getFileName().toString();; String rightName = right.getFileName().toString();; SequenceDictionaryUtils.validateDictionaries(leftName,; SAMSequenceDictionaryExtractor.extractDictionary(left),; rightName,; SAMSequenceDictionaryExtractor.extractDictionary(right));; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3754#issuecomment-494924193:1485,validat,validateDictionaries,1485,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3754#issuecomment-494924193,1,['validat'],['validateDictionaries']
Security,"NFO: Registering coder for VariantAnnotation; Jul 14, 2015 1:14:51 PM com.google.cloud.genomics.dataflow.utils.DataflowWorkarounds registerGenomicsCoders; INFO: Registering coder for SearchAnnotationSetsRequest; Jul 14, 2015 1:14:51 PM com.google.cloud.genomics.dataflow.utils.DataflowWorkarounds registerGenomicsCoders; INFO: Registering coder for RangePosition; Jul 14, 2015 1:14:51 PM com.google.cloud.genomics.dataflow.utils.DataflowWorkarounds registerGenomicsCoders; INFO: Registering coder for SearchJobsRequest; Jul 14, 2015 1:14:51 PM com.google.cloud.genomics.dataflow.utils.DataflowWorkarounds registerGenomicsCoders; INFO: Registering coder for ExportVariantSetRequest; 15/07/14 13:14:51 INFO spark.SparkPipelineRunner: Executing pipeline using the SparkPipelineRunner.; 15/07/14 13:14:51 INFO spark.SparkContext: Running Spark version 1.3.1; 15/07/14 13:14:51 INFO spark.SecurityManager: Changing view acls to: louisb; 15/07/14 13:14:51 INFO spark.SecurityManager: Changing modify acls to: louisb; 15/07/14 13:14:51 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(louisb); users with modify permissions: Set(louisb); 15/07/14 13:14:52 INFO slf4j.Slf4jLogger: Slf4jLogger started; 15/07/14 13:14:52 INFO Remoting: Starting remoting; 15/07/14 13:14:52 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@wm1b0-8ab.broadinstitute.org:65238]; 15/07/14 13:14:52 INFO util.Utils: Successfully started service 'sparkDriver' on port 65238.; 15/07/14 13:14:52 INFO spark.SparkEnv: Registering MapOutputTracker; 15/07/14 13:14:52 INFO spark.SparkEnv: Registering BlockManagerMaster; 15/07/14 13:14:52 INFO storage.DiskBlockManager: Created local directory at /var/folders/xt/vq7wz8955r1401mv8w0f4zf9qbfwzl/T/louisb/spark-7b286138-5fde-4fcb-bc34-3c6a86da6c0c/blockmgr-46eb69b0-d3ca-4004-bf32-ab103c0787f2; 15/07/14 13:14:52 INFO storage.MemoryStore: MemoryStore started with capacity 265.1 MB; 15/",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/639#issuecomment-121313713:16276,Secur,SecurityManager,16276,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/639#issuecomment-121313713,1,['Secur'],['SecurityManager']
Security,"Need some guidance here. The CompareSAMs tool was not propagating the validation stringency. I have a fix for that, but that alone doesn't fix the compareBAMFiles test in BaseRecalibrationIntegrationTest.java, since that uses SamAssertionUtils.assertSamsEqual, which also doesn't propagate (or accept) a validation stringency. Changing SamAssertionUtils to use either SILENT or LENIENT does fix the integration test, and all the other tests pass, but it seems like a relaxing of the stringency, and I'm not sure it should be necessary to the BQSR test. If relaxing the stringency for BQSR test _IS_ the right path, one possibility is to add a new method to SamAssertionUtils that accepts a validation stringency argument.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/419#issuecomment-109796266:70,validat,validation,70,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/419#issuecomment-109796266,3,['validat'],['validation']
Security,"No problem – I can see how this would be challenging to review – maybe it’s not even practical – if you decide it’s best just not to use it that’s OK - working on these issues has been a valuable learning experience for me. . I sent an archive with the new validations and the diffs between the old and new bams to akiezun. Although in many cases the files shared types of errors, I had to look at each file individually to take into account the particular errors in each file and how to fix them without (to the best of my knowledge) interfering with the purpose of the test. I did write a python script to use where necessary for converting multiple unpaired reads in a file to single reads, and I used bash scripts to call the picard tools to convert multiple files at a time from bam to sam for editing and then back again after they were modified. In some cases I had to modify the values in test output files to match the values produced by the test using the modified bams/sams, or just capture the new output files and use them to replace the old with the new. In two cases where file format errors appeared to be necessary but the filename did not indicate this, I renamed the files to make this clear. From: Louis Bergelson ; Sent: Thursday, August 20, 2015 2:13 PM; To: broadinstitute/hellbender ; Cc: nenewell ; Subject: Re: [hellbender] Issue 569 - bam and sam file cleanup. (#809). @nenewell Sorry this has been sitting. We've been trying to figure out how to review this one. Could you describe how you made the changes? Did you script it or go through by hand and modify them all?. —; Reply to this email directly or view it on GitHub.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/809#issuecomment-133123051:257,validat,validations,257,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/809#issuecomment-133123051,1,['validat'],['validations']
