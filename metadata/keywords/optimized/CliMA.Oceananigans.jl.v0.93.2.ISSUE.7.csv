quality_attribute,sentence,source,author,repo,version,id,keyword,matched_word,match_idx,wiki,url,total_similar,target_keywords,target_matched_words
Performance,":String) at /Users/truedichotomy/.julia/packages/GPUArrays/PkHCM/src/GPUArrays.jl:1; [5] top-level scope at /Users/truedichotomy/.julia/packages/GPUArrays/PkHCM/src/GPUArrays.jl:25; [6] include(::Function, ::Module, ::String) at ./Base.jl:380; [7] include(::Module, ::String) at ./Base.jl:368; [8] top-level scope at none:2; [9] eval at ./boot.jl:331 [inlined]; [10] eval(::Expr) at ./client.jl:467; [11] top-level scope at ./none:3; in expression starting at /Users/truedichotomy/.julia/packages/GPUArrays/PkHCM/src/host/abstractarray.jl:24; in expression starting at /Users/truedichotomy/.julia/packages/GPUArrays/PkHCM/src/GPUArrays.jl:25; ERROR: LoadError: Failed to precompile GPUArrays [0c68f7d7-f131-5f86-a1c3-88cf8149b2d7] to /Users/truedichotomy/.julia/compiled/v1.5/GPUArrays/v5u0T_IyCmP.ji.; Stacktrace:; [1] error(::String) at ./error.jl:33; [2] compilecache(::Base.PkgId, ::String) at ./loading.jl:1290; [3] _require(::Base.PkgId) at ./loading.jl:1030; [4] require(::Base.PkgId) at ./loading.jl:928; [5] require(::Module, ::Symbol) at ./loading.jl:923; [6] include(::Function, ::Module, ::String) at ./Base.jl:380; [7] include(::Module, ::String) at ./Base.jl:368; [8] top-level scope at none:2; [9] eval at ./boot.jl:331 [inlined]; [10] eval(::Expr) at ./client.jl:467; [11] top-level scope at ./none:3; in expression starting at /Users/truedichotomy/.julia/packages/CUDA/7vLVC/src/CUDA.jl:5; ERROR: LoadError: Failed to precompile CUDA [052768ef-5323-5732-b1bb-66c8b64840ba] to /Users/truedichotomy/.julia/compiled/v1.5/CUDA/oWw5k_IyCmP.ji.; Stacktrace:; [1] error(::String) at ./error.jl:33; [2] compilecache(::Base.PkgId, ::String) at ./loading.jl:1290; [3] _require(::Base.PkgId) at ./loading.jl:1030; [4] require(::Base.PkgId) at ./loading.jl:928; [5] require(::Module, ::Symbol) at ./loading.jl:923; [6] include(::Function, ::Module, ::String) at ./Base.jl:380; [7] include(::Module, ::String) at ./Base.jl:368; [8] top-level scope at none:2; [9] eval at ./boot.jl:331 [inlined]; [",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/854:1506,load,loading,1506,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/854,1,['load'],['loading']
Performance,"; > > ; > > ; > > But for one layer, WENO vector invariant plus WENO for mass flux may be sufficient, and we don't need any other dissipation (which is nice, less work for us).; > ; > I agree. If we try using whatever closure here that is used in the hydrostatic model, I would think that it should yield a similar result. But that's maybe better saved for a future PR?. Let me clarify. We've never run the hydrostatic model with a single layer on the sphere, but this is a good idea. Most of our runs have been with 50 vertical levels. In the 3D configuration with 50 vertical levels, we smooth results with 1) WENO5 vector invariant advection and 2) biharmonic ""divergence damping"" of the divergence component of the flow. If we are going to compare the shallow water on a sphere to they hydrostatic model, we will have to produce additional runs with just 1 vertical level. In other words, there are no results to compare with right now. When we perform additional runs, we may choose to include divergence damping, or another kind of closure, if we want to. The results presented in #2317 also use a single layer hydrostatic model, and these did not require any explicit dissipation. Therefore, there is reason to believe that divergence damping is only required for 3D simulations. An additional point is that in the shallow water equations we can introduce a WENO reconstruction of the mass flux. This reconstruction may provide divergence damping (whether or not we need it!). In summary, if we would like to perform a comparison, we can choose to either 1) produce a comparison between the shallow water model and one layer hydrostatic model with no explicit closure on the sphere or 2) run new 1 layer hydrostatic simulations with some closure (of our choosing) and then _also_ implement that closure in the shallow water model. My opinion is that with these nice WENO5 numerics there may not be a need for using explicit dissipation right now, so we might as well do the easy thing and avoid",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2522#issuecomment-1119959226:1057,perform,perform,1057,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2522#issuecomment-1119959226,1,['perform'],['perform']
Performance,; GPU Link Info; PCIe Generation; Max : 3; Current : 3; Device Current : 3; Device Max : 3; Host Max : 3; Link Width; Max : 16x; Current : 16x; Bridge Chip; Type : N/A; Firmware : N/A; Replays Since Reset : 0; Replay Number Rollovers : 0; Tx Throughput : 0 KB/s; Rx Throughput : 0 KB/s; Atomic Caps Inbound : N/A; Atomic Caps Outbound : N/A; Fan Speed : N/A; Performance State : P0; Clocks Throttle Reasons; Idle : Active; Applications Clocks Setting : Not Active; SW Power Cap : Not Active; HW Slowdown : Not Active; HW Thermal Slowdown : Not Active; HW Power Brake Slowdown : Not Active; Sync Boost : Not Active; SW Thermal Slowdown : Not Active; Display Clock Setting : Not Active; FB Memory Usage; Total : 32768 MiB; Reserved : 267 MiB; Used : 0 MiB; Free : 32500 MiB; BAR1 Memory Usage; Total : 32768 MiB; Used : 2 MiB; Free : 32766 MiB; Compute Mode : Default; Utilization; Gpu : 0 %; Memory : 0 %; Encoder : 0 %; Decoder : 0 %; Encoder Stats; Active Sessions : 0; Average FPS : 0; Average Latency : 0; FBC Stats; Active Sessions : 0; Average FPS : 0; Average Latency : 0; Ecc Mode; Current : Enabled; Pending : Enabled; ECC Errors; Volatile; Single Bit; Device Memory : 0; Register File : 0; L1 Cache : 0; L2 Cache : 0; Texture Memory : N/A; Texture Shared : N/A; CBU : N/A; Total : 0; Double Bit; Device Memory : 0; Register File : 0; L1 Cache : 0; L2 Cache : 0; Texture Memory : N/A; Texture Shared : N/A; CBU : 0; Total : 0; Aggregate; Single Bit; Device Memory : 0; Register File : 0; L1 Cache : 0; L2 Cache : 0; Texture Memory : N/A; Texture Shared : N/A; CBU : N/A; Total : 0; Double Bit; Device Memory : 0; Register File : 0; L1 Cache : 0; L2 Cache : 0; Texture Memory : N/A; Texture Shared : N/A; CBU : 0; Total : 0; Retired Pages; Single Bit ECC : 0; Double Bit ECC : 0; Pending Page Blacklist : No; Remapped Rows : N/A; Temperature; GPU Current Temp : 41 C; GPU Shutdown Temp : 90 C; GPU Slowdown Temp : 87 C; GPU Max Operating Temp : 83 C; GPU Target Temperature : N/A; Memory Curre,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2922#issuecomment-1437515895:21199,Latency,Latency,21199,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2922#issuecomment-1437515895,1,['Latency'],['Latency']
Performance,"; [20] #invoke_in_world#3; @ ./essentials.jl:926 [inlined]; [21] invoke_in_world; @ ./essentials.jl:923 [inlined]; [22] require(into::Module, mod::Symbol); @ Base ./loading.jl:1746; [23] include; @ ./Base.jl:495 [inlined]; [24] include_package_for_output(pkg::Base.PkgId, input::String, depot_path::Vector{String}, dl_load_path::Vector{String}, load_path::Vector{String}, concrete_deps::Vector{Pair{Base.PkgId, UInt128}}, source::String); @ Base ./loading.jl:2222; [25] top-level scope; @ stdin:3; in expression starting at /glade/u/home/knudsenl/.julia/packages/CUDA/Tl08O/src/CUDA.jl:1; in expression starting at stdin:3; ERROR: LoadError: Failed to precompile CUDA [052768ef-5323-5732-b1bb-66c8b64840ba] to ""/glade/u/home/knudsenl/.julia/compiled/v1.10/CUDA/jl_zRopeZ"".; Stacktrace:; [1] error(s::String); @ Base ./error.jl:35; [2] compilecache(pkg::Base.PkgId, path::String, internal_stderr::IO, internal_stdout::IO, keep_loaded_modules::Bool); @ Base ./loading.jl:2468; [3] compilecache; @ ./loading.jl:2340 [inlined]; [4] (::Base.var""#968#969""{Base.PkgId})(); @ Base ./loading.jl:1974; [5] mkpidlock(f::Base.var""#968#969""{Base.PkgId}, at::String, pid::Int32; kwopts::@Kwargs{stale_age::Int64, wait::Bool}); @ FileWatching.Pidfile /glade/u/apps/casper/23.10/spack/opt/spack/julia/1.10.2/gcc/7.5.0/apod/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:93; [6] #mkpidlock#6; @ /glade/u/apps/casper/23.10/spack/opt/spack/julia/1.10.2/gcc/7.5.0/apod/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:88 [inlined]; [7] trymkpidlock(::Function, ::Vararg{Any}; kwargs::@Kwargs{stale_age::Int64}); @ FileWatching.Pidfile /glade/u/apps/casper/23.10/spack/opt/spack/julia/1.10.2/gcc/7.5.0/apod/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:111; [8] #invokelatest#2; @ ./essentials.jl:894 [inlined]; [9] invokelatest; @ ./essentials.jl:889 [inlined]; [10] maybe_cachefile_lock(f::Base.var""#968#969""{Base.PkgId}, pkg::Base.PkgId, srcpath::String; stale_age::Int64); @ Base ./loading.jl:2983; [1",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2231635812:3299,load,loading,3299,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2231635812,1,['load'],['loading']
Performance,; cpu__advect_particles! at /home/alir/.julia/packages/KernelAbstractions/491pi/src/macros.jl:291 [inlined]; cpu__advect_particles! at ./none:0; __thread_run at /home/alir/.julia/packages/KernelAbstractions/491pi/src/cpu.jl:144; unknown function (ip: 0x7c0090512182); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; __run at /home/alir/.julia/packages/KernelAbstractions/491pi/src/cpu.jl:111; unknown function (ip: 0x7c009050feb3); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; #_#16 at /home/alir/.julia/packages/KernelAbstractions/491pi/src/cpu.jl:46; _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; jl_apply at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/julia.h:1982 [inlined]; do_apply at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/builtins.c:768; Kernel at /home/alir/.julia/packages/KernelAbstractions/491pi/src/cpu.jl:39; _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; advect_lagrangian_particles! at /home/alir/atdepth/Oceananigans.jl/src/Models/LagrangianParticleTracking/lagrangian_particle_advection.jl:193; step_lagrangian_particles! at /home/alir/atdepth/Oceananigans.jl/src/Models/LagrangianParticleTracking/LagrangianParticleTracking.jl:143 [inlined]; step_lagrangian_particles! at /home/alir/atdepth/Oceananigans.jl/src/Models/HydrostaticFreeSurfaceModels/HydrostaticFreeSurfaceModels.jl:107 [inlined]; #time_step!#8 at /h,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3852:2778,cache,cache,2778,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3852,1,['cache'],['cache']
Performance,"; | ErrorException(""could not load library ""libopenblas64_""; &nbsp; | libopenblas64_.so: ELF load command past end of file""); &nbsp; | ┌ Error: Error during initialization of module CHOLMOD; &nbsp; | │ exception =; &nbsp; | │ could not load library ""libcholmod""; &nbsp; | │ libopenblas64_.so.0: ELF load command past end of file; &nbsp; | │ Stacktrace:; &nbsp; | │ [1] dlopen(::String, ::UInt32; throw_error::Bool) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Libdl/src/Libdl.jl:109; &nbsp; | │ [2] dlopen at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Libdl/src/Libdl.jl:109 [inlined] (repeats 2 times); &nbsp; | │ [3] __init__() at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/SuiteSparse/src/cholmod.jl:90; &nbsp; | └ @ SuiteSparse.CHOLMOD /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/SuiteSparse/src/cholmod.jl:187; &nbsp; | /storage7/buildkite-agent/julia-1.5.4/bin/julia: error while loading shared libraries: libLLVM-9jl.so: ELF load command past end of file; &nbsp; | ERROR: LoadError: LoadError: IOError: write: broken pipe (EPIPE); &nbsp; | Stacktrace:; &nbsp; | [1] uv_write(::Base.PipeEndpoint, ::Ptr{UInt8}, ::UInt64) at ./stream.jl:951; &nbsp; | [2] unsafe_write(::Base.PipeEndpoint, ::Ptr{UInt8}, ::UInt64) at ./stream.jl:1005; &nbsp; | [3] write(::Base.PipeEndpoint, ::String) at ./strings/io.jl:183; &nbsp; | [4] create_expr_cache(::String, ::String, ::Array{Pair{Base.PkgId,UInt64},1}, ::Base.UUID) at ./loading.jl:1194; &nbsp; | [5] compilecache(::Base.PkgId, ::String) at ./loading.jl:1286; &nbsp; | [6] _require(::Base.PkgId) at ./loading.jl:1030; &nbsp; | [7] require(::Base.PkgId) at ./loading.jl:928; &nbsp; | [8] require(::Module, ::Symbol) at ./loading.jl:923; &nbsp; | [9] include(::Function, ::Module, ::String) at ./Base.jl:380; &nbsp; | [10] include at ./Base.jl:368 [inlined]; &nbsp; | [11] include(::String) at /storage7/buildkite-agent/.julia-2581/packages/PencilA",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1662#issuecomment-843325731:3480,load,loading,3480,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1662#issuecomment-843325731,1,['load'],['loading']
Performance,"; │ @ ./loading.jl:2340 [inlined]; │ [4] (::Base.var""#968#969""{Base.PkgId})(); │ @ Base ./loading.jl:1974; │ [5] mkpidlock(f::Base.var""#968#969""{Base.PkgId}, at::String, pid::Int32; kwopts::@Kwargs{stale_age::Int64, wait::Bool}); │ @ FileWatching.Pidfile ~/julia-1.10/usr/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:93; │ [6] #mkpidlock#6; │ @ ~/julia-1.10/usr/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:88 [inlined]; │ [7] trymkpidlock(::Function, ::Vararg{Any}; kwargs::@Kwargs{stale_age::Int64}); │ @ FileWatching.Pidfile ~/julia-1.10/usr/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:111; │ [8] #invokelatest#2; │ @ ./essentials.jl:894 [inlined]; │ [9] invokelatest; │ @ ./essentials.jl:889 [inlined]; │ [10] maybe_cachefile_lock(f::Base.var""#968#969""{Base.PkgId}, pkg::Base.PkgId, srcpath::String; stale_age::Int64); │ @ Base ./loading.jl:2983; │ [11] maybe_cachefile_lock; │ @ ./loading.jl:2980 [inlined]; │ [12] _require(pkg::Base.PkgId, env::Nothing); │ @ Base ./loading.jl:1970; │ [13] __require_prelocked(uuidkey::Base.PkgId, env::Nothing); │ @ Base ./loading.jl:1812; │ [14] #invoke_in_world#3; │ @ ./essentials.jl:926 [inlined]; │ [15] invoke_in_world; │ @ ./essentials.jl:923 [inlined]; │ [16] _require_prelocked; │ @ ./loading.jl:1803 [inlined]; │ [17] _require_prelocked; │ @ ./loading.jl:1802 [inlined]; │ [18] run_extension_callbacks(extid::Base.ExtensionId); │ @ Base ./loading.jl:1295; │ [19] run_extension_callbacks(pkgid::Base.PkgId); │ @ Base ./loading.jl:1330; │ [20] run_package_callbacks(modkey::Base.PkgId); │ @ Base ./loading.jl:1164; │ [21] _tryrequire_from_serialized(modkey::Base.PkgId, path::String, ocachepath::String, sourcepath::String, depmods::Vector{Any}); │ @ Base ./loading.jl:1487; │ [22] _require_search_from_serialized(pkg::Base.PkgId, sourcepath::String, build_id::UInt128); │ @ Base ./loading.jl:1574; │ [23] _require(pkg::Base.PkgId, env::String); │ @ Base ./loading.jl:1938; │ [24] __require_prelocked(uuidkey::Base.PkgId, env::S",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3686#issuecomment-2272432528:2324,load,loading,2324,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3686#issuecomment-2272432528,1,['load'],['loading']
Performance,"<td style = ""text-align: right; "">LinearEquationOfState</td>; <td style = ""text-align: right; "">1.0</td>; <td style = ""text-align: right; "">1.0</td>; <td style = ""text-align: right; "">1.0</td>; </tr>; <tr>; <td style = ""text-align: right; "">CPU</td>; <td style = ""text-align: right; "">RoquetEquationOfState</td>; <td style = ""text-align: right; "">1.14378</td>; <td style = ""text-align: right; "">1.00266</td>; <td style = ""text-align: right; "">1.0</td>; </tr>; <tr>; <td style = ""text-align: right; "">CPU</td>; <td style = ""text-align: right; "">TEOS10EquationOfState</td>; <td style = ""text-align: right; "">1.32274</td>; <td style = ""text-align: right; "">1.0</td>; <td style = ""text-align: right; "">1.0</td>; </tr>; </table>. <table>; <caption style = ""text-align: center; "">Equation of state relative performance (GPU)</caption>; <tr class = ""header headerLastRow"">; <th style = ""text-align: right; "">Architectures</th>; <th style = ""text-align: right; "">EquationsOfState</th>; <th style = ""text-align: right; "">slowdown</th>; <th style = ""text-align: right; "">memory</th>; <th style = ""text-align: right; "">allocs</th>; </tr>; <tr>; <td style = ""text-align: right; "">GPU</td>; <td style = ""text-align: right; "">LinearEquationOfState</td>; <td style = ""text-align: right; "">1.0</td>; <td style = ""text-align: right; "">1.0</td>; <td style = ""text-align: right; "">1.0</td>; </tr>; <tr>; <td style = ""text-align: right; "">GPU</td>; <td style = ""text-align: right; "">RoquetEquationOfState</td>; <td style = ""text-align: right; "">1.00497</td>; <td style = ""text-align: right; "">1.00222</td>; <td style = ""text-align: right; "">1.00018</td>; </tr>; <tr>; <td style = ""text-align: right; "">GPU</td>; <td style = ""text-align: right; "">TEOS10EquationOfState</td>; <td style = ""text-align: right; "">1.02813</td>; <td style = ""text-align: right; "">0.999823</td>; <td style = ""text-align: right; "">1.00018</td>; </tr>; </table>. ## Static ocean benchmarks. <table>; <caption style = ""text-align: center; "">Incompr",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1169#issuecomment-725471594:10255,perform,performance,10255,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1169#issuecomment-725471594,1,['perform'],['performance']
Performance,"= (:b, :τ1, :τ2),; closure = SmagorinskyLilly(C=0.1),; background_fields = (b=B_field,),; forcing = (u=Fᵤ, v=Fᵥ, w=Fw, b=Fb),; ); @info model. simulation = Simulation(model, Δt=1, stop_iteration=10). run!(simulation); ```. This (and way more complex examples) runs fine on the CPU but when I run that on the GPU I get:. ```; [ Info: Executing initial time step...; ERROR: LoadError: CUDA error: device kernel image is invalid (code 200, ERROR_INVALID_IMAGE). Stacktrace:; [1] CUDA.CuModule(data::Vector{UInt8}, options::Dict{CUDA.CUjit_option_enum, Any}); @ CUDA /glade/work/tomasc/.julia/packages/CUDA/Ey3w2/lib/cudadrv/module.jl:58; [2] CuModule; @ /glade/work/tomasc/.julia/packages/CUDA/Ey3w2/lib/cudadrv/module.jl:23 [inlined]; [3] cufunction_link(job::GPUCompiler.CompilerJob, compiled::NamedTuple{(:image, :entry, :external_gvars), Tuple{Vector{UInt8}, String, Vector{String}}}); @ CUDA /glade/work/tomasc/.julia/packages/CUDA/Ey3w2/src/compiler/execution.jl:481; [4] cached_compilation(cache::Dict{UInt64, Any}, job::GPUCompiler.CompilerJob, compiler::typeof(CUDA.cufunction_compile), linker::typeof(CUDA.cufunction_link)); @ GPUCompiler /glade/work/tomasc/.julia/packages/GPUCompiler/qdoh1/src/cache.jl:95; [5] cufunction(f::typeof(Cassette.overdub), tt::Type{Tuple{Cassette.Context{nametype(CUDACtx), Nothing, Nothing, KernelAbstractions.var""##PassType#312"", Nothing, Cassette.DisableHooks}, typeof(Oceananigans.Models.NonhydrostaticModels.gpu_calculate_Gu!), KernelAbstractions.CompilerMetadata{KernelAbstractions.NDIteration.StaticSize{(8, 8, 6)}, KernelAbstractions.NDIteration.DynamicCheck, Nothing, Nothing, KernelAbstractions.NDIteration.NDRange{3, KernelAbstractions.NDIteration.StaticSize{(1, 1, 6)}, KernelAbstractions.NDIteration.StaticSize{(16, 16, 1)}, Nothing, Nothing}}, OffsetArrays.OffsetArray{Float64, 3, CUDA.CuDeviceArray{Float64, 3, 1}}, RectilinearGrid{Float64, Periodic, Periodic, Bounded, Float64, Float64, OffsetArrays.OffsetVector{Float64, CUDA.CuDeviceVector{Float",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2869:2165,cache,cache,2165,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2869,1,['cache'],['cache']
Performance,"= RegularRectilinearGrid(size = (512, 512, 1), extent = (2π, 2π, 2π), topology = (Periodic, Periodic, Bounded)); xz_grid = RegularRectilinearGrid(size = (512, 1, 512), extent = (2π, 2π, 2π), topology = (Periodic, Periodic, Bounded)); yz_grid = RegularRectilinearGrid(size = (1, 512, 512), extent = (2π, 2π, 2π), topology = (Periodic, Periodic, Bounded)). function ten_steps!(model); for i = 1:10; time_step!(model, 1e-6); end; return nothing; end. for arch in (CPU(), GPU()); for grid in (; xy_grid,; xz_grid,; yz_grid,; ). model = NonhydrostaticModel(architecture = arch,; timestepper = :RungeKutta3,; advection = UpwindBiasedFifthOrder(),; grid = grid,; buoyancy = nothing,; tracers = nothing). @info ""Benchmarking $model...""; @btime ten_steps!($model); end; end; ```. The results are alarming: on the CPU we find. * `xy_grid`: 616.285 ms (429912 allocations: 114.29 MiB); * `xz_grid`: 4.638 s (944291 allocations: 2.63 GiB); * `yz_grid`: 3.240 s (405223 allocations: 2.60 GiB). Notice the `xz` configuration is 7 times slower than the `xy` configuration. In addition to that, allocations are through the roof --- 200 MiB of allocation per time-step?? (the benchmark tests 10 time-steps). On the GPU we find. * `xy_grid`: 48.438 ms (128139 allocations: 45.68 MiB); * `xz_grid`: 531.077 ms (637499 allocations: 53.51 MiB); * `yz_grid`: 147.963 ms (166797 allocations: 46.33 MiB). and therefore an even larger (10x) performance difference, though allocations are more under control. I ran the benchmarks for a few different topologies to see if the pressure solver was the culprit. But I noticed similar behavior whether or not `y` or `z` was `Periodic` or `Bounded`. I also tested whether the vertical integral for hydrostatic pressure was a problem by updating and then running code from #1910 . This didn't fix the problem either. I'm perplexed, so I think we should drill into this a bit more. Or --- am I misinterpreting or setting up the benchmarks incorrectly?. @ali-ramadhan @christophernhill",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1919:2170,perform,performance,2170,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1919,1,['perform'],['performance']
Performance,"> ""Done"" isn't very precise since the cubed sphere will never be ""done"". But perhaps we can put a number on performance for the first milestone, which will allow us to conclude whether we need this optimization or not. True. Ideally we want to be close to the scalings/performance we got with lat-lon grid? That’s perhaps not feasible..? I don’t know how close is good enough tho. . > Can you explain where the gut feeling comes from? Will filling halos be so expensive even on just one GPU, or is this a distributed problem? Currently, 1/4 degree is performant on one GPU. Well at least some gut feeling comes from that am pretty sure that it can be reduced in half by getting done in a single pass. But you are on point, I don’t have a gut feeling regarding how much impact the two passes have on performance.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3201#issuecomment-1718757478:108,perform,performance,108,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3201#issuecomment-1718757478,5,"['optimiz', 'perform']","['optimization', 'performance', 'performant']"
Performance,"> * I removed the `BackgroundField` v-velocity, which had no effect on the solution because the domain is `Flat` in y. The only effect of the background flow is through the bottom drag boundary condition. The docs preview hasn't updated yet, so I can't see the video, but I suspect removing `v` as a background creates oscillations. @glwagner have you checked? If not, let's wait for the video to see what happens... > * I reduced the domain aspect ratio to 400 x 100 because based on the visualization it seemed the domain didn't need to be so wide. This lets us increase the resolution and reduce the diffusivity, which is neat. It's a bit more turbulent now. Good move :+1: . > * Note using a tuple for `ĝ` rather than `Array` means it can be used as a parameter on the GPU, so that's probably preferred. Arrays are needed only if we need to mutate elements or perform linear algebra.; > * Don't import CUDA because the example wasn't GPU friendly anyways (if you like, we can make it GPU friendly but I don't think it should be ""partially"" GPU friendly since it just makes the code more complicated). I tried to make it 100% GPU friendly actually (although I didn't test it). You already pointed out one flaw, which is the use of an array instead of a tuple for the unit vertical vector. > * When I try to run the example multiple times I get `ERROR: LoadError: NetCDF error: Permission denied (NetCDF error code: 13)`. How can we avoid this error? I think it's important that users can easily change parameters and re-run without having to manually delete a file; this is key to productivity. This only happens with me when the NetCDF file is open by another program (generally python in my case) while Julia is trying to write to it. Was that the case? If so, that's the usual behavior afaik and I don't think we can do anything on our end except inform users about it. > * This is a great inexpensive example. I do wonder if we should make it 3D with an LES closure?. If we can get away with t",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2333#issuecomment-1065266138:865,perform,perform,865,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2333#issuecomment-1065266138,1,['perform'],['perform']
Performance,"> 90% sure, but if it's not there it's in `centered_reconstruction.jl` or `upwind_biased_reconstruction.jl` I haven't gotten around to test performance for non-weno schemes but I should do it....; > ; > I ll guess I ll dedicate today to searching the issue, so we can merge. `@inbounds` in . https://github.com/CliMA/Oceananigans.jl/blob/91dfb119917f33514dbf8cd833778c44f6cea9b3/src/Advection/weno_interpolants.jl#L45-L46",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2603#issuecomment-1185611545:140,perform,performance,140,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2603#issuecomment-1185611545,1,['perform'],['performance']
Performance,"> === means that ""no program can distinguish"" between the objects. For arrays this basically means they point to the same space in memory; changes to one imply changes in another. == is a weaker statement, usually about numerical equality. What's counter intuitive?. Oh yeah I know about `===`. What I mean is that you test two grids with `==`, but Julia then tests each property with `===`. So the user is actually performing a much stricter operation than what it seems at first sight. That's the un-intuitive part to me. Although I may be missing something.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2018#issuecomment-945862255:416,perform,performing,416,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2018#issuecomment-945862255,1,['perform'],['performing']
Performance,"> > 90% sure, but if it's not there it's in `centered_reconstruction.jl` or `upwind_biased_reconstruction.jl` I haven't gotten around to test performance for non-weno schemes but I should do it....; > > I ll guess I ll dedicate today to searching the issue, so we can merge; > ; > `@inbounds` in; > ; > https://github.com/CliMA/Oceananigans.jl/blob/91dfb119917f33514dbf8cd833778c44f6cea9b3/src/Advection/weno_interpolants.jl#L45-L46. I'm not sure that this is the deal breaker... I should have phrased is as a ""perhaps add @inbounds in.... ???""",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2603#issuecomment-1185612738:142,perform,performance,142,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2603#issuecomment-1185612738,1,['perform'],['performance']
Performance,"> > > > @glwagner any idea why `gpu-simulations-tests` fail?; > > > ; > > > ; > > > Which grid points are specifically failing in the test?; > > ; > > ; > > How does one go about and answers this question? The log is not informative... Shall I run the tests myself online and then printout the two arrays?; > ; > Yeah, that's what you have to do to determine it... not urgent though... I tried to do that but the HPC at ANU when I use a queue with GPU access it does not have internet access. So it stops at `include(""data_dependencies.jl"")` line...",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1988#issuecomment-928225330:437,queue,queue,437,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1988#issuecomment-928225330,1,['queue'],['queue']
Performance,"> > > The preview isn't showing for me. But we can fix any issues later, so I say merge away.; > > ; > > ; > > Yeah, It's also not loading for me. It feels like whenever I try to check PR previews it's kinda hit or miss. I wonder if this is an issue with Documenter.; > > But if you're okay with it, I'll merge it and we can fix any problems later.; > ; > It looks like it works for PRs that I submit, but not others. It may have to do with repo privileges. I think, for some reason whatsoever, the preview is not pushed just from the first commit. I think it's because there is a test whether that was a commit on a PR. And unless you first open the PR and then made the first commit that test returns false...",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2045#issuecomment-963656757:131,load,loading,131,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2045#issuecomment-963656757,1,['load'],['loading']
Performance,> > @TZTsai thanks for this! I'm wondering whether this worths your time and effort since after we [removed](https://github.com/CliMA/Oceananigans.jl/pull/3052) the multigrid pressure solver from the code perhaps this is not so much needed.; > ; > Alright. But is `compute_matrix_for_linear_operation` still used in this project? If so it may still provide some optimization. @navidcy can you answer @TZTsai's question?,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3229#issuecomment-1708642106:362,optimiz,optimization,362,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3229#issuecomment-1708642106,1,['optimiz'],['optimization']
Performance,> > @TZTsai thanks for this! I'm wondering whether this worths your time and effort since after we [removed](https://github.com/CliMA/Oceananigans.jl/pull/3052) the multigrid pressure solver from the code perhaps this is not so much needed.; > ; > Alright. But is `compute_matrix_for_linear_operation` still used in this project? If so it may still provide some optimization. Sorry I missed the question!. I think in principle it could be used for the construction of the Heptadiagonal solver matrix (cc @simone-silvestri) but even for that the matrix is actually hardcoded for the particular free-surface problem. Right @simone-silvestri?,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3229#issuecomment-1711066770:362,optimiz,optimization,362,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3229#issuecomment-1711066770,1,['optimiz'],['optimization']
Performance,"> > @siddharthabishnu, `CUDA.@allowscalar` introduced by [3cdd470](https://github.com/CliMA/Oceananigans.jl/pull/3488/commits/3cdd4705fedf3238d8858bb0f56e9b27b01ba34a) is detrimental for performance. Like it induces O(10-100x) slowdown I think....; > > Is this a temporary solution?; > > cc @glwagner, @simone-silvestri; > ; > I think it is. We can try to see if this works on one GPU. If it does we can keep the allowscalar for the moment otherwise we can remove them. In the end all this will have to live in a kernel. Note that this will still not work on multiple GPUs as you cannot explicitly access one region from another one on a different GPU without switching to the device that holds the data. @simone-silvestri and @navidcy, I totally agree. I only introduced CUDA.@allowscalar under the impression it was necessary for certain GPU tests to pass. However, now understanding that isn’t the case, I've removed it in commit [7f54c3c](https://github.com/CliMA/Oceananigans.jl/pull/3488/commits/7f54c3c451c9753524056d45c6578a9cf865be68).",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3488#issuecomment-2035930379:187,perform,performance,187,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3488#issuecomment-2035930379,1,['perform'],['performance']
Performance,"> > @siddharthabishnu, how do we know that now the metrics are filled correctly? you were comparing with a grid from MITgcm?; > ; > Yes. Consider the following sources for the grid metrics:; > ; > 1. the `cs32` grid with one halo layer, used by:; > ; > * Ali for the Rossby-Haurwitz test case in Oceananigans v0.82.0; and; > * yourself to check the interior coordinates and grid metrics of the Oceananigans `cc32` grid;; > 2. the `cs32` grid with 4 halo layers created by @jm-c using MITgcm;; > 3. the `cc32` grid created by Oceananigans.; > ; > In the validation scripts for solid body rotation and the Rossby-Haurwitz wave within the `ncc-glw/cubed-sphere-dynamics` branch associated with PR #3306, I compared the metrics of grid (3) against both (1) and (2), and plotted their absolute and relative differences. With the latest modifications, these differences have been minimized though not entirely eliminated. OK, so the benchmark is the `cs32` grid by MITgcm. Can we do the comparison in this PR? I'd like to see a test ideally because otherwise how do we assess that the changes we are suggesting here are correct. I can also do that, just give me a code snippet that loads the two grids?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3488#issuecomment-1978039025:1176,load,loads,1176,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3488#issuecomment-1978039025,1,['load'],['loads']
Performance,"> > @simone-silvestri will the changes here impact CATKE on [CliMA/ClimaOcean.jl#17](https://github.com/CliMA/ClimaOcean.jl/pull/17) ?; > ; > not on the GPU, this PR affects only CPU performance. sorry, I was not clear... I was asking whether any syntax for adding parametrization with additional tracers changed. (But I think, no, right?)",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3030#issuecomment-1487536293:183,perform,performance,183,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3030#issuecomment-1487536293,1,['perform'],['performance']
Performance,"> > As an aside, I think this issue illustrates that users are indeed interested in being able to evaluate gradients across boundaries. This is important because @simone-silvestri proposed a change that would make this impossible (eg it has been proposed we do not fill halos for `Value` and `Gradient` boundary conditions, and instead evaluate the associated fluxes in the same way we do for immersed boundaries --- because this has performance advantages for very large models).; > ; > I agree, but this patch-up will not work for immersed boundaries anyway. What do you mean? What patch-up?. > I still advocate for (maybe not now but later down the line) a general line of thought that ensures consistency between immersed boundaries and ""regular"" boundaries (a la MITgcm) treating them always the same way. I agree I think that would be nice. It means that operators need to know about boundary conditions though, which is a major refactor... > As an example, this issue could have been brought up for immersed boundaries, which would have required a (definitely more lengthy) rework of boundaries in abstract operations but would have solved the issue in both boundaries and immersed boundaries. We support values and gradients on non-immersed boundaries, but we do _not_ support evaluating them across immersed boundaries. We have to support what we claim / say that we support, that is the only issue.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3224#issuecomment-1690226786:434,perform,performance,434,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3224#issuecomment-1690226786,1,['perform'],['performance']
Performance,"> > Average reduction with conditional expressions, e.g., like; > > https://github.com/CliMA/Oceananigans.jl/blob/748feab10a55fa65a46455620203252a6fc0646e/test/test_field_reductions.jl#L107; > > ; > > induce scalar operations on the GPU. I guess it's not a surprise. I just had to add, e.g,; > > ```julia; > > @compute Txyz = CUDA.@allowscalar Field(Average(T, condition=T.>3)); > > ```; > > ; > > ; > > ; > > ; > > ; > > ; > > ; > > ; > > ; > > ; > > ; > > Only mentioning this here in case it might something in the source code that could be affecting code performance.; > ; > The condition should be a function of `f(i, j, k, grid, args...)` returning a boolean, or an `AbstractArray` of booleans.; > ; > We should implement boolean operations with fields to return boolean fields, so we will be able to pass an `AbstractOperation`; > ; > ```; > condition = T > 3 # This should be a boolean AbstractOperation; > @compute Txyz = Field(Average(T; condition)) ; > ```; > ; > at the moment the best way to specify a condition is through a function; > ; > ```; > @inline condition(i, j, k, grid, T) = T[i, j, k] > 3; > @compute Txyz = Field(Average(T; condition)) ; > ```. Agree with this. What is `T .> 3`? Is that a field? Either way, what we really want is to pass `T > 3` as @simone-silvestri says.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2701#issuecomment-1239601022:559,perform,performance,559,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2701#issuecomment-1239601022,1,['perform'],['performance']
Performance,"> > Average reduction with conditional expressions, e.g., like; > > https://github.com/CliMA/Oceananigans.jl/blob/748feab10a55fa65a46455620203252a6fc0646e/test/test_field_reductions.jl#L107; > > ; > > induce scalar operations on the GPU. I guess it's not a surprise. I just had to add, e.g,; > > ```julia; > > @compute Txyz = CUDA.@allowscalar Field(Average(T, condition=T.>3)); > > ```; > > ; > > ; > > ; > > ; > > ; > > ; > > ; > > ; > > ; > > ; > > ; > > Only mentioning this here in case it might something in the source code that could be affecting code performance.; > ; > The condition should be a function of `f(i, j, k, grid, args...)` returning a boolean, or an `AbstractArray` of booleans.; > ; > We should implement boolean operations with fields to return boolean fields, so we will be able to pass an `AbstractOperation`; > ; > ```; > condition = T > 3 # This should be a boolean AbstractOperation; > @compute Txyz = Field(Average(T; condition)) ; > ```; > ; > at the moment the best way to specify a condition is through a function; > ; > ```; > @inline condition(i, j, k, grid, T) = T[i, j, k] > 3; > @compute Txyz = Field(Average(T; condition)) ; > ```. I get an error... ```Julia; navidcy@tartarus:~/Oceananigans.jl$ julia-1.8 --project; _; _ _ _(_)_ | Documentation: https://docs.julialang.org; (_) | (_) (_) |; _ _ _| |_ __ _ | Type ""?"" for help, ""]?"" for Pkg help.; | | | | | | |/ _` | |; | | |_| | | | (_| | | Version 1.8.1 (2022-09-06); _/ |\__'_|_|_|\__'_| |; |__/ |. julia> using Oceananigans; [ Info: Oceananigans will use 4 threads. julia> using Oceananigans.Fields: CenterField, @compute. julia> trilinear(x, y, z) = x + y + z; trilinear (generic function with 1 method). julia> arch = GPU(); GPU(). julia> grid = RectilinearGrid(arch, size = (2, 2, 2),; x = (0, 2), y = (0, 2), z = (0, 2),; topology = (Periodic, Periodic, Bounded)); 2×2×2 RectilinearGrid{Float64, Periodic, Periodic, Bounded} on GPU with 3×3×3 halo; ├── Periodic x ∈ [0.0, 2.0) regularly spaced with Δx=1",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2701#issuecomment-1242894568:559,perform,performance,559,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2701#issuecomment-1242894568,1,['perform'],['performance']
Performance,"> > Can't we just plot a fixed-color shape on top on the mount on all panels? [`Makie.poly`](https://docs.makie.org/stable/reference/plots/poly/) seems to be able to do that.; > ; > We can.; > ; > We can also mask the output after we load it, e.g, via; > ; > using something like; > ; > ```julia; > using Oceananigans.ImmersedBoundaries: mask_immersed_field!; > ; > function mask_and_get_interior(φ_t, n; value=NaN); > mask_immersed_field!(φ_t[n], value); > return interior(φ_t[n], :, 1, :); > end; > ; > u′ₙ = @lift mask_and_get_interior(u′_t, $n); > ```; > ; > which gives; > ; > internal_tide.mp4 ; > But either of these solutions complicate the example a bit. Ideally, `mask_immersed_field!` should not be user-facing. And plotting a mountain on top of the mountain is a bit of a hack and would require a bit of explaining and justifying why we do that. I think we should mask the solution during visualization in the script. This is actually what users will need to do currently to make decent visualizations, so it is good to illustrate how to do it --- even if, hopefully, we will eventually have a better solution. As for plotting a shape on top, I'm inclined to encourage visualization that directly represents the domain / data for the purposes of an Oceananigans example. I think its ok if people want to incorporate visualizations like that in their own work but we may not want to promote it as ""the best"" way to visualize complex domains in this example.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3132#issuecomment-1986381358:234,load,load,234,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3132#issuecomment-1986381358,1,['load'],['load']
Performance,"> > I think something like you're proposing would have added to my confusion.; > ; > Isn't the confusion a problem with the output writers API? I think it sounds like a great idea to support `AbstractOperation` output. It's kind of logical. Doing this even allows us to do some clever stuff behind the scenes, like using one underlying array to store the results of multiple computations performed serial (thus saving memory). The confusion is that I wasn't aware that that a `Field` has `data`, and therefore takes up memory, which allows it to store values. While an `AbstractOperation` is just instructions on how to compute things. This extended to the output writers, but I wouldn't say output writers were the source of confusion. If we make it so that users don't have to know what a `Field` is and we can use abstract operations everywhere, then my comment is moot.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2235#issuecomment-1036795591:388,perform,performed,388,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2235#issuecomment-1036795591,1,['perform'],['performed']
Performance,"> > I think to preserve the work in this PR, we should add a `Float32` test which will fail if a spurious promotion undermines performance; > ; > Agreed. I'll revisit this PR later to see if I can find where the conversion happens. The test I added only checks to see if we can take a time step. But I should be able to also add a test to ensure no spurious promotion occurred. Ah, that will work as a test if we remove the `convert`. The `convert` is a good sanity check to find where the problem is, but its not a solution since it merely allows the code to run without error --- it doesn't actually allow us to realize the benefits of using `Float32`. Arguably with this it is actually worse to use `Float32`, since the numerics are degraded bbut the perfrmance benefit is not fully realized",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3876#issuecomment-2445330720:127,perform,performance,127,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3876#issuecomment-2445330720,1,['perform'],['performance']
Performance,"> > I'm wondering if we should provide a separate page on ""Using GPUs""? While the simulation tips for CPUs are really performance optimizations that are optional, the GPU simulation tips are mostly required to run without errors.; > ; > That's a good point. Although I think we could avoid creating another page and put that information in the [""Using GPUs""](https://clima.github.io/OceananigansDocumentation/stable/using_gpus/) page, so that things are more condensed. 🤦 there's already Using GPUs of course, silly me...",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1543#issuecomment-818368200:118,perform,performance,118,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1543#issuecomment-818368200,2,"['optimiz', 'perform']","['optimizations', 'performance']"
Performance,"> > If our objective purely regards broadcasting, then we could probably generalize broadcasting so that `Fields` with singleton indices and/or `Nothing` location act more like reduced-dimensionality arrays. Is that what you would like to see?; > ; > Yes, it would be nice to have that feature. Do you want to have a crack at it? I don't use broadcasting with fields so much personally. It's a bit slow for some reason, which we have never quite figured out. Convenient for some things but not to be relied on unless we can solve the performance issue.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3361#issuecomment-2038969357:534,perform,performance,534,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3361#issuecomment-2038969357,1,['perform'],['performance']
Performance,"> > The preview isn't showing for me. But we can fix any issues later, so I say merge away.; > ; > Yeah, It's also not loading for me. It feels like whenever I try to check PR previews it's kinda hit or miss. I wonder if this is an issue with Documenter.; > ; > But if you're okay with it, I'll merge it and we can fix any problems later. It looks like it works for PRs that I submit, but not others. It may have to do with repo privileges.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2045#issuecomment-963592320:119,load,loading,119,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2045#issuecomment-963592320,1,['load'],['loading']
Performance,"> > To use python, we'd have to invoke `PyCall` right? Running python directly in the CI is possible but a bit of work and might be a pain to maintain.; > > I think its ""good"" to illustrate python usage, but also building and maintaining that example requires work that might be put to better use elsewhere... I'd shy away from doing this until there's either a more compelling need, or we have more resources for code development / maintenance...; > ; > Yes I think we'd need to use `Pycall`. I agree that's hard to maintain so let's not pursue that now.; > ; > I still think it'd be good to show one example with `NetCDF` writer though. Here's a question: is post-processing results directly in julia (making animations, plots, etc.) using the `NetCDF` output as easy as with the `jld2` output?. There's currently one example with NetCDF: https://clima.github.io/OceananigansDocumentation/stable/generated/shallow_water_Bickley_jet/. I'm not totally sure what's meant by easy. If plotting arrays, then the tasks are identical. If using `FieldTimeSeries` (to perform non-trivial finite volume calculations in post-processing for example) then that isn't supported with `NetCDFOutputWriter` (this is possible, but requires someone passionate about NetCDF to put in the effort there).",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1498#issuecomment-954122449:1060,perform,perform,1060,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1498#issuecomment-954122449,1,['perform'],['perform']
Performance,"> > Very nice work @glwagner , and thanks for making this. Lots of good stuff here.; > > In your calculations, you find that there is saturation at 16 threads. I might guess that you have 16 cores on one node? I would think that this should be node dependent.; > > Also, in the table, might it be possible to compute the efficiency as well? I think that's more standard than speed up.; > ; > Ah, this machine has 48 cores. Since threading has an overhead cost, we expect saturation at some point. It's surprising that this happens at just 16 cores for such a large problem (512^3) though.; > ; > We can calculate more metrics for sure.; > ; > I think it would be worthwhile to investigate whether other threading paradigms scale differently for the same problem. Numba + parallel accelerator might be a good test case. @hennyg888 would you be interested in that?; > ; > Here are some docs:; > ; > https://numba.pydata.org/numba-doc/latest/user/parallel.html. You run out of memory bandwidth at some point - usually before you get to saturate all the cores for something; like diffusion. So some of 16 thread drop off could be that. . I guess we could get even more minimalist and check a multi-threaded stream benchmark to see that?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1861#issuecomment-886090304:1188,multi-thread,multi-threaded,1188,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1861#issuecomment-886090304,1,['multi-thread'],['multi-threaded']
Performance,"> > When utilizing a split-explicit free surface, additional errors arise. For instance, for MultiRegionGrid and ConformalCubedSphereGrid (when specifying the number of substeps), we encounter:; > > ```julia; > > ERROR: LoadError: UndefVarError: `settings` not defined; > > ```; > > ; > > ; > > ; > > ; > > ; > > ; > > ; > > ; > > ; > > ; > > ; > > When specifying both grid and cfl for ConformalCubedSphereGrid, the following error occurs:; > > ```julia; > > ERROR: LoadError: type OrthogonalSphericalShellGrid has no field Lz; > > ```; > ; > Do you need help fixing these?. @glwagner thanks for your help on Zoom yesterday. @navidcy thanks for creating a new PR #3305 to fix the second issue.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3302#issuecomment-1747159712:220,Load,LoadError,220,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3302#issuecomment-1747159712,2,['Load'],['LoadError']
Performance,"> > Where does this PR leave #2538? As I understand it we do not treat vertically stretched grids here (or a mixed FFT/tridiagonal solver). So this moves laterally compared to #2538 and perhaps the algorithm implemented there can be adapted once this is merged?; > ; > I was planning was to implement the tridiagonal solve similarly to what we do for single GPU as such:; > ; > ```julia; > transpose_z_to_y!(storage) # copy data from storage.zfield to storage.yfield; > solver.plan.forward.y!(parent(storage.yfield), buffer.y) ; > transpose_y_to_x!(storage) # copy data from storage.yfield to storage.xfield; > solver.plan.forward.x!(parent(storage.xfield), buffer.x); > transpose_x_to_y!(storage) # copy data from storage.xfield to storage.yfield; > transpose_y_to_z!(storage) # copy data from storage.yfield to storage.zfield; > ; > # Perform the implicit vertical solve here on storage.zfield...; > ; > transpose_z_to_y!(storage); > solver.plan.backward.y!(parent(storage.yfield), buffer.y); > transpose_y_to_x!(storage) # copy data from storage.yfield to storage.xfield; > solver.plan.backward.y!(parent(storage.xfield), buffer.x); > transpose_x_to_y!(storage) # copy data from storage.xfield to storage.yfield; > transpose_y_to_z!(storage) # copy data from storage.yfield to storage.zfield; > ```; > ; > This is not super great because it requires eight transposes, but I think it's the same thing that was happening in #2538. Can you just say, you are planning to implement the algorithm in #2538? The way you describe it, I can't figure out if you are coming up with something new or not.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3279#issuecomment-2197393925:837,Perform,Perform,837,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3279#issuecomment-2197393925,1,['Perform'],['Perform']
Performance,"> > Yeah this is pretty worrying... I'm pretty sure this is the cause of #1420 which has been open for a while so this slowdown must have been around for a while (and just flew under the radar). [Profiling](https://docs.julialang.org/en/v1/manual/profile/) might help pinpoint the bottleneck.; > ; > It looks like there are some extra temporary arrays being created for the cases with non-trivial (although still pretty trivial) indexing patterns. Profiling could be good - also just turning off as much as possible and then building up from the FFT alone, simple stencil etc... - if that is possible?. I did try with a different advection scheme (and `halo_size=(1, 1, 1)`) and the allocations were mitigated. This and @tomchor's result suggests that halo filling for periodic directions is a likely culprit. Profiling is a good idea. I also had the thought of commenting out much of the time-stepping loop and building up to try to pinpoint which functions trigger allocations as @christophernhill suggests.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1919#issuecomment-891521065:281,bottleneck,bottleneck,281,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1919#issuecomment-891521065,1,['bottleneck'],['bottleneck']
Performance,"> > the time integral when we use variable time stepping that is a feature we use quite often; > ; > What do you mean by this?. Actually, I should probably say that I use the wizard quite often to change the time step. In my opinion, AB2 is a good compromise between accuracy, stability, and performance. RK3 is better only when you can achieve a CFL 3 times larger. We should fix the variable AB2 time stepper or discourage the use of frequent updates of the time step when using AB2. The first option is probably better in my opinion. I ll look into fixing AB2 and what it entails",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3738#issuecomment-2313376167:292,perform,performance,292,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3738#issuecomment-2313376167,1,['perform'],['performance']
Performance,> @TZTsai thanks for this! I'm wondering whether this worths your time and effort since after we [removed](https://github.com/CliMA/Oceananigans.jl/pull/3052) the multigrid pressure solver from the code perhaps this is not so much needed. Alright. But is `compute_matrix_for_linear_operation` still used in this project? If so it may still provide some optimization.,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3229#issuecomment-1703878093:353,optimiz,optimization,353,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3229#issuecomment-1703878093,1,['optimiz'],['optimization']
Performance,"> @ali-ramadhan are those regressions on the CPU or GPU? On the CPU, we might need `inline` (we need inline to elide a number of potential memory allocation points I think). Not sure about GPU. Just CPU right now. They're taking too long so I'm just gonna kill it and run the GPU tests by themselves. I'd be pretty surprised if `@inline` gives a 100x performance boost but I'm still not super familiar with writing performant Julia code.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/245#issuecomment-496473977:351,perform,performance,351,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/245#issuecomment-496473977,2,['perform'],"['performance', 'performant']"
Performance,"> @ali-ramadhan do you mind running some benchmarks to test for performance regression under this PR?. Looks really bad, still waiting on GPU results but CPU is ~100x slower! We can't merge this as is. Something must be wrong somewhere. Memory allocations went from `6.00 KiB` per iteration to `1.04 GiB` per iteration. It used to be resolution independent but now the allocation increases with the number of grid points. I'll edit this comment with the full benchmark timings when they're done. I benchmarked master yesterday so I know it's still performing well (maybe 2-3% slower because of the extra stuff and fixes we recently introduced).",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/245#issuecomment-496472556:64,perform,performance,64,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/245#issuecomment-496472556,2,['perform'],"['performance', 'performing']"
Performance,> @ali-ramadhan why not broadcast over CuArrays? Is there a Performance hit?; > ; > _Originally posted by @glwagner in https://github.com/climate-machine/Oceananigans.jl/issues/104#issuecomment-469749450_,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/108:60,Perform,Performance,60,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/108,1,['Perform'],['Performance']
Performance,"> @glwagner : I agree that the major savings would be the lack of vertical grid points, but having a full free-surface will likely force a smaller time step because of CFL. There would be two ways to reduce this constraint (in the future).; > ; > 1. Rigid lid (solve for the surface pressure); > 2. Implicit free-surface (treat the free-surface implicitly in the time-stepping); > ; > Both of these would borrow from the hydrostatic model, but the ingrediants are there, and would make it a lot faster. I would vote for implementing a rigid lid / vertically-`Flat` mode for the hydrostatic model instead, in order to keep the shallow water model as simple as possible (generalizing to multiple layers as an alternative direction would be nice I think). Have you benchmarked this? It is true that vertically `Flat` avoids a few vertical operations in the vertical advection term but since the kernel sizes are the same I'd be surprised if the performance differences are huge in terms of cost per time step.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2522#issuecomment-1119195736:942,perform,performance,942,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2522#issuecomment-1119195736,1,['perform'],['performance']
Performance,"> @glwagner CUDA aware MPI is supported for 11.2 but it is only built against OpenMPI 4.1.0 and 4.1.1, I think we had previously 4.0.4. Also julia 1.6.2 is available. good beta, I'll upgrade, thanks!. > looks like maybe some more race conditions on different hardware?. Heh yeah, pretty interesting. sverdrup is running on Quadro P6000 with CUDA 11; tartarus (where we sometimes run tests for development) has a Titan V, also I think with CUDA 11. These are running on a P100? With this new CI I'll need to make sure I can log in to central to debug.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1962#issuecomment-906751484:230,race condition,race conditions,230,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1962#issuecomment-906751484,1,['race condition'],['race conditions']
Performance,"> @glwagner I also ran into some problems using `StatProfilerHTML.jl` to make flame graphs for CPU profiles. This is from the same script used to obtain the results above and shown in #1914 and it's a 128^3 nonhydrostatic model. The flame graphs don't display the function names, and all I can see is ""overdub"". By hovering my mouse over the slabs and going up each flame stack I can usually find a function name that makes sense somewhere but that prevents us from making at-a-glance analysis of the profile flame graph.; > ![image](https://user-images.githubusercontent.com/45054739/128443657-7b18d4f9-0168-4bee-b85b-2ade021165d3.png); > I thought that this might have something to do with profiling `run(simulation, 10)` instead of a for loop of `time_step!(model,1)` but apparently the result is the same for both cases. I believe this is inevitable, because all our kernels are compiled through `Cassette.jl`, which ""overdubs"" the julia compiler when compiling functions tagged with `@kernel` (the majority of our expensive kernels). This is part of the design of `KernelAbstractions.jl`... Really great work @hennyg888. Perhaps the complexity of our function calls via `KernelAbstractions.jl` argues for a better profiling approach? Is there a way to ""filter"" the profiled output to remove data? . I think the next step towards improving performance is to figure out how to optimize the tendency calculations for CPU or GPU.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1912#issuecomment-894239292:1344,perform,performance,1344,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1912#issuecomment-894239292,2,"['optimiz', 'perform']","['optimize', 'performance']"
Performance,"> @glwagner I'm not sure whats causing these tests to fail now?. @jagoosw afaik sometimes we get errors of the kind. `ERROR: LoadError: SystemError: opening file ""/data5/glwagner/.julia-10703/compiled/v1.8/Oceananigans/hU93i_Y0P9A.ji"": No such file or directory`. on the gpu tests. Which is what's hapenning now. It's now clear to me at least what the cause is, but restarting the tests generally makes them pass. I just restarted them so it be fine. I should note that we got an error of the kind. `Expression: all(test_fields.v .≈ truth_fields.v)`. in the GPU shallow water tests, which I thought were corrected. Is this something we need to worry about?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2912#issuecomment-1481364658:125,Load,LoadError,125,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2912#issuecomment-1481364658,1,['Load'],['LoadError']
Performance,"> @glwagner, regarding the; > ; > ```julia; > ERROR: LoadError: UndefVarError: bc not defined; > ```; > ; > in the Docs, is it related to; > [JuliaDocs/Documenter.jl#228](https://github.com/JuliaDocs/Documenter.jl/issues/228); > perhaps?. Nice find!",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1843#issuecomment-879849296:53,Load,LoadError,53,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1843#issuecomment-879849296,1,['Load'],['LoadError']
Performance,"> @matinraayai is working on making PencilArrays performant. This PR is exploratory and is a fallback that might not be merged if we find an efficient way to do GPU transposes with PencilArrays (requires reducing memory allocations and improving the efficiency of permute operations) and implement r2r Fourier transforms in PencilFFTs. For the moment those two elements are part of this PR.; > ; > This PR follows the (simple) configuration of the 2decomp library https://github.com/2decomp-fft/2decomp-fft, the difference between PencilFFT/PencilArray and this PR (a part bounded domain ffts) is that here (at the moment) we impose the stricter limitation that `Ny` has to be divisible by `Rx` and `Ry` while `Nz` has to be divisible by `Ry`, where `Rx` and `Ry` are the number of ranks (divisions) in the x and y direction. Relaxing the requirements should not be too difficult. Nice, thanks for that explanation. Why are we following 2decomp? PencilArrays has some benchmarking that shows it can compete with the fastest codes out there. I don't see anything similar for 2decomp, so I can't figure out what the motivation for following that strategy would be. I'm not sure if they are different, either. Something we do not previously support (but which is implemented in https://github.com/CliMA/Oceananigans.jl/pull/2538) was an algorithm that could support any topology with vertically-stretched grids. What is the relationship between this PR and https://github.com/CliMA/Oceananigans.jl/pull/2538, and does this PR support vertically stretched grids?. One of the main limitations of PencilArrays from our perspective is that it could not distribute an array along the first dimension. Since we almost always would like to use vertically stretched grids (and for various reasons, we may want to also compute the hydrostatic pressure with a vertical integral), ocean LES are typically distributed in x and y. Therefore, in order to support 2D domain decompositions, we were faced with somehow ch",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3279#issuecomment-1728278859:49,perform,performant,49,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3279#issuecomment-1728278859,1,['perform'],['performant']
Performance,"> @navidcy thanks for the commits. I am now testing the script against a free-explicit free surface and a MultiRegionGrid. @navidcy, as you may have anticipated, a MultiRegionGrid does not result in this error. Code Modification:; ```julia; #=; grid = ConformalCubedSphereGrid(; panel_size = (Nx, Ny, Nz),; z = (-1, 0),; radius = R,; horizontal_direction_halo = 6,; partition = CubedSpherePartition(; R = 1)); =#. latlongrid = LatitudeLongitudeGrid(size=(Nx, Ny, Nz),; longitude = (-90, 90),; latitude = (-45, 45),; z = (-1, 0)); grid = MultiRegionGrid(latlongrid, partition = XPartition(2)); ```; Terminal Output:; ```julia; julia> include(""validation/multi_region/cubed_sphere_steady_state.jl""); ┌ Warning: MultiRegion functionalities are experimental: help the development by reporting bugs or non-implemented features!; └ @ Oceananigans.MultiRegion /Users/Sid/Library/CloudStorage/Dropbox/StudyFolder/PostDocMITDesktop/Codes/Oceananigans/cubed-sphere-steady-state/src/MultiRegion/multi_region_grid.jl:102; [ Info: Initializing simulation...; Iteration: 0000, time: 0 seconds, Δt: 7.722 days, wall time: 0 seconds; [ Info: ... simulation initialization complete (111.288 ms); [ Info: Executing initial time step...; [ Info: ... initial time step complete (994.424 ms).; [ Info: Simulation is stopping after running for 0 seconds.; [ Info: Simulation time 628.319 ms equals or exceeds stop time 628.319 ms.; [ Info: Making an animation from the saved data...; ```; When utilizing a split-explicit free surface, additional errors arise. For instance, for MultiRegionGrid and ConformalCubedSphereGrid (when specifying the number of substeps), we encounter:; ```julia; ERROR: LoadError: UndefVarError: `settings` not defined; ```; When specifying both grid and cfl for ConformalCubedSphereGrid, the following error occurs:; ```julia; ERROR: LoadError: type OrthogonalSphericalShellGrid has no field Lz; ```",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3302#issuecomment-1743475155:1675,Load,LoadError,1675,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3302#issuecomment-1743475155,2,['Load'],['LoadError']
Performance,> @sandreza might be a good idea to code up the case we were using the diagnose the race condition as a test and see if we can catch the race condition in CI @ali-ramadhan. Perhaps a regression test would work for this... and it'd be nice to have a regression test for the hydrostatic model too.,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1985#issuecomment-926695971:84,race condition,race condition,84,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1985#issuecomment-926695971,2,['race condition'],['race condition']
Performance,"> @sangeethasankar01 are you trying to get this to run on the GPU for some specific purpose? Or is this merely an educational exercise?. I am a research scholar at IIT Madras, working under the supervision of Dr. Arjun Jagannathan. Currently, I'm exploring various problems related to flow instabilities. However, my current focus on this Kelvin-Helmholtz instability problem is purely for educational practice. I aim to understand the architecture of the code and optimize it for GPU execution.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3522#issuecomment-2029214744:465,optimiz,optimize,465,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3522#issuecomment-2029214744,1,['optimiz'],['optimize']
Performance,"> @siddharthabishnu, `CUDA.@allowscalar` introduced by [3cdd470](https://github.com/CliMA/Oceananigans.jl/pull/3488/commits/3cdd4705fedf3238d8858bb0f56e9b27b01ba34a) is detrimental for performance. Like it induces O(10-100x) slowdown I think....; > ; > Is this a temporary solution?; > ; > cc @glwagner, @simone-silvestri. I think it is. We can try to see if this works on one GPU. If it does we can keep the allowscalar for the moment otherwise we can remove them.; In the end all this will have to live in a kernel.; Note that this will still not work on multiple GPUs as you cannot explicitly access one region from another one on a different GPU without switching to the device that holds the data.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3488#issuecomment-2034043615:185,perform,performance,185,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3488#issuecomment-2034043615,1,['perform'],['performance']
Performance,"> @simone-silvestri will the changes here impact CATKE on [CliMA/ClimaOcean.jl#17](https://github.com/CliMA/ClimaOcean.jl/pull/17) ?. not on the GPU, this PR affects only CPU performance",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3030#issuecomment-1486715558:175,perform,performance,175,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3030#issuecomment-1486715558,1,['perform'],['performance']
Performance,"> @xkykai do you think you could run some immersed boundary tests with this branch to make sure this change doesn’t affect your work? I think we’re interested in both performance and making sure the solution is high quality. Do you mean using the immersed pressure solver in this branch, and comparing the solution this produces with the one before this change?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3080#issuecomment-1582859937:167,perform,performance,167,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3080#issuecomment-1582859937,1,['perform'],['performance']
Performance,"> A more hierarchical organization of the docs is; > ; > 1. Introduction; > ; > * What's Oceananigans?; > * Installation; > * Writing your first script; > 2. Examples; > 3. Model setup; > ; > * Fields, BoundaryCondition, and AbstractOperations; > * IncompressibleModel; > * HydrostaticFreeSurfaceModel; > * ShallowWaterModel; > * Setting initial conditions; > * Diffusion, viscosity, and TurbulenceClosures; > * Forcing functions; > * Coriolis forces; > * Buoyancy forces; > 4. Simulations and post processing; > ; > * Simulation; > * OutputWriters; > * OutputReaders, post-processing, and plotting; > 5. Useful tips; > ; > * Using Graphics Processing Units (GPUs); > * Common errors and performance pitfalls; > 6. Contributor's guide; > 7. Gallery (this should be way higher eventually, but we need more recent content to motivate that...); > 8. Physics and numerical implementation; > 9. Appendix. I wholeheartedly agree with this structure :)",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1852#issuecomment-879265116:688,perform,performance,688,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1852#issuecomment-879265116,1,['perform'],['performance']
Performance,"> Ah interesting. Does this work because broadcasting over GPU subarray views is a little rough around the edges?; > ; > Also, is it worth adding the MWE from [#1767 (comment)](https://github.com/CliMA/Oceananigans.jl/issues/1767#issuecomment-868793917) as a test?. Certainly _something_ is rough... I think it's ""broadcasting over ReducedField"" that's the issue here. Prior to this PR, `mean!` used a broadcast over `R::AbstractReducedField` to compute the normalization. For some reason this has data synchronicity issues on the GPU (I'm at a loss to explain why). This PR just changes that broadcast operation to compute over all members of `R`. The halo regions of `R` (presumably) aren't touched during `sum!`, so doing some extra normalization in the halos doesn't really matter, I guess... (if we want halos to be right we should probably fill them after executing `mean!` in `compute!`, or something). But the _reason_ why this change fixes the issue isn't at all obvious to me. Maybe there's a bad interaction between `KernelAbstractions` (which gets used for the broadcast) and `sum!`? Not sure. Maybe another solution would wrap `sum!` in `CUDA.@sync` (eg the solution here could _implicitly_ synchronize in order to perform the broadcast correctly; otherwise CUDA wouldn't work generally... ?!?). Might be worth testing that, though I don't know which solution we should prefer (if any). In fact, it seems better to avoid using custom broadcasting machinery if possible (which is the current solution)...",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1769#issuecomment-868839671:1228,perform,perform,1228,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1769#issuecomment-868839671,1,['perform'],['perform']
Performance,"> Ah sorry I must have misunderstood your question. Yeah I think broadcasts; > tend to perform really well on scalar operations so I don't see why not. there is sadly a performance difference between `map`, since `broadcast` need to check dimensions",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/108#issuecomment-469891336:87,perform,perform,87,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/108#issuecomment-469891336,2,['perform'],"['perform', 'performance']"
Performance,"> Ah thanks for pointing this out @zhenwu0728! I didn't realize that.; > ; > It also seems that `deleteat!` is not defined for `StructArray`...; > ; > ```julia; > julia> s = StructArray{ComplexF64}((randn(5), randn(5))); > 5-element StructArray(::Array{Float64,1}, ::Array{Float64,1}) with eltype Complex{Float64}:; > 0.6663581952584336 - 1.5283519296646326im; > -0.4906568589874505 - 0.325094980686686im; > 1.1453745422319603 + 0.07874273417016024im; > 1.0280155738262986 - 0.37561698848991654im; > 1.1466416201897343 - 1.1904623364760012im; > ; > julia> deleteat!(s, 2); > ERROR: MethodError: no method matching deleteat!(::StructArray{Complex{Float64},1,NamedTuple{(:re, :im),Tuple{Array{Float64,1},Array{Float64,1}}},Int64}, ::Int64); > Closest candidates are:; > deleteat!(::Array{T,1} where T, ::Integer) at array.jl:1306; > deleteat!(::Array{T,1} where T, ::Any) at array.jl:1343; > deleteat!(::BitArray{1}, ::Integer) at bitarray.jl:935; > ...; > Stacktrace:; > [1] top-level scope at REPL[13]:1; > ```. Yes, I think so. But there's `StructArrays.append!!` to add particles. I don't know why there's no `delete!!` or `deleteat!!`. Maybe we could open an issue in `StructArrays.jl`. Also we need to pay attention to the performance as well.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1091#issuecomment-733888198:1227,perform,performance,1227,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1091#issuecomment-733888198,1,['perform'],['performance']
Performance,> Also I found it quite cumbersome in scripts when I wanted to load these constants I had to load one from `Grids` and one from `Coriolis` and one from `BuoyancyModels`.... That's a bit counterintuitive from a user's perspective. Shouldn't you be defining and setting constants in the scripts if you are doing that?,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3045#issuecomment-1492624226:63,load,load,63,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3045#issuecomment-1492624226,2,['load'],['load']
Performance,"> Although I'm somewhat worried whether the errors in this PR are acceptable. A test that should be implemented is that mass does not accumulate in time. In other words, the round-off error incurred by the pressure projection is constant, as expected. > Do we still expect the velocity field to be divergence-free up to machine precision?. We should regard the calculated divergence as an empirical measurement of the round-off error that accumulates during the discrete pressure solve. If the divergence is incorrect (ie, larger than expected due to round off error), then the pressure solver is incorrect, and the fix for that certainly should not be to recompute w!. I am not 100% sure how to theoretically determine the expected round off error associated with the pressure solve. I suppose that we expect that eigenfunction pressure solution to be accurate to within machine precision. However, we subsequently perform a 3D inverse FFT, followed next by a subtraction to calculate the velocity update, and then a second subtraction to calculate the divergence. > The sum still accumulates in time but went from e-22 -> e-20 to e-18 -> e-12. Any significant change in accuracy may be an indication that this PR is important. If the divergence changes significantly after recomputing w, this indicates that a substantial amount of round off error is being accumulated in the velocity field. . We may be able to calculate the expected round off error per grid point.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/711#issuecomment-602552703:916,perform,perform,916,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/711#issuecomment-602552703,1,['perform'],['perform']
Performance,"> Another caveat is that this method of restoring checkpointed data will not work for large CPU models that consume almost all of the CPU memory (such that a single field cannot be loaded from file after `model` has been instantiated). These cases are relatively rare right now, since such large models would typically run very slowly on a typical single node. Not sure how it would work with JLD2 but `Base.read!` can fill an array by reading data from disk: [https://docs.julialang.org/en/v1/base/io-network/#Base.read!](https://docs.julialang.org/en/v1/base/io-network/#Base.read!)",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1068#issuecomment-711036331:181,load,loaded,181,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1068#issuecomment-711036331,1,['load'],['loaded']
Performance,"> Another thought for @christophernhill; > ; > At the talk today on `ImplicitGlobalGrid.jl`, they were using `@view` in the simplest code but they dropped it as soon as they started to optimize the code. I believe they started using `LazyArrays.jl`. I don't know what it is but I suspect it doesn't have the problems that `@view` might have. @francispoulin thanks. I think we probably just want to do some buffer. I looked at LazyArrays.jl and I could imagine how that could maybe also be included, but I suspect the main thing is having a buffer (which https://github.com/eth-cscs/ImplicitGlobalGrid.jl has). I don't see any sign of LazyArrays in https://github.com/eth-cscs/ImplicitGlobalGrid.jl code! We can check with Ludovic though.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1882#issuecomment-885972875:185,optimiz,optimize,185,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1882#issuecomment-885972875,1,['optimiz'],['optimize']
Performance,"> Another thought for @christophernhill; > ; > At the talk today on `ImplicitGlobalGrid.jl`, they were using `@view` in the simplest code but they dropped it as soon as they started to optimize the code. I believe they started using `LazyArrays.jl`. I don't know what it is but I suspect it doesn't have the problems that `@view` might have. We think that we cannot send non-contiguous data over MPI between GPUs (only CPUs). Thus certain `view`s will not work. Possibly in this case the data is transferred to CPU, sent over MPI, and then copied back to the GPU (slow).",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1882#issuecomment-885935875:185,optimiz,optimize,185,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1882#issuecomment-885935875,1,['optimiz'],['optimize']
Performance,"> Apologies for the late reply, I got distracted and accidentally forgot to respond. I tested out the fix this morning/early afternoon and I keep getting an error along the lines of what I have below. Did I compile the branch of oceananigans incorrectly?; > ; > ```julia; > wireless-10-104-201-207:BottomBoundaryLayer loganknudsen$ julia ""/Users/loganknudsen/Documents/GitHub/BottomBoundaryLayer/PSI_Base_Test.jl""; > ┌ Warning: Overwriting existing ./psi_base_ocng_test.nc.; > └ @ Oceananigans.OutputWriters ~/.julia/packages/Oceananigans/Feeqx/src/OutputWriters/netcdf_output_writer.jl:359; > ERROR: LoadError: NetCDF error: Permission denied (NetCDF error code: 13); > Stacktrace:; > [1] check; > @ ~/.julia/packages/NCDatasets/st9Jz/src/errorhandling.jl:25 [inlined]; > [2] nc_create(path::String, cmode::UInt16); > @ NCDatasets ~/.julia/packages/NCDatasets/st9Jz/src/netcdf_c.jl:255; > [3] NCDatasets.NCDataset(filename::String, mode::String; format::Symbol, share::Bool, diskless::Bool, persist::Bool, memory::Nothing, attrib::Dict{Any, Any}); > @ NCDatasets ~/.julia/packages/NCDatasets/st9Jz/src/dataset.jl:236; > [4] NCDataset; > @ ~/.julia/packages/NCDatasets/st9Jz/src/dataset.jl:177 [inlined]; > [5] NetCDFOutputWriter(model::NonhydrostaticModel{Oceananigans.TimeSteppers.RungeKutta3TimeStepper{Float64, NamedTuple{(:u, :v, :w, :b), Tuple{Field{Face, Center, Center, Nothing, RectilinearGrid{Float64, Flat, Periodic, Bounded, Float64, Float64, Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, CPU}, Tuple{Colon, Colon, Colon}, OffsetArrays.OffsetArray{Float64, 3, Array{Float64, 3}}, Float64, FieldBoundaryConditions{Nothing, Nothing, BoundaryCondition{Oceananigans.BoundaryConditions.Per",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3228#issuecomment-1747420839:601,Load,LoadError,601,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3228#issuecomment-1747420839,1,['Load'],['LoadError']
Performance,"> Apparently optimizing reductions on the GPU is non-trivial but can be done: https://developer.download.nvidia.com/compute/cuda/1.1-Beta/x86_website/projects/reduction/doc/reduction.pdf Not sure if an optimized reduction kernel is available in Julia/CUDA.jl but maybe they will be in the future. @maleadt or @vchuravy might know... The CUDA.jl implementation is such an implementation: https://github.com/JuliaGPU/CUDA.jl/blob/2310b88738c11449867e6f37c5acbfcd453dc2c8/src/mapreduce.jl#L60-L69. It probably needs to be profiled and optimized if you really want to squeeze out performance, but it isn't a naive implementation either. It's exposed via `Base.mapreducedim` and friends.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1497#issuecomment-802599152:13,optimiz,optimizing,13,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1497#issuecomment-802599152,4,"['optimiz', 'perform']","['optimized', 'optimizing', 'performance']"
Performance,> Are we making AMDGPU an extension?. I was asked to keep the PR minimal. I have a good deal of follow up work related to performance coming. You'll need to make AMDGPU a package extension after.,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3468#issuecomment-1936969288:122,perform,performance,122,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3468#issuecomment-1936969288,1,['perform'],['performance']
Performance,"> As an aside, I think this issue illustrates that users are indeed interested in being able to evaluate gradients across boundaries. This is important because @simone-silvestri proposed a change that would make this impossible (eg it has been proposed we do not fill halos for `Value` and `Gradient` boundary conditions, and instead evaluate the associated fluxes in the same way we do for immersed boundaries --- because this has performance advantages for very large models). I agree, but this patch-up will not work for immersed boundaries anyway. I still advocate for (maybe not now but later down the line) a general line of thought that ensures consistency between immersed boundaries and ""regular"" boundaries (a la MITgcm) treating them always the same way. As an example, this issue could have been brought up for immersed boundaries, which would have required a (definitely more lengthy) rework of boundaries in abstract operations but would have solved the issue in both boundaries and immersed boundaries.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3224#issuecomment-1689967113:432,perform,performance,432,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3224#issuecomment-1689967113,1,['perform'],['performance']
Performance,"> Average reduction with conditional expressions, e.g., like; > ; > https://github.com/CliMA/Oceananigans.jl/blob/748feab10a55fa65a46455620203252a6fc0646e/test/test_field_reductions.jl#L107; > ; > induce scalar operations on the GPU. I guess it's not a surprise. I just had to add, e.g,; > ; > ```julia; > @compute Txyz = CUDA.@allowscalar Field(Average(T, condition=T.>3)); > ```; > ; > Only mentioning this here in case it might something in the source code that could be affecting code performance. The condition should be a function of `f(i, j, k, grid, args...)` returning a boolean, or an `AbstractArray` of booleans. . We should implement boolean operations with fields to return boolean fields, so we will be able to pass an `AbstractOperation`; ```; condition = T > 3 # This should be a boolean AbstractOperation; @compute Txyz = Field(Average(T; condition)) ; ```. at the moment the best way to specify a condition is through a function; ```; @inline condition(i, j, k, grid, T) = T[i, j, k] > 3; @compute Txyz = Field(Average(T; condition)) ; ```",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2701#issuecomment-1224162912:489,perform,performance,489,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2701#issuecomment-1224162912,1,['perform'],['performance']
Performance,"> Benchmark? The reason we combined the updates for velocities was a perceived performance gain. Probably we were wrong about that, but it'd be good to show it. Do I understand that to mean that you want to test the performance of the code before and after the PR? If there are tests that I can run to do this with both versions, I would be happy to try that on my desktop, only CPU, but that probably wouldn't be as nice as trying it on a better computer.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1210#issuecomment-734371151:79,perform,performance,79,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1210#issuecomment-734371151,2,['perform'],['performance']
Performance,> Better than multi-threading though. Them fighting words ;),MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/590#issuecomment-795861999:14,multi-thread,multi-threading,14,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/590#issuecomment-795861999,1,['multi-thread'],['multi-threading']
Performance,"> Billy mentioned it in some other comments but while we do splat args for some of the function calls, the function definitions use Varargs instead. This should avoid the catastrophic slowdown we saw with splatting earlier, but I agree that it should be tested. Do you have any good CPU performance tests set up @glwagner ?. Any simple test will do, for example one of the examples",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3480#issuecomment-2150416681:287,perform,performance,287,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3480#issuecomment-2150416681,1,['perform'],['performance']
Performance,"> Bonus, also the `mask_immersed_field` was missing. I don't expect it will be ever used (why using an immersed boundary grid in a single column?), however I have added a test for it. We can remove it if not needed. Good to have because it might be useful for testing and also, its definitely best if the single column mode is identical (except for performance) to 3D mode.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3756#issuecomment-2326759623:349,perform,performance,349,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3756#issuecomment-2326759623,1,['perform'],['performance']
Performance,"> But it gets worse. If the average of a field is taken, we zero out the halo regions. Looks like we can use `mean!` on views to fix this issue as they don't perform scalar operations (interestingly `mean` uses scalar operations). Not sure if it recently started working or if I was blind to `mean!` when we first started taking horizontal averages but if we are using `sum!` we must have considered `mean!`... ```julia; julia> using Statistics, BenchmarkTools, CUDA. julia> CUDA.allowscalar(false). julia> N = 512;. julia> Rgpu = randn(N+2, N+2, N+2) |> CuArray;. julia> Vgpu = @views Rgpu[2:N+1, 2:N+1, 2:N+1];. julia> vgpu = zeros(1, 1, N) |> CuArray;. julia> mean!(vgpu, Vgpu);. julia> @benchmark CUDA.@sync mean!(vgpu, Vgpu); BenchmarkTools.Trial: ; memory estimate: 2.08 KiB; allocs estimate: 85; --------------; minimum time: 2.427 ms (0.00% GC); median time: 2.567 ms (0.00% GC); mean time: 2.584 ms (0.00% GC); maximum time: 8.747 ms (0.00% GC); --------------; samples: 1930; evals/sample: 1; ```. which is basically the same speed as `sum!`. ```julia; julia> @benchmark CUDA.@sync sum!(vgpu, Vgpu); BenchmarkTools.Trial: ; memory estimate: 1.48 KiB; allocs estimate: 62; --------------; minimum time: 2.428 ms (0.00% GC); median time: 2.564 ms (0.00% GC); mean time: 2.566 ms (0.00% GC); maximum time: 3.228 ms (0.00% GC); --------------; samples: 1944; evals/sample: 1; ```. and ~34x faster than the CPU version (maybe we usually expect more but reduction operations aren't the best for GPUs). ```julia; julia> using Statistics, BenchmarkTools. julia> N = 512;. julia> Rcpu = randn(N+2, N+2, N+2);. julia> Vcpu = @views Rcpu[2:N+1, 2:N+1, 2:N+1];. julia> vcpu = zeros(1, 1, N);. julia> mean!(vcpu, Vcpu);. julia> @benchmark mean!(vcpu, Vcpu); BenchmarkTools.Trial: ; memory estimate: 0 bytes; allocs estimate: 0; --------------; minimum time: 85.751 ms (0.00% GC); median time: 86.201 ms (0.00% GC); mean time: 86.316 ms (0.00% GC); maximum time: 87.483 ms (0.00% GC); --------------; samp",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1063#issuecomment-708314963:158,perform,perform,158,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1063#issuecomment-708314963,1,['perform'],['perform']
Performance,"> By the way @tomchor, is this also the ratio that you find in your simulations (maybe with the latest commit)? If yes, I ll give up trying to inbound and only try to speed up the advection. I just tried with the latest commit and got this error:. ```julia; ERROR: LoadError: TaskFailedException. nested task error: UndefVarError: coeff_β_2_0 not defined; Stacktrace:; [1] left_biased_β(::Tuple{Float64, Float64}, ::WENO{2, Float64, Nothing, Nothing, Tuple{OffsetArrays.OffsetVector{Tuple{Float64, Float64}, Vector{Tuple{Float64, Float64}}}, OffsetArrays.OffsetVector{Tuple{Float64, Float64}, Vector{Tuple{Float64, Float64}}}, OffsetArrays.OffsetVector{Tuple{Float64, Float64}, Vector{Tuple{Float64, Float64}}}}, Nothing, true, Nothing, UpwindBiased{1, Float64, Nothing, Nothing, Nothing, Nothing, Centered{1, Float64, Nothing, Nothing, Nothing, Nothing}}, Centered{1, Float64, Nothing, Nothing, Nothing, Nothing}}, ::Val{0}); @ /glade/work/tomasc/.julia/packages/Oceananigans/dgCcB/src/Advection/weno_interpolants.jl:115 [inlined]; [2] overdub; @ /glade/work/tomasc/.julia/packages/Oceananigans/dgCcB/src/Advection/weno_interpolants.jl:115 [inlined]; [3] beta_loop(::WENO{2, Float64, Nothing, Nothing, Tuple{OffsetArrays.OffsetVector{Tuple{Float64, Float64}, Vector{Tuple{Float64, Float64}}}, OffsetArrays.OffsetVector{Tuple{Float64, Float64}, Vector{Tuple{Float64, Float64}}}, OffsetArrays.OffsetVector{Tuple{Float64, Float64}, Vector{Tuple{Float64, Float64}}}}, Nothing, true, Nothing, UpwindBiased{1, Float64, Nothing, Nothing, Nothing, Nothing, Centered{1, Float64, Nothing, Nothing, Nothing, Nothing}}, Centered{1, Float64, Nothing, Nothing, Nothing, Nothing}}, ::Tuple{Tuple{Float64, Float64}, Tuple{Float64, Float64}}, ::typeof(Oceananigans.Advection.left_biased_β)); @ /glade/work/tomasc/.julia/packages/Oceananigans/dgCcB/src/Advection/weno_interpolants.jl:163 [inlined]; [4] overdub; @ /glade/work/tomasc/.julia/packages/Oceananigans/dgCcB/src/Advection/weno_interpolants.jl:163 [inlined]; ",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2603#issuecomment-1186215132:265,Load,LoadError,265,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2603#issuecomment-1186215132,1,['Load'],['LoadError']
Performance,"> By the way, i didn't use the scheme described in #1704 because it didn't give great results when implemented. I have implemented the ENO coefficients as derived before the assumption of uniform grid to the stencil interpolation (tomorrow I'll go back and check the report which describes them) . The smoothness functions are kept the same. That's interesting. Does that mean you were not able to reproduce the results reported in ""[A simple algorithm to improve the performance of the WENO scheme on non-uniform grids](https://link.springer.com/article/10.1007/s10409-017-0715-2)""?. > > By the way, it would be quite easy to modify the type to include the order of the WENO scheme which we could change at will. What do you think? Would it be usefull to do that?; > ; > If it's easy, then I definitely vote that we do that! A lower order, faster WENO scheme may be a good compromise between computational cost and accuracy. If that's true, we could use it as the default scheme for models. I think a higher order WENO scheme is more important than lower-order scheme. However, a low-order WENO method is proposed [here](https://link.springer.com/article/10.1007/s10915-020-01164-6). I think the 3rd order scheme may be quite diffusive and runs the risk of limiting to 1st order (?!). #995 attempts to implement nth order WENO but I think failed to achieve good CPU performance / compilation on the GPU. But @simone-silvestri perhaps you're able to achieve this?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2060#issuecomment-969290093:468,perform,performance,468,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2060#issuecomment-969290093,2,['perform'],['performance']
Performance,"> Can we check with atmospheric clima model and with emerging DG hydrostatic ocean model, that z index increases upwards there too... best to have consistency across models. Last time this came up in https://github.com/climate-machine/Oceananigans.jl/issues/90 it seemed that `k` increases as you move away from the surface of the Earth (although they may have meant the center of the Earth? Otherwise DG ocean and atmosphere would have opposing conventions). Also mentioned was that the unstructured grid means sometimes you just do unstructured stuff and there's no `k` index I guess. @blallen Does increasing the vertical `k` index go up or down for the DG ocean model?. > Finite volume is good and hopefully need not slow model down. If we do move on to other grids, will be useful. Yeah for uniform grids it shouldn't. We have benchmarks we can check against. Things might slow down a bit with `z` operators, but that's to be expected. If we do shared memory right, it might not be by a huge amount. > Be good to chat next week. The general FV piece intersects with thinking about one day being able to do LES with bathymetry and still some performance. That would be good, haven't though much about topography. I'll be around.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/471#issuecomment-541423970:1146,perform,performance,1146,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/471#issuecomment-541423970,1,['perform'],['performance']
Performance,"> Can you put @inline in front of sponge_u_disc, etc?. Done. Same result. I also tried `ContinuousForcing` with `bottom_mask(x, y, z) = 1` and it's slower than its discrete counterpart. Apparently `DiscreteForcing` is a bit faster than `ContinuousForcing`, everything else being the same. > Again unsure if it affects performance but since rate is referenced as global it needs to be const; eg const rate = 1/10. Yes! That makes a big difference! I feel silly that I forgot that. With `const rate=1/10` and `DiscreteForcing` things are as fast as with no forcing. Using the same ""trick"" with `ContinuousForcing` doesn't change things though. So it does seem like the source of the issue is `ContinuousForcing`. I should say though, I'm having some trouble securing a GPU right now, so I haven't been able to run these tests on a GPU. Would a MWE help here?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1827#issuecomment-875702050:318,perform,performance,318,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1827#issuecomment-875702050,1,['perform'],['performance']
Performance,"> Can you try this branch and see if it goes better? `ss/fix-memory-issue`. It seems to solve the memory issue! But I do get an error when writing to disk:. ```; ERROR: LoadError: NetCDF error: size mismatch for variable 'dbdz' in file './data/out.PPN-R02F02A01.nc'. Trying to write (1022, 1022, 106) elements while [1014, 1014, 98, 1] are expected (NetCDF error code: -57); Stacktrace:; [1] _nc_check_size_put_vars(ncid::Int32, varid::Int32, countp::Vector{Int64}, op::Array{Float64, 3}); @ NCDatasets /glade/work/tomasc/.julia/packages/NCDatasets/ipGBH/src/netcdf_c.jl:973; [2] nc_put_vars(ncid::Int32, varid::Int32, startp::Vector{Int64}, countp::Vector{Int64}, stridep::Vector{Int64}, op::Array{Float64, 3}); @ NCDatasets /glade/work/tomasc/.julia/packages/NCDatasets/ipGBH/src/netcdf_c.jl:984; [3] setindex!; @ /glade/work/tomasc/.julia/packages/NCDatasets/ipGBH/src/variable.jl:460 [inlined]; [4] setindex!(::NCDatasets.Variable{Float64, 4, NCDatasets.NCDataset{Nothing}}, ::Array{Float64, 3}, ::Colon, ::Colon, ::Colon, ::Int64); @ NCDatasets /glade/work/tomasc/.julia/packages/NCDatasets/ipGBH/src/variable.jl:493; [5] setindex!(::NCDatasets.CFVariable{Float64, 4, NCDatasets.Variable{Float64, 4, NCDatasets.NCDataset{Nothing}}, NCDatasets.Attributes{NCDatasets.NCDataset{Nothing}}, NamedTuple{(:fillvalue, :missing_values, :scale_factor, :add_offset, :calendar, :time_origin, :time_factor), Tuple{Nothing, Tuple{}, Nothing, Nothing, Nothing, Nothing, Nothing}}}, ::Array{Float64, 3}, ::Colon, ::Colon, ::Colon, ::Int64); @ NCDatasets /glade/work/tomasc/.julia/packages/NCDatasets/ipGBH/src/cfvariable.jl:765; [6] save_output!(ds::NCDatasets.NCDataset{Nothing}, output::Field{Center, Center, Face, Oceananigans.AbstractOperations.Derivative{Center, Center, Face, typeof(∂zᶜᶜᶠ), Field{Center, Center, Center, Nothing, ImmersedBoundaryGrid{Float64, Periodic, Periodic, Bounded, RectilinearGrid{Float64, Periodic, Periodic, Bounded, Float64, Float64, OffsetArrays.OffsetVector{Float64, CuArray{Fl",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2794#issuecomment-1300963165:169,Load,LoadError,169,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2794#issuecomment-1300963165,1,['Load'],['LoadError']
Performance,"> Can't we just plot a fixed-color shape on top on the mount on all panels? [`Makie.poly`](https://docs.makie.org/stable/reference/plots/poly/) seems to be able to do that. We can. We can also mask the output after we load it, e.g, via. using something like. ```Julia; using Oceananigans.ImmersedBoundaries: mask_immersed_field!. function mask_and_get_interior(φ_t, n; value=NaN); mask_immersed_field!(φ_t[n], value); return interior(φ_t[n], :, 1, :); end. u′ₙ = @lift mask_and_get_interior(u′_t, $n); ```. which gives. https://github.com/CliMA/Oceananigans.jl/assets/7112768/a402bfcd-ef72-42d8-bbb9-97fb98be4fe3. But either of these solutions complicate the example a bit. Ideally, `mask_immersed_field!` should not be user-facing. And plotting a mountain on top of the mountain is a bit of a hack and would require a bit of explaining and justifying why we do that.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3132#issuecomment-1983782339:218,load,load,218,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3132#issuecomment-1983782339,1,['load'],['load']
Performance,"> Copied below.; > ; > ```; > ERROR: LoadError: `makedocs` encountered an error. Terminating build; > --; >   | Stacktrace:; >   | [1] error(s::String); >   | @ Base ./error.jl:33; >   | [2] runner(#unused#::Type{Documenter.Builder.RenderDocument}, doc::Documenter.Documents.Document); >   | @ Documenter.Builder /storage7/buildkite-agent/.julia-3423/packages/Documenter/4JDQo/src/Builder.jl:255; >   | [3] dispatch(#unused#::Type{Documenter.Builder.DocumentPipeline}, x::Documenter.Documents.Document); >   | @ Documenter.Utilities.Selectors /storage7/buildkite-agent/.julia-3423/packages/Documenter/4JDQo/src/Utilities/Selectors.jl:170; >   | [4] #2; >   | @ /storage7/buildkite-agent/.julia-3423/packages/Documenter/4JDQo/src/Documenter.jl:257 [inlined]; >   | [5] cd(f::Documenter.var""#2#3""{Documenter.Documents.Document}, dir::String); >   | @ Base.Filesystem ./file.jl:106; >   | [6] #makedocs#1; >   | @ /storage7/buildkite-agent/.julia-3423/packages/Documenter/4JDQo/src/Documenter.jl:256 [inlined]; >   | [7] top-level scope; >   | @ ~/builds/tartarus-9/clima/oceananigans/docs/make.jl:155; >   | in expression starting at /var/lib/buildkite-agent/builds/tartarus-9/clima/oceananigans/docs/make.jl:155; >   | 🚨 Error: The command exited with status 1; >   | user command error: exit status 1; > ```. This is only the top of the error. Farther down, we see. ```; ┌ Warning: failed to run `@example` block in src/generated/shallow_water_Bickley_jet.md:70-77; --;   | │ ```@example shallow_water_Bickley_jet;   | │ model = ShallowWaterModel(architecture = GPU(),;   | │ timestepper = :RungeKutta3,;   | │ advection = WENO5(),;   | │ grid = grid,;   | │ gravitational_acceleration = g,;   | │ coriolis = FPlane(f=f));   | │ ```;   | │ c.value = CUDA error (code 100, CUDA_ERROR_NO_DEVICE);   | └ @ Documenter.Expanders /storage7/buildkite-agent/.julia-3423/packages/Documenter/4JDQo/src/Expanders.jl:563; ```",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1881#issuecomment-887500390:37,Load,LoadError,37,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1881#issuecomment-887500390,1,['Load'],['LoadError']
Performance,"> Could you explain why using ifelse has better performance?; Is this because the ternary ? : is an alias for ifelse?. No, the ternary operator `a ? b : c` is shorthand for . ```julia; if a; b; else; c; end; ```. This `if`-statement (as well as the logicals `&&` and `||`) are _short-circuiting_. That is, `c` is guaranteed _not_ to run if `a === true`. For example. ```; julia> f(a, first) = first ? a[1] : a[2]; f (generic function with 1 method). julia> a = rand(1); 1-element Vector{Float64}:; 0.6018054291910822. julia> f(a, true); 0.6018054291910822. julia> f(a, false); ERROR: BoundsError: attempt to access 1-element Vector{Float64} at index [2]; Stacktrace:; [1] getindex; @ ./array.jl:805 [inlined]; [2] f(a::Vector{Float64}, first::Bool); @ Main ./REPL[7]:1; [3] top-level scope; @ REPL[10]:1; ```. The first call to `f(a, true)` executes without problems, because the second _branch_ isn't executed at all. On the other hand. ```julia; julia> g(a, first) = ifelse(first, a[1], a[2]); g (generic function with 1 method). julia> g(a, true); ERROR: BoundsError: attempt to access 1-element Vector{Float64} at index [2]; Stacktrace:; [1] getindex; @ ./array.jl:805 [inlined]; [2] g(a::Vector{Float64}, first::Bool); @ Main ./REPL[11]:1; [3] top-level scope; @ REPL[12]:1. julia> g(a, false); ERROR: BoundsError: attempt to access 1-element Vector{Float64} at index [2]; Stacktrace:; [1] getindex; @ ./array.jl:805 [inlined]; [2] g(a::Vector{Float64}, first::Bool); @ Main ./REPL[11]:1; [3] top-level scope; @ REPL[13]:1; ```. `ifelse` is _not_ short-circuiting --- _both_ branches are executed, even though only the correct value is returned:. ```julia; julia> b = rand(2); 2-element Vector{Float64}:; 0.5340042876487958; 0.7031634999748222. julia> g(b, true); 0.5340042876487958. julia> g(b, false); 0.7031634999748222; ```. It's easier for the compiler to optimize code that involves `ifelse`, especially on the GPU. The reason is that it's allowed to execute all code on both branches. If w",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2336#issuecomment-1066115583:48,perform,performance,48,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2336#issuecomment-1066115583,1,['perform'],['performance']
Performance,"> Distributed IO is not supported yet and will require a lot of work and refactor so for the time being we have to do with the rank in the output writers. Distributed IO is not necessary to solve this problem. We can introduce a convention for filenames whereby the rank number is automatically inserted into the filename for distributed simulations. Similarly the convention can be reversed for `FieldTimeSeries` to make the process seamless for users. There are probably plenty of other possible solutions. The first step is recognizing that a problem or deficiency exists, and having the desire to fix it. Then we can do the fun part, which is to design solutions to an important problem. Note that distributed IO is unlikely to ever be supported for JLD2. So to provide a seamless experience with JLD2, we really do require a filename convention to solve the problem. (For NetCDF, we could envision providing a choice between file splitting or distributed IO.). Filename conventions also seem preferred to me for many cases in a world of GPUs where each rank likely needs to hold a substantial portion of the computation for performance. Splitting files by rank will limit the filesize, which makes it easier to transfer data when the simulations are very large --- which will probably often be the case when doing distributed simulations.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3429#issuecomment-1895891029:1129,perform,performance,1129,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3429#issuecomment-1895891029,1,['perform'],['performance']
Performance,"> Do I understand that to mean that you want to test the performance of the code before and after the PR? If there are tests that I can run to do this with both versions, I would be happy to try that on my desktop, only CPU, but that probably wouldn't be as nice as trying it on a better computer. We would like to test the performance of the version of Oceananigans on this PR versus Oceananigans#master on the CPU and GPU and for a variety of problem sizes. Maintaining good performance is a top priority of ours. Generally speaking we would like to avoid performance regressions --- even small ones (which accumulated over many PRs could become significant).",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1210#issuecomment-734413997:57,perform,performance,57,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1210#issuecomment-734413997,4,['perform'],['performance']
Performance,"> For non local directions, if `storage` and `plan` are `unified_array`s, the non local FFT can be performed by permuting the partitioning of the `MultiRegionGrid` without having to transpose memory (that will happen under the hood thanks to unified memory). Just `storage` is an array; `plan` is a `CUFFT` object, not an array. If we use `cufftxt` would this happen be default?. ie with `cufftxt` we have to build a unified `storage` (and maybe unified eigenvalues). Then provided we can fill up storage correctly, and empty it correctly at the end, the thing that's left is to ""just do"" the fft (transposes etc handled under the hood). Does broadcasting work with unified memory arrays?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2523#issuecomment-1119868092:99,perform,performed,99,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2523#issuecomment-1119868092,1,['perform'],['performed']
Performance,"> Hello, @amontoison. Nice work. Do you see a performance improvement when switching to this package? There should be some benchmarks in the `benchmark` folder that we can test (probably we need to update that folder a bit, let me know if you have problems with it). Hi @simone-silvestri, `cg` is not too hard to implement, so I don't think we will see a significant performance improvement on CPU. ; However, for other methods like `gmres`, `Krylov.jl` easily outperforms `IterativeSolvers.jl`.; On GPU, though, we might see a difference because I try to dispatch to the BLAS/LAPACK routines of the GPU vendors as much as possible, whereas `IterativeSolvers.jl` relies on some broadcast. Also, to the best of my knowledge, only `cg` works on (NVIDIA) GPUs for `IterativeSolvers.jl`, while all solvers in `Krylov.jl` work on the GPUs of any vendor. I'll try to run some benchmarks before the end of the week.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3778#issuecomment-2373076041:46,perform,performance,46,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3778#issuecomment-2373076041,2,['perform'],['performance']
Performance,"> Hmm. I believe that scalar operations are (or were) _specifically_ allowed globally in our `runtests.jl`, overriding any default. I was considering deprecating that option though, so that scalar iteration would be disabled by default and can only be allowed for a limited number of statements using `@allowscalar` or `allowscalar() do ... end`, because it's such a performance trap. Is there so much scalar iteration being triggered by the Oceananigans tests?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1740#issuecomment-863772729:367,perform,performance,367,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1740#issuecomment-863772729,1,['perform'],['performance']
Performance,"> Hmm... is this called every time Oceananigans is _run_, or only when it is _built_? The number of threads at build-time may not be equal to the number of threads desired at run-time... ?. It's at run-time (it worked for #869). From https://docs.julialang.org/en/v1/manual/modules/#Module-initialization-and-precompilation. > In particular, if you define a function `__init__()` in a module, then Julia will call `__init__()` immediately after the module is loaded (e.g., by `import`, `using`, or `require`) at runtime for the _first_ time (i.e., `__init__` is only called once, and only after all statements in the module have been executed).",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/868#issuecomment-681952346:459,load,loaded,459,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/868#issuecomment-681952346,1,['load'],['loaded']
Performance,"> Hmmm, I was thinking it would be good to benchmark each operator at least once but I suppose if sin is fast then we can assume cos and tanh will also be fast. I think the proper benchmark is to compare a unary operation via one function of our choice (say, `sin`) and a hand-coded benchmark for the same function. We don't benchmark additional functionality of `AbstractOperations` by testing multiple unary functions; those would just benchmark the performance of the unary function itself, which we don't care about (because we are not going to improve the performance of `Base.sin` or `CUDAnative.sin`.)",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/463#issuecomment-545895227:452,perform,performance,452,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/463#issuecomment-545895227,2,['perform'],['performance']
Performance,"> I am on the fence. In the cubed sphere aquaplanet (not even MPI but just on one GPU) the gain of performance is a factor 5 by using the efficient split explicit rather than filling the halos at each substep. We do not have to tackle this problem here or now, but we have keep in mind that fill halo is very inefficient and probably not the way to go. Maybe @ali-ramadhan finds a better way to have a gpu-compatible code that does not require adding boundary conditions in the operators. At least that is what @siddharthabishnu told me, I am not sure about the timings, can you confirm @siddharthabishnu?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3268#issuecomment-2353979984:99,perform,performance,99,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3268#issuecomment-2353979984,1,['perform'],['performance']
Performance,"> I am on the fence. In the cubed sphere aquaplanet (not even MPI but just on one GPU) the gain of performance is a factor 5 by using the efficient split explicit rather than filling the halos at each substep. We do not have to tackle this problem here or now, but we have keep in mind that fill halo is very inefficient and probably not the way to go. Maybe @ali-ramadhan finds a better way to have a gpu-compatible code that does not require adding boundary conditions in the operators. For example, if the boundary is fixed and not dependent on the interior variables, it might be possible just to fix the BC once before the sub stepping and just iterate without needing to use any update",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3268#issuecomment-2353997319:99,perform,performance,99,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3268#issuecomment-2353997319,1,['perform'],['performance']
Performance,"> I believe in this case that the function d.▶ from the above is identity, which could fail because it's also called in the calculation of the binary operation w * v?. I gotta be honest that whole discussion went right over my head, haha. I think mostly because I don't really understand what's the exact use of `identities` and what the functions `d.▶` actually do. . Am I correct in assuming that if I specify where each calculation is performed (by using `@at`) I can decrease the computational complexity and maybe make `ComputedField`s easier to compile on GPUs?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1401#issuecomment-786854048:438,perform,performed,438,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1401#issuecomment-786854048,1,['perform'],['performed']
Performance,"> I could use some explanations on your closure usage. @glwagner is the mastermind behind the implementation of the LES closures. It's still not documented but he's written up some documentation about them in a different package: https://dedales.readthedocs.io/en/latest/summaryclosures.html. > If it was up to me I would prefer an extended documentation of GPUifyLoops ;) I only understand how it works by reading what you do with it. Yeah GPUifyLoops could do with an extra example or two... I have an old and still open PR showing an example of a 3D stencil computation but you probably already know how to do this: https://github.com/vchuravy/GPUifyLoops.jl/pull/18. Ah interesting, yeah I found papers online that described a 50-80% speedup but sounds like a lot of manual work which Julia may be able to do for us :) We've barely thought about CPU performance let alone SIMD but maybe something like GPUifyLoops.jl can figure out the multicore SIMD code. > I will try to use part of your code to rewrite the toy 2D CFD solver I have translated from Matlab (https://discourse.julialang.org/t/asynchronous-makie/27127/9?u=laurentplagne). That's awesome! Do let us know if you have any questions. That's a pretty cool use of Makie with `fillrange=true`!",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/333#issuecomment-518441255:854,perform,performance,854,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/333#issuecomment-518441255,1,['perform'],['performance']
Performance,"> I don't know exactly what to do, so we'll need help. I've started with a post to the julia slack: https://julialang.slack.com/archives/C67910KEH/p1635909337281400; > ; > On the other hand, it may not be a huge issue if it only affects diagnostics that are evaluated fewer than 5 times. For long running simulations, our methods do eventually get cached? Do you have an example of a method that doesn't get cached after 5 evaluations?. ~When I was running an actual simulation this issue seemed to persist after many evaluations (i.e. writings to disk), and not just the first few times. So it may be that doing it 5 times consecutively on the REPL is different from the simulation behavior. That said, I didn't pay a ton a attention to that, so I may have been wrong.~. ~I'll try to investigate this further tomorrow on my end if I have time.~",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2024#issuecomment-958632710:348,cache,cached,348,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2024#issuecomment-958632710,2,['cache'],['cached']
Performance,"> I don't think we ever used `power_by_squaring`, because the ""pre 1.6"" Oceananigans kernels were defined via`KernelAbstractions`, which in turn translated `^(a, b)` to `CUDA.pow(a, b)`:. I know, I was thinking about what would be a good ""new"" default behavior. CUDA.jl used to truncate Int64 exponents to Int32, resulting in `powi` use, but although that was good for Oceananigans it's a bad default. Using `pow` after casting the exponent to Float64 seemed like a good idea, but is apparently slow in your use case. `Base.power_by_squaring` is slow in microbenchmarks, but may perform better in a realistic application? If that's the case, I could change the implementation of `^` in CUDA.jl. But I also realize now that exponentation by a small constant, which you seem to be doing here, should probably be handled differently. Base has `literal_pow`, not sure why that isn't kicking in here. Feel free to file an issue about that.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1770#issuecomment-870021086:579,perform,perform,579,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1770#issuecomment-870021086,1,['perform'],['perform']
Performance,"> I guess I also see timers as a debugging tool for users. Totally agree since simulation performance can depend entirely on performance of forcing functions, boundary conditions, and diagnostics; not just code that ships in src.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1862#issuecomment-888341027:90,perform,performance,90,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1862#issuecomment-888341027,2,['perform'],['performance']
Performance,"> I just pushed changes for the geostrophic adjustment case. The code seems alright, but it returns the following warning:; > ; > `warning: ~/.julia/packages/KernelAbstractions/GCOhX/src/extras/loopinfo.jl:28:0: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering`; > ; > Then it returns the following error:; > ; > Error output:; > Any idea what it could be, @simone-silvestri ?. That's related with #3374 and the warning comes from Julia v1.10 (wasn't there with Julia v1.9).; The PR #3403 will deal with these warnings.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3429#issuecomment-1895005561:235,optimiz,optimizer,235,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3429#issuecomment-1895005561,2,"['optimiz', 'perform']","['optimizer', 'perform']"
Performance,"> I never understood why a `CFL` object had to be pre-constructed, but I always assumed there was a good (probably GPU-related) reason for that. I guess it's helpful if we have competing functions for computing time-scales (for performance reasons). But if we have just one fast function then it doesn't seem like there's a good reason.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2037#issuecomment-958584552:228,perform,performance,228,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2037#issuecomment-958584552,1,['perform'],['performance']
Performance,"> I over-simplified. FFTs are efficient when there are a small number of prime divisors --- 2, 3, 5, 7 --- of the length of the sample. 100 may actually be ok because it is 2*2*5*5. I don't feel strongly, but you will find that most people running simulations are using powers of 2 and 3 (128, 192, 256, 384, 512, 768, 1024) --- even when their code has no FFTs! Perhaps the purpose of this convention is so people who _do_ use codes that rely _heavily_ on FFTs are able to run code comparisons. I find the convention to be an appropriate best practice. An 1D FFT of length 257, for example, is 4 times slower than an FFT of length 256 on my machine. For tests, of course, using small numbers of prime divisors is completely irrelevant; I just want to encourage best practices and promote awareness of them, and also implicitly emphasize to users that this code uses FFTs, and don't want random person X to read the tests and scratch their head and ask ""doesn't this code rely on FFTs?"" (like I did when I saw the examples). I don't disagree with anything here. I just think users should be able to run using whatever resolution they want and know that the code has been tested and works with weird inefficient resolutions. To encourage users maybe we can print a ""performance warning"" if the grid resolution is not the product of a small number of prime divisors?. > I certainly prefer that. It may be academic because the architecture should probably be a parameter of `Model`. That makes sense. I think we should just be wary of having too many parametric types but architecture probably warrants being one as we'll dispatching on it so frequently.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/97#issuecomment-468537451:1265,perform,performance,1265,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/97#issuecomment-468537451,1,['perform'],['performance']
Performance,"> I see a problem. The kernels seem to make an assumption about a relationship between the axes of the data arrays and the size of the blocks. It seems that generalizing this problem will involve rewriting all of the kernels... ?. Hmmm, I don't immediately see it, but yes for example the kernels that perform vertical integrals do assume this. Actually, the vertical integrals are precisely fast because we use `NT=2` for them. I don't think kernels need to be rewritten, we just need to use an appropriate thread-block layout with `NT=2` in this example.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/249#issuecomment-496520500:302,perform,perform,302,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/249#issuecomment-496520500,1,['perform'],['perform']
Performance,"> I see that one of the errors is `UndefVarError: device_event not defined`, which I suppose means we need to load it?. Indeed, I'll fix that.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1913#issuecomment-890028428:110,load,load,110,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1913#issuecomment-890028428,1,['load'],['load']
Performance,"> I share here a double drake experiment that makes full use of this PR; > ; > #### Grid setup; > * latitude longitude grid from 75 S to 75 N; > * 1/3 of a degree in the horizontal (1080 points in longitude, 450 in latitude); > * 150 exponentially stretched vertical levels for a 3km deep ocean; > * double drake bathymetry (https://doi.org/10.1175/2009JCLI3197.1); > ; > #### Top BC:; > * temperature: restoring to reference profile (cosine shape); > * salinity: prescribed latitudinally dependent surface flux; > * zonal velocity: prescribed latitudinally dependent wind stress; > ; > #### Bottom BC:; > * velocities: linear bottom drag with a drag coefficient of 0.003 ms⁻¹; > ; > #### Initial conditions; > * zero velocities; > * exponentially stratified temperature with SST equal to the reference temperature; > * constant salinity; > ; > #### Model setup; > * linear equation of state; > * Richardson-based diffusivity for BL mixing; > * vertical background viscosity and diffusivity of 5e-4 and 3e-5, respectively; > * vector invariant momentum advection with WENO for vorticity and divergence flux as well as vertical transport (no horizontal viscosity); > * WENO for tracer advection (no horizontal diffusivity); > * Split explicit free surface using an averaging shape function and a CFL of 0.7 (23 substeps per time step); > ; > #### Simulation setup; > * time step of 10 minutes; > * ran on 2 MPI processes with CUDA-aware MPI; > * performs about 10 simulated years per day; > ; > On the left, there is the free surface evolution, on the right the surface vertical vorticity (evolved for 9 years). This would be a good setup to add to `ClimaOcean.jl`.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2888#issuecomment-1431581342:1445,perform,performs,1445,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2888#issuecomment-1431581342,1,['perform'],['performs']
Performance,"> I think prior to the changes we grouped into our ""upgrade to julia 1.6"" we were using the function `CUDA.pow` (from an ancient `CUDA.jl` version). Ah, in that case we were doing something pretty bad: https://github.com/JuliaGPU/CUDAnative.jl/blob/199f9b8ebb1f2fc9d7c14547efa9bcc14f4130c3/src/device/cuda/math.jl#L209. The new default, converting Int64 exponents to Float64, seems better. Unless `power_by_squaring` performs better in realistic applications?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1770#issuecomment-869833684:417,perform,performs,417,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1770#issuecomment-869833684,1,['perform'],['performs']
Performance,"> I think something like you're proposing would have added to my confusion. Isn't the confusion a problem with the output writers API? I think it sounds like a great idea to support `AbstractOperation` output. It's kind of logical. Doing this even allows us to do some clever stuff behind the scenes, like using one underlying array to store the results of multiple computations performed serial (thus saving memory).",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2235#issuecomment-1036767779:379,perform,performed,379,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2235#issuecomment-1036767779,1,['perform'],['performed']
Performance,"> I think that should be revisited when we decide to take on the more challenging issue of checkpointing partially-accumulated time-averages so that windowed_time_average always works as intended when picking up from checkpoints. good point!. There is a possibility that it is not very hard. It will require reshuffling code (which I can do), but with Julia we can serialize objects to disk and then load them back seamlessly in a single line... which might be all we need. The only limitation of serialization is that we haven't yet figured out how to serialize _methods_ (eg functions) which prevents us from serializing entire models. When functions are not involved things can be pretty simple.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3721#issuecomment-2379890956:400,load,load,400,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3721#issuecomment-2379890956,1,['load'],['load']
Performance,> I think the new diffusion/cosine tests? Can't think of something else... Ah indeed. It's also possible that some PRs caused a compile-time regression. Hopefully not a performance regression though. We should test that.,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2324#issuecomment-1062527828:169,perform,performance,169,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2324#issuecomment-1062527828,1,['perform'],['performance']
Performance,"> I think this makes things more intuitive, no? Are there any downsides besides a slightly longer building time for `model`?. There's no downside. `update_state!` has to be efficient for the model to run so I don't think there's a performance issue. I think it's more ""correct"", since without that call the auxiliary state may be wrong initially.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1889#issuecomment-885340708:231,perform,performance,231,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1889#issuecomment-885340708,1,['perform'],['performance']
Performance,> I think to do this cleanly we might want to follow up with what @vchuravy was doing here #3042. I tried experimenting with this on the OceanBioME tests and it seems to be preventing it from segfaultinig https://github.com/OceanBioME/OceanBioME.jl/pull/190 (and testing running them both at the same time here https://github.com/OceanBioME/OceanBioME.jl/pull/196). But it does also seem to run quite a bit slower so I'm going to see how much I can get it to cache without it causing problems. And then I can copy the implementation over to here.,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3662#issuecomment-2265916968:459,cache,cache,459,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3662#issuecomment-2265916968,1,['cache'],['cache']
Performance,"> I think to preserve the work in this PR, we should add a `Float32` test which will fail if a spurious promotion undermines performance. Agreed. I'll revisit this PR later to see if I can find where the conversion happens. The test I added only checks to see if we can take a time step. But I should be able to also add a test to ensure no spurious promotion occurred.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3876#issuecomment-2445251168:125,perform,performance,125,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3876#issuecomment-2445251168,1,['perform'],['performance']
Performance,"> I think we can achieve this by splitting a kernel like calculate_interior_source_terms! into two kernels, one that computes source terms ""near"" the boundary (1-2? grid points from any boundary as needed), then halo communication can happen while a second more compute-intensive kernel computes the source terms in the rest of the interior. Don't you want the opposite? You want a kernel that computes source terms in the ""deep interior"", which can be performed without knowledge of halos and thus can be performed simultaneous to performing communication. After communication + deep interior calculations are complete, you then perform calculations on near-boundary elements. Are communications restricted to `fill_halo_regions!` and the pressure solve? If so, we can start to prepare for such optimizations by refactoring the time-stepping slightly. This is is the part of our current algorithm that involves interior tendency computation (there are additional halo filling calls associated with the fractional step):. ```julia; function calculate_explicit_substep!(tendencies, velocities, tracers, pressures, diffusivities, model); time_step_precomputations!(diffusivities, pressures, velocities, tracers, model); calculate_tendencies!(tendencies, velocities, tracers, pressures, diffusivities, model); return nothing; end; ```. The function `calculate_tendencies!` calculates interior and boundary contributions to tendencies and does not involve communication. The function `time_step_precomputations!` is. ```julia; function time_step_precomputations!(diffusivities, pressures, velocities, tracers, model). fill_halo_regions!(merge(velocities, tracers), model.boundary_conditions.solution, model.architecture,; model.grid, boundary_condition_function_arguments(model)...). calculate_diffusivities!(diffusivities, model.architecture, model.grid, model.closure, model.buoyancy,; velocities, tracers). fill_halo_regions!(diffusivities, model.boundary_conditions.diffusivities, model.architecture, ",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/615#issuecomment-583379290:453,perform,performed,453,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/615#issuecomment-583379290,5,"['optimiz', 'perform']","['optimizations', 'perform', 'performed', 'performing']"
Performance,"> I think we should distinguish between output we need when we run the model, and output we may eventually provide to the broader community. Ideally, formats for both would be the same, but this may not be the best solution. E.g., netCDF has obvious disadvantages but is still widely used. That does not mean it should be the output format we use by default (although we may want to provide model statistics in netCDF in the end, if a few years from now this is still what everyone uses). So: separate the discussion of what is best for us now from what we should provide (e.g., in any CMIPx archive down the line). Zarr and HDF both seem worth discussing.; > ; > Also important to keep in mind in this discussion: Our workflow will be different from most standard models, which write out instantaneous output that then is post-processed to get statistics etc. We will have to accumulate statistics on the fly, and we can (and should) forgo most instantaneous output, at least for the atmosphere. The model will learn from the accumulated statistics. Otherwise, with instantaneous output, the data volume, especially with embedded LES, will create an I/O and data transfer bottleneck that will limit us, and, e.g., will limit our ability to use distributed computing platforms. I agree. Ideally we'd support different formats (e.g. NetCDF, JLD2, HDF, Zarr, etc.) and have the option to use the best format for your application. We can already switch between output writers and choose which field(s)/diagnostics to output but we only do binary, NetCDF, and JLD for now. We were just focusing on NetCDF for our short-term needs, but this will definitely be a challenge for large problems.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/145#issuecomment-476303978:1173,bottleneck,bottleneck,1173,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/145#issuecomment-476303978,1,['bottleneck'],['bottleneck']
Performance,"> I think you may need @inline in front of most of those functions (only matters for CPU). ~That was my first attempt, but I get an error saying `ERROR: LoadError: LoadError: -(grid.Lz) + grid.Δz * (k - 1 / 2) is not a function expression`. I guess I need to choose between `@inline` or `@inbounds`?~. Nevermind, I was doing something very dumb. Inlining gives me exact same performance as not inlining (0.20% of the simulation). I guess the compiler is getting smarter about inlining. > If the slow down is the same for DiscreteForcing then the problem may really just be evaluating exp, sadly... You could try @inline bottom_mask(k) = 1 to test... I'll try that. Although I have tried non-exponential masks in the past with a similar slowdown, so I'm not sure if that's the issue.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1827#issuecomment-875673783:153,Load,LoadError,153,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1827#issuecomment-875673783,3,"['Load', 'perform']","['LoadError', 'performance']"
Performance,"> I would advocate again for moving both CUDA and AMDGPU support into package extensions (see #3066 for an outdated start); > ; > Having the user install both AMDGPU and CUDA unconditionally is both space and time consuming, loading them both should be unnecessary on most systems and they may at times be incompatible with each other since both are developed independently from each other. That's a good idea. Maybe we can start by developing AMD support (this PR) in an extension, and then move CUDA after that (just to relieve some pressure on @fluidnumerics-joe).",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3468#issuecomment-1935396471:225,load,loading,225,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3468#issuecomment-1935396471,1,['load'],['loading']
Performance,"> I'll explain quickly what is happening to document it. In this new PR I added a new way to check boundary stencils for advection so that the correct reconstruction method is always used.; > ; > This entails checking differently for `Face` reconstructions (where we have to ensure that `Center` locations are active) and vice versa for `Center` reconstructions where `Face` locations have to be active.; > ; > The problem occurs when checking the last cell for `Center` reconstructions (on a `Periodic` direction) let's say the advection is centered order 4 so hypothetically it requires two halo points. We then need to ensure that the nodes at `N + 1` and `N + 2` are active. A `Face` node (i) is active if either centered cell (i) or (i+1) is active, which means that the check will be performed on cells `N+1`, `N+2` and `N+3` (one more than the required halo size = 2!).; > ; > This is not a problem for a underlying grid where the `inactive_node` function can check out-of-bounds locations and will just return a `true`. On the other hand, it is a problem for an `ImmersedBoundary` where a conditional has to be evaluated against an AbstractArray.; > ; > My first solution was to increase by one the halo under the hood in the `ImmersedBoundaryGrid` constructor. This bug with `set!` demonstrated that this is probably not the best solution as this can have a lot of unwanted repercussions. The way I implemented it now is that, when performing the halo checking in the model constructor, if the grid is an `ImmersedBoundaryGrid`, the `required_halo` is incremented by one, and an appropriate warning message is displayed. This was the problem. So in the end I decided to just require one additional halo for the immersed boundary grid",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2603#issuecomment-1181806144:790,perform,performed,790,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2603#issuecomment-1181806144,2,['perform'],"['performed', 'performing']"
Performance,"> I'm a little worried about the increase in size of the already-quite-large time_step! function, but otherwise as far as I can tell things look ok. I agree and at some point in this PR I switched to using broadcasts for the permutations which gets rid of all the `idct_permute!` kernels and keeps one `calculate_pressure_source_term` kernel, but I couldn't get it to perform well on the GPU so I scrapped it. Worth revisiting... > One question: is it the name of the field that needs to be known for halo regions, or the type (FaceFieldZ, FaceFieldX, etc). If its the type, then this suggests two improvements:. Yeah it's kind of hacked together for now. When you want to fill halo regions you feed in a tuple like `(:u, bcs, u_data)` which fills the halos of `u_data` assuming it's a u-velocity field with boundary conditions `bcs`. It kind of crap because for pressure halos I fill them like I would fill in the w-velocity field. This makes the code confusing I think. Might be good to attach boundary conditions to each field, so that all fields get assigned boundary conditions, not just `u, v, w, T, S`. Would be great if we can can adapt fields to work well on GPUs. I think last time we tried we couldn't get them to perform for some reason. One of our oldest issues: https://github.com/climate-machine/Oceananigans.jl/issues/13",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/290#issuecomment-506899367:368,perform,perform,368,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/290#issuecomment-506899367,2,['perform'],['perform']
Performance,"> I'm not sure if `cuStreamQuery` being called 400,000 times is an error with our code, an error with CUDA.jl, not an error at all, or an error with my profiling. I didn't know this was a KA.jl-based GPU workload when commenting on Slack. The dependency/event model of KernelAbstractions.jl also uses stream queries (i.e. `cuStreamQuery`) when selecting a new stream. Maybe that's the source of these calls. It'd be good to figure out where they come from: if it's from CUDA.jl, and thus presumably because of calling the `synchronize` function, (1) why are you synchronizing that much [1], and if it's for good reasons (2) does it hurt performance and should we tweak our `synchronize` implementation to perform fewer stream queries?. [1]: some synchronization happens implicitly, e.g. when copying memory to or from the CPU (https://github.com/JuliaGPU/CUDA.jl/blob/6758fcab7ae0d72659a1ca0d56ad2c86d3b451f1/src/array.jl#L385-L399). One way to avoid some of those synchronizations, is by using pinned memory, but that's up to the application.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1912#issuecomment-900059869:637,perform,performance,637,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1912#issuecomment-900059869,2,['perform'],"['perform', 'performance']"
Performance,> I'm not sure if we've tested but I've assumed there is a performance benefit to the simpler version for regularly spaced grids rather than using the binary search. @simone-silvestri have you ever benchmarked this?,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3356#issuecomment-1775545546:59,perform,performance,59,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3356#issuecomment-1775545546,1,['perform'],['performance']
Performance,"> I'm still confused about how this changes what was previously done. Did we previously create arrays on the GPU, copy checkpoint from the CPU to ""temporary"" GPU arrays, and then copy from temporary GPU arrays to previously-instantiated model data?. Yeah this is what we were previously doing in `restore_fields!`:; https://github.com/climate-machine/Oceananigans.jl/blob/8352e56f5839b23d3441f6f8bd0f297f3e0b508f/src/OutputWriters/checkpointer.jl#L102. which is kinda stupid stupid because you create a temporary `CuArray` then copy elements over (we already allocated memory for the fields in the `model`). We could have done it with a `copyto!(::CuArray, ::Array)` which would avoid unnecessary allocations except for one temporary array. Now we construct the fields with the restored data and pass it to the model constructor so there are no temporary arrays and zero unnecessary allocations.; https://github.com/climate-machine/Oceananigans.jl/blob/4ed366019c3f6a9b3ba9cf19691fef721204ea3c/src/OutputWriters/checkpointer.jl#L110-L114. > is it possible to load data from disk directly to the GPU?. Hmmm, not sure but doesn't seem impossible. At the lowest level it'll have to do some host to device copies though I think. @leios or @vchuravy would know.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/628#issuecomment-589160127:1059,load,load,1059,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/628#issuecomment-589160127,1,['load'],['load']
Performance,"> I'm wondering if we should provide a separate page on ""Using GPUs""? While the simulation tips for CPUs are really performance optimizations that are optional, the GPU simulation tips are mostly required to run without errors. That's a good point. Although I think we could avoid creating another page and put that information in the [""Using GPUs""](https://clima.github.io/OceananigansDocumentation/stable/using_gpus/) page, so that things are more condensed.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1543#issuecomment-818364275:116,perform,performance,116,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1543#issuecomment-818364275,2,"['optimiz', 'perform']","['optimizations', 'performance']"
Performance,"> If I understand correctly this would make the `field_slicer` argument for output writers obsolete, no?. I missed this! Yes, that's correct. We _could_ keep it, however I think it's nicer to remove because it's one of the best ways to ensure that the ""windowing infrastructure"" implemented in this PR is performant and easy to use. In place of `field_slicer`, we will add some convenience arguments (`indices` and `with_halos`) that allow output fields to be ""constructed"" from what's given to `OutputWriter`. This will also allow us to close #2242 (but I haven't implemented that yet here).",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2246#issuecomment-1044762888:305,perform,performant,305,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2246#issuecomment-1044762888,1,['perform'],['performant']
Performance,"> If I understand this issue correctly, we are facing a basic trade-off between (compiler?) performance and the use of convenient but complicated abstraction objects?. It's a hardware limitation, really. The compiler could anticipate though, e.g. by not passing very large objects by value, or by providing an escape hatch (like the `Ref` suggestion in https://github.com/JuliaGPU/CUDA.jl/issues/267). You can experiment with this yourself, by changing which arguments get tagged `byval` in https://github.com/JuliaGPU/GPUCompiler.jl/blob/master/src/irgen.jl#L607, and changing the logic that packs arguments in https://github.com/JuliaGPU/CUDA.jl/blob/master/lib/cudadrv/execution.jl#L8-L37 accordingly (to pass a pointer to a pointer instead of a pointer to a value).",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/746#issuecomment-655304824:92,perform,performance,92,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/746#issuecomment-655304824,1,['perform'],['performance']
Performance,"> If forcing functions are expensive there may be simpler ways to do performance optimization though. For example, we can recommend piecewise linear masking functions for sponge layers instead of tanh or exp. From my tests I think this optimization can shave off 10% of time on the CPU and 25% on the GPU. This isn't bad, but I anticipate launching `Forcing` only where necessary would shave of much more in the situations I mentioned. Although when I originally posted the issue I thought it could be directly implemented in `Forcing`, but you make a good point that it can't, so it might not be worth the effort (at least not right now). I'll close this for now but feel free to re-open it if you think it's worth discussing.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3525#issuecomment-2033489092:69,perform,performance,69,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3525#issuecomment-2033489092,3,"['optimiz', 'perform']","['optimization', 'performance']"
Performance,"> If it doesn't slow the tests down, I'm happy to merge and we can consider changing the optimization level in the future if its warranted. I think being explicit and deliberate about it is a positive change. Looks like this makes a significant difference as the latest build on this PR took 25 minutes 1 second (https://buildkite.com/clima/oceananigans/builds/333). Previous 3 builds (which all passed) took 32~33 minutes:; https://buildkite.com/clima/oceananigans/builds/330; https://buildkite.com/clima/oceananigans/builds/331; https://buildkite.com/clima/oceananigans/builds/332. > We do have a small number of tests that might benefit from faster code, like the pressure solver convergence tests?. Yes but pressure solver FFTs go through the FFTW C library where the `julia -O` flag doesn't affect performance. If we want some performance optimization for specific test sets we can do that, e.g. `-O2` on unit tests.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1078#issuecomment-712939378:89,optimiz,optimization,89,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1078#issuecomment-712939378,4,"['optimiz', 'perform']","['optimization', 'performance']"
Performance,"> If we assume that reduced fields ""lives"" nowhere in the reduced direction, then we can remove the k index and just perform an ""immersed-boundary-unaware"" derivative. > On the other hand, if the reduced field lives on the whole reduced column (like for example an integral) then we need to be aware of the ""immersed column"", because if the whole column is immersed then the derivative should return a zero. What is the difference between ""living nowhere"" and ""living on the whole column""?. I think the point of reduced fields is that they are derived from a reduction over one or more dimensions. Since they are derived from a reduction, they invoke values from every element in the column / reduced direction. I don't understand what it means to ""live nowhere"".",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3588#issuecomment-2099199410:117,perform,perform,117,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3588#issuecomment-2099199410,1,['perform'],['perform']
Performance,"> Implement a multi GPU pressure solver. This can be achieved in a couple of different ways. (1) transpose local memory and perform one direction FFT at the time (at we do now in the Distributed module through PencilArrays). (2) exploit the multi GPU capabilities of cuda through the cufftxt library that can perform single node distributed FFT to up to 16 GPUs. (3) Allocate storage and plan in Unified memory and perform the FFT in only one GPU. Ideally we would implement (3) only if we are desperate. The best solution would be to go with method (2), as (1) incurs in hefty memory transfer costs (I am not sure as to how the cufftxt implements multi GPU FFT though). I think (but am not 100% sure) that PencilArrays is tied to MPI. So I guess for 1 we are either ""borrowing"" (but not using directly) the transpose algorithm from PencilArrays, or we are extending the code so that it works ""without MPI"" -- and also with GPUs, which is work in progress: https://github.com/jipolanco/PencilFFTs.jl/pull/37. Another reason to focus on 2 is that we can use PencilFFTs for _distributed_ (but _not_ `MultiRegion`) nonhydrostatic model. So even if PencilFFTs had support for CuArray now (and if `Distributed` were performant for multi GPU --- both of which may not be too close), using cufftxt could still _potentially_ be motivated by performance reasons. In other words, if we develop a capability with cufftxt, then in the future we can also support multi GPU via PencilFFT and `Distributed`, and we have two options (one perhaps performant on single node, multi GPU, and another to use when you need GPUs spread across multiple nodes).",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2523#issuecomment-1118680852:124,perform,perform,124,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2523#issuecomment-1118680852,6,['perform'],"['perform', 'performance', 'performant']"
Performance,"> In particular, this PR introduces the async keyword argument to fill_halo_regions! that allows launching MPI operations without waiting for the communication to complete. Would `blocking=true/false` be a better word to describe how we are controlling `fill_halo_regions!`?. I think ""async"" isn't quite the right word. ""async"" describes the overall algorithm (eg performing some computation while the communication is taking place), but does not describe what is happening specifically within `fill_halo_regions`. It's possible to write `async=true` without overlapping communication and computation. https://github.com/CliMA/Oceananigans.jl/pull/2881 introduces the kwarg `blocking` for `mask_immersed_field!`, which means the same thing:. https://github.com/CliMA/Oceananigans.jl/blob/4a71c834cbc057fcc27061a1d22b4a7cc3eb38fe/src/ImmersedBoundaries/mask_immersed_field.jl#L13. so either way let's use one word.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2953#issuecomment-1452252842:364,perform,performing,364,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2953#issuecomment-1452252842,1,['perform'],['performing']
Performance,"> Instead of saying that trig functions should be avoided on gpus, which seems very strong, I suggest pointing out that there have been some examples where trig functions have performed much slower, and include this example.; > ; > This clearly problem needs further exploration but I don't want people to be scared to use sin and cos because sometimes they just make a lot of sense. My two cents worth. I think there may be a bug in the setup that produces 100x slowdown. (@tomchor if you supply the whole script, we can investigate.) We've run many successful problems with trig functions. Some recent work by @simone-silvestri suggests we can get 2x speed up for _some_ problems by precomputing grid metrics for the `RegularLatitudeLongitudeGrid` rather than computing them on the fly:. https://github.com/CliMA/Oceananigans.jl/blob/da9c53ddd9e28d123b40726cfac2fad835284879/src/Operators/spacings_and_areas_and_volumes.jl#L178-L179. But I don't think we've definitely shown that we _always_ will get 2x speed up. Thus we are going to retain the option to compute metrics on the fly in #2025 so we can continue to investigate it. 2x is a long way from 100x though. If used in a boundary conditions, its basically irrelevant whether one uses a trig function or not. Even a forcing function is only evaluated once per grid point compared to 15x (or more?) for a `BackgroundField` velocity component with high-order advection. Some of these thoughts might be distilled into useful advice in this section of the docs. But we should definitely focus on _approaches_ to performance optimization rather than advice for specific scenarios.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2029#issuecomment-951555050:176,perform,performed,176,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2029#issuecomment-951555050,3,"['optimiz', 'perform']","['optimization', 'performance', 'performed']"
Performance,"> Interesting idea @christophernhill . For the last results that @hennyg888 posted in #1722, I did some calculations and found the following.; > ; > ```; > GPU; > N=256 3.0e9; > N=128 2.6e9; > N=64 6.6e8; > ; > CPU; > N=256 8.6e6; > N=128 9.1e6; > N=64 9.0e6; > ```; > ; > In an article that @ali-ramadhan referenced on the slack channel recently, a paper using a shallow water model in python, Roullet and Gaillard (2021), said they were getting 2 TFlops per second using a thousand cores. We are getting 3 GigaFlops on GPU and 9 MegaFlops.; > ; > Certainly very good speedup since we have O(400) with `WENO5`, but this makes me wonder whether we could do better?; > ; > But to answer your question, when @hennyg888 has the data, we can certainly produce these plots easily enough (unless there is a problem that I'm missing). We have to do more work to compare with Roullet and Gaillard (2021). First of all, there are typos in the paper: sometimes the performance is listed as 2 GFlops, other times as 2 TFlops. Second --- if I understand the situation correctly --- I don't think we've ever measured floating point operations per second. The numbers you've calculated are grid points per second; however we do many floating point operations per grid point. Roullet and Gaillard (2021) estimate their code performs something like 700-800 Flops per grid point. ![image](https://user-images.githubusercontent.com/15271942/126053492-345154c2-22e2-4af8-a898-ac68e889733d.png)",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1861#issuecomment-881986070:955,perform,performance,955,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1861#issuecomment-881986070,2,['perform'],"['performance', 'performs']"
Performance,"> Interesting, thanks for those details! That's odd that there are changes in memory allocation associated with building diagnostics. I don't think there's been changes to `Field` between 0.77.5 and 0.78.0. @navidcy might be able to say more. I think 0.78.0 only upgraded the tests to julia 1.8?; ; Maybe this indicates that the changes are due to changes in the dependencies when upgrading to 1.8?. > I'm also curious why the diagnostics consume so much memory. Are you producing a lot of 3D time averages (which can't be constructed in post-processing?) We've attempted to design the code so that reductions can be performed with minimal memory allocation. 3D diagnostics can simply be calculated from snapshots of the model state, so there's no need to allocate memory (assuming that static memory greatly exceeds GPU memory, this would be preferred). Are we missing a feature to help reduce memory requirements of diagnostics perhaps?. That's interesting. I'm actually also performing a significant amount of 3D averages, as you picked up on. The reason for that is because I want them at a higher frequency than the 3D snapshots that I'm outputting. I'd need a lot of disk-space to output the 3D fields at such high frequencies, so it's a storage limitation issue. That said, I wasn't aware that 3D averages in particular were memory-intensive, so I might rethink that rationale.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2794#issuecomment-1300682800:617,perform,performed,617,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2794#issuecomment-1300682800,2,['perform'],"['performed', 'performing']"
Performance,"> Is that what you recommend?. That wouldn't solve your problem here. KA gives you reasonable performance on the CPU, but since KA 0.8 it is execution story on the CPU much closer to the rest of Julia and it doesn't play special tricks.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2996#issuecomment-1481565094:94,perform,performance,94,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2996#issuecomment-1481565094,1,['perform'],['performance']
Performance,"> Is there a way to test x_f_cross_U et al?. Yes: construct a model with the given feature and then perform a time-step. I guess ideally we would form a giant matrix representing all possible model configurations and time-step them all once. This would be expensive. Maybe we should just loop over each physics specification individually and cross our fingers that there's no bad interactions (I can already tell this would be naive, for example, when a turbulence closure depends on buoyancy).",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/616#issuecomment-583371090:100,perform,perform,100,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/616#issuecomment-583371090,1,['perform'],['perform']
Performance,"> It is what the right panel shows, if I am not mistaken. But the simulation crashed after thousands of iterations. I heard that the PCG solver in Oceananigans has not been widely tested, so that is why I turned to the HeptadiagonalIterativeSolver. Both of those solvers actually use the preconditioned conjugate gradient method. It's also not true --- the `PreconditionedConjugateGradientSolver` has been validated. I'm not even sure it's possible to use the FFT-based preconditioner with the heptadiagonal solver, they have different interfaces. Maybe you worked on that. It's not obvious how to generalize the `HeptadiagonalIterativeSolver` to support `Distributed` architecture, and its also likely more difficult to optimize for immersed boundary methods using an active cells map. We shouldn't waste our time with the `HeptadiagonalIterativeSolver`. If the `PreconditionedConjugateGradientSolver` has issues, we should fix them. It's a waste of energy to work on both.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3552#issuecomment-2071221835:721,optimiz,optimize,721,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3552#issuecomment-2071221835,1,['optimiz'],['optimize']
Performance,"> It returns a int with the value of the number of files or file divisions already performed and the file name. if files are file_part_1.nc, file_part_2.nc, file_part_3.nc, the output will be: `3 file_part_3.nc`. How about calling it `split_files` or `number_of_split_files`?. Could also improve clarity to split it into two functions. One to return the number of split files, and the other to return `current_split_filename`?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3818#issuecomment-2414761941:83,perform,performed,83,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3818#issuecomment-2414761941,1,['perform'],['performed']
Performance,> It seems that you have not performed a convergence test of your numerical model (based on analytical known solutions or manufactured solutions). I suggest that you add this (for instance based on the Taylor Green example); >; > _Originally posted by @funsim in https://github.com/openjournals/joss-reviews/issues/2018#issuecomment-580286479_,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/611:29,perform,performed,29,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/611,1,['perform'],['performed']
Performance,"> It seems very strange to me that evaluating trig functions is slow on GPUs. Is this we problem known to CUDA.jl?; > ; > The information you have here seems good to me. ""slow"" is ill-defined. We recently found that precomputing grid metrics for the latitude-longitude grid leads to a 2x speed up for simple 2D cases (less for more complex cases, probably). But it's all relative to other calculations that are being performed. For many ocean codes the limiting step is evaluating the nonlinear equation of state (a 55 term polynomial) at various locations on the grid. For these cases, the cost of trigonometric functions may not be noticeable.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2029#issuecomment-951425085:417,perform,performed,417,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2029#issuecomment-951425085,1,['perform'],['performed']
Performance,"> It'd be nice to state in writing the justification for writing a separate CPU solver for certain problems. In general, I think that any algorithm that works on the GPU will also work on the CPU. Thus at least in principle the simplest choice is presumably to use the same solver on both architectures. Good point, I've been meaning to set up a script for benchmarking the different pressure solvers. We should use performance benchmarking results to make decisions. > Also, I'd encourage writing this code into as self-contained a submodule as possible. I think there are other codes in the julia ecosystem (not least FourierFlows.jl!) that would benefit from fast and multi-architecture Poisson solvers. We don't have to break this into a separate package just yet, but we do want to ensure this is easy to do in the future. Another good point. As you pointed out some of these solvers depend on the `grid` but if we take that out (which would be trivial) then I think the solvers in PR #589 would be pretty reusable by other packages. Although right now they're pretty specific to staggered grids (except for `BatchedTridiagonalSolver`).",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/586#issuecomment-572303204:416,perform,performance,416,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/586#issuecomment-572303204,1,['perform'],['performance']
Performance,"> Just a quick update, zero-viscosity Bicklet jet test case for `VectorInvariant`, `WENOVectorInvariant` (smoothness calculated based on `ζ`), modified VectorInvariant WENO with smoothness based on 2D stencils of `u` and `v`, here called `WENOVectorInvariantZVEL`; > ; > `VectorInvariant` https://user-images.githubusercontent.com/33547697/157745561-a8e5f128-2f4e-42e3-9305-3f624498590b.mp4; > ; > `WENOVectorInvariant` https://user-images.githubusercontent.com/33547697/157745569-41c52e2d-c80b-4d43-b2bf-8a914e8856a2.mp4; > ; > `WENOVectorInvariantZVEL` https://user-images.githubusercontent.com/33547697/157745571-725ea604-8dec-44bd-bd08-dcd70d9ed4b1.mp4; > ; > `WENOVectorInvariantZVEL` seems to perform actually very well compared to a (somewhat) standard ""vorticity-reconstruction"" `WENOVectorInvariant` and compared to the very noisy standard `VectorInvariant` in lie of the fact that; > ; > * Noise is reduced significantly despite dissipation not being too high; > * Agreement between different resolutions is much higher. Both the WENO vector invariant forms look great, and much less noisy than the non-WENO version. Why do you say the third is better than the second? I don't doubt that it us but my eyes don't see much of a difference. . Out of curiosity, have you computed a time series of the energy?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2317#issuecomment-1064467000:699,perform,perform,699,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2317#issuecomment-1064467000,1,['perform'],['perform']
Performance,"> Just an idea but in #685 the performance regression was actually due to not eliding _apply_*_bcs! for NotFluxBC so maybe we could be eliding some computation or function call?. That's a good hint, I'll look. . But it also seems like a problem that not eliding those functions leads to massive allocation. Why is that?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/710#issuecomment-605241310:31,perform,performance,31,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/710#issuecomment-605241310,1,['perform'],['performance']
Performance,"> Just ran into this and wanted to mention [NCTiles.jl](https://github.com/gaelforget/NCTiles.jl) which we, https://github.com/lmilechin and myself, recently released.; > ; > Not sure if `NCTiles.jl` is readily applicable to your package output but maybe it would be interesting to discuss some sort of integration. What do you think?. Hi @gaelforget , . Thank you for bringing this to our notice! From what I understand, NCTiles provides convenient functions to either use NCDatasets.jl or NetCDF.jl and integrates with MeshArrays.jl, which has representations for tri-polar grids and such. I haven't had the time to read your 2015 paper so please correct me if I did not grasp the full extent of NCTiles' capabilities. So far Oceananigans does not have support for anything other than regular cartesian grids so NCTiles might be an overkill. But we can definitely consider it in the future when Oceananigans gains new capabilities. . By the way, did you notice any differences in performance between NCDatasets.jl and Netcdf.jl?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/433#issuecomment-537189439:982,perform,performance,982,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/433#issuecomment-537189439,1,['perform'],['performance']
Performance,"> Nah…; > ; > Every preview is 200MB of load on the repo I then have to clean and it’s not that easy to clean up a repository’s history. I’d rather I make the PR twice or built the docs locally :). Oof, didn't know that. Sounds fair :). I'll approve it, although we probably have to restart the failed GPU tests",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2886#issuecomment-1409061309:40,load,load,40,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2886#issuecomment-1409061309,1,['load'],['load']
Performance,"> Naively, I would think that if one term is computed with WENO5 then 4 terms would be 4 times the cost. Okay, I understand what you're saying and I agree. Assuming that the advection term dominates the cost of a time-step, we could count the number of times the advection term is evaluated and calculate a multiplicative slowdown based on that assumption. One issue with that assumption is that the advection terms compile to different code when using a `BackgroundField`. With a `BackgroundField` the velocity field is stored as a `FunctionField` (with almost no additional memory allocation) rather than raw data. Calling `getindex` on a `FunctionField` resolves to calling a function, whereas calling `getindex` on a `Field` / `Array` fetches data from memory. So they are different. Naively I would expect that evaluating a complicated advection term involving `BackgroundField` would be cheaper than one that involved two concrete `Array`. But sometimes unexpected things happen (the compiler might decide to optimize the code differently...). Can you try running the code on the GPU? I think it would be enlightening to see if there's a slowdown in that case. We could potentially implement an interface whereby `Field`s can be used as `BackgroundField` rather than functions. That might give us some insight, because then the ""additional"" advection terms associated with `BackgroundField` would truly be identical to the ""original"" advection term. > because presumably the advection of the background state by the background state is zero. I would say that the background self-interaction terms are _neglected_ rather than presumed to be zero. Linear terms associated with the background fields are also neglected. The idea being that if there is a valid way to decompose a flow into background and perturbation components, then the equation that governs the background component is completely neglected (this includes both the nonlinear terms and any linear terms involving the background flo",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1564#issuecomment-816760623:1015,optimiz,optimize,1015,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1564#issuecomment-816760623,1,['optimiz'],['optimize']
Performance,"> New log level should be enough if we just need flat per-iteration timings to time a few blocks for a few iterations. Are you thinking of just logging raw `@time` data?; > ; > I'm not sure if we need anything more than a new log level, but I imagine some of the benefits of using a package like TimerOutputs.jl would make the timer blocks much more useful for debugging and for users. It allows for nested timers and produces a very nice table summary at the end which includes number of calls and average time/memory allocations.; > ; > This could also be useful for users wishing to time their simulations to figure out how much time is being spent on I/O vs. in kernels vs. solvers vs. callbacks, etc. I guess I also see timers as a debugging tool for users. Could help be figure out cluster filesystem issues or figure out whether Oceananigans or my coupled model (via callback) is the bottleneck.; > ; > Otherwise if the timers are just a developer debugging tool that dumps timing information, that's useful but it might not be useful for timing real-world scripts/simulations since the log would fill up with a huge number of lines that can't be interpreted without further processing.; > ; > Here's an example of it in use: [JuliaGPU/CUDA.jl#149 (comment)](https://github.com/JuliaGPU/CUDA.jl/issues/149#issuecomment-461943376) (CUDA.jl has been using TimerOutputs.jl for a long time I think); > ; > TimerOutputs.jl is easy to use since you just add `@timeit` blocks but that does add some noise to the code (not sure if more or less than using a timing log level). @ali-ramadhan I was thinking timing would be useful for general use around figuring out where time is going in day-to-day modeling - so some summary table as in TimerOutputs.jl would be a great thing to aim for - I think?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1862#issuecomment-887967029:891,bottleneck,bottleneck,891,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1862#issuecomment-887967029,1,['bottleneck'],['bottleneck']
Performance,> Nice one. Should we benchmark anything just to make sure this hasn't affected performance?. That's a good idea!,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2063#issuecomment-971712282:80,perform,performance,80,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2063#issuecomment-971712282,1,['perform'],['performance']
Performance,"> No flux are the default for immersed boundary. It looks like you are explicitly setting an immersed boundary condition for a non-immersed field. What is `b`s grid?. The same as `model`s: a MultiRegion grid. I'm not yet using the IB grid since apparently `MultiRegionGrid` isn't working with IBs yet. When I use it, I get this error. ```julia; ERROR: LoadError: MethodError: no method matching PressureSolver(::CPU, ::MultiRegionGrid{Float64, Periodic, Periodic, Bounded, XPartition{Int64}, MultiRegionObject{Tuple{ImmersedBoundaryGrid{Float64, FullyConnected, Periodic, Bounded, RectilinearGrid{Float64, FullyConnected, Periodic, Bounded, Float64, Float64, OffsetArrays.OffsetVector{Float64, Vector{Float64}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVector{Float64, Vector{Float64}}, CPU}, GridFittedBottom{OffsetArrays.OffsetMatrix{Float64, Matrix{Float64}}, Oceananigans.ImmersedBoundaries.CenterImmersedCondition}, CPU}, ImmersedBoundaryGrid{Float64, FullyConnected, Periodic, Bounded, RectilinearGrid{Float64, FullyConnected, Periodic, Bounded, Float64, Float64, OffsetArrays.OffsetVector{Float64, Vector{Float64}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVector{Float64, Vector{Float64}}, CPU}, GridFittedBottom{OffsetArrays.OffsetMatrix{Float64, Matrix{Float64}}, Oceananigans.ImmersedBoundaries.CenterImmersedCondition}, CPU}}, Tuple{CPU, CPU}}, Tuple{CPU, CPU}, CPU}); Closest candidates are:; PressureSolver(::Any, ::ImmersedBoundaryGrid) at ~/.julia/packages/Oceananigans/Kq8xW/src/Models/NonhydrostaticModels/NonhydrostaticMod",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2795#issuecomment-1309394728:352,Load,LoadError,352,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2795#issuecomment-1309394728,1,['Load'],['LoadError']
Performance,"> No. I have no idea why that happened. What can we do to remove them?. Looks like you already did. I would usually try something like `]update Printf` which should remove orphaned packages from `Manifest.toml`. > We can. Such a test may end up running on the CPU via scalar operations though... ?. Ah ok, maybe not a great idea then as it would slow testing down. I guess the computations are tested on the GPU which is good enough. > Note that `Computation` allows the user to specify their own temporary array. `model.pressures.pHY′` is used as a default when `model` is passed to `Computation` in place of an array or field. Ah nice. I guess I was thinking in case `model.pressures.pHY′` disappears one day. > I think just a few will suffice for shallow and deep operations trees, perhaps choosing common use cases to ensure that using abstract operations rather than hard-coded kernels doesn't result in a big performance hit. It will be hard to interpret the results of a benchmark on a deep tree anyways, because we won't have an alternate implementation to compare against. Future performance optimization could use some kind of tree analysis utility + shared memory to accelerate kernels. . Hmmm, I was thinking it would be good to benchmark each operator at least once but I suppose if `sin` is fast then we can assume `cos` and `tanh` will also be fast. Shallow and deep trees makes sense. True we may not have an alternative implementation but we can compare the deep and shallow tree computations to get an idea. I find comparing the computation time to the time per iteration (~30 ms for 256^3) to be helpful. > Why extensive? I'm just not sure what to write: the rules for how things work are already all there in the docstrings. Maybe examples are what's needed?. Didn't mean to suggest that we need extensive documentation right away. Having examples of what's possible will be really useful, but we can build up a collection of good examples over time. PS: Think you missed half my c",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/463#issuecomment-545887965:915,perform,performance,915,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/463#issuecomment-545887965,1,['perform'],['performance']
Performance,"> Okay finally tests passed!; > ; > @navidcy I tried restarting the tests over and over (honestly more than 10 times over the past few days) and they always failed. I could only make them pass this morning when I restarted them one at a time. That is, starting one, and only restarting the next failed test when the previous one had fully run.; > ; > So maybe the variability we see in tests has to do with different processes trying to access the same resources... ?. Yes it is a race condition",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3648#issuecomment-2223710307:481,race condition,race condition,481,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3648#issuecomment-2223710307,1,['race condition'],['race condition']
Performance,> On Satori using stupidly large meshes gives 85% - 89% efficiency going from 1 to 2 GPU for the `multi_region_turbulence.jl` benchmark (Note `1440×600×48` is the size of the 1/4 degree simulation) Unfortunately the efficiency decreases on a larger number of GPUs... we definitely have to fix the scaling; > ; > #### Strong Scaling; > Grid size	Grid	GPUs	wall time	efficiency; > `1024×1024×100`	`RectilinearGrid`	1	3.4 minutes	100%; > `1024×1024×100`	`MultiRegionGrid`	2	1.9 minutes	89.5%; > `1440×600×48`	`RectilinearGrid`	1	1.4 minutes	100%; > `1440×600×48`	`MultiRegionGrid`	2	49.2 seconds	85.4%; > `1440×600×48`	`MultiRegionGrid`	3	38.8 seconds	72.2%; > Going to smaller meshes than these hampers the efficiency incredibly. I think there might be a lot of low hanging fruits to optimize multi GPU. Nice results though! Is this for implicit or explicit free surface?,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2253#issuecomment-1116681325:782,optimiz,optimize,782,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2253#issuecomment-1116681325,1,['optimiz'],['optimize']
Performance,"> One issue with that assumption is that the advection terms compile to different code when using a `BackgroundField`. With a `BackgroundField` the velocity field is stored as a `FunctionField` (with almost no additional memory allocation) rather than raw data. Calling `getindex` on a `FunctionField` resolves to calling a function, whereas calling `getindex` on a `Field` / `Array` fetches data from memory. So they are different. Good to know. Thanks for explaining. > Naively I would expect that evaluating a complicated advection term involving `BackgroundField` would be cheaper than one that involved two concrete `Array`. But sometimes unexpected things happen (the compiler might decide to optimize the code differently...). Agreed! Often unexpected things happen, which keeps us on our toes. > We could potentially implement an interface whereby `Field`s can be used as `BackgroundField` rather than functions. That might give us some insight, because then the ""additional"" advection terms associated with `BackgroundField` would truly be identical to the ""original"" advection term. I don't pretend to understand the details of this and don't know the potential pay off so not sure whether I think this will bear fruit or not. > I would say that the background self-interaction terms are _neglected_ rather than presumed to be zero. Linear terms associated with the background fields are also neglected. . Good. > _Side note:_ some nonlinear terms are additionally neglected in the case of nonlinear viscosity. Ah, I don't think I'm getting into that now but it's something to be aware of.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1564#issuecomment-816780988:699,optimiz,optimize,699,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1564#issuecomment-816780988,1,['optimiz'],['optimize']
Performance,"> One thing to note is that the current implementation appears to be very slow. While the simulation with the `SmagorinskyLilly` closure runs on my laptop in 10 seconds, it takes 4 minutes for the simulation with the `ScaleInvariantSmagorinsky`. I know the dynamic model will be slower given the extra computations, but such a difference seems large to me, so I'm hoping something can be changed here to improve performance:. Avoided recomputation of the strain rate at `ccc` and sped things up a bit more. Now it runs in 2.9 minutes. A lot more to go...",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3642#issuecomment-2241696460:412,perform,performance,412,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3642#issuecomment-2241696460,1,['perform'],['performance']
Performance,"> Output needs to be arbitrary. We may need to perform on-line analysis and output the result (example: turbulent dissipation rate, time-averages, slices of fields, point values, etc).; > ; > We should design an additional interface for Fields. The type of the field indicates the coordinates on which the field is defined, so we should design an interface that uses that information.; > ; > _Originally posted by @glwagner in https://github.com/ali-ramadhan/Oceananigans.jl/pull/93#issuecomment-468290310_. Just adding your comment here as I think there are two new questions raised:; 1. How to integrate diagnostics with the output writing framework?; 2. Right now each NetCDF output file shows a single snapshot. Maybe it makes more sense to keep appending to an existing NetCDF file. This might also make addressing (1) easier especially if the diagnostics have a different output frequency that other fields.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/31#issuecomment-468304803:47,perform,perform,47,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/31#issuecomment-468304803,1,['perform'],['perform']
Performance,"> Perhaps a silly question but with the new MPI parallelism, will be able to do a traditional multi-CPU run with Oceananigans? I presume so but the fact that people are talking about the MPI mostly supporting GPU's, I'm a bit confused. Yes! Actually only CPU + MPI is supported right now haha. GPU + MPI will require distributed FFTs across multiple GPUs. Unfortunately right now PencilFFTs.jl only provides distributed FFTs across multiple CPUs. The benefit for multi-GPU parallelism was brought up since we are currently limited to decomposing the domain in the y-direction only. This is probably fine if you don't have too many ranks so the current limitation favors multi-GPU performance over multi-CPU performance. For models that don't need distributed FFTs, e.g. `ShallowWaterModel`, we can decompose in x, y, and/or z, and getting multi-GPU support might not be that much more extra work.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/590#issuecomment-786687524:680,perform,performance,680,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/590#issuecomment-786687524,2,['perform'],['performance']
Performance,"> Perhaps we could convert to this: https://github.com/DTolm/VkFFT which supports hardware-accelerated FFT on CUDA, Metal and lots of others. It looks like that library is more performant than `cuFFT` as well. Why do we have to convert? Can we use that only for Metal?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3288#issuecomment-1734543134:177,perform,performant,177,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3288#issuecomment-1734543134,1,['perform'],['performant']
Performance,"> Point taken, but I think there are still Oceananigans-relevant applications where Value / Gradient boundary conditions on non-immersed boundaries are useful enough to be a ""core"" feature (e.g. imposing observed SST patterns rather than observed air-sea heat fluxes), but there is always the workaround of strongly restoring boundary-adjacent sponge regions. I think this is what many ocean modelers do to implement such boundary conditions anyway. True! I'm not aware that has ever been done, but since it's not difficult to support (notwithstanding @simone-silvestri's concerns about parallel performance) it's interesting to allow it --- agree. For future readers I want to point out that SST restoring (and similar models) are typically be implemented as a `FluxBoundaryCondition` using a piston velocity model, rather than using `ValueBoundaryCondition` (which implies a flux mediated by some diffusivity / viscosity, possibly derived from a parameterization). (`FluxBoundaryCondition` is mathematically identical to restoring in the surface grid point, though it would be a slightly different model to distribute the restoring over some depth). It could be an interesting project to explore using some parameterization-derived diffusivity together with `ValueBoundaryCondition` to predict surface fluxes, though, I'm not sure what the implications would be.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3224#issuecomment-1690278452:596,perform,performance,596,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3224#issuecomment-1690278452,1,['perform'],['performance']
Performance,"> Possibly, we should start using a merge queue that would disallow PRs from being merged unless tests pass. Yeap. I added these in https://github.com/CliMA/Oceananigans.jl/settings/branches. <img width=""905"" alt=""Screenshot 2023-12-23 at 1 10 26 pm"" src=""https://github.com/CliMA/Oceananigans.jl/assets/7112768/3137d7ef-2f34-4f05-86fa-c759177ac164"">. How does this sound?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3413#issuecomment-1868271224:42,queue,queue,42,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3413#issuecomment-1868271224,1,['queue'],['queue']
Performance,"> Seems like we have to ensure that types are correct... I ll take a look. This is really a compiler issue... I guess we ""shouldn't"" have to enforce types, but maybe it helps with inlining (which we need for performant code and GPU compilation)",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2558#issuecomment-1128949553:208,perform,performant,208,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2558#issuecomment-1128949553,1,['perform'],['performant']
Performance,"> Shoudn't we write a kernel? Presumably that explicit 3D loop is incredibly slow?. Indeed! Unfortunately it's not embarrassingly parallel so my kernel skills are no longer sophisticated enough :) I don't know how to gather `count`, `rowval` or `colval`. The conditional preseumably also leads to poor performance (but no where near as poor as it is now!).",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2737#issuecomment-1248734171:302,perform,performance,302,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2737#issuecomment-1248734171,1,['perform'],['performance']
Performance,"> Should we also try things like; > ; > ```julia; > node(i, j, k, grid, ::Nothing, ℓy, ℓz) = _node(i, j, k, grid, nothing, ℓy, ℓz)[1:2]; > ```; > ; > I think we determined there could be a tiny performance loss but it would make the code a little simpler and also easier to read since we don't have to define every combination of locations for `_node`. Hmm this is more annoying than I thought so I'll leave it for later.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3355#issuecomment-1775509960:194,perform,performance,194,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3355#issuecomment-1775509960,1,['perform'],['performance']
Performance,"> Since the aim is to work with abstract operations, FieldTimeSeries right now only work with JLD2 data that includes halos. Do we want to support loading data without halos?. We generally need to support `SlicedField` so I suggest we implement such support in that context. Until then, I think users need to be aware that they need `field_slicer=nothing` to use this experimental feature. Once this feature is mature, I think that `field_slicer=nothing` should be the default. Some of the most important calculations we do are on boundaries. The reason to omit halos that contain useful information about boundary conditions is only for ""convenience"" --- this feature eliminates that concern so that more science can be done.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1641#issuecomment-835434572:147,load,loading,147,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1641#issuecomment-835434572,1,['load'],['loading']
Performance,"> Since we need the performance provided by KA 0.7, and we need to use KA 0.8+ on GPU, does that mean that we should invest in developing our own CPU infrastructure (replicating what KA 0.7 offered) to achieve that performance?; > ; > Another possibility is that we re-write much of the code base to avoid the performance pitfalls we are currently facing in order to get back to the level of performance we have with current code + KA 0.7. I believe the issue is basically an interaction between some of the abstractions / indirection we have developed and the compiler, so possibly rolling back that abstraction / indirection will bring us back to where we were previously. To follow up with @vchuravy, it seems that rewriting just _some_ of the code was sufficient, so we are (probably) in the clear! The lesson learned is that we cannot slurp / splat `@kernel` function arguments, because it prevents the kernel code from being inlined.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2996#issuecomment-1482198741:20,perform,performance,20,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2996#issuecomment-1482198741,4,['perform'],['performance']
Performance,"> So it appears that there are two competing bug here, which kinda cancel out in the case where gravity aligned with the vertical direction. I also think that back in #1910 when we saw some weird effects when not separating the pressure, this might have been it. No, I don't think so. We reconstructed correctly previously when performing the hydrostatic pressure integral. The only bug was for tilted gravity, or not using the pressure integral.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3079#issuecomment-1518046160:328,perform,performing,328,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3079#issuecomment-1518046160,1,['perform'],['performing']
Performance,"> So the first task is to extend the tridiagonal solver to support integrals in `x` and `y`.; > ; > This could be straightforwardly supported by adding some kind of tag to indicate the ""tridiagonal direction"" (ie `:x`, `:y`, or `:z`), and copy-pasting the functionality for each case. It's a bit of code duplication but pretty straightforward...; > ; > Alternatively we could introduce some kind of abstraction that permutes array dimensions. Then we just have one algorithm which assumes the tridiagonal index is `k`, and support `i` or `j` under the hood via an array wrapper that performs an index permutation.; > ; > I'm leaning towards copy/paste because it's a little easier to understand and it's not that much code in this case... I also prefer the copy/paste method (that's what I used in https://github.com/CliMA/Oceananigans.jl/pull/3111). If I understand correctly these two functions are the only ones that need to be generalized, no?: . https://github.com/CliMA/Oceananigans.jl/blob/4551a78b1f3fe4bb3b238676c128dc751be9b934/src/Solvers/batched_tridiagonal_solver.jl#L74-L83. https://github.com/CliMA/Oceananigans.jl/blob/4551a78b1f3fe4bb3b238676c128dc751be9b934/src/Solvers/batched_tridiagonal_solver.jl#L88. if so, that's not too bad. Is duplicating one of the tests in [test_batched_tridiagonal_solver.jl](https://github.com/CliMA/Oceananigans.jl/blob/main/test/test_batched_tridiagonal_solver.jl) but rotating everything to two different directions enough to test this new functionality?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3116#issuecomment-1560271584:583,perform,performs,583,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3116#issuecomment-1560271584,1,['perform'],['performs']
Performance,"> Some of the abstract operations tests are CPU-only (the non-computation ones?). Would it make sense to run them on the GPU as well?. We can. Such a test may end up running on the CPU via scalar operations though... ?. > It seems that model.pressures.pHY′ has taken on the role of being the temporary array. Can't think of a great way of doing temporary arrays without using up extra memory but maybe it makes sense to define a function so it's easy to change the temporary array?. That's a good idea for sure to permit dispatch on various type parameters of `model`. . Note that `Computation` allows the user to specify their own temporary array. `model.pressures.pHY′` is used as a default when `model` is passed to `Computation` in place of an array or field. > Performance benchmarking (will likely be quite extensive to cover many possible use cases). I think just a few will suffice for shallow and deep operations trees, perhaps choosing common use cases to ensure that using abstract operations rather than hard-coded kernels doesn't result in a big performance hit. It will be hard to interpret the results of a benchmark on a deep tree anyways, because we won't have an alternate implementation to compare against. Future performance optimization could use some kind of tree analysis utility + shared memory to accelerate kernels.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/463#issuecomment-545718073:766,Perform,Performance,766,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/463#issuecomment-545718073,4,"['Perform', 'optimiz', 'perform']","['Performance', 'optimization', 'performance']"
Performance,"> Sorry I don't quite understand: Is this potentially affecting the calculations right now, or will it only be an issue if we upgrade to CUDA 3.5+?. CUDA 3.8 includes features that may allow us to decrease register pressure or implement performance optimizations that would otherwise be difficult or impossible.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2202#issuecomment-1025963063:237,perform,performance,237,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2202#issuecomment-1025963063,2,"['optimiz', 'perform']","['optimizations', 'performance']"
Performance,"> Sorry for not following, but I don't see what breaks here for the end user! Could someone give a quick example/explanation?. Apologies that my explanation was not clear. It's a breaking change because the same user input, such as. ```julia; tke = @at (Center, Center, Center) ((u - U)^2 + (v - V)^2 + w^2) / 2; ```. produces a different object after this update:. * Before this PR, `u - U` would be interpolated to `(Center, Center, Center)`, and then the binary operation `^(2, u - U)` would be calculated at `(Center, Center, Center)`.; * After this PR, both `u - U` and `^(2, u - U) = (u - U)^2` are calculated at `(Face, Center, Center)`. Interpolation is then performed to `(Center, Center, Center)` _after_ the exponentiation to form the three-part sum. Let me know if that makes sense or if another example would be useful.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1599#issuecomment-823698116:667,perform,performed,667,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1599#issuecomment-823698116,1,['perform'],['performed']
Performance,"> Sponge layers can be added with the existing API. A 1 line implementation (by defining the forcing function to implement a sponge layer, using the currently provided arguments) is currently possible. Yes that's how I did it in PR #291. It can be done in a user script but I figured why not have a `add_sponge_layer!(model; damping_timescale)` convenience function in case users want one. > What is meant by “arbitrary”?. By arbitrary I mean you can keep adding or piling on forcing functions. The purpose of this PR is just to prototype this and see if it can be made performant.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/294#issuecomment-504417716:570,perform,performant,570,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/294#issuecomment-504417716,1,['perform'],['performant']
Performance,> Testing with validation/mesoscale/baroclinic_adjustment.jl seems to indicate that the race condition is eliminated from the changes to the fill_halo_region! function. Was there a race condition there?. Could you please point to the relevant issue describing the condition just for completeness?,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1985#issuecomment-920113136:88,race condition,race condition,88,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1985#issuecomment-920113136,2,['race condition'],['race condition']
Performance,"> That probably shouldn't have changed, can you file an issue on CUDA.jl/GPUArrays.jl? I'll have a look next week. The only change to `@allowscalar` that comes to mind is task/thread-safety, which does come at a certain performance cost (it now does a TLS lookup instead of a simple pointer check, but the cost of that should be negligible compared to the subsequent memory transfer). Seems I can't reproduce the supposed error so, sorry, my bad... Something else must have been the issue. 😔",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1514#issuecomment-817385393:220,perform,performance,220,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1514#issuecomment-817385393,1,['perform'],['performance']
Performance,"> That said, I wasn't aware that 3D averages in particular were memory-intensive, so I might rethink that rationale. 3D averages require scratch space for accumulating the average; this is why they are memory intensive. It's possible to compute averages just on slices or windowed regions of the domain. If this works for your application, you can consider it to reduce memory allocation. > I'd need a lot of disk-space to output the 3D fields at such high frequencies, so it's a storage limitation issue. . In principle, you can use a callback to accumulate an average on the CPU from data on disk, and then delete the data periodically as the simulation runs. You can also accumulate the time-average on the CPU. These methods may not be performant, however, depending on the balance between the cost of CPU-GPU data transfer, and other costs in your simulation. If you have to accumulate a 3D time-average for performance reasons, then you have no choice but to allocate a 3D field on the GPU for this purpose.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2794#issuecomment-1301417685:740,perform,performant,740,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2794#issuecomment-1301417685,2,['perform'],"['performance', 'performant']"
Performance,"> The Roquet’s approximation is perfectly sufficient for Oceananigans, because it will never be used for global calculations where local approximations are an issue. However I agree with everybody else that it would be best to use the same EOS in Ocenanigans and Climate_Ocean. in that case we should adopt TEOS-10. Be warned that it is quite inefficient through. So we may be hit performance-wise. Hard to tell without trying. Thanks for the feedback @rafferrari. I talked to @leios earlier today and we think it shouldn't be a problem on the GPU. It's just straight up number crunching so it might benefit from being run on a GPU. But we can make sure by doing a quick benchmark.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/692#issuecomment-596868076:381,perform,performance-wise,381,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/692#issuecomment-596868076,1,['perform'],['performance-wise']
Performance,"> The main issues are when the entirety of a heavy kernel (like one that calculates a tendency) may be promoted to higher precision. `fill(var, 0)` is cheap and unlikely to affect performance.; > ; > That said it's just more precise to write `fill(var, zero(eltype(var)))` (this is _exactly_ what you are trying to do) and therefore the preferred way to write it. You were meant to write `fill!` not `fill` right? For `fill!(::AbstractArray,0)` this is what happens. ```julia; julia> a = Float32[1,2,3]; 3-element Vector{Float32}:; 1.0; 2.0; 3.0. julia> @code_llvm fill!(a,0); ; @ array.jl:346 within `fill!`; define nonnull {}* @""julia_fill!_127""({}* noundef nonnull align 16 dereferenceable(40) %0, i64 signext %1) #0 {; top:; ; @ array.jl:347 within `fill!`; ; ┌ @ number.jl:7 within `convert`; ; │┌ @ float.jl:159 within `Float32`; %2 = sitofp i64 %1 to float; ; └└; ```. So the very first thing is that if `eltype` of the array and type of second argument aren't the same then it's converted (the `%2 ...` line); Using `fill!(var, zero(eltype(var)))` then can skip this conversion (it's compiled away) but the result is the same. You can make and educated guess of the type of the zero, but honestly, I wouldn't even bother. It has probably zero impact on performance for any larger than a few elements and I find `fill!(A,0)` very clear to read too, so that's what I now always try to write.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3271#issuecomment-1723777023:180,perform,performance,180,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3271#issuecomment-1723777023,2,['perform'],['performance']
Performance,> The new commit follows the suggestion of overloading the compute! method to apply it on a tuple of ComputedField. Good point that we may not need an independent type (except for the very minor performance optimization of precomputing the compatibility between fields -- which probably doesn't matter),MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3234#issuecomment-1696117967:195,perform,performance,195,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3234#issuecomment-1696117967,2,"['optimiz', 'perform']","['optimization', 'performance']"
Performance,"> The new default, converting Int64 exponents to Float64, seems better. Unless `power_by_squaring` performs better in realistic applications?. I don't think we ever used `power_by_squaring`, because the ""pre 1.6"" Oceananigans kernels were defined via`KernelAbstractions`, which in turn translated `^(a, b)` to `CUDA.pow(a, b)`:. https://github.com/JuliaGPU/KernelAbstractions.jl/blob/fdb7415b6f6083c23451cc526b0637144322b1cb/lib/CUDAKernels/src/CUDAKernels.jl#L289. This means that ""pre 1.6"" we were exponentiating with `nv_powi`... In ""current"" Oceanagnians (before this PR), changes to KernelAbstractions and CUDA seem to imply that we invoke `nv_pow` instead. And for some reason this can slow down our code by 10-15x (!!) as discussed on #1764 . Does CUDA C do special transformations for `powi(x, Int32(2))`? Eg, does it convert `powi(x, Int32(2))` to `x*x` (or something like that)? I've no idea but maybe there are some massive algebra reductions / expression eliminations that occur for our weird WENO5 code when we ask for `x*x` versus `x^2.0` ... ?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1770#issuecomment-870014674:99,perform,performs,99,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1770#issuecomment-870014674,1,['perform'],['performs']
Performance,> The performance plots on the README.md file show that the single precision speed up for CPUs is below 1. This is surprising and should be commented on.; >; > _Originally posted by @funsim in https://github.com/openjournals/joss-reviews/issues/2018#issuecomment-580286479_,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/607:6,perform,performance,6,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/607,1,['perform'],['performance']
Performance,"> The point of this was to finally close issue #13. I believe issue #13 occurs when `@inbounds` is not used. I found this same issue (and its resolution) a while ago during the development of `OceanTurb.jl`. Edit: I see the benchmarks in thabbott/JULES.jl#22. Are you sure that corresponds to a measurable performance improvement (eg, is signal, not noise). It seems more likely to me that the two benchmarks are identical, which is consistent with the fact that `@propagate_inbounds` implies `@inline` (what we have been told).",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/434#issuecomment-536318549:306,perform,performance,306,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/434#issuecomment-536318549,1,['perform'],['performance']
Performance,"> The preview isn't showing for me. But we can fix any issues later, so I say merge away. Yeah, It's also not loading for me. It feels like whenever I try to check PR previews it's kinda hit or miss. I wonder if this is an issue with Documenter. But if you're okay with it, I'll merge it and we can fix any problems later.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2045#issuecomment-963566951:110,load,loading,110,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2045#issuecomment-963566951,1,['load'],['loading']
Performance,"> The problem is not only with the `Nothing`, it's that the reduced fields are effectively launched with a reduced grid size; > ; > for example the west boundary kernel launch for a `(Center, Center, Center)` field will be of size `Ny*Nz` for a `(Center, Center, Nothing)` it will be of size `Ny`; > ; > For the moment I just filter the reduced fields and do them individually. I don't think it will create too much problem. This is only an optimization for time-stepping where we do not have too many reduced fields (only free surface I think). Ah indeed, I see now.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2335#issuecomment-1065529029:441,optimiz,optimization,441,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2335#issuecomment-1065529029,1,['optimiz'],['optimization']
Performance,"> The tests use a lot of scalar indexing that's why they fail on the GPU:; > ; > https://buildkite.com/clima/oceananigans/builds/15604#018f40ed-787d-4e74-a5f4-ae1656fa3043/18-724; > ; > I think if we are comparing single numbers it makes sense to use `@allowscalar`.; > ; > If we are comparing vectors it could be nice to figure out how to get the tests to run without `@allowscalar` since presumably this is possible. Given that we're comparing elements of vectors with a maximum length of 6, I opted to use `CUDA.@allowscalar`. The impact on performance in this situation is minimal when running on a GPU.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3573#issuecomment-2103388821:544,perform,performance,544,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3573#issuecomment-2103388821,1,['perform'],['performance']
Performance,"> There was a bug in some recent updates to KernelAbstractions.jl that caused Enzyme to break on broadcasting arrays in Oceananigans. This PR includes a test to make sure this bug doesn't occur again. Interesting! I think it's ok to add a broadcasting test. But it will be confusing to future developers if the test is explained / written as somehow testing a bug in _another_ package. If there's a bug somewhere else, we need a test in that packge (presumably that has been added). . This test also seems a little complicated. Why not just write a simple function that does a broadcast, and then try to autodiff that? Why do we need initial conditions, models, etc?. For example. ```julia; function times_c!(a, b, c); a .= b .* c # c is a number; return sum(a) # or whatever we gotta return; end. grid = RectilinearGrid(arch, size=(1, 1, 1), extent=(1, 1, 1)); a = CenterField(grid); b = CenterField(grid); c = 2; @test try ; autodiff(times_c!, a, b, c... # or something); true; catch; false; end; ```. It's super important for tests to be as short and easy to understand as possible, because maintaining test code is one of the main bottlenecks on development.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3598#issuecomment-2105014020:1135,bottleneck,bottlenecks,1135,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3598#issuecomment-2105014020,1,['bottleneck'],['bottlenecks']
Performance,"> Thinking of revamping this, I think `T` is a good notation; we do not need to specify `U` and `C` since probably (in the future) we will need to refactor these a little to pass through boundary conditions. Hopefully we don't have to pass boundary conditions 🥺 . Not all complexity is justified by the performance gains...",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3268#issuecomment-2329666236:303,perform,performance,303,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3268#issuecomment-2329666236,1,['perform'],['performance']
Performance,"> This PR removes the PencilFFT library from Oceananigans and builds a distributed FFT solver using Oceananigans' inhouse transforms. This allows us to run on GPUs both periodic and bounded domains. No stretched mesh is supported at the moment (that will come in a later PR); > ; > The transposition is performed through a custom `transpose` routine built for Oceananigans' fields that assumes; > ; > * the starting configuration is always a _z-free_ configuration.; > * the transpose directions are _z-free_ -> _y-free_ -> _x-free_ -> _y-free_ -> _x-free_; > * the y-direction is integer divisible by the number of ranks that divide the x-direction; > * the z-direction is integer divisible by the number of ranks that divide the y-direction; > ; > An additional assumption is that:; > ; > * if TY is Bounded, also TZ needs to be Bounded; > * if TX is Bounded, also TY needs to be Bounded; > ; > All these assumptions can be relaxed in following PRs. Are these statements still accurate?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3279#issuecomment-2197303850:303,perform,performed,303,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3279#issuecomment-2197303850,1,['perform'],['performed']
Performance,"> This is only relevant for CPU models --- correct? In other words, when running on the GPU we still need to load data onto the CPU and then transfer to the GPU. That is technically true although in general you tend to have a lot more CPU memory than GPU memory so I suspect it won't be an issue there. You could reduce memory allocation even further by loading all fields from disk into a temporary array, then copying to a CuArray one field at a time. But I don't think that'll work with JLD2 as the entire file is loaded into memory at once. You could do it with chunked NetCDF files, for example, where you read specific chunks into memory at a time. > Previously, the option was available to create a model on the GPU, load data on to the CPU, and then copy that data to the GPU --- right? Or am I missing something?. You could do that but no user would have gone through the trouble. > Does this PR impact the user API for checkpointing at all, or does it just change `restore_from_checkpoint`, which does everything behind the scenes?. Just changes what happens behind the scenes. The API is the same but checkpoint files now have file names like `convection_iteration12500.jld2` instead of just `convection_12500.jld2` although maybe we should use the word `checkpoint` in the file names. > We may want to move the section on checkpointing that is currently in the documentation at Model setup > Output writers to its own section within the documentation. Sounds like a good idea, will do before merging.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/628#issuecomment-588990471:109,load,load,109,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/628#issuecomment-588990471,4,['load'],"['load', 'loaded', 'loading']"
Performance,"> This is super exciting! Out of curiosity is there any/which Krylov solver is compatible on multiple GPUs? Seems to be an important bottleneck for our current `PreconditionedConjugateGradientSolver` approach. Also to be clear about what this can do --- with Krylov, we can still use the FFT preconditioner. When we do that the parallelism issues are identical to the issues with our current CG solver, it's just that tweaking the solver method might allow us to do fewer iterations. So there are two things going on in this discussion which are independent. First is whether conjugate gradient is optimal or whether we should use a different method. The second issue is the preconditioner, which is the more uncertain part but where we might have more gains.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3803#issuecomment-2387264231:133,bottleneck,bottleneck,133,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3803#issuecomment-2387264231,1,['bottleneck'],['bottleneck']
Performance,"> This is the same benchmark performed with `ImplicitFreeSurface`, by imposing a divergent velocity `u(x, y, z) = x / 10` to make sure the implicit solver iterates. Looking at the results it seems like it doesn't iterate too much... (probably WENO cleans up?) And it is very weird that the `RectilinearGrid` version is not affected by the FreeSurface calculation? (I have double checked that the free surface solver is correct); > ; > #### Strong Scaling; > Grid size	Grid	GPUs	wall time	efficiency; > `1440×600×48`	`RectilinearGrid`	1	1.37 minutes	100%; > `1440×600×48`	`MultiRegionGrid`	2	1.05 minutes	65.2%. THATS CALLED A SPEED UP 🍻 . I think bathymetry interferes with the solver more. When the velocity field is _purely_ divergent + barotropic, it just produces waves that dissipate nearly instantly ?. It'd be good to come up with a reliable 3D initial condition for benchmarking iterative solver stuff....",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2253#issuecomment-1116787626:29,perform,performed,29,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2253#issuecomment-1116787626,1,['perform'],['performed']
Performance,"> True. Ideally we want to be close to the scalings/performance we got with lat-lon grid? That’s perhaps not feasible..? I don’t know how close is good enough tho. We expect to be at lower performance. For that reason we have dedicated two independent milestones to the cubed sphere. The first milestone is rather susinct ""complete the cubed sphere implementation"". The second milestone pertain to performance: ""achieve 10 SYPD at 25 km resolution"". I think this is nice, because we want to separate tasks into ones that are _required_ for correct functionality, versus tasks that are oriented towards performance rather than correctness.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3201#issuecomment-1719389468:52,perform,performance,52,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3201#issuecomment-1719389468,4,['perform'],['performance']
Performance,"> We could restrict ourselves to specific combinations of boundary conditions instead, which would reduce the number of possibilities. For example, we might have just doubly periodic in (x, y) plus flux in z on all fields, or singly-periodic in x and flux in (y, z) on all fields. Agree that `CoordinateBoundaryConditions` might be a weird name but yeah, maximum flexibility would be very powerful. Maybe the common use case isn't to impose each of the 30 boundary conditions one-by-one but we can just have nice helper functions/abstractions like; ```julia; model.boundary conditions += HorizontallyPeriodic(); ```. > @ali-ramadhan do you mean with regards to performance? I'm not sure. With multiple dispatch being core to julia it seems this scenario is not uncommon (30+ may not be very large). I'm still pretty new to Julia so yeah don't know if that will be an issue, especially on the GPU. Only way to find out is to try and benchmark! Maybe you're right and 30+ isn't a lot. @vchuravy any idea on whether 30+ parameterized types for a struct is too many? Would this hurt performance on the GPU?. > The function calc does not actually impose a boundary condition --- the imposition of boundary condition depends on, for example, the viscosity and diffusivity, and is a property of the equation (or turbulent closure) being implemented. Again for example, the K-Profile-Parameterization includes a modification of how a flux boundary condition is implemented. In other words, the ""specification of flux"" is separate from the ""imposition of a boundary condition"". The former is determined by the user. The latter is determined by the model/governing equation. I see. So if no parameterizations are being used, are the boundary conditions actually being _imposed_ then? Even with KPP, isn't the boundary condition still being _imposed_ only to later be modified by KPP?. I still feel like `bc.calc()` feels obscure, I'm not sure why a boundary condition should have to calculated. Perhaps it's jus",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/118#issuecomment-472241993:661,perform,performance,661,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/118#issuecomment-472241993,1,['perform'],['performance']
Performance,"> We need some PlotRecipes for fields...; > ; > (In general, not for this PR!). We need that badly... I think we should take the approach in `ClimaCore.jl` which is to develop an ""external"" package that lives in `/lib`. Perhaps `Makinanigans.jl` or `Vizinanigans.jl`. Performing new releases is a little more arduous with that setup but its probably worth it.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2238#issuecomment-1034179606:268,Perform,Performing,268,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2238#issuecomment-1034179606,1,['Perform'],['Performing']
Performance,"> We won't reduce computations, because these operators are not short-circuiting (this is what we want to maximize performance for these inner functions).; > ; > Evaluating booleans isn't expensive. Short-circuiting logic can be expensive (or rather, prevent compiler optimizations) in hot inner loops. We've written the code so we don't use short-circuiting logic (`ifelse` rather than `if`, `&` rather than `&&`). Ah, I see. I hadn't noticed that. . > Either way we need both `east_bounded` _and_ `solid_interface` so the question is just how we shuffle logic between the two. We can consider changing `solid_interface`. If we change its behavior I would recommend calling it `fluid_solid_interface`. I think changing `solid_interface` to `fluid_solid_interface` (along with it's definition of course) is clearer. Especially because `solid_interface` confused me at first; I thought it effectively was supposed to test for the fluid-solid interface. But at this point I don't feel strongly about it.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2263#issuecomment-1047193281:115,perform,performance,115,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2263#issuecomment-1047193281,2,"['optimiz', 'perform']","['optimizations', 'performance']"
Performance,"> What does the `T(0.5)` term do in the following snippet? Does it translate to `Float64(0.5)`?. Yeah exactly, it's there so we don't mix precisions when running with e.g. `Float32`. That can slow down the code as you have to convert between `Float32` and `Float64` to do operations. Could do `/ 2` which gets converted to the correct float type, but division is more expensive than multiplication and not sure if `x / 2` will get optimized to `0.5 * x` in all cases.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/438#issuecomment-537955930:431,optimiz,optimized,431,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/438#issuecomment-537955930,1,['optimiz'],['optimized']
Performance,"> What if we get rid of `parameters` and `computed_dependencies`, and call `compute!` on all the arguments?. That'd work for me. Would that have the same performance or would it add some overhead?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2340#issuecomment-1068561902:154,perform,performance,154,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2340#issuecomment-1068561902,1,['perform'],['performance']
Performance,"> What if you edit the forcing functions into the discrete form, e.g. invoking `Forcing` with `discrete_form=true`?. It helps! But doesn't solve the problem. In particular the MWE above (with two tracer) in discrete form compiles for me. But when I add more tracers (I need at least 6 tracers for my simulations) it fails again. Sometimes with a different error:. ```; ERROR: LoadError: Failed to compile PTX code (ptxas exited with code 255); Invocation arguments: --generate-line-info --verbose --gpu-name sm_60 --output-file /glade/scratch/tomasc/jl_hs9AZo7IJq.cubin /glade/scratch/tomasc/jl_XSJ4P4z47a.ptx; ptxas /glade/scratch/tomasc/jl_XSJ4P4z47a.ptx, line 5136; error : Entry function '_Z23julia_gpu_calculate_Gu_7ContextI14__CUDACtx_Namevv14__PassType_312v12DisableHooksE18_gpu_calculate_Gu_16CompilerMetadataI10StaticSizeI9_8__8__6_E12DynamicCheckvv7NDRangeILi3ES5_I9_1__1__6_ES5_I11_16__16__1_EvvEE11OffsetArrayI7Float64Li3E13CuDeviceArrayIS9_Li3ELi1EEE15RectilinearGridIS9_8PeriodicS12_7BoundedS9_S9_S8_IS9_Li1ES10_IS9_Li1ELi1EEES8_IS9_Li1E12StepRangeLenIS9_14TwicePrecisionIS9_ES15_IS9_E5Int64EES8_IS9_Li1ES14_IS9_S15_IS9_ES15_IS9_ES16_EES8_IS9_Li1ES10_IS9_Li1ELi1EEEvE4WENOILi3ES9_vv5TupleIS8_IS18_IS9_S9_S9_ELi1ES10_IS18_IS9_S9_S9_ELi1ELi1EEES8_IS18_IS9_S9_S9_ELi1ES10_IS18_IS9_S9_S9_ELi1ELi1EEES8_IS18_IS9_S9_S9_ELi1ES10_IS18_IS9_S9_S9_ELi1ELi1EEES8_IS18_IS9_S9_S9_ELi1ES10_IS18_IS9_S9_S9_ELi1ELi1EEEELitrueEvS17_ILi2ES9_vvS18_IS8_IS18_IS9_S9_ELi1ES10_IS18_IS9_S9_ELi1ELi1EEES8_IS18_IS9_S9_ELi1ES10_IS18_IS9_S9_ELi1ELi1EEES8_IS18_IS9_S9_ELi1ES10_IS18_IS9_S9_ELi1ELi1EEEELitrueEv12UpwindBiasedILi1ES9_vvvv8CenteredILi1ES9_vvvvEES20_ILi1ES9_vvvvEES20_ILi2ES9_vvvS20_ILi1ES9_vvvvEEEvv16SmagorinskyLillyI26ExplicitTimeDiscretizationS9_10NamedTupleI34__b____1____2____3____4____5____6_S18_IS9_S9_S9_S9_S9_S9_S9_EEE17BoundaryConditionI4FluxvEvS23_I23__velocities___tracers_S18_IS23_I12__u___v___w_S18_I9ZeroFieldIS16_Li3EES26_IS16_Li3EES26_IS16_Li3EEEES23_I34__b____1____2____3____4____5____6",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2869#issuecomment-1401133112:376,Load,LoadError,376,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2869#issuecomment-1401133112,1,['Load'],['LoadError']
Performance,"> What parallelism strategy makes sense? One particle per thread?. I think that makes sense, and we just queue up a huge amount of blocks like we already do with 3D kernels. Although if we use DifferentialEquations.jl then maybe we won't have to worry about parallelism here?. > Do we use linear interpolation between velocity nodes or assume a constant velocity within each cell (nearest neighbor interpolation)?. Yeah I'm not super sure what's a good approach here. We're on a regular Cartesian grid right now so maybe the difference isn't huge?. The [Parcels v2.0 paper by Delandmeter & van Sebille (2019)](https://www.geosci-model-dev.net/12/3571/2019/) discusses interpolation schemes for curvilinear C-grids using mostly Lagrange polynomials. But on a rectilinear grid like ours, it seems that it just reduces down to linear interpolation [see Eq. (3)]. Maybe a good question for @jm-c. > Obtaining prognostic quantities within particles (like reacting chemical species with reaction rates that depend on ambient temperature) may be challenging and probably requires a careful design. Agreed. It may not even be clear what a good solution is. Apparently DARWIN has a similar problem I think where many plankton species may be uptaking nutrients so the question of what to do when a nutrient are depleted is hard to solve, especially as having nutrients go below zero can be bad for the model.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/511#issuecomment-547215399:105,queue,queue,105,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/511#issuecomment-547215399,1,['queue'],['queue']
Performance,> When I ran this locally I got a load of `@test_broken` passing for computed fields which is quite strange. We don't test those regularly --- it might be a new compiler etc. You can feel free to convert those to `@test` if you want to be a hero 💪,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3482#issuecomment-2181261472:34,load,load,34,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3482#issuecomment-2181261472,1,['load'],['load']
Performance,"> When utilizing a split-explicit free surface, additional errors arise. For instance, for MultiRegionGrid and ConformalCubedSphereGrid (when specifying the number of substeps), we encounter:; > ; > ```julia; > ERROR: LoadError: UndefVarError: `settings` not defined; > ```; > ; > When specifying both grid and cfl for ConformalCubedSphereGrid, the following error occurs:; > ; > ```julia; > ERROR: LoadError: type OrthogonalSphericalShellGrid has no field Lz; > ```. Do you need help fixing these?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3302#issuecomment-1744978276:218,Load,LoadError,218,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3302#issuecomment-1744978276,2,['Load'],['LoadError']
Performance,"> Where does this PR leave #2538? As I understand it we do not treat vertically stretched grids here (or a mixed FFT/tridiagonal solver). So this moves laterally compared to #2538 and perhaps the algorithm implemented there can be adapted once this is merged?. I was planning was to implement the tridiagonal solve similarly to what we do for single GPU as such:; ```julia; transpose_z_to_y!(storage) # copy data from storage.zfield to storage.yfield; solver.plan.forward.y!(parent(storage.yfield), buffer.y) ; transpose_y_to_x!(storage) # copy data from storage.yfield to storage.xfield; solver.plan.forward.x!(parent(storage.xfield), buffer.x); transpose_x_to_y!(storage) # copy data from storage.xfield to storage.yfield; transpose_y_to_z!(storage) # copy data from storage.yfield to storage.zfield; ; # Perform the implicit vertical solve here on storage.zfield... transpose_z_to_y!(storage); solver.plan.backward.y!(parent(storage.yfield), buffer.y); transpose_y_to_x!(storage) # copy data from storage.yfield to storage.xfield; solver.plan.backward.y!(parent(storage.xfield), buffer.x); transpose_x_to_y!(storage) # copy data from storage.xfield to storage.yfield; transpose_y_to_z!(storage) # copy data from storage.yfield to storage.zfield; ```. This is not super great because it requires eight transposes, but I think it's the same thing that was happening in #2538",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3279#issuecomment-2197384003:807,Perform,Perform,807,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3279#issuecomment-2197384003,1,['Perform'],['Perform']
Performance,"> Where in the model should we store the forcing array in this case?. The user would define an array in a script, declare it `const` to the compiler, and then write a function that indexes into it as a global:. ```julia; # define a; forcing(..., i, j, k) = a[i, j, k]; ```. We can also include a constructor for `Forcing` that allows the user to pass some function that defines a constant array, and set up the same functionality internally. > The main motivation being that we don't have to write extra functions that dispatch on the forcing, thus simplifying the time stepping code. As you point out we don't want to write 5! = 120 new functions. The update_source_terms! function is already 52 lines long so I'd rather avoid having to dispatch on this function. I think the problem is that our functions are trying to do too much at once. We need smaller functions that perform more atomic operations so we can dispatch on atomic operations. I don't think we need to re-invent multiple dispatch with macros. We just need to refactor the code so we can use multiple dispatch effectively.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/110#issuecomment-470556414:873,perform,perform,873,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/110#issuecomment-470556414,1,['perform'],['perform']
Performance,"> While we should try to use powers of 2 for performance, I think that overly restricts the model resolutions we can be running at. There are a LOT of choices between e.g. `512x512x128` and `1024x1024x128`. It might be that the largest model that fits in memory isn't nice powers of 2. Users may have various reasons for running resolutions that aren't powers of 2. I over-simplified. FFTs are efficient when there are a small number of prime divisors --- 2, 3, 5, 7 --- of the length of the sample. 100 may actually be ok because it is 2\*2\*5\*5. I don't feel strongly, but you will find that most people running simulations are using powers of 2 and 3 (128, 192, 256, 384, 512, 768, 1024) --- even when their code has no FFTs! Perhaps the purpose of this convention is so people who *do* use codes that rely *heavily* on FFTs are able to run code comparisons. I find the convention to be an appropriate best practice. An 1D FFT of length 257, for example, is 4 times slower than an FFT of length 256 on my machine. For tests, of course, using small numbers of prime divisors is completely irrelevant; I just want to encourage best practices and promote awareness of them, and also implicitly emphasize to users that this code uses FFTs, and don't want random person X to read the tests and scratch their head and ask ""doesn't this code rely on FFTs?"" (like I did when I saw the examples). > Just a note that if we want to reuse `model.metadata.arch` with GPUifyLoops.jl then the options should be `:CPU` and `:GPU` (instead of `:cpu:` and `:gpu` which is what `ModelMetadata` currently uses) as I believe GPUifyLoops.jl expects ""capitalized"" Symbols. I certainly prefer that. It may be academic because the architecture should probably be a parameter of `Model`.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/97#issuecomment-468533848:45,perform,performance,45,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/97#issuecomment-468533848,1,['perform'],['performance']
Performance,"> Yeah exactly, it's there so we don't mix precisions when running with e.g. Float32. That can slow down the code as you have to convert between Float32 and Float64 to do operations. Just to expand on what @ali-ramadhan said: in writing numbers, we adopt a mixed approach -- in some cases we use integers and rely on promotion to obtain the correct floating point precision. In other cases, we explicitly impose the precision of a number. For fractions, we typically impose precision. For multiplication by integers, we just use the integer and promotion. Also, I don't think its actually the floating point conversation that is costly here. Rather I think the added cost is doing the arithmetic at higher precision. Also, due to promotion rules, I think if a single kernel operator outputs the wrong floating point type, we could end up performing much of an entire kernel's computation at the wrong precision. This appears to have a negligible effect on a Tesla V100 --- but it may have more important effects on other machines, especially if we try to run at half precision on machines especially designed for half precision calculations.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/438#issuecomment-537997825:838,perform,performing,838,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/438#issuecomment-537997825,1,['perform'],['performing']
Performance,"> Yeah sure, we can do it. ~~Is there a performance difference?~~. ~~The best solution might use dispatch to enable ""non-stretched WENO"" automatically when the grid has constant spacing.~~. After actually looking at the code, it seems this is what this PR does do, indeed.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2060#issuecomment-969063314:40,perform,performance,40,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2060#issuecomment-969063314,1,['perform'],['performance']
Performance,"> Yeah this is pretty worrying... I'm pretty sure this is the cause of #1420 which has been open for a while so this slowdown must have been around for a while (and just flew under the radar). [Profiling](https://docs.julialang.org/en/v1/manual/profile/) might help pinpoint the bottleneck. It looks like there are some extra temporary arrays being created for the cases with non-trivial (although still pretty trivial) indexing patterns. Profiling could be good - also just turning off as much as possible and then building up from the FFT alone, simple stencil etc... - if that is possible?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1919#issuecomment-891443627:279,bottleneck,bottleneck,279,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1919#issuecomment-891443627,1,['bottleneck'],['bottleneck']
Performance,"> Yep, I would call it bulk velocity though, instead of phase speed, and change the name from `Orlanski` to something more descriptive like `AdvectiveOutflow`. But these are mathematically identical right? Orlanski called is a ""phase speed"", but ""outflow velocity"" is equally valid and refers to exactly the same mathematical object. The reference you posted says. > The test results also confirm that this type of boundary condition, which was originally designed by Orlanski primarily for equations which are hyperbolic in nature, also performs well for parabolic problems. I think we can keep the name ""Orlanski"" and provide a generic interface for specifying the outflow speed (whatever you want to call it). It can be user-specific, dynamically computed, etc. The code can be extensible, so if users want to experiment with different methods for computing the outflow speed, this is possible.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3482#issuecomment-1965488832:538,perform,performs,538,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3482#issuecomment-1965488832,1,['perform'],['performs']
Performance,"> You can't have multiple threads incrementing the same variable, you get a race condition. Ok, that's just me showing my ignorance. . Perhaps we can change the name of `gpu_accumulate_xy` to something like `horizontal_integral!` that works on both the CPU and GPU? And with a docstring that explains inputs and outputs?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/352#issuecomment-520524067:76,race condition,race condition,76,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/352#issuecomment-520524067,1,['race condition'],['race condition']
Performance,"> You could do that but no user would have gone through the trouble. Well, fair, I was more commenting that ""we"" could have done that behind the scenes for the GPU case. > That is technically true although in general you tend to have a lot more CPU memory than GPU memory so I suspect it won't be an issue there. I'm still confused about how this changes what was previously done. Did we previously create arrays on the GPU, copy checkpoint from the CPU to ""temporary"" GPU arrays, and then copy from temporary GPU arrays to previously-instantiated model data? Or am I missing something (is it possible to load data from disk directly to the GPU?)",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/628#issuecomment-589151190:605,load,load,605,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/628#issuecomment-589151190,1,['load'],['load']
Performance,"> [AMGX.jl](https://github.com/JuliaGPU/AMGX.jl) only has prebuilt libraries for linux systems. That said, calling using AMGX will fail on, e.g., Mac OS X. What's the best way to optionally load AMGX.jl? Or is there another way around that?. @simone-silvestri any ideas for how to solve this?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2688#issuecomment-1242544194:190,load,load,190,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2688#issuecomment-1242544194,1,['load'],['load']
Performance,"> `2)` is because I cannot think of a situation where you would want to output something different than Array? But please correct me if I'm wrong. I haven't found another example in this repository. We chose `array_type` to permit the flexibility of other array types. I don't know enough to say that we would _never_ want another array type. Better to be defensive than aggressively constraining user action?. `Float32` used to be the default. However, this produced a lot of pain in some testing situations where we wanted to show bitwise reproducability / accuracy in saving. I can't remember all the details, but after a few user issues (in addition to our own pain), we decided to switch to Float64. I agree that Float32 is better, but could be regarded as ""premature optimization"" in this case. Definitely open to discuss though.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3214#issuecomment-1679415894:773,optimiz,optimization,773,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3214#issuecomment-1679415894,1,['optimiz'],['optimization']
Performance,"> ```julia; > [ Info: ... simulation initialization complete (1.471 minutes); > [ Info: Executing initial time step...; > ERROR: LoadError: TaskFailedException; > ; > nested task error: TaskFailedException; > ; > nested task error: type Tuple has no field surface_TKE_flux; > Stacktrace:; > [1] getproperty(x::Tuple{CATKEVerticalDiffusivity{VerticallyImplicitTimeDiscretization, Float64, Oceananigans.TurbulenceClosures.CATKEVerticalDiffusivities.MixingLength{Float64}, Oceananigans.TurbulenceClosures.CATKEVerticalDiffusivities.SurfaceTKEFlux{Float64}}, AnisotropicDiffusivity{VerticallyImplicitTimeDiscretization, Float64, Float64, Float64, NamedTuple{(:b, :c, :e), Tuple{Float64, Float64, Float64}}, NamedTuple{(:b, :c, :e), Tuple{Float64, Float64, Float64}}, NamedTuple{(:b, :c, :e), Tuple{Float64, Float64, Float64}}}}, f::Symbol); > @ Base ./Base.jl:33; > [2] call; > @ ~/.julia/packages/Cassette/1lyEM/src/context.jl:456 [inlined]; > [3] fallback; > @ ~/.julia/packages/Cassette/1lyEM/src/context.jl:454 [inlined]; > [4] overdub; > @ ~/.julia/packages/Cassette/1lyEM/src/context.jl:279 [inlined]; > [5] overdub; > @ ~/.julia/packages/Oceananigans/H39qI/src/TurbulenceClosures/turbulence_closure_implementations/CATKEVerticalDiffusivities/surface_TKE_flux.jl:39 [inlined]; > [6] getbc(::BoundaryCondition{Oceananigans.BoundaryConditions.Flux, Oceananigans.BoundaryConditions.DiscreteBoundaryFunction{Oceananigans.TurbulenceClosures.CATKEVerticalDiffusivities.TKETopBoundaryConditionParameters{NamedTuple{(:b, :c, :e), Tuple{BoundaryCondition{Oceananigans.BoundaryConditions.Flux, Oceananigans.BoundaryConditions.DiscreteBoundaryFunction{NamedTuple{(:Ly, :Lz, :Qᵇ, :y_shutoff, :τ, :μ, :ΔB, :H, :h, :y_sponge, :λt), NTuple{11, Float64}}, typeof(buoyancy_flux)}}, BoundaryCondition{Oceananigans.BoundaryConditions.Flux, Nothing}, BoundaryCondition{Oceananigans.BoundaryConditions.Flux, Nothing}}}, NamedTuple{(:u, :v), Tuple{BoundaryCondition{Oceananigans.BoundaryConditions.Flux, Oceananigans.Boun",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2225#issuecomment-1030744418:129,Load,LoadError,129,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2225#issuecomment-1030744418,1,['Load'],['LoadError']
Performance,"> and we'll only have to dispatch on one type instead of three. I guess the point of this design is to simplify more generalized dispatch by ""unfusing"" the three directions. The solution you're proposing is simply returning to what we were doing before, right?. Note one immediate advantage: the `nodes` function now works with *any* field (and the implementation is actually less code than before). We lose this feature and must return to dispatching on lots of different cases (as you say, unions), if we fuse the three directions into one type. . However, this becomes even more critical when we think about abstracting operators. When performing an `x` derivative, for example, we know that the field changes location from `{Cell, Y, Z}` to `{Face, Y, Z}`. If the directions were fused, we would have to code each 3D case manually, rather than the 1 (or 2) cases we need with this design. When writing an abstracted interpolation function from an arbitrary location `X1, Y1, Z1` to `X2, Y2, Z2`, it greatly simplifies the task if the directions are ""unfused"". I think it's also easier to reason about abstracted operators on fields this way.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/409#issuecomment-531763391:639,perform,performing,639,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/409#issuecomment-531763391,1,['perform'],['performing']
Performance,"> as performance during testing doesn't matter. We do have a small number of tests that might benefit from faster code, like the pressure solver convergence tests? . > Should we still consider merging this? Once tests run I can do a comparison to see how many minutes (or seconds?) `-O0` saves us?. If it doesn't slow the tests down, I'm happy to merge and we can consider changing the optimization level in the future if its warranted. I think being explicit and deliberate about it is a positive change.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1078#issuecomment-712844340:5,perform,performance,5,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1078#issuecomment-712844340,2,"['optimiz', 'perform']","['optimization', 'performance']"
Performance,"> new HorizontallyAveragedVerticalProfile diagnostic that can calculate; > vertical profiles efficiently on-the-fly on CPUs and GPUs. Other; > diagnostics like product and covariance profiles can be built on top of it.; >; > HorizontallyAveragedVerticalProfile is kind of a mouthful, suggestions; > welcome if anyone has a better name. I wanted to explicitly state; > ""horizontally averaged"" as vertical profiles are commonly used as well and; > imply no averaging.; >; > The profile can be passed to an output writer which can write it to disk.; >; > The horizontal averaging currently relies on a parallel reduction prefix; > sum algorithm that I hacked over a CUDAnative.jl example, although I do; > have a test for the diagnostic so it does work. The algorithm can be more; > efficient (see JuliaGPU/CuArrays.jl#68; > <https://github.com/JuliaGPU/CuArrays.jl/issues/68>).; >; > It allocates very minimal amounts of memory (less than mean) and; > benchmarks show that it is ~20x faster than what we were doing before; > (copying to CPU and calculating there) which is great but it's ~5x slower; > than optimal performance.; >; > As it does not allocate memory, we can now calculate vertical profiles; > even when running large models that fill up memory.; >; > Although I should mention that an intermediate array with a size of at; > least 1*Ny*Nz is required for the parallel reduction step (so I'm using; > poisson_solvers.storage because it's a vanilla CuArray that can be; > overwritten).; >; > N, H = 512, 1; > T = N + 2H; >; > a = rand(T, T, T) |> CuArray; > h = zeros(N) |> CuArray; >; > What we were doing before:; >; > @benchmark CuArrays.@sync mean(Array(view(a, H:N+H, H:N+H, H:N+H)), dims=[1, 2]); >; > BenchmarkTools.Trial:; > memory estimate: 1.01 GiB; > allocs estimate: 250; > --------------; > minimum time: 684.013 ms (2.29% GC); > median time: 712.570 ms (6.28% GC); > mean time: 732.480 ms (8.79% GC); > maximum time: 807.437 ms (16.95% GC); > --------------; > samples: 7; > ",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/352#issuecomment-520187010:1276,perform,performance,1276,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/352#issuecomment-520187010,1,['perform'],['performance']
Performance,"> the constructors of a CuSparseMatrix are different from the ones for a SparseMatrix. Can the constructors for CuSparse be derived from Sparse? If so, we just build the sparse on CPU, and then implement `arch_array`. > It means that we will first have to create the full matrix (of size Nx * Ny x Nx * Ny!!) Or is this computationally/memory-wise restrictive?. I think you should avoid allocating the whole matrix and build the sparse representation on the fly. The sparse representation is like a graph --- we add nodes only when they exist. Once we've built the graph, we can convert it to a computationally efficient format for time-stepping on CPU or GPU. > Also it is kind of a brute force which would be quite computationally inefficient because it scales badly. I propose we get this up and running in 2D and see whether this is a feasible calculation to perform during model construction. There is the question of preconditioners / changing time-step. Do the sparse matrix implementations we work with have efficient implementations for operations like. ```julia; A += b * I; ```. where `I isa UniformScaling` and `b` is a scalar?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2396#issuecomment-1109680238:863,perform,perform,863,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2396#issuecomment-1109680238,1,['perform'],['perform']
Performance,> the gain of performance is a factor 5 by using the efficient split explicit rather than filling the halos at each substep. What does this have to do with putting boundary conditions in operators?,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3268#issuecomment-2356567627:14,perform,performance,14,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3268#issuecomment-2356567627,1,['perform'],['performance']
Performance,"> then we'd have to rewrite loads of stuff to make it so the user can specify whatever they want for the external state. I think see your point (I'm not sure I grasp the totality of it). Is your point that we are able to _re-use_ more existing code if we allow `ContinuousBoundaryFunction` and `DiscreteBoundaryFunction` as `condition`? I didn't understand this previously so thank you for clarifying. To re-use that code with a new `condition`, we can use a nesting technique:. ```julia; function getbc(open_bc::OpenBoundaryCondition, ...); state_value = getbc(open_bc.condition.external_state, ...); # other stuff related to matching; end; ```. This means that we don't have to rewrite anything, we just add new functionality for open boundary conditions on top. You could make the point that we have less _new_ code to write though, if we implement a design that allows conditions to be the same. And for users, this could be clearer, so I see the advantage of that. On the flip side, the advantage of making a new `OpenCondition` is that `getbc` retains its meaning as ""the function that returns the boundary condition value"". When we have an external state + matching scheme, the boundary condition values are not exactly the external state and are modified by the matching scheme. So I see pros and cons to both, but the good news is that we can actually talk coherently about the pros and cons. And if there are other pros and cons please mention them.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3482#issuecomment-1988781194:28,load,loads,28,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3482#issuecomment-1988781194,1,['load'],['loads']
Performance,> we now perform tons of little memory allocations somewhere. But only on CPU?,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/395#issuecomment-530398617:9,perform,perform,9,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/395#issuecomment-530398617,1,['perform'],['perform']
Performance,"> why not replace `const ƞ = 2` with `const ƞ = 2.0`, i.e. with a Float64?. I believe that if we do this, then we have to live with WENO5 that's 20x slower than the other schemes. I believe its the exponentiation with `Float64` that is slow (since exponentiation is compute intensive in general in my understanding) --- not the conversion from Int64 to Float64 (type conversions between numeric types are basically instantaneous as far as I know). In other words, the function `pow` for exponentiation with `Float64`:. https://github.com/JuliaGPU/CUDA.jl/blob/5d6127dbbef495c94d3dd8de98162188062e11b1/src/device/intrinsics/math.jl#L218. creates a bottleneck in simulations that use WENO5, whereas using `powi`:. https://github.com/JuliaGPU/CUDA.jl/blob/5d6127dbbef495c94d3dd8de98162188062e11b1/src/device/intrinsics/math.jl#L221. is much faster. > My understanding is that replacing it with with Int32 can make us forfeit some precision, no?. Why is that?. I believe we only need `Int64` if we need to represent very large integers. `Int32` integers can range from +/- 2^32 - 1, whereas 64 bit integers can range from +/- 2^64 - 1. Since 2 is represented with `Int32` there's no need to express its value in `Int64`.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1770#issuecomment-869776360:647,bottleneck,bottleneck,647,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1770#issuecomment-869776360,1,['bottleneck'],['bottleneck']
Performance,"> would there be 30+ nested parameterized types for ModelBoundaryConditions. Is/should that be a concern?. @ali-ramadhan do you mean with regards to performance? I'm not sure. With multiple dispatch being core to julia it seems this scenario is not uncommon (30+ may not be very large). > Just one initial question: I might be misunderstanding the purpose of bc.calc but why not bc.impose(args...) instead of bc.calc(args...) as we usually say that we impose boundary conditions?. The function `calc` does not actually impose a boundary condition --- the imposition of boundary condition depends on, for example, the viscosity and diffusivity, and is a property of the equation (or turbulent closure) being implemented. Again for example, the K-Profile-Parameterization includes a modification of how a flux boundary condition is implemented. In other words, the ""specification of flux"" is separate from the ""imposition of a boundary condition"". The former is determined by the user. The latter is determined by the model/governing equation.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/118#issuecomment-472225606:149,perform,performance,149,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/118#issuecomment-472225606,1,['perform'],['performance']
Performance,">Maybe I am misunderstanding you, but isn't that already what is done in the horizontal convection example?. No, you are understanding me! I think you're on to something. > Is the intended behavior that output writers automatically call fill_halo_region! before saving when with_halos=true?. Ah no this is not default. However, for prognostic fields, the halos are filled within `update_state!`:. https://github.com/CliMA/Oceananigans.jl/blob/cca182a11bcd1881e20316fc80ac7782286a8bfe/src/Models/NonhydrostaticModels/update_nonhydrostatic_model_state.jl#L24C1-L24C1. However, halos are not filled for diagnostic fields. We probably don't want to make filling halos default, since filling halo regions is expensive and useful only for a subset of experiments. However, we could add a keyword argument to `JLD2OutputWriter`, something like `fill_halos = true`. (Thinking about this with a little more coffee, it probably doesn't make sense to add something like this, because generally speaking the halo values for diagnostic fields are not useful except for periodic boundary conditions; only prognostic fields can have specified / meaningful boundary conditions.). I wonder if this is a bug in `FieldTimeSeries`. Are you using `FieldTimeSeries` to compute the diagnostics offline? Perhaps the halo data is saved correctly, but is not loaded correctly.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3224#issuecomment-1689838510:1333,load,loaded,1333,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3224#issuecomment-1689838510,1,['load'],['loaded']
Performance,">[AMGX.jl](https://github.com/JuliaGPU/AMGX.jl) only has prebuilt libraries for linux systems. That said, calling using AMGX will fail on, e.g., Mac OS X. What's the best way to optionally load AMGX.jl? Or is there another way around that?. We may be able to optionally call `using AMGX` + all the `AMGX`-related functionality by using:. [`@static` + `islinux()`]( https://docs.julialang.org/en/v1/manual/handling-operating-system-variation/) + [`CUDA.has_cuda_gpu()`](https://cuda.juliagpu.org/stable/api/essentials/#CUDA.has_cuda_gpu)",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2688#issuecomment-1244923054:189,load,load,189,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2688#issuecomment-1244923054,1,['load'],['load']
Performance,"@ Pkg.Operations /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.1/Pkg/src/Operations.jl:1075; ┌ Error: Error building `CUDAdrv`: ; └ @ Pkg.Operations /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.1/Pkg/src/Operations.jl:1075; ┌ Error: Error building `CUDAdrv`: ; └ @ Pkg.Operations /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.1/Pkg/src/Operations.jl:1075; Building SpecialFunctions → `~/.julia/packages/SpecialFunctions/fvheQ/deps/build.log`; Building LLVM ────────────→ `~/.julia/packages/LLVM/tg8MX/deps/build.log`; Building CUDAnative ──────→ `~/.julia/packages/CUDAnative/B210M/deps/build.log`; Dependent package CUDAdrv.jl has not been built successfully.; This is not a fatal error, but GPU functionality will be unavailable.; If you expected this to work, please open a thread on; https://discourse.julialang.org/c/domain/gpu; ┌ Error: Error building `CUDAnative`: ; └ @ Pkg.Operations /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.1/Pkg/src/Operations.jl:1075; ┌ Error: Error building `CuArrays`: ; └ @ Pkg.Operations /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.1/Pkg/src/Operations.jl:1075; ERROR: LoadError: LoadError: UndefVarError: CUBLAS not defined; Stacktrace:; [1] top-level scope at none:0 (repeats 2 times); [2] include at ./boot.jl:326 [inlined]; [3] include_relative(::Module, ::String) at ./loading.jl:1038; [4] include at ./sysimg.jl:29 [inlined]; [5] include(::String) at /home/travis/.julia/packages/CuArrays/qZCAt/src/CuArrays.jl:3; [6] top-level scope at none:0; [7] include at ./boot.jl:326 [inlined]; [8] include_relative(::Module, ::String) at ./loading.jl:1038; [9] include(::Module, ::String) at ./sysimg.jl:29; [10] top-level scope at none:2; [11] eval at ./boot.jl:328 [inlined]; [12] eval(::Expr) at ./client.jl:404; [13] top-level scope at ./none:3; ```; The cuBLAS error might be related to CuArrays: https://github.com/JuliaGPU/CuArrays.jl/issues/255",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/178:2298,Load,LoadError,2298,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/178,4,"['Load', 'load']","['LoadError', 'loading']"
Performance,"@ChrisRackauckas can the constant-dt Runge-Kutta methods be adapted to accept a changing time-step? This is important because we often have external criteria available (the CFL criterion, for example) that can be used to adapt time-step size. As for the fractional step method, we can also write a function that performs the fractional step algorithm using two substeps and an implicit pressure solve. Multiple fractional steps can then be embedded in a multi-step algorithm like Runge-Kutta; this may provide a route to integration with DifferentialEquations.jl. Note that the implicit pressure solves that forms the second part of the fractional step method requires a specialized fast solver for the 3D elliptic problem. We use this method:. https://www.sciencedirect.com/science/article/pii/0021999188901027. Integration with DifferentialEquations.jl will require integration of this FFT-based Poisson/Helmholtz solver into the algorithm. Our implicit solves are usually coupled, such that they require the use of fast methods for the solution of elliptic PDEs to time-step efficiently (either the FFT-based algorithm described above, or a fast batched tridiagonal solver for the GPU that we are currently working on). Can the user provide their own fast solver for implicit time stepping with the split ODE solver?. Our time-stepping method is roughly described here:. https://climate-machine.github.io/Oceananigans.jl/stable/manual/time_stepping/",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/391#issuecomment-549149954:312,perform,performs,312,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/391#issuecomment-549149954,1,['perform'],['performs']
Performance,"@Yixiao-Zhang do you want to open a PR?. Sensible errors on GPU are a persistent problem... this seems like one of the trickiest though... I think the best easy thing we can do in this case is to add an article in our Wiki about how to debug `CUDA error: an illegal memory access was encountered (code 700, ERROR_ILLEGAL_ADDRESS)`, making @simone-silvestri 's points. For documentation the error being referred to is. ```julia; ERROR: LoadError: CUDA error: an illegal memory access was encountered (code 700, ERROR_ILLEGAL_ADDRESS); Stacktrace:; [1] throw_api_error(res::CUDA.cudaError_enum); @ CUDA ~/.julia/packages/CUDA/35NC6/lib/cudadrv/libcuda.jl:27; [2] check; @ ~/.julia/packages/CUDA/35NC6/lib/cudadrv/libcuda.jl:34 [inlined]; [3] cuMemcpyHtoDAsync_v2; @ ~/.julia/packages/CUDA/35NC6/lib/utils/call.jl:26 [inlined]; [4] #unsafe_copyto!#9; @ ~/.julia/packages/CUDA/35NC6/lib/cudadrv/memory.jl:397 [inlined]; [5] (::CUDA.var""#1012#1013""{ComplexF64, CUDA.CuArray{ComplexF64, 3, CUDA.Mem.DeviceBuffer}, Int64, Vector{ComplexF64}, Int64, Int64})(); @ CUDA ~/.julia/packages/CUDA/35NC6/src/array.jl:464; [6] #context!#887; @ ~/.julia/packages/CUDA/35NC6/lib/cudadrv/state.jl:170 [inlined]; [7] context!; @ ~/.julia/packages/CUDA/35NC6/lib/cudadrv/state.jl:165 [inlined]; [8] unsafe_copyto!(dest::CUDA.CuArray{ComplexF64, 3, CUDA.Mem.DeviceBuffer}, doffs::Int64, src::Vector{ComplexF64}, soffs::Int64, n::Int64); @ CUDA ~/.julia/packages/CUDA/35NC6/src/array.jl:457; [9] copyto!; @ ~/.julia/packages/CUDA/35NC6/src/array.jl:415 [inlined]; [10] setindex!(::CUDA.CuArray{ComplexF64, 3, CUDA.Mem.DeviceBuffer}, ::ComplexF64, ::Int64, ::Int64, ::Int64); @ GPUArrays ~/.julia/packages/GPUArrays/5XhED/src/host/indexing.jl:20; [11] setindex!; @ ~/.julia/packages/GPUArrays/5XhED/src/host/indexing.jl:24 [inlined]; [12] macro expansion; @ ~/.julia/packages/GPUArraysCore/uOYfN/src/GPUArraysCore.jl:136 [inlined]; [13] solve!(ϕ::Field{Center, Center, Center, Nothing, RectilinearGrid{Float64, Periodic, Boun",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3320#issuecomment-1755989332:435,Load,LoadError,435,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3320#issuecomment-1755989332,1,['Load'],['LoadError']
Performance,"@ali-ramadhan , I tried running the example using the following `julia --project -p 4 mpi_shallow_water_turbulence.jl; `, and it seemed to fail because of error, see below. Any idea what I might have done wrong?. ```; ERROR: LoadError: ProcessExitedException(3). ...and 2 more exception(s). Stacktrace:; [1] sync_end(::Channel{Any}) at ./task.jl:314; [2] macro expansion at ./task.jl:333 [inlined]; [3] _require_callback(::Base.PkgId) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Distributed/src/Distributed.jl:75; [4] #invokelatest#1 at ./essentials.jl:710 [inlined]; [5] invokelatest at ./essentials.jl:709 [inlined]; [6] require(::Base.PkgId) at ./loading.jl:931; [7] require(::Module, ::Symbol) at ./loading.jl:923; [8] include(::Function, ::Module, ::String) at ./Base.jl:380; [9] include(::Module, ::String) at ./Base.jl:368; [10] exec_options(::Base.JLOptions) at ./client.jl:296; [11] _start() at ./client.jl:506; in expression starting at /home/fpoulin/software/Oceananigans.jl/examples/mpi_shallow_water_turbulence.jl:6; ```",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1430#issuecomment-792895347:225,Load,LoadError,225,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1430#issuecomment-792895347,3,"['Load', 'load']","['LoadError', 'loading']"
Performance,"@ali-ramadhan ; Thanks for the suggestion. I tried running it on the CPU and got a different error message, which is shown below. Additionally, the error occurs when defining background fields in the model as such; ```julia ; background_fields = Oceananigans.BackgroundFields(; background_closure_fluxes=true, b=B̄_field); ```; But it runs fine like the usual way; ```julia; background_fields = (; b=B̄_field); ```. ```; [ Info: Initializing simulation...; ERROR: LoadError: type BackgroundFields has no field u; Stacktrace:; [1] getproperty; @ ./Base.jl:37 [inlined]; [2] assemble_closure_velocities; @ ~/.julia/packages/Oceananigans/xmqSH/src/Models/NonhydrostaticModels/nonhydrostatic_tendency_kernel_functions.jl:35 [inlined]; [3] u_velocity_tendency; @ ~/.julia/packages/Oceananigans/xmqSH/src/Models/NonhydrostaticModels/nonhydrostatic_tendency_kernel_functions.jl:92 [inlined]; [4] cpu_compute_Gu!; @ ~/.julia/packages/KernelAbstractions/491pi/src/macros.jl:291 [inlined]; [5] __thread_run(tid::Int64, len::Int64, rem::Int64, obj::KernelAbstractions.Kernel{…}, ndrange::Nothing, iterspace::KernelAbstractions.NDIteration.NDRange{…}, args::Tuple{…}, dynamic::KernelAbstractions.NDIteration.DynamicCheck); @ KernelAbstractions ~/.julia/packages/KernelAbstractions/491pi/src/cpu.jl:144; [6] __run(obj::KernelAbstractions.Kernel{…}, ndrange::Nothing, iterspace::KernelAbstractions.NDIteration.NDRange{…}, args::Tuple{…}, dynamic::KernelAbstractions.NDIteration.DynamicCheck, static_threads::Bool); @ KernelAbstractions ~/.julia/packages/KernelAbstractions/491pi/src/cpu.jl:111; [7] (::KernelAbstractions.Kernel{…})(::Field{…}, ::Vararg{…}; ndrange::Nothing, workgroupsize::Nothing); @ KernelAbstractions ~/.julia/packages/KernelAbstractions/491pi/src/cpu.jl:46; [8] (::KernelAbstractions.Kernel{…})(::Field{…}, ::Vararg{…}); @ KernelAbstractions ~/.julia/packages/KernelAbstractions/491pi/src/cpu.jl:39; [9] launch!(::CPU, ::ImmersedBoundaryGrid{…}, ::Symbol, ::typeof(Oceananigans.Models.Nonhydros",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3646#issuecomment-2432725343:464,Load,LoadError,464,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3646#issuecomment-2432725343,1,['Load'],['LoadError']
Performance,"@ali-ramadhan I found that expanding the tuple did not affect performance, while adding `@inbounds` was dramatic. Tips on writing performant (and also GPU compile-able) forcing functions is a good idea. `@inbounds` seems to be the main trip-up for us.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/365#issuecomment-525328128:62,perform,performance,62,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/365#issuecomment-525328128,2,['perform'],"['performance', 'performant']"
Performance,"@ali-ramadhan I'm running on my desktop's GPU just to get a sense of the performance increase at the moment. According to `nvidia-smi` my card is a `NVS 310` (I apologize if I didn't get that right, I really have zero experience with GPUs). I'll create another issue to report this properly. Thanks for the feedback, though!. And I'm running our own group's LES model. Unfortunately I'm not allowed to share the code, but you can read about it [here](https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2018GL080296). @glwagner I'd be very willing to compare performances of our LES with Oceananigans. The caveats here being that; - our model of choice ([lagrangian-averaged scale-dynamic](https://aip.scitation.org/doi/10.1063/1.1839152)) takes relatively long to calculate compared to other models, but generally needs lower resolution to achieve the same results (so in this sense it might be similar to AMD) and that; - I'm not sure how relevant it would be to compare with us, being that I'm not allowed to share the source code. If you're still interested to make a comparison just let me know! I'd be happy to help. @ali-ramadhan @glwagner I'm not expecting at all to run their simulation at the resolution that they're running. My intent was to set-up the same case (but very coarse) and see how much I could increase the resolution (and how much activity I could see) until it became too slow or until I ran out of memory. This is basically a fun side-project, and not an actual research attempt if I'm being honest :). Thank you both for the helpful responses! I'm glad to help with whatever bugs or attempts at simulations that I can!",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/449#issuecomment-539785839:73,perform,performance,73,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/449#issuecomment-539785839,2,['perform'],"['performance', 'performances']"
Performance,"@ali-ramadhan I've addressed your concerns: we now have a LES regression test, and some changes to inner kernels means that performance is now *improved* by this PR over master (for large models).",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/452#issuecomment-543208118:124,perform,performance,124,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/452#issuecomment-543208118,1,['perform'],['performance']
Performance,"@ali-ramadhan agree that good performance would be great. I think that having a flexible and maximally useful feature might be a higher priority though. @johncmarshall54 's requirements provide a good design target, I think.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1091#issuecomment-733191938:30,perform,performance,30,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1091#issuecomment-733191938,1,['perform'],['performance']
Performance,"@ali-ramadhan and @glwagner I tried the code below in some simple tests while waiting; for my car to be serviced. . ```; using Pkg; Pkg.add(""LinearAlgebra""); using LinearAlgebra. function solveLinearSystem(A,f); # Solve Aϕ=f; tol=1.e-12; E=eigen(A);; L=E.values;; V=E.vectors;; # Get amplitudes, F, of eigenvectors that give f; F=V'*f; # Get inverse eigenvalues (zeroing inverse for v. small ones); rL=map(x -> if (abs(x)>tol) 1.0/x; else 0. ; end , L);; # Get amplitudes, Φ, of eigenvectors that give ϕ; Φ=F.*rL; # Solve for ϕ given Φ; ϕ=V*Φ; println(A*ϕ,f,ϕ); return ϕ; end. Acyc=[-2. 1 1; 1 -2. 1; 1 1 -2.];; Aneu=[-1 1 0; 1. -2. 1; 0. 1. -1];; s=size(Acyc);; nx=s[1];; g=rand(nx+1,1);; divg=g[1:end-1]-g[2:end]; mdivg=sum(divg)./size(divg)[1]; divg=divg.-mdivg; solveLinearSystem(Acyc,divg); solveLinearSystem(Aneu,divg); ```. This is algorithm that underlies the FFT approach. The FFT just optimizes (and makes it more complicated) by utilizing the fact that the eigenvector/eigenvalue coefficients for the simple, constant spacing Poisson problem, are the cos and sin terms in an FFT. Code appears to work so I am going to try and hack together a ""_plugin_"" for ```solve_poisson_3d_mbc```. . The code won't be super high performance (or work for really big problems) but (fingers crossed) it should give something clean (and short) to get started and help with debugging/optimizing on GPU. Then we can work on various FFT approaches on CPU and GPU (3-d FFT, 2-d + cyclic reduction), Greg's thought on saving for gradients in continuous form. . In principle the eigenvectors and eigenvalues above should correspond with amplitudes that come out of FFTW - except that there are a bunch of 1/2 factors, N versus m numbers, complex versus split cos/sin notation bits that need to be carefully understood etc.... . Just going to learn a little about sparse matrices in Julia - I assume they must exist! Hopefully the car will take a little longer to be finished. . Chris",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/8#issuecomment-442521365:895,optimiz,optimizes,895,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/8#issuecomment-442521365,3,"['optimiz', 'perform']","['optimizes', 'optimizing', 'performance']"
Performance,"@ali-ramadhan can you maybe summarize the problem? Sounds like there are some non-trivial trade-offs; eg opening and closing is expensive, so you don't want to do it often; but optimization has downsides... ?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1227#issuecomment-739537712:177,optimiz,optimization,177,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1227#issuecomment-739537712,1,['optimiz'],['optimization']
Performance,"@ali-ramadhan currently waiting for the preview to load so that I can merge, but I don't think it'll do that with the GPU test failing. How do we fix that?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1543#issuecomment-819553284:51,load,load,51,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1543#issuecomment-819553284,1,['load'],['load']
Performance,@ali-ramadhan do you mind running some benchmarks to test for performance regression under this PR?,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/245#issuecomment-496335163:62,perform,performance,62,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/245#issuecomment-496335163,1,['perform'],['performance']
Performance,"@ali-ramadhan do you mind running the benchmarks again?. There are still some performance issues, especially with `ConstantSmagorinsky`. But I think it might be wise to deal with these in a future PR if we are happy with these changes.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/245#issuecomment-496891797:78,perform,performance,78,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/245#issuecomment-496891797,1,['perform'],['performance']
Performance,"@ali-ramadhan ok, that sounds good to me to run in a separate PR. To clarify, is the intent of this PR to perform benchmarking, or just to ensure that the benchmark scripts do not fall out of date?. If the goal is the latter, it could make sense to decrease the size of the benchmark run to as small a model as possible --- 16^3? . What's the increase in the time taken to run a test? Looks like its about 1/3, from 20m to 30m? That's not so bad, but we should make sure this is what we want given that we may not be able to use the benchmark run output directly due to stochasticity in the Travis build. Looks like gitlab is failing...",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/727#issuecomment-613360541:106,perform,perform,106,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/727#issuecomment-613360541,1,['perform'],['perform']
Performance,@ali-ramadhan this looks good. Be good to chat next week. The general FV piece intersects with thinking about one day being able to do LES with bathymetry and still some performance.,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/471#issuecomment-541363473:170,perform,performance,170,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/471#issuecomment-541363473,1,['perform'],['performance']
Performance,@ali-ramadhan why not broadcast over CuArrays? Is there a Performance hit?,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/104#issuecomment-469749450:58,Perform,Performance,58,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/104#issuecomment-469749450,1,['Perform'],['Performance']
Performance,"@christophernhill : I wanted to confirm that I took your clever idea of using `select_device()` and added that into my code. When I ran it on 1, 2 and 4 GPUs I was able to get efficiences of 97 percent. So the code is performing very well, and the server can be efficent on multi GPUs. . The link to where the function is defined is copied below. Is this something that is done automatically in Oceananigans through `AbstractKernels.jl` or something else?. https://github.com/christophernhill/ImplicitGlobalGrid.jl/blob/5e4fd0698b7087467d9314bfa253d6bc9a09a40a/src/select_device.jl. In chatting with the developers of `ImplicitGlobalGrid.jl` they mentioned that to get efficiency I should use something called `@hide_communication` in `ParallelKernel.jl`. Again, I don't pretend to understand what this does but wanted to share the information I was given. https://github.com/omlins/ParallelStencil.jl/blob/main/src/ParallelKernel/hide_communication.jl",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1882#issuecomment-885799405:218,perform,performing,218,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1882#issuecomment-885799405,1,['perform'],['performing']
Performance,"@christophernhill Do you know if it's possible/reasonable to change the Slurm limit on Satori to 4 GPUs per user (instead of the current 1 job/1 node/4 GPU limit)? If it's a helpful change to other Satori users and does not impact cluster performance/scheduling, it might help enable all the automation we develop for Satori to seamlessly work on other clusters?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1137#issuecomment-720610838:239,perform,performance,239,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1137#issuecomment-720610838,1,['perform'],['performance']
Performance,"@christophernhill I also took a look at the `GFlops.jl` package. As said on its homepage: ""GFlops.jl does not see what happens outside the realm of Julia code. It especially does not see operations performed in external libraries such as BLAS calls.""; It works similarly to the profile macro and it can count basic math operations performed by whatever follows the macro or benchmark it for its Flops metric. These doesn't seem to work with simulations but works fine for `time_step!(model, 1)` due to the benchmarking process performing many evaluations of the code.; For the nonhydrostatic model running on CPU, `@count_ops` did not produce any results for either the simulation run or the time_step!, and `@gflops` produced the results below for time step!:; ```; 0.02 GFlops, 0.04% peak (1.89e+07 flop, 1.01e+00 s); ```",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1912#issuecomment-898103351:198,perform,performed,198,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1912#issuecomment-898103351,3,['perform'],"['performed', 'performing']"
Performance,"@francispoulin ! No worries about the question. I like questions. To answer your latest question --- our nonhydrostatic pressure solver is based on FFTs for regular grids and is therefore quite fast. In simple benchmarks long ago we found that other parts of the algorithm dominated the cost of a time-step. Because of that we aren't sure that having a hydrostatic-only solver would help. Of course, the story is different if we need to stretch the grid in horizontal directions. That said, it would still be interesting to be able to solve hydrostatic-only problems. This would complicate the algorithm a bit because you have to distinguish the barotropic mode, and perform a 2D pressure solve. We already have a function to integrate the buoyancy field to obtain the hydrostatic pressure. For ""things I want worked on"" my wish list falls into three categories: new physics features, new numerics / algorithms, and more boring software / UI work. Here's a couple... 1. Finishing the vertically-stretched grid implementation (numerics). We started work on this but its incomplete. This is a tricky and arduous task but would be quite nice to have... 2. Vertically-implicit time-stepping for diffusion terms (numerics). Ocean models typically use a time-stepping method that treats vertical diffusion terms implicitly. We haven't worried about this because we are focused on LES for the most part, or problems with very little diffusion. But ultimately we will need this, especially when we get around to implementing boundary layer closures. We have a tridiagonal solver that works on the GPU, so in some respects the hard work is already done for this problem. 3. Closures for LES and ocean modeling (Dynamic Smagorinsky, Deardorff, k-epsilon, Gent-McWilliams, convective adjustment (?) etc --- physics). We have a need to implement new turbulence closures new and old alike. Gent-McWilliams is probably easy since we already have a Leith closure implemented which calculates the tensor needed to rot",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1153#issuecomment-724262281:667,perform,perform,667,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1153#issuecomment-724262281,1,['perform'],['perform']
Performance,"@francispoulin @glwagner @ali-ramadhan @christophernhill ; I took a look at all the benchmarking scripts in our benchmarks folder and realized that many of them are very similar and can be unified. For example, the single script for shallow water model's strong and weak scaling differ only by one substring. ; The latest commit I pushed to this branch unifies all of the launcher and single scripts for shallow water model into three scripts. Now, at the top of the launcher script `distributed_shallow_water_model.jl`, there are two boolean variables that the user can toggle for strong vs weak scaling and mpi vs threaded parallel execution. Everything including output graphs, HTML tables, and info messages also change accordingly based on the two booleans. There are some other features that Francis and I have discussed but would like your approval first before adding them in.; Other possible additions to the script can include a for loop which wraps around the whole launcher script which loops through the strong/weak scaling and mpi/threaded parallelism options to allow for running 4 benchmarks at once. Another possible addition is to have what model is benchmarked also be an option. Granted, I could just copy and paste the shallow water model scripts and replace all instances of `shallow water` with `nonhydrostatic` or `hydrostatic` and tune some options a little bit, but then this would again cause avoidable clutter. Having what model is benchmarked as an easily changeable option can be achieved through a model setup function that dispatches what model is initialized based on a custom model type object that's passed to it. Everything else would be cosmetic formatting of outputs and info messages.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1881#issuecomment-899928441:1356,tune,tune,1356,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1881#issuecomment-899928441,1,['tune'],['tune']
Performance,@francispoulin I think it's always helpful to post the full error and stacktrace. From your original post. ```; ERROR: LoadError: InvalidIRError: compiling kernel gpu__compute!; ```. suggests that the error could be happening since `b̄` is being used in a `ComputedField`'s `compute!`: https://github.com/CliMA/Oceananigans.jl/blob/9eca5780658bb8f5c0debd34146a0ad5cb73c872/examples/inertially_unstable_jet.jl#L51,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1554#issuecomment-815055237:119,Load,LoadError,119,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1554#issuecomment-815055237,1,['Load'],['LoadError']
Performance,"@francispoulin sounds like a good plan! Then we can see how to proceed. Concerning the way forward with `WENOVectorInvariant`, it seems that it is very diffusive when compared to standard WENO. an example on a 256^2 periodic Bickley jet below, contours are of vertical vorticity (left) and zonal velocity (right). WENO5. https://user-images.githubusercontent.com/33547697/157509797-d61f45ba-8284-40f2-b9e9-4cafcba6db68.mp4. WENOVectorIvariant. https://user-images.githubusercontent.com/33547697/157533165-9281949b-1579-462d-ad16-e27b2f954969.mp4. Integrated Enstrophy (blue is 64^2, red is 128^2, green is 256^2, solid lines are WENO5, dashed are WENOVectorInvariant); ![enstrophy](https://user-images.githubusercontent.com/33547697/157510310-3acfaeaf-4d05-4fac-b332-e7b66563147e.png). @sandreza was pointing out that maybe the extra diffusivity comes from the fact that we are advecting vorticity which contains much thinner structures than velocity, then maybe the ""optimal WENO weight"" are not so optimal anymore. These weights are 3 constant parameters, which are quite empirical and ""free-to-choose"" (in our case just `1/10` for the downstream, `6/10` for the central and `3/10` for the upstream stencil). Dissipation can be reduced by increasing the ""central stencil weight"". The idea with @sandreza was to perform an easy calibration to try to ""reasonably"" preserve enstrophy in the Bickley jet (then maybe calibrate it also against the spherical bicklet jet or R-H waves where also Coriolis is involved). The objective is to completely eliminate explicit viscous dissipation in the horizontal direction (harmonic and/or biharmonic) for the global simulation",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2317#issuecomment-1063249741:1313,perform,perform,1313,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2317#issuecomment-1063249741,1,['perform'],['perform']
Performance,"@glwagner ; After merging this PR and running it on the GPU, I encounter the following error. (I'm using Julia 1.10.2 and Oceananigans v0.92.0):. ```; [ Info: Initializing simulation...; ERROR: LoadError: InvalidIRError: compiling MethodInstance for Oceananigans.Models.NonhydrostaticModels.gpu_compute_Gu!(::KernelAbstractions.CompilerMetadata{KernelAbstractions.NDIteration.StaticSize{(10, 10, 10)}, KernelAbstractions.NDIteration.DynamicCheck, Nothing, Nothing, KernelAbstractions.NDIteration.NDRange{3, KernelAbstractions.NDIteration.StaticSize{(1, 1, 10)}, KernelAbstractions.NDIteration.StaticSize{(16, 16, 1)}, Nothing, Nothing}}, ::OffsetArrays.OffsetArray{Float64, 3, CuDeviceArray{Float64, 3, 1}}, ::ImmersedBoundaryGrid{Float64, Oceananigans.Grids.Periodic, Oceananigans.Grids.Periodic, Bounded, RectilinearGrid{Float64, Oceananigans.Grids.Periodic, Oceananigans.Grids.Periodic, Bounded, Float64, Float64, OffsetArrays.OffsetVector{Float64, CuDeviceVector{Float64, 1}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVector{Float64, CuDeviceVector{Float64, 1}}, Nothing}, GridFittedBottom{Field{Center, Center, Nothing, Nothing, Nothing, Nothing, OffsetArrays.OffsetArray{Float64, 3, CuDeviceArray{Float64, 3, 1}}, Float64, Nothing, Nothing, Nothing}, Oceananigans.ImmersedBoundaries.CenterImmersedCondition}, Nothing, Nothing, Nothing}, ::Nothing, ::Tuple{WENO{3, Float64, Nothing, Nothing, Nothing, Nothing, WENO{2, Float64, Nothing, Nothing, Nothing, Nothing, UpwindBiased{1, Float64, Nothing, Nothing, Nothing, Nothing, Centered{1, Float64, Nothing, Nothing, Nothing, Nothing}}, Centered{1, Float64, Nothing, Nothing, Nothing, Nothing}}, Centered{2, Float64, Nothing, Nothing, Nothing, Centered{1, Float64, Nothing, Nothing, Nothing, Nothing}}}, ConstantCartesia",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3646#issuecomment-2428001700:194,Load,LoadError,194,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3646#issuecomment-2428001700,1,['Load'],['LoadError']
Performance,"@glwagner @tomchor ; I have just tested the turbulent boundary layer with wall model using Oceananigans on Julia 1.6.7 and Julia 1.9.2. The performances of AMD in both versions are exactly the same. It turns to overestimate the velocity shear at the second node from the wall, therefore, turns to overestimate the velocity in the middle and upper parts of the boundary layers. I used to use the SGS model based on the Lagrangian-averaged scale-dependent dynamic model (LASD) (https://pubs.aip.org/aip/pof/article/17/2/025105/895722/A-scale-dependent-Lagrangian-dynamic-model-for). @tomchor is very familiar with this SGS model. The performance of the LASD close to the wall is usually good, as you can see here; ![025105_1_f2](https://github.com/CliMA/Oceananigans.jl/assets/20816949/e4aa1b73-d37e-4afa-b3ec-317d2edf7769). I guess the problem of AMD is partly solved in reference Yang et al. (2017). Now the problem is that if someone can implement this filtering in the code or not. I am stilling learning the Oceananigans and Julia. I hope that someday in the future, I am able to implement this technique.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3195#issuecomment-1652877426:140,perform,performances,140,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3195#issuecomment-1652877426,2,['perform'],"['performance', 'performances']"
Performance,@glwagner Do you think you'll have some time to review this PR this week?. It affects a lot of code you've written and is a bit of a bottleneck as it refactors a lot of the existing code.,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/529#issuecomment-558325070:133,bottleneck,bottleneck,133,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/529#issuecomment-558325070,1,['bottleneck'],['bottleneck']
Performance,"@glwagner I'm running into a similar issue (v0.30.0):. ```; ERROR: LoadError: AssertionError: CUDAdrv.jl did not successfully initialize, and is not usable.; Stacktrace:; [1] libcuda at /home/mike/.julia/packages/CUDAdrv/Uc14X/src/CUDAdrv.jl:82 [inlined]; [2] (::CUDAdrv.var""#535#cache_fptr!#11"")() at /home/mike/.julia/packages/CUDAapi/XuSHC/src/call.jl:31; [3] macro expansion at /home/mike/.julia/packages/CUDAapi/XuSHC/src/call.jl:39 [inlined]; [4] macro expansion at /home/mike/.julia/packages/CUDAdrv/Uc14X/src/libcuda.jl:36 [inlined]; [5] macro expansion at /home/mike/.julia/packages/CUDAdrv/Uc14X/src/error.jl:110 [inlined]; [6] cuDeviceGetCount at /home/mike/.julia/packages/CUDAapi/XuSHC/src/call.jl:93 [inlined]; [7] length at /home/mike/.julia/packages/CUDAdrv/Uc14X/src/devices.jl:105 [inlined]; [8] iterate(::CUDAdrv.DeviceSet, ::Int64) at /home/mike/.julia/packages/CUDAdrv/Uc14X/src/devices.jl:100 (repeats 2 times); [9] iterate at ./iterators.jl:139 [inlined]; [10] iterate(::Base.Iterators.Enumerate{CUDAdrv.DeviceSet}) at ./iterators.jl:138; [11] top-level scope at /home/mike/.julia/packages/Oceananigans/1xP6n/src/Oceananigans.jl:131; [12] include(::Module, ::String) at ./Base.jl:377; [13] top-level scope at none:2; [14] eval at ./boot.jl:331 [inlined]; [15] eval(::Expr) at ./client.jl:449; [16] top-level scope at ./none:3; in expression starting at /home/mike/.julia/packages/Oceananigans/1xP6n/src/Oceananigans.jl:124; ```",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/788#issuecomment-653563655:67,Load,LoadError,67,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/788#issuecomment-653563655,1,['Load'],['LoadError']
Performance,"@glwagner [AMGX.jl](https://github.com/JuliaGPU/AMGX.jl) only has prebuilt libraries for linux systems. That said, calling `using AMGX` will fail on, e.g., Mac OS X. What's the best way to optionally load AMGX.jl? Or is there another way around that?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2688#issuecomment-1241308749:200,load,load,200,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2688#issuecomment-1241308749,1,['load'],['load']
Performance,"@glwagner please help here... I can't even say `using Oceananigans`.... ```julia; julia> using Oceananigans; [ Info: Precompiling Oceananigans [9e8cae18-63c1-5223-a75c-80ca9d6e9a09]; ERROR: LoadError: LoadError: LoadError: UndefVarError: NFBC not defined; Stacktrace:; [1] top-level scope; @ ~/Research/OC/src/BoundaryConditions/apply_flux_bcs.jl:23; [2] include(mod::Module, _path::String); @ Base ./Base.jl:386; [3] include(x::String); @ Oceananigans.BoundaryConditions ~/Research/OC/src/BoundaryConditions/BoundaryConditions.jl:1; [4] top-level scope; @ ~/Research/OC/src/BoundaryConditions/BoundaryConditions.jl:45; [5] include(mod::Module, _path::String); @ Base ./Base.jl:386; [6] include(x::String); @ Oceananigans ~/Research/OC/src/Oceananigans.jl:1; [7] top-level scope; @ ~/Research/OC/src/Oceananigans.jl:173; [8] include; @ ./Base.jl:386 [inlined]; [9] include_package_for_output(pkg::Base.PkgId, input::String, depot_path::Vector{String}, dl_load_path::Vector{String}, load_path::Vector{String}, concrete_deps::Vector{Pair{Base.PkgId, UInt64}}, source::Nothing); @ Base ./loading.jl:1213; [10] top-level scope; @ none:1; [11] eval; @ ./boot.jl:360 [inlined]; [12] eval(x::Expr); @ Base.MainInclude ./client.jl:446; [13] top-level scope; @ none:1; in expression starting at /Users/navid/Research/OC/src/BoundaryConditions/apply_flux_bcs.jl:23; in expression starting at /Users/navid/Research/OC/src/BoundaryConditions/BoundaryConditions.jl:1; in expression starting at /Users/navid/Research/OC/src/Oceananigans.jl:1; ERROR: Failed to precompile Oceananigans [9e8cae18-63c1-5223-a75c-80ca9d6e9a09] to /Users/navid/.julia/compiled/v1.6/Oceananigans/jl_fSB7qw.; Stacktrace:; [1] error(s::String); @ Base ./error.jl:33; [2] compilecache(pkg::Base.PkgId, path::String, internal_stderr::Base.TTY, internal_stdout::Base.TTY); @ Base ./loading.jl:1360; [3] compilecache(pkg::Base.PkgId, path::String); @ Base ./loading.jl:1306; [4] _require(pkg::Base.PkgId); @ Base ./loading.jl:1021; [5] require(",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1744#issuecomment-877746334:190,Load,LoadError,190,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1744#issuecomment-877746334,3,['Load'],['LoadError']
Performance,"@glwagner that sounds like a good strategy, ideally you would want to front-load the most useful tests for catching errors to the degree possible (the whole goal here is to reduce iteration time). If using Linux we can have up to 60 concurrent actions going so spawning as many as is useful for reducing the iteration time is the correct strategy (probably you could set this up as a job matrix for compactness). If spawning up a ton of github actions you can maximize concurrency by killing stale jobs (old push commits) as we do here: https://github.com/CliMA/ClimaCore.jl/blob/main/.github/workflows/Linux-UnitTests.yml#L24",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1962#issuecomment-906584759:76,load,load,76,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1962#issuecomment-906584759,3,"['concurren', 'load']","['concurrency', 'concurrent', 'load']"
Performance,"@glwagner the solution seems the same but two difference are that (1) `VectorInvariant` does not crash, (2) `WENOVectorInvariant` deviates from the pattern later. ; ; In this paper https://www.tandfonline.com/doi/pdf/10.3402/tellusa.v52i2.12258 they claim that : ""In the FV model, small departures from the flow pattern are triggered initially by grid related truncation errors but subsequently grow through the dynamical instability"" . I guess ""time-to-instability"" is itself a diagnostic of the performance of the scheme? . @francispoulin As implemented in (https://reader.elsevier.com/reader/sd/pii/S0021999105800166?token=A3A72AC493072CED8ECF098513A0BD1F822D2F2224207E533C86FB7D40361903E7AC0B4304841E64E089CBBFCEAFD08B&originRegion=us-east-1&originCreation=20220308154939) the cosine bell advection only tests the tracer advection, do you have any example of a test of that kind that tests momentum advection?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2317#issuecomment-1061931114:497,perform,performance,497,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2317#issuecomment-1061931114,1,['perform'],['performance']
Performance,"@glwagner, @navidcy ; Poll to decide where to put `η` in `z`. . The possibilities are:; - location `Face`, index `grid.Nz+1`. Pros: natural location of `η` field (same position of the `w` velocity).; - location `Center`, index `grid.Nz`. Pros: every calculation dealing with `η` is now performed at centers (ex `Az_∇h²ᶜᶜᶜ` and `linear_operation!`. . In case of the second option should we change all the metrics to reflect the fact that we are at faces?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2740#issuecomment-1248631573:286,perform,performed,286,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2740#issuecomment-1248631573,1,['perform'],['performed']
Performance,"@glwagner, I think you're right that the error is coming from this line:; https://github.com/CliMA/Oceananigans.jl/blob/10a9479ec462f4f5f053e7447ff667fdfe20542d/src/Distributed/distributed_fft_based_poisson_solver.jl#L65; I checked the sizes of these variables, and here xc is sized on the global grid while lambda x is sized on the local grid. I'm not sure which grid this operation is intended to be performed on. The code here is impressively compact but not particularly easy to decipher (at least for me).",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2347#issuecomment-1067340658:402,perform,performed,402,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2347#issuecomment-1067340658,1,['perform'],['performed']
Performance,"@glwagner, regarding the; ```Julia; ERROR: LoadError: UndefVarError: bc not defined; ```; in the Docs, is it related to; https://github.com/JuliaDocs/Documenter.jl/issues/228; perhaps?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1843#issuecomment-879623792:43,Load,LoadError,43,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1843#issuecomment-879623792,1,['Load'],['LoadError']
Performance,"@hennyg888 do you have the same problems using MPI instead of multi-threaded, and on the same CPU ( Intel(R) Xeon(R) Platinum 8260 CPU @ 2.40GHz ). ?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1861#issuecomment-880838040:62,multi-thread,multi-threaded,62,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1861#issuecomment-880838040,1,['multi-thread'],['multi-threaded']
Performance,"@jakebolewski I'm wondering what you think about the strategy of running a few key integration tests via GitHub actions, in addition perhaps to unit tests that have low compilation costs. For example, we have a couple ""regression tests"" that ensure the output of a simulation is identical to one run previously. These regression tests are imprecise (we only know that something has changed, but can't identify exactly what it is), but nevertheless catch both obvious API bugs and subtle numerics bugs incurred by refactoring. Another nice feature is that regression tests invoke (or are supposed to invoke) configurations that are most common / most valuable to users. They are also relatively cheap since we load states from a file and run for 10-100 time-steps at modest resolutions, and there are a small number of them which mitigates compilation cost. When I undertake a major refactor I often find myself running regression tests first. Testing corner cases and/or catching bugs associated with interactions between model components in less-frequently used configurations requires more extensive testing (a combinatorial explosion of cases...) and incurs heavier compilation costs; so these are probably better tested via `bors try`.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1962#issuecomment-906570444:709,load,load,709,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1962#issuecomment-906570444,1,['load'],['load']
Performance,"@matinraayai is working on making PencilArrays performant. This PR is exploratory and is a fallback that might not be merged if we find an efficient way to do GPU transposes with PencilArrays (requires reducing memory allocations and improving the efficiency of permute operations) and implement r2r Fourier transforms in PencilFFTs. ; For the moment those two elements are part of this PR. This PR follows the (simple) configuration of the 2decomp library https://github.com/2decomp-fft/2decomp-fft,; the difference between PencilFFT/PencilArray and this PR (a part bounded domain ffts) is that here (at the moment) we impose the stricter limitation that `Ny` has to be divisible by `Rx` and `Ry` while `Nz` has to be divisible by `Ry`, where `Rx` and `Ry` are the number of ranks (divisions) in the x and y direction. Relaxing the requirements should not be too difficult.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3279#issuecomment-1727661521:47,perform,performant,47,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3279#issuecomment-1727661521,1,['perform'],['performant']
Performance,"@mukund-gupta reported the following error:. ```; [ Info: Oceananigans will use 16 threads; [ Info: Starting the simulation...; [ Info: i: 0010, t: 1.667 minutes, Δt: 10 seconds; ERROR: LoadError: MethodError: no method matching isless(::typeof(depth_dependent_κ), ::typeof(depth_dependent_κ)); Closest candidates are:; isless(!Matched::Missing, ::Any) at missing.jl:87; isless(::Any, !Matched::Missing) at missing.jl:88; Stacktrace:; [1] max(::Function, ::Function) at ./operators.jl:417; [2] BottomRF at ./reduce.jl:81 [inlined]; [3] _foldl_impl(::Base.BottomRF{typeof(max)}, ::Base._InitialValue, ::NamedTuple{(:T, :S),Tuple{typeof(depth_dependent_κ),typeof(depth_dependent_κ)}}) at ./reduce.jl:62; [4] foldl_impl(::Base.BottomRF{typeof(max)}, ::NamedTuple{(),Tuple{}}, ::NamedTuple{(:T, :S),Tuple{typeof(depth_dependent_κ),typeof(depth_dependent_κ)}}) at ./reduce.jl:48; [5] mapfoldl_impl(::typeof(identity), ::typeof(max), ::NamedTuple{(),Tuple{}}, ::NamedTuple{(:T, :S),Tuple{typeof(depth_dependent_κ),typeof(depth_dependent_κ)}}) at ./reduce.jl:44; [6] mapfoldl(::Function, ::Function, ::NamedTuple{(:T, :S),Tuple{typeof(depth_dependent_κ),typeof(depth_dependent_κ)}}; kw::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at ./reduce.jl:160; [7] mapfoldl at ./reduce.jl:160 [inlined]; [8] #mapreduce#208 at ./reduce.jl:287 [inlined]; [9] mapreduce(::Function, ::Function, ::NamedTuple{(:T, :S),Tuple{typeof(depth_dependent_κ),typeof(depth_dependent_κ)}}) at ./reduce.jl:287; [10] maximum(::NamedTuple{(:T, :S),Tuple{typeof(depth_dependent_κ),typeof(depth_dependent_κ)}}) at ./reduce.jl:652; [11] cell_diffusion_timescale(::AnisotropicDiffusivity{Int64,Int64,typeof(depth_dependent_κ),NamedTuple{(:T, :S),Tuple{Int64,Int64}},NamedTuple{(:T, :S),Tuple{Int64,Int64}},NamedTuple{(:T, :S),Tuple{typeof(depth_dependent_κ),typeof(depth_dependent_κ)}}}, ::Tuple{Nothing,Nothing}, ::RegularCartesianGrid{Float64,Periodic,Periodic,Bounded,OffsetArrays.OffsetArray{Float64,1,StepRange",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1104:186,Load,LoadError,186,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1104,1,['Load'],['LoadError']
Performance,"@navidcy I'm trying to add a drag flux to this example like we talked about in https://github.com/CliMA/Oceananigans.jl/issues/3148 but I'm getting an error that I can't figure out. Using `u` as an example, I'm trying to implement the BCs as . ```julia; @inline u_drag(x, y, t, u, v, w, p) = - p.cᵈ * u * √(u^2 + v^2 + w^2); @inline u_drag(x, y, z, t, u, v, w, cᵈ) = u_drag(x, y, t, u, v, w, cᵈ). u_drag_bc = FluxBoundaryCondition(u_drag, field_dependencies=(:u, :v, :w), parameters=(; cᵈ=1e-3)); boundary_conditions = (u = FieldBoundaryConditions(bottom=u_drag_bc),); ```. but when time-stepping the model I get. ```; ERROR: LoadError: MethodError: no method matching field_arguments(::Int64, ::Int64, ::Int64, ::ImmersedBoundaryGrid{Float64, Periodic, Flat, Bounded, RectilinearGrid{Float64, Periodic, Flat, Bounded, Float64, Float64, Float64, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, CPU}, GridFittedBottom{OffsetArrays.OffsetMatrix{Float64, Matrix{Float64}}, Oceananigans.ImmersedBoundaries.CenterImmersedCondition}, Nothing, CPU}, ::NamedTuple{(:u, :v, :w, :b, :η), Tuple{Field{Face, Center, Center, Nothing, ImmersedBoundaryGrid{Float64, Periodic, Flat, Bounded, RectilinearGrid{Float64, Periodic, Flat, Bounded, Float64, Float64, Float64, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, CPU}, GridFittedBottom{OffsetArrays.OffsetMatrix{Float64, Matrix{Float64}}, Oceananigans.ImmersedBoundaries.Cent",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3132#issuecomment-1667152069:626,Load,LoadError,626,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3132#issuecomment-1667152069,1,['Load'],['LoadError']
Performance,"@navidcy In the context of Enzyme (a tool for automatically differentiating LLVM), to ""mark"" a function as non-differentiable means to use `inactive_noinl` to tell Enzyme that it should not try to apply the chain rule to that function. Marking functions as non-differentiable has various reasons but here the reason is given as ""performance"". The code changed is also quite short so I think the description is complete.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3799#issuecomment-2383154154:329,perform,performance,329,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3799#issuecomment-2383154154,1,['perform'],['performance']
Performance,"@navidcy it shouldn't impact developer workflow too much. When going over the test structure with @glwagner on Tues. I suggested the following restructuring:. * Break-out ""quick"" unit-tests to be run by github actions under a single configuration (ex: linux, julia 1.6) and be able to run them in parallel. We have a ""Team"" account donated by Github so we can have tons of concurrent GitHub actions so this is beneficial if you can take advantage (might be limited by compilation costs). These tests will be run for every PR push and fast fail on error.; * `bors try` trigger a more expensive CI job to be submitted to the cluster, allowing for GPU / MPI tests. The logic here is that if the cpu tests are not working then the GPU tests almost certainly won't so you can get away with executing them less often. We have a daemon running on the cluster that synchronizes the jobs from buildkite with the local slurm controller, so every step in the buildkite config is submitted as a separate slurm job and canceling buildkite jobs kills them with slum. What is nice about that setup is you can tailor the resources used for each buildkite step just as you would slurm (ex. ""gres:1"" for 1 gpu). You can run jobs on multiple ranks, multiple GPU's, different resource limits, timeouts, etc. basically anything you can pass through to as a cli argument to a slurm batch job is supported. Also it's running on a cluster so obviously your job parallelism is very good.; * `bors r+` trigger merging the PR into `main` branch. This serializes the commits to `main` (and roll-up concurrent PR's to be submitted) so that all merge commits will pass the tests. This is an opportunity to also maybe run more expensive tests (it's easy in buildkite to conditionally run steps if running on `staging` branch) because you'll probably only run the staging CI step one or at most a few times at the very end,. the general strategy is to tier the tests so that they get progressively more expensive and to maximize ci-p",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1962#issuecomment-906541778:373,concurren,concurrent,373,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1962#issuecomment-906541778,1,['concurren'],['concurrent']
Performance,"@navidcy thanks for the review! ; Indeed maybe we should document it. . Just to give a quick explanation of the different flavours. WENO involves the reconstruction of the value of the advected quantity on the face performed on different ""biased stencils"". <img src=""https://render.githubusercontent.com/render/math?math=v_{i %2B 1/2 ,r} = p_r(x_{i %2B 1/2}) = \sum_{j=0}^k c_{rj} \overline{v}_{i - r %2B j}"">. where <img src=""https://render.githubusercontent.com/render/math?math=p""> is the reconstruction polynomial constructed from the biased stencil <img src=""https://render.githubusercontent.com/render/math?math=r""> and <img src=""https://render.githubusercontent.com/render/math?math=k""> is the order of the polynomial. As we want a 5th order scheme, there will be three polynomials (<img src=""https://render.githubusercontent.com/render/math?math=r = 0, 1, 2"">) with (<img src=""https://render.githubusercontent.com/render/math?math=k = 2"">). Now the coefficients of these polynomials (<img src=""https://render.githubusercontent.com/render/math?math=c_{rj}"">) are based on the Lagrange interpolation of the integral of <img src=""https://render.githubusercontent.com/render/math?math=\overline{v}_{i}""> and, as such, are coordinate-dependent. The first thing to do, when wanting to implement the WENO scheme on a stretched grid, is then to make sure that the reconstruction is correctly done, and, as such, `WENO5(grid = grid)` takes care that (<img src=""https://render.githubusercontent.com/render/math?math=c_{rj}"">) are correctly calculated based on the underlying grid. Now, WENO schemes differ from ENO schemes since they do not just choose the ""smoothest stencil"" among the (in this case) three stencils, but weight them as such. <img src=""https://render.githubusercontent.com/render/math?math=v_{i %2B 1/2} = \sum_{r=0}^k w_{r} v_{i %2B 1/2,r} "">. Those weights have to satisfy <img src=""https://render.githubusercontent.com/render/math?math=\sum_{r=0}^k w_{r} = 1""> and are mostly a matte",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2060#issuecomment-975649397:215,perform,performed,215,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2060#issuecomment-975649397,1,['perform'],['performed']
Performance,@navidcy what's the last version before the catastrophic performance loss? I'll do a benchmark to compare with `main`.,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2996#issuecomment-1480454534:57,perform,performance,57,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2996#issuecomment-1480454534,1,['perform'],['performance']
Performance,"@peterahrens suggested removing `ModelMetadata` from `Field` structs and make the `Grid` a parameter, i.e. `Grid{T,G}` then it should be isbitstype. It's not isbits right now because T<:AbstractArray. Then we should be able to use some abstractions in the CPU/GPU element-wise kernels as well as multiple dispatch and won't need to have `δx_c2f`, `δx_f2c`, `δx_e2f`, `δx_f2e`, etc. Some goals for guidance:; - [ ] Prototype grid and field types that are `isbitstype`.; - [ ] Test that they do work with GPUifyLoops on the GPU. For this I will extend the example from https://github.com/vchuravy/GPUifyLoops.jl/pull/18; - [ ] Benchmark the simple example to see things slow down. I don't expect to see much of a difference.; - [ ] Refactor the operators and time stepping loop to use the new abstractions!; - [ ] Add tests to make sure any structure that may be passed to a CUDA kernel `isbitstype`. Just construct a bunch of grids, fields, and forcing functions and test that they are indeed `isbitstype`.; - [ ] Benchmark the model with `isbitstype` abstractions to make sure performance hasn't degraded.",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/59:1077,perform,performance,1077,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/59,1,['perform'],['performance']
Performance,"@raphaelouillon I can reproduce `ERROR: LoadError: CUDA error (code 201, CUDA_ERROR_INVALID_CONTEXT)` when running `pkg> test Oceananigans` for `0.28.0`--`0.30.0`. Tests run for `0.27.1`, however.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/788#issuecomment-654868729:40,Load,LoadError,40,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/788#issuecomment-654868729,1,['Load'],['LoadError']
Performance,@sandreza and Adeline have a script that explodes into NaNs almost immediately at 128³ but takes many thousands of iterations to produce a NaN at 32³. Reverting to Oceananigans v0.27.0 the script runs fine. @sandreza suggested that it might be a race condition between kernels since KernelAbstractions.jl allows for multiple streams. Race conditions get much more (exponentially?) frequent as the number of threads increase I guess. I'm trying to reproduce this issue with a minimal working example.,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/816:246,race condition,race condition,246,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/816,2,"['Race condition', 'race condition']","['Race conditions', 'race condition']"
Performance,@sandreza might be a good idea to code up the case we were using the diagnose the race condition as a test and see if we can catch the race condition in CI @ali-ramadhan,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1985#issuecomment-926693915:82,race condition,race condition,82,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1985#issuecomment-926693915,2,['race condition'],['race condition']
Performance,"@siddharthabishnu, `CUDA.@allowscalar` introduced by [3cdd470](https://github.com/CliMA/Oceananigans.jl/pull/3488/commits/3cdd4705fedf3238d8858bb0f56e9b27b01ba34a) is detrimental for performance. Like it induces O(10-100x) slowdown I think.... Is this a temporary solution?. cc @glwagner, @simone-silvestri",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3488#issuecomment-2033827149:183,perform,performance,183,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3488#issuecomment-2033827149,1,['perform'],['performance']
Performance,"@simone-silvestri : I tried running the branch in it's current version and received a GPU error. Is this something that works on your machine?. ```; ERROR: LoadError: InvalidIRError: compiling kernel gpu_calculate_Guh!(Cassette.Context{nametype(CUDACtx), Nothing, Nothing, KernelAbstractions.v; ...; ```",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2522#issuecomment-1130253785:156,Load,LoadError,156,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2522#issuecomment-1130253785,1,['Load'],['LoadError']
Performance,@simone-silvestri @jagoosw I think this PR may solve the race condition issue. The essential reason is that the Manifest was being re-resolved after init because downstream tests imported additional packages that were not imported during the initial instantiation.,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3783#issuecomment-2361545069:57,race condition,race condition,57,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3783#issuecomment-2361545069,1,['race condition'],['race condition']
Performance,"@simone-silvestri In addition to the NetCDF issue I mentioned [here](https://github.com/CliMA/Oceananigans.jl/pull/2795#issuecomment-1304038524), I found out that the `cell_advection_timescale()` and `interior()` aren't working with `MultiRegionGrids` yet. Here's an error example:. ```julia; ERROR: LoadError: type MultiRegionObject has no field parent; Stacktrace:; [1] getproperty; @ ./Base.jl:38 [inlined]; [2] cell_advection_timescale(model::NonhydrostaticModel{Oceananigans.TimeSteppers.QuasiAdamsBashforth2TimeStepper{Float64, NamedTuple{(:u, :v, :w), Tuple{Field{Face, Center, Center, Nothing, MultiRegionGrid{Float64, Bounded, Periodic, Bounded, XPartition{Int64}, MultiRegionObject{Tuple{RectilinearGrid{Float64, RightConnected, Periodic, Bounded, Float64, Float64, Float64, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, CPU}, RectilinearGrid{Float64, LeftConnected, Periodic, Bounded, Float64, Float64, Float64, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, CPU}}, Tuple{CPU, CPU}}, Tuple{CPU, CPU}, CPU}, MultiRegionObject{Tuple{Tuple{Colon, Colon, Colon}, Tuple{Colon, Colon, Colon}}, Tuple{CPU, CPU}}, MultiRegionObject{Tuple{OffsetArrays.OffsetArray{Float64, 3, Array{Float64, 3}}, OffsetArrays.OffsetArray{Float64, 3, Array{Float64, 3}}}, Tuple{CPU, CPU}}, Any, MultiRegionObject{Tuple{FieldBoundaryConditions{Nothing, Bo",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2795#issuecomment-1304574572:300,Load,LoadError,300,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2795#issuecomment-1304574572,1,['Load'],['LoadError']
Performance,"@simone-silvestri Will do!. At the moment I am using `momentum_advection = VectorInvariant()`. If it goes unstable I will try the one you suggested on the other PR. I also tried what you suggested on the other PR,. ```; VectorInvariant(vorticity_scheme = WENO(), kinetic_energy_gradient_scheme = WENO()); ```. This failed because of the error below. Any idea what I need to do to fix this?. ```; ERROR: LoadError: MethodError: no method matching _symmetric_interpolate_yᵃᶜᵃ(::Int64, ::Int64, ::Int64, ::ImmersedBoundaryGrid{…}, ::EnergyConserving{…}, ::typeof(Oceananigans.Advection.δx_v²), ::Field{…}, ::Field{…}). Closest candidates are:; _symmetric_interpolate_yᵃᶜᵃ(::Any, ::Any, ::Any, ::ImmersedBoundaryGrid, ::Union{Centered{2}, Centered{3}, Centered{4}, Centered{5}, Centered{6}, UpwindBiased{2}, UpwindBiased{3}, UpwindBiased{4}, UpwindBiased{5}, UpwindBiased{6}, WENO}, ::Any...); @ Oceananigans ~/Software/Oceananigans.jl/src/ImmersedBoundaries/conditional_fluxes.jl:210; _symmetric_interpolate_yᵃᶜᵃ(::Any, ::Any, ::Any, ::ImmersedBoundaryGrid, ::Union{Centered{1}, UpwindBiased{1}}, ::Any...); @ Oceananigans ~/Software/Oceananigans.jl/src/ImmersedBoundaries/conditional_fluxes.jl:207; _symmetric_interpolate_yᵃᶜᵃ(::Any, ::Any, ::Any, ::Any, ::VectorInvariant{<:Any, <:Any, true}, ::Any, ::Any...); @ Oceananigans ~/Software/Oceananigans.jl/src/Advection/vector_invariant_advection.jl:250; ... ```",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3502#issuecomment-1984534233:403,Load,LoadError,403,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3502#issuecomment-1984534233,1,['Load'],['LoadError']
Performance,"@simone-silvestri and I have discussed a number of improvements to solvers (mostly refactoring and cleanup); I'm opening this issue to record our discussion. Here are some items in no particular order:. 1. `HeptadiagonalIterativeSolver` should use our internal `PreconditionedConjugateGradientSolver`. This is important because we need to use `PreconditionedConjugateGradientSolver` elsewhere, and we want to a) ensure that it's optimized and b) ensure we can reason about _why_ one solver is faster than another. 2. Related to 1., we can refactor `PreconditionedConjugateGradientSolver` to have a similar interface as `cg` from `IterativeSolvers`. Then we can swap in and out. 3. `HeptadiagonalIterativeSolver` should solve linear systems only, and have nothing to do with time-stepping. Right now `solve!` for the `HeptadiagonalIterativeSolver` is entangled with time-stepping:. https://github.com/CliMA/Oceananigans.jl/blob/90f7de4cb3ed2fa9e22231177fd56b0f33c39df9/src/Solvers/heptadiagonal_iterative_solver.jl#L286-L308. since it ""knows"" about ""`previous_Δt`"". This logic should be moved to `MatrixIterativeFreeSurfaceSolver`. 4. We should disentangle / modularize the implementation of matrix-based preconditioners in `HeptadiagonalIterativeSolver`. In particular we should be able to use matrix-based preconditioners with any iterative solver that uses `PreconditionedConjugateGradientSolver`. Right now, the fastest way to compute left-hand-sides is with `PCGImplicitFreeSurfaceSolver`, but the best preconditioners can only be used with `MatrixIterativeSolver`. If our code were more modular, we could use the matrix-based preconditioners with `PCGImplicitFreeSurfaceSolver` to achieve the fastest possible combination of methods. Note that some of the changes in #2412 will help (in particular, an improvement to the interface into `PreconditionedConjugateGradientSolver` for defining preconditioners). There's probably a few other things on the todo list so please add them here.",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2418:429,optimiz,optimized,429,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2418,1,['optimiz'],['optimized']
Performance,"@simone-silvestri any tips?. For conditionals, we should use `ifelse`; then there is no issue for performance.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2737#issuecomment-1252872891:98,perform,performance,98,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2737#issuecomment-1252872891,1,['perform'],['performance']
Performance,"@simone-silvestri can I convince you to rewrite this section with updated benchmarks, and include results for distributed systems?. https://github.com/CliMA/Oceananigans.jl?tab=readme-ov-file#performance-benchmarks",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3684:192,perform,performance-benchmarks,192,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3684,1,['perform'],['performance-benchmarks']
Performance,"@simone-silvestri this doesn't seem to be working when the domain is `Bounded` in the x direction. Is this expected?:. ```julia; ERROR: LoadError: MethodError: no method matching PressureSolver(::CPU, ::MultiRegionGrid{Float64, Bounded, Bounded, Bounded, XPartition{Int64}, MultiRegionObject{Tuple{RectilinearGrid{Float64, RightConnected, Bounded, Bounded, Float64, Float64, Float64, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, CPU}, RectilinearGrid{Float64, LeftConnected, Bounded, Bounded, Float64, Float64, Float64, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, CPU}}, Tuple{CPU, CPU}}, Tuple{CPU, CPU}, CPU}); Closest candidates are:; PressureSolver(::Any, ::RectilinearGrid{<:Any, <:Any, <:Any, <:Any, <:Number, <:Number, <:Number}) at ~/.julia/packages/Oceananigans/F1fni/src/Models/NonhydrostaticModels/NonhydrostaticModels.jl:24; PressureSolver(::Any, ::RectilinearGrid{<:Any, <:Any, <:Any, <:Any, <:Number, <:Number}) at ~/.julia/packages/Oceananigans/F1fni/src/Models/NonhydrostaticModels/NonhydrostaticModels.jl:25; PressureSolver(::Any, ::ImmersedBoundaryGrid) at ~/.julia/packages/Oceananigans/F1fni/src/Models/NonhydrostaticModels/NonhydrostaticModels.jl:28; ...; Stacktrace:; [1] NonhydrostaticModel(; grid::MultiRegionGrid{Float64, Bounded, Bounded, Bounded, XPartition{Int64}, MultiRegionObject{Tuple{RectilinearGrid{F",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2795#issuecomment-1302898706:136,Load,LoadError,136,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2795#issuecomment-1302898706,1,['Load'],['LoadError']
Performance,@simone-silvestri you mentioned that in this PR the pressure solver is done on a single GPU for the time being. Can https://github.com/CliMA/Oceananigans.jl/pull/2538 be a stating point to optimize that as well? That PR is really close to ready I think,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2795#issuecomment-1302354230:189,optimiz,optimize,189,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2795#issuecomment-1302354230,1,['optimiz'],['optimize']
Performance,"@simone-silvestri's recent work on scalable performance for the `HydrostaticFreeSurfaceModel` used the concept of ""inner"" and ""boundary"" regions to overlap communication and computation. The basic components are:. 1) Reduce communication as much as possible (for example, compute diagnostic variables wherever needed rather than communicating results); 2) Compute the ""interior"" of the tendencies first, which do not require halo regions, while communication is occurring; 3) Once communication is finished, compute the ""boundary regions"" of the tendency. Point (1) requires us to compute diagnostic variables inside halo regions. This requires both expanded sizes and offsets for each diagnostic variable. Different diagnostic variables have different sizes --- the width of the diagnostics kernels depends on how many points are accessed. Points (2) and (3) require computations over parts of the grid (reduced size + offset). These considerations demand a nice abstraction so that the code for launching kernels is readable and understandable (and also concise).",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3067:35,scalab,scalable,35,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3067,2,"['perform', 'scalab']","['performance', 'scalable']"
Performance,"@tomchor I agree wholeheartedly, both performance and model accuracy are interesting. I focused on performance because I suspect that because `Oceananigans` uses the GPU, it may be substantially faster than existing CPU-based codes. Also, I feel performance optimization can be a bit tricky, and in some cases is limited more by technology / hardware / programming language than software --- thus its important to know where `Oceananigans` stands now so we know how much effort we need to invest there. Implementing a new LES model or upgrading model numerics is, on other hand, typically easier and mostly a case of upgrading high-level software, I think --- especially in a high-level language like julia. We may also have abstractions soon that make implementing new LES models / equations much easier. (Verifying the accuracy of the LES model is another story...). It could be interesting to set up a model intercomparison project like you suggest, similar to what's been done in the atmosphere. We could pick a few physical scenarios for the comparison --- perhaps a reproduction of observations in a place that we think LES might be successful, and perhaps some idealized experiment(s), maybe involving submesoscales or reactive tracers or a combination of the two. Are you interested in helping set something like that up? . We have a nice opportunity at the upcoming Ocean Sciences to coordinate such a project --- people will be there who have deep familiarity with most of the ocean LES codes that I'm aware of (all 7 of them...). In fact, they are all presenting in the boundary layer turbulence session that we are both presenting in!",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/449#issuecomment-540072744:38,perform,performance,38,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/449#issuecomment-540072744,4,"['optimiz', 'perform']","['optimization', 'performance']"
Performance,@tomchor I guess we are hoping we can merge without the 3/4 performance penalty.,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2603#issuecomment-1190886871:60,perform,performance,60,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2603#issuecomment-1190886871,1,['perform'],['performance']
Performance,"@tomchor just wanted to echo @ali-ramadhan's thank you for finding that bug!. Right now Oceananigans shines for medium-size LES problems around 256^3, perhaps up to 384^3. For that application I think you may find it is as fast or faster than other codes. On that note, it'd be great to compare performance with other codes for this problem size. . For utterly massive problems like the one reported in the Sullivan paper you've posted, Oceananigans is probably not the right tool right now. We haven't experimented with multi-node parallelism yet, but as @ali-ramadhan suggests, its possible we will start working on either multi-CPU parallelism or multi-GPU parallelism soon. For reproducing the Sullivan paper, you may want multi-CPU parallelism simply because of the sheer number of nodes you'll need. It's worth noting that our AMD turbulence closure *may* be less resolution-dependent than the TKE-based closure used by NCAR LES / Sullivan (and yourself?) and thus it may be possible to run at lower resolution (potentially saving factors of 10-100 in resolution. A [paper by Abkar and Moin (2017)](https://link.springer.com/article/10.1007/s10546-017-0288-4) hints at the potential for very coarse simulations with AMD.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/449#issuecomment-539774569:295,perform,performance,295,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/449#issuecomment-539774569,1,['perform'],['performance']
Performance,"@tomchor unfortunately we don't have an issue for this problem. @sandreza is it possible to open an issue to document the problem that we are seeing? In the case that this PR does not resolve the issue, we'd like to have an issue open... I don't have much hard evidence to provide but I can describe the problem as I've seen it. Basically, very rare, subtle irregularities have been observed on the GPU when using `HydrostaticFreeSurfaceModel` in a `Periodic, Bounded, Bounded` configuration. I think that it is possible the main issue is an interaction (a read-write race condition) associated with both impenetrable boundary conditions and periodic boundary conditions that affects the 8 corner points. The race condition affects model trajectories via the Coriolis force (which is the only term as far as I know that touches the 8 ""corner"" points affected by this race condition). Because the race condition only manifests when a `Coriolis` or `VectorInvariant` stencil touches corner points, it may not affect _most_ `Periodic, Periodic, Bounded` models, which could explain why we haven't caught it. The reason it doesn't affect those models is because this race condition would only affect the corner points of `w`, which are not touched when using an `FPlane` Coriolis model. However, it's possible (I'm not sure) that the race condition could affect models using `NonTraditionalFPlane` in `Periodic, Periodic, Bounded` configurations. More generally, it will also affect models that are bounded in the `y`-direction, because in those models the corner points of the `y`-velocity are affected and also invoked when using `FPlane` or `BetaPlane` coriolis. That's as much as I know. It's very hard to gather information about this bug because it's so rare are subtle. In other words, only one grid point among 10,000 iterations might be affected, and the errors induced are very small. To find this issue, we have to run tens of thousands of iterations of identical models on the GPU, and then co",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1985#issuecomment-921143865:568,race condition,race condition,568,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1985#issuecomment-921143865,4,['race condition'],['race condition']
Performance,"@tomchor when I get the chance, I will open another issue about performance comparisons and tag you. Right now for 256^3 we are around 0.03--0.05 s per time-step with two tracers and the AMD closure. This means that an 8 day simulation with 5 s time-steps will run in a few hours of wall time. Since we haven't attempted much of any performance optimization, we hope that performance will improve further in the future. It's unclear what will happen for multi-CPU and multi-GPU architectures, however. Interesting that you're using a Lagrangian-averaged dynamic closure. I'd like to see how that compares to AMD. It's possible we could implement a Lagrangian-averaged dynamic closure, but probably not in the near term since I think we'd want to have particle advection features first.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/449#issuecomment-539801646:64,perform,performance,64,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/449#issuecomment-539801646,4,"['optimiz', 'perform']","['optimization', 'performance']"
Performance,"@vchuravy - I modified `src/Grid/zeros_and_ones.jl` to be; ```; using CUDA; using AMDGPU; using Oceananigans.Architectures: CPU, CUDAGPU, ROCmGPU, AbstractArchitecture; using KernelAbstractions; import Base: zeros. zeros(FT, ::CPU, N...) = zeros(FT, N...); zeros(FT, ::CUDAGPU, N...) = CUDA.zeros(FT, N...); zeros(FT, ::ROCmGPU, N...) = KernelAbstractions.zeros(AMDGPU.ROCBackend(), FT, N...). zeros(arch::AbstractArchitecture, grid, N...) = zeros(eltype(grid), arch, N...); zeros(grid::AbstractGrid, N...) = zeros(eltype(grid), architecture(grid), N...). @inline Base.zero(grid::AbstractGrid) = zero(eltype(grid)); @inline Base.one(grid::AbstractGrid) = one(eltype(grid)); ```. This throws the same error as just using `AMDGPU.jl`. For reference. ```; ERROR: LoadError: Not implemented; Stacktrace:; [1] error(s::String); @ Base ./error.jl:35; [2] runtime_module(job::GPUCompiler.CompilerJob); @ GPUCompiler ~/.julia/packages/GPUCompiler/U36Ed/src/interface.jl:173; [3] build_runtime(job::GPUCompiler.CompilerJob); @ GPUCompiler ~/.julia/packages/GPUCompiler/U36Ed/src/rtlib.jl:101; [4] (::GPUCompiler.var""#136#138""{GPUCompiler.CompilerJob{GPUCompiler.GCNCompilerTarget, AMDGPU.Compiler.HIPCompilerParams}})(); @ GPUCompiler ~/.julia/packages/GPUCompiler/U36Ed/src/rtlib.jl:139; [5] lock(f::GPUCompiler.var""#136#138""{GPUCompiler.CompilerJob{GPUCompiler.GCNCompilerTarget, AMDGPU.Compiler.HIPCompilerParams}}, l::ReentrantLock); @ Base ./lock.jl:229; [6] macro expansion; @ GPUCompiler ~/.julia/packages/GPUCompiler/U36Ed/src/rtlib.jl:120 [inlined]; [7] load_runtime(job::GPUCompiler.CompilerJob); @ GPUCompiler ~/.julia/packages/GPUCompiler/U36Ed/src/utils.jl:92; [8] macro expansion; @ ~/.julia/packages/GPUCompiler/U36Ed/src/driver.jl:290 [inlined]; [9] emit_llvm(job::GPUCompiler.CompilerJob; libraries::Bool, toplevel::Bool, optimize::Bool, cleanup::Bool, only_entry::Bool, validate::Bool); @ GPUCompiler ~/.julia/packages/GPUCompiler/U36Ed/src/utils.jl:92; [10] emit_llvm; @ ~/.julia/packages/GP",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3468#issuecomment-1936309420:760,Load,LoadError,760,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3468#issuecomment-1936309420,1,['Load'],['LoadError']
Performance,"@vchuravy Just benchmarked again with `@unroll` but couldn't see any difference in performance. Good to know about it though, will try it if we have more loops that could be unrolled.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/172#issuecomment-481445838:83,perform,performance,83,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/172#issuecomment-481445838,1,['perform'],['performance']
Performance,@xkykai do you think you could run some immersed boundary tests with this branch to make sure this change doesn’t affect your work? I think we’re interested in both performance and making sure the solution is high quality.,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3080#issuecomment-1582732172:165,perform,performance,165,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3080#issuecomment-1582732172,1,['perform'],['performance']
Performance,"A consideration when picking up from a checkpoint and using `NetCDFOutputWriter` is that `mode=""a""` (append) needs to be used instead of `mode=""c""` (create or clobber) when creating the `NetCDFOutputWriter`. This functionality works and is tested, but currently needs to be set manually by the user. Not sure of the best way of making this easy for users without accidentally overwriting their data. I can think of three solutions:; 1. Not specifying a `mode` causes `mode=""c""` if the file does not exist and `mode=""a""` if the file does exist. I like this solution the most as it works well with and without a checkpointer (and users don't have to do anything to get reasonable default behavior).; 2. Add a `force` kwarg to `NetCDFOutputWriter` that is `false` by default. The `NetCDFOutputWriter` will error if you try to overwrite an existing file, allowing the user to go back and set `mode=""a""` without any data loss. A `pickup` kwarg could perform a similar function if it's `false` by default.; 3. Setting the `PICKUP` environment variable causes `mode=""a""` to be the default if the file already exists. But I think we should avoid using global environment variables to modify internal behavior.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1068#issuecomment-711035671:945,perform,perform,945,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1068#issuecomment-711035671,1,['perform'],['perform']
Performance,A longer-term solution for efficient global ocean simulations is to implement a split-explicit time-stepping scheme. The first step should involve a simple but stable method where efficiency/accuracy tests can be performed,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2012:213,perform,performed,213,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2012,1,['perform'],['performed']
Performance,"A more hierarchical organization of the docs is. 1. Introduction; * What's Oceananigans?; * Installation; * Writing your first script; * Do you want to save the world? (by contributing to Oceananigans); - Contributing guide; - Features that need validating; - Features that need documenting and examples; * Gallery; 2. Examples; 3. Building models, running simulations, and post processing; * Grids and computer architectures; * Fields, BoundaryCondition, and AbstractOperations; * IncompressibleModel; * HydrostaticFreeSurfaceModel; * ShallowWaterModel; * Setting initial conditions; * Diffusion, viscosity, and TurbulenceClosures; * Forcing functions; * Coriolis forces; * Buoyancy forces; * Simulation; * OutputWriters; * OutputReaders, post-processing, and plotting; 5. Useful tips; * Using Graphics Processing Units (GPUs); * Common errors and performance pitfalls; 8. Fluid dynamics and ocean physics; * Navier-Stokes and tracer conservation equations; * The hydrostatic approximation and shallow water equations; * The Boussinesq approximation and equations of state for seawater; * Rotating domains and Coriolis forces; * Diffusion, hyperdiffusion, and parameterization of subgrid processes; - Basic models for diffusive processes; - Large eddy simulation; - Boundary layer parameterization; 9. Numerical implementation; * The finite volume discretization on a staggered grid; * Interpolation, differencing, and advection schemes; * Boundary conditions and immersed boundaries; * The pressure correction method for enforcing mass conservation; * Discretization of hydrostatic and non-hydrostatic pressure; * Implicit time-stepping; * Multi-domain simulations and the cubed sphere; 10. Developer guide; 11. Appendix; * References",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1852#issuecomment-879259142:849,perform,performance,849,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1852#issuecomment-879259142,1,['perform'],['performance']
Performance,"A new idea for extending the pressure solver to `MultiRegion` is to divide the FFT computations into ""local"" and ""non-local"" directions. . The FFT in local directions can be easily performed wrapping the transform in `@apply_regionally`; For non local directions, if `storage` and `plan` are `unified_array`s, the non local FFT can be performed by permuting the partitioning of the `MultiRegionGrid` without having to transpose memory (that will happen under the hood thanks to unified memory). . This strategy would not be easily extensible to generally subdivided regions and will play well only with one direction partitioning, but given the current limitations of the FFT solver (only regular grid), I think it is a good strategy to get something to work",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2523#issuecomment-1119850969:181,perform,performed,181,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2523#issuecomment-1119850969,2,['perform'],['performed']
Performance,"A performance warning is a pretty cool idea. Yes, let's do the changes tomorrow. Agreed that many parametric types becomes cumbersome. I think architecture is appropriate for the current model type (perhaps we will eventually call it `LESModel`... ?) We can define new types for new flavors of model (`HydrostaticModel`, `VariableBathymetryModel`...), rather than introducing type parameters.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/97#issuecomment-468538413:2,perform,performance,2,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/97#issuecomment-468538413,1,['perform'],['performance']
Performance,A potential race condition fix [WIP],MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1985:12,race condition,race condition,12,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1985,1,['race condition'],['race condition']
Performance,"A preliminary implementation of (constant) smagorinsky.; - temporary arrays to be revisited; - need to improve the way to disable it (without loss of performance); - no attempt to try it on GPU. ; Tested with deep_convection example, running nicely with smag_coeff=3.3e-2 and Prandtl_num=1",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/218:150,perform,performance,150,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/218,1,['perform'],['performance']
Performance,"ADER: failed to open iris: /usr/lib/dri/iris_dri.so: cannot open shared object file: No such file or directory (search paths /usr/lib/x86_64-linux-gnu/dri:\$${ORIGIN}/dri:/usr/lib/dri, suffix _dri); libGL error: failed to load driver: iris; libGL error: MESA-LOADER: failed to open swrast: /usr/lib/dri/swrast_dri.so: cannot open shared object file: No such file or directory (search paths /usr/lib/x86_64-linux-gnu/dri:\$${ORIGIN}/dri:/usr/lib/dri, suffix _dri); libGL error: failed to load driver: swrast; ┌ Warning: GLFW couldn't create an OpenGL window.; │ This likely means, you don't have an OpenGL capable Graphic Card,; │ or you don't have an OpenGL 3.3 capable video driver installed.; │ Have a look at the troubleshooting section in the GLMakie readme:; │ https://github.com/JuliaPlots/Makie.jl/tree/master/GLMakie#troubleshooting-opengl.; └ @ GLMakie ~/.julia/packages/GLMakie/XG7Hm/src/screen.jl:381; ERROR: LoadError: GLFWError (VERSION_UNAVAILABLE): GLX: Failed to create context: GLXBadFBConfig; Stacktrace:; [1] _ErrorCallbackWrapper(code::Int32, description::Cstring); @ GLFW ~/.julia/packages/GLFW/BWxfF/src/callback.jl:43; [2] CreateWindow(width::Int64, height::Int64, title::String, monitor::GLFW.Monitor, share::GLFW.Window); @ GLFW ~/.julia/packages/GLFW/BWxfF/src/glfw3.jl:499; [3] GLFW.Window(; name::String, resolution::Tuple{Int64, Int64}, debugging::Bool, major::Int64, minor::Int64, windowhints::Vector{Tuple{UInt32, Integer}}, contexthints::Vector{Tuple{UInt32, Integer}}, visible::Bool, focus::Bool, fullscreen::Bool, monitor::Nothing, share::GLFW.Window); @ GLFW ~/.julia/packages/GLFW/BWxfF/src/glfw3.jl:344; [4] GLMakie.Screen(; resolution::Tuple{Int64, Int64}, visible::Bool, title::String, kw_args::Base.Iterators.Pairs{Union{}, Union{}, Tuple{}, NamedTuple{(), Tuple{}}}); @ GLMakie ~/.julia/packages/GLMakie/XG7Hm/src/screen.jl:373; [5] Screen; @ ~/.julia/packages/GLMakie/XG7Hm/src/screen.jl:345 [inlined]; [6] global_gl_screen; @ ~/.julia/packages/GLMakie/XG7Hm/",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2542#issuecomment-1123091985:1386,Load,LoadError,1386,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2542#issuecomment-1123091985,1,['Load'],['LoadError']
Performance,"Abstraction for ""equation"" for performance, code clarity, memory footprint reduction, and powerful user interface",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/259:31,perform,performance,31,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/259,1,['perform'],['performance']
Performance,"According to @maleadt on the Julia slack's GPU channel and in regards to the shallow water model profiles:. > Don't focus on time spent in API calls to much. since GPU execution is asynchronous, you'll have to synchronize at some point, and that API call will then 'soak up' time until the stream has finished executing. and here that's literally the synchronize function, which is implemented using cuStreamQuery: https://github.com/JuliaGPU/CUDA.jl/blob/2b3ec03ff9774b65541fc88dd6b0f1f7aea5d9e0/lib/cudadrv/stream.jl#L115-L144. >use a timeline profiler (i.e. NSNight Systems) to profile your app, or nvpp if you really want to use the old profiler toolchain. plain nvprof results are too simple once your application hits some level of complexity. >now, it is possible that our CPU-side implementation of synchronize does too many API calls and could be optimized a little, but in the end the call serves to wait until the GPU has finished so it probably doesn't matter much. if it does, e.g. because you want to perform other useful work on another CPU task concurrently, you could try to profile that in isolation and file an issue. Essentially, Tim explains that `cuStreamQuery` takes up more time as the grid size increases because it's called in the synchronize function. The synchronize function as shown in the link above tends to be called more and soaks up more waiting time the bigger the problem hence why it scales positively to grid size. ; Taking a closer look at the shallow water gpu profiling results above, it seems that `cuStreamQuery` takes up a lot of time in the finer resolution runs because it is called many times and not because each call takes a lot of time. For example, in the 16k case, `cuSteamQuery` is called three order of magnitudes more times than `cuLaunchKernel` while both calls are measured in microseconds. ; I'm not sure if `cuStreamQuery` being called 400,000 times is an error with our code, an error with CUDA.jl, not an error at all, or an error with my ",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1912#issuecomment-899945654:856,optimiz,optimized,856,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1912#issuecomment-899945654,1,['optimiz'],['optimized']
Performance,"Adapt.jl isn't relevant here, that's for CPU->GPU conversions. This is a tuple operation not getting inferred properly, leading to a dynamic call to a runtime function. Those aren't supported. With 1.5, there's been a bunch of latency optimizations that have affected inference quality, e.g., when not forcing specialization on functions with `::F where F`. Your best bet here would be to step through using Cthulhu and figure out which code isn't inferring properly. If it's tough code to get though, you could always add the call to `Cthulhu.descend_code_warntype` at the place where the InvalidIRError is thrown.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/828#issuecomment-673479769:227,latency,latency,227,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/828#issuecomment-673479769,2,"['latency', 'optimiz']","['latency', 'optimizations']"
Performance,Adds test for race condition in `compute!(::AveragedField)`,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1805:14,race condition,race condition,14,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1805,1,['race condition'],['race condition']
Performance,"After adding `MPI.Init()` back to `distributed_nonhydrostatic_model_mpi.jl`, the script seems to run (it's still running...). Here's a few idle thoughts (independent of this PR, just leaving here for future reference):. * The ""Benchmarks"" module in `/benchmarks/src` causes a few issues like:; ```julia; ┌ Warning: Package Benchmarks does not have Oceananigans in its dependencies:; │ - If you have Benchmarks checked out for development and have; │ added Oceananigans as a dependency but haven't updated your primary; │ environment's manifest file, try `Pkg.resolve()`.; │ - Otherwise you may need to report an issue with Benchmarks; └ Loading Oceananigans into Benchmarks from project dependency, future warnings for Benchmarks are suppressed.; ```; It'd be nice to fix that eventually. * The output is kind of annoying:. ```julia; [2022/03/13 13:17:49.875] INFO Benchmarking weak scaling nonhydrostatic model with Slab decomposition [N=(128, 128, 32), ranks=(1, 2, 1)]...; Invalid MIT-MAGIC-COOKIE-1 keyInvalid MIT-MAGIC-COOKIE-1 keyNo protocol specified; [ Info: Oceananigans will use 24 threads; [ Info: Oceananigans will use 24 threads; ┌ Warning: Package Benchmarks does not have Oceananigans in its dependencies:; │ - If you have Benchmarks checked out for development and have; │ added Oceananigans as a dependency but haven't updated your primary; │ environment's manifest file, try `Pkg.resolve()`.; │ - Otherwise you may need to report an issue with Benchmarks; └ Loading Oceananigans into Benchmarks from project dependency, future warnings for Benchmarks are suppressed.; ┌ Warning: Package Benchmarks does not have Oceananigans in its dependencies:; │ - If you have Benchmarks checked out for development and have; │ added Oceananigans as a dependency but haven't updated your primary; │ environment's manifest file, try `Pkg.resolve()`.; │ - Otherwise you may need to report an issue with Benchmarks; └ Loading Oceananigans into Benchmarks from project dependency, future warnings for ",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2338#issuecomment-1066145590:637,Load,Loading,637,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2338#issuecomment-1066145590,1,['Load'],['Loading']
Performance,"After discussing with @ali-ramadhan, it seems more clear that a simple solution is just to use a default halo size of 3. For most models that use the highest order advection scheme we offer this has no effect. For models that use a lower-order advection scheme but don't change the halo size, the memory foot print of the model is ever-so-slightly larger than it needs to be. But this slightly-larger footprint probably isn't noticeable for most problems. So in summary, minimal halo sizes are a minor optimization that has little effect on most problems. Auto-optimizing the halo size has major downsides for usability, so I think the trade-off leans towards big default halos.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1245#issuecomment-741886883:502,optimiz,optimization,502,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1245#issuecomment-741886883,2,['optimiz'],"['optimization', 'optimizing']"
Performance,"After some discussion with @glwagner , I'm going to close this PR and rebase off of CLIMA:main. Stay tuned for a new PR..",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3466#issuecomment-1934850421:101,tune,tuned,101,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3466#issuecomment-1934850421,1,['tune'],['tuned']
Performance,"After the big changes yesterday I decided to run the tests to make sure everything was working. Thanks again @ali-ramadhan for helping me get that started. Now I'm finding that there are some failures and somethings that are broken. See below. Is it just me or do others get this now?. ```; Test Summary: | Pass Fail Broken Total; Oceananigans | 2987 8 5 3000; Unit tests | 1511 1 1512; Model and time stepping tests (part 1) | 99 99; Model and time stepping tests (part 2) | 214 1 215; Simulation tests | 1142 2 3 1147; Simulations | 26 26; Diagnostics | 12 12; Output writers | 409 2 411; FieldSlicer | 1 1; WindowedTimeAverage | 2 2; NetCDF [GPU] | 198 198; JLD2 [GPU] | 11 11; Checkpointer [GPU] | 166 2 168; Dependency adding [GPU] | 2 2; Time averaging of output [GPU] | 29 29; Abstract operations | 695 3 698; Regression | 14 6 20; Thermal bubble [GPU] | 5 5; Rayleigh–Bénard tracer [GPU] | 5 5; Ocean large eddy simulation [GPU] | 4 6 10; Scripts | 7 7; ERROR: LoadError: Some tests did not pass: 2987 passed, 8 failed, 0 errored, 5 broken.; in expression starting at /home/fpoulin/software/Oceananigans.jl/test/runtests.jl:77; ERROR: Package Oceananigans errored during testing; ```",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1179:969,Load,LoadError,969,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1179,1,['Load'],['LoadError']
Performance,"After updating to julia 1.8.2 and updating branch to main, I tried running the validaton experiment `resting_stratified_bumpy_ocean.jl` and it failed. @fadaie91 also tried the same thing, and saw the same error. The problem is when we define the model, it complains about a `nested task error`. ```; grid = 1×128×64 ImmersedBoundaryGrid{Float64, Flat, Periodic, Bounded} on CPU with 0×3×3 halo:; ├── immersed_boundary: PartialCellBottom(min(h)=-1.00e+00, max(h)=-5.00e-01, ϵ=0.2); ├── underlying_grid: 1×128×64 RectilinearGrid{Float64, Flat, Periodic, Bounded} on CPU with 0×3×3 halo; ├── Flat x; ├── Periodic y ∈ [-1.0, 1.0) regularly spaced with Δy=0.015625; └── Bounded z ∈ [-1.0, 0.0] regularly spaced with Δz=0.015625; ERROR: LoadError: TaskFailedException. nested task error: BoundsError: attempt to access 1×134×1 OffsetArray(::Array{Float64, 3}, 1:1, -2:131, 1:1) with eltype Float64 with indices 1:1×-2:131×1:1 at index [0, 1]; Stacktrace:; [1] throw_boundserror(A::OffsetArrays.OffsetArray{Float64, 3, Array{Float64, 3}}, I::Tuple{Int64, Int64}); @ Base ./abstractarray.jl:703; [2] overdub; @ ~/.julia/packages/KernelAbstractions/3ZHln/src/compiler.jl:51 [inlined]; [3] overdub; @ ./abstractarray.jl:668 [inlined]; [4] overdub; @ ./abstractarray.jl:1273 [inlined]; [5] overdub; @ ./abstractarray.jl:1241 [inlined]; [6] overdub; @ ~/Software/Oceananigans.jl/src/ImmersedBoundaries/partial_cell_immersed_boundaries.jl:49 [inlined]; ...; ```",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2798:731,Load,LoadError,731,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2798,1,['Load'],['LoadError']
Performance,Again unsure if it affects performance but since `rate` is referenced as global it needs to be `const`; eg `const rate = 1/10`.,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1827#issuecomment-875683170:27,perform,performance,27,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1827#issuecomment-875683170,1,['perform'],['performance']
Performance,"Agreed. Useful and related comment. > I was thinking of doing some prototyping and benchmarking in a sandbox by building off the example in my PR [vchuravy/GPUifyLoops.jl#18](https://github.com/vchuravy/GPUifyLoops.jl/pull/18).; > ; > The PR contains an example that can be extended to rely on a `Grid` struct, multiple `FaceField`s and ` CellField`. So I'll prototype grids and fields that are `isbitstype` (you already helped by doing this for a grid in [#59 (comment)](https://github.com/climate-machine/Oceananigans.jl/issues/59#issuecomment-467660181)) and test to see if they work on the GPU with GPUifyLoops.jl. If they do work and performance isn't degraded then I'll rewrite the operators to use grid and field structs.; > ; > You probably know how to do this better than me, but might be good if I rewrite the operators as they's still undocumented and do some _slightly convoluted_ stuff to avoid having to store intermediate calculations.; > ; > Right now I'm focusing on system tests and benchmarks but once @christophernhill @jm-c and I get closer to implementing the variable _Δz_ grid #47 I will work on this.; >; >_Originally posted by @ali-ramadhan in https://github.com/climate-machine/Oceananigans.jl/issues/115#issuecomment-470782067_",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/59#issuecomment-470297910:639,perform,performance,639,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/59#issuecomment-470297910,1,['perform'],['performance']
Performance,"Ah I think you encountered an example of CUDA scalar operations: https://juliagpu.gitlab.io/CUDA.jl/usage/workflow/#UsageWorkflowScalar. This is when the GPU doesn't know how to do a certain operation quickly using GPU kernels, in this case `any(isnan.(oc.interior(model.velocities.u)))`, so it does it one-by-one on the CPU. We disable CUDA scalar operations in Oceananigans.jl because it can lead to huge performance hits that can be hard to locate. In your case I think it's because `interior(model.velocities.u)` returns a view into `u` without halos (but CUDA.jl doesn't always know how to broadcast over views into CuArrays). I think it should work if you change it to. ```julia; if any(isnan, model.velocities.u.data.parent); throw(""Simulation crashed!""); end; ```. Hopefully we can merge PR #1198 soon so you don't have to worry about this kind of stuff...",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1196#issuecomment-733827588:407,perform,performance,407,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1196#issuecomment-733827588,1,['perform'],['performance']
Performance,"Ah ok I see why your original script didn't work now. When executing. ```julia; model = IncompressibleModel(grid=grid,; closure = closure,; boundary_conditions = (v=v_bcs); ); ```. this assigned `v_bcs` to the new variable `v` and passes it to the `IncompressibleModel` function. `v_bc` itself is a `NamedTuple` containing `x, y, z` properties so the `IncompressibleModel` constructor does not complain. Since the named tuple that is passed does not not have a `v` property, the velocities get the default boundary conditions (free slip and no normal flow). Then when you call `set!(model, v=-0.5)` it fills v to be -0.5 everywhere but then `update_state!(model)` is called to ensure that the velocity field is incompressible (divergence-free) by performing a pressure projection step. Since the default boundary condition enforces no-normal flow, the velocity cannot be -0.5 everywhere so the pressure field updates it to be zero to enforce incompressibility. This is a pretty subtle bug (related to #1204)... Since the proper named tuple can't be checked for in the constructor function signature, I wonder if it's worth adding a manual check to make sure a `NamedTuple{(:x,:y,:z)}` was not passed to the model constructor.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1294#issuecomment-756979930:747,perform,performing,747,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1294#issuecomment-756979930,1,['perform'],['performing']
Performance,"Ah so I've realised this isn't the fix we needed, and I was just hiding it from myself in the profile because I replaced the function by writing it in the REPL. I made an MWE:. ```jula; using Oceananigans. grid = RectilinearGrid(GPU(), topology = (Flat, Flat, Bounded), size = (100, ), extent = (400, )). model = HydrostaticFreeSurfaceModel(; grid, velocities = PrescribedVelocityFields(), momentum_advection=nothing, buoyancy=nothing, tracers = ntuple(n->Symbol(:T, n), Val(30))); ```; <img width=""1361"" alt=""Screenshot 2024-09-26 at 12 00 29"" src=""https://github.com/user-attachments/assets/2cf2a379-88e9-428f-8156-4ddead2a02e4"">; You can see from this profile that `fill_open_boundary_regions!` takes a lot longer than `fill_halo_event!`, even though there are no velocity open boundaries. This is because it is launching a load of zero size kernels where as `fill_halo_event!` just returns nothing instead. I've fixed this now and get this from the profile instead:; <img width=""1251"" alt=""Screenshot 2024-09-26 at 12 02 11"" src=""https://github.com/user-attachments/assets/0bf91086-bc12-4a17-ba48-89b9b1c2e7ae"">. In numbers, the original version benchmarks `time_step!` at around 4.074 ms ± 581.472 μs and the new version 2.438 ms ± 501.642 μs",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3792#issuecomment-2376508207:827,load,load,827,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3792#issuecomment-2376508207,1,['load'],['load']
Performance,"Ah sorry I must have misunderstood your question. Yeah I think broadcasts; tend to perform really well on scalar operations so I don't see why not. On Tue, Mar 5, 2019, 12:13 PM Gregory L. Wagner <notifications@github.com>; wrote:. > I'm not suggesting we should always use broadcasting. I'm just wondering; > if it's ok to use it for simple operations. It's a nice abstraction that; > works on CPUs and GPUs for simple calculations / global array updates (for; > example; > <https://github.com/glwagner/StaggeredPoisson.jl/blob/d104825ba33f184af3b90ca0d958247d0011c7ad/src/solvers.jl#L148>; > ).; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/climate-machine/Oceananigans.jl/issues/108#issuecomment-469768233>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ATKyBSiIR6hlh-r4hjpSmahUJhp3x7Gzks5vTqXEgaJpZM4bfFUj>; > .; >",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/108#issuecomment-469769111:83,perform,perform,83,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/108#issuecomment-469769111,1,['perform'],['perform']
Performance,"Ah yes as noted here:. > This speed issue disappears either when ζ = ∂x(v) - ∂y(u) is replaced with ζ = Field(∂x(v) - ∂y(u)), or when ζ is the only variable in fields_slice. That's pretty bizarre. That makes me think it's some kind of weird interaction between `NCDatasets` and `Oceananigans`. I can say, with `ζ = Field(∂x(v) - ∂y(u))` the output type is different. It would seem more complex in fact, because it has one more layer of indirection (ie it's the window that refers to a 3D computed field, rather than a windowed computed field). So I don't know why that would compile faster. Honestly I don't think any of us has much experience optimizing compile time. Perhaps first reading this blog post:. https://julialang.org/blog/2020/08/invalidations/. and then the source code for `NetCDFOutputWriter` will lead to revelations.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3381#issuecomment-1805104189:644,optimiz,optimizing,644,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3381#issuecomment-1805104189,1,['optimiz'],['optimizing']
Performance,"Ah! In `HydrostaticFreeSurfaceModel`, the implicit free surface algorithm uses a pressure correction / operator splitting method. It's really a sort of pseudo-implicit treatment (because the velocity field is _corrected_ rather than time-stepped concurrently with the free surface. I think it would be possible to implement an operator splitting method in `ShallowWaterModel`. But is this useful? This could lead to a loss of accuracy (I'm not sure). In the event that the pressure / free surface displacement is passive (so one doesn't care about accuracy), it might be better to use a rigid lid approximation (and thus a two-dimensional `IncompressibleModel`, rather than `ShallowWaterModel`). But I am ready to stand corrected.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1378#issuecomment-781574578:246,concurren,concurrently,246,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1378#issuecomment-781574578,1,['concurren'],['concurrently']
Performance,"Ah, apparently the error appears when we have an `AveragedTimeInterval`. Change the `simple_output` call to. ```julia; simulation.output_writers[:simple_output] = NetCDFOutputWriter(model, wout,; schedule = AveragedTimeInterval(10seconds),; filepath = ""windowed_avg.nc"", mode = ""c""); ```. and we have the following error:. ```; ERROR: LoadError: Custom output Uw needs dimensions!; Stacktrace:; [1] error(::String) at ./error.jl:33; [2] define_output_variable!(::NCDatasets.NCDataset{Nothing}, ::WindowedTimeAverage{WindowedSpatialAverage{Field{Face,Center,Center,OffsetArrays.OffsetArray{Float64,3,Array{Float64,3}},RegularRectilinearGrid{Float64,Periodic,Periodic,Periodic,OffsetArrays.OffsetArray{Float64,1,StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}}},NamedTuple{(:x, :y, :z),Tuple{CoordinateBoundaryConditions{BoundaryCondition{Oceananigans.BoundaryConditions.Periodic,Nothing},BoundaryCondition{Oceananigans.BoundaryConditions.Periodic,Nothing}},CoordinateBoundaryConditions{BoundaryCondition{Oceananigans.BoundaryConditions.Periodic,Nothing},BoundaryCondition{Oceananigans.BoundaryConditions.Periodic,Nothing}},CoordinateBoundaryConditions{BoundaryCondition{Oceananigans.BoundaryConditions.Periodic,Nothing},BoundaryCondition{Oceananigans.BoundaryConditions.Periodic,Nothing}}}}},FieldSlicer{Colon,UnitRange{Int64},Colon},Int64},Array{Float64,2},FieldSlicer{Colon,Colon,Colon}}, ::String, ::Type{T} where T, ::Int64, ::Tuple{}, ::Dict{Any,Any}) at /home/tomas/repos2/Oceananigans.jl/src/OutputWriters/netcdf_output_writer.jl:357; [3] NetCDFOutputWriter(::IncompressibleModel{Oceananigans.TimeSteppers.RungeKutta3TimeStepper{Float64,NamedTuple{(:u, :v, :w, :b),Tuple{Field{Face,Center,Center,OffsetArrays.OffsetArray{Float64,3,Array{Float64,3}},RegularRectilinearGrid{Float64,Periodic,Periodic,Periodic,OffsetArrays.OffsetArray{Float64,1,StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}}},NamedTuple{(:x, :y, :z),Tuple{Coordinat",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1397#issuecomment-784246056:335,Load,LoadError,335,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1397#issuecomment-784246056,1,['Load'],['LoadError']
Performance,"Ah, nice! Maybe using `ConditionalOperation` is a bit slower than `WindowSpatialAverage` because the reduction is performed on the whole domain. Anyways, I don't think reduction is performance-critical so I agree to nuke `WindowSpatialAverage`.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2185#issuecomment-1022501885:114,perform,performed,114,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2185#issuecomment-1022501885,2,['perform'],"['performance-critical', 'performed']"
Performance,"Ah, thanks for that @maleadt. So microbenchmarks suggest squaring by `Float64` and `Int32` are virtually indistinguishable in simple code. Is there any way that this change is somehow affected by compiler heuristics; eg code inlining is somehow much more effective / optimized when we can invoke `nv_pow`... ? Otherwise I'm at a loss. I think prior to the changes we grouped into our ""upgrade to julia 1.6"" we were using the function `CUDA.pow` (from an ancient `CUDA.jl` version).",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1770#issuecomment-869812530:267,optimiz,optimized,267,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1770#issuecomment-869812530,1,['optimiz'],['optimized']
Performance,Alright I think `WENO5` is fully working now and good to be merged. Some comments about the contents of this PR:; 1. For now we have a fast and accurate `WENO5` implementation.; 2. We can look at optimizing the `WENO{N}` schemes down the road. They are currently are very experimental since they allocate tons of memory and only define the left-biased interpolants. I'll need to think a bit to get both left- and right-biased interpolants working so I'm proposing merging `weno_nth_order.jl` as-is. It still works and looks good if you run `periodic_advection.jl` with `WENO{N}` with positive velocities.; 3. I added a _temporary_ thermal bubble verification to quickly test whether advection schemes behave responsible near boundaries. We can remove it when we have a better (and more rigorous) test/verification.,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/592#issuecomment-699497422:196,optimiz,optimizing,196,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/592#issuecomment-699497422,1,['optimiz'],['optimizing']
Performance,"Alright so I decided to compromise so the NaN checker is always correct. Now it goes back to checking for NaNs in the entire field (halos included). But to reduce the performance impact, the simulation's default NaN checker only checks once every 100 iterations.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1198#issuecomment-732960955:167,perform,performance,167,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1198#issuecomment-732960955,1,['perform'],['performance']
Performance,"Alright, I think this is ready to be merged. Performance looks alright. Model has slowed down a tiny bit (6% for large GPU stuff) but that's expected because we're using halo regions but still computing differences the ""slow"" way with `incmod1` and `decmod1`. The next PR will address this. We also do more calculations for the Poisson equation RHS and in diagnosing _w_ which is why the 6% slowdown is expected.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/250#issuecomment-497048059:45,Perform,Performance,45,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/250#issuecomment-497048059,1,['Perform'],['Performance']
Performance,"Also important to keep in mind in this discussion: Our workflow will be different from most standard models, which write out instantaneous output that then is post-processed to get statistics etc. We will have to accumulate statistics on the fly, and we can (and should) forgo most instantaneous output, at least for the atmosphere. The model will learn from the accumulated statistics. Otherwise, with instantaneous output, the data volume, especially with embedded LES, will create an I/O and data transfer bottleneck that will limit us, and, e.g., will limit our ability to use distributed computing platforms.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/145#issuecomment-475692509:509,bottleneck,bottleneck,509,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/145#issuecomment-475692509,1,['bottleneck'],['bottleneck']
Performance,"Also reported by @qwert2266 but this is my fault since GitLab CI doesn't seem to be actually running GPU tests... Using something like; ```julia; @inline FT(x, y, z, t, T, p) = - exp(z/p.ℓ) * 2p.K/p.Δz^2 * T; T_forcing = Forcing(FT, field_dependencies=:T, parameters=bc_params); forcing = (T=T_forcing,); ```. causes. ```; ERROR: LoadError: GPU compilation of kernel gpu_calculate_Gu!(Cassette.Context{nametype(CUDACtx),KernelAbstractions.CompilerMetadata{KernelAbstractions.NDIteration.StaticSize{(128, 128, 128)},KernelAbstractions.NDIteration.DynamicCheck,Nothing,Nothing,KernelAbstractions.NDIteration.NDRange{3,KernelAbstractions.NDIteration.StaticSize{(8, 8, 128)},KernelAbstractions.NDIteration.StaticSize{(16, 16, 1)},Nothing,Nothing}},Nothing,KernelAbstractions.var""##PassType#253"",Nothing,Cassette.DisableHooks}, typeof(Oceananigans.TimeSteppers.gpu_calculate_Gu!), OffsetArrays.OffsetArray{Float64,3,CUDA.CuDeviceArray{Float64,3,CUDA.AS.Global}}, RegularCartesianGrid{Float64,Periodic,Periodic,Bounded,OffsetArrays.OffsetArray{Float64,1,StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}}}, Oceananigans.Advection.CenteredSecondOrder, NonTraditionalFPlane{Float64}, Nothing, AnisotropicDiffusivity{Float64,Float64,Float64,NamedTuple{(:T,),Tuple{Float64}},NamedTuple{(:T,),Tuple{Float64}},NamedTuple{(:T,),Tuple{Float64}}}, NamedTuple{(:u, :v, :w),Tuple{OffsetArrays.OffsetArray{Float64,3,CUDA.CuDeviceArray{Float64,3,CUDA.AS.Global}},OffsetArrays.OffsetArray{Float64,3,CUDA.CuDeviceArray{Float64,3,CUDA.AS.Global}},OffsetArrays.OffsetArray{Float64,3,CUDA.CuDeviceArray{Float64,3,CUDA.AS.Global}}}}, NamedTuple{(:T,),Tuple{OffsetArrays.OffsetArray{Float64,3,CUDA.CuDeviceArray{Float64,3,CUDA.AS.Global}}}}, Nothing, NamedTuple{(:u, :v, :w, :T),Tuple{Oceananigans.Forcings.ContinuousForcing{Oceananigans.Grids.Face,Oceananigans.Grids.Cell,Oceananigans.Grids.Cell,Nothing,0,typeof(Oceananigans.Forcings.zeroforcing),Tuple{}},Oceananigans.Forcings.ContinuousForcing",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1010:330,Load,LoadError,330,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1010,1,['Load'],['LoadError']
Performance,"Also, I forgot to post an example of the error that happens when you pass an operation as a `condition`. Here's the error that I get in that case:. ```; ERROR: LoadError: MethodError: no method matching arch_array(::CPU, ::KernelFunctionOperation{Center, Center, Face, RectilinearGrid{Float64, Periodic, Periodic, Bounded, Float64, Float64, Float64, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, CPU}, Float64, typeof(boundary_node), Tuple{Center, Center, Face}}). Closest candidates are:; arch_array(::Distributed, ::Any); @ Oceananigans ~/repos/Oceananigans.jl/src/DistributedComputations/distributed_architectures.jl:263; arch_array(::CPU, ::Array); @ Oceananigans ~/repos/Oceananigans.jl/src/Architectures.jl:59; arch_array(::CPU, ::CUDA.CuArray); @ Oceananigans ~/repos/Oceananigans.jl/src/Architectures.jl:60; ... Stacktrace:; [1] condition_operand; @ ~/repos/Oceananigans.jl/src/ImmersedBoundaries/immersed_reductions.jl:24 [inlined]; [2] sum(f::Function, c::Oceananigans.AbstractOperations.GridMetricOperation{Center, Center, Center, ImmersedBoundaryGrid{Float64, Periodic, Periodic, Bounded, RectilinearGrid{Float64, Periodic, Periodic, Bounded, Float64, Float64, Float64, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, CPU}, GridFittedBottom{Field{Center, Center, Nothing, Nothing, RectilinearGrid{Float64, Periodic, Periodic, B",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3439#issuecomment-1907385408:160,Load,LoadError,160,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3439#issuecomment-1907385408,1,['Load'],['LoadError']
Performance,"Also, the docs are failing to build with this error:. ```; ERROR: LoadError: IOError: sendfile: no space left on device (ENOSPC); ```. Do we still have a storage problem? I believe these tests run on tartarus, not sverdrup, no?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1762#issuecomment-867678106:66,Load,LoadError,66,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1762#issuecomment-867678106,1,['Load'],['LoadError']
Performance,"Also, the implicit vertical solver seems to affect the performance. I would have to guess that it is because we are passing functions as arguments to the kernel.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2996#issuecomment-1485410927:55,perform,performance,55,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2996#issuecomment-1485410927,1,['perform'],['performance']
Performance,"Also, what's the correct syntax to define kernel_parameters over one dimension? For example, the line; ```julia; kernel_parameters = KernelParameters((Nc, 1), (0, Nz_grid-1)); ```; works correctly. However, when I try to define it for one dimension with; ```julia; kernel_parameters = KernelParameters((1,), (Nz_grid-1,)); ```; I encounter the following error:; ```julia; ERROR: LoadError: MethodError: no method matching heuristic_workgroup(::Int64). Closest candidates are:; heuristic_workgroup(::Any, ::Any, ::Any, ::Any); @ Oceananigans /Users/Sid/Library/CloudStorage/Dropbox/StudyFolder/PostDocMITDesktop/Codes/Oceananigans/cubed-sphere-aquaplanet/src/Utils/kernel_launching.jl:48; heuristic_workgroup(::Any, ::Any, ::Any); @ Oceananigans /Users/Sid/Library/CloudStorage/Dropbox/StudyFolder/PostDocMITDesktop/Codes/Oceananigans/cubed-sphere-aquaplanet/src/Utils/kernel_launching.jl:48; heuristic_workgroup(::Any, ::Any); @ Oceananigans /Users/Sid/Library/CloudStorage/Dropbox/StudyFolder/PostDocMITDesktop/Codes/Oceananigans/cubed-sphere-aquaplanet/src/Utils/kernel_launching.jl:48; ```",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3611#issuecomment-2138476721:379,Load,LoadError,379,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3611#issuecomment-2138476721,1,['Load'],['LoadError']
Performance,"Am I doing something wrong?; ```julia; navid:Oceananigans.jl/ (master) $ julia --project [19:00:13]; _; _ _ _(_)_ | Documentation: https://docs.julialang.org; (_) | (_) (_) |; _ _ _| |_ __ _ | Type ""?"" for help, ""]?"" for Pkg help.; | | | | | | |/ _` | |; | | |_| | | | (_| | | Version 1.1.0 (2019-01-21); _/ |\__'_|_|_|\__'_| |; |__/ |. julia> include(""examples/internal_wave.jl""); ERROR: LoadError: could not open file /Users/navid/Research/Oceananigans.jl/examples/utils.jl; ...; ```. I guess the problem is the `include(""utils.jl"")` line found in all example .jl scripts. Is this supposed to be `src/utils.jl`?",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/254:389,Load,LoadError,389,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/254,1,['Load'],['LoadError']
Performance,"Among the packages that are loaded in `dependencies_for_runtests.jl` are `DataDeps` and `TimeDate`, which aren't part of Oceananigans dependencies. So I always have to either install those packages or comment out those lines when running tests locally. (Unless there's an easier solution that I'm not aware of!). While this isn't a huge hassle, it does make it less likely (at least for me) to test my changes locally, so I'm attempting this slight change in the loading of packages where only standard Julia packages and Oceananigans dependencies are loaded in `dependencies_for_runtests.jl` and the rest are loaded only in the scripts where they used. Hopefully this should make testing changes locally easier for everyone.",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2715:28,load,loaded,28,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2715,4,['load'],"['loaded', 'loading']"
Performance,"An easier course of action would be to forget about performance at first and just fill halos every substep.; This will be quite inefficient but will allow us to test open boundary conditions for the hydrostatic model and validate them first. There is an implementation of a split explicit free surface solver that does not require special operators (specifically for a multi region grid) in #3596 ; https://github.com/CliMA/Oceananigans.jl/blob/a6e9a465aa9528b5b3afd49737310e710e4681b0/src/MultiRegion/multi_region_split_explicit_free_surface.jl#L130-L170. We could adapt this implementation for normal grids by implementing a keyword argument in the `SpliExplicitFreeSurface` constructor. Something like the `extended_halos` that is mentioned here; https://github.com/CliMA/Oceananigans.jl/blob/a6e9a465aa9528b5b3afd49737310e710e4681b0/src/MultiRegion/multi_region_split_explicit_free_surface.jl#L19-L28. For serial grids, `extended_halos` is not the correct argument, though, because we do not extend halos (we do that only on distributed and multi region grids), so maybe something like `use_boundary_aware_operators.` . Once the numerics have been settled we can adapt the open boundary condition implementation to the special operators.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3828#issuecomment-2399186268:52,perform,performance,52,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3828#issuecomment-2399186268,1,['perform'],['performance']
Performance,"Another important point is that this discussion is not about ""deciding"" on a _single_ user interfaces, but rather deciding what interfaces we want to put into the code. We can and perhaps should have multiple ways of specifying stretched grids. We have already agreed that specifying cell interface locations with an array (loaded from file, for example) is an important pattern to support.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1551#issuecomment-814924072:324,load,loaded,324,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1551#issuecomment-814924072,1,['load'],['loaded']
Performance,"Another thought for @christophernhill . At the talk today on `ImplicitGlobalGrid.jl`, they were using `@view` in the simplest code but they dropped it as soon as they started to optimize the code. I believe they started using `LazyArrays.jl`. I don't know what it is but I suspect it doesn't have the problems that `@view` might have.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1882#issuecomment-885901320:178,optimiz,optimize,178,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1882#issuecomment-885901320,1,['optimiz'],['optimize']
Performance,"Apologies - other urgent work and family needs have delayed me. I'm using; Julia 1.9.3; CairoMakie v0.9.4; and have in my status report; GLMakie v0.7.4 and Makie v0.18.4. I tried to use update with the pkg manager to no effect, but see I should be using CairoMakie@0.11, which I am now installing.; This was not successful. For example the file S7LmV_3TYIX.dll would not load giving a permission denied error, but my check of the properties/security did not reveal a deficiency. However, the example worked fine. Many thanks - Kevin",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3442#issuecomment-1920152651:371,load,load,371,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3442#issuecomment-1920152651,1,['load'],['load']
Performance,"Apologies for the late reply, I got distracted and accidentally forgot to respond. I tested out the fix this morning/early afternoon and I keep getting an error along the lines of what I have below. Did I compile the branch of oceananigans incorrectly?. ```julia; wireless-10-104-201-207:BottomBoundaryLayer loganknudsen$ julia ""/Users/loganknudsen/Documents/GitHub/BottomBoundaryLayer/PSI_Base_Test.jl""; ┌ Warning: Overwriting existing ./psi_base_ocng_test.nc.; └ @ Oceananigans.OutputWriters ~/.julia/packages/Oceananigans/Feeqx/src/OutputWriters/netcdf_output_writer.jl:359; ERROR: LoadError: NetCDF error: Permission denied (NetCDF error code: 13); Stacktrace:; [1] check; @ ~/.julia/packages/NCDatasets/st9Jz/src/errorhandling.jl:25 [inlined]; [2] nc_create(path::String, cmode::UInt16); @ NCDatasets ~/.julia/packages/NCDatasets/st9Jz/src/netcdf_c.jl:255; [3] NCDatasets.NCDataset(filename::String, mode::String; format::Symbol, share::Bool, diskless::Bool, persist::Bool, memory::Nothing, attrib::Dict{Any, Any}); @ NCDatasets ~/.julia/packages/NCDatasets/st9Jz/src/dataset.jl:236; [4] NCDataset; @ ~/.julia/packages/NCDatasets/st9Jz/src/dataset.jl:177 [inlined]; [5] NetCDFOutputWriter(model::NonhydrostaticModel{Oceananigans.TimeSteppers.RungeKutta3TimeStepper{Float64, NamedTuple{(:u, :v, :w, :b), Tuple{Field{Face, Center, Center, Nothing, RectilinearGrid{Float64, Flat, Periodic, Bounded, Float64, Float64, Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, CPU}, Tuple{Colon, Colon, Colon}, OffsetArrays.OffsetArray{Float64, 3, Array{Float64, 3}}, Float64, FieldBoundaryConditions{Nothing, Nothing, BoundaryCondition{Oceananigans.BoundaryConditions.Periodic, Nothing}, BoundaryCondition{O",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3228#issuecomment-1747397784:585,Load,LoadError,585,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3228#issuecomment-1747397784,1,['Load'],['LoadError']
Performance,"Apparently defining a forcing in this way actually slows the model down by a lot. It's ~2x slower with just this one forcing function which seems a little excessive. ```julia; @inline FT(grid, U, Φ, i, j, k) = ifelse(k == 1, -1e-4 * (Φ.T[i, j, 1] - 0), 0); forcing = Forcing(FT=FT); ```. Seems that maybe the exact way the forcing function is defined could have a huge impact on performance.",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/365:379,perform,performance,379,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/365,1,['perform'],['performance']
Performance,"Are the current options (`@async` blocks) not sufficient? I'd rather not have a parallel mechanism and have to maintain the APIs (explicitly passing streams/queues) to support it. Even right now it's relatively broken, only supporting kernel launches and memory copies (i.e. BLAS APIs do not take explicit stream arguments).",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2924#issuecomment-1429353692:157,queue,queues,157,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2924#issuecomment-1429353692,1,['queue'],['queues']
Performance,"Are we ok with this as a user interface? it's a little implicit. We could alternatively use a function-based API to make things a little more obvious, something like. ```julia; fts[1, 2, 3, 4] # get 4th time-index; ```. ```julia; at_time(4, fts, 1, 2, 3) # linearly interpolate to t=4; ```. mainly i'd be worried about issues like. ```julia; fts[1, 2, 3, 4] \ne fts[1, 2, 3, 4.0]; ```. which is rather easy to confuse?. We also might be able to use syntax like. ```julia; fts[1, 2, 3, time=4]; ```. if that is performant. Or. ```julia; fts[1, 2, 3, Time(4)]; ```",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3236#issuecomment-1696028139:510,perform,performant,510,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3236#issuecomment-1696028139,1,['perform'],['performant']
Performance,"Are you proposing a third interface that takes a 3-argument function of `x, L, N`? I think that's fine as a proposal, I just want to be clear about what is being suggested, exactly. What I didn't understand is how one can specify the cell interface locations (call it the ""`z_faces` interface"" --- the one we currently have) without knowing the number of grid points:. > I agree that to define the grid with z_faces we need to know the number of grid points, but that can be done internally. I agree there are other possible interfaces that _do not_ require passing the number of grid points as a parameter; or rather simplify this process by performing calculations under the hood. It could be nice to design types such as `LinearStretching` or `ChebyshevStretching` that provide users with out-of-the-box stretched grids to be used with interface 2 proposed above (call this the ""coordinate map interface""). We can incorporate dispatch on such types (versus on plain functions) in the grid constructor. An example of this being used is. ```julia; grid = VerticallyStretchedRectilinearGrid(size = (Nx, Ny, Nz), ; x = (0, 2π),; y = (0, 2π),; z = (-1, 1),; z_stretching = ChebyshevStretching()); ```. or, when a parameter is involves such as for hyperbolic stretching, something like. ```julia; grid = VerticallyStretchedRectilinearGrid(size = (Nx, Ny, Nz), ; x = (0, 2π),; y = (0, 2π),; z = (-1, 1),; z_stretching = HyperbolicStretching(stretching_parameter=1.3)); ```. It seems like maybe there is a 3rd option too? It looks somewhat similar to the coordinate map interface, except that the map expects three arguments rather than one, and returns an array rather than a scalar value that represents the map?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1551#issuecomment-815419100:643,perform,performing,643,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1551#issuecomment-815419100,1,['perform'],['performing']
Performance,"As a first step towards porting the Simulations infrastructure out of Oceananigans and into (ClimaEarth)[https://github.com/CliMA/ClimaEarth.jl], we should refactor the Simulations implementation so that it gets loaded _first_. Currently, it is loaded last:. https://github.com/CliMA/Oceananigans.jl/blob/fad81b074cc914173d1760ae4769a24841e20a83/src/Oceananigans.jl#L195-L229. This reorganization will help disentangle the current implementation from Oceananigans.",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3088:212,load,loaded,212,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3088,2,['load'],['loaded']
Performance,"As an aside, I think this issue illustrates that users are indeed interested in being able to evaluate gradients across boundaries. This is important because @simone-silvestri proposed a change that would make this impossible (eg it has been proposed we do not fill halos for `Value` and `Gradient` boundary conditions, and instead evaluate the associated fluxes in the same way we do for immersed boundaries --- because this has performance advantages for very large models).",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3224#issuecomment-1689901112:430,perform,performance,430,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3224#issuecomment-1689901112,1,['perform'],['performance']
Performance,"As an update, I have pushed a commit consisting of a working version of the double gyre example, where Δz is replaced by zspacings and znodes, the plots are improved with additional attributes, and visualization is performed on the CPU even if the code runs on the GPU. I still need to incorporate some of the suggested modifications listed above. I am uploading the plots and animation here. In today's meeting with Navid and Simone, we looked at switching to the lat-lon grid and more. In the upcoming commits, I will run for longer time on a lat-lon grid, introduce checkpoints for restarting the simulation, and add a topography. . [double_gyre_grid_spacing.pdf](https://github.com/CliMA/Oceananigans.jl/files/11493173/double_gyre_grid_spacing.pdf). [double_gyre_circulation.pdf](https://github.com/CliMA/Oceananigans.jl/files/11493174/double_gyre_circulation.pdf). https://github.com/CliMA/Oceananigans.jl/assets/12926768/cf148a0c-58c8-4ba2-b649-cb40418d0665",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3087#issuecomment-1550535207:215,perform,performed,215,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3087#issuecomment-1550535207,1,['perform'],['performed']
Performance,"As discussed via zoom with @kburns, passing a key to computations to be stored and used to determine whether a computation needs to be performed is a simple method that may work for us. A simple option for a key is the current model time, which works for all the time stepping methods we employ and has the additional advantage of interpretability. One complication is that we allow users to specify memory space for `ComputedField`s and `AveragedField`s. As a result, two `ComputedField`s that share memory space may have incorrect `data` if the memory is overwritten. This is, in fact, a problem even in the current code and not dependent on the optimizations discussed in this issue. Since we think it is important to give users the option of avoiding unnecessary memory allocation by managing the allocation of scratch space for computations, we cannot prevent incorrect output resulting from overwriting of scratch space during operations with embedded averaged fields and computed fields. We simply have to document this potential ""gotcha"". We can make avoiding repeated operations a bit safer by requiring users to enable it when a `ComputedField` or `AveragedField` is constructed by a keyword argument, something like `recompute_safely`: . ```julia; U = AveragedField(model.velocities.u, dims=(1, 2), data=scratch, recompute_safely=false); ```. When `recompute_safely` is disabled, the model time at computation will be cached. The cache can either be inside `AveragedField`, or in a global cache. (A global cache has the advantage of being on the CPU; a local cache has the advantage of being local).",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/955#issuecomment-694601458:135,perform,performed,135,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/955#issuecomment-694601458,7,"['cache', 'optimiz', 'perform']","['cache', 'cached', 'optimizations', 'performed']"
Performance,"As far as I know the stock FFTW comes in multithreaded version. Passing; nthreads to the planner will result in a planned fft. Could make an “fftkwargs” argument to Model. Then the user can specify; whatever arg they want to the planner via a Dict. On Sat, Mar 9, 2019 at 8:42 AM Gregory Wagner <; gregory.leclaire.wagner@gmail.com> wrote:. > I think “Nthreads” is a keyword argument in plan_fft.; >; > On Sat, Mar 9, 2019 at 8:27 AM Ali Ramadhan <notifications@github.com>; > wrote:; >; >> From Googling around I believe FFTW.jl is build in serial mode? I; >> couldn't see how to make use of multi-threading or specify the number of; >> threads.; >>; >> A related performance optimization would be to consider using Intel's MKL; >> which is usually faster on Intel machines (and might come with; >> multi-threading out of the box).; >>; >> But since we're not running large models on single-core CPUs this seems; >> like a low priority consideration for now.; >>; >> —; >> You are receiving this because you were mentioned.; >> Reply to this email directly, view it on GitHub; >> <https://github.com/climate-machine/Oceananigans.jl/issues/119#issuecomment-471176913>,; >> or mute the thread; >> <https://github.com/notifications/unsubscribe-auth/AOkIBka4H13o1NmaeZ8LRM2DPBpur-kDks5vU7a0gaJpZM4bmrZ0>; >> .; >>; >",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/119#issuecomment-471178290:593,multi-thread,multi-threading,593,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/119#issuecomment-471178290,4,"['multi-thread', 'optimiz', 'perform']","['multi-threading', 'optimization', 'performance']"
Performance,"As suggested by @francispoulin, the following was commented out https://github.com/CliMA/Oceananigans.jl/blob/master/src/Models/ShallowWaterModels/update_shallow_water_state.jl#L19-L22 to remove filling halo regions and buffering between ranks.; This gave perfect efficiency up to 3 ranks. This was mainly done to locate where possible bottlenecks are and not a legitimate change to the code. It was expected that buffering is what's causing efficiency decreases, and this confirms that there are no other additional undetected causes for efficiency drops.; <html>; <body>; <!--StartFragment-->. size | ranks | slowdown | efficiency | memory | allocs; -- | -- | -- | -- | -- | --; (4096, 256) | (1, 1) | 1.0 | 1.0 | 1.0 | 1.0; (4096, 512) | (1, 2) | 0.988079 | 1.01206 | 1.06328 | 1.0406; (4096, 768) | (1, 3) | 0.992832 | 1.00722 | 1.06328 | 1.0406. <!--EndFragment-->; </body>; </html>. system environment and CUDA.versioninfo():; ```; Oceananigans v0.60.0; Julia Version 1.6.2; Commit 1b93d53fc4 (2021-07-14 15:36 UTC); Platform Info:; OS: Linux (powerpc64le-unknown-linux-gnu); CPU: unknown; WORD_SIZE: 64; LIBM: libopenlibm; LLVM: libLLVM-11.0.1 (ORCJIT, pwr9); Environment:; JULIA_MPI_PATH = /home/software/spack/openmpi/3.1.4-nhjzelonyovxks5ydtrxehceqxsbf7ik; JULIA_CUDA_USE_BINARYBUILDER = false; JULIA_DEPOT_PATH = /nobackup/users/henryguo/projects/henry-test/.julia; GPU: Tesla V100-SXM2-32GB. CUDA toolkit 10.1.243, local installation; CUDA driver 10.2.0; NVIDIA driver 440.64.0; Libraries: ; - CUBLAS: 10.2.2; - CURAND: 10.1.1; - CUFFT: 10.1.1; - CUSOLVER: 10.2.0; - CUSPARSE: 10.3.0; - CUPTI: 12.0.0; - NVML: 10.0.0+440.64.0; - CUDNN: missing; - CUTENSOR: missing; Toolchain:; - Julia: 1.6.2; - LLVM: 11.0.1; - PTX ISA support: 3.2, 4.0, 4.1, 4.2, 4.3, 5.0, 6.0, 6.1, 6.3, 6.4; - Device capability support: sm_30, sm_32, sm_35, sm_37, sm_50, sm_52, sm_53, sm_60, sm_61, sm_62, sm_70, sm_72, sm_75; Environment:; - JULIA_CUDA_USE_BINARYBUILDER: false; 3 devices:; 0: Tesla V100-SXM2-32GB (",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1882#issuecomment-887933846:336,bottleneck,bottlenecks,336,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1882#issuecomment-887933846,1,['bottleneck'],['bottlenecks']
Performance,"At the GPU hackathon way back in June we learned that the `calculate_interior_source_terms` kernel was a bottleneck as each thread required a lot of registers. It could benefit greatly from shared memory to reduce register pressure and allow more threads to run at a time. Some preliminary work has been done in PR https://github.com/climate-machine/Oceananigans.jl/pull/293. @vchuravy has an `@stencil` abstraction in development at https://github.com/vchuravy/GPUifyLoops.jl/pull/81. But would be good to implement plain shared memory without an abstraction and see how much of a performance boost we get, especially with LES closures.",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/442:105,bottleneck,bottleneck,105,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/442,2,"['bottleneck', 'perform']","['bottleneck', 'performance']"
Performance,"At the moment we fill the velocity halos with multiple passes, e.g., . https://github.com/CliMA/Oceananigans.jl/blob/2447ea7c15d552fb6a50d3fd347d6534af0018c7/validation/multi_region/multi_region_cubed_sphere.jl#L115-L119. We should utilize the grid's connectivity and develop a method to fill the velocity halos that only requires _one_ pass. This is very important for performance and scaling on distributed systems.",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3201:370,perform,performance,370,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3201,1,['perform'],['performance']
Performance,"At the moment, the pressure solve is performed on one GPU/CPU, but this PR builds the infrastructure to allow multi-process global solves",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2795:37,perform,performed,37,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2795,1,['perform'],['performed']
Performance,"Average reduction with conditional expressions, e.g., like. https://github.com/CliMA/Oceananigans.jl/blob/748feab10a55fa65a46455620203252a6fc0646e/test/test_field_reductions.jl#L107. induce scalar operations on the GPU. I guess it's not a surprise. I just had to add, e.g,. ```Julia; @compute Txyz = CUDA.@allowscalar Field(Average(T, condition=T.>3)); ```. Only mentioning this here in case it might something in the source code that could be affecting code performance.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2701#issuecomment-1221170568:459,perform,performance,459,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2701#issuecomment-1221170568,1,['perform'],['performance']
Performance,"Averaging operations does not allocate any extra memory and is more performant than precalculating a field, storing the data, and then taking the average of that. In general, you only need 3D scratch space if you have 3D output.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2931#issuecomment-1439244612:68,perform,performant,68,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2931#issuecomment-1439244612,1,['perform'],['performant']
Performance,Avoid data dep race condition in CI,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1804:15,race condition,race condition,15,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1804,1,['race condition'],['race condition']
Performance,"Awesome stuff! Should make scripting life much more comfortable. We should release v0.9.3 once this is merged. GPU tests don't start due to this error. Will see if I can fix it.; ```julia; ERROR: LoadError: LoadError: UndefVarError: T1 not defined; Stacktrace:; [1] top-level scope at none:0; [2] include at ./boot.jl:326 [inlined]; [3] include_relative(::Module, ::String) at ./loading.jl:1038; [4] include at ./sysimg.jl:29 [inlined]; [5] include(::String) at /builds/JuliaGPU/Oceananigans-jl/src/Oceananigans.jl:1; [6] top-level scope at none:0; [7] include at ./boot.jl:326 [inlined]; [8] include_relative(::Module, ::String) at ./loading.jl:1038; [9] include(::Module, ::String) at ./sysimg.jl:29; [10] top-level scope at none:2; [11] eval at ./boot.jl:328 [inlined]; [12] eval(::Expr) at ./client.jl:404; [13] top-level scope at ./none:3; in expression starting at /builds/JuliaGPU/Oceananigans-jl/src/fields.jl:188; in expression starting at /builds/JuliaGPU/Oceananigans-jl/src/Oceananigans.jl:204; ```",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/343#issuecomment-519320783:196,Load,LoadError,196,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/343#issuecomment-519320783,4,"['Load', 'load']","['LoadError', 'loading']"
Performance,"Awesome! The two solvers do pretty different things at this point because; CuFFT doesn't do DCTs or r2r transforms but I've also baked in some; performance optimizations to reduce memory access and operation count. We; should be able to hide some of it away with dispatch but some of the; optimizations are baked into the time stepping so not completely... Might be easier to talk in person when we meet later today?. On Tue, Mar 5, 2019, 9:00 AM Gregory L. Wagner <notifications@github.com>; wrote:. > I'm going to work on this. I would also like to improve the solver; > implementation. Why are there different spectral solvers; > <https://github.com/climate-machine/Oceananigans.jl/blob/3cd4ae32cb4d716bc6470a6e7ba484ed98d60de7/src/spectral_solvers.jl#L4>; > for different devices; > <https://github.com/climate-machine/Oceananigans.jl/blob/3cd4ae32cb4d716bc6470a6e7ba484ed98d60de7/src/spectral_solvers.jl#L215>?; > Can we combine them into one type with the Device as a type parameter?; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/climate-machine/Oceananigans.jl/issues/102#issuecomment-469689574>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ATKyBV5TY3u4FjjnraBoJT3fw5L4JqWNks5vTnhggaJpZM4bahv3>; > .; >",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/102#issuecomment-469691219:144,perform,performance,144,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/102#issuecomment-469691219,3,"['optimiz', 'perform']","['optimizations', 'performance']"
Performance,"Base.PkgId, input::String, depot_path::Vector{String}, dl_load_path::Vector{String}, load_path::Vector{String}, concrete_deps::Vector{Pair{Base.PkgId, UInt64}}, source::String); @ Base ./loading.jl:1318; [16] top-level scope; @ none:1; [17] eval; @ ./boot.jl:373 [inlined]; [18] eval(x::Expr); @ Base.MainInclude ./client.jl:453; [19] top-level scope; @ none:1; during initialization of module MPICH_jll; in expression starting at /Users/sean/.julia/packages/MPI/08SPr/deps/deps.jl:1; ERROR: LoadError: Failed to precompile MPI [da04e1cc-30fd-572f-bb4f-1f8673147195] to /Users/sean/.julia/compiled/v1.7/MPI/jl_AfEwik.; Stacktrace:; [1] error(s::String); @ Base ./error.jl:33; [2] compilecache(pkg::Base.PkgId, path::String, internal_stderr::IO, internal_stdout::IO, ignore_loaded_modules::Bool); @ Base ./loading.jl:1466; [3] compilecache(pkg::Base.PkgId, path::String); @ Base ./loading.jl:1410; [4] _require(pkg::Base.PkgId); @ Base ./loading.jl:1120; [5] require(uuidkey::Base.PkgId); @ Base ./loading.jl:1013; [6] require(into::Module, mod::Symbol); @ Base ./loading.jl:997; [7] include(mod::Module, _path::String); @ Base ./Base.jl:418; [8] include(x::String); @ Oceananigans ~/.julia/packages/Oceananigans/jmNfq/src/Oceananigans.jl:5; [9] top-level scope; @ ~/.julia/packages/Oceananigans/jmNfq/src/Oceananigans.jl:190; [10] include; @ ./Base.jl:418 [inlined]; [11] include_package_for_output(pkg::Base.PkgId, input::String, depot_path::Vector{String}, dl_load_path::Vector{String}, load_path::Vector{String}, concrete_deps::Vector{Pair{Base.PkgId, UInt64}}, source::Nothing); @ Base ./loading.jl:1318; [12] top-level scope; @ none:1; [13] eval; @ ./boot.jl:373 [inlined]; [14] eval(x::Expr); @ Base.MainInclude ./client.jl:453; [15] top-level scope; @ none:1; in expression starting at /Users/sean/.julia/packages/Oceananigans/jmNfq/src/Distributed/Distributed.jl:1; in expression starting at /Users/sean/.julia/packages/Oceananigans/jmNfq/src/Oceananigans.jl:1; ERROR: Failed to precompile Oce",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2480:4791,load,loading,4791,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2480,1,['load'],['loading']
Performance,"Base.jl:418 [inlined]; [15] include_package_for_output(pkg::Base.PkgId, input::String, depot_path::Vector{String}, dl_load_path::Vector{String}, load_path::Vector{String}, concrete_deps::Vector{Pair{Base.PkgId, UInt64}}, source::String); @ Base ./loading.jl:1318; [16] top-level scope; @ none:1; [17] eval; @ ./boot.jl:373 [inlined]; [18] eval(x::Expr); @ Base.MainInclude ./client.jl:453; [19] top-level scope; @ none:1; during initialization of module MPICH_jll; in expression starting at /Users/sean/.julia/packages/MPI/08SPr/deps/deps.jl:1; ERROR: LoadError: Failed to precompile MPI [da04e1cc-30fd-572f-bb4f-1f8673147195] to /Users/sean/.julia/compiled/v1.7/MPI/jl_AfEwik.; Stacktrace:; [1] error(s::String); @ Base ./error.jl:33; [2] compilecache(pkg::Base.PkgId, path::String, internal_stderr::IO, internal_stdout::IO, ignore_loaded_modules::Bool); @ Base ./loading.jl:1466; [3] compilecache(pkg::Base.PkgId, path::String); @ Base ./loading.jl:1410; [4] _require(pkg::Base.PkgId); @ Base ./loading.jl:1120; [5] require(uuidkey::Base.PkgId); @ Base ./loading.jl:1013; [6] require(into::Module, mod::Symbol); @ Base ./loading.jl:997; [7] include(mod::Module, _path::String); @ Base ./Base.jl:418; [8] include(x::String); @ Oceananigans ~/.julia/packages/Oceananigans/jmNfq/src/Oceananigans.jl:5; [9] top-level scope; @ ~/.julia/packages/Oceananigans/jmNfq/src/Oceananigans.jl:190; [10] include; @ ./Base.jl:418 [inlined]; [11] include_package_for_output(pkg::Base.PkgId, input::String, depot_path::Vector{String}, dl_load_path::Vector{String}, load_path::Vector{String}, concrete_deps::Vector{Pair{Base.PkgId, UInt64}}, source::Nothing); @ Base ./loading.jl:1318; [12] top-level scope; @ none:1; [13] eval; @ ./boot.jl:373 [inlined]; [14] eval(x::Expr); @ Base.MainInclude ./client.jl:453; [15] top-level scope; @ none:1; in expression starting at /Users/sean/.julia/packages/Oceananigans/jmNfq/src/Distributed/Distributed.jl:1; in expression starting at /Users/sean/.julia/packages/Oceananigans/",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2480:4731,load,loading,4731,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2480,1,['load'],['loading']
Performance,"Based on some previous benchmarks I think so; https://github.com/climate-machine/Oceananigans.jl/blob/3cd4ae32cb4d716bc6470a6e7ba484ed98d60de7/benchmark/gpu.jl#L164-L198; Basically I wrote an _x_-difference operator that used `@. @views` which allowed it to act on Arrays and CuArrays, and when acting on a CuArray it was only ~10x faster compared to single-core CPU performance. While the entire time-stepping loop we have using GPUifyLoops.jl is ~90x faster. **However** I don't think I did the benchmarking there properly (have to use `@benchmark CuArrays.@sync` I believe) and maybe I didn't use `@. @views` properly. I think using broadcasts would make the code much more readable, but I can think of a few drawbacks (mainly related to performance):. 1. I don't know how to fuse kernels when doing broadcasts. E.g. we might be able to do something like; ```julia; @. Gu = calc_RHS_u(u, v, w, ...); @. Gv = calc_RHS_v(u, v, w, ...); ...; ```; which would look cleaner but then `update_source_terms!` would end essentially end up in several kernels. From talking with you I believe this shouldn't matter as kernel launches are cheap and the broadcast operator should essentially do exactly what `update_source_terms!` does, but I'm not 100% sure of this and we may lose out on performance. 2. We may want to fine tune our kernel launches, e.g. by using the thread-block layout we want which is probably possible, I'm just not very familiar with the CuArrays.jl package. 3. Some kernels, e.g. ones that do permutations like `calculate_source_term_divergence_gpu!` and `idct_permute!` can't be broadcasted over I think, but these are the exception rather than the rule. To really find out whether we take a performance hit we might have to refactor the time-stepping to use broadcasting then benchmark the two approaches on CPU, GPU, and multi-GPU architectures (not sure if CuArrays.jl will do multi-GPU arrays yet). This will become much easier once #67 is resolved.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/108#issuecomment-469765393:367,perform,performance,367,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/108#issuecomment-469765393,5,"['perform', 'tune']","['performance', 'tune']"
Performance,"Based on the answers here I tried to set my own mask function. It appears to run fine on the CPU but when I try to run it on the GPU I get this error when running the simulation (with a few subsequent errors after these lines, but this is the first one):. ```; ERROR: LoadError: InvalidIRError: compiling kernel gpu_calculate_Gw!(Cassette.Context{nametype(CUDACtx),KernelAbstractions.CompilerMetadata{KernelAbstractions.NDIteration.StaticSize{(2048, 1, 256)},KernelAbstractions.NDIteration.DynamicCheck,Nothing,Nothing,KernelAbstractions.NDIteration.NDRange{3,KernelAbstractions.NDIteration.StaticSize{(2048, 1, 256)},KernelAbstractions.NDIteration.StaticSize{(1, 256, 1)},Nothing,Nothing}},Nothing,KernelAbstractions.var""##PassType#253"",Nothing,Cassette.DisableHooks}, typeof(Oceananigans.TimeSteppers.gpu_calculate_Gw!), OffsetArrays.OffsetArray{Float64,3,CUDA.CuDeviceArray{Float64,3,1}}, RegularCartesianGrid{Float64,Periodic,Periodic,Bounded,OffsetArrays.OffsetArray{Float64,1,StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}}}, UpwindBiasedThirdOrder, FPlane{Float64}, Nothing, AnisotropicDiffusivity{Float64,Float64,Float64,NamedTuple{(:b,),Tuple{Float64}},NamedTuple{(:b,),Tuple{Float64}},NamedTuple{(:b,),Tuple{Float64}}}, NamedTuple{(:velocities, :tracers),Tuple{NamedTuple{(:u, :v, :w),Tuple{Oceananigans.Fields.ZeroField,Oceananigans.Fields.ZeroField,Oceananigans.Fields.ZeroField}},NamedTuple{(:b,),Tuple{Oceananigans.Fields.ZeroField}}}}, NamedTuple{(:u, :v, :w),Tuple{OffsetArrays.OffsetArray{Float64,3,CUDA.CuDeviceArray{Float64,3,1}},OffsetArrays.OffsetArray{Float64,3,CUDA.CuDeviceArray{Float64,3,1}},OffsetArrays.OffsetArray{Float64,3,CUDA.CuDeviceArray{Float64,3,1}}}}, NamedTuple{(:b,),Tuple{OffsetArrays.OffsetArray{Float64,3,CUDA.CuDeviceArray{Float64,3,1}}}}, Nothing, NamedTuple{(:u, :v, :w, :b),Tuple{typeof(Oceananigans.Forcings.zeroforcing),typeof(Oceananigans.Forcings.zeroforcing),Oceananigans.Forcings.ContinuousForcing{Cell,Cell,Face,Noth",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1203#issuecomment-733435704:268,Load,LoadError,268,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1203#issuecomment-733435704,1,['Load'],['LoadError']
Performance,"Below is a link to a paper that compares the scalability of multi-threading in Python, Julia and Chapel. . Brief Summary: They find that none of them do as well as OpenMP but give some reasons as to why. But they do find some improvements going up to 64 threads, but the effiicency in some cases go down to 20%. It seems that Python might do better on low numbers of threads but Julia does better on more. This was last year so I am sure this should probably redone. Also, I should mention I don't believe their problem is like ours but it's an example and has some pictures, so that's nice to see. https://hal.inria.fr/hal-02879767/document",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1861#issuecomment-886076610:45,scalab,scalability,45,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1861#issuecomment-886076610,2,"['multi-thread', 'scalab']","['multi-threading', 'scalability']"
Performance,"Benchmark results:. ```; Julia Version 1.3.1; Commit 2d5741174c (2019-12-30 21:36 UTC); Platform Info:; OS: macOS (x86_64-apple-darwin18.6.0); uname: Darwin 19.3.0 Darwin Kernel Version 19.3.0: Thu Jan 9 20:58:23 PST 2020; root:xnu-6153.81.5~1/RELEASE_X86_64 x86_64 i386; CPU: Intel(R) Core(TM) i7-7920HQ CPU @ 3.10GHz: ; speed user nice sys idle irq; #1 3100 MHz 7280472 s 0 s 3664940 s 37447135 s 0 s; #2 3100 MHz 962205 s 0 s 463846 s 46966309 s 0 s; #3 3100 MHz 6058127 s 0 s 2588161 s 39746073 s 0 s; #4 3100 MHz 940664 s 0 s 349843 s 47101853 s 0 s; #5 3100 MHz 5327039 s 0 s 1908912 s 41156410 s 0 s; #6 3100 MHz 958596 s 0 s 311019 s 47122744 s 0 s; #7 3100 MHz 4606842 s 0 s 1506190 s 42279330 s 0 s; #8 3100 MHz 985952 s 0 s 278546 s 47127861 s 0 s; ; Memory: 16.0 GB (398.0859375 MB free); Uptime: 4.914851e6 sec; Load Avg: 2.8193359375 2.333984375 1.83544921875; WORD_SIZE: 64; LIBM: libopenlibm; LLVM: libLLVM-6.0.1 (ORCJIT, skylake); Environment:; JULIA_EDITOR = vim; TERM = xterm-256color; PALMHOME = /Users/gregorywagner/Software/palm; PATH = /Users/gregorywagner/opt/anaconda3/condabin:/opt/anaconda3/bin:/Users/gregorywagner/Software/palm/palm/current_version/trunk/SCRIPTS:/Applications/Julia-1.3.app/Contents/Resources/julia/bin:/usr/local/opt/ruby/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/Library/TeX/texbin:/opt/X11/bin; XPC_FLAGS = 0x0; HOME = /Users/gregorywagner. ──────────────────────────────────────────────────────────────────────────────────────; Static ocean benchmarks Time Allocations ; ────────────────────── ───────────────────────; Tot / % measured: 104s / 55.2% 7.44GiB / 0.08% . Section ncalls time %tot avg alloc %tot avg; ──────────────────────────────────────────────────────────────────────────────────────; 32× 32× 32 [CPU, Float32] 10 44.9ms 0.08% 4.49ms 752KiB 12.5% 75.2KiB; 32× 32× 32 [CPU, Float64] 10 39.1ms 0.07% 3.91ms 752KiB 12.5% 75.2KiB; 64× 64× 64 [CPU, Float32] 10 428ms 0.75% 42.8ms 752KiB 12.5% 75.2KiB; 64× 64× 64 [CPU, Float64] 10 3",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/710#issuecomment-629688917:825,Load,Load,825,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/710#issuecomment-629688917,1,['Load'],['Load']
Performance,"Benchmark? The reason we combined the updates for velocities was a perceived performance gain. Probably we were wrong about that, but it'd be good to show it.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1210#issuecomment-734369612:77,perform,performance,77,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1210#issuecomment-734369612,1,['perform'],['performance']
Performance,Benchmarking fully loaded simulations,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1089:19,load,loaded,19,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1089,1,['load'],['loaded']
Performance,"Billy mentioned it in some other comments but while we do splat args for some of the function calls, the function definitions use Varargs instead. This should avoid the catastrophic slowdown we saw with splatting earlier, but I agree that it should be tested. Do you have any good CPU performance tests set up @glwagner ?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3480#issuecomment-2150096206:285,perform,performance,285,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3480#issuecomment-2150096206,1,['perform'],['performance']
Performance,"But I'm not sure which part are you referring as ""good idea or not"". The fact that they belong to a different module? Perhaps we could have these constants loaded/exported with Oceananigans main module. Is this what you are thinking? That would work also!. What I think is _not_ good is having, e.g., every module redefining constants. E.g., `R_Earth` is currently defined in two places. Say we change one definition thinking it will affect everything but it won't. Also I found it quite cumbersome in scripts when I wanted to load these constants I had to load one from `Grids` and one from `Coriolis` and one from `BuoyancyModels`.... That's a bit counterintuitive from a user's perspective.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3045#issuecomment-1492618180:156,load,loaded,156,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3045#issuecomment-1492618180,3,['load'],"['load', 'loaded']"
Performance,"But definitely clean up and performance. Just to clarify fixing type inference doesn't change the result of the reduction, it just makes it go much faster",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3794#issuecomment-2380363489:28,perform,performance,28,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3794#issuecomment-2380363489,1,['perform'],['performance']
Performance,"By the way, WENO (and other advection schemes) are always going to be at most second order as they are implemented right now. So the _order_ might not be the correct metric to look at the performance, the truncation error is probably what we want to look at",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2615#issuecomment-1164473690:188,perform,performance,188,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2615#issuecomment-1164473690,1,['perform'],['performance']
Performance,CPU performance regression: tons of allocations,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/675:4,perform,performance,4,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/675,1,['perform'],['performance']
Performance,"CUDA uses a special buffer, the parameter space, to put arguments in. This buffer is about 4K large, and has special semantics that benefit performance (read-only, so threads can read from it without synchronizing, etc). Although arguments in Julia are normally passed by reference, i.e. putting pointers in that space, when invoking kernels we change the calling convention and pass by reference such that loading e.g. the size or pointer of an array doesn't synchronize threads. That works great, until you pass a large (number of) arguments as you apparently do.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/746#issuecomment-653428852:140,perform,performance,140,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/746#issuecomment-653428852,2,"['load', 'perform']","['loading', 'performance']"
Performance,"Can we use a wrapper that automagically loads data from file for the specified time index (and also perform spatial slicing), using the nice GeoData getindex syntax?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1244#issuecomment-738272413:40,load,loads,40,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1244#issuecomment-738272413,2,"['load', 'perform']","['loads', 'perform']"
Performance,"Can you tell us a bit more about your system?. What is `versioninfo(verbose=true)` and `] status -m`?. Was there anything more printed above the message that precompilation; failed?. One thing you might want to try is using Julia 1.10, instead of 1.9, but; right now I don't have information to pinpoint what is causing the; segmentation fault. On Wed, Jul 17, 2024, 12:23 Logan Knudsen ***@***.***> wrote:. > Update: I have been able to reduce the error to be a procompiling error in; > oceananigans.jl:; >; > ERROR: LoadError: Failed to precompile Oceananigans [9e8cae18-63c1-5223-a75c-80ca9d6e9a09] to ""/glade/u/home/knudsenl/.julia/compiled/v1.9/Oceananigans/jl_AMNEzH"".; > Stacktrace:; > [1] error(s::String); > @ Base ./error.jl:35; > [2] compilecache(pkg::Base.PkgId, path::String, internal_stderr::IO, internal_stdout::IO, keep_loaded_modules::Bool); > @ Base ./loading.jl:2300; > [3] compilecache; > @ ./loading.jl:2167 [inlined]; > [4] _require(pkg::Base.PkgId, env::String); > @ Base ./loading.jl:1805; > [5] _require_prelocked(uuidkey::Base.PkgId, env::String); > @ Base ./loading.jl:1660; > [6] macro expansion; > @ ./loading.jl:1648 [inlined]; > [7] macro expansion; > @ ./lock.jl:267 [inlined]; > [8] require(into::Module, mod::Symbol); > @ Base ./loading.jl:1611; > in expression starting at /glade/derecho/scratch/knudsenl/BottomBoundaryLayer/testcode.jl:1; >; > I have been trying to make sure that everything is up to date, and I am; > running on Julia version 1.9.2. My code is just; >; > using Oceananingans; >; > as I have been trying to get the library to loas properly. Does anyone; > have any experience with this error or does it make things any clearer?; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2233710372>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AABDO2XAIMUIHCDCBIJA4NLZM2K75AVCNFSM6AAAAABK643UJWVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDEMZTG4YTAMZXGI",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2233731098:518,Load,LoadError,518,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2233731098,4,"['Load', 'load']","['LoadError', 'loading']"
Performance,Catching common performance / type inference issues with user functions,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1885:16,perform,performance,16,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1885,1,['perform'],['performance']
Performance,"Changing the function call to ; ```julia; function define_output_variable!(dataset,; wsa::Union{WindowedSpatialAverage, WindowedTimeAverage{<:WindowedSpatialAverage}},; name, array_type, compression, attributes, dimensions); ```; raises some other errors and I'm not sure the best way to fix them. I think the issue is that a `WindowedTimeAverage` doesn't have the properties `field` (it has `operand`) and it doesn't have the property `dims`. . For example, running the code above I get:. ```; ERROR: LoadError: type WindowedTimeAverage has no field field; Stacktrace:; [1] getproperty(::WindowedTimeAverage{WindowedSpatialAverage{Field{Face,Center,Center,OffsetArrays.OffsetArray{Float64,3,Array{Float64,3}},RegularRectilinearGrid{Float64,Periodic,Periodic,Periodic,OffsetArrays.OffsetArray{Float64,1,StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}}},NamedTuple{(:x, :y, :z),Tuple{CoordinateBoundaryConditions{BoundaryCondition{Oceananigans.BoundaryConditions.Periodic,Nothing},BoundaryCondition{Oceananigans.BoundaryConditions.Periodic,Nothing}},CoordinateBoundaryConditions{BoundaryCondition{Oceananigans.BoundaryConditions.Periodic,Nothing},BoundaryCondition{Oceananigans.BoundaryConditions.Periodic,Nothing}},CoordinateBoundaryConditions{BoundaryCondition{Oceananigans.BoundaryConditions.Periodic,Nothing},BoundaryCondition{Oceananigans.BoundaryConditions.Periodic,Nothing}}}}},FieldSlicer{Colon,UnitRange{Int64},Colon},Int64},Array{Float64,2},FieldSlicer{Colon,Colon,Colon}}, ::Symbol) at ./Base.jl:33; [2] define_output_variable!(::NCDatasets.NCDataset{Nothing}, ::WindowedTimeAverage{WindowedSpatialAverage{Field{Face,Center,Center,OffsetArrays.OffsetArray{Float64,3,Array{Float64,3}},RegularRectilinearGrid{Float64,Periodic,Periodic,Periodic,OffsetArrays.OffsetArray{Float64,1,StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}}},NamedTuple{(:x, :y, :z),Tuple{CoordinateBoundaryConditions{BoundaryCondition{Oceananigans.BoundaryCo",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1397#issuecomment-784288449:502,Load,LoadError,502,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1397#issuecomment-784288449,1,['Load'],['LoadError']
Performance,Closing as I think we agreed that in this case it's better to load initial conditions from disk than add a portable random number generate as a utility.,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/176#issuecomment-506994970:62,load,load,62,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/176#issuecomment-506994970,1,['load'],['load']
Performance,"Coded some difference and averaging operators using the `@views` macro. Just a proof of concept right now as the model goes run on a GPU when creating a `Problem` with `arch=:gpu`, but because the kernels are very small (and there are many of them) it's very slow. At least we have something that runs, now we can worry about how to optimize for performance. Some hacks were used to get this to work but the operators and time stepping is completely shared. The only bit that is different is the quasi-spectral solver as cuFFT does not perform R2R or DCT transforms (`FFTW.REDFT01` and `FFTW.REDFT10`) in particular. A GPU-compatible DCT/IDCT transform was coded that calculates the DCT/IDCT in terms of `fft!` and `ifft!`.",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/15:333,optimiz,optimize,333,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/15,3,"['optimiz', 'perform']","['optimize', 'perform', 'performance']"
Performance,"Compilation performance is affected by this, so I think it's more correct to say that performance is affected.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3851#issuecomment-2429332352:12,perform,performance,12,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3851#issuecomment-2429332352,2,['perform'],['performance']
Performance,"Compiler/eJOtJ/src/irgen.jl:4; [4] macro expansion; @ ~/.julia/packages/GPUCompiler/eJOtJ/src/driver.jl:142 [inlined]; [5] macro expansion; @ ~/.julia/packages/TimerOutputs/PZq45/src/TimerOutput.jl:226 [inlined]; [6] macro expansion; @ ~/.julia/packages/GPUCompiler/eJOtJ/src/driver.jl:141 [inlined]; [7] emit_llvm(job::GPUCompiler.CompilerJob, method_instance::Any, world::UInt64; libraries::Bool, deferred_codegen::Bool, optimize::Bool, only_entry::Bool); @ GPUCompiler ~/.julia/packages/GPUCompiler/eJOtJ/src/utils.jl:62; [8] emit_llvm(job::GPUCompiler.CompilerJob, method_instance::Any, world::UInt64); @ GPUCompiler ~/.julia/packages/GPUCompiler/eJOtJ/src/utils.jl:60; [9] cufunction_compile(job::GPUCompiler.CompilerJob); @ CUDA ~/.julia/packages/CUDA/3VnCC/src/compiler/execution.jl:300; [10] check_cache; @ ~/.julia/packages/GPUCompiler/eJOtJ/src/cache.jl:47 [inlined]; [11] cached_compilation; @ ~/.julia/packages/GPUArrays/Z5nPF/src/host/broadcast.jl:57 [inlined]; [12] cached_compilation(cache::Dict{UInt64, Any}, job::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget, CUDA.CUDACompilerParams, GPUCompiler.FunctionSpec{GPUArrays.var""#broadcast_kernel#16"", Tuple{CUDA.CuKernelContext, CUDA.CuDeviceArray{Float64, 3, 1}, Base.Broadcast.Broadcasted{Nothing, Tuple{Base.OneTo{Int64}, Base.OneTo{Int64}, Base.OneTo{Int64}}, typeof(identity), Tuple{Int64}}, Int64}}}, compiler::typeof(CUDA.cufunction_compile), linker::typeof(CUDA.cufunction_link)); @ GPUCompiler ~/.julia/packages/GPUCompiler/eJOtJ/src/cache.jl:0; [13] cufunction(f::GPUArrays.var""#broadcast_kernel#16"", tt::Type{Tuple{CUDA.CuKernelContext, CUDA.CuDeviceArray{Float64, 3, 1}, Base.Broadcast.Broadcasted{Nothing, Tuple{Base.OneTo{Int64}, Base.OneTo{Int64}, Base.OneTo{Int64}}, typeof(identity), Tuple{Int64}}, Int64}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{}, Union{}, Tuple{}, NamedTuple{(), Tuple{}}}); @ CUDA ~/.julia/packages/CUDA/3VnCC/src/compiler/execution.jl:289; [14] cufunction; @ ~/.julia/packages/CU",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1706:1676,cache,cache,1676,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1706,1,['cache'],['cache']
Performance,"ContextName, Any...) in module Cassette at /home/travis/.julia/packages/Cassette/1rVkq/src/overdub.jl:500 overwritten in module GPUifyLoops at /home/travis/.julia/packages/Cassette/1rVkq/src/overdub.jl:500.; WARNING: Method definition recurse(Cassette.Context{N, M, T, P, B, H} where H<:Union{Cassette.DisableHooks, Nothing} where B<:Union{Nothing, Base.IdDict{Module, Base.Dict{Symbol, Cassette.BindingMeta}}} where P<:Cassette.AbstractPass where T<:Union{Nothing, Cassette.Tag{N, X, E} where E where X where N<:Cassette.AbstractContextName} where M where N<:Cassette.AbstractContextName, Any...) in module Cassette at /home/travis/.julia/packages/Cassette/1rVkq/src/overdub.jl:512 overwritten in module GPUifyLoops at /home/travis/.julia/packages/Cassette/1rVkq/src/overdub.jl:512.; ERROR: LoadError: LoadError: UndefVarError: CUBLAS not defined; Stacktrace:; [1] top-level scope at none:0 (repeats 2 times); [2] include at ./boot.jl:326 [inlined]; [3] include_relative(::Module, ::String) at ./loading.jl:1038; [4] include at ./sysimg.jl:29 [inlined]; [5] include(::String) at /home/travis/.julia/packages/CuArrays/qZCAt/src/CuArrays.jl:3; [6] top-level scope at none:0; [7] include at ./boot.jl:326 [inlined]; [8] include_relative(::Module, ::String) at ./loading.jl:1038; [9] include(::Module, ::String) at ./sysimg.jl:29; [10] top-level scope at none:2; [11] eval at ./boot.jl:328 [inlined]; [12] eval(::Expr) at ./client.jl:404; [13] top-level scope at ./none:3; in expression starting at /home/travis/.julia/packages/CuArrays/qZCAt/src/deprecated.jl:5; in expression starting at /home/travis/.julia/packages/CuArrays/qZCAt/src/CuArrays.jl:54; ERROR: LoadError: LoadError: LoadError: LoadError: UndefVarError: @setup not defined; Stacktrace:; [1] top-level scope; [2] #macroexpand#35 at ./expr.jl:107 [inlined]; [3] macroexpand at ./expr.jl:106 [inlined]; [4] docm(::LineNumberNode, ::Module, ::Any, ::Any, ::Bool) at ./docs/Docs.jl:509 (repeats 2 times); [5] @doc(::LineNumberNode, ::Module, :",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/153#issuecomment-477579168:2816,load,loading,2816,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/153#issuecomment-477579168,1,['load'],['loading']
Performance,"Convection/blob/bf2c917e43a6198a55061a46b2097b6b4a9dda3b/horizontal_diffusion.jl), in which we turn off advection in the `horizontal_convection.jl` example and numerically integrate the solution to equilibrium. We compare timeseries of the volume-integrated buoyancy dissipation rates calculated online versus those calculated offline (as in the `horizontal_convection.jl` example). The results show that the online calculation correctly asymptotes to the numerical solution of the equilibrium boundary value problem while the offline calculation is erroneous and effectively yields a Nusselt number that is more than 6 times too high. ![equilibration_ratio](https://github.com/CliMA/Oceananigans.jl/assets/12971166/1f79e7eb-b361-4ea0-aa76-a81d6049c25a). The bug is also evident by comparing snapshots of the two buoyancy dissipation rate fields. The dissipation rates computed offline clearly do not satisfy the no-flux boundary conditions on the boundaries. <img width=""587"" alt=""Screenshot 2023-08-22 at 12 38 27 PM"" src=""https://github.com/CliMA/Oceananigans.jl/assets/12971166/eee4e10c-432b-413c-8670-aaf47c8b0d0a"">. This bug is present in the live `main` Oceananigans.jl branch (circa `v0.86.0`), as is evident from the movie of the buoyancy dissipation rate field in the [`horizontal_convection.jl` example documentation](https://clima.github.io/OceananigansDocumentation/v0.84.1/generated/horizontal_convection/#Load-saved-output,-process,-visualize) and verified locally. <img width=""519"" alt=""Screenshot 2023-08-22 at 1 13 30 PM"" src=""https://github.com/CliMA/Oceananigans.jl/assets/12971166/c61b002c-5fd5-453d-8809-623e577e54f3"">. I am referring to this as a bug because it is contrary to the expected behavior of halos containing the necessary information for satisfying boundary conditions, as discussed in the horizontal convection documentation example:; https://github.com/CliMA/Oceananigans.jl/blob/a226b3efa7db7426ccee03884d610035314955e4/examples/horizontal_convection.jl#L143-L147",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3224:2220,Load,Load-saved-output,2220,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3224,1,['Load'],['Load-saved-output']
Performance,"Copied below. ```; ERROR: LoadError: `makedocs` encountered an error. Terminating build; --;   | Stacktrace:;   | [1] error(s::String);   | @ Base ./error.jl:33;   | [2] runner(#unused#::Type{Documenter.Builder.RenderDocument}, doc::Documenter.Documents.Document);   | @ Documenter.Builder /storage7/buildkite-agent/.julia-3423/packages/Documenter/4JDQo/src/Builder.jl:255;   | [3] dispatch(#unused#::Type{Documenter.Builder.DocumentPipeline}, x::Documenter.Documents.Document);   | @ Documenter.Utilities.Selectors /storage7/buildkite-agent/.julia-3423/packages/Documenter/4JDQo/src/Utilities/Selectors.jl:170;   | [4] #2;   | @ /storage7/buildkite-agent/.julia-3423/packages/Documenter/4JDQo/src/Documenter.jl:257 [inlined];   | [5] cd(f::Documenter.var""#2#3""{Documenter.Documents.Document}, dir::String);   | @ Base.Filesystem ./file.jl:106;   | [6] #makedocs#1;   | @ /storage7/buildkite-agent/.julia-3423/packages/Documenter/4JDQo/src/Documenter.jl:256 [inlined];   | [7] top-level scope;   | @ ~/builds/tartarus-9/clima/oceananigans/docs/make.jl:155;   | in expression starting at /var/lib/buildkite-agent/builds/tartarus-9/clima/oceananigans/docs/make.jl:155;   | 🚨 Error: The command exited with status 1;   | user command error: exit status 1; ```",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1881#issuecomment-884364299:26,Load,LoadError,26,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1881#issuecomment-884364299,1,['Load'],['LoadError']
Performance,"Could be interesting to explore! @suyashbire1 has looked at this I think. The big concerns are obviously performance, GPU compatibility, and we probably don't want to depend on an immature or abandoned package. We depend heavily on OffsetArrays.jl but it doesn't really come with any features we use besides the array type itself.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/457#issuecomment-541417904:105,perform,performance,105,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/457#issuecomment-541417904,1,['perform'],['performance']
Performance,"Could you explain why using `ifelse` has better performance?; Is this because the ternary `? :` is an alias for `ifelse`? ; What about `@inline`?. In this case, we only had the wind velocity from the specifications of the computer fans we used for the rotating tank experiment, but I understand that even in this case, I could calculate `τ₀` using bulk formula while defining `p`. Thanks for the idea. For the boundary conditions. I was my bad. You specify on the [Documentation](https://clima.github.io/OceananigansDocumentation/stable/model_setup/boundary_conditions/#.-Spatially-and-temporally-varying-flux) that. > By default, a function boundary condition is called with the signature; > ; > `f(ξ, η, t)`; > ; > where t is time and ξ, η are spatial coordinates that vary along the boundary:; > ; > `f(y, z, t)` on x-boundaries;; > `f(x, z, t)` on y-boundaries;; > `f(x, y, t)` on z-boundaries. I am just repeating here in case someone falls in the same problem and comes to this issue.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2336#issuecomment-1066113425:48,perform,performance,48,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2336#issuecomment-1066113425,1,['perform'],['performance']
Performance,"Current operators assume constant Δz which allow the model to use faster operators and use a tiny bit less space, so they'd have to be rewritten a tiny bit to account for a variable Δz when that gets implemented. We can either write new operators that get dispatched on `HorizontallyRegularCartesianGrid` structs (already possible), or maybe the performance gain is so tiny that we just make `RegularCartesianGrid` a subset of `HorizontallyRegularCartesianGrid` and only have one set of operators. `HorizontallyRegularCartesianGrid` might be a descriptive but pretty bad struct name.",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/47:346,perform,performance,346,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/47,1,['perform'],['performance']
Performance,"Currently `BatchedTridiagonalSolver` supports `Callable` tridiagonal coefficients via. https://github.com/CliMA/Oceananigans.jl/blob/bcc34f07b3f949ea6fb34c7814f4b856d24924c2/src/Solvers/batched_tridiagonal_solver.jl#L58-L59. However, as noted on #3030 this can produce a catastrophic loss of performance. Therefore, this support should be discontinued. cc @simone-silvestri",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3047:292,perform,performance,292,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3047,1,['perform'],['performance']
Performance,"Currently halo regions are filled _prior_ to performing a time-step. This means that _after_ the time-step, they are incorrect. We therefore cannot output fields with correct halo regions, since data is outputted after a time-step is taken. But it gets worse. If the average of a field is taken, we zero out the halo regions. Zeroing out the halo regions corrupts near-boundary data for all subsequent computations with the fields. Currently, abstract operations cannot be trusted in boundary-adjacent cells. To remedy this we need to fill halo regions on fields prior to performing computations. One way we might do this is to write a `compute!` method for fields:. ```julia; compute!(field::Field) = fill_halo_regions!(field); ```. We can also define a `conditional_compute!` method for `Field`s and add a `status` property, so that halo regions are not filled ""redundantly"". For this to work, we also need to invalidate `field.status` when halo regions are zeroed out by `compute!(averaged_field::AveragedField)`, (for example by setting `field.status.time = NaN`). This won't work currently, of course, due to #971 . So this issue cannot be resolved until #971 is resolved.",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1063:45,perform,performing,45,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1063,2,['perform'],['performing']
Performance,"Currently in my workflow, I define an environment variable that contains an string like `$init_$end.nc`, where `init` is the initial time and `end` is the `stop_time` of the simulation. In the first instantiation of the model for 60 days, this string will be appended to my output filename e.g.`vel_field_`, resulting on something like `vel_field_0_60.nc`. For the next pickup the filename will be changed to `vel_field_60_120.nc`. This works well when the files are not split, i.e. `file_splitting = NoFileSplitting()`. However when splitting the file, e.g. `file_splitting = TimeInterval(30days)`, the first instantiation will result in the following files:; ```; vel_fields_0_60_part1.nc vel_fields_0_60_part2.nc vel_fields_0_60_part3.nc; ```; then the next pickup will result in files: ; ```; vel_fields_60_120_part1.nc vel_fields_60_120_part3.nc vel_fields_60_120_part5.nc; vel_fields_60_120_part2.nc vel_fields_60_120_part4.nc; ```; of this files, the first 2 parts (`vel_fields_60_120_part1.nc` and `vel_fields_60_120_part2.nc`) are empty, and the `vel_fields_60_120_part3.nc` contains data from the day 61 until 91. This makes the loading of the data for post-processing a bit tricky, since some of they are empty and the disk storage will make more difficult chunking data. . If I don't change the name each time I pickup, with the `overwrite_existing=false` it crashes, and with `overwrite_existing=true` it rewrites all the files. . Another workflow will be to create a new folder for each pickup keeping the same filename, but splitting the files will result in the same issue of file duplication. . I hope this description of the workflow is clearer.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3818#issuecomment-2391946301:1139,load,loading,1139,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3818#issuecomment-2391946301,1,['load'],['loading']
Performance,"Currently schedules are exported from `Oceananigans.Diagnostics` and `Oceananigans.OutputWriters` while being defined in `Oceananigans.Utils`. I guess an inconsistency is that we export output writers from the top-level `Oceananigans` module so users can end up with access to output writers via `using Oceananigans` but without any schedules/intervals, leading to errors like. ```; ERROR: LoadError: UndefVarError: TimeInterval not defined; ```; cc @mukund-gupta @qwert2266. Might make sense to either export schedules at the top-level or stop exporting output writers at the top-level. Both seem like consistent solutions to me but I'll argue",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1131:390,Load,LoadError,390,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1131,1,['Load'],['LoadError']
Performance,"Currently, we use the constructor `output_writer.array_type` to convert array data prior to outputting:. https://github.com/CliMA/Oceananigans.jl/blob/03a6f855f839504d94cb8cee3c2665b17afbc6d5/src/OutputWriters/fetch_output.jl#L17. But we should use `convert` instead, because this avoids allocating memory when no type conversion is needed:. ```julia; julia> a = rand(1, 1, 1); 1×1×1 Array{Float64,3}:; [:, :, 1] =; 0.7727202498256802. julia> b = convert(Array{Float64}, a); 1×1×1 Array{Float64,3}:; [:, :, 1] =; 0.7727202498256802. julia> b === a; true; ```. so `b` is just a reference to `a`, but. ```julia; julia> c = Array{Float64}(a); 1×1×1 Array{Float64,3}:; [:, :, 1] =; 0.7727202498256802. julia> c === a; false; ```. `c` is not a reference to `a`. This matters very little since we basically always need to allocate memory to convert from Float64 to Float32. A related minor optimization would be to avoid converting views to the same type as the parent array, since we could just output those directly. I think we have to use dispatch on one of the type parameters of `SubArray` for that. But maybe simpler code is worth not implementing a minor optimization for an edge case.",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1182:884,optimiz,optimization,884,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1182,2,['optimiz'],['optimization']
Performance,"Damn, it looks like the tests on the GPU are not working because CUDA is not loaded properly. ; I am trying to address this in #3880. A segmentation fault probably means the MPI is not CUDA-aware. Typically, the MPI that is shipped with MPI_jll is not cuda-aware. A good way to check is; ```julia; julia> using MPI. julia> MPI.has_cuda(); true; ```",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3878#issuecomment-2443869989:77,load,loaded,77,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3878#issuecomment-2443869989,1,['load'],['loaded']
Performance,Data is loaded into `FieldTimeSeries` by. https://github.com/CliMA/Oceananigans.jl/blob/cca182a11bcd1881e20316fc80ac7782286a8bfe/src/OutputReaders/field_time_series.jl#L146. which calls. https://github.com/CliMA/Oceananigans.jl/blob/cca182a11bcd1881e20316fc80ac7782286a8bfe/src/OutputReaders/field_time_series.jl#L200-L205. The data seems to be loaded into the intermediate `Field`:. https://github.com/CliMA/Oceananigans.jl/blob/cca182a11bcd1881e20316fc80ac7782286a8bfe/src/OutputReaders/field_time_series.jl#L237. so the problem may be. https://github.com/CliMA/Oceananigans.jl/blob/cca182a11bcd1881e20316fc80ac7782286a8bfe/src/OutputReaders/field_time_series.jl#L205. Voila... https://github.com/CliMA/Oceananigans.jl/blob/cca182a11bcd1881e20316fc80ac7782286a8bfe/src/Fields/set!.jl#L43-L55,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3224#issuecomment-1689910675:8,load,loaded,8,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3224#issuecomment-1689910675,2,['load'],['loaded']
Performance,"Dear Oceananigans team,. I am new to Julia and Oceananigans.; I installed Julia version 1.6.2 (2021-07-14) on Ubuntu 20.04 LTS, then Oceananigans v0.62.1 as instructed. I am interested in ""stratified plane Couette flow"" .; When I run the case using ""julia run_stratified_couette_flow_simulations.jl"", I am having following error;. **""; ERROR: LoadError: UndefVarError: Value not defined; Stacktrace:; [1] simulate_stratified_couette_flow(; Nxy::Int64, Nz::Int64, arch::GPU, h::Int64, U_wall::Int64, Re::Int64, Pr::Float64, Ri::Int64, Ni::Int64, end_time::Int64); @ Main ~/Desktop/stratified_couette_flow/stratified_couette_flow.jl:103; [2] top-level scope; @ ~/Desktop/stratified_couette_flow/run_stratified_couette_flow_simulations.jl:3; in expression starting at /home/ilyas/Desktop/stratified_couette_flow/run_stratified_couette_flow_simulations.jl:3; ""****. It complaints about undefined Boundary Condition Value (stratified_couette_flow.jl:103).; Can you help me to fix it?. Please forgive me if I missed something obvious or made a trivial mistake.; Thanks,. Ilyas",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1981:343,Load,LoadError,343,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1981,1,['Load'],['LoadError']
Performance,"Dedalus currently has a binary configuration option to cache the evaluation of every intermediate operation within a single timestep / sub-stage. The recursive evaluation of an operator tree accepts and propagates a cache-key argument, which could be related to e.g. the simulation iteration and sub-stage. If caching is enabled, each operator checks it's own size-1 cache for the cache-key, and returns the result if present. Otherwise it evaluates itself, and stores the result under the cache-key. . This eliminates the repeated evaluation of individual operators, but at the cost of storing the result of every intermediate operation. We're currently working on an upgrade that first traverses the tree, counts the number of references to a given operator, and deallocates the cached result after the cache has been accessed the corresponding number of times.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/955#issuecomment-694332991:55,cache,cache,55,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/955#issuecomment-694332991,7,['cache'],"['cache', 'cache-key', 'cached']"
Performance,"Did a quick small strong scaling benchmark on Tartarus (256^3) up to 16 cores but results don't look super great? ~9.5x speedup on 16 cores. Better than multi-threading though. Maybe I'm not benchmarking properly though. Could also be missing some MPI barriers. Should probably learn how to profile MPI code. ```; Incompressible model strong scaling benchmark; ┌─────────────────┬───────┬────────────┬────────────┬────────────┬────────────┬────────────┬────────┐; │ size │ ranks │ min │ median │ mean │ max │ memory │ allocs │; ├─────────────────┼───────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────┤; │ (256, 256, 256) │ 1 │ 3.641 s │ 3.686 s │ 3.686 s │ 3.730 s │ 355.28 KiB │ 2336 │; │ (256, 256, 256) │ 2 │ 1.917 s │ 1.918 s │ 1.921 s │ 1.928 s │ 346.00 KiB │ 2782 │; │ (256, 256, 256) │ 4 │ 1.249 s │ 1.283 s │ 1.279 s │ 1.300 s │ 348.47 KiB │ 2822 │; │ (256, 256, 256) │ 8 │ 652.029 ms │ 714.833 ms │ 704.940 ms │ 738.885 ms │ 353.84 KiB │ 2902 │; │ (256, 256, 256) │ 16 │ 377.153 ms │ 388.435 ms │ 394.780 ms │ 415.562 ms │ 366.16 KiB │ 3062 │; └─────────────────┴───────┴────────────┴────────────┴────────────┴────────────┴────────────┴────────┘; ```. ```; Incompressible model strong scaling speedup; ┌─────────────────┬───────┬─────────┬──────────┬─────────┐; │ size │ ranks │ speedup │ memory │ allocs │; ├─────────────────┼───────┼─────────┼──────────┼─────────┤; │ (256, 256, 256) │ 1 │ 1.0 │ 1.0 │ 1.0 │; │ (256, 256, 256) │ 2 │ 1.92195 │ 0.973876 │ 1.19092 │; │ (256, 256, 256) │ 4 │ 2.87312 │ 0.980825 │ 1.20805 │; │ (256, 256, 256) │ 8 │ 5.15614 │ 0.995954 │ 1.24229 │; │ (256, 256, 256) │ 16 │ 9.48879 │ 1.03061 │ 1.31079 │; └─────────────────┴───────┴─────────┴──────────┴─────────┘; ```",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/590#issuecomment-794954624:153,multi-thread,multi-threading,153,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/590#issuecomment-794954624,1,['multi-thread'],['multi-threading']
Performance,"Didn't know about lazy package loading, thanks for pointing it out!. The model runs on GPUs (albeit pretty slowly right right now) so I'm in the process of cleaning up the code before thinking about optimizing for performance, this should come in handy.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/14#issuecomment-459425779:31,load,loading,31,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/14#issuecomment-459425779,3,"['load', 'optimiz', 'perform']","['loading', 'optimizing', 'performance']"
Performance,Do not install/load CUDA packages if no GPU is detected.,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/178:15,load,load,15,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/178,1,['load'],['load']
Performance,"Doctests fail because of all the `loop not unrolled` warnings we get.; We should remedy this before we move to Julia v1.10. @glwagner you bumped onto this previously, right?. ```Julia; julia> using Oceananigans; Precompiling Oceananigans; 1 dependency successfully precompiled in 11 seconds. 143 already precompiled.; [ Info: Oceananigans will use 8 threads. julia> grid = RectilinearGrid(size=(1, 8, 8), extent=(1, 1, 1)); 1×8×8 RectilinearGrid{Float64, Periodic, Periodic, Bounded} on CPU with 3×3×3 halo; ├── Periodic x ∈ [0.0, 1.0) regularly spaced with Δx=1.0; ├── Periodic y ∈ [0.0, 1.0) regularly spaced with Δy=0.125; └── Bounded z ∈ [-1.0, 0.0] regularly spaced with Δz=0.125. julia> model = NonhydrostaticModel(; grid); warning: /Users/navid/.julia/packages/KernelAbstractions/WoCk1/src/extras/loopinfo.jl:28:0: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering; warning: /Users/navid/.julia/packages/KernelAbstractions/WoCk1/src/extras/loopinfo.jl:28:0: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering; warning: /Users/navid/.julia/packages/KernelAbstractions/WoCk1/src/extras/loopinfo.jl:28:0: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering; warning: /Users/navid/.julia/packages/KernelAbstractions/WoCk1/src/extras/loopinfo.jl:28:0: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering; warning: /Users/navid/.julia/packages/KernelAbstractions/WoCk1/src/extras/loopinfo.jl:28:0: loop not unrolled: the optimizer was unable to perform the requeste",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3403#issuecomment-1872469814:845,optimiz,optimizer,845,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3403#issuecomment-1872469814,2,"['optimiz', 'perform']","['optimizer', 'perform']"
Performance,"Documents/Projects/Oceananigans.jl/src/Models/HydrostaticFreeSurfaceModels/compute_hydrostatic_free_surface_tendencies.jl:236; [7] gpu_compute_hydrostatic_free_surface_Gu!; @ ~/.julia/packages/KernelAbstractions/cWlFz/src/macros.jl:90; [8] gpu_compute_hydrostatic_free_surface_Gu!; @ ./none:0; Hint: catch this exception as `err` and call `code_typed(err; interactive = true)` to introspect the erronous code with Cthulhu.jl; Stacktrace:; [1] check_ir(job::GPUCompiler.CompilerJob{GPUCompiler.MetalCompilerTarget, Metal.MetalCompilerParams}, args::LLVM.Module); @ GPUCompiler ~/.julia/packages/GPUCompiler/Cp7sE/src/validation.jl:147; [2] macro expansion; @ ~/.julia/packages/GPUCompiler/Cp7sE/src/driver.jl:440 [inlined]; [3] macro expansion; @ ~/.julia/packages/TimerOutputs/RsWnF/src/TimerOutput.jl:253 [inlined]; [4] macro expansion; @ ~/.julia/packages/GPUCompiler/Cp7sE/src/driver.jl:439 [inlined]; [5] emit_llvm(job::GPUCompiler.CompilerJob; libraries::Bool, toplevel::Bool, optimize::Bool, cleanup::Bool, only_entry::Bool, validate::Bool); @ GPUCompiler ~/.julia/packages/GPUCompiler/Cp7sE/src/utils.jl:89; [6] emit_llvm; @ ~/.julia/packages/GPUCompiler/Cp7sE/src/utils.jl:83 [inlined]; [7] codegen(output::Symbol, job::GPUCompiler.CompilerJob; libraries::Bool, toplevel::Bool, optimize::Bool, cleanup::Bool, strip::Bool, validate::Bool, only_entry::Bool, parent_job::Nothing); @ GPUCompiler ~/.julia/packages/GPUCompiler/Cp7sE/src/driver.jl:129; [8] codegen; @ ~/.julia/packages/GPUCompiler/Cp7sE/src/driver.jl:110 [inlined]; [9] compile(target::Symbol, job::GPUCompiler.CompilerJob; libraries::Bool, toplevel::Bool, optimize::Bool, cleanup::Bool, strip::Bool, validate::Bool, only_entry::Bool); @ GPUCompiler ~/.julia/packages/GPUCompiler/Cp7sE/src/driver.jl:106; [10] compile; @ ~/.julia/packages/GPUCompiler/Cp7sE/src/driver.jl:98 [inlined]; [11] #45; @ ~/.julia/packages/Metal/lnkVP/src/compiler/compilation.jl:57 [inlined]; [12] JuliaContext(f::Metal.var""#45#46""{GPUCompiler.CompilerJob{",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2618#issuecomment-1731573822:36336,optimiz,optimize,36336,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2618#issuecomment-1731573822,1,['optimiz'],['optimize']
Performance,Does this reduce performance or is the effect negligible? (Just curious.),MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2754#issuecomment-1261409044:17,perform,performance,17,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2754#issuecomment-1261409044,1,['perform'],['performance']
Performance,ERROR: LoadError: UndefVarError: SolutionBoundaryConditions not defined,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/686:7,Load,LoadError,7,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/686,1,['Load'],['LoadError']
Performance,Each Appveyor build takes 30-50 minutes to run (and they run sequentially) so Appveyor takes forever to finish testing. Compare with ~10 minutes per build on Travis and JuliaGPU's CI on GitLab. It also builds CUDA packages unsuccessfully so maybe defining a CPU testing env and using it on Travis and Appveyor can help? See https://github.com/ali-ramadhan/Oceananigans.jl/issues/79. Problem might go away on its own but otherwise this might help: https://www.appveyor.com/docs/build-cache/,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/89:483,cache,cache,483,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/89,1,['cache'],['cache']
Performance,Enzyme (performance) mark some functions as non-differentiable,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3799:8,perform,performance,8,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3799,1,['perform'],['performance']
Performance,"Exactly what I am struggling with right now: figuring out how to quickly get x, y, z coordinates when loading an output file created previously (with JLD2 output writer}). So as far as I understand in that case the 'model' and 'grid' variables do not exist. ; `xc = file([""grid/xC""]) `; etc. works, but contains the halos.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1850#issuecomment-1022295367:102,load,loading,102,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1850#issuecomment-1022295367,1,['load'],['loading']
Performance,"Executing the validation/multi_region/cubed_sphere_steady_state.jl file yields an error partially shown below:; ```julia; julia> include(""validation/multi_region/cubed_sphere_steady_state.jl""); [ Info: Initializing simulation...; Iteration: 0000, time: 0 seconds, Δt: 41.133 ms, wall time: 0 seconds; [ Info: ... simulation initialization complete (820.774 ms); [ Info: Executing initial time step...; ERROR: LoadError: MethodError: no method matching _fill_south_halo!(::Int64, ::Int64, ::Oceananigans.Grids.ZRegOrthogonalSphericalShellGrid{Float64, FullyConnected, FullyConnected, Bounded, OffsetArrays.OffsetMatrix{Float64, Matrix{Float64}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, Float64, NamedTuple{(:ξ, :η, :rotation), Tuple{Tuple{Float64, Float64}, Tuple{Float64, Float64}, Rotations.RotXY{Float64}}}, CPU}, ::OffsetArrays.OffsetArray{Float64, 3, Array{Float64, 3}}, ::BoundaryCondition{Oceananigans.BoundaryConditions.MultiRegionCommunication, Oceananigans.MultiRegion.CubedSphereRegionalConnectivity{Oceananigans.MultiRegion.South, Oceananigans.MultiRegion.North, Nothing}}, ::Tuple{Center, Center, Face}). Closest candidates are:; _fill_south_halo!(::Any, ::Any, ::Any, ::Any, ::BoundaryCondition{<:Oceananigans.BoundaryConditions.Open}, ::Any, ::Any...); @ Oceananigans /Users/Sid/Library/CloudStorage/Dropbox/StudyFolder/PostDocMITDesktop/Codes/Oceananigans/cubed-sphere-steady-state/src/BoundaryConditions/fill_halo_regions_open.jl:36; _fill_south_halo!(::Any, ::Any, ::Any, ::Any, ::BoundaryCondition{<:Oceananigans.BoundaryConditions.Flux}, ::Any...); @ Oceananigans /Users/Sid/Library/CloudStorage/Dropbox/StudyFolder/PostDocMITDesktop/Codes/Oceananigans/cubed-sphere-steady-state/src/BoundaryConditions/fill_halo_regions_flux.jl:32; _fill_south_halo!(::Any, ::Any, ::Any, ::Any, ::Union{BoundaryCondition{<:Oceananigans.BoundaryConditions.Value}, BoundaryCondition{<:Oceananigans.BoundaryCondition",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3302#issuecomment-1742355619:409,Load,LoadError,409,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3302#issuecomment-1742355619,1,['Load'],['LoadError']
Performance,Fix bug loading `netcdf`,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2847:8,load,loading,8,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2847,1,['load'],['loading']
Performance,Fix performance benchmarks dead link in README,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/822:4,perform,performance,4,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/822,1,['perform'],['performance']
Performance,Fix race condition affecting `HydrostaticFreeSurfaceModel` with `isnothing(free_surface)`,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2821:4,race condition,race condition,4,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2821,1,['race condition'],['race condition']
Performance,Fixes GPU to CPU loading and writing fields with function boundary conditions,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/797:17,load,loading,17,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/797,1,['load'],['loading']
Performance,Fixes checkpointer GPU to CPU loading and writing fields with function boundary conditions,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/797:30,load,loading,30,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/797,1,['load'],['loading']
Performance,"Float32, 3, 1}}}}, Nothing, NamedTuple{(:u, :v, :w, :T, :S), NTuple{5, typeof(Oceananigans.Forcings.zeroforcing)}}, NamedTuple{(:time, :iteration, :stage), Tuple{Float32, Int64, Int64}}}}}, args::LLVM.Module); @ GPUCompiler /g/data/v45/nc3020/.julia/packages/GPUCompiler/2WWTr/src/validation.jl:111; [2] macro expansion; @ /g/data/v45/nc3020/.julia/packages/GPUCompiler/2WWTr/src/driver.jl:319 [inlined]; [3] macro expansion; @ /g/data/v45/nc3020/.julia/packages/TimerOutputs/PZq45/src/TimerOutput.jl:226 [inlined]; [4] macro expansion; @ /g/data/v45/nc3020/.julia/packages/GPUCompiler/2WWTr/src/driver.jl:317 [inlined]; [5] emit_asm(job::GPUCompiler.CompilerJob, ir::LLVM.Module; strip::Bool, validate::Bool, format::LLVM.API.LLVMCodeGenFileType); @ GPUCompiler /g/data/v45/nc3020/.julia/packages/GPUCompiler/2WWTr/src/utils.jl:62; [6] cufunction_compile(job::GPUCompiler.CompilerJob); @ CUDA /g/data/v45/nc3020/.julia/packages/CUDA/mVgLI/src/compiler/execution.jl:313; [7] cached_compilation(cache::Dict{UInt64, Any}, job::GPUCompiler.CompilerJob, compiler::typeof(CUDA.cufunction_compile), linker::typeof(CUDA.cufunction_link)); @ GPUCompiler /g/data/v45/nc3020/.julia/packages/GPUCompiler/2WWTr/src/cache.jl:89; [8] cufunction(f::typeof(Cassette.overdub), tt::Type{Tuple{Cassette.Context{nametype(CUDACtx), KernelAbstractions.CompilerMetadata{KernelAbstractions.NDIteration.StaticSize{(32, 32, 32)}, KernelAbstractions.NDIteration.DynamicCheck, Nothing, Nothing, KernelAbstractions.NDIteration.NDRange{3, KernelAbstractions.NDIteration.StaticSize{(2, 2, 32)}, KernelAbstractions.NDIteration.StaticSize{(16, 16, 1)}, Nothing, Nothing}}, Nothing, KernelAbstractions.var""##PassType#257"", Nothing, Cassette.DisableHooks}, typeof(Oceananigans.Models.IncompressibleModels.gpu_calculate_Gw!), OffsetArrays.OffsetArray{Float32, 3, CUDA.CuDeviceArray{Float32, 3, 1}}, RegularRectilinearGrid{Float32, Periodic, Periodic, Bounded, OffsetArrays.OffsetVector{Float32, StepRangeLen{Float32, Float64, Float64}}}",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1780#issuecomment-870162360:17236,cache,cache,17236,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1780#issuecomment-870162360,1,['cache'],['cache']
Performance,"For better or for worse, Oceananigans currently does not store intermediate terms in the computation of a PDE's right hand side (with notable exceptions hydrostatic pressure and eddy diffusivities). In other words, a single, sometimes large kernel that evaluates the right hand side at each grid point `i, j, k` is compiled for each PDE in a user's model, and there's no way to pull out intermediate steps in that calculation to use elsewhere. It's important to note when considering optimization strategies that our computations are probably memory-limited, rather than compute-limited. In other words, we think the process of transferring data from global memory to local thread memory is a bottleneck for our computations (we can really only know this through profiling a particular application, however, since all models are different...) Storing intermediate components of the tendency terms would probably create more memory accesses overall (since rather than immediately using intermediate results for subsequent calculations, we would have to send them to global memory, and then back, to complete the evaluation of a tendency) --- and thus could slow down tendency evaluations that are performed 1-3 times per time-step. For example, our best idea for speeding up tendency evaluations is to better manage memory movement using GPU shared memory (unfortunately, we haven't had the time to explore such optimization strategies...). I think there may be other ways to optimize diagnostics calculations, however. # Fusing `ComputedField` kernels. One possibility to speed up diagnostics is to ""fuse"" kernels for different `ComputedField` diagnostics. The kernel for a `ComputedField` is. https://github.com/CliMA/Oceananigans.jl/blob/9b52f3f911d26a66c75f1c3cb58fdd0a1cecb131/src/Fields/computed_field.jl#L112-L115. where `operand` is an `AbstractOperation`. But different `ComputedField`s may somehow depend on the same underlying data in memory. Thus if the kernels for differnet `ComputedField",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1483#issuecomment-800567837:484,optimiz,optimization,484,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1483#issuecomment-800567837,2,"['bottleneck', 'optimiz']","['bottleneck', 'optimization']"
Performance,"For cases in which we need to utilize the original set-up script, would a function that looks something like . ```julia; set_to_checkpoint!(model, ""checkpoint_filename"", iteration); ```. do the trick? If the set-up script is provided, running from a checkpoint should be easy because we only need to load the solution and tendencies onto the CPU and then copy to the already-initialized GPU field, and set `model.clock.iteration` and `model.clock.time`. The more difficult code-design case is initializing a model when the original set-up script is not available. The above line can be placed just prior to time-stepping, and commented out when users want to run from the initial condition. So only one line needs to be changed provided that time-stepping is enclosed within a `while model.clock.time < end_time` or `while model.clock.iteration < end_iteration` block. . I agree with @ali-ramadhan that a `Simulation` type would make the scripting flow a bit more readable and robust.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/602#issuecomment-580286724:300,load,load,300,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/602#issuecomment-580286724,1,['load'],['load']
Performance,"For complicated models and examples, user-defined forcing functions can impose significant penalties on simulation performance. In addition to that, models with extensive and complicated diagnostics (especially those involving time-averaging) can further slow down time-to-science. It thus might be useful to provide some utilities that make benchmarking forcing functions, boundary condition functions, and diagnostics a bit easier. For forcing functions, I think a utility that benchmarks time-stepping for two models that are identical except for forcing might be useful. Something along the lines of. ```julia; function benchmark_user_forcing(model); ; # Build a ""forcingless_model"" that's identical to model, but with no forcing functions; model_property_names = propertynames(model); forcingless_model_properties = Dict{Any, Any}(name => getproperty(model, name) for name in model_property_names); forcingless_model_properties[:forcing] = NamedTuple{}() # default; ModelConstructor = typeof(model).name.wrapper # or whatever this needs to be; forcingless_model = ModelConstructor(Tuple(forcingless_model_properties[name] for name in model_property_names)...). @info ""Benchmarking model with user forcing...""; @btime time_step!(model, 1). @info ""Benchmarking model without user forcing...""; @btime time_step!(forcingless_model, 1). return nothing; end; ```. might work for forcing functions. For boundary conditions, we have to build default boundary conditions manually and use the model's outer constructor (to avoid excess memory allocation), which is slightly more annoying, but should be supported by at least the nonhydrostatic and hydrostatic model constructors. For output I think we want to benchmark `fetch_output`:. https://github.com/CliMA/Oceananigans.jl/blob/051e03ecfcb0c00e0c6ed4dd2808148e700d0342/src/OutputWriters/fetch_output.jl#L17-L18. Worth noting that fetching is actually already timed by `JLD2OutputWriter` with `verbose=true`:. https://github.com/CliMA/Oceananigans.jl/b",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1918:115,perform,performance,115,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1918,1,['perform'],['performance']
Performance,"For example, when running on MIT's Satori cluster one must use a magic incantation to obtain useful output (and also to run 4 simulations on a single node):. ```bash; #!/bin/bash. #SBATCH --job-name=eady; #SBATCH --output=slurm-eady-%j.out; #SBATCH --error=slurm-eady-%j.err; #SBATCH --time=12:00:00; #SBARCH --mem=0; #SBATCH --nodes=1; #SBATCH --ntasks-per-node=4; #SBATCH --gres=""gpu:4"" # GPUs per Node; #SBATCH --cpus-per-task=4. # Clear the environment from any previously loaded modules; module purge > /dev/null 2>&1. module load spack/0.1; module load gcc/8.3.0 # to get libquadmath; module load julia/1.4.1; module load cuda/10.1.243; module load openmpi/3.1.4-pmi-cuda; module load py-matplotlib/3.1.1. DIR=""$( cd ""$( dirname ""${BASH_SOURCE[0]}"" )"" >/dev/null 2>&1 && pwd )"". cd $DIR/../Oceananigans/. CUDA_VISIBLE_DEVICES=0 unbuffer julia --project run_small_eady_problem.jl --Nh 128 --Nz 96 --geostrophic-shear 0.25 --years 2.0 2>&1 | tee quarter_shear.out &; CUDA_VISIBLE_DEVICES=1 unbuffer julia --project run_small_eady_problem.jl --Nh 128 --Nz 96 --geostrophic-shear 0.1 --years 2.0 2>&1 | tee tenth_shear.out &; CUDA_VISIBLE_DEVICES=2 unbuffer julia --project run_small_eady_problem.jl --Nh 192 --Nz 96 --geostrophic-shear 0.25 --years 2.0 2>&1 | tee quarter_shear_hires.out &; CUDA_VISIBLE_DEVICES=3 unbuffer julia --project run_small_eady_problem.jl --Nh 192 --Nz 96 --geostrophic-shear 0.1 --years 2.0 2>&1 | tee tenth_shear_hires.out. sleep 42480 # sleep for 11.8 hours; ```. (and explanations for each part of the script might be helpful)",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1045:477,load,loaded,477,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1045,7,['load'],"['load', 'loaded']"
Performance,"For example,. ```julia; using Oceananigans. grid = RectilinearGrid(size = (16, 1, 16),; x = (0, 1),; y = (0, 1),; z = (0, 1),; topology = (Periodic, Periodic, Bounded)). model = NonhydrostaticModel(; grid, advection = WENO(order=5)); ```. errors with. ```julia; julia> include(""problem.jl""); ┌ Warning: Inflating model grid halo size to (3, 3, 3) and recreating grid. Note that an ImmersedBoundaryGrid requires an extra halo point in all non-flat directions compared to a non-immersed boundary grid.; └ @ Oceananigans.Models.NonhydrostaticModels ~/.julia/packages/Oceananigans/OHYQj/src/Models/NonhydrostaticModels/nonhydrostatic_model.jl:223; ERROR: LoadError: ArgumentError: halo must be ≤ size for coordinate y; ```. Sad, because I didn't actually ask for a halo (3, 3, 3)! Manually setting things doesn't help:. ```julia; using Oceananigans. grid = RectilinearGrid(size = (16, 1, 16),; halo = (3, 1, 3),; x = (0, 1),; y = (0, 1),; z = (0, 1),; topology = (Periodic, Periodic, Bounded)). model = NonhydrostaticModel(; grid, advection = WENO(order=5)); ```. I guess the restriction on the halo size is supposed to be an optimization, but its not working in this case because I can't even set up my model. @navidcy",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3622:651,Load,LoadError,651,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3622,2,"['Load', 'optimiz']","['LoadError', 'optimization']"
Performance,"For example:. ```julia; julia> using Oceananigans. julia> grid = RectilinearGrid(size=(1, 1, 1), extent=(1, 1, 1)); 1×1×1 RectilinearGrid{Float64, Periodic, Periodic, Bounded} on CPU with 1×1×1 halo; ├── Periodic x ∈ [0.0, 1.0) regularly spaced with Δx=1.0; ├── Periodic y ∈ [0.0, 1.0) regularly spaced with Δy=1.0; └── Bounded z ∈ [-1.0, 0.0] regularly spaced with Δz=1.0. julia> b = CenterField(grid); 1×1×1 Field{Center, Center, Center} on RectilinearGrid on CPU; ├── grid: 1×1×1 RectilinearGrid{Float64, Periodic, Periodic, Bounded} on CPU with 1×1×1 halo; ├── boundary conditions: FieldBoundaryConditions; │ └── west: Periodic, east: Periodic, south: Periodic, north: Periodic, bottom: ZeroFlux, top: ZeroFlux, immersed: ZeroFlux; └── data: 3×3×3 OffsetArray(::Array{Float64, 3}, 0:2, 0:2, 0:2) with eltype Float64 with indices 0:2×0:2×0:2; └── max=0.0, min=0.0, mean=0.0. julia> f = 1; 1. julia> vz_op = @at (Face, Center, Center) - ∂x(b) / f; ERROR: LoadError: MethodError: no method matching var""@at""(::LineNumberNode, ::Module, ::Expr); Closest candidates are:; var""@at""(::LineNumberNode, ::Module, ::Any, ::Any) at /Users/gregorywagner/Projects/Oceananigans.jl/src/AbstractOperations/at.jl:42; in expression starting at REPL[16]:1; ```. Found with @iuryt",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2415:957,Load,LoadError,957,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2415,1,['Load'],['LoadError']
Performance,"For now this PR just adds a function `convective_adjustment!(model, Δt, K)` that performs a convective adjustment step on a model. . I believe this results in an operator splitting method for treating vertically implicit diffusion using backward Euler. ## TODO. If this seems like an appropriate method for implementing vertically implicit diffusion, I'd suggest the following steps for turning this PR into something that can be merged:; 1. Define a new closure; ```julia; 	struct ConvectiveAdjustment{K, ∂}; 	 κ :: K; 	∂b∂z :: ∂; 		...; 	end; ```; 2. Maybe `ConvectiveAdjustment` should act on a `BuoyancyField`?; 3. Refactor `convective_adjustment!` to use the `BatchedTridiagonalSolver`.; 4. Add a free convection test to test that using `ConvectiveAdjustment` on a linearly stratified column model results in a mixed layer with ∂b/∂z ≈ 0 (could also test for the mixed layer depth). `ConvectiveAdjustment` could then be used as part of a tuple of turbulence closures, e.g. ```julia; closure = (IsotropicDiffusivity(κ=1e-4), ConvectiveAdjustment(κv=10)); ```. ## Future plans?. Vertically implicit diffusion with the `BatchedTridiagonalSolver` could then be abstracted to support other parameterizations such as `OceanTurb.KPP` and `OceanTurb.TKEMassFlux`. I think @glwagner envisioned a more general way of time-stepping implicit terms in general, i.e. adding IMEX time-steppers I think?. ## Note on user interface. Right now the user must manually call `convective_adjustment!` inside the `simulation.progress` callback so it's very awkward to use, but it should be usable if we need it. Ideally `convective_adjustment!` would be called at the end of each time step, perhaps by `time_step!` or by a simulation callback. The second approach would require resolving #1138.",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1342:81,perform,performs,81,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1342,1,['perform'],['performs']
Performance,"For run, I just ran `barotropic_gyre.jl` found [here](https://github.com/CliMA/Oceananigans.jl/blob/main/validation/barotropic_gyre/barotropic_gyre.jl). The simulation part ran nicely but when it came to visualization I got the following error. I guess the plotting needs to be updated?. ```; ERROR: LoadError: LoadError: setting show_axis for scene via plot attribute not supported anymore; Stacktrace:; [1] error(s::String); @ Base ./error.jl:33; [2] plot!(scene::Scene, P::Type{Combined{Makie.wireframe, Tuple{Sphere{Float32}}}}, attributes::Attributes, input::Tuple{Observable{Sphere{Float32}}}, args::Observable{Tuple{Sphere{Float32}}}); @ Makie ~/.julia/packages/Makie/umL6V/src/interfaces.jl:399; [3] plot!(scene::Scene, P::Type{Combined{Makie.wireframe, ArgType} where ArgType}, attributes::Attributes, args::Sphere{Float32}; kw_attributes::Base.Iterators.Pairs{Union{}, Union{}, Tuple{}, NamedTuple{(), Tuple{}}}); @ Makie ~/.julia/packages/Makie/umL6V/src/interfaces.jl:320; [4] plot!; @ ~/.julia/packages/Makie/umL6V/src/interfaces.jl:288 [inlined]; [5] plot!(lscene::LScene, P::Type{Combined{Makie.wireframe, ArgType} where ArgType}, attributes::Attributes, args::Sphere{Float32}; kw_attributes::Base.Iterators.Pairs{Union{}, Union{}, Tuple{}, NamedTuple{(), Tuple{}}}); @ Makie.MakieLayout ~/.julia/packages/Makie/umL6V/src/makielayout/blocks/scene.jl:6; [6] plot!; @ ~/.julia/packages/Makie/umL6V/src/makielayout/blocks/scene.jl:6 [inlined]; [7] plot!(P::Type{Combined{Makie.wireframe, ArgType} where ArgType}, ls::LScene, args::Sphere{Float32}; kw_attributes::Base.Iterators.Pairs{Symbol, Bool, Tuple{Symbol}, NamedTuple{(:show_axis,), Tuple{Bool}}}); @ Makie.MakieLayout ~/.julia/packages/Makie/umL6V/src/makielayout/blocks/scene.jl:14; [8] #wireframe!#600; @ ~/.julia/packages/MakieCore/aD9Dy/src/recipes.jl:37 [inlined]; [9] visualize_barotropic_gyre(filepath::String); @ Main ~/Software/Oceananigans.jl/validation/barotropic_gyre/visualize_barotropic_gyre.jl:64; [10] top-level scop",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2542:300,Load,LoadError,300,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2542,2,['Load'],['LoadError']
Performance,"For the last few days I've been getting weird errors running hydrostatic-free surface models with immersed boundaries on GPUs. First I was getting some ""illegal instruction"" core dump errors from CUDA but upgrading various things and playing around with the CUDA configurations fixed that (and I don't think this is an Oceananigans issue), but now with `v0.85.0` I get this error:; ```; ERROR: LoadError: DivideError: integer division error; Stacktrace:; [1] macro expansion; @ ~/.julia/packages/CUDA/pCcGc/lib/cublas/libcublas.jl:102 [inlined]; [2] #21; @ ~/.julia/packages/CUDA/pCcGc/lib/utils/call.jl:27 [inlined]; [3] #1; @ ~/.julia/packages/CUDA/pCcGc/lib/cublas/libcublas.jl:17 [inlined]; [4] retry_reclaim(f::CUDA.CUBLAS.var""#1#2""{CUDA.CUBLAS.var""#21#22""{Ptr{CUDA.CUBLAS.cublasContext}, Int64, CUDA.CuArray{Float64, 1, CUDA.Mem.DeviceBuffer}, Int64, Base.R$; @ CUDA ~/.julia/packages/CUDA/pCcGc/src/pool.jl:337; [5] check; @ ~/.julia/packages/CUDA/pCcGc/lib/cublas/libcublas.jl:16 [inlined]; [6] cublasDnrm2_v2; @ ~/.julia/packages/CUDA/pCcGc/lib/utils/call.jl:26 [inlined]; [7] nrm2; @ ~/.julia/packages/CUDA/pCcGc/lib/cublas/wrappers.jl:172 [inlined]; [8] nrm2; @ ~/.julia/packages/CUDA/pCcGc/lib/cublas/wrappers.jl:177 [inlined]; [9] norm; @ ~/.julia/packages/CUDA/pCcGc/lib/cublas/linalg.jl:131 [inlined]; [10] norm; @ ~/.julia/packages/CUDA/pCcGc/lib/cublas/linalg.jl:130 [inlined]; [11] cg_iterator!(x::CUDA.CuArray{Float64, 1, CUDA.Mem.DeviceBuffer}, A::CUDA.CUSPARSE.CuSparseMatrixCSC{Float64, Int32}, b::CUDA.CuArray{Float64, 1, CUDA.Mem.DeviceBuff$; @ IterativeSolvers ~/.julia/packages/IterativeSolvers/rhYBz/src/cg.jl:140; [12] cg_iterator!; @ ~/.julia/packages/IterativeSolvers/rhYBz/src/cg.jl:120 [inlined]; [13] cg!(x::CUDA.CuArray{Float64, 1, CUDA.Mem.DeviceBuffer}, A::CUDA.CUSPARSE.CuSparseMatrixCSC{Float64, Int32}, b::CUDA.CuArray{Float64, 1, CUDA.Mem.DeviceBuffer}; abst$; @ IterativeSolvers ~/.julia/packages/IterativeSolvers/rhYBz/src/cg.jl:224; [14] cg!; @ ~/.julia/pack",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3189:394,Load,LoadError,394,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3189,1,['Load'],['LoadError']
Performance,From Googling around I believe FFTW.jl is build in serial mode? I couldn't see how to make use of multi-threading or specify the number of threads. A related performance optimization would be to consider using Intel's MKL which is usually faster on Intel machines (and might come with multi-threading out of the box). But since we're not running large models on single-core CPUs this seems like a low priority consideration for now.,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/119#issuecomment-471176913:98,multi-thread,multi-threading,98,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/119#issuecomment-471176913,4,"['multi-thread', 'optimiz', 'perform']","['multi-threading', 'optimization', 'performance']"
Performance,"From a correctness and functionality stand point this PR should be ready to merge. I have some problems with performance though. It looks like with `--check-bounds=no` this PR has roughly the same wall time than main (slightly more if you have weno because of additional boundary weno schemes) but without it's 2 - 2.5X slower. (Exacerbated on the GPU). I guess it might be a matter of `@inbounds` having to be placed strategically, but I seem to miss it; @glwagner, @navidcy do you have any easy suggestion that I might have missed?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2603#issuecomment-1177948330:109,perform,performance,109,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2603#issuecomment-1177948330,1,['perform'],['performance']
Performance,"From the discussion #3587 @scott-conn found out that our definition of immersed derivatives ~~does not work very well~~ is **wrong** when using reduced fields. Consider, for example, a field reduced in z. If we try performing a derivative in the x-direction it will check the immersed condition ; https://github.com/CliMA/Oceananigans.jl/blob/7a4b3f04e402be70d45fcb775a4dedef087f3bb0/src/ImmersedBoundaries/conditional_differences.jl#L21; on `k == 1`. This might be ~~very~~ wrong if the field is a sum (or a mean) over the column. ; For example, for integrals we want to check if the whole column is immersed.; If we are dealing with a `GridFittedBottom`, instead, we just need to check the upmost grid cell (`k == Nz`). We could procede in a couple of ways from here:; - use simple derivatives for `AsbtractOperations` that do not account for immersed boundaries. ; - We can augment operations on Immersed `Reduced` fields by attaching to them a `condition` like in conditional operations",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3588:215,perform,performing,215,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3588,1,['perform'],['performing']
Performance,"From the standpoint of `NonhydrostaticModel`, the only requirement (I think?) is a pressure solver that's valid on more grids. The `FourierTridiagonalPoissonSolver` combines FFTs with a tridiagonal solve in one direction to solve the Poisson equation. Thus, this method can be used if the grid is stretched in just one (1) direction. Right now, we only support grids that are stretched in `z`, the third direction / index. Presumably it's at most a matter of copy and paste and index permutation to support grids that are stretched in `x` (and regular in `y, z`) or stretched in `y` (and regular in `x, z`). Under the hood, the `FourierTridiagonalPoissonSolver` relies on the `BatchedTridiagonalSolver` to perform the tridiagonal solve:. https://github.com/CliMA/Oceananigans.jl/blob/4551a78b1f3fe4bb3b238676c128dc751be9b934/src/Solvers/fourier_tridiagonal_poisson_solver.jl#L81-L82. The `BatchedTridiagonalSolver` launches a kernel over `xy` and performs a tridiagonal solve in `z`:. https://github.com/CliMA/Oceananigans.jl/blob/4551a78b1f3fe4bb3b238676c128dc751be9b934/src/Solvers/batched_tridiagonal_solver.jl#L79-L80. So the first task is to extend the tridiagonal solver to support integrals in `x` and `y`. . This could be straightforwardly supported by adding some kind of tag to indicate the ""tridiagonal direction"" (ie `:x`, `:y`, or `:z`), and copy-pasting the functionality for each case. It's a bit of code duplication but pretty straightforward... Alternatively we could introduce some kind of abstraction that permutes array dimensions. Then we just have one algorithm which assumes the tridiagonal index is `k`, and support `i` or `j` under the hood via an array wrapper that performs an index permutation. I'm leaning towards copy/paste because it's a little easier to understand and it's not that much code in this case...",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3116#issuecomment-1557689620:706,perform,perform,706,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3116#issuecomment-1557689620,3,['perform'],"['perform', 'performs']"
Performance,"From the talks we've had here and in [this discussion](https://github.com/CliMA/Oceananigans.jl/discussions/1482), as well me looking things I have some things we could include as tips. Please let me know if I'm missing something. ## General simulation tips; - In general defining variables (that are used in the calculations) as constants makes things faster as it helps the compiler optimize things; - It's probably worth inlining small functions that get called often to reduce function call overhead (at the very least it's worth playing with this). ## GPU simulation tips:; - Any global variable that needs to be accessed by the GPU needs to be a constant or the simulation will crash; - Complex `ComputedField`s may not work, so the user can either nest `ComputedField`s (simple, but costly; probably good for development) or use `KernelComputedField`s (complex but efficient; probably what you wanna use for production-ready code); - GPU runs are generally memory-limited, so it's good to both keep track of and try to reduce the size of your runs. Useful tips in this regard are; - Try to use higher-order schemes as you need fewer grid points to achieve the same resolution; - Use `nvidia-smi` to monitor the memory usage of the GPU; - Manually define scratch space to be reused in diagnostics, to avoid creating one scratch space for each separate diagnostic you have.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1478#issuecomment-802225575:385,optimiz,optimize,385,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1478#issuecomment-802225575,1,['optimiz'],['optimize']
Performance,Fusing kernels for calculating diagnostics to improve performance,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1483:54,perform,performance,54,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1483,1,['perform'],['performance']
Performance,"Given that there seems to be increasing interest in vertically-periodic simulations, we could revive #3080 (despite the caveats mentioned there, which mainly unknowns associated with the performance of potential future nonhydrostatic solvers for complex domains), since it's always possible to reverse course in the future and restore the separation (perhaps when the separation is restored, it can be done in a way that's compatible with vertically periodic domains).",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3364#issuecomment-1782219409:187,perform,performance,187,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3364#issuecomment-1782219409,1,['perform'],['performance']
Performance,Glw/performance,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2033:4,perform,performance,4,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2033,1,['perform'],['performance']
Performance,Going down to 128³ it still produced NaNs after ~10 iterations. Going down to 32³ it always seems to make it to 100 iterations. So race condition seems to make sense as they occur when the GPU is saturated I think. @glwagner Ah good point. I'll try again with just the `FluxBoundaryCondition` and then with just the `GradientBoundaryCondition`.,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/816#issuecomment-662727050:131,race condition,race condition,131,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/816#issuecomment-662727050,1,['race condition'],['race condition']
Performance,"Good idea, let's change the phrase but keep this issue open so that I remember to tackle the performance benchmarks",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3758#issuecomment-2327680150:93,perform,performance,93,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3758#issuecomment-2327680150,1,['perform'],['performance']
Performance,Good news is that using a `StructArray{Particle}` doesn't impact performance :rocket:,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1091#issuecomment-733713396:65,perform,performance,65,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1091#issuecomment-733713396,1,['perform'],['performance']
Performance,"Good news: shallow water tests pass!. Bad news: the halo tests now has 12 fails, of 600 in total. . ```; <div class=""JobLogOutputComponent"" style=""box-sizing: border-box; background: rgb(23, 23, 23); border-radius: 3px; min-height: 85px; color: white; font-size: 12px; padding: 0px 0px 15px; width: 1108px; font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Roboto, &quot;Helvetica Neue&quot;, Helvetica, sans-serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;""><div class=""JobLogOutputComponent__Body"" style=""box-sizing: border-box; font-family: SFMono-Regular, Monaco, Menlo, Consolas, &quot;Liberation Mono&quot;, Courier, monospace;"">; Oceananigans \| 818 12 830; --; &nbsp; | Distributed MPI Oceananigans \| 816 12 828; &nbsp; | Multi architectures rank connectivity \| 28 28; &nbsp; | Local grids for distributed models \| 24 24; &nbsp; | Injection of halo communication BCs \| 168 168; &nbsp; | Halo communication \| 588 12 600; &nbsp; | Time stepping IncompressibleModel \| 4 4; &nbsp; | Time stepping ShallowWaterModel \| 4 4; &nbsp; | Distributed FFT-based Poisson solver \| 2 2; &nbsp; | ERROR: ERROR: ERROR: ERROR: LoadError: LoadError: LoadError: LoadError: Some tests did not pass: 818 passed, 0 failed, 12 errored, 0 broken.; &nbsp; | in expression starting at /storage7/buildkite-agent/builds/tartarus-mit-edu-6/clima/oceananigans/test/runtests.jl:80; &nbsp; | Some tests did not pass: 818 passed, 0 failed, 12 errored, 0 broken.; &nbsp; | in expression starting at /storage7/buildkite-agent/builds/tartarus-mit-edu-6/clima/oceananigans/test/runtests.jl:80; &nbsp; | Some tests did not pass: 818 passed, 0 failed, 12 errored, 0 broken.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1662#issuecomment-843285081:1477,Load,LoadError,1477,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1662#issuecomment-843285081,4,['Load'],['LoadError']
Performance,"Good question @simone-silvestri !. On main it doesn't run as is. There are two issues. . First, there is `device!(2)` in line 30 but I commented that out easily. . Second, on line 159 we load `VorticityStencil`, which is not defined. I deleted that and changed the momentum_advection to `VectorInvariant()`, but it occurs to me that this is not what was done before. Before it was using a vorticity stencil. How would I change to use the other formulation?. However, when I try running this on main it goes far beyond what I see in the PR. So I guess the version on main does run and is stable. Pasat a day easy. Hmm... If we have a problem with the height going negative, I wonder if this is a sign that the conservation of mass equation is not quite right? I will look at that today.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3486#issuecomment-1983382184:187,load,load,187,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3486#issuecomment-1983382184,1,['load'],['load']
Performance,"Got bitten by CuArray scalar operations while debugging some Europa runs so this PR adds an `__init__()` function to the Oceananigans module that disables CuArray scalar operations once the module has been loaded and only allows scalar ops via the `CUDA.@allowscalar` macro. Now when we accidentally perform scalar ops on CuArrays we should get an error, which is much better than having silent performance regressions which has happened multiple times in the past. Two remaining problems:; 1. `set!(u::Field, v::Number) = @. u.data = v` actually incurs scalar ops because `a::CuArray .= 1.5` does not but `a::OffsetArray{CuArray .= 1.5` does, so it's probably something we have to fix by adapting broadcasting over offsetarrays?; 2. For similar broadcasting reasons I think; ```julia; data = cpudata(output); ds[name][:, :, :, time_index] = view(data, ow.slices[name]...); ```; also incurs scalar ops during NetCDF output so I will try changing back to the old behavior where we used something like `	 ; ```julia; data = interiorparent(output); !isa(output, Array) && (data = Array(data)); ds[name][:, :, :, time_index] = view(data, ow.slices[name]...); ```. Resolves #276",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/851:206,load,loaded,206,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/851,3,"['load', 'perform']","['loaded', 'perform', 'performance']"
Performance,"Greg, Ali, and what if we began to consider more complex geometries etc -; hard to resist going in that direction..... we'd have to change the solvers; - more MITgcm-like, congrad etc - but what would the implications be for; boundary conditions? More food for thought. John. On Sat, Mar 28, 2020 at 10:22 AM Gregory L. Wagner <notifications@github.com>; wrote:. > Mostly I am worried about scalability and sustainability in this design,; > or future designs.; >; > Currently our models are fairly simple, but its challenging to place; > bounds on potential future complexity. For example, models in the future; > may require additional fields associated with closures or; > parameterizations, such as (two-dimensional) boundary layer depth fields,; > plume quantities, mixing lengths and perhaps other auxiliary fields; > associated with various prognostic / diagnostic LES models. We probably; > can't plan to support setting boundary conditions on every possible field; > via the model constructor.; >; > With our current design we have essentially special-cased turbulent; > diffusivities because our focus is LES, turbulent diffusivities are; > relatively common, and it convenient for us. However doing this incurs some; > maintenance burden --- which will increase if we plan to hard-code; > validation and error checking.; >; > Food for thought.; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/climate-machine/Oceananigans.jl/issues/721#issuecomment-605453798>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AKXUEQT642WME2EIX3DDSITRJYBYXANCNFSM4LVSZPAA>; > .; >. -- ; ==========================================; John Marshall; Cecil and Ida Green Professor of Oceanography; Massachusetts Institute of Technology; http://oceans.mit.edu/JohnMarshall/; ==========================================",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/721#issuecomment-605455060:391,scalab,scalability,391,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/721#issuecomment-605455060,1,['scalab'],['scalability']
Performance,"Grids.Cell,Nothing,0,typeof(Oceananigans.Forcings.zeroforcing),Tuple{}},Oceananigans.Forcings.ContinuousForcing{Oceananigans.Grids.Cell,Oceananigans.Grids.Face,Oceananigans.Grids.Cell,Nothing,0,typeof(Oceananigans.Forcings.zeroforcing),Tuple{}},Oceananigans.Forcings.ContinuousForcing{Oceananigans.Grids.Cell,Oceananigans.Grids.Cell,Oceananigans.Grids.Face,Nothing,0,typeof(Oceananigans.Forcings.zeroforcing),Tuple{}},Oceananigans.Forcings.ContinuousForcing{Oceananigans.Grids.Cell,Oceananigans.Grids.Cell,Oceananigans.Grids.Cell,NamedTuple{(:K, :ℓ, :Δz),Tuple{Float64,Float64,Float64}},1,typeof(FT),Tuple{typeof(identity)}}}},OffsetArrays.OffsetArray{Float64,3,CUDA.CuDeviceArray{Float64,3,CUDA.AS.Global}},NamedTuple{(:time, :iteration, :stage),Tuple{Float64,Int64,Int64}}}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:maxthreads,),Tuple{Int64}}}) at /home/ptuckman/.julia/packages/CUDA/dZvbp/src/compiler/execution.jl:310; [8] #87 at /home/ptuckman/.julia/packages/GPUCompiler/4e9CU/src/cache.jl:21 [inlined]; [9] get!(::GPUCompiler.var""#87#88""{Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:maxthreads,),Tuple{Int64}}},typeof(CUDA._cufunction),GPUCompiler.FunctionSpec{typeof(Cassette.overdub),Tuple{Cassette.Context{nametype(CUDACtx),KernelAbstractions.CompilerMetadata{KernelAbstractions.NDIteration.StaticSize{(128, 128, 128)},KernelAbstractions.NDIteration.DynamicCheck,Nothing,Nothing,KernelAbstractions.NDIteration.NDRange{3,KernelAbstractions.NDIteration.StaticSize{(8, 8, 128)},KernelAbstractions.NDIteration.StaticSize{(16, 16, 1)},Nothing,Nothing}},Nothing,KernelAbstractions.var""##PassType#253"",Nothing,Cassette.DisableHooks},typeof(Oceananigans.TimeSteppers.gpu_calculate_Gu!),OffsetArrays.OffsetArray{Float64,3,CUDA.CuDeviceArray{Float64,3,CUDA.AS.Global}},RegularCartesianGrid{Float64,Periodic,Periodic,Bounded,OffsetArrays.OffsetArray{Float64,1,StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}}},Oceananigans.Ad",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1010:7642,cache,cache,7642,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1010,1,['cache'],['cache']
Performance,Haha so the link is dead again since we don't have a `benchmarks.jl` file. It should link to the documentation page on performance benchmarks. I will open a PR.,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/608#issuecomment-663171078:119,perform,performance,119,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/608#issuecomment-663171078,1,['perform'],['performance']
Performance,Halo optimization and solving the race condition on corner nodes,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2035:5,optimiz,optimization,5,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2035,2,"['optimiz', 'race condition']","['optimization', 'race condition']"
Performance,"Happy to open an issue (or post to #1634) when I have the time to work on CI. What should we do with this PR? I think it's a net positive change so I'm happy to approve as long as the images aren't in git history. If we decide to nuke this part of the docs then I suppose this PR is moot and should be closed. > _when_ / _if_ somebody can take responsibility for maintaining it. I think responsibility for maintaining the pipeline should fall on all maintainers/developers, otherwise it's not sustainable. Ideally if you open a PR that breaks a validation experiment you should fix it. If done concurrently it should only consist of small changes so it should only be a small burden (although burdens to add up). I guess we don't run the validation CI on every PR since it's too expensive so maintenance is tough right now. Maybe we can run validation CI before every tagged release or something? Better infrastructure is needed I suppose.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1797#issuecomment-872481889:594,concurren,concurrently,594,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1797#issuecomment-872481889,1,['concurren'],['concurrently']
Performance,"Hello, @amontoison. Nice work. Do you see a performance improvement when switching to this package?; There should be some benchmarks in the `benchmark` folder that we can test (probably we need to update that folder a bit, let me know if you have problems with it)",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3778#issuecomment-2353126955:44,perform,performance,44,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3778#issuecomment-2353126955,1,['perform'],['performance']
Performance,"Hello, I am getting this error while trying to add Oceananigans on the NASA Pleiade cluster. Any ideas? Thanks a lot ! Lia. (@v1.6) pkg> update Oceananigans; Updating registry at `~/.julia/registries/General`; Installed HDF5_jll ─── v1.10.5+7; Installed ColorTypes ─ v0.11.0; Installed HTTP ─────── v0.9.8; Installed HDF5 ─────── v0.13.7; Installed URIs ─────── v1.3.0; Downloaded artifact: HDF5; No Changes to `~/.julia/environments/v1.6/Project.toml`; Updating `~/.julia/environments/v1.6/Manifest.toml`; [3da002f7] ↑ ColorTypes v0.10.12 ⇒ v0.11.0; [f67ccb44] ↑ HDF5 v0.13.6 ⇒ v0.13.7; [cd3eb016] ↑ HTTP v0.8.19 ⇒ v0.9.8; [5c2747f8] + URIs v1.3.0; [0234f1f7] ↓ HDF5_jll v1.12.0+1 ⇒ v1.10.5+7; Building HDF5 → `~/.julia/scratchspaces/44cfe95a-1eb2-52ea-b672-e2afdf69b78f/0b812e7872e2199a5a04944f486b4048944f1ed8/build.log`; Precompiling project...; ✗ Oceananigans; 15 dependencies successfully precompiled in 101 seconds (143 already precompiled); 1 dependency errored. To see a full report either run `import Pkg; Pkg.precompile()` or load the package",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1635:1037,load,load,1037,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1635,1,['load'],['load']
Performance,"Hello, I am having some technical trouble installing and using Oceananigans. I am new to Julia and would like to try to set up Oceananigans on a Macbook. But it is running into issues. . (1) - installation process says some things are missing:; Building MPI → `~/.julia/scratchspaces/44cfe95a-1eb2-52ea-b672-e2afdf69b78f/d56a80d8cf8b9dc3050116346b3d83432b1912c0/build.log`; Precompiling project...; ✗ MPI; ✗ PencilArrays; ✗ PencilFFTs; ✗ Oceananigans; 89 dependencies successfully precompiled in 41 seconds; 4 dependencies errored. To see a full report either run `import Pkg; Pkg.precompile()` or load the packages. (2) - using Oceananigans is looking for MPI, but to my knowledge if I want to run on my computer, it shouldn't need MPI, like MITgcm using serial instead of parallel. But I don't know where to change the setting? This is what happens when I call using Oceananigans:. julia> using Oceananigans; [ Info: Precompiling Oceananigans [9e8cae18-63c1-5223-a75c-80ca9d6e9a09]; ERROR: LoadError: MPI.jl not properly configured, please run `Pkg.build(""MPI"")`.; Stacktrace:; [1] error(s::String); @ Base ./error.jl:33; [2] top-level scope; @ ~/.julia/packages/MPI/08SPr/src/MPI.jl:38; [3] include; @ ./Base.jl:418 [inlined]; [4] include_package_for_output(pkg::Base.PkgId, input::String, depot_path::Vector{String}, dl_load_path::Vector{String}, load_path::Vector{String}, concrete_deps::Vector{Pair{Base.PkgId, UInt64}}, source::String); @ Base ./loading.jl:1318; [5] top-level scope; @ none:1; [6] eval; @ ./boot.jl:373 [inlined]; [7] eval(x::Expr); @ Base.MainInclude ./client.jl:453; [8] top-level scope; @ none:1; in expression starting at /Users/sean/.julia/packages/MPI/08SPr/src/MPI.jl:1. caused by: LoadError: InitError: could not load library ""/Users/sean/.julia/artifacts/48a9a608db31268626d8b8d4d1272c3e7ccbf7d5/lib/libmpifort.12.dylib""; dlopen(/Users/sean/.julia/artifacts/48a9a608db31268626d8b8d4d1272c3e7ccbf7d5/lib/libmpifort.12.dylib, 0x0001): Library not loaded: @rpath/libquadm",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2480:598,load,load,598,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2480,2,"['Load', 'load']","['LoadError', 'load']"
Performance,"Here are the results of `benchmark_tracers.jl` between the `arbitrary-tracers-outer-loops` branch (this PR) and the `arbitrary-tracers-inner-loops` branch. Outer loops is faster in all cases. PS: `arbitrary-tracers-inner-loops` errored for 0 active + 0 passive tracers so this particular benchmark isn't included below, but maybe it doesn't matter as outer loops seem to perform much better. ---. ### `arbitrary-tracers-outer-loops`; ```; ───────────────────────────────────────────────────────────────────────────────────────────────────────────; Tracer benchmarks Time Allocations ; ────────────────────── ───────────────────────; Tot / % measured: 119s / 2.11% 19.7GiB / 0.48% . Section ncalls time %tot avg alloc %tot avg; ───────────────────────────────────────────────────────────────────────────────────────────────────────────; 256×256×256 0 active + 0 passive (GPU, Float64) 10 206ms 8.24% 20.6ms 6.11MiB 6.37% 625KiB; 256×256×256 0 active + 1 passive (GPU, Float64) 10 234ms 9.36% 23.4ms 7.62MiB 7.94% 780KiB; 256×256×256 0 active + 2 passive (GPU, Float64) 10 258ms 10.3% 25.8ms 9.17MiB 9.56% 939KiB; 256×256×256 1 active + 0 passive (GPU, Float64) 10 232ms 9.28% 23.2ms 7.68MiB 8.01% 787KiB; 256×256×256 2 active + 0 passive (GPU, Float64) 10 266ms 10.6% 26.6ms 9.14MiB 9.53% 936KiB; 256×256×256 2 active + 3 passive (GPU, Float64) 10 348ms 13.9% 34.8ms 13.8MiB 14.4% 1.38MiB; 256×256×256 2 active + 5 passive (GPU, Float64) 10 409ms 16.3% 40.9ms 17.0MiB 17.7% 1.70MiB; 256×256×256 2 active + 10 passive (GPU, Float64) 10 551ms 22.0% 55.1ms 25.4MiB 26.5% 2.54MiB; ───────────────────────────────────────────────────────────────────────────────────────────────────────────; ```. ### `arbitrary-tracers-inner-loops`; ```; ───────────────────────────────────────────────────────────────────────────────────────────────────────────; Tracer benchmarks Time Allocations ; ────────────────────── ───────────────────────; Tot / % measured: 120s / 2.20% 19.1GiB / 0.33% . Section ncalls time %tot a",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/452#issuecomment-542302613:371,perform,perform,371,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/452#issuecomment-542302613,1,['perform'],['perform']
Performance,"Here's a few options for optimizing AbstractOperations with `^`:. 1. Auto-convert exponents to `Int32`. For this we'd define. ```julia; Base.^(L::Tuple, a::AbstractField, b::Int64) = ^(L::Tuple, a, Int32(b)); ```. Basically implementing the approach CUDA previously took. For us it's a ""less egregious"" hack since, unlike CUDA.jl, we can ""almost surely"" guarantee that users won't exponentiate with integers larger than `2^31-1 = 2147483647`. Probably the easiest thing to do in the near term. This is a specific extension of the abstract operation defined via `@binary ^` (such that `op = ^`):. https://github.com/CliMA/Oceananigans.jl/blob/1756bc9380999f160f3d2b96f64bf76771614c60/src/AbstractOperations/binary_operations.jl#L100-L108. 2. Convert small-power exponents to literal multiplications. Like `Base.literal_pow`:. ```julia; Base.^(L::Tuple, a::AbstractField, b::Integer) = our_literal_pow(L, a, Val(b)). our_literal_pow(L, a, Val{0}) = one(eltype(a)); our_literal_pow(L, a, Val{1}) = a; our_literal_pow(L, a, Val{2}) = *(L, a, a) # binary operation; our_literal_pow(L, a, Val{3}) = *(L, a, a, a) # multiary operation; our_literal_pow(L, a, Val(b)) where b = _binary_operation(location(a), ^, a, b, location(a), location(a), a.grid) ; ```. etc. I guess 2 would instead happen under the hood when abstract operations are compiled, hopefully, in the best of worlds.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1832#issuecomment-876812225:25,optimiz,optimizing,25,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1832#issuecomment-876812225,1,['optimiz'],['optimizing']
Performance,"Here's a summary of the current design:. I've introduced a new type called `ImmersedBoundaryCondition` to represent boundary conditions on immersed boundaries. The type has 6 fields for each direction, west, east, south, north, bottom, top. I believe this covers the general case in which we might have different fluxes on any face of a cell that's the boundary between a wet node and an immersed node. For example, we can implement ""bottom drag"" for large scale ocean models, with no ""side drag"". In practice this is really just a performance optimization rather than a physical model (the horizontal drag has no effect on the solution), but it's probably important that we can support it. This low-level, but fully general interface allow advanced users to specify any kind of immersed flux / boundary condition they need to. The next challenge is to design a convenient API that interprets user-input and builds `ImmersedBoundaryCondition` appropriately under the hood. For constant `ValueBoundaryCondition` (eg no-slip) or constant `FluxBoundaryCondition`, this is simple --- just copy the boundary condition into every direction. For `FluxBoundaryCondition` we orient the flux along the inward normal (eg positive for `west, south, bottom` and negative for `east, north, top`. This breaks existing convention for fluxes... but we really don't have a choice. We may want to change the convention for the _other_ boundaries eventually. `ContinuousBoundaryFunction` is perhaps the most important case since we'll use this to implement drag boundary conditions. For this I believe we can use the existing infrastructure for `ContinuousBoundaryFunction` (eg we have a way to properly interpolate fields in a manner consistent with the application of drag boundary conditions, etc). It's functionality will have to be extended to accept functions of `i, j, k` (the way it works now is that the ""third"" index (normal to the boundary) is stored inside the object; for immersed boundary conditions we don'",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2437#issuecomment-1100563991:532,perform,performance,532,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2437#issuecomment-1100563991,2,"['optimiz', 'perform']","['optimization', 'performance']"
Performance,"Here's an example:. ```julia; using Oceananigans; using MPI; using JLD2. arch = Distributed(); @show arch. x = y = z = (0, 1); global_size = (8, 2, 2); grid = RectilinearGrid(arch, size=global_size; x, y, z); @show size(grid). rank = arch.local_rank; if rank === 0; bathymetry = 0.1 * rand(global_size...); @save ""bathymetry.jld2"" bathymetry; end. MPI.Barrier(arch.communicator); @load ""bathymetry.jld2"" bathymetry. @show size(bathymetry). grid = ImmersedBoundaryGrid(grid, GridFittedBottom(bathymetry)). @show grid; ```. Run this with. ```bash; mpiexec -n 2 julia --project mwe.jl; ```. from the Oceananigans repo. I get. ```julia; $ /Users/gregorywagner/.julia/bin/mpiexecjl -n 2 julia --project mwe.jl [17:11:03]; [ Info: MPI has not been initialized, so we are calling MPI.Init().; [ Info: MPI has not been initialized, so we are calling MPI.Init().; arch = arch = Distributed{CPU} across 2 = 2×1×1 ranks:; ├── local_rank: 1 of 0-1; ├── local_index: [2, 1, 1]; └── connectivity: east=0 west=0Distributed{CPU} across 2 = 2×1×1 ranks:; ├── local_rank: 0 of 0-1; ├── local_index: [1, 1, 1]; └── connectivity: east=1 west=1. size(grid) = size(grid) = (4, 2, 2); (4, 2, 2); size(bathymetry) = (8, 2, 2); size(bathymetry) = (8, 2, 2); ERROR: LoadError: ERROR: LoadError: ArgumentError: ERROR: DimensionMismatch: array could not be set to match destination field; Stacktrace:; [1] ArgumentError: ERROR: DimensionMismatch: array could not be set to match destination fieldset!(u::Field{Center, Center, Nothing, Nothing, RectilinearGrid{Float64, FullyConnected, Periodic, Bounded, Float64, Float64, Float64, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, Distributed{CPU, fal",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3816:381,load,load,381,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3816,1,['load'],['load']
Performance,Here's the [PR](https://github.com/climate-machine/Oceananigans.jl/pull/478). I'm running into a `LoadError: MethodError for min_enabled_level(::ModelLogger)`. I'm not sure how to rectify that issue. Any help will be appreciated!,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/71#issuecomment-542223794:98,Load,LoadError,98,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/71#issuecomment-542223794,1,['Load'],['LoadError']
Performance,"Here's the julia docs on constants for reference:. https://docs.julialang.org/en/v1/manual/variables-and-scoping/#Constants. This statement in the julia docs could maybe be clarified:. > It is difficult for the compiler to optimize code involving global variables, since their values (or even their types) might change at almost any time. because its ambiguous what ""code involving global variables"" is. For Oceananigans, this almost always means ""functions that capture global variables in their scope"". So `f(x) = a * x` is going to be slow unless `a` is `const`; otherwise `f(x)` cannot be inlined properly because the type of `a` is uncertain.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1863#issuecomment-881696539:223,optimiz,optimize,223,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1863#issuecomment-881696539,1,['optimiz'],['optimize']
Performance,"Here's where we're at.; I've made the following modifications to `baroclinic_adjustment.jl`. ```; using Oceananigans, AMDGPU; ```; and the grid construction now specifies GPU architecture with `GPU(AMDGPU.ROCBackend())`, ie,. ```; grid = RectilinearGrid(GPU(AMDGPU.ROCBackend());; size = (48, 48, 8),; x = (0, Lx),; y = (-Ly/2, Ly/2),; z = (-Lz, 0),; topology = (Periodic, Bounded, Bounded)); ```. When running this, we hit a runtime issue at `plan_forward_transform`. ```; $ julia --project=. baroclinic_adjustment.jl ; ERROR: LoadError: MethodError: no method matching plan_forward_transform(::ROCArray{ComplexF64, 3, AMDGPU.Runtime.Mem.HIPBuffer}, ::Periodic, ::Vector{Int64}, ::UInt32). Closest candidates are:; plan_forward_transform(::CUDA.CuArray, ::Union{Bounded, Periodic}, ::Any, ::Any); @ Oceananigans ~/.julia/packages/Oceananigans/DPfYS/src/Solvers/plan_transforms.jl:36; plan_forward_transform(::Array, ::Periodic, ::Any, ::Any); @ Oceananigans ~/.julia/packages/Oceananigans/DPfYS/src/Solvers/plan_transforms.jl:16; plan_forward_transform(::Union{CUDA.CuArray, Array}, ::Flat, ::Any...); @ Oceananigans ~/.julia/packages/Oceananigans/DPfYS/src/Solvers/plan_transforms.jl:47; ... Stacktrace:; [1] plan_transforms(grid::RectilinearGrid{Float64, Periodic, Bounded, Flat, Float64, Float64, Float64, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}, GPU{ROCBackend}}, storage::ROCArray{ComplexF64, 3, AMDGPU.Runtime.Mem.HIPBuffer}, planner_flag::UInt32); @ Oceananigans.Solvers ~/.julia/packages/Oceananigans/DPfYS/src/Solvers/plan_transforms.jl:93; [2] Oceananigans.Solvers.FFTBasedPoissonSolver(grid::RectilinearGrid{Float64, Periodic, Bounded, Flat, Float64, Float64, Float64, OffsetArrays.Offset",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3475#issuecomment-1946746065:528,Load,LoadError,528,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3475#issuecomment-1946746065,1,['Load'],['LoadError']
Performance,"Hey @ChrisRackauckas thanks for having a look!. We do this operator-splitting method and the pressure term is treated implicitly (while all other terms are treated explicitly). To step from time `n` to `n + 1` first an explicit 2nd-order Adams-Bashforth step is used to calculate the right-hand-side at time `n + 1/2` and the pressure is calculated at `n + 1/2` by solving a Poisson equation. Then we time step the solution from time `n` to `n + 1` via a forward Euler step (using the right-hand-side evaluated at `n + 1/2`). I'm not super familiar with IMEX schemes although perhaps this is equivalent to IMEXEuler?. PS: Yes our numerical methods are pretty pathetic :(. Could be a great idea to rely on DifferentialEquations.jl for time-stepping. We'd get more options and any performance boost would be awesome! In particular, we've been hoping to upgrade to a higher-order low-storage time stepping method. Maybe a good first step would be to try and replicate the current time-stepping method we use and see if all the tests pass?. I can start looking into trying to do this. I don't think I saw `AB2` as an [option](https://docs.juliadiffeq.org/latest/solvers/ode_solve.html) (only `AB3` and up) but maybe another method can replicate what `AB2` does for us (or I can code something up).",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/391#issuecomment-528849991:779,perform,performance,779,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/391#issuecomment-528849991,1,['perform'],['performance']
Performance,"Hey @LaurentPlagne thank you for your kind words!. The code and documentation definitely needs to be improved, and the CUDAnative.jl, CuArrays.jl, and GPUifyLoops.jl packages have helped a lot. Julia has been perfect for this package. We haven't really considered improving CPU performance yet as we've been focusing mostly on single-GPU performance (and we've worked on a little bit of multi-GPU stuff with MPI). As a result, running the model on a CPU isn't very useful as it only uses one core :(. Julia 1.3 seems to have some really promising multithreading so it might become easy to parallelize across multiple cores of one CPU. I think when we start improving CPU performance, we'll probably look to Julia 1.3 first. Or maybe it'll be implemented in GPUifyLoops.jl (or a renamed version of the package). I didn't know about blocking stencil computations and had to look them up. I might be wrong but it sounds like utilizing shared memory like you suggested. We haven't implemented anything like that yet but we've been playing around with an `@stencil` macro abstraction implemented in GPUifyLoops.jl (https://github.com/vchuravy/GPUifyLoops.jl/pull/81) in PR https://github.com/climate-machine/Oceananigans.jl/pull/293.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/333#issuecomment-518326019:278,perform,performance,278,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/333#issuecomment-518326019,3,['perform'],['performance']
Performance,"Hey @beta-effect!. Yes as of PR #282 (included in master, but not in v0.8.0), boundary conditions must be specified at `Model` creation time. This is part of an ongoing attempt at making the types we use in Oceananigans more concrete. If we use structs whose types are set once the struct is created (concrete types), then the compile can more easily optimize things as it knows the data type won't change. The default boundary conditions are given by `ModelBoundaryConditions()` where; ```julia; function ModelBoundaryConditions(;; u = DoublyPeriodicBCs(),; v = DoublyPeriodicBCs(),; w = DoublyPeriodicBCs(),; T = DoublyPeriodicBCs(),; S = DoublyPeriodicBCs(); ); return ModelBoundaryConditions(u, v, w, T, S); end; ```; so I think what you want instead is something like; ```julia; T_bc = FieldBoundaryConditions(; x = CoordinateBoundaryConditions(; BoundaryCondition(Periodic, nothing),; BoundaryCondition(Periodic, nothing)),; y = CoordinateBoundaryConditions(; BoundaryCondition(Periodic, nothing),; BoundaryCondition(Periodic, nothing)),; z = CoordinateBoundaryConditions(; BoundaryCondition(Flux, 0),; BoundaryCondition(Flux, bottom_flux))). bcs = ModelBoundaryConditions(; u = DoublyPeriodicBCs(),; v = DoublyPeriodicBCs(),; w = DoublyPeriodicBCs(),; T = T_bc,; S = DoublyPeriodicBCs()); ```. We still need to figure the API a little because this gets a little annoying: you need to compute `bottom_flux` before constructing the `Model` but it's nice to use `xC, yC, zC` to compute `bottom_flux` but we get them from `model.grid`... The easiest thing to do right now might be to just create the model again but with the new `bcs`:; ```julia; # Set up the model and use an artificially high viscosity ν and diffusivity κ.; model = Model(N=(Nx, Ny, Nz), L=(Lx, Ly, Lz), arch=GPU(),; ν=1e-4, κ=1e-4). # Get location of the cell centers in x, y, z and reshape them to easily; # broadcast over them when calculating hot_bubble_perturbation.; xC, yC, zC = model.grid.xC, model.grid.yC, model.grid.zC",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/314#issuecomment-515189875:351,optimiz,optimize,351,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/314#issuecomment-515189875,1,['optimiz'],['optimize']
Performance,"Hey thanks for doing all this! Things look a lot neater. Just have a few comments/thoughts that I'll pepper throughout but should be good to merge!. > A `Model` constructor is now provided in which all important information can be input via keyword arguments. This is great for understanding what the model does, I agree. Would still be nice to keep the ""legacy constructor"" around for when you just want to create a simple `Model` for playing around with or for testing. And it makes for very nice and simple example code. > I also reduced the computational burden of a few of the tests, and changed to factor of 2 resolutions since this makes sense for FFTs (though relatively unimportant for testing, I think should encourage users to use powers of 2 and make a habit of using them ourselves). I would argue against this. While we should try to use powers of 2 for performance, I think that overly restricts the model resolutions we can be running at. There are a LOT of choices between e.g. `512x512x128` and `1024x1024x128`. It might be that the largest model that fits in memory isn't nice powers of 2. Users may have various reasons for running resolutions that aren't powers of 2. Either way, we should always be testing a wide range of grid sizes (and weird grid sizes like `109x77x13`) because the code should work for all of them. If computational cost becomes an issue we should look into paying for extra CI resources rather than reduce testing. > A few more notes:. Might be good to create some new issues based on those just to keep track of what needs to be done. > The examples are outdated. We should probably reduce the number of examples until the code becomes more stable, and commit to maintaining the few that remain. This is something I'm actively working on in a different branch. I'll make sure they work with the changes in this pull request. > Lots of work to do!. For sure!",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/97#issuecomment-468525078:868,perform,performance,868,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/97#issuecomment-468525078,1,['perform'],['performance']
Performance,"Hey, original author of both https://github.com/JuliaGeo/NetCDF.jl and https://github.com/meggart/ZarrNative.jl here. Regarding the state of NetCDF.jl , yes I would say I mostly stopped developing the package due to time constraints and currently shift my focus towards Zarr since this is what we are using in our current project. . My last attempt at improving the NetCDF solved many of the issues with the package https://github.com/JuliaGeo/NetCDF.jl/pull/61 but was not merged because of conflicts with other bugfix PRs. However, might be source of inspiration if someone wants to do a rewrite. . Regarding write performance, I would be very interested to see examples where NetCDF.jl performs worse than e.g. python-netcdf4, since most of the time should be spent in the same NetCDF C library. I have been using the package extensively and did not experience it to be slower than comparable packages. . I you are worried about the robustness of NetCDF.jl, you should not even look at ZarrNative.jl, since it is still very young and rather a prototype. . I would be very happy to discuss the issues further, maybe in a call? Would also be interested to learn about your project which seems to be very cool.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/145#issuecomment-475680874:617,perform,performance,617,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/145#issuecomment-475680874,2,['perform'],"['performance', 'performs']"
Performance,"Hi @pnavaro thanks for opening this issue. Yeah this is an issue on some clusters so it might be worth adding a note in the documentation. In my case I think the login node had a GPU so I was able to build on the login node then just precompile and run on the compute node without internet access. But this was also before CUDA.jl which very nicely downloads CUDA artifacts for you so it just relied on the local CUDA toolkit. Do you have access to the CUDA toolkit on the login node via something like `module load cuda`? Might allow you to build on the login node without a GPU but might be weird to have CUDA but no GPU. Your compute node must have a CUDA installation. Looks like the CUDA.jl documentation has a section on how to make use of a local CUDA installation: https://juliagpu.gitlab.io/CUDA.jl/installation/overview/#CUDA-toolkit. If you can find where the CUDA toolkit is installed on your compute node, then maybe all you have to do is set one of the `CUDA_HOME`, `CUDA_ROOT` or `CUDA_PATH` environment variables?. Looks like creating a container with CUDA.jl is an option but it still requires a CUDA toolkit at runtime: https://juliagpu.gitlab.io/CUDA.jl/installation/overview/#Containers. Let us know if this helps or if you're able to get up and running on your compute node. Would be good to figure this out and add to the documentation. The #gpu channel on Julia's Slack or the GPU section of the Julia Discourse might also be good places to ask if we can't figure it out.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1035#issuecomment-707637840:511,load,load,511,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1035#issuecomment-707637840,1,['load'],['load']
Performance,"Hi @roxyboy, we'll make some docs/examples soon! There are some tools in a different repo but they are rough on the edges at the moment and I'm bit reluctant to point you to it. If you have the `bathymetry` loaded as an array of the same size as a flat-bottom latitude-longitude grid then you can use [GridFittedBottom](https://clima.github.io/OceananigansDocumentation/stable/appendix/library/#Oceananigans.ImmersedBoundaries.GridFittedBottom) to do something like:. ```Julia; underlying_grid = LatitudeLongitudeGrid(arch,; size = (Nx, Ny, Nz),; longitude = (-180, 180),; latitude = (-75, 75),; z = (-depth, 0),; topology = (Periodic, Bounded, Bounded)). grid = ImmersedBoundaryGrid(underlying_grid, GridFittedBottom(bathymetry)); ```",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3389#issuecomment-1809223215:207,load,loaded,207,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3389#issuecomment-1809223215,1,['load'],['loaded']
Performance,"Hi all!. With the help of Ali and a colleague at my school, I can now run Oceananigans using a GPU on my school's HPC. I am now at a step that is totally unfamiliar to me, coding for a GPU. . Based on what I have read from the Oceananigans documentation, I think the only trouble I will have is creating my forcing functions and my initial conditions. Currently, I read in data from csv's, perform a 1D spline interpolation, and then assign the interpolated function (which is 1D) to a 3D field function. . My understanding is that the way arrays are treated is quite different when working with GPUs and I honestly don't even know where to begin. Below is an excerpt of my run script that shows the initial and boundary condition creation with a CPU architecture. If anyone can share resources on how to code this for a GPU or just explain it if it is easy, I would greatly appreciate the help!. ```; ρₒ = 1025; ############## Boundary conditions ###################; ## Pulling the boundary conditions from the data csv; df = CSV.read(""data_inputs/kma_buoy_fluxes_soulik.csv"", DataFrame);. seconds = df.time[:]*86400; # convert to seconds; secs = [tnow - seconds[1] for tnow in seconds]; # set first time stamp to 0; secs = [s - 518400 for s in secs]; # Move 0 time stamp to later in the time series. ## make the splines; spl_taux = Spline1D(secs, df.taux/(ρₒ), k=1);; spl_tauy = Spline1D(secs, df.tauy/(ρₒ), k=1);; spl_latHF = Spline1D(secs, df.lat_hf/(ρₒ * cᴾ), k=1);; spl_senHF = Spline1D(secs, df.sens_hf/(ρₒ * cᴾ), k=1);. ## turn the splines into functional arguments; @inline Fxn_taux(x,y,t) = spl_taux(t);; @inline Fxn_tauy(x,y,t) = spl_tauy(t);; @inline Fxn_HFlx(x,y,t) = spl_latHF(t) + spl_senHF(t); # K m⁻¹ s⁻¹, surface _temperature_ flux. ############# Initial conditions #############; ## Initial surface stress condition; Qo = sqrt(spl_taux(0)^2 + spl_tauy(0)^2). ## Random noise damped at top and bottom; Ξ(z) = randn() * z / model.grid.Lz * (1 + z / model.grid.Lz); # noise. ## Veloci",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1509:390,perform,perform,390,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1509,1,['perform'],['perform']
Performance,"Hi all, I am trying to setup a DNS in a triply bounded domain with inflow conditions on the west wall. All boundary conditions are as default except for the bottom boundary (no-slip), west wall (inflow) and right wall (outflow). I tried setting up the boundaries as; ```; ubcs = UVelocityBoundaryConditions(grid, bottom = BoundaryCondition(Value, 0), west = BoundaryCondition(Value,0.1), east = BoundaryCondition(Gradient,0)); vbcs = VVelocityBoundaryConditions(grid, bottom = BoundaryCondition(Value, 0)); bbcs = TracerBoundaryConditions(grid, west = BoundaryCondition(Value,0)); ```; The tracer boundary condition is fixed to 0 on the west wall. I am getting the following error when running the simulation:. ERROR: LoadError: MethodError: no method matching fill_west_halo!(::OffsetArrays.OffsetArray{Float64,3,Array{Float64,3}}, ::BoundaryCondition{Value,Float64}, ::CPU, ::RegularCartesianGrid{Float64,Bounded,Bounded,Bounded,OffsetArrays.OffsetArray{Float64,3,Base.ReshapedArray{Float64,3,StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}},Tuple{}}}}, ::Clock{Float64}, ::NamedTuple{(:velocities, :tracers, :diffusivities),Tuple{NamedTuple{(:u, :v, :w),Tuple{OffsetArrays.OffsetArray{Float64,3,Array{Float64,3}},OffsetArrays.OffsetArray{Float64,3,Array{Float64,3}},OffsetArrays.OffsetArray{Float64,3,Array{Float64,3}}}},NamedTuple{(:b,),Tuple{OffsetArrays.OffsetArray{Float64,3,Array{Float64,3}}}},Nothing}}). I now wonder if it is currently possible to have inflow conditions with buoyancy, or if this is a boundary condition that is not implemented. Are inflow conditions incompatible with buoyancy or is the error unrelated? Thanks very much for your help! Entire script below:. ```; # # Lock-release gravity current example. using Oceananigans, Oceananigans.Grids, Printf, SpecialFunctions, Plots. # ## Physical and numerical parameters; #; # First, we pick a resolution and domain size,. Nx = 256; Ny = 256 # x resolution; Nz = 32 # z resolution; Lx = 10; Ly = ",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/789:718,Load,LoadError,718,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/789,1,['Load'],['LoadError']
Performance,"Hi all,. I'm stuck trying to debug an error I keep getting when running a non-hydrostatic model on GPU. . It runs for a bit and then throws this error:; ```; ... (loads of similar CUDA stuff that goes on for a very very long time); @ ~/.julia/packages/CUDA/35NC6/lib/cudadrv/state.jl:170 [inlined]; [16] context!; @ ~/.julia/packages/CUDA/35NC6/lib/cudadrv/state.jl:165 [inlined]; [17] unsafe_free!(xs::CUDA.CuArray{ComplexF64, 3, CUDA.Mem.DeviceBuffer}, stream::CUDA.CuStream) ; @ CUDA ~/.julia/packages/CUDA/35NC6/src/array.jl:129; [18] unsafe_finalize!(xs::CUDA.CuArray{ComplexF64, 3, CUDA.Mem.DeviceBuffer}); @ CUDA ~/.julia/packages/CUDA/35NC6/src/array.jl:150; [19] top-level scope; @ ~/.julia/packages/InteractiveErrors/JOo2y/src/InteractiveErrors.jl:329; [20] eval; @ ./boot.jl:370 [inlined]; [21] eval_user_input(ast::Any, backend::REPL.REPLBackend, mod::Module); @ REPL /rds/user/js2430/hpc-work/julia-1.9.2/share/julia/stdlib/v1.9/REPL/src/REPL.jl:153; [22] repl_backend_loop(backend::REPL.REPLBackend, get_module::Function); @ REPL /rds/user/js2430/hpc-work/julia-1.9.2/share/julia/stdlib/v1.9/REPL/src/REPL.jl:249; [23] start_repl_backend(backend::REPL.REPLBackend, consumer::Any; get_module::Function); @ REPL /rds/user/js2430/hpc-work/julia-1.9.2/share/julia/stdlib/v1.9/REPL/src/REPL.jl:234; [24] run_repl(repl::REPL.AbstractREPL, consumer::Any; backend_on_current_task::Bool, backend::Any); @ REPL /rds/user/js2430/hpc-work/julia-1.9.2/share/julia/stdlib/v1.9/REPL/src/REPL.jl:379; [25] run_repl(repl::REPL.AbstractREPL, consumer::Any); @ REPL /rds/user/js2430/hpc-work/julia-1.9.2/share/julia/stdlib/v1.9/REPL/src/REPL.jl:365; [26] (::Base.var""#1017#1019""{Bool, Bool, Bool})(REPL::Module); @ Base ./client.jl:421; [27] #invokelatest#2; @ ./essentials.jl:816 [inlined]; [28] invokelatest; @ ./essentials.jl:813 [inlined]; [29] run_main_repl(interactive::Bool, quiet::Bool, banner::Bool, history_file::Bool, color_set::Bool); @ Base ./client.jl:405; [30] exec_options(opts::Base.JLOpti",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3267:163,load,loads,163,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3267,1,['load'],['loads']
Performance,"Hi! I am trying to implement the following setup with a `VerticallyStretchedRectilinearGrid` and `AnisotropicBiharmonicDiffusivity`:. ```; ## Initializing grid; # Horizontal grid; Nx = 64; Ny = 64; Δy = 250.0; Δx = 250.0; Lx = Δx*Nx; Ly = Δy*Ny; # Vertical grid; Lz = 160; zF = collect(0:2.5:160); zF = -zF[end:-1:1]; ; Nz = length(zF) - 1; # Setup grid; grid = VerticallyStretchedRectilinearGrid(Float64; architecture = GPU(),size = (Nx,Ny,Nz), x=(0, Lx), y=(0, Ly), zF=zF, halo = (3, 3, 3), topology = (Periodic, Bounded, Bounded)). ## Turbulence closure; kappaH = 1e5 # m4/s; kappaV = 5e-5 # m2/s. ## Setting up model; model = IncompressibleModel(; architecture = GPU(),; grid = grid,; closure = (AnisotropicDiffusivity(νh=0, κh=0, κz = kappaV, νz = kappaV),; AnisotropicBiharmonicDiffusivity(νh=kappaH, κh=kappaH)); ). ## Running; simulation = Simulation(model, Δt=2, stop_iteration=1) ; run!(simulation); ```. I get the following error:. ```; ERROR: LoadError: time = 2.0, iteration = 1: NaN found in u. Aborting simulation.; Stacktrace:; [1] error(::String) at ./error.jl:33; [2] run_diagnostic!(::NaNChecker{IterationInterval,NamedTuple{(:u,),Tuple{Field{Face,Center,Center,OffsetArrays.OffsetArray{Float64,3,CuArray{Float64,3}},VerticallyStretchedRectilinearGrid{Float64,Periodic,Bounded,Bounded,OffsetArrays.OffsetArray{Float64,1,StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}},OffsetArrays.OffsetArray{Float64,1,CuArray{Float64,1}}},NamedTuple{(:x, :y, :z),Tuple{CoordinateBoundaryConditions{BoundaryCondition{Oceananigans.BoundaryConditions.Periodic,Nothing},BoundaryCondition{Oceananigans.BoundaryConditions.Periodic,Nothing}},CoordinateBoundaryConditions{BoundaryCondition{Flux,Nothing},BoundaryCondition{Flux,Nothing}},CoordinateBoundaryConditions{BoundaryCondition{Flux,Nothing},BoundaryCondition{Flux,Nothing}}}}}}}}, ::IncompressibleModel{Oceananigans.TimeSteppers.RungeKutta3TimeStepper{Float64,NamedTuple{(:u, :v, :w, :T, :S),Tuple{Field{Face,Center",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1574:955,Load,LoadError,955,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1574,1,['Load'],['LoadError']
Performance,"Hi, PencilArrays / PencilFFTs author here. I'd be happy to help solving this issue, and I'd also welcome any ideas that would help improve the PencilArrays interface and docs. I am not familiar with how you define your domain partition. However, from the point of view of PencilFFTs, there is one restriction, which is that the first dimension (`x`) must *not* be decomposed in physical space. This is because FFTs are first performed along this dimension (along which data is contiguous, as usual for Julia arrays). Also, to avoid potential issues, your eigenvalues should be `PencilArray` wrappers. For now, PencilArrays allows broadcasting together `PencilArray`s and regular `Array`s, but I'm thinking this is not a good idea since the behaviour is non-intuitive and can lead to precisely this kind of issue. So I'm thinking about changing the current behaviour in the future -- and improving documentation on this. Finally, when you write:. ```julia; xc = b = solver.storage[2]; ```. you probably mean `xc = solver.storage[3]` (or, equivalently, `last(solver.storage)`, as in [this example](https://jipolanco.github.io/PencilFFTs.jl/dev/generated/in-place/#Applying-plans)). `storage[2]` holds an intermediate state that has no ""physical"" meaning, since it has been overwritten by the end of a transform. I should clarify all this in the in-place transforms example... Feel free to ping me if things are unclear in the docs or if I can provide any other information.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2347#issuecomment-1102319088:425,perform,performed,425,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2347#issuecomment-1102319088,1,['perform'],['performed']
Performance,"Hi,; I encounter an error when running `pkg> test Oceananigans` despite being able to run example simulation internal_wave.jl on both CPU and GPU. The description of CUDA error code 201 is also attached below. ```; ERROR: LoadError: CUDA error (code 201, CUDA_ERROR_INVALID_CONTEXT); Stacktrace:; [1] throw_api_error(::CUDAdrv.cudaError_enum) at /home/raphael/.julia/packages/CUDAdrv/Uc14X/src/error.jl:105; [2] macro expansion at /home/raphael/.julia/packages/CUDAdrv/Uc14X/src/error.jl:112 [inlined]; [3] cuCtxGetDevice(::Base.RefValue{Int32}) at /home/raphael/.julia/packages/CUDAapi/XuSHC/src/call.jl:93; [4] device at /home/raphael/.julia/packages/CUDAdrv/Uc14X/src/context.jl:142 [inlined]; [5] device! at /home/raphael/.julia/packages/CUDAnative/ierw8/src/init.jl:198 [inlined]; [6] device!(::CUDAdrv.CuDevice) at /home/raphael/.julia/packages/CUDAnative/ierw8/src/init.jl:188; [7] top-level scope at /home/raphael/.julia/packages/Oceananigans/1xP6n/test/runtests.jl:74; [8] include(::String) at ./client.jl:439; [9] top-level scope at none:6; in expression starting at /home/raphael/.julia/packages/Oceananigans/1xP6n/test/runtests.jl:61; ERROR: Package Oceananigans errored during testing; ```. Has anyone encountered this issue? I am on master 0.30.0. Thanks for your help!; ![Screenshot_2020-07-02 CUDA Driver API CUDA Toolkit Documentation](https://user-images.githubusercontent.com/31293515/86409162-4f484f80-bc86-11ea-8736-bad7a9da5345.png)",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/788:222,Load,LoadError,222,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/788,1,['Load'],['LoadError']
Performance,"Hitting some weird stack overflow error on the GPU when attempting to launch the `calculate_interior_source_terms!` time stepping kernel. Just documenting it here in case it shows up again in the future (hopefully will follow up with a solution). ```; ERROR: StackOverflowError:; Stacktrace:; [1] snca_compress!(::Array{Core.Compiler.Node,1}, ::Array{UInt64,1}, ::UInt64, ::UInt64) at ./compiler/ssair/domtree.jl:209 (repeats 25910 times); [2] SNCA(::Core.Compiler.CFG) at ./compiler/ssair/domtree.jl:260; [3] construct_domtree(::Core.Compiler.CFG) at ./compiler/ssair/domtree.jl:104; [4] run_passes(::Core.CodeInfo, ::Int64, ::Core.Compiler.OptimizationState) at ./compiler/ssair/driver.jl:122; [5] optimize(::Core.Compiler.OptimizationState, ::Any) at ./compiler/optimize.jl:164; [6] typeinf(::Core.Compiler.InferenceState) at ./compiler/typeinfer.jl:35; [7] typeinf_edge(::Method, ::Any, ::Core.SimpleVector, ::Core.Compiler.InferenceState) at ./compiler/typeinfer.jl:497; [8] abstract_call_method(::Method, ::Any, ::Core.SimpleVector, ::Core.Compiler.InferenceState) at ./compiler/abstractinterpretation.jl:345; [9] abstract_call_gf_by_type(::Any, ::Array{Any,1}, ::Any, ::Core.Compiler.InferenceState) at ./compiler/abstractinterpretation.jl:85; [10] abstract_call(::Any, ::Tuple{}, ::Array{Any,1}, ::Array{Any,1}, ::Core.Compiler.InferenceState) at ./compiler/abstractinterpretation.jl:776; [11] abstract_apply(::Any, ::Array{Any,1}, ::Array{Any,1}, ::Core.Compiler.InferenceState) at ./compiler/abstractinterpretation.jl:519; [12] abstract_call(::Any, ::Array{Any,1}, ::Array{Any,1}, ::Array{Any,1}, ::Core.Compiler.InferenceState) at ./compiler/abstractinterpretation.jl:567; [13] abstract_eval_call(::Array{Any,1}, ::Array{Any,1}, ::Array{Any,1}, ::Core.Compiler.InferenceState) at ./compiler/abstractinterpretation.jl:805; [14] abstract_eval(::Any, ::Array{Any,1}, ::Core.Compiler.InferenceState) at ./compiler/abstractinterpretation.jl:890; [15] typeinf_local(::Core.Compiler.InferenceState",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/153#issuecomment-477727916:642,Optimiz,OptimizationState,642,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/153#issuecomment-477727916,4,"['Optimiz', 'optimiz']","['OptimizationState', 'optimize']"
Performance,"Hmmm - I am skeptical about that from a computer point of view? In an implicit algorithm and/or radiative transfer alg the next step often depends on the result of the previous steps. . As every CCE graduate student learns, computers take many tens of cycles to evaluate an operation like a an add or multiply. The operation is sequential and carried out in a multi-stage pipeline in the heart of a CPU (or GPU). So unless the compiler has something else for the processor to do, the processor will have to wait for one step to make it through the pipeline before the next step? . I think the normal way to do this is to have some inner horiz blocking that is flexible (and can be 1,1) and then iterate over levels with some intermediate stores? The horiz block can be some fraction of inner cache or GPU local proc shared mem. The math doesn't quite look at this way because it assumes that a+b and/or a*b etc.. happen ""instantaneously"". It does not take into account that the awnser from a*b might take; 5-10 clock cycles to pass through the CPU floating point unit. . I think that is fairly generally true? Functional style code in Julia should make it possible; to express this in a fairly clean way, but with flexibility to change blocking for different ; target arch.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/470#issuecomment-541363073:792,cache,cache,792,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/470#issuecomment-541363073,1,['cache'],['cache']
Performance,Hmmm a lot of failures due to CUDA scalar `getindex` operations even though we explicitly set `CUDA.allowscalar(true)` in `runtests.jl`... We could take this opportunity to get rid of all scalar operations in the tests and just use `CUDA.@allowscalar` where it's needed. Maybe new CUDA scalar operations are hurting performance and that's why GPU CI has slowed down?,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1514#issuecomment-816964002:316,perform,performance,316,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1514#issuecomment-816964002,1,['perform'],['performance']
Performance,Hmmm yeah... Just an idea but in https://github.com/climate-machine/Oceananigans.jl/pull/685 the performance regression was actually due to not eliding `_apply_*_bcs!` for `NotFluxBC` so maybe we could be eliding some computation or function call?,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/710#issuecomment-604372102:97,perform,performance,97,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/710#issuecomment-604372102,1,['perform'],['performance']
Performance,"Hmmm, since the conveniently shaped arrays won't be used in performance-critical code like time-stepping, maybe it makes sense to to just add a convenience function, e.g.; ```julia; xC, yC, zC = coordsC(grid); ```; that returns reshaped arrays.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/348#issuecomment-519872781:60,perform,performance-critical,60,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/348#issuecomment-519872781,1,['perform'],['performance-critical']
Performance,"Hmmm, yeah indeed the halos aren't exactly correct. For example, rows 1-2 (j=-1,0). ```; -0.0085938 -0.00844015 -0.0085938 -0.00844015 -0.0085938 -0.00844015; -0.010009 -0.0104645 -0.010009 -0.0104645 -0.010009 -0.0104645; ```. should exactly match rows 33-34 (j=Ny-1,Ny) but don't. ```; -0.0085938 -0.00844015 -0.0102716 -0.0103836 -0.0085938 -0.00844015; -0.010009 -0.0104645 -0.0101071 -0.0100802 -0.010009 -0.0104645; ```. I don't think this is a problem for time-stepping as the Oceananigans algorithm only fills halo regions right before it is needed (to improve performance). If you need the halo regions to be correct then you could manually call `fill_halo_regions!(model.velocities.u, CPU())`. A cleaner alternative if this is a desirable feature might be to add a `fill_halo_regions` keyword argument to `time_step!` (and `Simulation`) to ensure all halos are filled and the fields are consistent at the end of each time step.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/949#issuecomment-693639468:569,perform,performance,569,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/949#issuecomment-693639468,1,['perform'],['performance']
Performance,How about benchmarking the examples?. We should have a fully-loaded example too.,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1089#issuecomment-713730714:61,load,loaded,61,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1089#issuecomment-713730714,1,['load'],['loaded']
Performance,"How exactly would we enforce incompressibility? Should we perform a pressure correction step? Something like. ```julia; function set!(model; enforce_incompressibility=false, kwargs... # everything else in `set!`. if enforce_incompressibility; calculate_pressure_correction!(model, 1.0); pressure_correct_velocities!(model, 1.0); end. return nothing; end; ```. (This syntax will work once #1057 is merged). The ""1.0"" is a pseudo-timestep that is technically irrelevant (but can't be `Inf` or `0`). The two above functions come from the time-stepping routine:. https://github.com/CliMA/Oceananigans.jl/blob/c35af739186434d754c70966ecc52e4cc61db5a2/src/TimeSteppers/runge_kutta_3.jl#L73-L74. I suppose this projects the user-defined velocity field onto an incompressible field, which is more or less what we want? It seems better than recomputing `w` from continuity.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1027#issuecomment-710686571:58,perform,perform,58,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1027#issuecomment-710686571,1,['perform'],['perform']
Performance,"Huh --- on my GPU I was getting 2x speed up for `Float32`. I'll have to check that again. The slow down has to do with the abstractions I have introduced. 30% is a huge slow down for one function, indicative of a major problem --- probably a type inference issue? . I think that once this problem is solved the code may become faster because of the disambiguation this PR lends to the innermost kernels. This problem becomes catastrophic for the closures, which make heavy use of the abstraction. So solving this problem is imperative. We can restore the performance of the default closure by simply pasting the old operators into `constant_diffusivity_closures.jl`. However, I believe the issue with type inference is solvable. . Unfortunately, I'm in `Cthulhu` hell right now trying to figure it out... I'm wondering whether these problems will vanish once we eliminate branches from the inmost functions...",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/245#issuecomment-496902962:555,perform,performance,555,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/245#issuecomment-496902962,1,['perform'],['performance']
Performance,"I add a bit of context here:. @Yixiao-Zhang is doing a non-hydrostatic simulation with an immersed boundary and he finds that the code crashes with the PCG solver. That is not necessarily connected with the PCG solver but it might be caused by simulation setup or other issues. Since the pressure solver used is experimental (from [this branch](https://github.com/CliMA/Oceananigans.jl/tree/glw-xk/divergence-free-immersed-velocity-field)), as a way to assess where the crashing comes from, I suggested using another method to see if the crash would also happen, which would validate or not the experimental pressure solver. @Yixiao-Zhang, optimizing GPU preconditioners is a quite difficult task as demonstrated by the preconditioners slowing down the simulation, and probably not a good use of time of trying to figure out a way to speed up these solvers that we are not sure we want to use. ; Since this simple attempt to have a simulation that runs (with another _correct_ solver) up to the crashing point does not work, I would suggest to just trying to use a (slightly non-correct) FFT pressure solver to see if the simulation still crashes.; If not, then we can assume the crashing occurs due to the PCG solver and try to catch the bug. ; This will probably be a better way to ""optimize"" the solver since we know that the PCG preconditioned with FFT is faster than these other methods.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3552#issuecomment-2052606749:640,optimiz,optimizing,640,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3552#issuecomment-2052606749,2,['optimiz'],"['optimize', 'optimizing']"
Performance,I added a `.gitignore` to the `gh-pages` branch so we should be good now. This PR now deletes leftover JLD2 files before deploying (just to be safe and also to unclutter the CI server in case we need to cache files between builds).,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/558#issuecomment-563309498:203,cache,cache,203,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/558#issuecomment-563309498,1,['cache'],['cache']
Performance,"I agree that I would expect it to saturate at higher than 16 if there were 48 cores, but clearly I'm wrong. Getting another benchmark would be a good idea. I'm happy to consider the numba + parallel idea since that would be good to test the architecture. This [mini-course](https://github.com/omlins/parallel-gpu-workshop-JuliaCon21) did give some threaded examples to solve the diffusion equation in 3D. I wonder if we might want to ask Ludovic if they have done any scalings for multi-threading?. I'm happy to discuss this with @hennyg888 on Monday and see what we come up with. Others are happy to join the discussion if they like.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1861#issuecomment-886074548:481,multi-thread,multi-threading,481,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1861#issuecomment-886074548,1,['multi-thread'],['multi-threading']
Performance,"I agree with @navidcy's points. Of course, it doesn't even matter whether the tracer is initialized or not (referring to the comment in @navidcy's script). The main point is that you can ""re-initialize"" a state whenever you like. I'd also like to make the extra point that @navidcy's pattern is interpretable and readable. I'm not sure we would achieve the same if we hide such a feature inside the source code. Does @navidcy's suggestion work for you @tomchor ?. I also think it is preferred to use separate files for a situation like this (though it may not be necessary to save intermediate times in the spin-up at all --- so a second file might not be necessary). If the spin up is recorded, I think it's better to use a separate file for its data. If the spin up is expensive, the following performance optimization could be used (we can leverage the `velocities` kwarg to save a bit of allocation):. ```julia; # Spin up with no tracers; model = NonhydrostaticModel(; tracers=nothing, kwargs...); simulation = Simulation(model, ...) # etc; run!(simulation). # Run for real, re-using the old `velocities` fields but overwriting the old `model`; model = NonhydrostaticModel(; tracers=:c, velocities=model.velocities, kwargs...); simulation = Simulation(model, ...) # etc; ```. There is some additional memory allocation for tendencies in this case, however, so it may not work for simulations that push GPU memory. > (I'm not aware of any way to ""remove"" the old Simulation from the GPU memory, but if there is, then this downside can be negated.). This occurs automatically with garbage collection, provided that there's no reference to the old simulation in the name space. CUDA may have a way to manually call the garbage collector, after doing something like `model=nothing; simulation=nothing`. If you figure that out, it'd be nice to know. > It makes for more complex code, especially when figuring out when it's safe to pick-up a simulation or not, due to the way pick-ups work. I don't foll",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3154#issuecomment-1605717680:796,perform,performance,796,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3154#issuecomment-1605717680,2,"['optimiz', 'perform']","['optimization', 'performance']"
Performance,"I agree. The focus should be on moving to a proper file format (#30, #31) then this will be a non-issue. The only use case I see for frequent massive data dumps is if you're running such a massive simulation where writing output almost becomes a bottleneck (apparently was a big issue with [LLC4320](http://online.kitp.ucsb.edu/online/blayers18/menemenlis/)). No point worrying about this case.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/37#issuecomment-462853542:246,bottleneck,bottleneck,246,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/37#issuecomment-462853542,1,['bottleneck'],['bottleneck']
Performance,"I also had this issue, as new into GPU running, I was super confused about this error. It will be helpful if this issue is not fixable, to at least point out in the documentation. . I encountered this error by running a simulation based on the tutorial ([Langmuir turbulence](https://clima.github.io/OceananigansDocumentation/stable/generated/langmuir_turbulence/#Langmuir-turbulence-example)) in GPUs. Note that the print function prints the `maximum(abs, u), maximum(abs, v), maximum(abs, w)`:; ```; msg = @sprintf(""i: %04d, t: %s, Δt: %s, umax = (%.1e, %.1e, %.1e) ms⁻¹, wall time: %s\n"",; iteration(simulation),; prettytime(time(simulation)),; prettytime(simulation.Δt),; maximum(abs, u), maximum(abs, v), maximum(abs, w),; prettytime(simulation.run_wall_time)); ```; thus resulting in the error:; ```; LoadError: CUDA error: too many resources requested for launch; ```; For reference, the code works once the `maximum` functions are removed:; ```; msg = @sprintf(""i: %04d, t: %s, �~Tt: %s, wall time: %s\n"",; iteration(simulation),; prettytime(time(simulation)),; prettytime(simulation.�~Tt),; prettytime(simulation.run_wall_time)); ```",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3427#issuecomment-1992106103:807,Load,LoadError,807,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3427#issuecomment-1992106103,1,['Load'],['LoadError']
Performance,"I also tried to run `baroclinic_adjustmenet.jl` found [here](https://github.com/CliMA/Oceananigans.jl/blob/main/validation/mesoscale_turbulence/baroclinic_adjustment.jl) and got the following error. ```; ERROR: LoadError: MethodError: Cannot `convert` an object of type Float64 to an object of type VerticallyImplicitTimeDiscretization; Closest candidates are:; convert(::Type{T}, ::T) where T at essentials.jl:205; Stacktrace:; [1] convert_diffusivity(FT::Type, κ::Float64; kw::Base.Iterators.Pairs{Symbol, Bool, Tuple{Symbol}, NamedTuple{(:discrete_form,), Tuple{Bool}}}); @ Oceananigans.TurbulenceClosures ~/Software/Oceananigans.jl/src/TurbulenceClosures/turbulence_closure_utils.jl:15; [2] ScalarDiffusivity(time_discretization::ExplicitTimeDiscretization, formulation::Oceananigans.TurbulenceClosures.VerticalFormulation, FT::Type; ν::Float64, κ::Float64, discrete_form::Bool); @ Oceananigans.TurbulenceClosures ~/Software/Oceananigans.jl/src/TurbulenceClosures/turbulence_closure_implementations/scalar_diffusivity.jl:52; [3] #VerticalScalarDiffusivity#39; @ ~/Software/Oceananigans.jl/src/TurbulenceClosures/turbulence_closure_implementations/scalar_diffusivity.jl:87 [inlined]; [4] top-level scope; @ ~/Software/Oceananigans.jl/validation/mesoscale_turbulence/baroclinic_adjustment.jl:51; in expression starting at /home/fpoulin/Software/Oceananigans.jl/validation/mesoscale_turbulence/baroclinic_adjustment.jl:51; ```",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2543:211,Load,LoadError,211,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2543,1,['Load'],['LoadError']
Performance,"I also want to drop another optimization that can be used here --- using a `Float32` grid with the preconditioner. This will speed things up a bit. Eg:. ```julia; using Oceananigans.Grids: with_number_type. reduced_precision_grid = with_number_type(Float32, underlying_grid). pressure_solver = ConjugateGradientPoissonSolver(grid;; preconditioner = fft_poisson_solver(reduced_precision_grid),; maxiter = 10; ); ```. I also suggest plotting the divergence to get a handle on how the solver is working. A well-converged solution seems to have isotropically-distributed divergence errors. With looser tolerances, the divergence may be concentrated around the bathymetry (and of course with the naive FFT solver it has a thin layer adjacent to the bathymetry). I also think it would be nice if setting `maxiter=0` had a similar effect as simply using the FFT-based preconditioner. It's not the case right now, I think because we do not apply the preconditioner to the pressure initially before doing the CG iteration. It might not work (I believe I experimented briefly with that to no avail).",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3831#issuecomment-2414308611:28,optimiz,optimization,28,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3831#issuecomment-2414308611,1,['optimiz'],['optimization']
Performance,I am actually looking into implementing a multidimensional _sweep_ approach that will allow us to break the 2nd order limit. https://d-nb.info/1124132775/34; https://www.sciencedirect.com/science/article/pii/S0021999104002281. I ll see if it is too difficult or unfeasible in terms of performance,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2615#issuecomment-1164483965:285,perform,performance,285,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2615#issuecomment-1164483965,1,['perform'],['performance']
Performance,I am currently running a 0.083-degree global ocean simulation on Tesla V100 GPUs. I am benchmarking a bit to understand where we can target optimization to improve the performance and I thought I would leave the benchmarks here to document them for later use. (we do not have to merge this PR),MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2760:140,optimiz,optimization,140,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2760,2,"['optimiz', 'perform']","['optimization', 'performance']"
Performance,"I am happy to open a PR but when I try the line you suggested , unfortunately, there is still a problem. Some good news! This fixes it for the vector invariant formulation. However, when I try the conervative form it complains about not knowing `u`. See the start of the output below. ```; ERROR: LoadError: TaskFailedException. nested task error: type NamedTuple has no field u; Stacktrace:; [1] getproperty(x::NamedTuple{(:uh, :vh, :h), Tuple{Field{Face, Center, Center, Nothing, RectilinearGrid{Float64, Periodic, Periodic, Flat, Float64, Float64, Float64, OffsetArrays.; ```",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2606#issuecomment-1152672984:297,Load,LoadError,297,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2606#issuecomment-1152672984,1,['Load'],['LoadError']
Performance,"I am happy with both names but do we need a distributed for each framework? I thought they were almost identical but don't remember the details. Francis; ________________________________; From: Ali Ramadhan ***@***.***>; Sent: Tuesday, March 23, 2021 10:34:09 AM; To: CliMA/Oceananigans.jl ***@***.***>; Cc: Francis Poulin ***@***.***>; Author ***@***.***>; Subject: Re: [CliMA/Oceananigans.jl] `MultiCPU` or `MPI_CPU` (#1502). True. I guess we don't have a separate architecture for multi-threaded. I agree that MPI_CPU or MPICPU would be more precise. What do you think of DistributedCPU and DistributedGPU?. -; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/CliMA/Oceananigans.jl/issues/1502#issuecomment-804952861>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AB63PQIRKGMIJUKMEU5CYQTTFCRGDANCNFSM4ZUIJ5FQ>.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1502#issuecomment-804964762:484,multi-thread,multi-threaded,484,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1502#issuecomment-804964762,1,['multi-thread'],['multi-threaded']
Performance,"I am having a problem with `NonTraditionalBetaPlane`. Please see the minimum working example below. . If is run it on a grid of `(4,4,4)` it works fine or if I use `FPlane`, it works fine. However, if I have 8 points in the vertical, it gives an error, which I have copied part of below, involving the size fo OffsetArrays. . Can someone help me understand how to fix this?. ```; using Oceananigans. grid = RectilinearGrid(CPU();; size = (4, 4, 8), halo = (3, 3, 3),; x = (-1, 1), y = (-1, 1), z = (-1, 0),; topology = (Periodic, Periodic, Bounded)); ; model = NonhydrostaticModel( grid = grid,; coriolis = NonTraditionalBetaPlane(fy=1e-4, fz=1e-4, β = 0, γ = 0,)). simulation = Simulation(model, Δt=1, stop_time=4); run!(simulation); ```. Error:. ```; julia> include(""mwe_nontraditional.jl""); [ Info: Initializing simulation...; [ Info: ... simulation initialization complete (188.650 μs); [ Info: Executing initial time step...; ERROR: LoadError: TaskFailedException. nested task error: BoundsError: attempt to access 10-element OffsetArray(::StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}, -2:7) with eltype Float64 with indices -2:7 at index [8]; Stacktrace:; [1] throw_boundserror(A::OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, I::Tuple{Int64}); @ Base ./abstractarray.jl:703; [2] overdub; @ ~/.julia/packages/KernelAbstractions/3ZHln/src/compiler.jl:51 [inlined]; [3] overdub; @ ./abstractarray.jl:668 [inlined]; [4] getindex(::OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, ::Int64); @ ~/.julia/packages/OffsetArrays/WvkHl/src/OffsetArrays.jl:435 [inlined]; [5] overdub; @ ~/.julia/packages/OffsetArrays/WvkHl/src/OffsetArrays.jl:435 [inlined]. ```",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2876:938,Load,LoadError,938,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2876,1,['Load'],['LoadError']
Performance,"I am not certain what the `ignore-cache` flag does, I borrowed the module loading sequence. I am pretty it ignores previously loaded modules with the same or similar name.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2256247465:34,cache,cache,34,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2256247465,3,"['cache', 'load']","['cache', 'loaded', 'loading']"
Performance,"I am on the fence. In the cubed sphere aquaplanet (not even MPI but just on one GPU) the gain of performance is a factor 5 by using the efficient split explicit rather than filling the halos at each substep. We do not have to tackle this problem here or now, but we have keep in mind that fill halo is very inefficient and probably not the way to go. Maybe @ali-ramadhan finds a better way to have a gpu-compatible code that does not require adding boundary conditions in the operators.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3268#issuecomment-2353966686:97,perform,performance,97,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3268#issuecomment-2353966686,1,['perform'],['performance']
Performance,"I am talking about changes to `Field`, which wraps around `OffsetArrays` so there is no effect on performance. We currently use `OffsetArrays`, not `Field`s, in our kernels. For the way we currently time-step this would also have no effect on GPU compatibility, because, again, we use `OffsetArrays` in our kernels, rather than `Field`s.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/457#issuecomment-542272226:98,perform,performance,98,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/457#issuecomment-542272226,1,['perform'],['performance']
Performance,"I am trying to run a script and when opening packages as below:; ```; ### Load in Packages; # using Pkg; # Pkg.instantiate(); using Oceananigans; using Oceananigans.AbstractOperations: @at, ∂x, ∂y, ∂z; using Oceananigans.AbstractOperations: @at, Average; using Oceananigans.Grids: Center, Face; using Oceananigans.Units; using Random; using Printf; using ArgParse; using CUDA: has_cuda_gpu; using CUDA ; using Oceanostics; ```; I get the following error:; ```; [32609] signal (11.1): Segmentation fault; in expression starting at /glade/u/home/knudsenl/.julia/packages/Oceanostics/cbCj1/src/Oceanostics.jl:35; Allocations: 575575 (Pool: 574672; Big: 903); GC: 1; ┌ Warning: You are using a non-official build of Julia. This may cause issues with CUDA.jl.; │ Please consider using an official build from https://julialang.org/downloads/.; └ @ CUDA ~/.julia/packages/CUDA/Tl08O/src/initialization.jl:180; ┌ Warning: CUDA runtime library `libcublasLt.so.12` was loaded from a system path, `/glade/u/apps/common/23.08/spack/opt/spack/cuda/12.2.1/lib64/libcublasLt.so.12`.; │; │ This may cause errors. Ensure that you have not set the LD_LIBRARY_PATH; │ environment variable, or that it does not contain paths to CUDA libraries.; │; │ In any other case, please file an issue.; └ @ CUDA ~/.julia/packages/CUDA/Tl08O/src/initialization.jl:219; ┌ Warning: CUDA runtime library `libnvJitLink.so.12` was loaded from a system path, `/glade/u/apps/common/23.08/spack/opt/spack/cuda/12.2.1/lib64/libnvJitLink.so.12`.; │; │ This may cause errors. Ensure that you have not set the LD_LIBRARY_PATH; ""BBL_O_10.out"" 47L, 2643B 1,0-1 Top; ```; How do I fix this? Thank you for any help; Edit: I have been working with @tomchor on trying to debug this issue",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3655:74,Load,Load,74,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3655,3,"['Load', 'load']","['Load', 'loaded']"
Performance,"I am using Oceananigans v0.54.0 with Julia v1.6 on GPU. I tried setting an initial condition in salinity as follows:. ```; Stop = 29.5; Sbot = 32; ztop = -35; zbot = -70; dSdz = (Sbot-Stop)/ztop; S_func(x, y, z) = Stop + dSdz*(z > ztop)*z ; set!(model, S=S_func); ```. and got the error copied below. Note that this works fine in Julia v1.5.4. . ```; ┌ Warning: You appear to be using MPI.jl with the default MPI binary on a cluster.; │ We recommend using the system-provided MPI, see the Configuration section of the MPI.jl docs.; └ @ MPI ~/.julia/packages/MPI/3q18R/deps/deps.jl:15; ERROR: LoadError: MethodError: no method matching set!(::Field{Center, Center, Center, OffsetArrays.OffsetArray{Float64, 3, CuArray{Float64, 3}}, RegularRectilinearGrid{Float64, Periodic, Periodic, Bounded, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}}}}, NamedTuple{(:x, :y, :z), Tuple{CoordinateBoundaryConditions{BoundaryCondition{Oceananigans.BoundaryConditions.Periodic, Nothing}, BoundaryCondition{Oceananigans.BoundaryConditions.Periodic, Nothing}}, CoordinateBoundaryConditions{BoundaryCondition{Oceananigans.BoundaryConditions.Periodic, Nothing}, BoundaryCondition{Oceananigans.BoundaryConditions.Periodic, Nothing}}, CoordinateBoundaryConditions{BoundaryCondition{Flux, Nothing}, BoundaryCondition{Flux, CuArray{Float64, 2}}}}}}, ::typeof(S_func)); Closest candidates are:; set!(::Oceananigans.Fields.AbstractField, !Matched::Number) at /home/guptam/.julia/packages/Oceananigans/SPGnT/src/Fields/set!.jl:14; set!(::Oceananigans.Fields.AbstractField{X, Y, Z, A, G} where G, !Matched::Oceananigans.Fields.AbstractField{X, Y, Z, A, G} where G) where {X, Y, Z, A} at /home/guptam/.julia/packages/Oceananigans/SPGnT/src/Fields/set!.jl:16; set!(!Matched::Oceananigans.Fields.AbstractField{X, Y, Z, A, G} where {X, Y, Z, A<:(OffsetArrays.OffsetArray{T, D, var""#s203""} where {T, D, var""#s203""<:Array}), G}, ::Function) at /home/guptam/.julia/p",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1555:592,Load,LoadError,592,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1555,1,['Load'],['LoadError']
Performance,"I attempted to run the [eady_turbulence.jl](https://github.com/CliMA/Oceananigans.jl/blob/master/examples/eady_turbulence.jl) example with `TwoDimensionalLeith()`closure as follows:. ```; closure = (AnisotropicDiffusivity(νh=0, κh=0, νz=κᵥ, κz=κᵥ),; #AnisotropicBiharmonicDiffusivity(νh=κ₄h, κh=κ₄h)); TwoDimensionalLeith()); ```. and got the error posted below. I am using Oceananigans.jl v0.40.0. . ```; ERROR: LoadError: TaskFailedException:; MethodError: no method matching ∂ⱼ_2ν_Σ₁ⱼ(::Int64, ::Int64, ::Int64, ::RegularCartesianGrid{Float64,Periodic,Periodic,Bounded,OffsetArrays.OffsetArray{Float64,1,StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}}}, ::Clock{Float64}, ::TwoDimensionalLeith{Float64,NamedTuple{(:b,),Tuple{Float64}},NamedTuple{(:b,),Tuple{Float64}}}, ::NamedTuple{(:u, :v, :w),Tuple{OffsetArrays.OffsetArray{Float64,3,Array{Float64,3}},OffsetArrays.OffsetArray{Float64,3,Array{Float64,3}},OffsetArrays.OffsetArray{Float64,3,Array{Float64,3}}}}, ::NamedTuple{(:νₑ,),Tuple{OffsetArrays.OffsetArray{Float64,3,Array{Float64,3}}}}); Closest candidates are:; ∂ⱼ_2ν_Σ₁ⱼ(::Any, ::Any, ::Any, ::AbstractGrid, ::Any, ::Tuple{C1,C2}, ::Any, ::Any) where {C1, C2} at /home/guptam/.julia/packages/Oceananigans/g8qkN/src/TurbulenceClosures/closure_tuples.jl:13; ∂ⱼ_2ν_Σ₁ⱼ(::Any, ::Any, ::Any, ::AbstractGrid{FT,TX,TY,TZ} where TZ where TY where TX, ::Any, ::Tuple, ::Any, ::Any, ::Any...) where FT at /home/guptam/.julia/packages/Oceananigans/g8qkN/src/TurbulenceClosures/TurbulenceClosures.jl:110; ∂ⱼ_2ν_Σ₁ⱼ(::Any, ::Any, ::Any, ::Any, ::Any, ::IsotropicDiffusivity, ::Any, ::Any...) at /home/guptam/.julia/packages/Oceananigans/g8qkN/src/TurbulenceClosures/turbulence_closure_implementations/isotropic_diffusivity.jl:53; ...; Stacktrace:; [1] call at /home/guptam/.julia/packages/Cassette/158rp/src/context.jl:456 [inlined]; [2] fallback at /home/guptam/.julia/packages/Cassette/158rp/src/context.jl:454 [inlined]; [3] _overdub_fallback at /home/guptam/.juli",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1034:413,Load,LoadError,413,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1034,1,['Load'],['LoadError']
Performance,I believe #1770 does the trick:. ```; [2021/06/25 18:04:55.066] INFO Writing Advection_schemes_relative_performance_(CPU).html...; Advection schemes relative performance (GPU); ┌───────────────┬────────────────────────┬──────────┬─────────┬─────────┐; │ Architectures │ Schemes │ slowdown │ memory │ allocs │; ├───────────────┼────────────────────────┼──────────┼─────────┼─────────┤; │ GPU │ CenteredFourthOrder │ 1.36629 │ 1.07711 │ 1.66944 │; │ GPU │ CenteredSecondOrder │ 1.0 │ 1.0 │ 1.0 │; │ GPU │ UpwindBiasedFifthOrder │ 1.53522 │ 1.11266 │ 1.9781 │; │ GPU │ UpwindBiasedThirdOrder │ 1.31322 │ 1.03505 │ 1.30432 │; │ GPU │ WENO5 │ 1.84272 │ 1.1889 │ 2.64008 │. ```. would be good to get confirmation from someone.,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1764#issuecomment-868903480:158,perform,performance,158,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1764#issuecomment-868903480,1,['perform'],['performance']
Performance,"I believe there's some issue with initialization. It doesn't help to rebuild the individual jobs, because the initialization only happens once and the results are cached are re-used within each build. I think somehow we have to start a new build with a new ID.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3580#issuecomment-2371783629:163,cache,cached,163,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3580#issuecomment-2371783629,1,['cache'],['cached']
Performance,"I can't quite identify where this issue is coming from but if you load a `FieldTimeSeries` in the normal way, i.e.:; ```julia; u = FieldTimeSeries(""path.jld2"", ""u"");; ```; when it gets adapted the `data` and `times` are not adapted to GPU arrays if the `FieldTimeSeries` is adapted to `GPUAdaptedFieldTimeSeries` when it's passed to a model etc. If instead I specify the grid like:; ```julia; u = FieldTimeSeries(""path.jld2"", ""u""; grid);; ```; then the data is correctly adapted but the `times` are still a regular vector. I can of course manually create the adapted time series like:; ```julia; u = FieldTimeSeries(""path.jld2"", ""u""; grid);; u_adapt = GPUAdaptedFieldTimeSeries{Face, Center, Center, eltype(grid)}(arch_array(arch, u.data), arch_array(arch, u.times)); ```; and also defining:; ```julia; adapt_structure(to, fts::GPUAdaptedFieldTimeSeries{LX, LY, LZ, FT}) where {LX, LY, LZ, FT} = ; GPUAdaptedFieldTimeSeries{LX, LY, LZ, FT}(adapt(to, fts.data),; adapt(to, fts.times)); ```; but this should probably be part of the default behaviour.",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3472:66,load,load,66,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3472,1,['load'],['load']
Performance,"I can't seem to construct a `Float32` hydrostatic model with CATKE. I'm pretty sure this used to work but I can't figure out when this error started happening or why. But it's happening as of the current `main` branch or v0.93.0 on two different machines. I guess with CUDA illegal memory access errors, they tend to occur after the illegal memory access has actually occured so the stacktrace might not be useful. MWE:. ```julia; using Oceananigans; using Oceananigans.TurbulenceClosures: CATKEVerticalDiffusivity. grid = LatitudeLongitudeGrid(; GPU(),; Float32,; topology = (Bounded, Bounded, Bounded),; size = (16, 16, 16),; longitude = (0, 1),; latitude = (0, 1),; z = (-100, 0); ). model = HydrostaticFreeSurfaceModel(;; grid,; buoyancy = BuoyancyTracer(),; tracers = (:b, :e),; closure = CATKEVerticalDiffusivity(Float32); ); ```. Error:. ```julia; ERROR: LoadError: CUDA error: an illegal memory access was encountered (code 700, ERROR_ILLEGAL_ADDRESS); Stacktrace:; [1] throw_api_error(res::CUDA.cudaError_enum); @ CUDA ~/.julia/packages/CUDA/2kjXI/lib/cudadrv/libcuda.jl:30; [2] check; @ ~/.julia/packages/CUDA/2kjXI/lib/cudadrv/libcuda.jl:37 [inlined]; [3] cuStreamGetCaptureInfo; @ ~/.julia/packages/CUDA/2kjXI/lib/utils/call.jl:34 [inlined]; [4] capture_status(stream::CUDA.CuStream); @ CUDA ~/.julia/packages/CUDA/2kjXI/lib/cudadrv/graph.jl:174; [5] is_capturing (repeats 2 times); @ ~/.julia/packages/CUDA/2kjXI/lib/cudadrv/graph.jl:179 [inlined]; [6] checked_cuModuleLoadDataEx(_module::Base.RefValue{Ptr{CUDA.CUmod_st}}, image::Ptr{UInt8}, numOptions::Int64, options::Vector{CUDA.CUjit_option_enum}, optionValues::Vector{Ptr{Nothing}}); @ CUDA ~/.julia/packages/CUDA/2kjXI/lib/cudadrv/module.jl:17; [7] CUDA.CuModule(data::Vector{UInt8}, options::Dict{CUDA.CUjit_option_enum, Any}); @ CUDA ~/.julia/packages/CUDA/2kjXI/lib/cudadrv/module.jl:60; [8] CuModule; @ ~/.julia/packages/CUDA/2kjXI/lib/cudadrv/module.jl:49 [inlined]; [9] link(job::GPUCompiler.CompilerJob, compiled::@NamedTupl",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3870:862,Load,LoadError,862,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3870,1,['Load'],['LoadError']
Performance,"I changed `∂xᶠᵃᵃ` to use `Δxᶠᵃᵃ` and when I try and compute the `x` derivative of the free surface in the `x` momentum equation I get the following error. It seems to me like we need to generalize this function, and others, to work on immersed grids. I pressume this is what the other models do? Looks like the fix could be easy. ```; ERROR: LoadError: TaskFailedException. nested task error: MethodError: no method matching ∂xᶠᵃᵃ(::Int64, ::Int64, ::Int64, ::ImmersedBoundaryGrid{Float64, Periodic, Bounded, Flat, RegularRectilinearGrid{Float64, Periodic, Bounded, Flat, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}}}}, GridFittedBoundary{typeof(bump)}}, ::typeof(Oceananigans.Models.ShallowWaterModels.gh2), ::Field{Center, Center, Center, CPU, OffsetArrays.OffsetArray{Float64, 3, Array{Float64, 3}}, ImmersedBoundaryGrid{Float64, Periodic, Bounded, Flat, RegularRectilinearGrid{Float64, Periodic, Bounded, Flat, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}}}}, GridFittedBoundary{typeof(bump)}}, Float64, NamedTuple{(:x, :y, :z), Tuple{CoordinateBoundaryConditions{BoundaryCondition{Oceananigans.BoundaryConditions.Periodic, Nothing}, BoundaryCondition{Oceananigans.BoundaryConditions.Periodic, Nothing}}, CoordinateBoundaryConditions{BoundaryCondition{Oceananigans.BoundaryConditions.Flux, Nothing}, BoundaryCondition{Oceananigans.BoundaryConditions.Flux, Nothing}}, CoordinateBoundaryConditions{Nothing, Nothing}}}}, ::Float64); Closest candidates are:; ∂xᶠᵃᵃ(::Any, ::Any, ::Any, ::AbstractRectilinearGrid, ::F, ::Any...) where F<:Function at /home/fpoulin/software/Oceananigans.jl/src/Operators/derivative_operators.jl:16; ∂xᶠᵃᵃ(::Any, ::Any, ::Any, ::AbstractRectilinearGrid, ::Any) at /home/fpoulin/software/Oceananigans.jl/src/Operators/derivative_operators.jl:6; ```",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1663#issuecomment-849719531:342,Load,LoadError,342,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1663#issuecomment-849719531,1,['Load'],['LoadError']
Performance,"I completely agree. I'd even go further and suggest a section for simulation tips in general, and then a subsection for GPU tips specifically. As an example, I noticed that defining as many things as `const` as possible helps with performance, even if I'm running on a CPU. It seems kinda obvious in hindsight, but it took me a few months to think of that.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1478#issuecomment-800468178:231,perform,performance,231,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1478#issuecomment-800468178,1,['perform'],['performance']
Performance,"I copied one of the errors below. I remember seeing this error before but I'm not sure how it was resolved. Restarting the tests?. ```; Precompiling project...;   | ✓ Oceananigans;   | 1 dependency successfully precompiled in 87 seconds (200 already precompiled);   | Testing Running tests...;   | ERROR: LoadError: LoadError: SystemError: opening file ""/data5/glwagner/.julia-7803/compiled/v1.6/Oceananigans/hU93i_V4y9F.ji"": No such file or directory;   | Stacktrace:; ...; ```",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2607#issuecomment-1157668343:305,Load,LoadError,305,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2607#issuecomment-1157668343,2,['Load'],['LoadError']
Performance,"I could not get the Poisson pressure solver to work on the GPU. Most of it works but CUDA does not have a DCT function so I had to perform the DCT/IDCT in terms of the FFT/IFFT. The DCT/IDCT functions work in isolation (regression tested with `FFTW.r2r!`, see link to Jupyter notebook below) but not in the Poisson solver. More specifically, the IDCT fails when applied to the third dimension (after or before the IFFT is applied to dimensions 1 and 2). For now I got around this by copying the right hand side to the CPU, doing the transform on the CPU, and copying the geopotential back to the GPU. This operation is so much slower than the time stepping that it takes up like 98%+ of wall clock time. It might also be introducing further numerical errors. Link to current Poisson GPU solver:; https://github.com/ali-ramadhan/Oceananigans.jl/blob/93aa0038b3126470f263475d648bceb9562bbe91/src/spectral_solvers.jl#L421. Messy Jupyter notebook: [Testing DCT/IDCT on the GPU](https://github.com/ali-ramadhan/random-jupyter-notebooks/blob/master/Oceananigans.jl/DCT%2BIDCT%20from%20DFT%2BIDFT.ipynb). Messy Jupyter notebook: [Testing GPU Poisson solver](https://github.com/ali-ramadhan/random-jupyter-notebooks/blob/master/Oceananigans.jl/GPU/Testing%20GPU%20Poisson%20solver.ipynb)",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/56:131,perform,perform,131,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/56,1,['perform'],['perform']
Performance,"I couldn't run docs locally to make sure all is good... running ; ```bash; julia --project=docs/ -e 'using Pkg; Pkg.instantiate(); Pkg.develop(PackageSpec(path=pwd()))'; julia --project=docs/ docs/make.jl;; ```. returns. ```bash; ...; [ Info: Expanding citation: Kundu15.; [ Info: CheckDocument: running document checks.; [ Info: Populate: populating indices.; ERROR: LoadError: `makedocs` encountered an error. Terminating build; Stacktrace:; [1] error(::String) at ./error.jl:33; [2] runner(::Type{Documenter.Builder.RenderDocument}, ::Documenter.Documents.Document) at /Users/navid/.julia/packages/Documenter/pjwqp/src/Builder.jl:255; [3] dispatch(::Type{Documenter.Builder.DocumentPipeline}, ::Documenter.Documents.Document) at /Users/navid/.julia/packages/Documenter/pjwqp/src/Utilities/Selectors.jl:167; [4] #2 at /Users/navid/.julia/packages/Documenter/pjwqp/src/Documenter.jl:241 [inlined]; [5] cd(::Documenter.var""#2#3""{Documenter.Documents.Document}, ::String) at ./file.jl:104; [6] #makedocs#1 at /Users/navid/.julia/packages/Documenter/pjwqp/src/Documenter.jl:240 [inlined]; [7] top-level scope at /Users/navid/Research/Oceananigans.jl/docs/make.jl:141; [8] include(::Function, ::Module, ::String) at ./Base.jl:380; [9] include(::Module, ::String) at ./Base.jl:368; [10] exec_options(::Base.JLOptions) at ./client.jl:296; [11] _start() at ./client.jl:506; in expression starting at /Users/navid/Research/Oceananigans.jl/docs/make.jl:141; ```. which I couldn't trace down...",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1072:368,Load,LoadError,368,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1072,1,['Load'],['LoadError']
Performance,"I d like to try to improve performance a bit first... Anyways, by next week I ll merge",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2603#issuecomment-1190872511:27,perform,performance,27,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2603#issuecomment-1190872511,1,['perform'],['performance']
Performance,"I do hope that I found something useful but at the moment I am a bit confued as to what's going wrong. I'm going to copy the errors below so others can see this more easily. The error in the docs complains about `PlotUtils` failing to precompile. That doesn't seem related to shallow water so I am confused. The CPU test seems to be with `MPI`, but I didn't know we had any `MPI` tests that used shallow water that were being run. Docs:; ```; ERROR: could not load library ""/storage7/buildkite-agent/julia-1.5.4/lib/julia/sys.so""; --; &nbsp; | /storage7/buildkite-agent/julia-1.5.4/lib/julia/sys.so: ELF load command past end of file; &nbsp; | ERROR: LoadError: Failed to precompile PlotUtils [995b91a9-d308-5afd-9ec6-746e21dbc043] to /storage7/buildkite-agent/.julia-2556/compiled/v1.5/PlotUtils/YveHG_R3lk8.ji.; &nbsp; | Stacktrace:; &nbsp; | [1] top-level scope at none:2; &nbsp; | [2] eval at ./boot.jl:347 [inlined]; &nbsp; | in expression starting at /storage7/buildkite-agent/.julia-2556/packages/Plots/SjqWU/src/Plots.jl:20; &nbsp; | ERROR: LoadError: Failed to precompile Plots [91a5bcdd-55d7-5caf-9e0b-520d859cae80] to /storage7/buildkite-agent/.julia-2556/compiled/v1.5/Plots/ld3vC_R3lk8.ji.; &nbsp; | in expression starting at /storage7/buildkite-agent/builds/tartarus-mit-edu-1/clima/oceananigans/docs/make.jl:6; &nbsp; | 🚨 Error: The command exited with status 1. ```. CPU test; ```; [8] test() at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Pkg/src/API.jl:72; --; &nbsp; | [9] top-level scope at none:1; &nbsp; | Union{},Union{},Tuple{},NamedTuple{(test(::Pkg.Types.Context, ::Array{Pkg.Types.PackageSpec,1}; ERROR: failed process: Process(`/storage7/buildkite-agent/.julia-2556/artifacts/2fcd463fb9498f362be9d1c4ef70a63c920b0e96/bin/mpiexec -np 4 /storage7/buildkite-agent/julia-1.5.4/bin/julia -O0 --color=yes -e 'using Pkg; Pkg.test()'`, ProcessExited(1)) [1]; &nbsp; | &nbsp;; &nbsp; | Stacktrace:; &nbsp; | [1] pipeline_error at ./process.jl:525 [inlined]",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1662#issuecomment-842643141:460,load,load,460,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1662#issuecomment-842643141,3,"['Load', 'load']","['LoadError', 'load']"
Performance,"I don't follow everything here but it looks like the simulation is being performed on a login node in the above screenshot. @Sumanshekhar17, your cluster may have a policy in place to stop (""kill"") jobs that run on a login node. To sort out script vs cluster issues I suggest running the script on a local machine (for example, your laptop) first. If it runs to completion, and also starts and runs on the cluster with `architecture=GPU()`, then we know the problem is due to cluster policy or some other cluster-specific setting, rather than a problem with your script or Oceananigans.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1289#issuecomment-756149003:73,perform,performed,73,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1289#issuecomment-756149003,1,['perform'],['performed']
Performance,"I don't know exactly what to do, so we'll need help. I've started with a post to the julia slack: https://julialang.slack.com/archives/C67910KEH/p1635909337281400. On the other hand, it may not be a huge issue if it only affects diagnostics that are evaluated fewer than 5 times. For long running simulations, our methods do eventually get cached? Do you have an example of a method that doesn't get cached after 5 evaluations?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2024#issuecomment-958631268:340,cache,cached,340,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2024#issuecomment-958631268,2,['cache'],['cached']
Performance,"I don't know if this helps to make the issue clearer, but I just re-ran the code for just ; ```; using Oceananigans ; ```; and it executed without error. I ran it again to see if it was just a fluke and I got the following error:; ```. The following have been reloaded with a version change:; 1) cuda/12.2.1 => cuda/11.8.0. [59837] signal (11.1): Segmentation fault; in expression starting at /glade/derecho/scratch/knudsenl/BottomBoundaryLayer/testcode.jl:1; Allocations: 605144 (Pool: 604223; Big: 921); GC: 1; /var/spool/pbs/mom_priv/jobs/1741845.casper-pbs.SC: line 31: 59837 Segmentation fault (core dumped) julia --project testcode.jl /glade/derecho/scratch/knudsenl/BottomBoundaryLayer/; ```; Could it be an issue with the computer loading Oceananigans.jl with a bunch of extra functions?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2237133645:739,load,loading,739,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2237133645,1,['load'],['loading']
Performance,"I don't know. Increasing grid points to 4, there's still a problem with 6 threads. ```julia; (base) gregorywagner:Oceananigans.jl/ (main✗) $ JULIA_NUM_THREADS=6 julia --project race_condition_test.jl [19:56:49]; [ Info: Oceananigans will use 6 threads; [ Info: Initializing simulation...; [ Info: ... simulation initialization complete (79.416 ms); [ Info: Executing initial time step...; [ Info: ... initial time step complete (6.660 seconds).; [ Info: Simulation is stopping. Model iteration 100 has hit or exceeded simulation stop iteration 100.; (parent(simulation.model.velocities.u))[1, 1, :] = [1.9557581998545617, 1.9557581998545617, 1.956214574857873, 1.9553566305291932, 1.9553371609848056, 1.9553371609848056]; Test Failed at /Users/gregorywagner/Projects/test/Oceananigans.jl/race_condition_test.jl:17; Expression: (parent(simulation.model.velocities.u))[1, 1, 2] == (parent(simulation.model.velocities.u))[1, 1, 3]; Evaluated: 1.9557581998545617 == 1.956214574857873; ERROR: LoadError: There was an error during testing; in expression starting at /Users/gregorywagner/Projects/test/Oceananigans.jl/race_condition_test.jl:17; ```. and also with 4. But with 3,. ```julia; (base) gregorywagner:Oceananigans.jl/ (main✗) $ JULIA_NUM_THREADS=3 julia --project race_condition_test.jl [19:58:49]; [ Info: Oceananigans will use 3 threads; [ Info: Initializing simulation...; [ Info: ... simulation initialization complete (98.396 ms); [ Info: Executing initial time step...; [ Info: ... initial time step complete (6.548 seconds).; [ Info: Simulation is stopping. Model iteration 100 has hit or exceeded simulation stop iteration 100.; (parent(simulation.model.velocities.u))[1, 1, :] = [1.9543734841879783, 1.9543734841879783, 1.9543734841879783, 1.9560232965664703, 1.9567081251492398, 1.9567081251492398]; ```",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2809#issuecomment-1308177839:988,Load,LoadError,988,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2809#issuecomment-1308177839,1,['Load'],['LoadError']
Performance,"I don't suspect this is the best place to post this but I'll start here and happy to move the conversation over to wherever it should be happening. I have created a new branch called ""ShallowWaterS1waveeqn"". S1 is for step 1. It doesn't do much right now but I'm trying to set up the skeleton so that we can actually do something intereting, like evolve the equation. At the moment I have the following:. - Created a ShallowWaterModels folder. - Created a ShallowWaterModels.jl module. It doesn't do much except include shallowwater_model.jl. - shallowwater_model.jl defines a mutable struct and a function. I couldn't get a lot of things to work but it does regularlize the boundary condtions, set up the velocities and a new variable called layer_depth. The name is not great and can go with depth or height, as people prefer. It will be in in the governing equations. - Created an example, in the same folder, called onedim_shallowwater.jl. Currently, this includes the module mentioned above, defines a grid (yes, a single point!), defines the initial conditions and sets the velocity. Questions: . 1. This is pretty bare bones so far but does this follow the philosophy of Oceananigans?; 2. I have tried to inititalze the layer_depth by adding an argument into the set! command but it failed. Below is the command that I thought would work and below is the error because layer_depth is not model.velocities or model.tracers. Can someone help me to see what silliness I am doing?. ``` ; set!(model, u = u, v = v, layer_deth = layer_depth); fails with ""ERROR: LoadError: ArgumentError: name layer_depth not found in model.velocities or model.tracers.""",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1165#issuecomment-728964105:1563,Load,LoadError,1563,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1165#issuecomment-728964105,1,['Load'],['LoadError']
Performance,"I don't think it's the tests that are slow. One of the tests involves time stepping a model for 10 time steps and it prints the wall clock time taken for each time step. It's only 2-3 ms so it's executing the tests at the expected speed. I've looked into the logs and it seems to be busy building packages. It's no different than what Travis and GitLab CI do, so maybe this just takes much longer on Windows CI? If we reduce the number of build dependencies that should help then. And if we can cache the builds then that would be excellent.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/89#issuecomment-468291280:495,cache,cache,495,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/89#issuecomment-468291280,1,['cache'],['cache']
Performance,"I don't think we should depend on a heavy package like Plots.jl, but we might want to automatically install and load it when someone runs an example like the rising thermal bubble one on the README. Might be related to #14 so Requires.jl would help but we also want to add/build the package too.",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/195:112,load,load,112,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/195,1,['load'],['load']
Performance,"I found this issue [here](https://github.com/JuliaGPU/CUDA.jl/issues/84) that seems to discuss the problem that we had with this PR (or at least my limited understanding of the problem). I went through the steps and I think they fixed it as it worked for me. . @ali-ramadhan , do you know which norm failed on a `CuArray` or did they all fail?. ```; julia> using LinearAlgebra, CUDA. julia> x=cu([1.,2.]); 2-element CuArray{Float32,1}:; 1.0; 2.0. julia> norm(x); 2.236068f0. julia> norm(x,2); ┌ Warning: Performing scalar operations on GPU arrays: This is very slow, consider disallowing these operations with `allowscalar(false)`; └ @ GPUArrays ~/.julia/packages/GPUArrays/WV76E/src/host/indexing.jl:43; 2.236068f0. julia> norm(x,0); 2.0f0. julia> norm(x,1); 3.0f0. julia> norm(x,4); 2.030543f0. julia> norm(x,Inf); 2.0f0; ```",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1562#issuecomment-817778463:504,Perform,Performing,504,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1562#issuecomment-817778463,1,['Perform'],['Performing']
Performance,"I get a `BoundsError` when running the following MWE using the latest version of Oceananigans:. ```julia; using Oceananigans. Nx=Ny=Nz=10. z_faces(k) = k/Nz; grid = RectilinearGrid(topology=(Bounded, Bounded, Bounded),; size=(Nx, Ny, Nz),; x=(0,1), y=(0,1), ; z=z_faces,; halo=(3,3,3),; ). advection = WENO(grid=grid, order=7); ```. The error I get is:. ```julia; ERROR: LoadError: BoundsError: attempt to access 19-element OffsetArray(::Vector{Float64}, -3:15) with eltype Float64 with indices -3:15 at index [-4]; Stacktrace:; [1] throw_boundserror(A::OffsetArrays.OffsetVector{Float64, Vector{Float64}}, I::Tuple{Int64}); @ Base ./abstractarray.jl:651; [2] checkbounds; @ ./abstractarray.jl:616 [inlined]; [3] getindex; @ ~/.julia/packages/OffsetArrays/80Lkc/src/OffsetArrays.jl:435 [inlined]; [4] #1; @ ./none:0 [inlined]; [5] MappingRF; @ ./reduce.jl:93 [inlined]; [6] FilteringRF; @ ./reduce.jl:105 [inlined]; [7] _foldl_impl(op::Base.FilteringRF{Oceananigans.Advection.var""#2#4""{Int64, Int64}, Base.MappingRF{Oceananigans.Advection.var""#1#3""{Int64, Int64, OffsetArrays.OffsetVector{Float64, Vector{Float64}}, OffsetArrays.OffsetVector{Float64, Vector{Float64}}, Int64, typeof(-)}, Base.BottomRF{typeof(Base.mul_prod)}}}, init::Base._InitialValue, itr::UnitRange{Int64}); @ Base ./reduce.jl:58; [8] foldl_impl; @ ./reduce.jl:48 [inlined]; [9] mapfoldl_impl; @ ./reduce.jl:44 [inlined]; [10] #mapfoldl#214; @ ./reduce.jl:160 [inlined]; [11] mapfoldl; @ ./reduce.jl:160 [inlined]; [12] #mapreduce#218; @ ./reduce.jl:287 [inlined]; [13] mapreduce; @ ./reduce.jl:287 [inlined]; [14] #prod#225; @ ./reduce.jl:582 [inlined]; [15] prod; @ ./reduce.jl:582 [inlined]; [16] num_prod; @ ~/.julia/packages/Oceananigans/W63bs/src/Advection/reconstruction_coefficients.jl:31 [inlined]; [17] #18; @ ./none:0 [inlined]; [18] MappingRF; @ ./reduce.jl:93 [inlined]; [19] (::Base.FilteringRF{Oceananigans.Advection.var""#19#23""{Int64}, Base.MappingRF{Oceananigans.Advection.var""#18#22""{Int64, typeof(-), Int64, Noth",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2717:371,Load,LoadError,371,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2717,1,['Load'],['LoadError']
Performance,I got good results in the past on discretized PDE problems. The main bottleneck is to perform the sparse triangular solves at each iteration of the Krylov method (`ldiv!`) on GPU but I did some operators in `KrylovPreconditioners.jl` to reuse the analysis of the sparsity pattern as well as the buffers:; - https://github.com/JuliaSmoothOptimizers/KrylovPreconditioners.jl/blob/main/ext/CUDA/operators.jl#L82; - https://github.com/JuliaSmoothOptimizers/KrylovPreconditioners.jl/blob/main/ext/AMDGPU/operators.jl#L103; - https://github.com/JuliaSmoothOptimizers/KrylovPreconditioners.jl/blob/main/ext/oneAPI/operators.jl#L50. We can also apply the same strategy for the ILU(0) preconditioner provided by the GPU vendors.,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3789#issuecomment-2374729281:69,bottleneck,bottleneck,69,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3789#issuecomment-2374729281,2,"['bottleneck', 'perform']","['bottleneck', 'perform']"
Performance,"I got it. ```julia; julia> using Oceananigans; Precompiling Oceananigans; 1 dependency successfully precompiled in 11 seconds. 139 already precompiled. julia> grid = RectilinearGrid(size=1, x=(0, 1), topology=(Periodic, Flat, Flat)); 1×1×1 RectilinearGrid{Float64, Periodic, Flat, Flat} on CPU with 1×0×0 halo; ├── Periodic x ∈ [0.0, 1.0) regularly spaced with Δx=1.0; ├── Flat y; └── Flat z. julia> grid = ImmersedBoundaryGrid(grid, GridFittedBoundary(x -> true)); warning: /Users/gregorywagner/.julia/packages/KernelAbstractions/MAxUm/src/cpu.jl:118:0: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering; 1×1×1 ImmersedBoundaryGrid{Float64, Periodic, Flat, Flat} on CPU with 1×0×0 halo:; ├── immersed_boundary: GridFittedBoundary{Field{Center, Center, Center, Nothing, RectilinearGrid{Float64, Periodic, Flat, Flat, Float64, Float64, Float64, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, Nothing, Nothing, CPU}, Tuple{Colon, Colon, Colon}, OffsetArrays.OffsetArray{Bool, 3, Array{Bool, 3}}, Bool, FieldBoundaryConditions{BoundaryCondition{Oceananigans.BoundaryConditions.Periodic, Nothing}, BoundaryCondition{Oceananigans.BoundaryConditions.Periodic, Nothing}, Nothing, Nothing, Nothing, Nothing, BoundaryCondition{Oceananigans.BoundaryConditions.Flux, Nothing}}, Nothing, Oceananigans.Fields.FieldBoundaryBuffers{Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing}}}; ├── underlying_grid: 1×1×1 RectilinearGrid{Float64, Periodic, Flat, Flat} on CPU with 1×0×0 halo; ├── Periodic x ∈ [0.0, 1.0) regularly spaced with Δx=1.0; ├── Flat y; └── Flat z. julia> c = CenterField(grid); 1×1×1 Field{Center, Center, Center} on ImmersedBoundaryGrid on CPU; ├── grid: 1×1×1 ImmersedBoundaryGrid{Float64, Periodic, Flat, Flat} on CPU with 1×0×0 halo; ├── boundary conditions: ",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3742#issuecomment-2314132900:578,optimiz,optimizer,578,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3742#issuecomment-2314132900,2,"['optimiz', 'perform']","['optimizer', 'perform']"
Performance,"I guess in our terminology ""saving"" is a bit more restricted than ""serializing"". When we ""save"" an object, we do it in a way that might be readable from Python or MATLAB (for example). This means that we only ""save"" common types like numbers, arrays, strings. Anything we output to NetCDF is ""saved"". When we ""serialize"" we might embed a Julia struct in a file. So when we serialize something we can only load it back from Julia. We can only serialize with JLD2.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2245#issuecomment-1039401633:405,load,load,405,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2245#issuecomment-1039401633,1,['load'],['load']
Performance,"I have a start to this soon to be PR (I hope) but nothing working yet. . One issue that comes up is that `automatic_halo_sizing.jl` should not extend the halo if the topology is flat This is easy enough to set this up in principle, however, when I try telling it what `Flat` means using the following `using Oceananigans.Grids: Flat`, it fails. . How should I load `Flat` at this stage?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1499#issuecomment-803738292:360,load,load,360,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1499#issuecomment-803738292,1,['load'],['load']
Performance,"I have been playing with the one_dimensional_diffusion.jl example and have found something a bit odd. When I run it, with everything already installed, it seems to complain about [line18](https://github.com/CliMA/Oceananigans.jl/blob/a343711f1101b1a433124f5f9697ce60b1011a40/examples/one_dimensional_diffusion.jl#L18) with the following error,. ```; ERROR: LoadError: package `Oceananigans [9e8cae18]` has same name or UUID as the active project; ```. Is this expected? . Is this something we want to avoid?",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1205:357,Load,LoadError,357,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1205,1,['Load'],['LoadError']
Performance,"I have been running on the system Julia for these, I attempted to do my own install but I do not think I ended up doing much with it. I believe `gcc` is a dependency for the subsequent modules as for example when I run:; ```; module --force purge; module load ncarenv/23.10 ; module load ncarcompilers/1.0.0; ```; the last line returns ; ```; Lmod has detected the following error: These module(s) or; extension(s) exist but cannot be loaded as requested: ""ncarcompilers/1.0.0""; Try: ""module spider ncarcompilers/1.0.0"" to see how to load the module(s).; ```; which does not happen if I include `gcc`. I was also able to run my code without netcdf!",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2258913846:255,load,load,255,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2258913846,4,['load'],"['load', 'loaded']"
Performance,"I have checked thoroughly _every_ function in the advection module for a `checkbounds()` call and couldn't produce any Bounds error. This is remaining a mystery to me because it seems that the whole advection module elides bounds checking. On the other hand, I found that `main` has the same problem, on the `near_global_quarter_degree.jl` experiment this is the wall time per 10 time steps; (with `--check-bounds=no`, without `--check-bounds=no`); main -> 2.7 / 3.6 s; this PR -> 3.3 / 4.4 s. so there is for sure a problem of performance (which I will try to solve) but it does not seem to be related to bounds checking as; `2.7 / 3.6 = 3.3 / 4.4 = 0.75`. It is a must to find out where these bounds checks are eating up 25% of our computational time (do you know an easy way to profile it?).; By the way @tomchor, is this also the ratio that you find in your simulations (maybe with the latest commit)? If yes, I ll give up trying to inbound and only try to speed up the advection.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2603#issuecomment-1186040507:528,perform,performance,528,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2603#issuecomment-1186040507,1,['perform'],['performance']
Performance,"I have continued to retest the 2D turbulence example and now get some loop warnings from several calls with the CPU architecture:; It would be nice to remove these or fix a possible flaw. I am using the latest versions of Julia and Oceananigans as at 5th March 2024. model = NonhydrostaticModel(; grid,; timestepper = :RungeKutta3,; advection = UpwindBiasedFifthOrder(),; closure = ScalarDiffusivity(ν=1e-5)); warning: C:\Users\kab\.julia\packages\KernelAbstractions\Zcyra\src\extras\loopinfo.jl:28:0: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering; warning: C:\Users\kab\.julia\packages\KernelAbstractions\Zcyra\src\extras\loopinfo.jl:28:0: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering; warning: C:\Users\kab\.julia\packages\KernelAbstractions\Zcyra\src\extras\loopinfo.jl:28:0: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering; warning: C:\Users\kab\.julia\packages\KernelAbstractions\Zcyra\src\extras\loopinfo.jl:28:0: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering; warning: C:\Users\kab\.julia\packages\KernelAbstractions\Zcyra\src\extras\loopinfo.jl:28:0: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering; warning: C:\Users\kab\.julia\packages\KernelAbstractions\Zcyra\src\extras\loopinfo.jl:28:0: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3494:525,optimiz,optimizer,525,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3494,4,"['optimiz', 'perform']","['optimizer', 'perform']"
Performance,"I have created a new branch called ""ShallowWaterS1waveeqn"". S1 is for step 1. It doesn't do much right now but I'm trying to set up the skeleton so that we can actually do something intereting, like evolve the equation. At the moment I have the following:. 1. Created a ShallowWaterModels folder. 2. Created a ShallowWaterModels.jl module. It doesn't do much except include shallowwater_model.jl. 3. shallowwater_model.jl defines a mutable struct and a function. I couldn't get a lot of things to work but it does regularlize the boundary condtions, set up the velocities and a new variable called layer_depth. The name is not great and can go with depth or height, as people prefer. It will be in in the governing equations. 4. Created an example, in the same folder, called onedim_shallowwater.jl. Currently, this includes the module mentioned above, defines a grid (yes, a single point!), defines the initial conditions and sets the velocity. Questions:. 1. This is pretty bare bones so far but does this follow the philosophy of Oceananigans?. 2. I have tried to inititalze the layer_depth by adding an argument into the set! command but it failed. Below is the command that I thought would work and below is the error because layer_depth is not model.velocities or model.tracers. Can someone help me to see what silliness I am doing?. ```; set!(model, u = u, v = v, layer_deth = layer_depth); fails with ""ERROR: LoadError: ArgumentError: name layer_depth not found in model.velocities or model.tracers."". After I get the initial conditions defined I then need to specify the fluxes for the PDE. That should not be difficult but one issue is the nomenclature. The vector we have now is velocities, but we actually want, velocity transport. I should use and define hu and hv instead of u and v. That should be easy enough, I think.",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1188:1417,Load,LoadError,1417,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1188,1,['Load'],['LoadError']
Performance,"I have moved the open fill to nominally be called within `fill_halo_regions!` as the new format (`fill_halo_regions!` and `fill_open_boundary_regions!` both having to be called separately) is confusing. I think a user would expect `fill_halo_regions!` to fill all of the halos. Instead `fill_halo_regions!` has a kwarg `fill_open_boundaries` which is nominally true, and I have set to `false` in the halo fill following the pressure correction. If we didn't do this we would also have needed to add loads of calls to `fill_open_boundary_regions!` in the hydrostatic model code, which would have confused matters.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3482#issuecomment-2150059461:499,load,loads,499,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3482#issuecomment-2150059461,1,['load'],['loads']
Performance,I have never tried benchmarking this. maybe the gain in performance is negligible. I guess it will depend on the number of particles and the size of the grid.,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3356#issuecomment-1775560431:56,perform,performance,56,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3356#issuecomment-1775560431,1,['perform'],['performance']
Performance,"I have put together an example for my own benefit (not to be merged into master unless people want it to be) that looks at the simulation of inertial instability in 2D. See [here](https://github.com/CliMA/Oceananigans.jl/blob/fjp/inertia-instability-example/examples/inertially_unstable_jet.jl) for the code. . I believe everything is working on a CPU but I am having two difficulties with GPUs. . 1. In line 39, where I define the background buoyancy of the jet, I can't use `coriolis.f` because CUDA seems to need a global variable and this doesn't cut it. I am presently using `f` instead and this works, but should this work? `ERROR: LoadError: InvalidIRError: compiling kernel gpu__compute!`; 2. When I make a simulation from a gpu calculation, I get that the buoyancy perturbation has perturbations at the top and bottom, which are not physical. Which is to say that it has different boundary conditions. I didn't actually specify the boundary conditions differently but just wanted a solid top and bottom. If I specify the boundary conditions explicitly, should this fix the problem? Maybe that's a fix but is this expected behaviour?. https://user-images.githubusercontent.com/8239041/113899192-8d851900-979a-11eb-97a7-b7f8085864b7.mp4",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1554:638,Load,LoadError,638,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1554,1,['Load'],['LoadError']
Performance,"I have some validation scripts already that work on the CPU and indicate that the new closure is working. However, I can't make this work on the GPU. I keep getting this error:. ```; ERROR: LoadError: GPU compilation of MethodInstance for Oceananigans.TurbulenceClosures.gpu__compute_scale_invariant_smagorinsky_viscosity!(::KernelAbstractions.CompilerMetadata{…}, ::OffsetArrays.OffsetArray{…}, ::Field{…}, ::Field{…}, ::RectilinearGrid{…}, ::ScaleInvariantSmagorinsky{…}, ::Nothing, ::@NamedTuple{…}, ::@NamedTuple{}) failed; KernelError: passing and using non-bitstype argument. Argument 7 to your kernel function is of type ScaleInvariantSmagorinsky{Oceananigans.TurbulenceClosures.ExplicitTimeDiscretization, Oceananigans.TurbulenceClosures.DirectionalAveraging{Tuple{Int64, Int64}}, Float64, @NamedTuple{}, Integer}, which is not isbits:; .update_frequency is of type Integer which is not isbits.; ```. Reading up on the CUDA.jl docs I _think_ I understand where this error comes from (although I thought `update_frequency`, which is an `Integer`, should work, but it throws an error). Still couldn't figure out how to fix it in this case here. I assume it's not hard to fix though, so I was wondering if someone (I'm assuming @simone-silvestri or @glwagner) can please give me a hand or at least point me in the right direction?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3642#issuecomment-2294992357:190,Load,LoadError,190,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3642#issuecomment-2294992357,1,['Load'],['LoadError']
Performance,I haven't looked at performance / GPU compilation in detail. I do think there is a type inference issue somewhere in this PR now because the flow over hills experiment is 7-8x slower on my laptop even without any immersed boundary condition. This likely indicates a problem with type inference in the flux divergence function (might also prevent GPU compilation). So we'll have to solve that and also add tests for CPU + GPU...,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2437#issuecomment-1100751806:20,perform,performance,20,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2437#issuecomment-1100751806,1,['perform'],['performance']
Performance,"I hit these issues when setting a field on an immersed boundary grid.; ```; ERROR: LoadError: MethodError: no method matching ξname(::ImmersedBoundaryGrid{Float64, Periodic, RightConnected, Bounded, OrthogonalSphericalShellGrid{Float64, Periodic, RightConnected, Bounded, OffsetArrays.OffsetMatrix{Float64, CUDA.CuArray{Float64, 2, CUDA.DeviceMemory}}, OffsetArrays.OffsetVector{Float64, CUDA.CuArray{Float64, 1, CUDA.DeviceMemory}}, OffsetArrays.OffsetVector{Float64, CUDA.CuArray{Float64, 1, CUDA.DeviceMemory}}, OrthogonalSphericalShellGrids.Tripolar{Int64, Int64, Int64}, GPU}, GridFittedBottom{Field{Center, Center, Nothing, Nothing, OrthogonalSphericalShellGrid{Float64, Periodic, RightConnected, Bounded, OffsetArrays.OffsetMatrix{Float64, CUDA.CuArray{Float64, 2, CUDA.DeviceMemory}}, OffsetArrays.OffsetVector{Float64, CUDA.CuArray{Float64, 1, CUDA.DeviceMemory}}, OffsetArrays.OffsetVector{Float64, CUDA.CuArray{Float64, 1, CUDA.DeviceMemory}}, OrthogonalSphericalShellGrids.Tripolar{Int64, Int64, Int64}, GPU}, Tuple{Colon, Colon, Colon}, OffsetArrays.OffsetArray{Float64, 3, CUDA.CuArray{Float64, 3, CUDA.DeviceMemory}}, Float64, FieldBoundaryConditions{BoundaryCondition{Oceananigans.BoundaryConditions.Periodic, Nothing}, BoundaryCondition{Oceananigans.BoundaryConditions.Periodic, Nothing}, BoundaryCondition{Flux, Nothing}, BoundaryCondition{OrthogonalSphericalShellGrids.Zipper, Int64}, Nothing, Nothing, Oceananigans.BoundaryConditions.DefaultBoundaryCondition{BoundaryCondition{Flux, Nothing}}}, Nothing, Oceananigans.Fields.FieldBoundaryBuffers{Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing}}, Oceananigans.ImmersedBoundaries.CenterImmersedCondition}, CUDA.CuArray{Tuple{UInt16, UInt16, UInt16}, 1, CUDA.DeviceMemory}, CUDA.CuArray{Tuple{UInt16, UInt16}, 1, CUDA.DeviceMemory}, GPU}). Closest candidates are:; ξname(!Matched::LatitudeLongitudeGrid); @ Oceananigans ~/.julia/packages/Oceananigans/O8Ult/src/Grids/latitude_longitude_grid.jl:574; ξname(!Match",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3742:83,Load,LoadError,83,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3742,1,['Load'],['LoadError']
Performance,"I implemented a simple new validation test that runs a two-dimensional barotropic turbulence problem with `ExplicitFreeSurface`: https://github.com/CliMA/Oceananigans.jl/blob/ss/multi_region/validation/multi_region/multi_region_turbulence.jl. Here's some miscellaneous notes:. * `WENO5(vector_invariant=VelocityStencil())` is faster than `WENO5()`. Note that when we write ""WENO5(vector_invariant=stencil)"" we mean that we are using the vector invariant formulation of momentum with a WENO reconstruction for vorticity, using either ""velocity"" or ""vorticity"" in the WENO smoothness metric. The ""WENO, Vector Invariant"" scheme is probably faster because it has fewer WENO interpolations (just one per momentum component rather than 2).; * `WENO5(vector_invariant=VelocityStencil())` blows up with `MultiRegionGrid`. Some timings:. | Resolution | Grid | Advection scheme | Wall time for 1000 time steps |; | ------------- | ------------- | -- | -- |; | 128^2 | `RegularRectilinearGrid` | `WENO5()` | 3.9 s |; | 128^2 | `MultiRegionGrid` | `WENO5()` | 7.4 s |; | 128^2 | `RegularRectilinearGrid` | `WENO5(vector_invariant=VelocityStencil())` | 2.8 s |; | 256^2 | `RegularRectilinearGrid` | `WENO5()` | 14.3 s |; | 256^2 | `MultiRegionGrid ` | `WENO5()` | 18.9 s |; | 256^2 | `RegularRectilinearGrid` | `WENO5(vector_invariant=VelocityStencil())` | 10.3 s |; | 512^2 | `RegularRectilinearGrid` | `WENO5()` | 56.3 s |; | 512^2 | `MultiRegionGrid ` | `WENO5()` | 62.3 s |; | 512^2 | `RegularRectilinearGrid` | `WENO5(vector_invariant=VelocityStencil())` | 40.3 s |. I propose that we. 1) Close the gap between multi-region and single-region performance?; 2) Understand why WENO vector invariant blows up on a multi region grid.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2253#issuecomment-1107942730:1635,perform,performance,1635,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2253#issuecomment-1107942730,1,['perform'],['performance']
Performance,"I just noticed that the Nusselt number calculation result in the horizontal convection example changed between v0.71.6 and v0.72.4. It almost doubled from around 10 at. https://clima.github.io/OceananigansDocumentation/v0.71.6/generated/horizontal_convection/#Load-saved-output,-process,-visualize. to around 20 in . https://clima.github.io/OceananigansDocumentation/v0.72.4/generated/horizontal_convection/#Load-saved-output,-process,-visualize. The calculation is done via. ```Julia; χ_diff = κ * b★^2 * π * tanh(2π * H / Lx). for i = 1:length(t); b = b_timeseries[i]; sum!(∫ⱽ_mod²_∇b, (∂x(b)^2 + ∂z(b)^2) * volume); Nu[i] = (κ * ∫ⱽ_mod²_∇b[1, 1, 1]) / χ_diff; end; ```. Did anything changed between these two version in the way `sum!` or `volume` works?. Here's the diff between v0.71.6 and v0.72.4: https://github.com/CliMA/Oceananigans.jl/compare/v0.71.6...v0.72.4",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2735:260,Load,Load-saved-output,260,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2735,2,['Load'],['Load-saved-output']
Performance,I just noticed:. https://buildkite.com/clima/oceananigans/builds/11955#018909c3-4c60-4b9c-b4e2-c6260d8b2189/40-2662. ```; [ Info: shallow_water_Bickley_jet.jl example took 28.452 minutes to build.; ```. We should optimize that or make it run first!,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3151#issuecomment-1613995023:213,optimiz,optimize,213,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3151#issuecomment-1613995023,1,['optimiz'],['optimize']
Performance,"I just pushed changes for the geostrophic adjustment case. The code seems alright, but it returns the following warning:. `warning: ~/.julia/packages/KernelAbstractions/GCOhX/src/extras/loopinfo.jl:28:0: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering`. Then it returns the following error:. <details>; <summary>Error output:</summary>. ```; ERROR: ERROR: LoadError: ERROR: LoadError: LoadError: ERROR: LoadError: MethodError: no method matching MethodError: no method matching MethodError: iterate(::SplitExplicitFreeSurface{Field{Center, Center, Face, Nothing, ImmersedBoundaryGrid{Float64, FullyConnected, Periodic, Bounded, RectilinearGrid{Float64, FullyConnected, Periodic, Bounded, Float64, Float64, Float64, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, Distributed{CPU, false, Partition{Int64, Int64, Int64}, Tuple{Int64, Int64, Int64}, Int64, Tuple{Int64, Int64, Int64}, Oceananigans.DistributedComputations.RankConnectivity{Int64, Int64, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing}, MPI.Comm, Vector{MPI.Request}, Base.RefValue{Int64}}}, GridFittedBottom{Field{Center, Center, Nothing, Nothing, RectilinearGrid{Float64, FullyConnected, Periodic, Bounded, Float64, Float64, Float64, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{F",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3429#issuecomment-1892087212:227,optimiz,optimizer,227,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3429#issuecomment-1892087212,6,"['Load', 'optimiz', 'perform']","['LoadError', 'optimizer', 'perform']"
Performance,"I just tried running `validation/periodic_advection.jl` and see that it can't find WENO, which is I think because we removed the N-th order WENO code. Is it better to change it to WENO5 for the moment, since that's what we have?. ```; julia> include(""periodic_advection.jl""); [ Info: Precompiling Oceananigans [9e8cae18-63c1-5223-a75c-80ca9d6e9a09]; ERROR: LoadError: UndefVarError: WENO not defined; Stacktrace:; [1] top-level scope at /home/fpoulin/software/Oceananigans.jl/validation/periodic_advection/periodic_advection.jl:55; [2] include(::String) at ./client.jl:457; [3] top-level scope at REPL[1]:1; in expression starting at /home/fpoulin/software/Oceananigans.jl/validation/periodic_advection/periodic_advection.jl:55; ```",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1271:357,Load,LoadError,357,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1271,1,['Load'],['LoadError']
Performance,"I just updated my packages and now I get an error .... UndefVarError: RectilinearGrid not defined; Stacktrace:; [1] top-level scope; @ In[8]:18; [2] eval; @ .\boot.jl:360 [inlined]; [3] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String); @ Base .\loading.jl:1116. ....with even the simples grid-defenition, e.g. ; ```; Nz = 24 # number of points in the vertical direction; Lz = 32 # domain depth; grid = RectilinearGrid(size = (32, 32, Nz),; x = (0, 64),; y = (0, 64),; z = (-64, 0)); ```; What am I doing wrong?",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2077:289,load,loading,289,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2077,1,['load'],['loading']
Performance,"I just want to point out that this involves an extra memory fetch so it might affect performance, probably very negligibly and maybe the compiler is smart enough to elide the extra node call, but it is an extra operation nonetheless.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3074#issuecomment-1516299701:85,perform,performance,85,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3074#issuecomment-1516299701,1,['perform'],['performance']
Performance,"I looked at one of the errors and saw the message below. I think this means we need to restart the tests. ```; ERROR: LoadError: LoadError: SystemError: opening file ""/data5/glwagner/.julia-8057/compiled/v1.6/Oceananigans/hU93i_xHskz.ji"": No such file or directory;  ```",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2645#issuecomment-1178207246:118,Load,LoadError,118,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2645#issuecomment-1178207246,2,['Load'],['LoadError']
Performance,"I looked through the package and it seems like things are already in CuArrays then. https://github.com/climate-machine/Oceananigans.jl/blob/master/src/time_steppers.jl#L42-L78. this should quite readily port over to using DifferentialEquations.jl. It looks like you're using an IMEXEuler scheme? I think there would be some pretty good performance gains, and it would be interesting to start being able to use this entire package as a benchmark.",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/391:336,perform,performance,336,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/391,1,['perform'],['performance']
Performance,"I mean, we can move the docs to the caltech cluster but I think they will slow down a lot. This is a bottleneck for us right now so I don't think we can afford to move them... Notice that the out of memory error doesn't occur when we are using the GPU. We only use the GPU for the one quick start example --- and for nothing else. If we want to ""solve"" this, we can just get rid of the quick start example and then return to the previous behavior where we set `CUDA_VISIBLE_DEVICES=-1` for the docs build. Another solution is to hide / prevent tartarus users from using GPU 0.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3779#issuecomment-2356553645:101,bottleneck,bottleneck,101,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3779#issuecomment-2356553645,1,['bottleneck'],['bottleneck']
Performance,"I mean, we could actually provide this implementation ourself with something like. ```julia; u = TimeseriesField(filepath, ""u""). u[i] # returns `Field` at save point `i`. u[i, a, b, c] # calls `getindex(file[""output/u""][string(i)], a, b, c)`; ``` . actually with `TimeseriesField` we could also automatically wrap data loaded from file into the appropriate field type as well.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/963#issuecomment-702105112:319,load,loaded,319,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/963#issuecomment-702105112,1,['load'],['loaded']
Performance,"I noticed ""On the other hand, a Z-WENO formulation is *always* beneficial (also in case of a uniform mesh) with no major; decrease in performance."". Why don't we then change `zweno = true`?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2060#issuecomment-976013319:134,perform,performance,134,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2060#issuecomment-976013319,1,['perform'],['performance']
Performance,"I noticed that some issues with buildkite have to do with needing to re-resolve the Manifest, so this is yet another attempt to fix the race condition in our CI...",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3783:136,race condition,race condition,136,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3783,1,['race condition'],['race condition']
Performance,"I often run Oceananigans on a cluster at my university that has a few nodes with IBM Power9. Since very few people use these nodes, it is a big advantage to be able to run Oceananigans there because they will almost always have the GPUs available. Almost everything seems to run as expected, but I cannot use `NetCDFOutputWriter`. I really prefer saving the output in NC files because this is more easily used by other programming languages. ```; UndefVarError: NetCDFOutputWriter not defined. Stacktrace:; [1] top-level scope; @ :0; [2] eval; @ ./boot.jl:368 [inlined]; [3] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String); @ Base ./loading.jl:1428; ```. I noticed that if I try to `using NCDatasets`, it returns. ```; ERROR: InitError: UndefVarError: libnetcdf not defined; Stacktrace:; [1] nc_inq_libvers; @ ~/.julia/packages/NCDatasets/h1epE/src/netcdf_c.jl:242 [inlined]; [2] netcdf_version(); @ NCDatasets ~/.julia/packages/NCDatasets/h1epE/src/netcdf_c.jl:2157; [3] init_certificate_authority(); @ NCDatasets ~/.julia/packages/NCDatasets/h1epE/src/netcdf_c.jl:2170; [4] __init__(); @ NCDatasets ~/.julia/packages/NCDatasets/h1epE/src/NCDatasets.jl:33; [5] _include_from_serialized(pkg::Base.PkgId, path::String, depmods::Vector{Any}); @ Base ./loading.jl:831; [6] _require_search_from_serialized(pkg::Base.PkgId, sourcepath::String, build_id::UInt64); @ Base ./loading.jl:1039; [7] _require(pkg::Base.PkgId); @ Base ./loading.jl:1315; [8] _require_prelocked(uuidkey::Base.PkgId); @ Base ./loading.jl:1200; [9] macro expansion; @ ./loading.jl:1180 [inlined]; [10] macro expansion; @ ./lock.jl:223 [inlined]; [11] require(into::Module, mod::Symbol); @ Base ./loading.jl:1144; during initialization of module NCDatasets; ```. It looks like the problem is because `NCDatasets` needs `NetCDF_jll`, which is not available for Power9.; https://github.com/JuliaBinaryWrappers/NetCDF_jll.jl#platforms. Any way to work around this?; Maybe using `NetCDF.jl`?",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2840:678,load,loading,678,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2840,7,['load'],['loading']
Performance,"I opened a wiki for Oceananigans:. https://github.com/CliMA/Oceananigans.jl/wiki. I think we should use the wiki to host practical information and tips for using Oceananigans on various hardware (laptops, CPUs, GPUs), clusters / high-performance computing system (HPCs), and more. Using the wiki this way will allow us to focus the docs on code and numerics, and alleviate the need to submit PRs to update details about using clusters. We need to provide an introduction to Oceananigans ""knowledge base"" (where to find useful information in the documentation, wiki, Github Discussions, and Github Issues) in the README, the docs, and the wiki.",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2470:234,perform,performance,234,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2470,1,['perform'],['performance']
Performance,I opened an issue about the `Int128`/`UInt128` segfault (https://github.com/JuliaGPU/CUDA.jl/issues/793) but will revisit this PR later to look into the performance regression.,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1514#issuecomment-809568648:153,perform,performance,153,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1514#issuecomment-809568648,1,['perform'],['performance']
Performance,"I ran the `benchmark_incompressible_model.jl` script on the master branch (twice) and this branch (also twice), and ; actually see a tiny bit of a speedup, maybe only significant for larger CPU models though. Hard to say whether it's noise, it might be more due to other processes causing small variations in runtime. To me I don't think this PR slows down or speeds up the code, but it simplifies and improves the time stepping code so it should be merged. There's a few more memory allocations now (due to extra kernel launches) but this shouldn't affect performance. # System info. ```; Oceananigans v0.44.1; Julia Version 1.5.2; Commit 539f3ce943 (2020-09-23 23:17 UTC); Platform Info:; OS: Linux (x86_64-pc-linux-gnu); CPU: Intel(R) Xeon(R) Silver 4214 CPU @ 2.20GHz; WORD_SIZE: 64; LIBM: libopenlibm; LLVM: libLLVM-9.0.1 (ORCJIT, cascadelake); GPU: TITAN V; ```. # Master branch; ```; Incompressible model benchmarks; ┌───────────────┬─────────────┬─────┬────────────┬────────────┬────────────┬────────────┬────────────┬────────┐; │ Architectures │ Float_types │ Ns │ min │ median │ mean │ max │ memory │ allocs │; ├───────────────┼─────────────┼─────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────┤; │ CPU │ Float32 │ 32 │ 5.399 ms │ 5.668 ms │ 5.758 ms │ 7.186 ms │ 242.42 KiB │ 1876 │; │ CPU │ Float32 │ 64 │ 36.710 ms │ 37.583 ms │ 37.974 ms │ 41.678 ms │ 242.42 KiB │ 1876 │; │ CPU │ Float32 │ 128 │ 312.780 ms │ 313.477 ms │ 313.622 ms │ 314.726 ms │ 242.42 KiB │ 1876 │; │ CPU │ Float32 │ 256 │ 2.802 s │ 2.819 s │ 2.819 s │ 2.836 s │ 242.42 KiB │ 1876 │; │ CPU │ Float64 │ 32 │ 5.828 ms │ 6.049 ms │ 6.157 ms │ 7.044 ms │ 293.44 KiB │ 1876 │; │ CPU │ Float64 │ 64 │ 43.084 ms │ 43.619 ms │ 43.650 ms │ 44.363 ms │ 293.44 KiB │ 1876 │; │ CPU │ Float64 │ 128 │ 365.051 ms │ 365.317 ms │ 365.475 ms │ 366.288 ms │ 293.44 KiB │ 1876 │; │ CPU │ Float64 │ 256 │ 3.602 s │ 3.653 s │ 3.653 s │ 3.703 s │ 293.44 KiB │ 1876 │; │ GPU │ Float32 │ 32 │ 2.797 ms │ 2.870 ms ",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1210#issuecomment-736692263:557,perform,performance,557,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1210#issuecomment-736692263,1,['perform'],['performance']
Performance,"I ran the advection scheme benchmarks and comparing with some older Julia 1.5 results it definitely is slower on the GPU. WENO5 used to only be ~3x slower than CenteredSecondOrder, but now it's 26x slower. All other advection schemes are just as fast as they used to be. Not slow enough to be CUDA scalar operations so maybe the GPU compiler changed in some way that kernels calling/using WENO5 are compiling into suboptimal machine code?. @maleadt might have some ideas/suggestions but maybe we just have to profile and find the new bottleneck?. ---. ```; Advection schemes relative performance (GPU); ┌───────────────┬────────────────────────┬──────────┬─────────┬─────────┐; │ Architectures │ Schemes │ slowdown │ memory │ allocs │; ├───────────────┼────────────────────────┼──────────┼─────────┼─────────┤; │ GPU │ CenteredFourthOrder │ 1.38356 │ 1.05911 │ 1.60067 │; │ GPU │ CenteredSecondOrder │ 1.0 │ 1.0 │ 1.0 │; │ GPU │ UpwindBiasedFifthOrder │ 1.53145 │ 1.0868 │ 1.88203 │; │ GPU │ UpwindBiasedThirdOrder │ 1.30611 │ 1.04135 │ 1.42012 │; │ GPU │ WENO5 │ 26.1429 │ 4.68526 │ 38.4468 │; └───────────────┴────────────────────────┴──────────┴─────────┴─────────┘; ```. Compare with: https://github.com/CliMA/Oceananigans.jl/pull/1169#issuecomment-725471594",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1764#issuecomment-868093699:534,bottleneck,bottleneck,534,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1764#issuecomment-868093699,2,"['bottleneck', 'perform']","['bottleneck', 'performance']"
Performance,"I ran the benchmark again with triply periodic but it's still much slower so the issue might be deeper than the logic in `topologically_conditional_interpolation.jl`. ```; Advection schemes relative performance (GPU); ┌───────────────┬────────────────────────┬──────────┬─────────┬─────────┐; │ Architectures │ Schemes │ slowdown │ memory │ allocs │; ├───────────────┼────────────────────────┼──────────┼─────────┼─────────┤; │ GPU │ CenteredFourthOrder │ 1.50326 │ 1.06836 │ 1.69674 │; │ GPU │ CenteredSecondOrder │ 1.0 │ 1.0 │ 1.0 │; │ GPU │ UpwindBiasedFifthOrder │ 1.69787 │ 1.09472 │ 1.96539 │; │ GPU │ UpwindBiasedThirdOrder │ 1.39899 │ 1.05598 │ 1.57057 │; │ GPU │ WENO5 │ 33.2728 │ 5.21273 │ 43.9286 │; └───────────────┴────────────────────────┴──────────┴─────────┴─────────┘; ```. ```diff; diff --git a/benchmark/benchmark_advection_schemes.jl b/benchmark/benchmark_advection_schemes.jl; index 81b083e1..e6ba8cd6 100644; --- a/benchmark/benchmark_advection_schemes.jl; +++ b/benchmark/benchmark_advection_schemes.jl; @@ -7,7 +7,8 @@ using Benchmarks; # Benchmark function. function benchmark_advection_scheme(Arch, Scheme); - grid = RegularRectilinearGrid(size=(192, 192, 192), extent=(1, 1, 1)); + topo = (Periodic, Periodic, Periodic); + grid = RegularRectilinearGrid(topology=topo, size=(192, 192, 192), extent=(1, 1, 1)); model = IncompressibleModel(architecture=Arch(), grid=grid, advection=Scheme()); ; time_step!(model, 1) # warmup; ```",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1764#issuecomment-868122855:199,perform,performance,199,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1764#issuecomment-868122855,1,['perform'],['performance']
Performance,"I realize that https://github.com/CliMA/Oceananigans.jl/issues/3609 exists, but I think this is a different issue. The following MWE, which tries to advect `LagrangianParticle`s over an `ImmersedGrid` fails with this error:. ```; ERROR: LoadError: MethodError: no method matching cpu__advect_particles!(::KernelAbstractions.CompilerMetadata{…}, ::StructArrays.StructVector{…}, ::Float64, ::ImmersedBoundaryGrid{…}, ::Int64, ::@NamedTuple{…}). Closest candidates are:; cpu__advect_particles!(::Any, ::Any, ::Any, ::Oceananigans.Grids.AbstractUnderlyingGrid, ::Any, ::Any); @ Oceananigans none:0. Stacktrace:; [1] __thread_run(tid::Int64, len::Int64, rem::Int64, obj::KernelAbstractions.Kernel{…}, ndrange::Nothing, iterspace::KernelAbstractions.NDIteration.NDRange{…}, args::Tuple{…}, dynamic::KernelAbstractions.NDIteration.NoDynamicCheck); @ KernelAbstractions ~/.julia/packages/KernelAbstractions/60cqT/src/cpu.jl:144; [2] __run(obj::KernelAbstractions.Kernel{…}, ndrange::Nothing, iterspace::KernelAbstractions.NDIteration.NDRange{…}, args::Tuple{…}, dynamic::KernelAbstractions.NDIteration.NoDynamicCheck, static_threads::Bool); @ KernelAbstractions ~/.julia/packages/KernelAbstractions/60cqT/src/cpu.jl:111; [3] (::KernelAbstractions.Kernel{…})(::StructArrays.StructVector{…}, ::Vararg{…}; ndrange::Nothing, workgroupsize::Nothing); @ KernelAbstractions ~/.julia/packages/KernelAbstractions/60cqT/src/cpu.jl:46; [4] (::KernelAbstractions.Kernel{…})(::StructArrays.StructVector{…}, ::Vararg{…}); ```. This is the MWE:. ```julia; using Oceananigans. grid_base = RectilinearGrid(size = (4, 4, 4), extent = (1,1,1)); GFB = GridFittedBottom((x, y) -> -1/2); grid = ImmersedBoundaryGrid(grid_base, GFB). n_particles = 3; x₀ = rand(n_particles); y₀ = rand(n_particles); z₀ = .-rand(n_particles). lagrangian_particles = LagrangianParticles(x=x₀, y=y₀, z=z₀). model = NonhydrostaticModel(; grid, particles=lagrangian_particles); time_step!(model, 1); ```. I'm a bit confused, because, while the function `",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3761:237,Load,LoadError,237,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3761,1,['Load'],['LoadError']
Performance,"I recently ran the weak scaling shallow water model benchmark with the MultiGPU architecture on Satori, thanks to @christophernhill.; Here are the results:; <html>; <body>; <!--StartFragment-->. size | ranks | min | median | mean | max | memory | allocs | samples; -- | -- | -- | -- | -- | -- | -- | -- | --; (4096, 256) | (1, 1) | 2.765 ms | 2.786 ms | 2.849 ms | 3.374 ms | 2.03 MiB | 5535 | 10; (4096, 512) | (1, 2) | 6.932 ms | 7.081 ms | 8.037 ms | 26.174 ms | 2.03 MiB | 5859 | 20; (4096, 1024) | (1, 4) | 12.592 ms | 14.603 ms | 16.417 ms | 31.468 ms | 2.03 MiB | 5859 | 40. <!--EndFragment-->; </body>; </html>. <html>; <body>; <!--StartFragment-->. size | ranks | slowdown | efficiency | memory | allocs; -- | -- | -- | -- | -- | --; (4096, 256) | (1, 1) | 1.0 | 1.0 | 1.0 | 1.0; (4096, 512) | (1, 2) | 2.54127 | 0.393505 | 1.00271 | 1.05854; (4096, 1024) | (1, 4) | 5.24053 | 0.19082 | 1.00271 | 1.05854. <!--EndFragment-->; </body>; </html>. The results are not good but at least we can benchmark multi-GPU performance now.",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1882:1018,perform,performance,1018,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1882,1,['perform'],['performance']
Performance,"I recently received the error. ```; ERROR: LoadError: ArgumentError: The grid halo (3, 3, 3) must be at least equal to (4, 4, 4). Note that an ImmersedBoundaryGrid requires an extra halo point.; ```. It's not clear what ""extra halo point"" means. I think this error needs to be a little bit more specific like, ""Note that an ImmersedBoundaryGrid requires an extra halo point in all directions compared to a non-immersed grid.""",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2983:43,Load,LoadError,43,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2983,1,['Load'],['LoadError']
Performance,"I removed `show_axis=false` and it still produces an error, see below. . ```; fpoulin@pop-os:~/Software/Oceananigans.jl/validation/barotropic_gyre$ julia visualize_barotropic_gyre.jl ; libGL error: MESA-LOADER: failed to open iris: /usr/lib/dri/iris_dri.so: cannot open shared object file: No such file or directory (search paths /usr/lib/x86_64-linux-gnu/dri:\$${ORIGIN}/dri:/usr/lib/dri, suffix _dri); libGL error: failed to load driver: iris; libGL error: MESA-LOADER: failed to open iris: /usr/lib/dri/iris_dri.so: cannot open shared object file: No such file or directory (search paths /usr/lib/x86_64-linux-gnu/dri:\$${ORIGIN}/dri:/usr/lib/dri, suffix _dri); libGL error: failed to load driver: iris; libGL error: MESA-LOADER: failed to open swrast: /usr/lib/dri/swrast_dri.so: cannot open shared object file: No such file or directory (search paths /usr/lib/x86_64-linux-gnu/dri:\$${ORIGIN}/dri:/usr/lib/dri, suffix _dri); libGL error: failed to load driver: swrast; ┌ Warning: GLFW couldn't create an OpenGL window.; │ This likely means, you don't have an OpenGL capable Graphic Card,; │ or you don't have an OpenGL 3.3 capable video driver installed.; │ Have a look at the troubleshooting section in the GLMakie readme:; │ https://github.com/JuliaPlots/Makie.jl/tree/master/GLMakie#troubleshooting-opengl.; └ @ GLMakie ~/.julia/packages/GLMakie/XG7Hm/src/screen.jl:381; ERROR: LoadError: GLFWError (VERSION_UNAVAILABLE): GLX: Failed to create context: GLXBadFBConfig; Stacktrace:; [1] _ErrorCallbackWrapper(code::Int32, description::Cstring); @ GLFW ~/.julia/packages/GLFW/BWxfF/src/callback.jl:43; [2] CreateWindow(width::Int64, height::Int64, title::String, monitor::GLFW.Monitor, share::GLFW.Window); @ GLFW ~/.julia/packages/GLFW/BWxfF/src/glfw3.jl:499; [3] GLFW.Window(; name::String, resolution::Tuple{Int64, Int64}, debugging::Bool, major::Int64, minor::Int64, windowhints::Vector{Tuple{UInt32, Integer}}, contexthints::Vector{Tuple{UInt32, Integer}}, visible::Bool, focus::Bool, fulls",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2542#issuecomment-1123091985:203,LOAD,LOADER,203,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2542#issuecomment-1123091985,6,"['LOAD', 'load']","['LOADER', 'load']"
Performance,"I report here some issues connected to updating to Oceananigans 0.80.0 . ```; ERROR: LoadError: MethodError: no method matching min_Δx(::ImmersedBoundaryGrid{Float64, Periodic, Bounded, Bounded, LatitudeLongitudeGrid{Float64, Periodic, Bounded, Bounded, OffsetArrays.OffsetVector{Float64, CUDA.CuArray{Float64, 1, CUDA.Mem.DeviceBuffer}}, Float64, Float64, Float64, OffsetArrays.OffsetVector{Float64, CUDA.CuArray{Float64, 1, CUDA.Mem.DeviceBuffer}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVector{Float64, CUDA.CuArray{Float64, 1, CUDA.Mem.DeviceBuffer}}, GPU}, GridFittedBottom{OffsetArrays.OffsetMatrix{Float64, CUDA.CuArray{Float64, 2, CUDA.Mem.DeviceBuffer}}, Oceananigans.ImmersedBoundaries.CenterImmersedCondition}, Nothing, GPU}); Closest candidates are:; min_Δx(::RectilinearGrid) at /nfs/cnhlab001/ssilvest/julia_pkg/packages/Oceananigans/KTw3g/src/Grids/rectilinear_grid.jl:463; min_Δx(::OrthogonalSphericalShellGrid) at /nfs/cnhlab001/ssilvest/julia_pkg/packages/Oceananigans/KTw3g/src/Grids/orthogonal_spherical_shell_grid.jl:937; min_Δx(::LatitudeLongitudeGrid) at /nfs/cnhlab001/ssilvest/julia_pkg/packages/Oceananigans/KTw3g/src/Grids/latitude_longitude_grid.jl:653; ```; `min_Δx` is not defined for immersed boundary grid anymore. ```; ERROR: LoadError: Scalar indexing is disallowed.; Invocation of getindex resulted in scalar indexing of a GPU array.; This is typically caused by calling an iterating implementation of a method.; Such implementations *do not* execute on the GPU, but very slowly on the CPU,; and therefore are only permitted from the REPL for prototyping purposes.; If you did intend to index this array, annotate the caller with @allowscalar.; Stacktrace:; [1] error(s::String); @ Base ./error.jl:35; [2] assertscalar(op::String); @ ",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3038:85,Load,LoadError,85,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3038,1,['Load'],['LoadError']
Performance,"I saw there's a problem with making the documentation, see below. This doesn't seem directly relelvant to what was changed here though?. ```. ERROR: LoadError: `makedocs` encountered an error. Terminating build; --;   | Stacktrace:;   | [1] error(s::String);   | @ Base ./error.jl:33;   | [2] runner(#unused#::Type{Documenter.Builder.RenderDocument}, doc::Documenter.Documents.Document);   | @ Documenter.Builder /storage5/buildkite-agent/.julia-7245/packages/Documenter/7hBIS/src/Builder.jl:255;   | [3] dispatch(#unused#::Type{Documenter.Builder.DocumentPipeline}, x::Documenter.Documents.Document);   | @ Documenter.Utilities.Selectors /storage5/buildkite-agent/.julia-7245/packages/Documenter/7hBIS/src/Utilities/Selectors.jl:170;   | [4] #2;   | @ /storage5/buildkite-agent/.julia-7245/packages/Documenter/7hBIS/src/Documenter.jl:266 [inlined];   | [5] cd(f::Documenter.var""#2#3""{Documenter.Documents.Document}, dir::String);   | @ Base.Filesystem ./file.jl:106;   | [6] #makedocs#1;   | @ /storage5/buildkite-agent/.julia-7245/packages/Documenter/7hBIS/src/Documenter.jl:265 [inlined];   | [7] top-level scope;   | @ ~/builds/tartarus-3/clima/oceananigans/docs/make.jl:154;   | in expression starting at /var/lib/buildkite-agent/builds/tartarus-3/clima/oceananigans/docs/make.jl:154;   | 🚨 Error: The command exited with status 1;   | user command error: exit status 1. ```",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2522#issuecomment-1119854495:149,Load,LoadError,149,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2522#issuecomment-1119854495,1,['Load'],['LoadError']
Performance,"I see a few things to improve with the grid right now. 1. We need to add some kind of type information that indicates whether a given dimension is 'flat'. One way to do this could simply be to add the grid sizes in each dimension as parameters in the abstract type `Grid`, aka:. ```julia; abstract type Grid{T, Nx, Ny, Nz} end. struct RegularCartesianGrid{T, R, Nx, Ny, Nz} <: Grid{T, Nx, Ny, Nz}; ...; end; ```. Functions can then dispatch when one of `Nx`, etc is `1` (including halo-filling functions, which I think may fail when the size of the halo is 0). Another option is to use flags for each dimension rather than the actual size of the grid. In my opinion using the size makes the most sense. Using the actual size could have future advantages; for example, if some optimizations are possible when `Nz=2`. It is also nice to see the size of the grid from the type signature. A disadvantage is that we then could not have `mutable` grid types, but I'm not sure we want that. 2. There is a lot of redundant information in the `RegularCartesianGrid` struct: cell areas, volumes, total number of grid points, etc. I think it would be better --- meaning that our code would be shorter, easier to read, easier to maintain, easier to reason about (since storing them implies they *cannot* be computed, which is incorrect) and more computationally efficient --- to add functions that compute these quantities on the fly, rather than storing them in memory. Related: #287.",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/330:776,optimiz,optimizations,776,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/330,1,['optimiz'],['optimizations']
Performance,"I see that one of the errors is `UndefVarError: device_event not defined`, which I suppose means we need to load it?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1913#issuecomment-890011382:108,load,load,108,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1913#issuecomment-890011382,1,['load'],['load']
Performance,"I see the error you got, its. ```julia; ERROR: LoadError: MethodError: no method matching AdvectiveForcing(::WENO5{Float64, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, true}; w=-1); Closest candidates are:; AdvectiveForcing(::Any) at /Users/gregorywagner/Projects/dev/Oceananigans.jl/src/Forcings/advective_forcing.jl:40 got unsupported keyword argument ""w""; AdvectiveForcing(::Any, ::Any) at /Users/gregorywagner/Projects/dev/Oceananigans.jl/src/Forcings/advective_forcing.jl:40 got unsupported keyword argument ""w""; AdvectiveForcing(::Any, ::Any, ::Any) at /Users/gregorywagner/Projects/dev/Oceananigans.jl/src/Forcings/advective_forcing.jl:40 got unsupported keyword argument ""w""; ```. This error means that we do indeed have the name `AdvectiveForcing`, but that the function signature is wrong. The reason here is a bug in the source code (missing semicolon: https://github.com/CliMA/Oceananigans.jl/pull/2389/commits/b30ce7ab846c713663949c3bb6ee8d1e2ec740b0).",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2389#issuecomment-1082002537:47,Load,LoadError,47,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2389#issuecomment-1082002537,1,['Load'],['LoadError']
Performance,"I share here a double drake experiment that makes full use of this PR ; #### Grid setup; - latitude longitude grid from 75 S to 75 N; - 1/3 of a degree in the horizontal (1080 points in longitude, 450 in latitude); - 150 exponentially stretched vertical levels for a 3km deep ocean; - double drake bathymetry (https://doi.org/10.1175/2009JCLI3197.1); #### Top BC: ; - temperature: restoring to reference profile (cosine shape); - salinity: prescribed latitudinally dependent surface flux; - zonal velocity: prescribed latitudinally dependent wind stress; #### Bottom BC:; - velocities: linear bottom drag with a drag coefficient of 0.003 ms⁻¹; #### Initial conditions; - zero velocities; - exponentially stratified temperature with SST equal to the reference temperature; - constant salinity; #### Model setup; - linear equation of state; - Richardson-based diffusivity for BL mixing ; - vertical background viscosity and diffusivity of 5e-4 and 3e-5, respectively; - vector invariant momentum advection with WENO for vorticity and divergence flux as well as vertical transport (no horizontal viscosity); - WENO for tracer advection (no horizontal diffusivity); - Split explicit free surface using an averaging shape function and a CFL of 0.7 (23 substeps per time step); #### Simulation setup; - time step of 10 minutes; - ran on 2 MPI processes with CUDA-aware MPI; - performs about 10 simulated years per day. On the left, there is the free surface evolution, on the right the surface vertical vorticity (evolved for 9 years). https://user-images.githubusercontent.com/33547697/219039209-e99ad100-2730-4805-bf57-c3b438f64537.mp4",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2888#issuecomment-1431366185:1370,perform,performs,1370,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2888#issuecomment-1431366185,1,['perform'],['performs']
Performance,"I started to learn about how to make docs today and found the instructions [here](https://clima.github.io/OceananigansDocumentation/stable/contributing/) very helpful. . I commented out all of the examples except for two and then ran it but found a bunch of `docstring` errors. It turns out that when I set `doctest` and `strict` to `false`, then things work. But I presume there is still a problem that needs to be resolved. First, when I tried making the docs I found the following message. ```; [ Info: Populate: populating indices.; ERROR: LoadError: `makedocs` encountered an error. Terminating build; Stacktrace:; [1] error(::String) at ./error.jl:33; [2] runner(::Type{Documenter.Builder.RenderDocument}, ::Documenter.Documents.Document) at /home/fpoulin/.julia/packages/Documenter/lul8Y/src/Builder.jl:255; [3] dispatch(::Type{Documenter.Builder.DocumentPipeline}, ::Documenter.Documents.Document) at /home/fpoulin/.julia/packages/Documenter/lul8Y/src/Utilities/Selectors.jl:170; [4] #2 at /home/fpoulin/.julia/packages/Documenter/lul8Y/src/Documenter.jl:249 [inlined]; [5] cd(::Documenter.var""#2#3""{Documenter.Documents.Document}, ::String) at ./file.jl:104; [6] #makedocs#1 at /home/fpoulin/.julia/packages/Documenter/lul8Y/src/Documenter.jl:248 [inlined]; [7] top-level scope at /home/fpoulin/software/Oceananigans.jl/docs/make.jl:137; [8] include(::Function, ::Module, ::String) at ./Base.jl:380; [9] include(::Module, ::String) at ./Base.jl:368; [10] exec_options(::Base.JLOptions) at ./client.jl:296; [11] _start() at ./client.jl:506; in expression starting at /home/fpoulin/software/Oceananigans.jl/docs/make.jl:137; ```. Then when I tried it again I found a bunch of the following `Error: doctest failure in src/model_setup/output_writers.md:55-76`. . Anyone have an idea as to what I might be doing wrong?",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1455:544,Load,LoadError,544,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1455,1,['Load'],['LoadError']
Performance,I think @glwagner was able to run the 2D turbulence example with the `CenteredFourthOrder` advection scheme but we never performed a convergence test. It's possible a smaller time step is needed. @glwagner also suggested trying `AnisotropicBiharmonicDiffusivity`. Side note: Might be good to add two new convergence tests modified from; https://github.com/CliMA/Oceananigans.jl/blob/master/verification/convergence_tests/one_dimensional_gaussian_advection_diffusion.jl; https://github.com/CliMA/Oceananigans.jl/blob/master/verification/convergence_tests/one_dimensional_cosine_advection_diffusion.jl; to test advection only with `CenteredFourthOrder` to ensure we see fourth-order convergence. Advection-diffusion tests should only show 2nd order convergence.,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/925#issuecomment-690411199:121,perform,performed,121,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/925#issuecomment-690411199,1,['perform'],['performed']
Performance,"I think @tomchor is hitting on the right solution: we should separate ""output for analysis"" from ""output for checkpointing"", if possible. Typically we need output for analysis more frequently than output for checkpointing, and we don't require things like the tendency fields for analysis. This motivates creating multiple output writers that are dedicated to their specific tasks. . On the possibility of a NetCDF checkpointer: the first checkpointer design that utilized `restore_from_checkpoint` attempted to serialize as much of a model's data structures as possible. This requirement prevented us from using NetCDF and motivated using JLD2 (there are a few other data formats that can serialize, but JLD2 is fast, simple, and HDF5 compatible, which means it can be loaded from python with `h5py`). However, we now support a simpler paradigm whereby the original run script needs to be modified with `run!(simulation, pickup=true)` in order to ""pick up"" a simulation from a checkpoint. With this simpler design we require only all the prognostic fields and tendencies (for AB2) in the checkpointer file and we could in principle support NetCDF. This has the downside that it would require us to maintain more code. If you do need to inspect, plot, or analyze data in JLD2 files, check out the [JLD2 documentation](https://juliaio.github.io/JLD2.jl/dev/).",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1929#issuecomment-899754924:770,load,loaded,770,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1929#issuecomment-899754924,1,['load'],['loaded']
Performance,I think I'm happy with the finite volume operators in this PR. Happy to make changes based on feedback so we have versatile and flexible operators. In the next PR I will fully integrate them with the code and make sure they are correct and performant. It will also show if I missed any operators.,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/283#issuecomment-546648773:240,perform,performant,240,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/283#issuecomment-546648773,1,['perform'],['performant']
Performance,"I think high performance at 25 km resolution will prove difficult also because we are effectively dividing our kernel size by 1/6 (unless we figure out how to coalesce kernels across panels). On a large GPU this will lead to performance degredation at 25 km resolution, because even a single-panel kernel covering the whole globe at 25 km barely saturates one GPU. Recovering that performance for multi-region simulations may be difficult, especially in the face of the added complexity of distribution across multiple GPUs.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3201#issuecomment-1719393187:13,perform,performance,13,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3201#issuecomment-1719393187,3,['perform'],['performance']
Performance,"I think it might make sense to serialize grids in JLD2 files with `CPU` architecture by default; this way they can be loaded on systems without a GPU. . The downside is that users with GPUs would have to write. ```julia; cpu_grid = file[""serialized/grid""]; grid = on_architecture(GPU(), cpu_grid); ```. to load their grid on the GPU.",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2179:118,load,loaded,118,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2179,2,['load'],"['load', 'loaded']"
Performance,"I think it's a plotting issue. We are filling up the immersed boundaries with NaN and, apparently, we cannot plot NaNs anymore? The error says:; ```julia; ERROR: LoadError: On worker 2:;   | Looking up a non-finite or NaN value in a colormap is undefined.; ```",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3836#issuecomment-2407601775:162,Load,LoadError,162,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3836#issuecomment-2407601775,1,['Load'],['LoadError']
Performance,"I think it's ok, the hydrostatic model tests are not the bottleneck",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2181#issuecomment-1021681357:57,bottleneck,bottleneck,57,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2181#issuecomment-1021681357,1,['bottleneck'],['bottleneck']
Performance,"I think it's probable that `DiscreteForcing` doesn't have the same performance issues. @ali-ramadhan put together a benchmark script for forcing functions a while ago I thought, but it might have disappeared (because it wasn't informative?) That might've been before we had `ContinuousForcing` though.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1827#issuecomment-875640465:67,perform,performance,67,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1827#issuecomment-875640465,1,['perform'],['performance']
Performance,"I think our race condition test is passing after this without the need for a custom `mean!`, so I think this is good to go. It's probably important because there could be other bugs associated with bad interactions between operations on the CUDA default stream and broadcasting...",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1803#issuecomment-873180797:12,race condition,race condition,12,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1803#issuecomment-873180797,1,['race condition'],['race condition']
Performance,I think putting boundary conditions with a `nothing` default on `ComputedField`s makes a lot of sense. We preserve existing behavior and performance while allowing the flexibility for users to design a computation tree with intermediate steps that's correct everywhere.,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1130#issuecomment-721742077:137,perform,performance,137,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1130#issuecomment-721742077,1,['perform'],['performance']
Performance,"I think that, at least in theory, defining our dependency graph more explicitly (eg, optimizing ""waits"") should help us saturate the GPU. If the problem is overhead, I'm not sure what we can do?. All of the errors are of the form. ```julia; LoadError: AssertionError: length(__workgroupsize) <= length(ndrange); ```. They may all be column models that are launched with the group (1, 1, 16)...",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/805#issuecomment-662455799:85,optimiz,optimizing,85,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/805#issuecomment-662455799,2,"['Load', 'optimiz']","['LoadError', 'optimizing']"
Performance,"I think the `weno_interpolants.jl` file is the culprit, @tomchor and I saw the loss of performance specifically when using the WENO scheme. I'll test the other advection schemes in the meantime to make sure that it is indeed only a problem of WENO advection.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2603#issuecomment-1177983798:87,perform,performance,87,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2603#issuecomment-1177983798,1,['perform'],['performance']
Performance,"I think the double precision numbers are coming from Nx/Ny/Nz being turned into Int64 (even if I manually set them to be Int32) when the grid is constructed. ~~I can't work out exactly where the promotion is happening though.~~ This happens because `Nx` etc. are `::Int` type in the grid, so get promoted when the grid is made, if I change them to be like the floats and be `IT` instead then the promotion doesn't happen. When I try to make the model I get loads of other things converted to `Int64` though so still doesn't work.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2618#issuecomment-1729331099:457,load,loads,457,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2618#issuecomment-1729331099,1,['load'],['loads']
Performance,"I think the right way forward is to implement something that works for all grids. Then we can implement the grid-specific versions --- which should be viewed as conveniences or optimizations rather than necessities --- as time allows. I think this is a better and more efficient approach then implementing convenience versions first, and figuring out the general version later. You might find that the convenience versions aren't really necessary because things are convenient enough...",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3143#issuecomment-1637681533:177,optimiz,optimizations,177,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3143#issuecomment-1637681533,1,['optimiz'],['optimizations']
Performance,I think the way to go is to write a single kernel (3D or 2D with a loop in z). In this way you avoid the cost of multiple kernel launching. If you have a 3D kernel with non Local dependencies and you have to update nonlocal values (I don't Think it's the case) you have to make sure that there are no write race conditions,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2600#issuecomment-1147416320:307,race condition,race conditions,307,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2600#issuecomment-1147416320,1,['race condition'],['race conditions']
Performance,"I think there are already a couple of challenges for this pull request:; 1. Implementing user-defined forcing functions without losing performance.; 2. Getting the user-defined forcing functions to work on the GPU without losing performance either. As #59 will result in a ton of refactoring, I think it might be more appropriate to address it in a different branch after #73 is resolved, so that everything (`Forcing` is already `isbitstype` but no other code depends on it) can be converted into `isbitstype`. If doing so results in large performance drops (which I think might have happened here) then it might take a while to figure out #59.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/85#issuecomment-467654884:135,perform,performance,135,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/85#issuecomment-467654884,3,['perform'],['performance']
Performance,"I think there's still a use for `KernelComputedField`! It's more expensive to construct a calculation by nesting `ComputedField`s inside it. The difference is in the number of kernel launches: when expressing a calculation with a single kernel, the computation is computed with a single loop over all grid points. When expressing a calculation using intermediate `ComputedField`, then first we launch kernels to calculate the intermediate `ComputedField`s, and launch a final kernel to compute the quantity of interest. It can also be more memory intensive since the intermediate `ComputedField`s need to be stored. For some applications, the ""optimization"" of avoiding intermediate kernel launches / calculations may be unimportant (for example, if plenty of memory is available and computations are made very rarely). I think nesting `ComputedField`s is a nice solution that avoids having to hand-write kernels for those cases. But sometimes I do think that users want to optimize diagnostics calculations. There are also some other nice applications of `KernelComputedField`; for example I think it would be nice to make the LES eddy diffusivities into `KernelComputedField` (which are computed every tendency evaluation so performance is crucial), and perhaps some other auxiliary variables. I think it would be fun to try the compiler hack-around that I suggested in my comment above (defining multiple `identity` functions and using an internal counter to scroll through them during compilation). It's just a matter of finding the time to do it.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1241#issuecomment-786813600:644,optimiz,optimization,644,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1241#issuecomment-786813600,3,"['optimiz', 'perform']","['optimization', 'optimize', 'performance']"
Performance,"I think these lines should be using `ξnode`, `ηnode`, and `rnode`:. https://github.com/CliMA/Oceananigans.jl/blob/7cbf013cb6bed2bd7cef0f4d8e5f04c078e50ee0/src/Models/LagrangianParticleTracking/lagrangian_particle_advection.jl#L136-L142. I'll open a PR with a fix tomorrow. Should probably also add a test for particle advection on a lat-lon grid. ---. Some debug printing inside `advect_particle` with 1 particle:. ```; [ Info: Iteration 1...; [ Info: X=(1.0, -1.5, -10.0), I=(47, 109, 53); [ Info: (before) X⁺=(1.0, -1.5, -10.0); (iᴿ, jᴿ, kᴿ) = (101, 201, 61); (xᴸ, yᴸ, zᴸ) = (87813.63270401207, -217942.05622333512, -100.0); (xᴿ, yᴿ, zᴿ) = (136722.49142523398, -124538.3178419058, 0.0); (x⁺, y⁺, z⁺) = (175626.26540802413, -249075.1356838116, -10.0); [ Info: (after) X⁺=(175626.26540802413, -249075.1356838116, -10.0); [ Info: Iteration 2...; [ Info: X=(175626.26540802413, -249075.1356838116, -10.0), I=(39914880, -59303137, 53); ERROR: LoadError: BoundsError: attempt to access 109×208×68 OffsetArray(::Array{Float64, 3}, -3:105, -3:204, -3:64) with eltype Float64 with indices -3:105×-3:204×-3:64 at index [39914881, -59303136, 54]; ```",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3852#issuecomment-2428098528:940,Load,LoadError,940,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3852#issuecomment-2428098528,1,['Load'],['LoadError']
Performance,"I think this PR is finally ready to be merged, provided that tests pass. To make sure that `VerticalDirection` is working, I ran the equation of state benchmarks and they seem to match the benchmarks performed in https://github.com/CliMA/Oceananigans.jl/pull/1169#issuecomment-725471594. If anything, benchmarks seem a bit better (probably different machine) and `RoquetEquationOfState` is surprisingly faster on the CPU (but might be a fluke). ```; Equation of state benchmarks; ┌───────────────┬───────────────────────┬───────────┬───────────┬───────────┬───────────┬─────────────┬────────┐; │ Architectures │ EquationsOfState │ min │ median │ mean │ max │ memory │ allocs │; ├───────────────┼───────────────────────┼───────────┼───────────┼───────────┼───────────┼─────────────┼────────┤; │ CPU │ LinearEquationOfState │ 2.037 s │ 2.040 s │ 2.039 s │ 2.041 s │ 372.66 KiB │ 2090 │; │ CPU │ RoquetEquationOfState │ 1.759 s │ 1.761 s │ 1.761 s │ 1.763 s │ 373.77 KiB │ 2090 │; │ CPU │ TEOS10EquationOfState │ 2.270 s │ 2.401 s │ 2.378 s │ 2.464 s │ 372.53 KiB │ 2090 │; │ GPU │ LinearEquationOfState │ 10.058 ms │ 13.161 ms │ 12.856 ms │ 13.215 ms │ 1022.19 KiB │ 7154 │; │ GPU │ RoquetEquationOfState │ 10.688 ms │ 13.236 ms │ 12.991 ms │ 13.322 ms │ 1.00 MiB │ 7054 │; │ GPU │ TEOS10EquationOfState │ 10.204 ms │ 13.463 ms │ 13.145 ms │ 13.504 ms │ 1017.58 KiB │ 7154 │; └───────────────┴───────────────────────┴───────────┴───────────┴───────────┴───────────┴─────────────┴────────┘; ```. ```; Equation of state CPU -> GPU speedup; ┌───────────────────────┬─────────┬─────────┬─────────┐; │ EquationsOfState │ speedup │ memory │ allocs │; ├───────────────────────┼─────────┼─────────┼─────────┤; │ LinearEquationOfState │ 154.965 │ 2.74298 │ 3.42297 │; │ RoquetEquationOfState │ 133.062 │ 2.74052 │ 3.37512 │; │ TEOS10EquationOfState │ 178.317 │ 2.73152 │ 3.42297 │; └───────────────────────┴─────────┴─────────┴─────────┘; ```. ```; Equation of state relative performance (CPU); ┌───────────────┬",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1242#issuecomment-800720099:200,perform,performed,200,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1242#issuecomment-800720099,1,['perform'],['performed']
Performance,"I think to preserve the work in this PR, we should add a `Float32` test which will fail if a spurious promotion undermines performance",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3876#issuecomment-2445215733:123,perform,performance,123,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3876#issuecomment-2445215733,1,['perform'],['performance']
Performance,I think we are also hitting this problem https://github.com/JuliaParallel/MPI.jl/issues/715; because it looks like the MPIPreferences are correctly loaded at the ; ```julia; julia -O0 --project -e 'using Pkg; Pkg.instantiate()`; ```; but then it loads a completely different MPI in the ; ```julia; julia -O0 --project -e 'using Pkg; Pkg.test()`; ```; step,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3897#issuecomment-2459444337:148,load,loaded,148,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3897#issuecomment-2459444337,2,['load'],"['loaded', 'loads']"
Performance,"I think we should merge this now, since Oceananigans is barely useable at the moment, and then pick up getting the Enzyme tests passing in @jlk9's PR (which also needs to involve performance benchmarking to ensure we maintain performance)",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3477#issuecomment-1948811417:179,perform,performance,179,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3477#issuecomment-1948811417,2,['perform'],['performance']
Performance,"I think you need to load cuda before you _build_ Oceananigans. You might need to do this from the login node, eg something like. ```; module load cuda; julia --project -e 'using Pkg; Pkg.build(""Oceananigans"")'; ```. Is that right @ali-ramadhan ?. The issue is that the functions `plan_forward_transforms` for `CuArray`s are not being loaded:. https://github.com/CliMA/Oceananigans.jl/blob/52bfeb09e3562f639deb32b8807f32a88e3a1cfa/src/Solvers/plan_transforms.jl#L30-L33. Note that your script is a julia file, so you should append it with `.jl` so that it's named `model_gpu_waves.jl`. As a side comment, you should take care when initializing a model with zero Eulerian-mean flow --- despite that this is common in the literature, it is unlikely to be a physically relevant initial condition (because it rarely occurs in nature, and because it will excite large inertial oscillations in your simulation). Some perspective on this issue is provided by [observations reported by Jerry Smith (2006)](https://journals.ametsoc.org/view/journals/phoc/36/7/jpo2910.1.xml?tab_body=abstract-display) and [a preprint that I'm first author on](https://glwagner.github.io/assets/pdf/near-inertial-waves-turbulence-growth-swell-preprint.pdf).",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1317#issuecomment-767021509:20,load,load,20,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1317#issuecomment-767021509,3,['load'],"['load', 'loaded']"
Performance,"I think “Nthreads” is a keyword argument in plan_fft. On Sat, Mar 9, 2019 at 8:27 AM Ali Ramadhan <notifications@github.com>; wrote:. > From Googling around I believe FFTW.jl is build in serial mode? I couldn't; > see how to make use of multi-threading or specify the number of threads.; >; > A related performance optimization would be to consider using Intel's MKL; > which is usually faster on Intel machines (and might come with; > multi-threading out of the box).; >; > But since we're not running large models on single-core CPUs this seems; > like a low priority consideration for now.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/climate-machine/Oceananigans.jl/issues/119#issuecomment-471176913>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AOkIBka4H13o1NmaeZ8LRM2DPBpur-kDks5vU7a0gaJpZM4bmrZ0>; > .; >",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/119#issuecomment-471178072:237,multi-thread,multi-threading,237,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/119#issuecomment-471178072,4,"['multi-thread', 'optimiz', 'perform']","['multi-threading', 'optimization', 'performance']"
Performance,"I thought of another way to achieve this performance optimization without any source-code-specific feature. For a simulation without buoyancy, for example, this might work:. ```julia; # Construct the full model with all desired tracers; model = NonHydrostaticModel(; grid, tracers, ...). model_properties = []; for name in propertynames(model); if name == :tracers; push!(model_properties, nothing); else; push!(model_properties, getproperty(model, name)); end; end. # Build a ""spin-up"" model using the inner constructor for NonhydrostaticModel, with tracers=nothing; spin_up_model = NonhydrostaticModel(model_properties...); ```. I believe that `spin_up_model` will run without evolving tracers. If you require _some_ tracers (ie active tracers like buoyancy) then things are slightly more complicated. You have to replace `tracers=nothing` with the appropriate `NamedTuple`. Also, I think you need to ensure that the active tracers are ""first"" in the list of tracers when the full model is built. Something like. ```julia; # Construct the full model with all desired tracers; model = NonHydrostaticModel(; grid, tracers=(T, S, c1, c2, c3, c4), ...). active_tracers = (T = model.tracers.T, S = model.tracers.S); model_properties = []; for name in propertynames(model); if name == :tracers; push!(model_properties, active_tracers); else; push!(model_properties, getproperty(model, name)); end; end. # Build a ""spin-up"" model using the inner constructor for NonhydrostaticModel, with tracers=(; T, S); spin_up_model = NonhydrostaticModel(model_properties...); ```",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3154#issuecomment-1613481559:41,perform,performance,41,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3154#issuecomment-1613481559,2,"['optimiz', 'perform']","['optimization', 'performance']"
Performance,"I took the shallow water Bickley jet example and made two modifications. I added `using CUDA` and changed the architecture to `GPU` and tried running it on my desktop. The GPU is nothing fancy but there is something and I thought that it should, based on previous tests. Unfortunately, `run!(simulation)` yields an error that you can find below. I also tried this on a server and found a similar error. Two questions.; 1. Could someone else try this in a GPU to see if they get an error?; 2. Anyone have a clue as to what id going wrong in this error?. ```; $ julia --project shallow_water_Bickley_jet.jl ; ERROR: LoadError: InvalidIRError: compiling kernel gpu__compute!(Cassette.Context{nametype(CUDACtx),KernelAbstractions.CompilerMetadata{KernelAbstractions.NDIteration.StaticSize{(128, 129, 1)},KernelAbstractions.NDIteration.DynamicCheck,Nothing,Nothing,KernelAbstractions.NDIteration.NDRange{3,KernelAbstractions.NDIteration.StaticSize{(8, 9, 1)},KernelAbstractions.NDIteration.StaticSize{(16, 16, 1)},Nothing,Nothing}},Nothing,KernelAbstractions.var""##PassType#253"",Nothing,Cassette.DisableHooks}, typeof(Oceananigans.Fields.gpu__compute!), OffsetArrays.OffsetArray{Float64,3,CuDeviceArray{Float64,3,1}}, Oceananigans.AbstractOperations.BinaryOperation{Face,Face,Center,typeof(-),OffsetArrays.OffsetArray{Float64,3,CuDeviceArray{Float64,3,1}},Oceananigans.Fields.FunctionField{Face,Face,Center,Nothing,Nothing,typeof(ω̄),RegularRectilinearGrid{Float64,Periodic,Bounded,Bounded,OffsetArrays.OffsetArray{Float64,1,StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}}}},typeof(identity),typeof(identity),typeof(identity),RegularRectilinearGrid{Float64,Periodic,Bounded,Bounded,OffsetArrays.OffsetArray{Float64,1,StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}}}}) resulted in invalid LLVM IR; Reason: unsupported dynamic function invocation (call to overdub); Stacktrace:; [1] - at /home/fpoulin/software/Oceananigans.jl/src/AbstractOpe",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1477:614,Load,LoadError,614,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1477,1,['Load'],['LoadError']
Performance,"I tried it again and received a similar error, copied below. What I find strange is that it mentions worker 5 but I only asked for 4. . @ali-ramadhan , what did you do to run this file exactly?. ```; ERROR: LoadError: Worker 5 terminated.ProcessExitedException(2). ...and 3 more exception(s). Stacktrace:; [1] sync_end(::Channel{Any}) at ./task.jl:314; [2] macro expansion at ./task.jl:333 [inlined]; [3] _require_callback(::Base.PkgId) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Distributed/src/Distributed.jl:75; [4] #invokelatest#1 at ./essentials.jl:710 [inlined]; [5] invokelatest at ./essentials.jl:709 [inlined]; [6] require(::Base.PkgId) at ./loading.jl:931; [7] require(::Module, ::Symbol) at ./loading.jl:923; [8] include(::Function, ::Module, ::String) at ./Base.jl:380; [9] include(::Module, ::String) at ./Base.jl:368; [10] exec_options(::Base.JLOptions) at ./client.jl:296; [11] _start() at ./client.jl:506; in expression starting at /home/fpoulin/software/Oceananigans.jl/examples/mpi_shallow_water_turbulence.jl:6; ```",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1430#issuecomment-794288378:207,Load,LoadError,207,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1430#issuecomment-794288378,3,"['Load', 'load']","['LoadError', 'loading']"
Performance,"I tried it again with a fresh clone and still received an error. The details are copied below. However, `make_example.jl` does seem to be working so it's not all bad. Unfortunately, I'm not sure why this occurs. ```; ERROR: LoadError: `makedocs` encountered a doctest error. Terminating build; Stacktrace:; [1] error(s::String); @ Base ./error.jl:33; [2] runner(#unused#::Type{Documenter.Builder.Doctest}, doc::Documenter.Documents.Document); @ Documenter.Builder ~/.julia/packages/Documenter/f5jts/src/Builder.jl:217; [3] dispatch(#unused#::Type{Documenter.Builder.DocumentPipeline}, x::Documenter.Documents.Document); @ Documenter.Utilities.Selectors ~/.julia/packages/Documenter/f5jts/src/Utilities/Selectors.jl:170; [4] #2; @ ~/.julia/packages/Documenter/f5jts/src/Documenter.jl:249 [inlined]; [5] cd(f::Documenter.var""#2#3""{Documenter.Documents.Document}, dir::String); @ Base.Filesystem ./file.jl:106; [6] #makedocs#1; @ ~/.julia/packages/Documenter/f5jts/src/Documenter.jl:248 [inlined]; [7] top-level scope; @ ~/software/Second_Oceananigans/Oceananigans.jl/docs/make.jl:160; in expression starting at /home/fpoulin/software/Second_Oceananigans/Oceananigans.jl/docs/make.jl:160; ```",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1772#issuecomment-869017574:224,Load,LoadError,224,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1772#issuecomment-869017574,1,['Load'],['LoadError']
Performance,"I tried running the `ShallowWaterModel` example on a `GPU` and it failed because of how we compute the norm, see the error message below. @glwagner , I remember we talked about this but, sadly, I don't know if we had a solution. What would you recommend?. ```; ERROR: LoadError: Scalar indexing is disallowed.; Invocation of getindex resulted in scalar indexing of a GPU array.; This is typically caused by calling an iterating implementation of a method.; Such implementations *do not* execute on the GPU, but very slowly on the CPU,; and therefore are only permitted from the REPL for prototyping purposes.; If you did intend to index this array, annotate the caller with @allowscalar.; Stacktrace:; [1] error(s::String); @ Base ./error.jl:33; [2] assertscalar(op::String); @ GPUArrays ~/.julia/packages/GPUArrays/8dzSJ/src/host/indexing.jl:53; [3] getindex(::CUDA.CuArray{Float64, 3}, ::Int64, ::Int64, ::Int64); @ GPUArrays ~/.julia/packages/GPUArrays/8dzSJ/src/host/indexing.jl:86; [4] getindex; @ ./subarray.jl:276 [inlined]; [5] _getindex; @ ./abstractarray.jl:1214 [inlined]; [6] getindex; @ ./abstractarray.jl:1170 [inlined]; [7] iterate; @ ./abstractarray.jl:1096 [inlined]; [8] iterate; @ ./abstractarray.jl:1094 [inlined]; [9] generic_normInf(x::SubArray{Float64, 3, CUDA.CuArray{Float64, 3}, Tuple{UnitRange{Int64}, UnitRange{Int64}, UnitRange{Int64}}, false}); @ LinearAlgebra /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/LinearAlgebra/src/generic.jl:465; [10] normInf; @ /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/LinearAlgebra/src/generic.jl:556 [inlined]; [11] generic_norm2(x::SubArray{Float64, 3, CUDA.CuArray{Float64, 3}, Tuple{UnitRange{Int64}, UnitRange{Int64}, UnitRange{Int64}}, false}); @ LinearAlgebra /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/LinearAlgebra/src/generic.jl:497; [12] norm2; @ /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/LinearAlgebra/src/generic.jl:558 ",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1863#issuecomment-882647783:268,Load,LoadError,268,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1863#issuecomment-882647783,1,['Load'],['LoadError']
Performance,"I tried submitting it as a package through an attobot request a few weeks ago but the PR build failed on METADATA.jl and I'm not sure why. Well, I think this was the error but not sure what to do about it as I put Julia 1.1 in the REQUIRE file.; ```; ERROR: LoadError: METADATA/Oceananigans/versions/0.4.0/requires: no julia entry (>= 0.6.0 needed); ```. See:; * https://github.com/JuliaLang/METADATA.jl/pull/21774",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/142:258,Load,LoadError,258,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/142,1,['Load'],['LoadError']
Performance,"I tried to follow the terminology in the regression test and sorry it was confusing. I used `truth` to refer to the data that is read from a file, done [here.](https://github.com/CliMA/Oceananigans.jl/blob/3676a718be1160f3ea70c3cce5dd21c5f06f144a/test/regression_tests/shallow_water_bickley_jet_regression.jl#L82) . I agree that only `v` fails but that means that `v` has larger differences compared to `u` and `h`. They all have differences. . I compared the initial data that we used in this regression test with the initial data read from the regression test. I saw that we had the 0th and 20th step saved. If there are differences at the beginning, then they are not solving exactly the same problem. In both we have that v is set to 0 and u and h are set to the Bickley jet with a random perturbation on `u`. The randomness will not be the same (unless we use a seed, which we don't, but we could) however the amplitude of the perturbations are different. This suggests to me that the initial conditions are not the same, and maybe the soruce of why the regression test fails. This is why I would like to know how the initial data was generated, using what script. I do suggest we regenerate it as that might solve a lot of the problems we are having with the regression tests. Do you want me to generate a script that load the inital data and compare it? You can see the results above. The fact that h has differences of `1e-7` is due to single precison, which means they are the same. However, u is different with errors much larger then `1e-7`, so the initial data is different. Unless there is something that I'm missing here?. I saw that the data files were dated June 1st 2022. For me it is stored at the following location. `/home/fpoulin/.julia/datadeps/regression_test_data/`",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2922#issuecomment-1446357883:1324,load,load,1324,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2922#issuecomment-1446357883,1,['load'],['load']
Performance,"I tried to run the `eady_turbulence.jl` example in Oceananigans v0.25.0 in Julia 1.4 RC2 on macOS 10.15.3 and got the following error:. ```; julia> include(""eady_turbulence.jl""); [ Info: CUDAdrv.jl failed to initialize, GPU functionality unavailable (set JULIA_CUDA_SILENT or JULIA_CUDA_VERBOSE to silence or expand this message); N² = ((Rᵈ * f) / Lz) ^ 2 = 0.0004; α = sqrt(N²) / (f * σᵇ) = 0.02314814814814815; ERROR: LoadError: UndefVarError: SolutionBoundaryConditions not defined; Stacktrace:; [1] top-level scope at /Users/truedichotomy/GitHub/Oceananigans.jl/examples/eady_turbulence.jl:138; [2] include(::String) at ./client.jl:439; [3] top-level scope at REPL[1]:1; in expression starting at /Users/truedichotomy/GitHub/Oceananigans.jl/examples/eady_turbulence.jl:138; ```; I noticed that there are a lot of recent commits with regards to boundary conditions, may be the example is using an older interface?",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/686:420,Load,LoadError,420,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/686,1,['Load'],['LoadError']
Performance,"I tried to use your `Diagnostics.NaNChecker`, but couldn't figure out how to do it. So implemented my own `nan` checker in the progress function, which works on CPUs, but I get following error when running on GPUs:. ```; i: 10, sim time: 22.857 seconds, wall time: 52.770 seconds, Δt: 2.286 seconds, diff CFL: 1.14e-01, adv CFL: 6.87e-01; ERROR: LoadError: scalar getindex is disallowed; Stacktrace:; [1] error(::String) at ./error.jl:33; [2] assertscalar(::String) at /glade/u/home/tomasc/.julia/packages/GPUArrays/ZxsKE/src/host/indexing.jl:41; [3] getindex at /glade/u/home/tomasc/.julia/packages/GPUArrays/ZxsKE/src/host/indexing.jl:96 [inlined]; [4] getindex at /glade/u/home/tomasc/.julia/packages/OffsetArrays/ExQCD/src/OffsetArrays.jl:271 [inlined]; [5] getindex at ./subarray.jl:257 [inlined]; [6] _getindex at ./abstractarray.jl:1100 [inlined]; [7] getindex at ./abstractarray.jl:1060 [inlined]; [8] _broadcast_getindex at ./broadcast.jl:614 [inlined]; [9] _getindex at ./broadcast.jl:645 [inlined]; [10] _broadcast_getindex at ./broadcast.jl:620 [inlined]; [11] getindex at ./broadcast.jl:575 [inlined]; [12] macro expansion at ./broadcast.jl:950 [inlined]; [13] macro expansion at ./simdloop.jl:77 [inlined]; [14] copyto! at ./broadcast.jl:949 [inlined]; [15] copyto! at ./broadcast.jl:886 [inlined]; [16] copy at ./broadcast.jl:862 [inlined]; [17] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.DefaultArrayStyle{3},Nothing,typeof(isnan),Tuple{SubArray{Float64,3,OffsetArrays.OffsetArray{Float64,3,CUDA.CuArray{Float64,3}},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},false}}}) at ./broadcast.jl:837; ```. This is my progress function:. ```; advCFL = oc.Diagnostics.AdvectiveCFL(wizard); difCFL = oc.Diagnostics.DiffusiveCFL(wizard); start_time = time_ns(); function progress(sim); msg = @printf(""i: % 6d, sim time: % 10s, wall time: % 10s, Δt: % 10s, diff CFL: %.2e, adv CFL: %.2e\n"",; sim.model.clock.iteration,; prettytime(sim.model.clock.time),; prettytime(1e-9 ",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1196#issuecomment-733811924:346,Load,LoadError,346,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1196#issuecomment-733811924,1,['Load'],['LoadError']
Performance,"I uploaded my first attempt. There is a new file in Fields/set_new!.jl that tries to do what I think it should do. I removed the , Z since we don't want to be able to specify depth anywhere in the height but I kept the k index since I know that halos are added. Is this inconsistent?. The error that I get is the following:. ```; ERROR: LoadError: LoadError: UndefVarError: AbstractField not defined",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1188#issuecomment-729953983:337,Load,LoadError,337,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1188#issuecomment-729953983,2,['Load'],['LoadError']
Performance,"I was able to get things and I see that 35 files were changed so I suspect a merge happened. However now, when I try the example @glwagner suggested, the first line with `using` errors with the following. Maybe there is a problem with my merge?. ```; julia> using Oceananigans.ImmersedBoundaries: ImmersedBoundaryGrid, GridFittedBoundary; [ Info: Precompiling Oceananigans [9e8cae18-63c1-5223-a75c-80ca9d6e9a09]; ERROR: LoadError: LoadError: LoadError: LoadError: UndefVarError: NoImmersedBoundary not defined; Stacktrace:; [1] include(mod::Module, _path::String); @ Base ./Base.jl:386; [2] include(x::String); @ Oceananigans.Models.ShallowWaterModels ~/software/Oceananigans.jl/src/Models/ShallowWaterModels/ShallowWaterModels.jl:1; [3] top-level scope; @ ~/software/Oceananigans.jl/src/Models/ShallowWaterModels/ShallowWaterModels.jl:15; [4] include(mod::Module, _path::String); @ Base ./Base.jl:386; [5] include(x::String); @ Oceananigans.Models ~/software/Oceananigans.jl/src/Models/Models.jl:1; [6] top-level scope; @ ~/software/Oceananigans.jl/src/Models/Models.jl:20; [7] include(mod::Module, _path::String); @ Base ./Base.jl:386; [8] include(x::String); @ Oceananigans ~/software/Oceananigans.jl/src/Oceananigans.jl:1; [9] top-level scope; @ ~/software/Oceananigans.jl/src/Oceananigans.jl:179; [10] include; @ ./Base.jl:386 [inlined]; [11] include_package_for_output(pkg::Base.PkgId, input::String, depot_path::Vector{String}, dl_load_path::Vector{String}, load_path::Vector{String}, concrete_deps::Vector{Pair{Base.PkgId, UInt64}}, source::Nothing); @ Base ./loading.jl:1213; [12] top-level scope; @ none:1; [13] eval; @ ./boot.jl:360 [inlined]; [14] eval(x::Expr); @ Base.MainInclude ./client.jl:446; [15] top-level scope; @ none:1; in expression starting at /home/fpoulin/software/Oceananigans.jl/src/Models/ShallowWaterModels/shallow_water_model.jl:16; in expression starting at /home/fpoulin/software/Oceananigans.jl/src/Models/ShallowWaterModels/ShallowWaterModels.jl:1; in expression st",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1663#issuecomment-843316815:420,Load,LoadError,420,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1663#issuecomment-843316815,4,['Load'],['LoadError']
Performance,I was doing some profiling on a model with no open boundaries and discovered that this function was causing a big slow down. I guess this is because the compiler isn't managing to work out its just a load of nothing operations but this change appears to make it completely go away.,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3792:200,load,load,200,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3792,1,['load'],['load']
Performance,"I was going quickly through some tests for a PR and found more than one instance of something like this:. https://github.com/CliMA/Oceananigans.jl/blob/93c497a5f78a9a422d8f597dbd5406ccc0c09ceb/test/test_output_writers.jl#L181-L213. Where, unless I'm missing something we run a couple of unnecessary loops. In this case I believe we're creating 4 models, when we could be creating only two. Since the tests are taking a considerable amount of time to run (I think something around 2 hours on the CI servers) I think it'd be a good idea for us to tackle these as time permits. Not necessarily all at once, which would take a huge amount of effort, but maybe one PR here and there when we catch these things. (Although I'm also not opposed to re-organizing all the tests if it'll significantly improve performance.)",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1990:799,perform,performance,799,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1990,1,['perform'],['performance']
Performance,"I was just trying to precompile Oceananigans 0.34.1 on my macOS 10.15.6 machine and got the following error:. ```; julia> using Oceananigans; [ Info: Precompiling Oceananigans [9e8cae18-63c1-5223-a75c-80ca9d6e9a09]; ERROR: LoadError: LoadError: too many parameters for type; Stacktrace:; [1] top-level scope at /Users/truedichotomy/.julia/packages/GPUArrays/PkHCM/src/host/abstractarray.jl:24; [2] include(::Function, ::Module, ::String) at ./Base.jl:380; [3] include at ./Base.jl:368 [inlined]; [4] include(::String) at /Users/truedichotomy/.julia/packages/GPUArrays/PkHCM/src/GPUArrays.jl:1; [5] top-level scope at /Users/truedichotomy/.julia/packages/GPUArrays/PkHCM/src/GPUArrays.jl:25; [6] include(::Function, ::Module, ::String) at ./Base.jl:380; [7] include(::Module, ::String) at ./Base.jl:368; [8] top-level scope at none:2; [9] eval at ./boot.jl:331 [inlined]; [10] eval(::Expr) at ./client.jl:467; [11] top-level scope at ./none:3; in expression starting at /Users/truedichotomy/.julia/packages/GPUArrays/PkHCM/src/host/abstractarray.jl:24; in expression starting at /Users/truedichotomy/.julia/packages/GPUArrays/PkHCM/src/GPUArrays.jl:25; ERROR: LoadError: Failed to precompile GPUArrays [0c68f7d7-f131-5f86-a1c3-88cf8149b2d7] to /Users/truedichotomy/.julia/compiled/v1.5/GPUArrays/v5u0T_IyCmP.ji.; Stacktrace:; [1] error(::String) at ./error.jl:33; [2] compilecache(::Base.PkgId, ::String) at ./loading.jl:1290; [3] _require(::Base.PkgId) at ./loading.jl:1030; [4] require(::Base.PkgId) at ./loading.jl:928; [5] require(::Module, ::Symbol) at ./loading.jl:923; [6] include(::Function, ::Module, ::String) at ./Base.jl:380; [7] include(::Module, ::String) at ./Base.jl:368; [8] top-level scope at none:2; [9] eval at ./boot.jl:331 [inlined]; [10] eval(::Expr) at ./client.jl:467; [11] top-level scope at ./none:3; in expression starting at /Users/truedichotomy/.julia/packages/CUDA/7vLVC/src/CUDA.jl:5; ERROR: LoadError: Failed to precompile CUDA [052768ef-5323-5732-b1bb-66c8b64840ba] to",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/854:223,Load,LoadError,223,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/854,2,['Load'],['LoadError']
Performance,I was reading about the extension feature in Julia v1.9. https://pkgdocs.julialang.org/v1/creating-packages/#Conditional-loading-of-code-in-packages-(Extensions). and it reads to me that it's ideal to add for plotting methods that step onto Makie functionality. This way the code won't load unless one loads GLMakie/CairoMakie in the environment. What do others think?. I have put together a few methods for plotting Oceananigans fields at https://github.com/navidcy/Imaginocean.jl. I could move those into an extension in Oceananigans. Ideally I would like these things to live outside Oceananigans repo to minimise the burden of maintenance. But from what I understand for how extensions work they have to live in this repo? Or in the Makie repo which makes even less sense? Am I right?. x-ref: https://github.com/navidcy/Imaginocean.jl/issues/2,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3178:121,load,loading-of-code-in-packages,121,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3178,3,['load'],"['load', 'loading-of-code-in-packages', 'loads']"
Performance,"I was thinking it might be cool/useful to inform users if their model is physically/numerically consistent. One example of this is to check that the boundary conditions being imposed match the grid topology (#890). But I think there are many more. Some examples:; * Using `BetaPlane` on a periodic grid.; * Using `WENO5` on a curvilinear grid.; * Using a free surface when z is `Periodic`.; * Using an `FFTBasedPressureSolver` on a curvilinear grid.; * ... Some errors are more likely to be made by users than others of course. But I don't think we can expect users to just know that e.g. `WENO5` doesn't work with `VerticallyStretchedRectilinearGrid` yet. To save ourselves and users some potential future headaches, it seems like we could create a function like `check_consistency(model)` that would perform these checks and print a warning for each inconsistency detected. Unfortunately this could become a pretty ugly monolithic function that's hard to modularize since we're checking the consistency of many different structs acting together.",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1453:802,perform,perform,802,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1453,1,['perform'],['perform']
Performance,"I was thinking of doing some prototyping and benchmarking in a sandbox by building off the example in my PR https://github.com/vchuravy/GPUifyLoops.jl/pull/18. The PR contains an example that can be extended to rely on a `Grid` struct, multiple `FaceField`s and ` CellField`. So I'll prototype grids and fields that are `isbitstype` (you already helped by doing this for a grid in https://github.com/climate-machine/Oceananigans.jl/issues/59#issuecomment-467660181) and test to see if they work on the GPU with GPUifyLoops.jl. If they do work and performance isn't degraded then I'll rewrite the operators to use grid and field structs. You probably know how to do this better than me, but might be good if I rewrite the operators as they's still undocumented and do some _slightly convoluted_ stuff to avoid having to store intermediate calculations. Right now I'm focusing on system tests and benchmarks but once @christophernhill @jm-c and I get closer to implementing the variable _Δz_ grid #47 I will work on this.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/115#issuecomment-470782067:547,perform,performance,547,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/115#issuecomment-470782067,1,['perform'],['performance']
Performance,"I was trying to add some Lagrangian particles to a 2D simulation I was running but Lagrangian particle advection currently assumes that the grid is bounded or periodic in all dimensions. An MWE (on Julia 1.10.1+0.x64.w64.mingw32 and Oceananigans version 0.90.12) that reproduces the issue is:. ```; using Oceananigans; NX, NZ = 16, 16; grid = RectilinearGrid(size=(NX,NZ), x=(0,1), z=(0,1), topology=(Periodic, Flat, Bounded)); # initialise some particles at the cell centres; x₀ = reshape(xnodes(grid, Center()) * ones(NZ)', NX * NZ); y₀ = zeros(NX * NZ); z₀ = reshape(ones(NX) * znodes(grid, Center())', NX * NZ); model = NonhydrostaticModel(; grid, particles=LagrangianParticles(x=x₀, y=y₀, z=z₀)); simulation = Simulation(model; Δt=1, stop_iteration=5); run!(simulation); ```. with output. ```; [ Info: Initializing simulation...; [ Info: ... simulation initialization complete (115.810 ms); [ Info: Executing initial time step...; ERROR: LoadError: MethodError: no method matching unsafe_trunc(::Type{Int64}, ::Nothing). Closest candidates are:; unsafe_trunc(::Type{Int64}, ::Union{Float16, Float32, Float64}); @ Base float.jl:336; unsafe_trunc(::Type{<:Integer}, ::BFloat16s.BFloat16); @ BFloat16s C:\Users\hildi\.julia\packages\BFloat16s\u3WQc\src\bfloat16.jl:288; unsafe_trunc(::Type{T}, ::BigFloat) where T<:Integer; @ Base mpfr.jl:358; ... Stacktrace:; [1] advect_particle; @ C:\Users\hildi\.julia\packages\Oceananigans\kBe5X\src\Models\LagrangianParticleTracking\lagrangian_particle_advection.jl:81 [inlined]. ...; ```. The offending line is https://github.com/CliMA/Oceananigans.jl/blob/ce4fabaa0c6ddfd20ae43c671bbaec7bb5dae847/src/Models/LagrangianParticleTracking/lagrangian_particle_advection.jl#L81 . which fails because ; https://github.com/CliMA/Oceananigans.jl/blob/ce4fabaa0c6ddfd20ae43c671bbaec7bb5dae847/src/Models/LagrangianParticleTracking/lagrangian_particle_advection.jl#L79; returns `j = nothing` when `y` is `Flat`. However, I think that in general `advect_particle` is not",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3545:943,Load,LoadError,943,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3545,1,['Load'],['LoadError']
Performance,"I was wondering if we can regrid in 3D simply by regridding in each dimension in serial. Eg first regrid in x, then, y, then z. It'll be a bit more computation, but we still have the option of future optimization...",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2067#issuecomment-973673941:200,optimiz,optimization,200,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2067#issuecomment-973673941,1,['optimiz'],['optimization']
Performance,"I was working on post-processing the data from a simulation I ran in Oceananigans when I decided I wanted to access the background field data from the run and write it to a netCDF4. I had tried `output = (;u,v,w,U=(model.background_fields.velocities.u),V=(model.background_fields.velocities.v))` as seen in my minimal working example below(please let me know if you need more details); ```; u,v,w = model.velocities. output = (;u,v,w,U=(model.background_fields.velocities.u),V=(model.background_fields.velocities.v)). simulation.output_writers[:fields] = NetCDFOutputWriter(model, output;; schedule = TimeInterval(10),; filename = ""test2.nc"",; overwrite_existing = true); ```; but kept getting this error:; ```; ERROR: LoadError: MethodError: no method matching Field(::Oceananigans.Fields.FunctionField{Face, Center, Center, Clock{Float64}, NamedTuple{(:Nₒ, :S, :γ, :ϕ, :f), Tuple{Float64, Float64, Float64, Int64, Float64}}, typeof(U_func), RectilinearGrid{Float64, Periodic, Periodic, Bounded, Float64, Float64, Float64, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, CPU}, Float64}; indices::Tuple{UnitRange{Int64}, UnitRange{Int64}, UnitRange{Int64}}); ```; Luckily I made a work around by changing the output to `output = (; u, v, w, U=(model.background_fields.velocities.u+0*u), V=(model.background_fields.velocities.v+0*v))` on a suggestion from a postdoc in my group who has much more experience using Oceananigans. I think the issue here is that the background fields are classified as function fields, so it may be worthwhile to create a method that converts function fields to fields or something similar to make outputting them easier.",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3226:719,Load,LoadError,719,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3226,1,['Load'],['LoadError']
Performance,"I wonder how hard it would be to spin up a Google Cloud instance with a V100 GPU (or something cheaper, doesn't matter too much since we have enough credits) and set up a GitLab CI pipeline with it just like the one JuliaGPU has. We could share it with the JuliaGPU organization as well. . And if we need 2+ GPUs to really test MPI that would be easy to change (just spin up a new instance and load the ""GitLab CI"" image maybe). It wouldn't run the tests on Windows or Mac, but we can pay a little bit more for dedicated Travis (Mac?) and Appveyor (Windows) resources if we want those to run fast as well. cc @vchuravy is this easy-ish to set up? I think you were involved in setting up the current GitLab CI pipeline?; cc @jkozdon since your Slack post reminded me about this issue. See: https://github.com/JuliaGPU/gitlab-ci",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/139#issuecomment-484997371:394,load,load,394,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/139#issuecomment-484997371,1,['load'],['load']
Performance,I wonder if it makes sense to spend some effort to try and get more complex operations to compile on the GPU (e.g. so we can see if it's even performant: https://github.com/CliMA/Oceananigans.jl/pull/870) so if it's performant we can start using them to work on issues like this.,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1073#issuecomment-717290756:142,perform,performant,142,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1073#issuecomment-717290756,2,['perform'],['performant']
Performance,"I would advocate again for moving both CUDA and AMDGPU support into package extensions (see https://github.com/CliMA/Oceananigans.jl/pull/3066 for an outdated start). Having the user install both AMDGPU and CUDA unconditionally is both space and time consuming, loading them both should be unnecessary on most systems and they may at times be incompatible with each other since both are developed independently from each other.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3468#issuecomment-1935362327:262,load,loading,262,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3468#issuecomment-1935362327,1,['load'],['loading']
Performance,I would like to suggest the following checklist moving forward. - [ ] Decide whether we like the structure of the script `new_rates_of_convergence.jl`. How can we might improve it?; - [ ] Find where the bottleneck is in Oceanangans to make things globally single precision.; - [ ] Adapt this approach to the other validation cases that currently exist.; - [ ] Make `UpwindFirstOrderBiased()` advection scheme.; - [ ] Make `CenteredSixthOrder()` advection scheme.,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1276#issuecomment-750476313:203,bottleneck,bottleneck,203,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1276#issuecomment-750476313,1,['bottleneck'],['bottleneck']
Performance,"I wrote a simple test that I believe represents a similar case, in which the fields of the type in question are pointers. On my machine there does not seem to be any performance difference. ```julia; using Random, BenchmarkTools, Printf ; ; struct Dummy ; a::Array{Float64,2} ; b::Array{Float64,2} ; c::Array{Float64,2} ; end ; ; mutable struct MutableDummy ; a::Array{Float64,2} ; b::Array{Float64,2} ; c::Array{Float64,2} ; end ; ; Dummy(n) = Dummy(rand(n, n), rand(n, n), rand(n, n)) ; MutableDummy(n) = MutableDummy(rand(n, n), rand(n, n), rand(n, n)) ; ; function crunch_dummy(d, nloops) ; for i = 1:nloops ; @. d.a = d.b * d.c ; end ; nothing ; end ; ; nloops = 1000 ; n = 1024 ; d = Dummy(n) ; mutable_d = MutableDummy(n) ; ; # Compile ; crunch_dummy(d, 1) ; crunch_dummy(mutable_d, 1) ; ; @printf ""Dummy crunching numbers: "" ; @btime crunch_dummy(d, nloops) ; ; @printf ""Mutable dummy crunching numbers: "" ; @btime crunch_dummy(mutable_d, nloops) ; ```",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/42#issuecomment-462544959:166,perform,performance,166,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/42#issuecomment-462544959,1,['perform'],['performance']
Performance,"I'd like to compute a `Field` (ideally in order to write it to a NetCDF file) but only at every `N` grid points. Something like the following example, which tries to compute `u` at every 2 grid points in the vertical direction:. ```julia; using Oceananigans; grid = RectilinearGrid(size = (1, 1, 8), extent = (1,1,1));; model = NonhydrostaticModel(; grid,); u_slices = Field(model.velocities.u, indices=(:, :, 1:2:grid.Nz)); ```. However, the above code fails since `Field` currently doesn't accept `StepRange`s as `indices`, just (I think) `UnitRange`s and `Int`s:. ```; ERROR: LoadError: MethodError: no method matching isinteger(::StepRange{Int64, Int64}). Closest candidates are:; isinteger(::Integer); @ Base number.jl:20; isinteger(::Complex); @ Base complex.jl:148; isinteger(::Rational); @ Base rational.jl:281; ... Stacktrace:; [1] validate_index(idx::StepRange{Int64, Int64}, loc::Center, topo::Bounded, N::Int64, H::Int64); @ Oceananigans.Grids ~/repos/Oceananigans.jl/src/Grids/input_validation.jl:196; [2] map (repeats 3 times); @ ./tuple.jl:318 [inlined]; [3] validate_indices(indices::Tuple{Colon, Colon, StepRange{Int64, Int64}}, loc::Tuple{DataType, DataType, DataType}, topo::Tuple{DataType, DataType, DataType}, sz::Tuple{Int64, Int64, Int64}, halo_sz::Tuple{Int64, Int64, Int64}); ```. Is there a workaround?. CC @iuryt",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3460:579,Load,LoadError,579,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3460,1,['Load'],['LoadError']
Performance,"I'd like to use the [Lambert W function](https://github.com/JuliaMath/LambertW.jl) in a `KernelFunctionOperation` but it doesn't seem to work on the GPU. Here's a MWE that works on the CPU:. ```julia; using Oceananigans; using Oceananigans.Grids: xnode, ynode; using CUDA: has_cuda_gpu; using LambertW: lambertw. arch = has_cuda_gpu() ? GPU() : CPU(); grid = RectilinearGrid(arch, size = (4, 4, 4), extent = (1,1,1)). @inline W(x, y) = lambertw((y/x)^2); @inline W(i, j, k, grid) = W(xnode(i, grid, Center()), ynode(j, grid, Center())); op = KernelFunctionOperation{Center, Center, Center}(W, grid); compute!(Field(op)); ```. When running on a GPU this throws a huge error message, that you can check in full [here](https://github.com/CliMA/Oceananigans.jl/files/14013534/error.txt), but here are the first few lines:. ```; ERROR: LoadError: InvalidIRError: compiling MethodInstance for Oceananigans.AbstractOperations.gpu__compute!(::KernelAbstractions.CompilerMetadata{KernelAbstractions.NDIteration.StaticSize{(4, 4, 4)}, KernelAbstractions.NDIteration.DynamicCheck, Nothing, Nothing, KernelAbstractions.NDIteration.NDRange{3, KernelAbstractions.NDIteration.StaticSize{(1, 1, 4)}, KernelAbstractions.NDIteration.StaticSize{(16, 16, 1)}, Nothing, Nothing}}, ::OffsetArrays.OffsetArray{Float64, 3, CUDA.CuDeviceArray{Float64, 3, 1}}, ::KernelFunctionOperation{Center, Center, Center, RectilinearGrid{Float64, Periodic, Periodic, Bounded, Float64, Float64, Float64, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, Nothing}, Float64, typeof(W), Tuple{}}, ::Tuple{Colon, Colon, Colon}) resulted in invalid LLVM IR; Reason: unsupported call to an unknown function (call to j",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3438:831,Load,LoadError,831,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3438,1,['Load'],['LoadError']
Performance,"I'll close this for now as it's not clear what we can do to make it better, and `@inbounds` helps a lot. We can revisit if we find ourselves in dire need of more performance out of the forcing functions.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/365#issuecomment-525333615:162,perform,performance,162,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/365#issuecomment-525333615,1,['perform'],['performance']
Performance,"I'll explain quickly what is happening to document it.; In this new PR I added a new way to check boundary stencils for advection so that the correct reconstruction method is always used. . This entails checking differently for `Face` reconstructions (where we have to ensure that `Center` locations are active) and vice versa for `Center` reconstructions where `Face` locations have to be active. The problem occurs when checking the last cell for `Center` reconstructions (on a `Periodic` direction); let's say the advection is centered order 4 so hypothetically it requires two halo points. ; We then need to ensure that the nodes at `N + 1` and `N + 2` are active. A `Face` node (i) is active if either centered cell (i) or (i+1) is active, which means that the check will be performed on cells `N+1`, `N+2` and `N+3` (one more than the required halo size = 2!). This is not a problem for a underlying grid where the `inactive_node` function can check out-of-bounds locations and will just return a `true`. On the other hand, it is a problem for an `ImmersedBoundary` where a conditional has to be evaluated against an AbstractArray. . My first solution was to increase by one the halo under the hood in the `ImmersedBoundaryGrid` constructor. This bug with `set!` demonstrated that this is probably not the best solution as this can have a lot of unwanted repercussions. The way I implemented it now is that, when performing the halo checking in the model constructor, if the grid is an `ImmersedBoundaryGrid`, the `required_halo` is incremented by one, and an appropriate warning message is displayed",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2603#issuecomment-1171677971:780,perform,performed,780,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2603#issuecomment-1171677971,2,['perform'],"['performed', 'performing']"
Performance,"I'll wait till it's ready to review in detail, but my main high-level comment is that we need to ensure that, at the end of a time-step, both the prognostic state (horizontal velocities and tracers) and auxiliary state (pressure, vertical velocity, eddy diffusivities) are all consistent and available for output at the current model time. If we intertwine communication with the computation of the auxiliary state and tendencies, then we should _define_ the tendencies as part of the auxiliary state. This will change the semantics and logic of the time stepping loop. But I think it at least as rational as our previous organization of events. The main change is that tendencies will now be computed one ""extra"" time in a simulation (at the very last time-step, the tendencies are not needed if no further time-steps will be taken). In the vast majority of cases this extra cost is negligible because simulations run for hundreds or hundreds of thousands of time-steps. There is the slightly possibility of pessimizing the edge case of a simulation that takes one time step, which may be useful for parameter estimation. For that purpose we may want to avoid computing the ""extra"" tendency. I'm thinking though that we should save that additional optimization until we need it.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2953#issuecomment-1452351616:1249,optimiz,optimization,1249,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2953#issuecomment-1452351616,1,['optimiz'],['optimization']
Performance,"I'm encountering an odd behavior when creating a `NetCDFWriter`. In the example below `writer2` gets created successfully, but `writer1` does not. . ```julia; using Oceananigans. N = 4; grid = RectilinearGrid(topology = (Periodic, Periodic, Periodic),; size = (N, N, N),; extent = (1,1,1)). model = NonhydrostaticModel(; grid,). u, v, w = model.velocities; indices = (1,:,:); slice1 = Field(u, indices=indices); slice2 = Field(u). writer2 = NetCDFOutputWriter(model, (; slice2,);; filename = ""mwe2.nc"",; schedule = TimeInterval(1),; overwrite_existing = true,; indices=indices,; ). writer1 = NetCDFOutputWriter(model, (; slice1,),; filename = ""mwe1.nc"",; schedule = TimeInterval(1),; overwrite_existing = true); ```. When running this I get this error:. ```; ERROR: LoadError: BoundsError: attempt to access 1×10×10 view(::Array{Float64, 3}, 4:4, :, :) with eltype Float64 at index [4:7, 4:7, 4:7]; Stacktrace:; [1] throw_boundserror(A::SubArray{Float64, 3, Array{Float64, 3}, Tuple{UnitRange{Int64}, Base.Slice{Base.OneTo{Int64}}, Base.Slice{Base.OneTo{Int64}}}, false}, I::Tuple{UnitRange{Int64}, UnitRange{Int64}, UnitRange{Int64}}); @ Base ./abstractarray.jl:651; [2] checkbounds; @ ./abstractarray.jl:616 [inlined]; [3] view; @ ./subarray.jl:177 [inlined]; [4] offset_windowed_data(data::OffsetArrays.OffsetArray{Float64, 3, SubArray{Float64, 3, Array{Float64, 3}, Tuple{UnitRange{Int64}, Base.Slice{Base.OneTo{Int64}}, Base.Slice{Base.OneTo{Int64}}}, false}}, loc::Tuple{DataType, DataType, DataType}, grid::RectilinearGrid{Float64, Periodic, Periodic, Periodic, Float64, Float64, Float64, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}}}, CPU}, indices::Tuple{UnitRange{Int64}, UnitRange",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2497:766,Load,LoadError,766,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2497,1,['Load'],['LoadError']
Performance,"I'm facing an error that I'm not able to pinpoint. Running the following example on a CPU works, but I get an error on GPUs:. ```julia; using Oceananigans; using CUDA. arch = has_cuda_gpu() ? GPU() : CPU(). grid_base = RectilinearGrid(arch, size=(4, 4, 4), extent=(1, 1, 1)); bathymetry(x, y) = -0.5; grid = ImmersedBoundaryGrid(grid_base, GridFittedBottom(bathymetry)). noflux = FluxBoundaryCondition(0); b_bcs = FieldBoundaryConditions(immersed=noflux). model = NonhydrostaticModel(grid = grid,; buoyancy = BuoyancyTracer(),; tracers = :b,; boundary_conditions = (b=b_bcs,),; ). simulation = Simulation(model, Δt=1, stop_iteration=100); run!(simulation); ```. The error:. ```; ERROR: LoadError: InvalidIRError: compiling kernel gpu_calculate_Gc!(Cassette.Context{nametype(CUDACtx), Nothing, Nothing, KernelAbstractions.var""##PassType#257"", Nothing, Cassette.DisableHooks}, typeof(Oceananigans.Models.NonhydrostaticModels.gpu_calculate_Gc!), KernelAbstractions.CompilerMetadata{KernelAbstractions.NDIteration.StaticSize{(4, 4, 4)}, KernelAbstractions.NDIteration.DynamicCheck, Nothing, Nothing, KernelAbstractions.NDIteration.NDRange{3, KernelAbstractions.NDIteration.StaticSize{(1, 1, 4)}, KernelAbstractions.NDIteration.StaticSize{(16, 16, 1)}, Nothing, Nothing}}, OffsetArrays.OffsetArray{Float64, 3, CuDeviceArray{Float64, 3, 1}}, ImmersedBoundaryGrid{Float64, Periodic, Periodic, Bounded, RectilinearGrid{Float64, Periodic, Periodic, Bounded, Float64, Float64, Float64, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}}}, Nothing}, GridFittedBottom{OffsetArrays.OffsetMatrix{Float64, CuDeviceMatrix{Float64, 1}}}, Nothing}, Val{1}, CenteredSecondOrder, Nothing, ImmersedBoundaryCondition{Bo",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2558:686,Load,LoadError,686,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2558,1,['Load'],['LoadError']
Performance,"I'm getting an error when trying to compile constant Smagorinsky:. ```julia; ERROR: LoadError: InvalidIRError: compiling #12(RegularCartesianGrid{Float64,StepRangeLen{Float64,Base.TwicePrecision{Flo; at64},Base.TwicePrecision{Float64}}}, PlanetaryConstants{Float64}, LinearEquationOfState{Float64}, ConstantSmagorinsky{Fl; oat64}, CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global}, CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global; }, CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global}, CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global}, CU; DAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global}, CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global}, CUDAnat; ive.CuDeviceArray{Float64,3,CUDAnative.AS.Global}, CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global}, CUDAnative.C; uDeviceArray{Float64,3,CUDAnative.AS.Global}, CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global}, CUDAnative.CuDevi; ceArray{Float64,3,CUDAnative.AS.Global}, Forcing{typeof(Oceananigans.zero_func),typeof(Oceananigans.zero_func),typeof(Oce; ananigans.zero_func),typeof(Oceananigans.zero_func),typeof(Oceananigans.zero_func)}) resulted in invalid LLVM IR; Reason: unsupported call to the Julia runtime (call to jl_f__apply); Stacktrace:; [1] overdub at /data5/glwagner/.julia/packages/Cassette/xggAf/src/context.jl:260; [2] ν_ccc at /data5/glwagner/Projects/Oceananigans.jl/src/closures/constant_smagorinsky.jl:109; [3] ν_Σᵢⱼ at /data5/glwagner/Projects/Oceananigans.jl/src/closures/closure_operators.jl:405; [4] ∂x_faa at /data5/glwagner/Projects/Oceananigans.jl/src/closures/closure_operators.jl:64; [5] ∂x_2ν_Σ₁₁ at /data5/glwagner/Projects/Oceananigans.jl/src/closures/closure_operators.jl:409; [6] ∂ⱼ_2ν_Σ₁ⱼ at /data5/glwagner/Projects/Oceananigans.jl/src/closures/closure_operators.jl:432; [7] calculate_interior_source_terms! at /data5/glwagner/Projects/Oceananigans.jl/src/time_steppers.jl:152; [8] #12 at /data5/glwagner/.julia/packages/GPUifyLoops/hBRid/src/context.jl:136",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/248:84,Load,LoadError,84,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/248,1,['Load'],['LoadError']
Performance,"I'm getting an error when trying to compile constant Smagorinsky:. ```julia; ERROR: LoadError: InvalidIRError: compiling #12(RegularCartesianGrid{Float64,StepRangeLen{Float64,Base.TwicePrecision{Flo; at64},Base.TwicePrecision{Float64}}}, PlanetaryConstants{Float64}, LinearEquationOfState{Float64}, ConstantSmagorinsky{Fl; oat64}, CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global}, CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global; }, CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global}, CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global}, CU; DAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global}, CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global}, CUDAnat; ive.CuDeviceArray{Float64,3,CUDAnative.AS.Global}, CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global}, CUDAnative.C; uDeviceArray{Float64,3,CUDAnative.AS.Global}, CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global}, CUDAnative.CuDevi; ceArray{Float64,3,CUDAnative.AS.Global}, Forcing{typeof(Oceananigans.zero_func),typeof(Oceananigans.zero_func),typeof(Oce; ananigans.zero_func),typeof(Oceananigans.zero_func),typeof(Oceananigans.zero_func)}) resulted in invalid LLVM IR; Reason: unsupported call to the Julia runtime (call to jl_f__apply); Stacktrace:; [1] overdub at /data5/glwagner/.julia/packages/Cassette/xggAf/src/context.jl:260; [2] ν_ccc at /data5/glwagner/Projects/Oceananigans.jl/src/closures/constant_smagorinsky.jl:109; [3] ν_Σᵢⱼ at /data5/glwagner/Projects/Oceananigans.jl/src/closures/closure_operators.jl:405; [4] ∂x_faa at /data5/glwagner/Projects/Oceananigans.jl/src/closures/closure_operators.jl:64; [5] ∂x_2ν_Σ₁₁ at /data5/glwagner/Projects/Oceananigans.jl/src/closures/closure_operators.jl:409; [6] ∂ⱼ_2ν_Σ₁ⱼ at /data5/glwagner/Projects/Oceananigans.jl/src/closures/closure_operators.jl:432; [7] calculate_interior_source_terms! at /data5/glwagner/Projects/Oceananigans.jl/src/time_steppers.jl:152; [8] #12 at /data5/glwagner/.julia/packages/GPUifyLoops/hBRid/src/context.jl:136",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/245#issuecomment-496472606:84,Load,LoadError,84,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/245#issuecomment-496472606,1,['Load'],['LoadError']
Performance,"I'm getting this error when trying to run my code on multiple GPUs (it works fine on CPUs):. ```julia; ERROR: LoadError: BoundsError: attempt to access Tuple{Vector{CuStream}} at index [2]; Stacktrace:; [1] getindex(t::Tuple, i::Int64); @ Base ./tuple.jl:29; [2] (::Oceananigans.Architectures.var""#3#6"")(); @ Oceananigans.Architectures /glade/work/tomasc/.julia/packages/Oceananigans/0tK7e/src/Architectures.jl:26; [3] lock(f::Oceananigans.Architectures.var""#3#6"", l::ReentrantLock); @ Base ./lock.jl:185; [4] next_stream; @ /glade/work/tomasc/.julia/packages/Oceananigans/0tK7e/src/Architectures.jl:24 [inlined]; [5] (::KernelAbstractions.Kernel{CUDAKernels.CUDADevice, KernelAbstractions.NDIteration.StaticSize{(1, 36)}, KernelAbstractions.NDIteration.StaticSize{(36, 1)}, typeof(Oceananigans.BoundaryConditions.gpu_fill_periodic_south_and_north_halo!)})(::CuArray{Float64, 3, CUDA.Mem.DeviceBuffer}, ::Vararg{Any}; ndrange::Nothing, dependencies::CUDAKernels.CudaEvent, workgroupsize::Nothing, progress::Function); @ CUDAKernels /glade/work/tomasc/.julia/packages/CUDAKernels/kCOA4/src/CUDAKernels.jl:218; [6] launch!(::GPU, ::RectilinearGrid{Float64, Periodic, Periodic, Bounded, Float64, Float64, OffsetArrays.OffsetVector{Float64, CuArray{Float64, 1, CUDA.Mem.DeviceBuffer}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVector{Float64, CuArray{Float64, 1, CUDA.Mem.DeviceBuffer}}, GPU}, ::Tuple{Int64, Int64}, ::typeof(Oceananigans.BoundaryConditions.fill_periodic_south_and_north_halo!), ::CuArray{Float64, 3, CUDA.Mem.DeviceBuffer}, ::Vararg{Any}; dependencies::CUDAKernels.CudaEvent, include_right_boundaries::Bool, reduced_dimensions::Tuple{Int64}, location::Nothing, kwargs::Base.Pairs{Symbol, Union{}, Tuple{}, NamedTuple{(), Tuple{}}}); @ Oceananigans.Utils /gl",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2795#issuecomment-1310681185:110,Load,LoadError,110,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2795#issuecomment-1310681185,1,['Load'],['LoadError']
Performance,I'm not sure --- but it's possible that some GPU utilities not under Oceananigans.jl control incur memory allocations. I don't think we have much GPU-specific code in our codebase (except for pressure solvers...) Definitely a good thing to keep tabs on and open issues in the relevant packages if it affects the performance of our code.,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1514#issuecomment-844319959:312,perform,performance,312,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1514#issuecomment-844319959,1,['perform'],['performance']
Performance,"I'm not sure if this is known (or even expected) or not, but with recent versions of Oceananigans I've noticed that we're allocating more and more memory on GPUs. Basically the scenario is that I've running simulations with a given script for the past 6 months or so. The simulation and model itself has stayed the same, I've been running on the same GPUs, and the only changes that I made were those forced by changes in Oceanigans. . When I started this setup, I could run up to ~105 million points. Then as the versions increased I had to decrease to size of simulations to 100 million and then to 95 million. Now with the change to Julia 1.8 (I had been using Julia 1.6 until version 0.77.5) there was a steep increase in memory use and my max size went from 95 to 80 million. Compared to my original size of ~105 million, that's a decrease of about 25% in the grid size. Has anyone else noticed this? Is this expected since we added more features/flexibility to the code lately?. A few notes:. - My simulation is pretty complex (it's got IBM, forcings for every variable, a tracer, LES closures and drag BCs), so I think for most other simulations the max size would be smaller; - I understand that part of the memory allocation depends on the package dependencies (and the Julia version itself), not being directly controlled by Oceananigans code; - It's also worth noting that since the cluster I use (Casper) hasn't installed Julia 1.8, I started trying the newest version from the pre-compiled binaries for Julia 1.8. I don't know if that can somehow affect the memory allocation since the binaries aren't optimized for that specific machine (I haven't yet tried compiling Julia 1.8 from source). CC @wenegrat @whitleyv",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2794:1615,optimiz,optimized,1615,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2794,1,['optimiz'],['optimized']
Performance,I'm not sure if we've tested but I've assumed there is a performance benefit to the simpler version for regularly spaced grids rather than using the binary search. It would probably be sensible to change the differentiation between the methods to just `fractional_index` though.,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3356#issuecomment-1775532157:57,perform,performance,57,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3356#issuecomment-1775532157,1,['perform'],['performance']
Performance,"I'm not sure why this was added, so we will see if something breaks. But through usage I've realized that its inconvenient (and unexpected) that the length of `FieldTimeSeries` depends on the backend. My intuition is that the length stays the same regardless of _where_ the data is (in memory, or on disk, or a combination of the two). It's also helpful that behavior is the same between backends, which makes switching backends for performance reasons painless.",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3825:433,perform,performance,433,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3825,1,['perform'],['performance']
Performance,"I'm only running the x64 builds so this should reduce Appveyor build times by a half to ~1 hour. Closing this issue as there's not much else we can do except remove dependencies or pay for better resources. Build cache could help but we'd have to manually update it each time package versions change, etc. and the extra maintenance isn't worth it.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/89#issuecomment-500234619:213,cache,cache,213,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/89#issuecomment-500234619,1,['cache'],['cache']
Performance,"I'm seeing some significant slow down with the boundary condition. ```julia; # Monin-Obukhov drag coefficient; z₀ = 1e-4 # Charnock roughness; κ = 0.4 # Von Karman constant; Cᴰ(Δz) = (κ / log(Δz / 2z₀))^2. @inline bottom_drag_u(x, y, t, u, w, Cᴰ) = - Cᴰ * u * sqrt(u^2 + w^2); @inline bottom_drag_w(x, y, t, u, w, Cᴰ) = - Cᴰ * w * sqrt(u^2 + w^2); @inline bottom_drag_u(x, y, z, t, u, w, Cᴰ) = - Cᴰ * u * sqrt(u^2 + w^2); @inline bottom_drag_w(x, y, z, t, u, w, Cᴰ) = - Cᴰ * w * sqrt(u^2 + w^2). Δz = 1 / Nz; Δx = 2π / Nz; u_drag_bc = FluxBoundaryCondition(bottom_drag_u, field_dependencies=(:u, :w), parameters=Cᴰ(Δz)); w_drag_bc = FluxBoundaryCondition(bottom_drag_w, field_dependencies=(:u, :w), parameters=Cᴰ(Δx)); u_bcs = FieldBoundaryConditions(bottom=u_drag_bc, immersed=u_drag_bc); w_bcs = FieldBoundaryConditions(immersed=w_drag_bc); ```. This is the basic way to implement a quadratic drag from the interface in this PR. In this case what happens under the hood is that we create 4 `ContinuousBoundaryFunction` for the relevant faces of boundary-adjacent cells (the other 2 faces are normal to the given velocity component, so receive a no-penetration boundary condition). So there could be a type instability compiling all of those (which have been notoriously fickle to compile in the past). We clearly need to hard code quadratic drag though, because for stretched grids and partial cells (and other types of immersed boundaries in the future) we have to do precompute the logarithm of the grid metrics (to use in a Monin-Obukhov-type model) in each direction independently, as well as the logarithm of the roughness. It's too much for this PR though, so I think we should just document how to specify no-slip on immersed boundaries (which appears to be performant), and add a few tests. Then in a future PR we can add a `QuadraticDrag` utility (I have a prototype for this object; others are welcome to collaborate on implementing the necessary functions to support it).",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2437#issuecomment-1104673081:1767,perform,performant,1767,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2437#issuecomment-1104673081,1,['perform'],['performant']
Performance,"I'm sorry, I misinterpreted the results @ali-ramadhan posted. I thought that `CenteredSecondOrder` was 1.0x slower with julia 1.6 than with 1.5 (and that small slowdowns were observed for the other schemes, which is why I recommended testing the biharmonic scheme.) Now I understand that these results are all for julia 1.6; we are comparing the results with previously obtained benchmarks (not posted) for julia 1.5. Looking at @tomchor and @ali-ramadhan's results then it looks like simulations with WENO5 are running approximately 6-8 times slower on julia 1.6 than it was on julia 1.5, while other advection schemes (and closures) are unchanged --- correct?. Is the _CPU_ performance of WENO5 roughly equivalent between julia 1.5 and julia 1.6?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1764#issuecomment-868677634:676,perform,performance,676,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1764#issuecomment-868677634,1,['perform'],['performance']
Performance,"I'm strongly opposed to having multiple small packages. . Maybe sub-modules could be a solution. Loading `Oceananigans.Plotting` can ""dynamically"" install Plots.jl, and loading stuff from `Oceananigans.Output` will dynamically install packages depending on the output writer loaded. We just have to keep things modular and neat/tidy. If the tests are taking a while, then we could split things up into quick unit tests and more comprehensive integration tests, or we need to pay for dedicate CI resources.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/284#issuecomment-501739828:97,Load,Loading,97,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/284#issuecomment-501739828,3,"['Load', 'load']","['Loading', 'loaded', 'loading']"
Performance,"I'm thinking about the future and hoping to start a discussion about the future of our equation abstraction system. ## The problem. We need an abstraction for the concept of an equation, so that we can make the model as performant and lightweight as possible for a given use case. For example, I think we should require that. 1. The memory footprint of our model is no larger than it needs to be for a given problem (no 'extra' allocation of memory for unused tracers, for hydrostatic pressure when running in non hydrostatic mode, etc). 2. We do not perform unnecessary floating-point computations or indexing into arrays (the latter is especially important in GPU code) for unused tracers or hydrostatic pressure fields. 3. Equations are constructed / specified clearly and concisely (both in source code and user scripts). 4. Users can specify arbitrary types of forcing, including numbers, arrays, or functions (solving #110). 5. We can support arbitrary tracers with various features, such as sinking/rising velocities, or reaction systems for biological/chemical tracer systems. When I have talked to various people about this, there was a concern that this system would be 'inelegant' or 'complex'. However I believe an equation abstraction system provides the opposite: with an abstraction system, equations are 'written down' in some logical place (like a file `equations.jl` in the `src` directory where they can be easily read and modified, rather than buried inside a time-stepping loop. Correspondly, our time-stepping code becomes shorter and more concise. Using multiple dispatch correctly, we avoid the `infinite if-statement` problem. This abstraction may also make the code more modular such that we move closer to supporting multiple time-steppers. Below I provide one example of an implementation that would solve some of the problems I listed. However, *this is not the only solution*, and I think we should expend some intellectual effort and have a discussion about what the bes",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/259:220,perform,performant,220,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/259,2,['perform'],"['perform', 'performant']"
Performance,"I'm trying to follow the [tutorial here](https://clima.github.io/OceananigansDocumentation/stable/generated/one_dimensional_diffusion/) using vscode (connected via remote-ssh to ubuntu machine). However, I consistently get errors below (and `@animate` step doesn't finish): ; ```; GKS: GKS not in proper state. GKS must be either in the state WSOP or WSAC in routine ACTIVATE_WS; qt.qpa.xcb: could not connect to display ; qt.qpa.plugin: Could not load the Qt platform plugin ""xcb"" in """" even though it was found.; This application failed to start because no Qt platform plugin could be initialized. Reinstalling the application may fix this problem. Available platform plugins are: eglfs, linuxfb, minimal, minimalegl, offscreen, vnc, xcb. Aborted (core dumped); connect: Connection refused; GKS: can't connect to GKS socket application. GKS: Open failed in routine OPEN_WS; GKS: GKS not in proper state. GKS must be either in the state WSOP or WSAC in routine ACTIVATE_WS; qt.qpa.xcb: could not connect to display ; qt.qpa.plugin: Could not load the Qt platform plugin ""xcb"" in """" even though it was found.; This application failed to start because no Qt platform plugin could be initialized. Reinstalling the application may fix this problem. Available platform plugins are: eglfs, linuxfb, minimal, minimalegl, offscreen, vnc, xcb. Aborted (core dumped); connect: Connection refused; GKS: can't connect to GKS socket application. GKS: Open failed in routine OPEN_WS; GKS: GKS not in proper state. GKS must be either in the state WSOP or WSAC in routine ACTIVATE_WS; qt.qpa.xcb: could not connect to display ; qt.qpa.plugin: Could not load the Qt platform plugin ""xcb"" in """" even though it was found.; This application failed to start because no Qt platform plugin could be initialized. Reinstalling the application may fix this problem. Available platform plugins are: eglfs, linuxfb, minimal, minimalegl, offscreen, vnc, xcb. ```",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1281:448,load,load,448,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1281,3,['load'],['load']
Performance,"I'm trying to run a simulation that has both forcings and some auxiliary variables which get updated with every time-step. However, I found that just the existence of a variable in the `auxiliary_fields`, coupled with a forcing function where parameters are included, makes the simulation fail on the GPU. Here's a MWE:. ```julia; using Oceananigans. grid = RectilinearGrid(GPU(), size=(4, 4, 4), extent = (1, 1, 1)). @inline forc_u(x, y, z, t, u) = x. model = NonhydrostaticModel(; grid, forcing = (; u=Forcing(forc_u, field_dependencies = :u)),; auxiliary_fields = (; a=0)); @show model.forcing model.auxiliary_fields; time_step!(model, 1); ```. the above runs fine on the CPU but fails on the GPU with:. ```; ERROR: LoadError: InvalidIRError: compiling kernel #gpu_calculate_Gu!(KernelAbstractions.CompilerMetadata{KernelAbstractions.NDIteration.StaticSize{(4, 4, 4)}, KernelAbstractions.NDIteration.DynamicCheck, Nothing, Nothing, KernelAbstractions.NDIteration.NDRange{3, KernelAbstractions.NDIteration.StaticSize{(1, 1, 4)}, KernelAbstractions.NDIteration.StaticSize{(16, 16, 1)}, Nothing, Nothing}}, OffsetArrays.OffsetArray{Float64, 3, CuDeviceArray{Float64, 3, 1}}, RectilinearGrid{Float64, Periodic, Periodic, Bounded, Float64, Float64, Float64, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, Nothing}, Centered{1, Float64, Nothing, Nothing, Nothing, Nothing}, Nothing, Nothing, Nothing, BoundaryCondition{Oceananigans.BoundaryConditions.Flux, Nothing}, Nothing, NamedTuple{(:velocities, :tracers), Tuple{NamedTuple{(:u, :v, :w), Tuple{Oceananigans.Fields.ZeroField{Int64, 3}, Oceananigans.Fields.ZeroField{Int64, 3}, Oceananigans.Fields.ZeroField{Int64, 3}}},",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3025:719,Load,LoadError,719,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3025,1,['Load'],['LoadError']
Performance,"I'm trying to run the hot bubble example but using a GPU. However I get an error when defining the model that I cannot understand (the caveat here being that I have zero experience working with GPUs). The minimum code to reproduce this is. ```; using Oceananigans; model = BasicModel(N=(256, 1, 256), L=(2000, 1, 2000), architecture=GPU(), ν=4e-2, κ=4e-2); ```. This is the error that I get:. ```; ERROR: LoadError: Device capability v2.1.0 not supported by available toolchain; Stacktrace:; [1] error(::String) at ./error.jl:33; [2] supported_capability(::CUDAdrv.CuDevice) at /home/tomaschor/.julia/packages/CUDAnative/Lr0yj/src/utils.jl:7; [3] macro expansion at /home/tomaschor/.julia/packages/CUDAnative/Lr0yj/src/execution.jl:388 [inlined]; [4] #cufunction#176(::Nothing, ::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}, ::typeof(CUDAnative.cufunction), ::getfield(GPUArrays, Symbol(""##23#24"")), ::Type{Tuple{CuArrays.CuKernelState,CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Int64}}}}) at /home/tomaschor/.julia/packages/CUDAnative/Lr0yj/src/execution.jl:357; [5] cufunction(::Function, ::Type) at /home/tomaschor/.julia/packages/CUDAnative/Lr0yj/src/execution.jl:357; [6] macro expansion at /home/tomaschor/.julia/packages/CUDAnative/Lr0yj/src/execution.jl:174 [inlined]; [7] macro expansion at ./gcutils.jl:87 [inlined]; [8] macro expansion at /home/tomaschor/.julia/packages/CUDAnative/Lr0yj/src/execution.jl:171 [inlined]; [9] _gpu_call(::CuArrays.CuArrayBackend, ::Function, ::CuArrays.CuArray{Float64,3}, ::Tuple{CuArrays.CuArray{Float64,3},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64},Base.OneTo{Int64},Base.OneTo{Int64}},typeof(identity),Tuple{Int64}}}, ::Tuple{Tuple{Int64},Tuple{Int64}}) at /home/tomaschor/.julia/packages/CuArrays/wXQp8/src/gpuarray_interface.jl:60; [10] gpu_call(::Function, ::CuArrays.CuArray{Fl",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/451:405,Load,LoadError,405,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/451,1,['Load'],['LoadError']
Performance,"I'm trying to run the near global shallow water model and having difficulties with the latitude longitude grid. The line that calls it is here. https://github.com/CliMA/Oceananigans.jl/blob/5ba82e5786d1b7725d6bfa3e45072d2f6fad487a/validation/shallow_water_model/near_global_shallow_water_quarter_degree.jl#L95. And the error that I get is the following,. ```; julia> include(""near_global_shallow_water_quarter_degree.jl""); ┌ Warning: Over-writing registration of the datadep; │ name = ""quarter_degree_near_global_lat_lon""; └ @ DataDeps ~/.julia/packages/DataDeps/Y2lje/src/registration.jl:15; ERROR: LoadError: UndefVarError: `Nλ` not defined; Stacktrace:; [1] validate_lat_lon_grid_args(topology::Tuple{…}, size::Tuple{…}, halo::Tuple{…}, FT::Type, latitude::Tuple{…}, longitude::Tuple{…}, z::Nothing, precompute_metrics::Bool); @ Oceananigans.Grids ~/Software/Oceananigans.jl/src/Grids/latitude_longitude_grid.jl:257; [2] LatitudeLongitudeGrid(architecture::GPU, FT::DataType; size::Tuple{…}, longitude::Tuple{…}, latitude::Tuple{…}, z::Nothing, radius::Float64, topology::Tuple{…}, precompute_metrics::Bool, halo::Tuple{…}); @ Oceananigans.Grids ~/Software/Oceananigans.jl/src/Grids/latitude_longitude_grid.jl:189; [3] macro expansion; @ show.jl:1181 [inlined]; [4] top-level scope; @ ~/Software/Oceananigans.jl/validation/shallow_water_model/near_global_shallow_water_quarter_degree.jl:95; [5] include(fname::String); @ Base.MainInclude ./client.jl:489; [6] top-level scope; @ REPL[70]:1; [7] top-level scope; @ ~/.julia/packages/CUDA/nbRJk/src/initialization.jl:205; in expression starting at /home/fpoulin/Software/Oceananigans.jl/validation/shallow_water_model/near_global_shallow_water_quarter_degree.jl:95; Some type information was truncated. Use `show(err)` to see complete types.; ```. Should the lat-lon grid be able to handle flat in the vertical? @simone-silvestri, maybe this is something you tried to fix the last time we chatted?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3486#issuecomment-1972247275:600,Load,LoadError,600,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3486#issuecomment-1972247275,1,['Load'],['LoadError']
Performance,"I'm trying to time the performance of this branch but I'm getting a out-of-bounds error when running my simulations on GPUs (haven't tried them on CPUs). I can reproduce it using the MWE below and I'm pretty sure it's related to the stretched grid:. ```julia; using Oceananigans.Units; using CUDA; using Oceananigans.ImmersedBoundaries: ImmersedBoundaryGrid, GridFittedBottom. if has_cuda_gpu(); arch = GPU(); else; arch = CPU(); end. grid_base = RectilinearGrid(arch,; size=(4, 4, 4),; x=(0, 1), y=(0,1),; z=0:0.25:1,; halo=(3,3,3),; ). bathymetry(x, y) = grid_base.Lz/2; grid = ImmersedBoundaryGrid(grid_base, GridFittedBottom(bathymetry)). model = NonhydrostaticModel(grid = grid,; advection = WENO5(grid_base),; ). using Oceanostics: TimedProgressMessenger; simulation = Simulation(model, Δt=1,; stop_iteration=10,; ); run!(simulation); ```",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2603#issuecomment-1162289700:23,perform,performance,23,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2603#issuecomment-1162289700,1,['perform'],['performance']
Performance,"I'm trying to validate a tilted bottom boundary layer example for https://github.com/CliMA/Oceananigans.jl/pull/1242 and I'm trying to use a `VerticallyStretchedGrid` to save computational resource (since I'm trying to do it on my laptop). I noticed that the simulation fails when using the `TimeStepWizard`:. ```julia; ERROR: LoadError: type VerticallyStretchedRectilinearGrid has no field Δz; Stacktrace:; [1] getproperty(::VerticallyStretchedRectilinearGrid{Float64,Periodic,Periodic,Bounded,OffsetArrays.OffsetArray{Float64,1,StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}},OffsetArrays.OffsetArray{Float64,1,Array{Float64,1}}}, ::Symbol) at ./Base.jl:33; [2] cell_advection_timescale(::Array{Float64,3}, ::Array{Float64,3}, ::Array{Float64,3}, ::VerticallyStretchedRectilinearGrid{Float64,Periodic,Periodic,Bounded,OffsetArrays.OffsetArray{Float64,1,StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}},OffsetArrays.OffsetArray{Float64,1,Array{Float64,1}}}) at /home/tomas/repos2/Oceananigans.jl/src/Utils/cell_advection_timescale.jl:9; [3] cell_advection_timescale(::IncompressibleModel{Oceananigans.TimeSteppers.QuasiAdamsBashforth2TimeStepper{Float64,NamedTuple{(:u, :v, :w, :T, :S),Tuple{Field{Face,Center,Center,OffsetArrays.OffsetArray{Float64,3,Array{Float64,3}},VerticallyStretchedRectilinearGrid{Float64,Periodic,Periodic,Bounded,OffsetArrays.OffsetArray{Float64,1,StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}},OffsetArrays.OffsetArray{Float64,1,Array{Float64,1}}},NamedTuple{(:x, :y, :z),Tuple{CoordinateBoundaryConditions{BoundaryCondition{Oceananigans.BoundaryConditions.Periodic,Nothing},BoundaryCondition{Oceananigans.BoundaryConditions.Periodic,Nothing}},CoordinateBoundaryConditions{BoundaryCondition{Oceananigans.BoundaryConditions.Periodic,Nothing},BoundaryCondition{Oceananigans.BoundaryConditions.Periodic,Nothing}},CoordinateBoundaryConditions{BoundaryCondition{Flux,Nothing},Bou",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1465:327,Load,LoadError,327,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1465,1,['Load'],['LoadError']
Performance,"I'm wondering if we should provide a separate page on ""Using GPUs""? While the simulation tips for CPUs are really performance optimizations that are optional, the GPU simulation tips are mostly required to run without errors. There's a few other things that are required to get things working on GPUs --- for example, Oceananigans must be _built_ (not just run) with a GPU / CUDA installation available; this is a common pitfall on clusters.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1543#issuecomment-818076711:114,perform,performance,114,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1543#issuecomment-818076711,2,"['optimiz', 'perform']","['optimizations', 'performance']"
Performance,"I've already made sure the forcing function doesn't reference anything outside the function (reduces clarity unfortunately) and pasted the benchmarks using the script from PR #370 at the bottom. Did not try changing the function signature to `FT(grid, u, v, w, T, S, i, j, k)` as that would make implementing #25 more difficult. Also, I was kind of lazy. Adding `@inbounds` seems to help a lot. Went from being 2.1x slower to being 1.3x slower. Still a significant slowdown considering that these forcing functions aren't as computationally demanding as the rest of the right-hand-side calculation. But good enough for me right now. It can be a very powerful feature (essentially replacing the MITgcm RBCS package, for one example) so would be good to get maximum performance out of the forcing functions. But it will probably always depend on exactly how you write them. So might make sense to have guidelines on writing ""performant forcing functions"" in the documentation. ---; Attempt 1:; ```julia; @inline function Fu(grid, U, Φ, i, j, k); if k == 1; return -2*0.1/grid.Δz^2 * (U.u[i, j, 1] - 0); elseif k == grid.Nz; return -2*0.1/grid.Δz^2 * (U.u[i, j, grid.Nz] - 0); else; return 0; end; end. @inline FT(grid, U, Φ, i, j, k) = ifelse(k == 1, -1e-4 * (Φ.T[i, j, 1] - 0), 0); ```; ```; ──────────────────────────────────────────────────────────────────────────────────────────────────; Forcing function benchmarks Time Allocations ; ────────────────────── ───────────────────────; Tot / % measured: 59.9s / 0.41% 7.38GiB / 0.36% . Section ncalls time %tot avg alloc %tot avg; ──────────────────────────────────────────────────────────────────────────────────────────────────; 128×128×128 with forcing (GPU, Float64) 10 166ms 68.2% 16.6ms 13.8MiB 51.2% 1.38MiB; 128×128×128 no forcing (GPU, Float64) 10 77.4ms 31.8% 7.74ms 13.1MiB 48.8% 1.31MiB; ──────────────────────────────────────────────────────────────────────────────────────────────────; ```; ---; Attempt 2:; ```julia. @inline function Fu",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/365#issuecomment-525326208:764,perform,performance,764,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/365#issuecomment-525326208,2,['perform'],"['performance', 'performant']"
Performance,"I've been trying to run [this code](https://github.com/CliMA/Oceananigans.jl/blob/00c98a72943cfaaa3b034770561b7ed6a408de40/benchmark/distributed_nonhydrostatic_model_mpi.jl), and I get an error depending on the number of points and ranks I choose in each direction. For example I noticed that when `Nx*Rx == Ny*Ry == Nz*Rz` the code runs successfully. But if that condition isn't satisfied (for example if I set `Nx=Ny=Nz=8`, `Rx=Rz=1` and `Ry=2`) I get an error like this:. ```; ERROR: ERROR: LoadError: LoadError: DimensionMismatch(DimensionMismatch(""arrays could not be broadcast to a common size; got a dimension wi""arrays could not be broadcast to a common size; got a dimension with lengths 8 andth lengths 8 and 4"") 4""); Stacktrace:; [1] ; Stacktrace:; [1] _bcs1_bcs1; @ ./; @ ./broadcast.jl:broadcast.jl:501 [inlined]; 501 [inlined]; [2] [2] _bcs(_bcs(shape::shape::Tuple{Base.OneTo{Int64}, Base.OneTo{Int64}, Base.OneTo{Int64}}, newshape::Tuple{Base.OneTo{Int64}, Base.OneTo{Int64}, Base.OneTo{Int64}}, newshape::Tuple{Base.OneTo{Int64}, Base.OneTo{Int64}, Base.OneTo{Int64}}); @ Tuple{Base.OneTo{Int64}, Base.OneTo{Int64}, Base.OneTo{Int64}}); @ Base.Broadcast ./broadcast.jl:Base.Broadcast ./broadcast.jl:495; [3]495; [3] broadcast_shape; @ broadcast_shape; @ ././broadcast.jl:broadcast.jl:489489 [inlined]; [inlined]; [4] [4] combine_axes combine_axes; @ ; @ ././broadcast.jl:broadcast.jl:484 [inlined]484 [inlined]. [5] [5] _axes_axes; @ ./; @ ./broadcast.jl:209broadcast.jl:209 [inlined]; [6] [inlined]; [6] axes; @ axes; @ ././broadcast.jl:207broadcast.jl:207 [inlined]; [inlined]; [7] [7] _unwrap_pa(bc::_unwrap_pa(bc::Base.Broadcast.Broadcasted{PencilArrays.PencilArrayStyle{3}, Nothing, typeof(/), Tuple{Base.Broadcast.Broadcasted{PencilArrays.PencilArrayStyle{3}, Nothing, typeof(-), Tuple{PencilArrays.PencilArrayBroadcastable{ComplexF64, 3, PencilArrays.PencilArray{ComplexF64, 3, Base.ReshapedArray{ComplexF64, 3, SubArray{ComplexF64, 1, Vector{ComplexF64}, Tuple{Base.OneTo{Int6",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2445:494,Load,LoadError,494,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2445,2,['Load'],['LoadError']
Performance,"I've been using `FieldTimeSeries` a lot over the last few weeks and have noticed a couple of issues calculating means and constructing operations on them. First, `mean` with `dims` only calculates it on the first time index (I think). For example, if I do `mean(f, dims = (1, 2, 3))` it returns a size `[1, 1, 1]` array vs calculating `mean(interior(f), dims = (1, 2, 3))` which returns a `[1, 1, 1, Nt]` array. I suspect it is also calculating means with no dimensions specified incorrectly. Second, constructing operators on field time series fails and returns operators with dimensions `[Nx, Ny, Nz]` which if you then try and index into fails as it tries to index into the underlying fields which have a time dimension. For example, `speed = √(u^2 + v^2 + w^2)` gives `speed` with size `[Nx, Ny, Nz]`, if I then try and index at `[1, 1, 1]` it throws a bounds error trying to access `Nx x Ny x Nz x Nt` array at `1, 1, 1`. Neither of these is particularly important but thought I would document in case anyone else has issues, and because I will try and fix them at some point. For reference, I am currently getting around this by calculating means on the interior (which I think would produce the wrong results with immersed boundaries because it wouldn't have the masking step), and by just calculating arrays like `speed = √(interior(u)^2 + interior(v)^2 + interior(w)^2)` which doesn't lose me too much performance since I end up indexing into the whole array anyway.; ```[tasklist]; ### Tasks; ```",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3144:1411,perform,performance,1411,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3144,1,['perform'],['performance']
Performance,"I've gotten the transforms taken care of, but now the baroclinic_adjustment example fails with ; ```; $ julia --project=. baroclinic_adjustment.jl ; ERROR: LoadError: Scalar indexing is disallowed.; Invocation of getindex resulted in scalar indexing of a GPU array.; This is typically caused by calling an iterating implementation of a method.; Such implementations *do not* execute on the GPU, but very slowly on the CPU,; and therefore are only permitted from the REPL for prototyping purposes.; If you did intend to index this array, annotate the caller with @allowscalar.; Stacktrace:; [1] error(s::String); @ Base ./error.jl:35; [2] assertscalar(op::String); @ GPUArraysCore ~/.julia/packages/GPUArraysCore/uOYfN/src/GPUArraysCore.jl:103; [3] getindex; @ ~/.julia/packages/GPUArrays/dAUOE/src/host/indexing.jl:48 [inlined]; [4] scalar_getindex(::ROCArray{Float64, 3, AMDGPU.Runtime.Mem.HIPBuffer}, ::Int64, ::Vararg{Int64}); @ GPUArrays ~/.julia/packages/GPUArrays/dAUOE/src/host/indexing.jl:34; [5] _getindex; @ ~/.julia/packages/GPUArrays/dAUOE/src/host/indexing.jl:17 [inlined]; [6] getindex; @ ~/.julia/packages/GPUArrays/dAUOE/src/host/indexing.jl:15 [inlined]; [7] getindex; @ ./subarray.jl:288 [inlined]; [8] macro expansion; @ ./multidimensional.jl:917 [inlined]; [9] macro expansion; @ ./cartesian.jl:64 [inlined]; [10] macro expansion; @ ./multidimensional.jl:912 [inlined]; [11] _unsafe_getindex!; @ ./multidimensional.jl:925 [inlined]; [12] _unsafe_getindex(::IndexCartesian, ::SubArray{Float64, 3, ROCArray{Float64, 3, AMDGPU.Runtime.Mem.HIPBuffer}, Tuple{UnitRange{Int64}, UnitRange{Int64}, UnitRange{Int64}}, false}, ::Int64, ::Base.Slice{Base.OneTo{Int64}}, ::Base.Slice{Base.OneTo{Int64}}); @ Base ./multidimensional.jl:903; [13] _getindex; @ ./multidimensional.jl:889 [inlined]; [14] getindex(::SubArray{Float64, 3, ROCArray{Float64, 3, AMDGPU.Runtime.Mem.HIPBuffer}, Tuple{UnitRange{Int64}, UnitRange{Int64}, UnitRange{Int64}}, false}, ::Int64, ::Function, ::Function); @ Base ",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3475#issuecomment-1947171608:156,Load,LoadError,156,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3475#issuecomment-1947171608,1,['Load'],['LoadError']
Performance,"I've pushed some commits that fix the performance issue. This is done by using the `Val` type to pass the tracer index into the kernel functions that compute `∇_κ_∇c`, rather than using the raw, unwrapped, integer `tracer_index`, eg. https://github.com/climate-machine/Oceananigans.jl/blob/606d40444038d058e32be71655a7239986114a56/src/time_steppers.jl#L172. The flux divergence for constant isotropic diffusivity, for example, is now. https://github.com/climate-machine/Oceananigans.jl/blob/606d40444038d058e32be71655a7239986114a56/src/TurbulenceClosures/constant_isotropic_diffusivity.jl#L42. Benchmarks on cyclops are now. ## Master. ```julia; Oceananigans package status:; Status `~/.julia/environments/v1.1/Project.toml`; [9e8cae18] Oceananigans v0.11.1 #master (https://github.com/climate-machine/Oceananigans.jl.git). ┌ Warning: Performing scalar operations on GPU arrays: This is very slow, consider disallowing these operations with `allowscalar(false)`; └ @ GPUArrays ~/.julia/packages/GPUArrays/fLiQ1/src/indexing.jl:16; Running static ocean benchmark: 32× 32× 32 (GPU, Float64)...; Running static ocean benchmark: 64× 64× 64 (GPU, Float64)...; Running static ocean benchmark: 128×128×128 (GPU, Float64)...; Running static ocean benchmark: 256×256×256 (GPU, Float64)...; ──────────────────────────────────────────────────────────────────────────────────────; Static ocean benchmarks Time Allocations ; ────────────────────── ───────────────────────; Tot / % measured: 82.9s / 0.58% 8.26GiB / 0.37% . Section ncalls time %tot avg alloc %tot avg; ──────────────────────────────────────────────────────────────────────────────────────; 256×256×256 (GPU, Float64) 10 287ms 59.8% 28.7ms 7.73MiB 24.5% 791KiB; 32× 32× 32 (GPU, Float64) 10 75.9ms 15.8% 7.59ms 8.28MiB 26.3% 848KiB; 128×128×128 (GPU, Float64) 10 66.7ms 13.9% 6.67ms 7.79MiB 24.7% 798KiB; 64× 64× 64 (GPU, Float64) 10 50.6ms 10.5% 5.06ms 7.73MiB 24.5% 791KiB; ─────────────────────────────────────────────────────────────────────────",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/452#issuecomment-542262852:38,perform,performance,38,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/452#issuecomment-542262852,2,"['Perform', 'perform']","['Performing', 'performance']"
Performance,"I've ran several different simulations with this branch and a majority (but curiously not all of them) failed with the following error:. `ERROR: LoadError: CUDA error: an illegal memory access was encountered (code 700, ERROR_ILLEGAL_ADDRESS)`. An older version of this branch doesn't show this error and instead runs fine for all simulations. I couldn't figure out what happened yet (or why some runs failed but others didn't) so I couldn't create a MWE yet, but I thought I'd report this in case someone can understand this better than me. Here's a [more complete error log](https://pastebin.com/dCbBAgKp) (although it's only about 5% of the full log since the last few lines repeat many many times).",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2603#issuecomment-1179751296:145,Load,LoadError,145,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2603#issuecomment-1179751296,1,['Load'],['LoadError']
Performance,"I've retried the tests a couple of times but I keep getting errors like this:. ```; Failed to precompile Oceananigans [9e8cae18-63c1-5223-a75c-80ca9d6e9a09] to ""/data5/glwagner/.julia-16001/compiled/v1.10/Oceananigans/jl_Bzx0zi"".; ERROR: LoadError: SystemError: opening file ""/data5/glwagner/.julia-16001/packages/CUDA_Runtime_jll/dOYZJ/.pkg/platform_augmentation.jl"": No such file or directory; ```. Is something wrong with the servers at the moment?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3623#issuecomment-2179086381:238,Load,LoadError,238,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3623#issuecomment-2179086381,1,['Load'],['LoadError']
Performance,"I've tried using the forcing as:. ```julia; @inline sponge_func(x, y, z, ϕ) = -rate * bot_mask(x, y, z) * (ϕ - 0); sponge_u(x, y, z, t, u) = sponge_func(x, y, z, u); sponge_v(x, y, z, t, v) = sponge_func(x, y, z, v); sponge_w(x, y, z, t, w) = sponge_func(x, y, z, w). forc_u = Forcing(sponge_u, field_dependencies=:u,); forc_v = Forcing(sponge_v, field_dependencies=:v,); forc_w = Forcing(sponge_w, field_dependencies=:w,); forcing = (u=forc_u, v=forc_v, w=forc_w); ```; and the same performance issues appear. But I guess this still uses `ContinuousForcing`. I've never used `DiscreteForcing` but I'll try to use it and see what happens. If you have any examples that would help.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1827#issuecomment-875651677:484,perform,performance,484,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1827#issuecomment-875651677,1,['perform'],['performance']
Performance,"Ideally you don't want `CUDA.@allowscalar` in front of `estimate_growth_rate` but rather you wanna go and modify `estimate_growth_rate` to ensure that you only put `CUDA.@allowscalar` in places that does not affect performance. Scalar operations on the GPU could diminish _all_ the speedup you get from the GPU, so `CUDA.@allowscalar` must be used with great caution!! There is a relevant discussion [in the docs](https://clima.github.io/OceananigansDocumentation/stable/simulation_tips/). (I'll convert this Issue into a Discussion btw.)",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3522#issuecomment-2029361635:215,perform,performance,215,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3522#issuecomment-2029361635,1,['perform'],['performance']
Performance,"Ideally you want the script to kill, no? I'm thinking users submitting jobs on HPC and ideally you don't want to waste SUs on computing NaNs or having the job there halted because simulation stopped but the job wasn't killed and was waiting there for the rest of the requested job queued time. Right?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1198#issuecomment-733395654:281,queue,queued,281,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1198#issuecomment-733395654,1,['queue'],['queued']
Performance,"If the docs actually recommend outputting fields themselves, we should obviously change that. We can perhaps add a note about why we cannot output fields (once we figure it out) --- or fix it so that we can output fields. I actually think it could be useful to output fields when we have a more powerful field abstraction system in the future, especially if it can do MPI operations, etc, which are useful in post processing. It'd be convenient to be able to load fields directly to memory rather than having to reconstruct them from their data and some grid.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/562#issuecomment-564382208:459,load,load,459,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/562#issuecomment-564382208,1,['load'],['load']
Performance,"If you didn't define new advection schemes but use the implemented ones they automatically lower the order near the boundaries. The implementation is in `topologically_conditional_interpolation.jl`. If you did not define new `symmetric_left_biased_interpolation_xᶠᵃᵃ` functions you should be ok. ; Otherwise you have to add your methods in the above mentioned file. If you are using an immersed boundary, the limiting is performed in the `conditional_fluxes.jl` file in the ImmersedBoundaries module. You can always use high order and fill more halos but then you are making an assumption on the field which you are trying to evolve",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2650#issuecomment-1182333702:421,perform,performed,421,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2650#issuecomment-1182333702,1,['perform'],['performed']
Performance,"If you search for `grep -r ""\^2"" *` from Oceananigans' root dir, for example, you get many places where we exponentiate with an Int64. Per the discussion in https://github.com/CliMA/Oceananigans.jl/pull/1770 it seems that this causes a pretty big performance drawback in Julia 1.6 on GPUs. What should be our approach here? Do we wait for this to be resolved upstream or do we change all exponentiation in critical places to use `Int32`?. CC: @maleadt @glwagner",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1832:247,perform,performance,247,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1832,1,['perform'],['performance']
Performance,"If you want to adopt this PR, that would be great. I would leave the boundary conditions outside the operator for the moment, then we can look at the influence of those in performance in another PR. To test the performance we should push PR #3596 that implements split explicit with fill halos at every substep. Using that implementation for split-explicit will make boundary conditions for barotropic variables quite trivial to implement, but probably extremely slow. However, we can use it as a benchmark",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3268#issuecomment-2353152298:172,perform,performance,172,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3268#issuecomment-2353152298,2,['perform'],['performance']
Performance,If you want to give an example of an expensive forcing function it could be instructive to discuss the options for performance optimization! Perhaps there will be some unexpectedly useful tips. A discussion might be a better place for that though.,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3525#issuecomment-2035251797:115,perform,performance,115,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3525#issuecomment-2035251797,2,"['optimiz', 'perform']","['optimization', 'performance']"
Performance,"If you write a 'kernel' to execute on a GPU, you will be writing a computation that is probably more amenable to cache optimization than otherwise. This is because the kernel will perform an operation that is local in physical space. Your task then will essentially be to arrange arrays in memory so that physical locations are nearby in memory. . I am not that surprised that this micro-optimization does not always bear fruit. Presumably it is sensitive to the particular details of the algorithm.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/44#issuecomment-462550797:113,cache,cache,113,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/44#issuecomment-462550797,4,"['cache', 'optimiz', 'perform']","['cache', 'optimization', 'perform']"
Performance,Improve immersed boundary performance,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2817:26,perform,performance,26,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2817,1,['perform'],['performance']
Performance,Improve the performance of the HydrostaticFreeSurfaceModel,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2874:12,perform,performance,12,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2874,1,['perform'],['performance']
Performance,In #3151 it was discovered that the shallow water example in the docs needs ~25 min to run while all other examples need from ~1min (2D) or up to ~7min (Baroclinic instability). Was this always the case? Did something happen to impede the performance of the SWE model?. #3168 is relevant; it does not resolve the issue but nevertheless minimizes the toll that this issue has on the overall building time for the docs.,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3169:239,perform,performance,239,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3169,1,['perform'],['performance']
Performance,"In PR #590 I added a small/quick strong scaling test and @francispoulin calculated the scaling efficiency which wasn't super great:. ```; np efficiency; == ==========; 2 0.96; 4 0.71; 8 0.62; 16 0.56; ```. I guess to improve performance we should do some MPI profiling to find bottlenecks. Could also benchmark the distributed pressure solve and the halo filling separately to see how they scale as well. Might also make sense to benchmark scaling with `ShallowWaterModel` to see if it's an `IncompressibleModel` issue. Might need a pretty large domain to see good scaling with a 2D shallow water model?. @tomchor pointed out that the benchmark could be flawed. We should make sure everything is compiled. Could also try different sizes and a weak scaling benchmark in case the 1D/slab decomposition isn't helping. Maybe trying on a different machine too. Not sure if there's a ""proper"" setup for doing these scaling benchmarks. Bad scaling efficiency might also be a sign of missing barriers/waits?. @vchuravy We might ask for your help!",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1451:225,perform,performance,225,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1451,2,"['bottleneck', 'perform']","['bottlenecks', 'performance']"
Performance,"In PR #590 [WIP] I've prototyped how I've thought about adding support for distributed parallelism by adding a non-invasive `Distributed` MPI layer on top of Oceananigans to keep the core code MPI-free. At last week's CliMA software meeting @lcw and @jkozdon have pointed out a potential limitation of this approach: when running on many nodes and communication starts to eat up a lot of compute time it becomes beneficial to overlap computation and communication. Abstractions such as `CLIMA.MPIStateArray` help a lot with this but require MPI to be ""baked in"". Obviously this issue won't be tackled for a while until we have a working distributed model and need more performance, so I'm just documenting the issue here for future discussion. I think we can achieve this by splitting a kernel like `calculate_interior_source_terms!` into two kernels, one that computes source terms ""near"" the boundary (1-2? grid points from any boundary as needed), then halo communication can happen while a second more compute-intensive kernel computes the source terms in the rest of the interior. But that only helps with one particular instance of halo communication. There will be other halo communications needed that may be impossible to overlap with compute-intensive kernels. Pursuing overlapping in this manner to the extreme and applying it to as many kernels as possible may be detrimental to code clarity. Once we want more distributed performance we should go through the algorithm and minimize the number of halo communications (i.e. calls to `fill_halo_regions!`). cc @leios @jm-c; ```[tasklist]; ### Tasks; ```",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/615:669,perform,performance,669,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/615,2,['perform'],['performance']
Performance,In [A simple algorithm to improve the performance of the WENO scheme on non-uniform grids](https://link.springer.com/article/10.1007/s10409-017-0715-2) a simple method for generalizing WENO stencils for stretched grids is described. This scheme is easy to implement and limits to what we already have implemented in the case of a regular grid.,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1704:38,perform,performance,38,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1704,1,['perform'],['performance']
Performance,"In addition to the bounded-x error above, interpolation methods also appear to be failing at times:. ```julia; using Oceananigans. grid_base = RectilinearGrid(size=(4, 4, 4), extent = (1,1,1)); grid = MultiRegionGrid(grid_base, partition = XPartition(2), devices = 2); @info grid. model = NonhydrostaticModel(grid = grid); @info """" model. u, v, w = model.velocities. ω_x = Field((@at (Center, Face, Face) ∂y(w)-∂z(v))); ```. This throws me the following error:. ```; ERROR: LoadError: MethodError: no method matching interpolate_index(::Tuple{Colon, Colon, Colon}, ::Colon, ::Type{Center}, ::Type{Center}); Closest candidates are:; interpolate_index(::UnitRange, ::Colon, ::Any, ::Any) at ~/.julia/packages/Oceananigans/F1fni/src/AbstractOperations/at.jl:80; interpolate_index(::Colon, ::Colon, ::Any...) at ~/.julia/packages/Oceananigans/F1fni/src/AbstractOperations/at.jl:77; interpolate_index(::Colon, ::UnitRange, ::Any...) at ~/.julia/packages/Oceananigans/F1fni/src/AbstractOperations/at.jl:78; ...; Stacktrace:; [1] interpolate_indices(::Oceananigans.AbstractOperations.Derivative{Center, Face, Face, typeof(Oceananigans.Operators.∂yᶜᶠᶠ), Field{Center, Center, Face, Nothing, MultiRegionGrid{Float64, Periodic, Periodic, Bounded, XPartition{Int64}, MultiRegionObject{Tuple{RectilinearGrid{Float64, FullyConnected, Periodic, Bounded, Float64, Float64, Float64, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, CPU}, RectilinearGrid{Float64, FullyConnected, Periodic, Bounded, Float64, Float64, Float64, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVector{Float64, St",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2795#issuecomment-1303864523:474,Load,LoadError,474,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2795#issuecomment-1303864523,1,['Load'],['LoadError']
Performance,"In discussion with Tim I decided against it. I tried implementing it on the CPU, but that's basically why I required events everywhere. See https://github.com/JuliaGPU/KernelAbstractions.jl/blob/main/examples/mpi.jl, but Tim and I are bullish on using Julia's native concurrency mechanism, to express what streams normally do.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2924#issuecomment-1464051102:267,concurren,concurrency,267,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2924#issuecomment-1464051102,1,['concurren'],['concurrency']
Performance,"In fact, the entire construction process could be simplified with the rules:. 1) If a location is shared, perform the operation there, and interpolate afterwards; 2) If a location is not shared, perform the operation at the final location. Currently rule 1) is enforced only if _all_ locations are shared (eg for tracer-tracer operations, or operations of fields with themselves).",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/959#issuecomment-694942449:106,perform,perform,106,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/959#issuecomment-694942449,2,['perform'],['perform']
Performance,"In hindsight I think that @christophernhill is probably right about; pipeline stalls becoming a bottleneck if we use z as the fast index. (Which; leaves me unsure why it's common in the atmospheric models I've used....; Maybe just prioritizing simplicity over performance?). On Sat, Oct 12, 2019, 5:33 PM Chris Hill <notifications@github.com> wrote:. > P.S. that was a comment on the @ali-ramadhan; > <https://github.com/ali-ramadhan> comment, not the @glwagner; > <https://github.com/glwagner> comment. Greg is correct that some code; > generation can help, although sometimes its cleaner just to write elegant; > code than get carried away with meta-programming ( e.g. see Steve J; > meta-programming regrets talk https://www.youtube.com/watch?v=mSgXWpvQEHE; > ).; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/climate-machine/Oceananigans.jl/issues/470?email_source=notifications&email_token=ACZDSTS5AH2A5CC2HC43EELQOI7CRA5CNFSM4JAEMGO2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEBCIYQQ#issuecomment-541363266>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ACZDSTWNQED7NDAS2U5KTETQOI7CRANCNFSM4JAEMGOQ>; > .; >",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/470#issuecomment-541364103:96,bottleneck,bottleneck,96,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/470#issuecomment-541364103,2,"['bottleneck', 'perform']","['bottleneck', 'performance']"
Performance,"In large computations, advection tendency and implicit free surface are the bottleneck. This can be improved (probably) with an improved `SplitExplicitFreeSurface` that scales better at high resolutions and by introducing shared memory for the calculation of advection. (Another way to improve advection performance is to split the kernels based on the distance to a solid boundary such that we do not have to perform the calculations of all orders starting from 1, but it is probably a slightly more complex solution). This is an explorative PR to try to address both problems.",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2874:76,bottleneck,bottleneck,76,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2874,3,"['bottleneck', 'perform']","['bottleneck', 'perform', 'performance']"
Performance,"In some way this comes back to the fundamental question of: What is the point of KernelAbstractions CPU support. I originally intended it only for making debugging easier... But folks seem to be depending on it as a performance solution... I think it is feasible to get there, but it would require quite a bit of time and effort",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2996#issuecomment-1481464331:216,perform,performance,216,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2996#issuecomment-1481464331,1,['perform'],['performance']
Performance,"In summary, we can encounter at least 3 different errors:. ### 1. Dynamic function invocation. ```; julia> compute!(ComputedField(u + v - w)); ERROR: LoadError: InvalidIRError: compiling kernel gpu__compute!(Cassette.Context{nametype(CUDACtx),...; Reason: unsupported dynamic function invocation...; ```. Solution: probably the compiler isn't inferring types correctly. I think we can use Cthulhu to fully diagnose this error (though we may still need to be creative to solve the problem). ### 2. device kernel image is invalid?!?!. ```; julia> compute!(ComputedField(∂x(u)^2 + ∂y(v)^2 + ∂z(w)^2 + ∂x(w)^2)); ERROR: CUDA error: device kernel image is invalid (code 200, ERROR_INVALID_IMAGE); ```. Solution: ??????. ### 3. ""Entry function uses too much parameter space"". ```; julia> compute!(ComputedField(∂x(u)^2 + ∂y(v)^2 + ∂z(w)^2 + ∂x(w)^2 + ∂y(w)^2)); ERROR: CUDA error: a PTX JIT compilation failed (code 218, ERROR_INVALID_PTX); ptxas application ptx input, line 803; error : Entry function '_Z19julia_gpu__compute_7ContextI14__CUDACtx_Name16CompilerMetadataI10StaticSizeI9_1__1__1_E12DynamicCheckvv7NDRangeILi3ES2_I9_1__1__1_ES2_I9_1__1__1_EvvEEv14__PassType_253v12DisableHooksE14_gpu__compute_11OffsetArrayI7Float64Li3E13CuDeviceArrayIS9_Li3ELi1EEE17MultiaryOperationI4CellS12_S12_Li5E2__5TupleI15BinaryOperationIS12_S12_S12_S13_10DerivativeIS12_S12_S12_6__x___S8_IS9_Li3ES10_IS9_Li3ELi1EEE9_identity20RegularCartesianGridIS9_8PeriodicS20_7BoundedS8_IS9_Li1E12StepRangeLenIS9_14TwicePrecisionIS9_ES23_IS9_EEEEE5Int64S18_S18_S18_S19_IS9_S20_S20_S21_S8_IS9_Li1ES22_IS9_S23_IS9_ES23_IS9_EEEEES15_IS12_S12_S12_S13_S16_IS12_S12_S12_6__y___S8_IS9_Li3ES10_IS9_Li3ELi1EEES18_S19_IS9_S20_S20_S21_S8_IS9_Li1ES22_IS9_S23_IS9_ES23_IS9_EEEEES24_S18_S18_S18_S19_IS9_S20_S20_S21_S8_IS9_Li1ES22_IS9_S23_IS9_ES23_IS9_EEEEES15_IS12_S12_S12_S13_S16_IS12_S12_S12_6__z___S8_IS9_Li3ES10_IS9_Li3ELi1EEES18_S19_IS9_S20_S20_S21_S8_IS9_Li1ES22_IS9_S23_IS9_ES23_IS9_EEEEES24_S18_S18_S18_S19_IS9_S20_S20_S21_S8_IS9_Li1ES2",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1241#issuecomment-738870593:150,Load,LoadError,150,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1241#issuecomment-738870593,1,['Load'],['LoadError']
Performance,"In that example I'm using [`schedule = AveragedTimeInterval(2Δt; window=1.999Δt, stride=1)`](https://pastebin.com/F6ankx4L). So (unless I'm missing something), I expected the time-averaged results to be very close (if not identical) to the snapshot one, since it's only an average over `2Δt`. . From the plot it looks like the average instead is being performed over the interval `[0, t]`, for every `t`. (i.e., a cumulative average starting at the beginning of the simulation)",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1517#issuecomment-817998872:352,perform,performed,352,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1517#issuecomment-817998872,1,['perform'],['performed']
Performance,Incorporate performance benchmarks into tests and CI,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/684:12,perform,performance,12,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/684,1,['perform'],['performance']
Performance,"Increasingly we have the need to offset kernel indices; ie to perform computations over ""windows"" for incomplete `Field` `indices`:. https://github.com/CliMA/Oceananigans.jl/blob/be00e364f9dcd712b3d0c3d48e32b94b181a02fc/src/AbstractOperations/computed_field.jl#L84-L92. or for @simone-silvestri's work to overlap communication and computation, eg #3067. Possibly, such a feature can be supported by `KernelAbstractions` eg https://github.com/JuliaGPU/KernelAbstractions.jl/issues/384 which would simplify our code a lot (since we won't have to pass the offsets into the kernel explicitly).",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3068:62,perform,perform,62,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3068,1,['perform'],['perform']
Performance,Indeed those are some great speedups! Doesn't even look like the GPU is saturated yet. > Do you think we need to get `MultiGPU` working or try and merge what we have with `MultiCPU` plus some tests? I'm tempted to merge sooner rather than later since already this is a big addition. It does seem to work :P But maybe it just needs some profiling to understand where the bottlenecks are. I guess we need to get some CUDA-aware MPI on Buildkite to test `MultiGPU` so maybe we don't have to explicitly test it just yet in this PR.,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1430#issuecomment-799598593:370,bottleneck,bottlenecks,370,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1430#issuecomment-799598593,1,['bottleneck'],['bottlenecks']
Performance,Initializing environment on Buildkite can be a testing bottleneck,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1056:55,bottleneck,bottleneck,55,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1056,1,['bottleneck'],['bottleneck']
Performance,"Instead of saying that trig functions should be avoided on gpus, which seems very strong, I suggest pointing out that there have been some examples where trig functions have performed much slower, and include this example. . This clearly problem needs further exploration but I don't want people to be scared to use sin and cos because sometimes they just make a lot of sense. My two cents worth.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2029#issuecomment-951478103:174,perform,performed,174,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2029#issuecomment-951478103,1,['perform'],['performed']
Performance,"Interesting that `mutable`s can't be `isbits`. But `Array` is mutable and `isbits`? Maybe some exceptions... I do think you should avoid fields with abstract or unspecified types in all cases (when simple, like this one) though, even if just for style points, but also to avoid an unexpected performance gotcha. `Clock{T}` seems appropriate.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/19#issuecomment-477677073:292,perform,performance,292,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/19#issuecomment-477677073,1,['perform'],['performance']
Performance,"Interesting, thanks for those details! That's odd that there are changes in memory allocation associated with building diagnostics. I don't think there's been changes to `Field` between 0.77.5 and 0.78.0. @navidcy might be able to say more. I think 0.78.0 only upgraded the tests to julia 1.8?. I'm also curious why the diagnostics consume so much memory. Are you producing a lot of 3D time averages (which can't be constructed in post-processing?) We've attempted to design the code so that reductions can be performed with minimal memory allocation. 3D diagnostics can simply be calculated from snapshots of the model state, so there's no need to allocate memory (assuming that static memory greatly exceeds GPU memory, this would be preferred). Are we missing a feature to help reduce memory requirements of diagnostics perhaps?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2794#issuecomment-1299561134:510,perform,performed,510,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2794#issuecomment-1299561134,1,['perform'],['performed']
Performance,"Interesting. I was under the impression that for some applications compression can be used to accelerate I/O (due to slow I/O bandwidth and ample CPU resources). NetCDF may be doing something wrong (cf [this HDF5 post](https://www.hdfgroup.org/2017/05/hdf5-data-compression-demystified-2-performance-tuning/)). Anyways, it's a bit outside our scope. Maybe the best option here is to provide a `jld2_kwargs` object (or something similar) that allows users to set arbitrary kwargs in `jldopen` (or whatever is appropriate).",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/357#issuecomment-523542973:288,perform,performance-tuning,288,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/357#issuecomment-523542973,1,['perform'],['performance-tuning']
Performance,Is there some kind of performance benefit of. https://github.com/CliMA/Oceananigans.jl/blob/d0b7ec8f98c860ce49927e0a7214961d2f47fb75/src/Fields/interpolate.jl#L63-L67. compared to. https://github.com/CliMA/Oceananigans.jl/blob/d0b7ec8f98c860ce49927e0a7214961d2f47fb75/src/Fields/interpolate.jl#L75-L81. ?. Why do we have both?,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3356#issuecomment-1775526816:22,perform,performance,22,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3356#issuecomment-1775526816,1,['perform'],['performance']
Performance,"Is this PR looking for an adopter? Happy to try to complete it since it seems like the first step towards open BCs for hydrostatic models. > > Thinking of revamping this, I think `T` is a good notation; we do not need to specify `U` and `C` since probably (in the future) we will need to refactor these a little to pass through boundary conditions.; > ; > Hopefully we don't have to pass boundary conditions 🥺; > ; > Not all complexity is justified by the performance gains... Ah I'm actually not sure where the performance gains would come from, but I thought that the operators need to be aware of the boundary condition in order to correctly implement open BCs (at least based on @simone-silvestri's comments in https://github.com/CliMA/Oceananigans.jl/issues/3628#issuecomment-2312725203).",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3268#issuecomment-2350225617:456,perform,performance,456,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3268#issuecomment-2350225617,2,['perform'],['performance']
Performance,"Is this task required to complete the cubed sphere, or should we regard it as an optimization that's important for performance but not functionality?. @simone-silvestri @navidcy",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3201#issuecomment-1718257374:81,optimiz,optimization,81,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3201#issuecomment-1718257374,2,"['optimiz', 'perform']","['optimization', 'performance']"
Performance,"It can't be completely independent because it still needs `using Oceananigans`, but it could be _more_ independent. I agree that's desirable. I think using absolute filepaths rather than referencing the existing output writers, and using more explicit `nodes` functions could help, eg. ```julia; xζ, yζ, zζ = nodes((Face, Face, Center), grid) ; xδ, yδ, zδ = nodes((Center, Center, Center), grid); ```. I think the grid is serialized so you could load it from file (though this feature isn't very robust right now since it doesn't work for stretched grids, for example).",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1850#issuecomment-878315017:446,load,load,446,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1850#issuecomment-878315017,1,['load'],['load']
Performance,"It is not an optimization; rather, to fill in periodic halos, a field must have a grid size larger than the halos (for other boundary conditions, only one is required). ; @navidcy, you could probably look at how the periodic halos are filled and make sure that they work also if the dimension is smaller than the number of halos. ; It might entail writing some ifelse in the fill periodic halo kernels. For example:; ```julia; @kernel function fill_periodic_west_and_east_halo!(c, ::Val{H}, N) where H; j, k = @index(Global, NTuple); @unroll for i = 1:H; @inbounds begin; # Put some ifelse condition here to make sure that halos pick from the interior; # and not other halos; c[i, j, k] = c[N+i, j, k] # west; c[N+H+i, j, k] = c[H+i, j, k] # east; end; end; end; ```",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3622#issuecomment-2309039454:13,optimiz,optimization,13,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3622#issuecomment-2309039454,1,['optimiz'],['optimization']
Performance,"It looks like KernelAbstractions.jl has added experimental support for AMD: https://github.com/JuliaGPU/KernelAbstractions.jl/releases/tag/v0.6.0. Should we try to make the code more GPU-architecture-agnostic based on that? From what I've heard, end-user level AMD GPUs are generally cheaper than Nvidia ones for similar performance. So running Oceananigans on AMD could potentially make it significantly easier for people to run simulations on their personal computer instead of a cluster.",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1546:321,perform,performance,321,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1546,1,['perform'],['performance']
Performance,"It makes sense that using `const` for variables referenced ""globally"" in functions increases CPU performance, see: https://docs.julialang.org/en/v1/manual/performance-tips/#Avoid-global-variables",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1478#issuecomment-800574436:97,perform,performance,97,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1478#issuecomment-800574436,2,['perform'],"['performance', 'performance-tips']"
Performance,"It returns a int with the value of the number of files or file divisions already performed and the file name. if files are file_part_1.nc, file_part_2.nc, file_part_3.nc, the output will be: `3 file_part_3.nc`",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3818#issuecomment-2414533461:81,perform,performed,81,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3818#issuecomment-2414533461,1,['perform'],['performed']
Performance,"It seems that with Julia 1.6 and recent versions of CUDA.jl, the output of averaged fields is wrong. By inspection it seems that the output is being written to disk prematurely somehow as some grid points contain the sum instead of the mean. @xkykai and I have tried to reproduce in CUDA.jl but with no luck so far. I've tried adding a `CUDA.@sync blocking=true` to https://github.com/CliMA/Oceananigans.jl/blob/383173d11a0c96182a4349fc1e33755207bf0886/src/Fields/averaged_field.jl#L62 but that didn't help. Adding a `sleep(0.01)` afterwards helped a bit (reduced the number of occurences of this issue). It's not a useful hack but it may suggest that a race condition is involved?. X-Ref: This is a minimal reproduction of https://github.com/CliMA/LESbrary.jl/issues/118; X-Ref: https://github.com/JuliaGPU/CUDA.jl/issues/929 (Attempt at reproducing but turned out to be a REPL issue). # Minimal working example. ```julia; using Oceananigans. grid = RegularRectilinearGrid(size=(256, 256, 128), extent=(1, 1, 1)); model = IncompressibleModel(architecture=GPU(), grid=grid); simulation = Simulation(model, Δt=1, stop_time=100). set!(model, u=1, v=1, T=1). U = AveragedField(model.velocities.u, dims=(1, 2)); V = AveragedField(model.velocities.v, dims=(1, 2)); T = AveragedField(model.tracers.T, dims=(1, 2)). simulation.output_writers[:jld2] =; JLD2OutputWriter(model, (; U, V, T),; prefix = ""stats"",; schedule = TimeInterval(10),; force = true). simulation.output_writers[:nc] =; NetCDFOutputWriter(model, (; U, V, T),; filepath = ""stats.nc"",; schedule = TimeInterval(10),; mode = ""c""). run!(simulation); ```. ## Looking at the NetCDF output. ```julia; using NCDatasets; ds = NCDataset(""stats.nc""); ds[""T""][:]; ```. ```; 128×11 Matrix{Float32}: ; 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 1.0 ",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1767:654,race condition,race condition,654,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1767,1,['race condition'],['race condition']
Performance,"It seems ths is pretty close, but I'm this error:. ```; ┌ Debug: checking footnote links.; --;   | └ @ Documenter.DocChecks /storage5/buildkite-agent/.julia-3990/packages/Documenter/R2HVS/src/DocChecks.jl:110;   | [ Info: Populate: populating indices.;   | ERROR: LoadError: `makedocs` encountered an error. Terminating build;   | Stacktrace:;   | [1] error(s::String);   | @ Base ./error.jl:33;   | [2] runner(#unused#::Type{Documenter.Builder.RenderDocument}, doc::Documenter.Documents.Document);   | @ Documenter.Builder /storage5/buildkite-agent/.julia-3990/packages/Documenter/R2HVS/src/Builder.jl:255;   | [3] dispatch(#unused#::Type{Documenter.Builder.DocumentPipeline}, x::Documenter.Documents.Document);   | @ Documenter.Utilities.Selectors /storage5/buildkite-agent/.julia-3990/packages/Documenter/R2HVS/src/Utilities/Selectors.jl:170;   | [4] #2;   | @ /storage5/buildkite-agent/.julia-3990/packages/Documenter/R2HVS/src/Documenter.jl:257 [inlined];   | [5] cd(f::Documenter.var""#2#3""{Documenter.Documents.Document}, dir::String);   | @ Base.Filesystem ./file.jl:106;   | [6] #makedocs#1;   | @ /storage5/buildkite-agent/.julia-3990/packages/Documenter/R2HVS/src/Documenter.jl:256 [inlined];   | [7] top-level scope;   | @ ~/builds/tartarus-13/clima/oceananigans/docs/make.jl:155;   | in expression starting at /var/lib/buildkite-agent/builds/tartarus-13/clima/oceananigans/docs/make.jl:155;   | 🚨 Error: The command exited with status 1;   | user command error: exit status 1; ```. which doesn't really give me any information, so I'm unsure about how to proceed.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1989#issuecomment-945152318:264,Load,LoadError,264,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1989#issuecomment-945152318,1,['Load'],['LoadError']
Performance,It seems to be good to merge! I have run at different resolutions and for longer times without seeing any race conditions,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1985#issuecomment-924096447:106,race condition,race conditions,106,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1985#issuecomment-924096447,1,['race condition'],['race conditions']
Performance,"It seems to have failed already so I will revert to what we had before. The error message is copied below. ```; [ Info: Populate: populating indices.; --;   | ERROR: LoadError: `makedocs` encountered an error. Terminating build;   | Stacktrace:;   | [1] error(::String) at ./error.jl:33;   | [2] runner(::Type{Documenter.Builder.RenderDocument}, ::Documenter.Documents.Document) at /storage7/buildkite-agent/.julia-1687/packages/Documenter/bFHi4/src/Builder.jl:255;   | [3] dispatch(::Type{Documenter.Builder.DocumentPipeline}, ::Documenter.Documents.Document) at /storage7/buildkite-agent/.julia-1687/packages/Documenter/bFHi4/src/Utilities/Selectors.jl:170;   | [4] #2 at /storage7/buildkite-agent/.julia-1687/packages/Documenter/bFHi4/src/Documenter.jl:249 [inlined];   | [5] cd(::Documenter.var""#2#3""{Documenter.Documents.Document}, ::String) at ./file.jl:104;   | [6] #makedocs#1 at /storage7/buildkite-agent/.julia-1687/packages/Documenter/bFHi4/src/Documenter.jl:248 [inlined];   | [7] top-level scope at /storage7/buildkite-agent/builds/tartarus-mit-edu-4/clima/oceananigans/docs/make.jl:145;   | [8] include(::Function, ::Module, ::String) at ./Base.jl:380;   | [9] include(::Module, ::String) at ./Base.jl:368;   | [10] exec_options(::Base.JLOptions) at ./client.jl:296;   | [11] _start() at ./client.jl:506;   | in expression starting at /storage7/buildkite-agent/builds/tartarus-mit-edu-4/clima/oceananigans/docs/make.jl:145;   | 🚨 Error: The command exited with status 1. ```",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1495#issuecomment-802268366:166,Load,LoadError,166,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1495#issuecomment-802268366,1,['Load'],['LoadError']
Performance,"It turns out this also affects `JLD2OutputWriter`. I believe this is due to us ""double-counting"" indices in `construct_output()`. Specifically, we _always_ slice outputs with `indices` here: https://github.com/CliMA/Oceananigans.jl/blob/3322f1879cf8c84c88e9c7cf4e33bdcf70520c36/src/OutputWriters/output_construction.jl#L49-L51. When a `Field` isn't ""pre-sliced"" (i.e., isn't constructed with the `indices` keyword) then that's okay. But if a `Field` is already sliced, then we slice it again there. Depending on how both `indices` are defined (the one when constructing the `Field` and the one passed to the output writer), we try to access outputs at indices that aren't there anymore, leading to a `BoundsError`:. ```; ERROR: LoadError: BoundsError: attempt to access 1×10×10 view(::Array{Float64, 3}, 4:4, :, :) with eltype Float64 at index [4:7, 4:7, 4:7]; ```. I'll open a PR soon to fix this, but I just wanted to leave this saved here. I think the way to go is to change the default `indices` in the constructors to `nothing` and then use that to avoid the double slicing in `construct_output()`.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2497#issuecomment-1115482067:728,Load,LoadError,728,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2497#issuecomment-1115482067,1,['Load'],['LoadError']
Performance,It'd be nice if we move CUDA functionality into an extension so it's only loaded for CUDA-enabled devices. @vchuravy made a first attempt to do that at https://github.com/CliMA/Oceananigans.jl/pull/3066. similar effort but for AMDGPU is at https://github.com/CliMA/Oceananigans.jl/pull/3475,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3481:74,load,loaded,74,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3481,1,['load'],['loaded']
Performance,"It'd be nice to state in writing the justification for writing a separate CPU solver for certain problems. In general, I think that any algorithm that works on the GPU will also work on the CPU. Thus at least in principle the simplest choice is presumably to use the same solver on both architectures. For example, benchmarking *might* show that a GPU-friendly algorithm performs poorly compared to a CPU-specific algorithm, which might justify maintaining separate solvers for the GPU and CPU. Is this the case?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/586#issuecomment-572064187:371,perform,performs,371,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/586#issuecomment-572064187,1,['perform'],['performs']
Performance,"It's a ""performance"" task really but I have the gut feeling that it might be impeding performance so much that we won't be able to consider the cubed sphere done if we don't deal with this. So probably good idea to leave it in the milestone of global simulation using cubed sphere as is now?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3201#issuecomment-1718731994:8,perform,performance,8,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3201#issuecomment-1718731994,2,['perform'],['performance']
Performance,"It's a problem of particles being advected in the x-direction. You can see that the out-of-bounds error is in the x-direction; `ERROR: LoadError: BoundsError: attempt to access 56×56×56 OffsetArray(::Array{Float64, 3}, -2:53, -2:53, -2:53) with eltype Float64 with indices -2:53×-2:53×-2:53 at index [54, 49, 1]`; You are moving out of the periodic grid because the particle is being advanced beyond the periodic domain.; There is no bouncing happening and it is not a problem of Bounded directions because the particle is not moving in the y- or in the z-direction. You are prescribing a steady state flow which is characterized by a u-velocity only. There is no tendency term that can develop a shear instability since a y-gradient in u is stable if there are no additional frictional forces, i.e.:; $$\frac{\partial u}{\partial t} = - \frac{\partial uu}{\partial x} - \frac{\partial uv}{\partial y} - \frac{\partial uw}{\partial z} - \frac{\partial p}{\partial x}$$; All the terms on the RHS of this equation are zero because; - $v = 0$; - $w = 0$; - $\frac{\partial uu}{\partial x} = 0$; - $\frac{\partial p}{\partial x} = 0$ since $\delta_x u^* + \delta_y v^* + \delta_z w^* = 0$ and you have periodic boundary conditions in the x-direction. from how you initialized it, the flow cannot change, irrespective of your CFL (if you remove your particles you'll see that the code will run indefinitely without changing, even with CFL = 100). In your second case, when you change the velocity to ; ```julia; function initial_u(x::R, y::R, z::R) where {R<:Real}; ϵ = 1e-7; return (max_velocity / Lx) * y + ϵ * max_velocity * sin(6π * x / Lx); end; ```; you are initializing your solution with a divergent flow $\partial_x u + \partial_y v + \partial_z w \ne 0$ which is not ""admissible"" in an incompressible model (such as Oceananigans' non-hydrostatic-model). The initialization then triggers a pressure correction which will act to suppress the divergence in your initial conditions, by either includi",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3320#issuecomment-1773296277:135,Load,LoadError,135,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3320#issuecomment-1773296277,1,['Load'],['LoadError']
Performance,"It's better to use `Field` for boundary conditions rather than `Array` --- this eliminates translation issues when switching architectures (eg CPU, to GPU, to multi-region, to distributed), and also allows diagnostics to be performed directly on the boundary conditions (which isn't always needed, but is very welcome to have available --- for example the spatial derivative of a flux). So, I think we should declare that it's ""best practice"" to use `Field`, not `Array`. To encourage this we should change the docs that show how to use an `Array`:. https://clima.github.io/OceananigansDocumentation/stable/model_setup/boundary_conditions/#.-A-random,-spatially-varying,-constant-in-time-temperature-flux-specified-by-an-array. to using a `Field`. Also, we should add correctness tests for using `Field`, which has a bug up until #3287. Curious about @simone-silvestri's input because he has used arrays a lot in boundary conditions. Note that we also are working on support for `FieldTimeSeries` as a boundary condition in #3233 .",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3298:224,perform,performed,224,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3298,1,['perform'],['performed']
Performance,"It's confusing why this fails, but the offending line is here:. https://github.com/climate-machine/Oceananigans.jl/blob/cdf5091a9164c1ee6efd2f0b349e6170f2fab749/src/OutputWriters/jld2_output_writer.jl#L81. I don't understand why `convert` is called here. Do we need to `deepcopy` the field in order to output it?. The stack trace is obscure because, I think, the error appears inside the generator in the line I've posted, and also because the function that's being called is anonymous. I think. We could try rewriting the offending line to create an empty `Dict{Any, Symbol}` and then load the outputs into that. Not sure if that would prevent `convert` from being called --- it might. I guess at the time I believed using the generator could be more efficient in some cases. I don't know if it matters in practical scenarios though.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/562#issuecomment-564381723:586,load,load,586,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/562#issuecomment-564381723,1,['load'],['load']
Performance,"It's interesting that this affects performance so much. IIRC, we previously called `Base.power_by_squaring`, which seems slower than the current Float64 intrinsic:. ```julia; julia> A = CUDA.rand(1024,1024);. # current version; julia> @benchmark CUDA.@sync broadcast!(A, A, 10) do a, b; a^b; end; BenchmarkTools.Trial: ; memory estimate: 496 bytes; allocs estimate: 7; --------------; minimum time: 33.429 μs (0.00% GC); median time: 33.970 μs (0.00% GC); mean time: 35.515 μs (0.00% GC); maximum time: 464.024 μs (0.00% GC); --------------; samples: 10000; evals/sample: 1. # Int32 is faster indeed; julia> @benchmark CUDA.@sync broadcast!(A, A, 10) do a, b; a^(b%Int32); end; BenchmarkTools.Trial: ; memory estimate: 496 bytes; allocs estimate: 7; --------------; minimum time: 29.600 μs (0.00% GC); median time: 30.289 μs (0.00% GC); mean time: 33.132 μs (0.00% GC); maximum time: 740.031 μs (0.00% GC); --------------; samples: 10000; evals/sample: 1. # old code path was slower than both; julia> @benchmark CUDA.@sync broadcast!(A, A, 10) do a, b; Base.power_by_squaring(a, b); end; BenchmarkTools.Trial: ; memory estimate: 496 bytes; allocs estimate: 7; --------------; minimum time: 39.410 μs (0.00% GC); median time: 40.629 μs (0.00% GC); mean time: 45.890 μs (0.00% GC); maximum time: 1.195 ms (0.00% GC); --------------; samples: 10000; evals/sample: 1; ```",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1770#issuecomment-869801976:35,perform,performance,35,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1770#issuecomment-869801976,1,['perform'],['performance']
Performance,"It's just a suggestion... Perhaps it's not needed. But having it as a parameter of the type could be useful, even if the parameter is the same value as `grid.Nz` because if it's part of type you can write methods that do different things based on the number of layers your model has. E.g.,. ```Julia; function compute_this_and_that(model::ShallowWaterModel{1}); a = 1 # simple calculation for single-layer models; return a; end. function compute_this_and_that(model::ShallowWaterModel); number_of_layers = model.number_of_layers; a = sum(rand(number_of_layers)) # more complicated stuff only needed for multi-layer models; return a; end; ```. This way we don't use `if number_of_layers > 1; do this; else; do that` which reduces performance because the compiler has to compile all cases and check every time the value of layers and do this or the other....",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2975#issuecomment-1469188127:729,perform,performance,729,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2975#issuecomment-1469188127,1,['perform'],['performance']
Performance,"It's now obvious that tests like the one in #1807 implicitly perform this test, so it's not needed.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1802#issuecomment-873294152:61,perform,perform,61,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1802#issuecomment-873294152,1,['perform'],['perform']
Performance,"I’m happy with output_writers.jl; I think all the code in that file is appropriately related. The checkpointer could provide its own constructor to avoid excess memory allocation. . For GPU problems I don’t think there is an issue: checkpointed arrays can b loaded into CPU memory rather than GPU memory, and then the data can be copied into the fields allocated by the model constructor. So at first glance the excess memory allocation does not seem like a major issue on modern CPUs. . I am particularly concerned about the maintainability of the checkpointer, since it will need to be updated every time a new feature is added. Let’s make sure the design is easy to maintain before merging.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/326#issuecomment-517777579:258,load,loaded,258,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/326#issuecomment-517777579,1,['load'],['loaded']
Performance,"Jumping the gun here but instead of accessing e.g. `model.tracers.T[i, j, k+1]` multiple times during a time step, can it be prefetched, i.e. `T_kp1 = model.tracers.T[i, j, k+1]`, and then reused multiple times? Would the value or the pointer need to be accessed?. The only reason to do this is performance gain. Will this work or will the code turn into spaghetti? Can some sort of compiler figure this stuff out for us?",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/44:295,perform,performance,295,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/44,1,['perform'],['performance']
Performance,"Just a clue from https://github.com/jipolanco/PencilArrays.jl/issues/49:. I think it's likely we need to use some `PencilArrays` interface to set up / partition the eigenvalues, rather than attempting to manually hack together a partition. Also by the way, it looks like we might be able to use ""pencil"" decomposition with the non-hydrostatic model now too: . https://github.com/jipolanco/PencilArrays.jl/issues/42#issuecomment-1029766858. previously we could not because PencilFFTs required FFTs to be performed along dimension 1. Since we can't decompose in ""z"" (because at least right now there are vertical integrals in `NonhydrostaticModel` --- this should also be relaxed), this only left the `y` dimension for partitioning. It feels like we might be relatively straightforward to solve if we become proficient with PencilArrays?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2347#issuecomment-1101627302:503,perform,performed,503,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2347#issuecomment-1101627302,1,['perform'],['performed']
Performance,"Just a couple of notes:; * We can probably rename the operators from e.g. `δx_caa` to `δ_caa` or even just `δcaa` as the dimension along which the difference or interpolation is performed is implied by the `caa`.; * To keep this PR small and self-contained with the goal of introducing correct finite volume operators (without the worry of performance issues), it will not introduce composite operators. I will do that in a future PR which should clean up some of the operators. We will need multiple levels of composition to calculate operators like `δy_fca(ϊx_faa(Ay * v) * ϊy_afa(u))`.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/283#issuecomment-503240347:178,perform,performed,178,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/283#issuecomment-503240347,2,['perform'],"['performance', 'performed']"
Performance,"Just a quick update, zero-viscosity Bicklet jet test case for `VectorInvariant`, `WENOVectorInvariant` (smoothness calculated based on `ζ`), modified VectorInvariant WENO with smoothness based on 2D stencils of `u` and `v`, here called `WENOVectorInvariantZVEL`. `VectorInvariant`. https://user-images.githubusercontent.com/33547697/157745561-a8e5f128-2f4e-42e3-9305-3f624498590b.mp4. `WENOVectorInvariant`. https://user-images.githubusercontent.com/33547697/157745569-41c52e2d-c80b-4d43-b2bf-8a914e8856a2.mp4. `WENOVectorInvariantZVEL`. https://user-images.githubusercontent.com/33547697/157745571-725ea604-8dec-44bd-bd08-dcd70d9ed4b1.mp4. `WENOVectorInvariantZVEL` seems to perform actually very well compared to a (somewhat) standard ""vorticity-reconstruction"" `WENOVectorInvariant` and compared to the very noisy standard `VectorInvariant` in lie of the fact that; - Noise is reduced significantly despite dissipation not being too high; - Agreement between different resolutions is much higher",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2317#issuecomment-1064455116:676,perform,perform,676,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2317#issuecomment-1064455116,1,['perform'],['perform']
Performance,Just a thought that we probably want reductions on field time series to be performant anyways. So it's better that we call data summary because then more people will be annoyed that they are slow => more pressure to fix it 😆,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3750#issuecomment-2322232441:75,perform,performant,75,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3750#issuecomment-2322232441,1,['perform'],['performant']
Performance,"Just an update (mostly for my own benefit later), apparently the outputs are constructed appropriately now, but the outputs still can't be written to NetCDF because the writer creates the variables with a different size:. ```julia; ERROR: LoadError: NetCDF error: size mismatch for variable 'op_sliced2' in file './sliced.nc'. Trying to write (2, 1, 6) elements while [2, 1, 4, 1] are expected (NetCDF error code: -57); Stacktrace:; [1] _nc_check_size_put_vars(ncid::Int32, varid::Int32, countp::Vector{Int64}, op::Array{Float32, 3}); @ NCDatasets ~/.julia/packages/NCDatasets/XVX8L/src/netcdf_c.jl:943; [2] nc_put_vars(ncid::Int32, varid::Int32, startp::Vector{Int64}, countp::Vector{Int64}, stridep::Vector{Int64}, op::Array{Float32, 3}); @ NCDatasets ~/.julia/packages/NCDatasets/XVX8L/src/netcdf_c.jl:954; [3] setindex!; @ ~/.julia/packages/NCDatasets/XVX8L/src/variable.jl:457 [inlined]; [4] setindex!(::NCDatasets.Variable{Float32, 4, NCDatasets.NCDataset{Nothing}}, ::Array{Float32, 3}, ::Colon, ::Colon, ::Colon, ::Int64); @ NCDatasets ~/.julia/packages/NCDatasets/XVX8L/src/variable.jl:490; [5] setindex!(::NCDatasets.CFVariable{Float32, 4, NCDatasets.Variable{Float32, 4, NCDatasets.NCDataset{Nothing}}, NCDatasets.Attributes{NCDatasets.NCDataset{Nothing}}, NamedTuple{(:fillvalue, :missing_values, :scale_factor, :add_offset, :calendar, :time_origin, :time_factor), Tuple{Nothing, Tuple{}, Nothing, Nothing, Nothing, Nothing, Nothing}}}, ::Array{Float32, 3}, ::Colon, ::Colon, ::Colon, ::Int64); @ NCDatasets ~/.julia/packages/NCDatasets/XVX8L/src/cfvariable.jl:732; [6] save_output!(ds::NCDatasets.NCDataset{Nothing}, output::Field{Face, Center, Center, Oceananigans.AbstractOperations.BinaryOperation{Face, Center, Center, typeof(*), Int64, Field{Face, Center, Center, Nothing, RectilinearGrid{Float64, Periodic, Periodic, Periodic, Float64, Float64, Float64, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}}}, OffsetArr",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2508#issuecomment-1131970655:239,Load,LoadError,239,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2508#issuecomment-1131970655,1,['Load'],['LoadError']
Performance,"Just some comments at this point:; * At this point, we have the HydrostaticFreeSurface model working with the split explicit free surface. It would be great to find some time later on to figure out what was going on with the implicit free surface on AMD GPUs (is the issue isolated only to that architecture??) and get this resolved.; * To get everything moved over to KernelAbstractions would constitute a rather large change, something I think @glwagner expressed an interest in avoiding. I'd vote in favor of pushing this change off for future PR's.; * I'm wrapping up a profiling report that includes MI210 and A100 GPU performance; this report will include some recommendations should we be interested in performance improvements on GPU hardware (AMD and Nvidia). This kind of work could also constitute PR's further down the road.; * The main outstanding issue seems to be that we need a platform for testing on AMD GPUs. . It appears the CliMA fork `Project.toml` and `Manifest.toml` have diverged; I'll take a look to see if I can fix.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3475#issuecomment-1997632147:624,perform,performance,624,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3475#issuecomment-1997632147,2,['perform'],['performance']
Performance,"Just some restructuring of the `fill_halo_regions!` to allow the application of `apply_regionally!` to the directional fill_halo.; The `fill_halo_regions!` looks like this; ```; halo_tuple = permute_boundary_conditions(boundary_conditions); ; for task = 1:3; barrier = device_event(arch); fill_halo_event!(task, halo_tuple, c, loc, arch, barrier, grid, args...; kwargs...); end; ```; this should actually not change the performance... but I guess it will change completely since we will not want a loop over halo events anymore, neither to permute BCs.; There are differences in the `HydrostaticFreeSurfaceModel` where I lumped all the `fill_halo_regions` together and exposed a little bit of parallelism, and in the `QuasiAdamsBashforthTimeStepper` which might affect the `NonhydrostaticModel` benchmarks",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2253#issuecomment-1116140545:420,perform,performance,420,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2253#issuecomment-1116140545,1,['perform'],['performance']
Performance,"Just to be clear, the point of doing this is because you want to use dispatch to write flexible array operations that are agnostic to whether the array is a 'raw' array (like an `Array` or `CuArray`), or some kind of wrapper like an `OffsetArray`. By writing `parent(a)`, you ensure correct behavior on `a` in both cases; you don't need to write new high-level functions for wrappers versus arrays because dispatch is performed at the lower level, where it belongs. With `data` we can use the same logic --- this concept is deployed extensively in PR #463.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/454#issuecomment-542269350:418,perform,performed,418,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/454#issuecomment-542269350,1,['perform'],['performed']
Performance,"Just to clarify things a bit for you @loganpknudsen --- your error says. > ERROR: LoadError: Failed to precompile Oceananigans. this means that the segmentation fault occurs during precompilation of `Oceananigans`, which occurs before any code you have written executes. That's why the error comes from ""line 1"" of your script (is that where you write `using Oceananigans`?). Another clue is the text. > [52922] signal (11.1): Segmentation fault; > in expression starting at /glade/u/home/knudsenl/.julia/packages/Oceananigans/M82LU/src/Oceananigans.jl:129. That says the error comes from line 129 in the file `src/Oceananigans.jl`. Going to that line on `main` branch we find:. https://github.com/CliMA/Oceananigans.jl/blob/d6e63e53e795272378b7657c4a6f32da2d62d6f9/src/Oceananigans.jl#L129. so there's something wrong with your CUDA / how it's loaded. The best course of action is probably to update to julia 1.10 first of all rather than using julia 1.9.2. Next, see if you can simply write `using CUDA` rather than trying to run your whole script. If you can get that to work (better yet if you can use some of `CUDA.jl` on a GPU on derecho then move on to simply writing `using Oceananigans` and trying to build a grid on the GPU. If that succeeds move on to your script.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2240940342:82,Load,LoadError,82,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2240940342,2,"['Load', 'load']","['LoadError', 'loaded']"
Performance,"Just to summarize where we are here:. 1. The new test fails.; 2. We also expect that changes on #3477 will cause the test to fail. A possible solution to the changes that #3477 will incur is to replace the slurped locations with `args::Varargs{N, T}` @wsmoses ; 3. We need to merge #3477 to restore normal CPU performance. Otherwise, Oceananigans can't be used even merely to demonstrate capabilities on the CPU.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3480#issuecomment-1948850386:310,perform,performance,310,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3480#issuecomment-1948850386,1,['perform'],['performance']
Performance,"Kinda feels like we're getting closer to a version 1.0 release, although I don't think we need to put a time frame on it. Is it worth discussing which major development milestones we think should be part of v1.0?. Since we're following [SemVer](https://semver.org/) I guess it's not really about which features we want to see in v1.0 but more about whether we think the user interface will be relatively stable. New features could be introduced in v1.1, v1.2, etc. but if we make any breaking changes we'll have to release v2.0. Maybe it's still useful to list some major milestones as they're quite likely to introduce breaking changes?. I'll start with the three big ones on my mind:; - [x] Pressure solvers for all topologies and grids (#586); - [x] MPI distributed parallelism (#590) [not necessarily super optimized, just something that works okay]; - [x] Vertically stretched grid (#471); - [ ] Abstraction for vectors. It's only for simple grids that we can really get away with referring to the velocity field component wise with `u, v, w`.; - [ ] Specifying `architecture` when building `grid` (#1825).; - [ ] Simplify grids. We really only need one `RectilinearGrid`, one `LatitudeLongitudeGrid`, and one ""arbitrary"" (not aligned with a coordinate system, like what's used for the cubed sphere); - [ ] Finalize spherical implementations; - [ ] Finalize bathymetry. Would be great to hear what people think and if anyone has any thoughts on v1.0. Might also be good to include @whitleyv's immersed boundary implementation and @francispoulin's shallow water model since both might bring some breaking changes as well. PS: Stuff added on 2021-07-23.",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1234:811,optimiz,optimized,811,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1234,1,['optimiz'],['optimized']
Performance,"LES closure is a very high priority. On Wed, Apr 24, 2019, 5:39 PM Ali Ramadhan <notifications@github.com> wrote:. > Not supposed to be concrete but just to lay out some goals for big new; > features and prioritize them based on our needs. Also in case I forgot; > about them.; >; > Most of these can be done independently so the version ordering is mostly; > for priority.; >; > - v0.6: Halo regions (#167; > <https://github.com/climate-machine/Oceananigans.jl/pull/167>); > - v0.7: More conservative time-stepping for Float32; > - v0.8: channel and boxed/cubed models, i.e. multiple wall-bounded; > dimensions (#180; > <https://github.com/climate-machine/Oceananigans.jl/pull/180>); > - v0.9: Arbitrary LES closures (Smagorinsky and AMD implemented); > - v0.10: Variable Δz grid; > - v0.11: GPU performance optimization (GPU hackathon); > - v0.12: Multi-GPU (and multi-CPU?) distributed parallelism (GPU; > hackathon); > - v0.13: ..?; > - v1.0: ....?; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/climate-machine/Oceananigans.jl/issues/207>, or mute; > the thread; > <https://github.com/notifications/unsubscribe-auth/AKXUEQUFOFFJEHO2C7FNEB3PSDHR3ANCNFSM4HIIDSNA>; > .; >",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/207#issuecomment-486464430:797,perform,performance,797,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/207#issuecomment-486464430,2,"['optimiz', 'perform']","['optimization', 'performance']"
Performance,"Last minute benchmarks update for JOSS. Resolves #607. The performance benchmarks have been updated from v0.18.0 to v0.34.0. While it seems that CPU models have gotten faster by about ~32% while large GPU models have slowed down by ~15%, I think that's just because the v0.18.0 benchmarks were run on Supercloud or somewhere with a V100 while the v0.34.0 benchmarks were run on Tartarus where the TITAN V is marginally slower but the CPUs are beefier. No noticeable regression is pretty great considering how many features and improvements we've added since v0.18.0 and the fact that we haven't really done any profiling and targeted performance optimization. What seems to have gotten worse is constant overhead costs so small models are slower because of it, but this is something we can tackle when we focus on performance optimization in the future. ![benchmark_plots](https://user-images.githubusercontent.com/20099589/89906791-d2c85b00-dbb9-11ea-969a-4b8db2c31680.png)",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/850:59,perform,performance,59,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/850,5,"['optimiz', 'perform']","['optimization', 'performance']"
Performance,"Latest error:. ```julia; ERROR: LoadError: task switch not allowed from inside staged nor pure functions; Stacktrace:; [1] try_yieldto(undo::typeof(Base.ensure_rescheduled)); @ Base ./task.jl:921; [2] wait(); @ Base ./task.jl:995; [3] uv_write(s::Base.TTY, p::Ptr{UInt8}, n::UInt64); @ Base ./stream.jl:1048; [4] unsafe_write(s::Base.TTY, p::Ptr{UInt8}, n::UInt64); @ Base ./stream.jl:1120; [5] write; @ Base ./strings/io.jl:248 [inlined]; [6] print; @ Base ./strings/io.jl:250 [inlined]; [7] print(::Base.TTY, ::String, ::String, ::Vararg{String}); @ Base ./strings/io.jl:46; [8] println(::Base.TTY, ::String, ::Vararg{String}); @ Base ./strings/io.jl:75; [9] println(::String, ::String); @ Base ./coreio.jl:4; [10] calling_conv_fixup(builder::LLVM.IRBuilder, val::LLVM.AddrSpaceCastInst, tape::LLVM.PointerType, prev::LLVM.UndefValue, lidxs::Vector{…}, ridxs::Vector{…}); @ Enzyme.Compiler ~/Projects/Enzyme.jl/src/compiler/utils.jl:270; [11] calling_conv_fixup (repeats 2 times); @ Enzyme.Compiler ~/Projects/Enzyme.jl/src/compiler/utils.jl:183 [inlined]; [12] calling_conv_fixup(builder::LLVM.IRBuilder, val::LLVM.AddrSpaceCastInst, tape::LLVM.PointerType); @ Enzyme.Compiler ~/Projects/Enzyme.jl/src/compiler/utils.jl:183; [13] enzyme_custom_common_rev(forward::Bool, B::LLVM.IRBuilder, orig::LLVM.CallInst, gutils::Enzyme.Compiler.GradientUtils, normalR::Ptr{…}, shadowR::Ptr{…}, tape::LLVM.ExtractValueInst); @ Enzyme.Compiler ~/Projects/Enzyme.jl/src/compiler.jl:4610; [14] enzyme_custom_rev(B::LLVM.IRBuilder, orig::LLVM.CallInst, gutils::Enzyme.Compiler.GradientUtils, tape::LLVM.ExtractValueInst); @ Enzyme.Compiler ~/Projects/Enzyme.jl/src/compiler.jl:4770; ```",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3360#issuecomment-1786194570:32,Load,LoadError,32,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3360#issuecomment-1786194570,1,['Load'],['LoadError']
Performance,Let's do a simple test of CPU performance to make sure we won't have to revert this soon given the change to splatting (which was implemented to solve a 100x slow down a few months ago),MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3480#issuecomment-2137702684:30,perform,performance,30,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3480#issuecomment-2137702684,1,['perform'],['performance']
Performance,"Looking at performance benchmarks, Float32 is slower than Float64 on the CPU. I suspect something is being done wrong somewhere maybe. I have a few ideas:; 1. Those benchmarks were run on Google Cloud where the virtual CPUs aren't very performant so I thought it was maybe just a low-end 64-bit CPU where Float32 operations were emulated via Float64 operations resulting in fewer FLOPS. However, even on my own laptop I found Float32 on a CPU to be a bit slower, but just by 5-10% whereas on Google Cloud it was like 30%+ slower.; 2. It's likely that Float32 is still being mixed with Float64, and maybe this slows the code down due to too many implicit conversions? (Somewhat related to #34).; 3. Unlikely but I wonder if this is a Julia issue... I could perhaps run some simple C code to see if it's a hardware thing or just a weird Julia thing.; 4. I thought it looked fine on the GPU as you do see a ""speedup"" of 10-15% which I took to mean that we were memory bandwidth/latency limited (which is probably true). The V100 GPUs have twice as many FP32 units as they do FP64 units and if FP32 operations are faster, then I expect more than just a 10-15% speedup. So maybe we have similar issues on the GPU, which are preventing us from doing better than 15% speedup. Or maybe the GPU doesn't do implicit conversions and we are actually just memory bandwidth/latency limited. @glwagner has also pointed out these posts:; * Check out these benchmarks for arithmetic cpu operations: http://nicolas.limare.net/pro/notes/2014/12/12_arit_speed/ Conclusion: _usually_ single precision is faster, but is _occasionally_ comparable or slower depending on the processor (check results for x86-64 Intel Ivy Bridge).; * https://stackoverflow.com/questions/3426165/is-using-double-faster-than-float",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/188:11,perform,performance,11,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/188,4,"['latency', 'perform']","['latency', 'performance', 'performant']"
Performance,"Looks good to me. My only question is about the derivation of ``wˢ``, which results from a vertical integration of the analytical functions. Currently this integral is performed under the assumption that ``wˢ`` is zero at large depths, which I clarified in the code:; https://github.com/CliMA/Oceananigans.jl/blob/06f6ac746742d08c78b3cfed0bd40f7df4392819/src/StokesDrifts.jl#L213-L215. **Is this the correct assumption here?** It effectively means that ``wˢ`` and its gradients are a maximum at the surface under a depth-decaying Stokes drift, so any horizontal convergence/divergence of Stokes drift manifests a local change in sea surface height:; https://github.com/CliMA/Oceananigans.jl/blob/06f6ac746742d08c78b3cfed0bd40f7df4392819/src/StokesDrifts.jl#L248-L250. An alternative would be to set ``wˢ`` to zero at the surface, which would cause ``wˢ`` and its horizontal gradients to reach a maximum at large depths, as the occurrences of ``ûˢ(z)`` above would become ``( ûˢ(z) - ûˢ(0) )``. Essentially, horizontal convergence/divergence would manifest as downwelling/upwelling (rather than SSH changes in the current case). I feel like this approach makes sense for LES setups, but it could cause problems for simulations of the full water column where you don't want direct Stokes drift effects deep in the ocean interior. I'm happy to go forward with this example as is, but the above would be a useful question to answer in the future.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3425#issuecomment-1891314292:168,perform,performed,168,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3425#issuecomment-1891314292,1,['perform'],['performed']
Performance,Looks like I forgot this in #3847. The performance should be identical if using both the methods provided the methods are precompiled correctly. The method in [`tracer_advection_operators.jl:11`](https://github.com/CliMA/Oceananigans.jl/blob/7cbf013cb6bed2bd7cef0f4d8e5f04c078e50ee0/src/Advection/tracer_advection_operators.jl#L11) avoids the immersed boundary check with `nothing` advection so probably it's a tad better.,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3851#issuecomment-2428436798:39,perform,performance,39,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3851#issuecomment-2428436798,1,['perform'],['performance']
Performance,Looks like it was the `cu(xs)` always returning `CuArray{Float32}` which I think we should never use. Will switch to `Float64` for now and worry about this factor of 2 in performance and memory usage later.,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/55#issuecomment-465594116:171,perform,performance,171,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/55#issuecomment-465594116,1,['perform'],['performance']
Performance,"Looks like you're referencing `grid` as a global in `Z(k)`. Not sure if that causes performance issues, but it won't compile on the GPU. So you might want to propagate that argument through.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1827#issuecomment-875673065:84,perform,performance,84,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1827#issuecomment-875673065,1,['perform'],['performance']
Performance,"M, T, P, B, H} where H<:Union{Cassette.DisableHooks, Nothing} where B<:Union{Nothing, Base.IdDict{Module, Base.Dict{Symbol, Cassette.BindingMeta}}} where P<:Cassette.AbstractPass where T<:Union{Nothing, Cassette.Tag{N, X, E} where E where X where N<:Cassette.AbstractContextName} where M where N<:Cassette.AbstractContextName, Any...) in module Cassette at /home/travis/.julia/packages/Cassette/1rVkq/src/overdub.jl:512 overwritten in module GPUifyLoops at /home/travis/.julia/packages/Cassette/1rVkq/src/overdub.jl:512.; ERROR: LoadError: LoadError: UndefVarError: CUBLAS not defined; Stacktrace:; [1] top-level scope at none:0 (repeats 2 times); [2] include at ./boot.jl:326 [inlined]; [3] include_relative(::Module, ::String) at ./loading.jl:1038; [4] include at ./sysimg.jl:29 [inlined]; [5] include(::String) at /home/travis/.julia/packages/CuArrays/qZCAt/src/CuArrays.jl:3; [6] top-level scope at none:0; [7] include at ./boot.jl:326 [inlined]; [8] include_relative(::Module, ::String) at ./loading.jl:1038; [9] include(::Module, ::String) at ./sysimg.jl:29; [10] top-level scope at none:2; [11] eval at ./boot.jl:328 [inlined]; [12] eval(::Expr) at ./client.jl:404; [13] top-level scope at ./none:3; in expression starting at /home/travis/.julia/packages/CuArrays/qZCAt/src/deprecated.jl:5; in expression starting at /home/travis/.julia/packages/CuArrays/qZCAt/src/CuArrays.jl:54; ERROR: LoadError: LoadError: LoadError: LoadError: UndefVarError: @setup not defined; Stacktrace:; [1] top-level scope; [2] #macroexpand#35 at ./expr.jl:107 [inlined]; [3] macroexpand at ./expr.jl:106 [inlined]; [4] docm(::LineNumberNode, ::Module, ::Any, ::Any, ::Bool) at ./docs/Docs.jl:509 (repeats 2 times); [5] @doc(::LineNumberNode, ::Module, ::String, ::Vararg{Any,N} where N) at ./boot.jl:459; [6] include at ./boot.jl:326 [inlined]; [7] include_relative(::Module, ::String) at ./loading.jl:1038; [8] include at ./sysimg.jl:29 [inlined]; [9] include(::String) at /home/travis/build/climate-machine/Oceanan",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/153#issuecomment-477579168:3079,load,loading,3079,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/153#issuecomment-477579168,1,['load'],['loading']
Performance,"MA/OceananigansArtifacts.jl/raw/ss/new_hydrostatic_data_after_cleared_bugs/quarter_degree_near_global_input_data/tau_x-1440x600-latitude-75.jld2""; │ dest = ""/u/fpoulin/.julia/scratchspaces/124859b0-ceae-595e-8997-d05f6a7a8dfe/datadeps/quarter_degree_near_global_lat_lon/tau_x-1440x600-latitude-75.jld2""; │ progress = NaN; │ time_taken = ""0.07 s""; │ time_remaining = ""NaN s""; │ average_speed = ""3.493 MiB/s""; │ downloaded = ""250.411 KiB""; │ remaining = ""∞ B""; └ total = ""∞ B""; ERROR: HTTP.Exceptions.StatusError(404, ""GET"", ""/CliMA/OceananigansArtifacts.jl/raw/ss/new_hydrostatic_data_after_cleared_bugs/quarter_degree_near_global_input_data/tau_x-1440x600-latitude-75.jld2"", HTTP.Messages.Response:; """"""; HTTP/1.1 404 Not Found; Server: GitHub.com; Date: Thu, 20 Jun 2024 13:36:59 GMT; Content-Type: text/html; charset=utf-8; Vary: X-PJAX, X-PJAX-Container, Turbo-Visit, Turbo-Frame, Accept-Encoding, Accept, X-Requested-With; Cache-Control: no-cache; Strict-Transport-Security: max-age=31536000; includeSubdomains; preload; X-Frame-Options: deny; X-Content-Type-Options: nosniff; X-XSS-Protection: 0; Referrer-Policy: origin-when-cross-origin, strict-origin-when-cross-origin; Content-Security-Policy: default-src 'none'; base-uri 'self'; child-src github.com/assets-cdn/worker/ gist.github.com/assets-cdn/worker/; connect-src 'self' uploads.github.com www.githubstatus.com collector.github.com raw.githubusercontent.com api.github.com github-cloud.s3.amazonaws.com github-production-repository-file-5c1aeb.s3.amazonaws.com github-production-upload-manifest-file-7fdce7.s3.amazonaws.com github-production-user-asset-6210df.s3.amazonaws.com api.githubcopilot.com objects-origin.githubusercontent.com copilot-proxy.githubusercontent.com/v1/engines/github-completion/completions *.actions.githubusercontent.com wss://*.actions.githubusercontent.com productionresultssa0.blob.core.windows.net/ productionresultssa1.blob.core.windows.net/ productionresultssa2.blob.core.windows.net/ productionresultssa3.b",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3502#issuecomment-2180730454:1148,Cache,Cache-Control,1148,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3502#issuecomment-2180730454,2,"['Cache', 'cache']","['Cache-Control', 'cache']"
Performance,"MITgcm I believe uses the same two-dimensional preconditioned conjugate gradient solver for the rigid lid case as for the implicit free surface case. Many ocean models often have a split explicit method for stepping forward the free surface so there is no elliptic solve. ([Killworth et al. 1991](https://journals.ametsoc.org/view/journals/phoc/21/9/1520-0485_1991_021_1333_tdoafs_2_0_co_2.xml) discusses the disadvantages of either implicit free surface or rigid lid for ocean modeling with realistically complex geometries / coastlines.). This optimization really applies just to hydrostatic models on regular grids (no horizontal stretching). Much of the time I think it would be preferable to use the nonhydrostatic model for this case, since our FFT solver is so fast that the price paid is utterly minor. Yet with an immersed boundary (and perhaps only with a non-grid-fitted immersed boundary), there are some lingering issues that we haven't resolved about whether the FFT solver can be used as is while maintaining mass conservation. The hydrostatic solver uses a vertical integral of the continuity equation and is thus far more straightforward to maintain incompressibility with non-grid-fitted boundaries. So there is a little corner case in which you might want this feature. It's also obviously useful for testing the hydrostatic model.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1727#issuecomment-851699446:546,optimiz,optimization,546,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1727#issuecomment-851699446,1,['optimiz'],['optimization']
Performance,"MWE. ```julia; using Oceananigans. grid = RectilinearGrid(size = (128, 128, 1),; x = (-5, 5),; y = (-5, 5),; z = (0, 1),; topology = (Periodic, Periodic, Bounded)). island(x, y) = (x^2 + y^2) < 1; grid = ImmersedBoundaryGrid(grid, GridFittedBottom(island)). Δx = xspacings(grid, Center(), Center(), Center()); ```. produces. ```julia; julia> include(""hydrostatic_turbulence.jl""); ERROR: LoadError: MethodError: no method matching xspacings(::ImmersedBoundaryGrid{…}, ::Center; with_halos::Bool). Closest candidates are:; xspacings(::Any, ::Any, ::Any, ::Any; with_halos); @ Oceananigans ~/.julia/packages/Oceananigans/OMBY0/src/Grids/nodes_and_spacings.jl:200; xspacings(::LatitudeLongitudeGrid{<:Any, <:Any, <:Any, <:Any, <:Any, <:Any, <:Number, <:Number}, ::Center, ::Center; with_halos); @ Oceananigans ~/.julia/packages/Oceananigans/OMBY0/src/Grids/latitude_longitude_grid.jl:674; xspacings(::LatitudeLongitudeGrid, ::Center, ::Center; with_halos); @ Oceananigans ~/.julia/packages/Oceananigans/OMBY0/src/Grids/latitude_longitude_grid.jl:658; ... Stacktrace:; [1] xspacings(grid::ImmersedBoundaryGrid{…}, ℓx::Center, ℓy::Center, ℓz::Center); @ Oceananigans.Grids ~/.julia/packages/Oceananigans/OMBY0/src/Grids/nodes_and_spacings.jl:200; ```. This is probably also a problem for `yspacings` and `zspacings`, but I'm not sure. For `GridFittedBottom`, the spacings should just return spacings for the underlying grid (right now only `PartialCellBottom` immersed boundaries have different metrics than the underlying grid).",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3701:387,Load,LoadError,387,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3701,1,['Load'],['LoadError']
Performance,"M_h.jl:3512; [4] emit(::LLVM.TargetMachine, ::LLVM.Module, ::LLVM.API.LLVMCodeGenFileType) at /home/ancellin/.julia/packages/LLVM/KITdB/src/targetmachine.jl:42; [5] mcgen at /home/ancellin/.julia/packages/GPUCompiler/4e9CU/src/mcgen.jl:73 [inlined]; [6] macro expansion at /home/ancellin/.julia/packages/TimerOutputs/dVnaw/src/TimerOutput.jl:206 [inlined]; [7] macro expansion at /home/ancellin/.julia/packages/GPUCompiler/4e9CU/src/driver.jl:254 [inlined]; [8] macro expansion at /home/ancellin/.julia/packages/TimerOutputs/dVnaw/src/TimerOutput.jl:206 [inlined]; [9] codegen(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDA.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, validate::Bool, only_entry::Bool) at /home/ancellin/.julia/packages/GPUCompiler/4e9CU/src/driver.jl:250; [10] compile(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDA.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, validate::Bool, only_entry::Bool) at /home/ancellin/.julia/packages/GPUCompiler/4e9CU/src/driver.jl:39; [11] compile at /home/ancellin/.julia/packages/GPUCompiler/4e9CU/src/driver.jl:35 [inlined]; [12] _cufunction(::GPUCompiler.FunctionSpec{typeof(Cassette.overdub),Tuple{Cassette.Context{nametype(CUDACtx),KernelAbstractions.CompilerMetadata{KernelAbstractions.NDIteration.StaticSize{(16, 16, 16)},KernelAbstractions.NDIteration.DynamicCheck,Nothing,Nothing,KernelAbstractions.NDIteration.NDRange{3,KernelAbstractions.NDIteration.StaticSize{(1, 1, 16)},KernelAbstractions.NDIteration.StaticSize{(16, 16, 1)},Nothing,Nothing}},Nothing,KernelAbstractions.var""##PassType#253"",Nothing,Cassette.DisableHooks},typeof(Oceananigans.TimeSteppers.gpu_calculate_Gu!),OffsetArray{Float64,3,CuDeviceArray{Float64,3,CUDA.AS.Global}},RegularCartesianGrid{Float64,Periodic,Periodic,Bounded,OffsetArray{Float64,1,StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}}},Ce",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/882:2283,optimiz,optimize,2283,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/882,2,['optimiz'],['optimize']
Performance,"Main motivation here is to make it possible to move CUDA with Julia 1.9 to a optional dependency,; making loading faster for non GPU workloads as well as making it easier for other GPU backends to be added. Package extensions is a backwards compatible 1.9 feature, in 1.8 and prior we still have to load CUDA.jl by default; but on 1.9 this is no longer required.",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3066:106,load,loading,106,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3066,2,['load'],"['load', 'loading']"
Performance,Major performance regression,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/726:6,perform,performance,6,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/726,1,['perform'],['performance']
Performance,"Maybe I misunderstood your suggestion but the problem with a kernel like; ```julia; integral[k] += field[i, j, k] * A_cell[i, j]; ```; is that it's not thread-safe on a GPU. You can't have multiple threads incrementing the same variable, you get a race condition. You can do that using atomic CUDA operations (e.g. an `atomic_add`) but I don't think they're part of CUDAnative.jl yet. That's why I went with a parallel reduction prefix sum algorithm which I essentially took and modified from https://github.com/JuliaGPU/CUDAnative.jl/blob/master/examples/scan.jl. Another way to calculate that horizontal integral on the GPU is to use 1 thread per vertical level, but I coded that up and it's slow as you'll need ~5000 vertical layers to saturate an Nvidia V100 GPU.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/352#issuecomment-520512225:248,race condition,race condition,248,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/352#issuecomment-520512225,1,['race condition'],['race condition']
Performance,"Might be better to use dispatch here (on the case where the kappa-tuple has length zero) for performance, clarity, and julianism!",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/691#issuecomment-596540947:93,perform,performance,93,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/691#issuecomment-596540947,1,['perform'],['performance']
Performance,"Might be cool to be able to dispatch on the order so the order could be specified as part of the model, e.g. ```julia; model = ShallowWaterModel(grid=grid, order=4); ```. and it would make it easier to use the operators in other models. We could define new types like. ```julia; struct SecondOrderCenteredDifference end # one option; struct CenteredDifference{N} end # another option; ```. then dispatch on `::SecondOrderCenteredDifference` or `::CenteredDifference{Val{4}}` or we could dispatch on numbers via `Val`. ```julia; julia> δ(i, A, ::Val{2}) = A[i] - A[i-1]; δ (generic function with 1 method). julia> δ(i, A, ::Val{4}) = (-2A[i+1] + 16A[i] - 16A[i-1] +2A[i-2]) / 12; δ (generic function with 2 methods). julia> δ(10, collect(1:20) .^ 2, Val(2)); 19. julia> δ(10, collect(1:20) .^ 2, Val(4)); 15.833333333333334; ```. but might have to be careful to [avoid performance regressions with `Val`](https://docs.julialang.org/en/v1/manual/performance-tips/#man-performance-value-type). That said it might take a non-trivial amount of refactoring to support and test dispatching on the operator order, at least for the incompressible model. Maybe it makes sense for `ShallowWaterModel` to add support for 4th-order operators first (with or without dispatch, probably easier without first) and from there we can investigate how to generalize?. If we go all out and start supporting lots of different operators I wonder if it's worth looking into [FiniteDiff.jl](https://github.com/JuliaDiff/FiniteDiff.jl) or [FiniteDifferences.jl](https://github.com/JuliaDiff/FiniteDifferences.jl). Not sure what role these packages would play. From skimming the FiniteDifferences.jl README it seems that there are no higher-order non-allocating implementations between the two packages.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1265#issuecomment-740952155:868,perform,performance,868,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1265#issuecomment-740952155,3,['perform'],"['performance', 'performance-tips', 'performance-value-type']"
Performance,"Might be good to start learning how to properly profile Oceananigans. It is slowing down a little bit as we add more things and develop stuff (see https://github.com/climate-machine/Oceananigans.jl/pull/147#issuecomment-479474578). We can easily profile it on a CPU to see where the code spends the most time and maybe find some easy things to optimize before profiling it on a GPU where things might be less obvious. Some useful links:; * https://docs.julialang.org/en/v1/manual/profile/index.html; * https://github.com/JuliaLang/julia/issues/4483 (might have to patch LLVM, could be a pain to get working).; * https://docs.nvidia.com/cuda/profiler-users-guide/index.html; * https://juliagpu.github.io/CUDAnative.jl/stable/man/performance.html#Optimizing-1; * https://discourse.julialang.org/t/writing-fast-stencil-computation-kernels-that-work-on-both-cpus-and-gpus/20200 (I pasted some commands where I use nvprof)",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/162:344,optimiz,optimize,344,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/162,3,"['Optimiz', 'optimiz', 'perform']","['Optimizing-', 'optimize', 'performance']"
Performance,"Might be worthwhile to profile with `timestepper=:RungeKutta3` as a sanity check, considering that this benchmark suggests a simple time-stepping function is 12% (!) of the cost. Another thought --- we should probably benchmark ""fully loaded"" models that at least use WENO advection (and perhaps some turbulence closure?), since that's more realistic. I think most usage of `NonhydrostaticModel` also has one tracer, rather than two (someday, we should change that default...)",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1912#issuecomment-890002262:235,load,loaded,235,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1912#issuecomment-890002262,1,['load'],['loaded']
Performance,Might have something to do with how many threads are running on how many physical cores. I'll try to tune how the threaded processes are launched and specify one thread per core more clearly. Hyperthreading seems to be what enables 2 threads to run on one core. There should be options to disable it either through slurm or julia.,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1902#issuecomment-890031307:101,tune,tune,101,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1902#issuecomment-890031307,1,['tune'],['tune']
Performance,Might not end up being merged but testing if using shared memory for the `calculate_interior_source_terms` kernel via GPUifyLoop's `@stencil` abstraction improves performance for register-heavy kernels. We can use #289 to benchmark. Reliant on https://github.com/vchuravy/GPUifyLoops.jl/issues/85.,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/293:163,perform,performance,163,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/293,1,['perform'],['performance']
Performance,Minor optimization: use `convert` rather than constructor to convert array type before output,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1182:6,optimiz,optimization,6,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1182,1,['optimiz'],['optimization']
Performance,Minor performance regression (more memory allocations),MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/397:6,perform,performance,6,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/397,1,['perform'],['performance']
Performance,More doc built optimizations -- run computationally expensive examples first,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3151:15,optimiz,optimizations,15,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3151,1,['optimiz'],['optimizations']
Performance,More docs built optimization,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3168:16,optimiz,optimization,16,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3168,1,['optimiz'],['optimization']
Performance,"More information, I tried changing Oceananigans.jl builds by running in Julia; ```; ] add https://github.com/climate-machine/Oceananigans.jl.git; ```; and I ran my code and it loaded fine the first time, but again it had a segmentation fault on the rerun. Similarly, when I executed; ```; ] add Oceananigans; ```; it ran the first time then got a segmentation fault on the second run again. Could this not be an issue with Oceananigans.jl but with how I am running my code on the GPU?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2237197159:176,load,loaded,176,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2237197159,1,['load'],['loaded']
Performance,More performant mod1 functions.,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/57:5,perform,performant,5,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/57,1,['perform'],['performant']
Performance,"Mostly I am worried about scalability and sustainability in this design, or future designs. . Currently our models are fairly simple, but its challenging to place bounds on potential future complexity. For example, models in the future may require additional fields associated with closures or parameterizations, such as boundary layer depth fields, in-plume tracer concentrations and vertical momentum, precomputed mixing length fields, and perhaps other auxiliary fields associated with various prognostic / diagnostic LES models. We probably can't plan to support setting boundary conditions on every possible field via the model constructor. With our current design we have essentially special-cased turbulent diffusivities because our focus is LES, turbulent diffusivities are relatively common, and it convenient for us. However doing this incurs some maintenance burden --- which will increase if we plan to hard-code validation and error checking. Food for thought.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/721#issuecomment-605453798:26,scalab,scalability,26,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/721#issuecomment-605453798,1,['scalab'],['scalability']
Performance,Move distributed buildkite CI to the `new-central` queue,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3489:51,queue,queue,51,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3489,1,['queue'],['queue']
Performance,"Mu understanding is that it is _required_ for GPU runs, but it also helps optimize CPU runs. In any case, you're using a `NamedTuple`, which is immutable, so it might not help here, but I think it's worth checking anyway.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1564#issuecomment-816749082:74,optimiz,optimize,74,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1564#issuecomment-816749082,1,['optimiz'],['optimize']
Performance,"My biggest concern is that introducing a grid topology and keeping the current implementation of boundary conditions will cause confusion as they perform overlapping functions. But I think with a bit of thinking there can be lots of benefits to adding a grid topology as you say. > This change will eliminate the need for a special `ChannelModel` constructor, and also eliminates the need for constructors with names like `HorizontallyPeriodicBCs`. Instead, we have a generic constructor for `FieldBoundaryConditions` that lets a user specify boundary conditions in any direction. That would be great I think. If each field now carries around its own boundary conditions and horizontally periodic and channel BCs are treated a grid topology then we can get rid of `HorizontallyPeriodicBCs`, `HorizontallyPeriodicSolutionBCs`, `ChannelBCs`, and `ChannelSolutionBCs` (+ same for triply periodic and boxes) which would greatly simplify model setup and eliminate most overlap between boundary conditions and grid topology. > A `Flat` topology will enable us to set halo sizes to 0 for `Flat` directions, and elide the unnecessary interpolation and differentiation operations in `Flat` directions that currently plagues two and one-dimensional models. True. We currently don't have a clean way of eliding unnecessary operations for 1D and 2D models. Dispatching on the grid topology would provide a way of doing this very cleanly.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/489#issuecomment-579888892:146,perform,perform,146,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/489#issuecomment-579888892,1,['perform'],['perform']
Performance,My goal right now is to do all of the changes without introducing new constructs and then evaluate performance from there.,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2924#issuecomment-1429925832:99,perform,performance,99,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2924#issuecomment-1429925832,1,['perform'],['performance']
Performance,"My suggestion of opening a separate issue comes from the fact that I don't have much time to work on this PR (as you guys could probably see by the month-delay in answering the last comments). And in the meantime there's a bug in the code that could be fixed rn with one extra commit. But fair enough. Let's solve this here. > I think the right way forward is to implement something that works for all grids. Then we can implement the grid-specific versions --- which should be viewed as conveniences or optimizations rather than necessities --- as time allows. I think this is a better and more efficient approach then implementing convenience versions first, and figuring out the general version later. You might find that the convenience versions aren't really necessary because things are convenient enough... If I understand what you're proposing, the way forward would be to implement something that always returns the 3D field/array, since that's the only way that works for all grids no matter what. Correct?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3143#issuecomment-1638223596:504,optimiz,optimizations,504,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3143#issuecomment-1638223596,1,['optimiz'],['optimizations']
Performance,Nah…. Every preview is 200MB of load on the repo I then have to clean and it’s not that easy to clean up a repository’s history. I’d rather I make the PR twice or built the docs locally :),MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2886#issuecomment-1409054886:32,load,load,32,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2886#issuecomment-1409054886,1,['load'],['load']
Performance,"New benchmarks: `WENO5` is ~2.2x slower on GPU (compared to ~6.4x slower on CPU) which is not bad (and there is room for future performance optimizations I think). ```; ────────────────────────────────────────────────────────────────────────────────────────────────────────────; Advection scheme benchmarks Time Allocations ; ────────────────────── ───────────────────────; Tot / % measured: 197s / 3.75% 19.9GiB / 0.17% ; Section ncalls time %tot avg alloc %tot avg; ────────────────────────────────────────────────────────────────────────────────────────────────────────────; 64× 64× 64 CenteredFourthOrder [CPU, Float64] 10 991ms 13.4% 99.1ms 2.19MiB 6.22% 224KiB; 64× 64× 64 CenteredSecondOrder [CPU, Float64] 10 497ms 6.74% 49.7ms 2.19MiB 6.22% 224KiB; 64× 64× 64 UpwindBiasedThirdOrder [CPU, Float64] 10 922ms 12.5% 92.2ms 2.19MiB 6.22% 224KiB; 64× 64× 64 WENO5 [CPU, Float64] 10 3.19s 43.3% 319ms 2.19MiB 6.22% 224KiB; 256×256×256 CenteredFourthOrder [GPU, Float64] 10 483ms 6.54% 48.3ms 6.61MiB 18.8% 677KiB; 256×256×256 CenteredSecondOrder [GPU, Float64] 10 277ms 3.76% 27.7ms 6.62MiB 18.8% 678KiB; 256×256×256 UpwindBiasedThirdOrder [GPU, Float64] 10 402ms 5.45% 40.2ms 6.61MiB 18.8% 677KiB; 256×256×256 WENO5 [GPU, Float64] 10 613ms 8.32% 61.3ms 6.61MiB 18.8% 677KiB; ────────────────────────────────────────────────────────────────────────────────────────────────────────────; ```",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/592#issuecomment-699498100:128,perform,performance,128,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/592#issuecomment-699498100,2,"['optimiz', 'perform']","['optimizations', 'performance']"
Performance,"New log level should be enough if we just need flat per-iteration timings to time a few blocks for a few iterations. Are you thinking of just logging raw `@time` data?. I'm not sure if we need anything more than a new log level, but I imagine some of the benefits of using a package like TimerOutputs.jl would make the timer blocks much more useful for debugging and for users. It allows for nested timers and produces a very nice table summary at the end which includes number of calls and average time/memory allocations. This could also be useful for users wishing to time their simulations to figure out how much time is being spent on I/O vs. in kernels vs. solvers vs. callbacks, etc. I guess I also see timers as a debugging tool for users. Could help be figure out cluster filesystem issues or figure out whether Oceananigans or my coupled model (via callback) is the bottleneck. Otherwise if the timers are just a developer debugging tool that dumps timing information, that's useful but it might not be useful for timing real-world scripts/simulations since the log would fill up with a huge number of lines that can't be interpreted without further processing. Here's an example of it in use: https://github.com/JuliaGPU/CUDA.jl/issues/149#issuecomment-461943376 (CUDA.jl has been using TimerOutputs.jl for a long time I think). TimerOutputs.jl is easy to use since you just add `@timeit` blocks but that does add some noise to the code (not sure if more or less than using a timing log level).",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1862#issuecomment-887921831:876,bottleneck,bottleneck,876,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1862#issuecomment-887921831,1,['bottleneck'],['bottleneck']
Performance,Nice! Performance differences could be noise too. Is it possible to say what the changes to fill halo regions were? Just in case #2477 takes longer than expected?,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2253#issuecomment-1116134666:6,Perform,Performance,6,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2253#issuecomment-1116134666,1,['Perform'],['Performance']
Performance,"Nice, maybe this will also increase the performance!",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2782#issuecomment-1284065270:40,perform,performance,40,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2782#issuecomment-1284065270,1,['perform'],['performance']
Performance,"No need to apologize, just trying to help. I think you can form an AbstractOperation for velocities when the model is constructed, eg:. ```julia; velocities = (u = uh/h, v = vh/h); ```. These can be stored in `ShallowWaterModel.velocities`, and used in kernels. They have a getindex method so they'll work with WENO functions too. This doesn't use any extra memory. AbstractOperations perform the calculation `uh/h` with correct interpolation on the fly. > Just so that I understand, instead of having momentum_flux_huu, advection and transport_tracer_flux_x we should have had something involving the area? I'm happy to help fix this where I can. We'd write a horizontal divergence either as. ```; div(Q) = 1 / V * (δx(Ax * Qx) + δy(Ay * Qy)); ```. or. ```; div(Q) = 1 / Az * (δx(Δy * Qx) + δy(Δx * Qy)); ```. I think this is written in the docstrings but doesn't appear to be reflected in the code. Correct me if I'm wrong.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1866#issuecomment-882666483:385,perform,perform,385,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1866#issuecomment-882666483,1,['perform'],['perform']
Performance,No need to worry about CUDA package errors anymore!. https://discourse.julialang.org/t/cuarrays-cudanative-psa-simplified-package-loading/27897,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/366#issuecomment-524361260:130,load,loading,130,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/366#issuecomment-524361260,1,['load'],['loading']
Performance,"No, the purpose of this line is to add the Oceananigans root directory to the load path. This allows the local version of `Oceananigans` to be used when building the docs.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1777#issuecomment-869743740:78,load,load,78,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1777#issuecomment-869743740,1,['load'],['load']
Performance,"Normally Julia will cache compilation, to reduce compilation costs. Sadly for the GPU we currently have to turn off all the caches and each GPU function basically compiles the entire world.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/66#issuecomment-466255161:20,cache,cache,20,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/66#issuecomment-466255161,2,['cache'],"['cache', 'caches']"
Performance,"Not supposed to be concrete but just to lay out some goals for big new features and prioritize them based on our needs. Also in case I forgot about them. Most of these can be done independently so the version ordering is mostly for priority. * v0.6: Arbitrary LES closures (Smagorinsky and AMD implemented); * v0.7: Halo regions (PR #167); * v0.8: Reduce memory usage by storing fewer fields.; * v0.9: Machine precision mass conservation; * v0.10: channel and boxed/cubed models, i.e. multiple wall-bounded dimensions (PR #180); * v0.11: True finite volume operators; * v0.12: Variable Δz grid; * v0.13: Higher-order advection schemes (4th order, 3rd order DST, WENO?, Prather??); * v0.14: GPU performance optimization (GPU hackathon); * v0.15: Multi-GPU (and multi-CPU?) distributed parallelism (GPU hackathon); * v0.16: Fuller high-level and API documentation; * v0.17: Fast on-the-fly diagnostics framework (CPU and GPU friendly); * v1.0: Awesome LES model!; * v1.1: Hydrostatic mode; * v1.2: Spherical grid (pole problem)?; * v1.3: Cubed sphere grid?; * ...",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/207:694,perform,performance,694,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/207,2,"['optimiz', 'perform']","['optimization', 'performance']"
Performance,"Not sure this is related, but while I usually use other languages to load and plot the outputs, I always come across `permission denied` when I have the file open by another kernel. I was wondering if there is a way to bypass that and force the overwrite.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3543#issuecomment-2097044533:69,load,load,69,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3543#issuecomment-2097044533,1,['load'],['load']
Performance,"Note that this PR does not affect at all `UpwindBiased` reconstruction: instead of this pattern ; ```julia; R_left = _left_reconstruction(.....); R_right = _right_reconstruction(.....). return ifelse(u > 0, u * R_left, u * R_right); ```. the upwind reconstruction ends up doing; ```julia; R = _reconstruction(bias(u), .....). return u * R; ```; where the double computation is performed _inside_ the reconstruction function; ```julia; _reconstruction(bias(u), .....) = ifelse(bias == LeftBias(), left_reconstruction(...), right_reconstruction(...); ````; so formally, nothing changes for linear upwind reconstructions",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3658#issuecomment-2243554967:377,perform,performed,377,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3658#issuecomment-2243554967,1,['perform'],['performed']
Performance,Note: It seems that it didn't really reduce the docs built overall time. I guess we've reached to the point where examples aren't the bottleneck anymore but rather doctests and compilation times are. :),MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3180#issuecomment-1631462344:134,bottleneck,bottleneck,134,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3180#issuecomment-1631462344,1,['bottleneck'],['bottleneck']
Performance,"Now I get this... ```julia; julia> include(""internal_wave.jl""); [ Info: Recompiling stale cache file /Users/navid/.julia/compiled/v1.1/Plots/ld3vC.ji for Plots [91a5bcdd-55d7-5caf-9e0b-520d859cae80]; σ / f = 1.0456506175619953; ERROR: LoadError: DimensionMismatch(""array could not be broadcast to match destination""); Stacktrace:; [1] check_broadcast_shape at ./broadcast.jl:456 [inlined]; [2] check_broadcast_axes at ./broadcast.jl:459 [inlined]; [3] check_broadcast_axes at ./broadcast.jl:462 [inlined]; [4] instantiate at ./broadcast.jl:258 [inlined]; [5] materialize!(::OffsetArrays.OffsetArray{Float64,3,Array{Float64,3}}, ::Base.Broadcast.Broadcasted{Base.Broadcast.DefaultArrayStyle{3},Nothing,typeof(u₀),Tuple{Array{Float64,3},Array{Float64,3},Array{Float64,3}}}) at ./broadcast.jl:756; [6] #set_ic!#3(::Function, ::Function, ::Function, ::Function, ::Function, ::Function, ::Model{CPU,RegularCartesianGrid{Float64,StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}},Oceananigans.TurbulenceClosures.ConstantAnisotropicDiffusivity{Float64},Float64}) at /Users/navid/Research/Oceananigans.jl/examples/utils.jl:17; [7] (::getfield(Main, Symbol(""#kw##set_ic!"")))(::NamedTuple{(:u, :v, :w, :T),Tuple{typeof(u₀),typeof(v₀),typeof(w₀),typeof(T₀)}}, ::typeof(set_ic!), ::Model{CPU,RegularCartesianGrid{Float64,StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}},Oceananigans.TurbulenceClosures.ConstantAnisotropicDiffusivity{Float64},Float64}) at ./none:0; [8] top-level scope at none:0; [9] include at ./boot.jl:326 [inlined]; [10] include_relative(::Module, ::String) at ./loading.jl:1038; [11] include(::Module, ::String) at ./sysimg.jl:29; [12] include(::String) at ./client.jl:403; [13] top-level scope at none:0; in expression starting at /Users/navid/Research/Oceananigans.jl/examples/internal_wave.jl:110; ```",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/254#issuecomment-497287117:90,cache,cache,90,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/254#issuecomment-497287117,3,"['Load', 'cache', 'load']","['LoadError', 'cache', 'loading']"
Performance,"Now that finally `MultiRegion` is merged we can implement the single node multi GPU paradigm also in the Nonhydrostatic model. cc @tomchor . The work can be divided in three tasks. - [x] Adapt the NonhydrostaticModel to accept a `MultiRegionGrid`. i.e., wrap local function calls in `@apply_regionally` and extend global methods in `multi_region_models.jl`. ; - [ ] Expose the parallelism in `RungeKutta3` timestepper and in the `update_state!` method. This is achieved by lumping together local function calls (all possible kernel calls such as calculate tendencies, rk substeps, etc) in outer functions and wrapping everything in `@apply_regionally`; - [ ] Implement a multi GPU pressure solver. This can be achieved in a couple of different ways. (1) transpose local memory and perform one direction FFT at the time (at we do now in the `Distributed` module through PencilArrays). (2) exploit the multi GPU capabilities of cuda through the cufftxt library that can perform single node distributed FFT to up to 16 GPUs. (3) Allocate storage and plan in Unified memory and perform the FFT in only one GPU. Ideally we would implement (3) only if we are desperate. The best solution would be to go with method (2), as (1) incurs in hefty memory transfer costs (I am not sure as to how the cufftxt implements multi GPU FFT though). The first two tasks are quite trivial so I think the bulk of the work will be on implementing the pressure solver",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2523:781,perform,perform,781,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2523,3,['perform'],['perform']
Performance,"Now that the spacing functions are in place, I'm going to start working on this. (I'll update the top post with the information below.). The goal here is to add grid metrics to NetCDF output. The are two main avenues to follow:. 1. We can follow Oceananigans nomenclature and conventions, which would make the output play more nicely with Oceananigans itself (and more generally in the Julia environment).; 2. We can follow standard community conventions, which would mean the output won't follow Oceananigans naming etc., but it would optimize its readability by other software. I think we should follow option 2, since if a user wants to work with the output in Oceananigans/Julia, then using JLD2 output is probably the right choice anyway. Given that most people in the community use Python, `xarray` and `xgcm` to analyze model output, I think we should optimize the output to work with that ecosystem out of the box. Based on the discussion in https://github.com/CliMA/Oceananigans.jl/issues/1334, it seems the preferred conventions to use are the [SGRID conventions](https://sgrid.github.io/sgrid/). For the more technical aspects, I'm planning on starting with `RectilinearGrids` and `LatLonGrids` in this PR since these are more straightforward. And then we can expand from there. I also think this should be presented to the user as an opt-in flag in `NetCDFWriter` constructor, as opposed to being included in every NetCDF output by default.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2652#issuecomment-1516604529:536,optimiz,optimize,536,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2652#issuecomment-1516604529,2,['optimiz'],['optimize']
Performance,"OK, so this _is_ the same issue as @navidcy experienced with FourierFlows.jl -- did you get this resolved? Looks to be happening on 1.3+. Backtrace from GDB:. ```; (gdb) bt; #0 0x00007f5e2a4626d6 in do_futex_wait.constprop () from target:/lib/x86_64-linux-gnu/libpthread.so.0; #1 0x00007f5e2a4627c8 in __new_sem_wait_slow.constprop.0 () from target:/lib/x86_64-linux-gnu/libpthread.so.0; #2 0x00007f5debf04b42 in lock_planner_mutex () from target:/builds/JuliaGPU/Oceananigans-jl/.julia/artifacts/e40697527cebb56d421346210295905df6e421dc/lib/libfftw3.so; #3 0x00007f5debdf3ae7 in fftw_destroy_plan () from target:/builds/JuliaGPU/Oceananigans-jl/.julia/artifacts/e40697527cebb56d421346210295905df6e421dc/lib/libfftw3.so; #4 0x00007f5d470b48c7 in ?? (); #5 0x00007f5d47fff8a8 in ?? (); #6 0x00007f5e2ab310cc in _jl_invoke (world=27509, mfunc=<optimized out>, nargs=1, args=0x7f5d47fff8a8, F=0x7f5e1a1c80e0) at /buildworker/worker/package_linux64/build/src/gf.c:2144; #7 jl_apply_generic (F=<optimized out>, args=args@entry=0x7f5d47fff8a8, nargs=nargs@entry=1) at /buildworker/worker/package_linux64/build/src/gf.c:2328; #8 0x00007f5e2ab7a9bf in jl_apply (nargs=2, args=0x7f5d47fff8a0) at /buildworker/worker/package_linux64/build/src/julia.h:1695; #9 run_finalizer (o=0x7f5dc6ae1210, ff=0x7f5e1a1c80e0, ptls=0x7f5e2b4794a0) at /buildworker/worker/package_linux64/build/src/gc.c:277; #10 0x00007f5e2ab7b500 in jl_gc_run_finalizers_in_list (ptls=ptls@entry=0x7f5e2b4794a0, list=list@entry=0x7f5d47fffa10) at /buildworker/worker/package_linux64/build/src/gc.c:363; #11 0x00007f5e2ab83885 in run_finalizers (ptls=0x7f5e2b4794a0) at /buildworker/worker/package_linux64/build/src/gc.c:391; #12 jl_gc_collect (collection=JL_GC_INCREMENTAL) at /buildworker/worker/package_linux64/build/src/gc.c:3128; #13 0x00007f5e0e8477a7 in ?? (); #14 0x00007f5e0e847730 in ?? (); #15 0x00007f5decb6e760 in ?? (); #16 0x00007f5e2b4794a0 in ?? (); #17 0x00007f5d47fffdb8 in ?? (); #18 0x00007f5e2b4794a0 in ?? (); #19 0x00007",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/636#issuecomment-589245137:842,optimiz,optimized,842,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/636#issuecomment-589245137,1,['optimiz'],['optimized']
Performance,"OK, with [9916af8](https://github.com/CliMA/Oceananigans.jl/pull/3475/commits/9916af841ae2b4069eef50b216a5ac5ac90ff1d2) I think I moved (almost) all the AMDGPU-related methods into an extension. @fluidnumerics-joe now when you do . ```Julia; julia> using Oceananigans; ```. you don't have access to the AMDGPU methods you added. But if you do. ```Julia; julia> using Oceananigans, AMDGPU; ```. then the extension loads and everything is available!",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3475#issuecomment-1944305273:413,load,loads,413,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3475#issuecomment-1944305273,1,['load'],['loads']
Performance,"Obukhov-compliant bottom drag. I couldn't reproduce the simulation perfectly (for example, they use a pseudo-spectral scheme in the horizontal directions, force their drag with a horizontal average, and modify the `SmagLilly` constant slightly) so the results are a bit different from what's in their paper, but I think that's okay. Let me know if anyone thinks otherwise. Importantly, the `ScaleInvSmag` shows an improvement over the `SmagLilly`, with the later being expectedly overly diffusive and pretty much killing any turbulence at this resolution:. https://github.com/user-attachments/assets/6440450d-52f4-43ef-9ad5-0f8fcd9993d5. For reference, this is what similar plots from the paper look like (the equivalent for us here would be SMAG and PASI):. ![image](https://github.com/user-attachments/assets/5607d340-d4b4-4405-a0a6-67ed43bfd274). Some quick notes:. - Many tests are failing become I made an ad-hoc modification for now which passes the velocities to `DiffusivityFields()`. I did that because otherwise it was hard to make the model performant and simulations were taking way too long. We can (and should) review this and either come up with a better way to construct the `diffusivity_fields` or make this change separately in another PR, which will require changing the other models too.; - I'm updating the dynamic model once every 5 time steps only (this is user-defined). This is common practice for dynamic models since their cost is significantly higher than that of constant Smag or even AMD. With that practice, the dynamic model is taking about 3 times longer to run than the constant Smagorinsky. It does take significantly longer to compile (I haven't timed it). I believe there might be some optimizations still on the table though.; - I also found that the precise value of the Smag coefficient calculated via the dynamic procedure is dependent on the advection scheme, with WENO generally leading to smaller values. In hindsight, I think that's not surprising though.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3642#issuecomment-2417879667:2322,perform,performant,2322,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3642#issuecomment-2417879667,2,"['optimiz', 'perform']","['optimizations', 'performant']"
Performance,"Occasionally you'll find user scripts peppered with things like. ```julia; Q = arch_array(arch, Q); ```. which changes `Q` to CuArray if needed and vice versa. Or, more recently:. https://github.com/CliMA/Oceananigans.jl/blob/5cc9653584370e7cbbd828583d4129628eb20fd0/validation/multi_region/multi_region_near_global_quarter_degree.jl#L117. on a multi-region grid, which ""partitions"" a global array onto difference devices. That last pattern is also needed for distributed problems in which global-size data is either built or loaded from disk. I propose we implement one utility for all these cases called something like `on_grid(obj, grid)` (note I'm reversing the argument intputs relative to `arch_array`; I think that's what we want, but it's something to discuss carefully. It's also a problem that `multi_region_object_from_array` and `arch_array` have different syntax). Usually one can write generic code for CPU/GPU --- except when building boundary conditions in terms of arrays, where we _do not_ want to automatically convert from CPU to GPU. In that case users need to write. ```julia; Q = on_grid(Q, grid); ```. since `grid` has `grid.architecture`, this will change to CPU or GPU as needed. For distributed problems we also want. ```julia; Q = on_grid(Q, grid); ```. if `Q` is loaded from file, for example. If `Q` has the size of global data, we will partition it into a local version (since the grid is also local). We can ""detect"" whether `Q` has a local size (though there are some subtleties re: dimensionality...) and handle that case. We can also transfer to correct architecture. For multi-region problems we write. ```julia; Q = on_grid(Q, grid); ```. which will return a `MultiRegionObject` with `Q` appropriately partitioned. I think this will help users write generic code that can run on any grid + architecture. Other names are definitely welcome!",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2534:526,load,loaded,526,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2534,2,['load'],['loaded']
Performance,Oceananigans.jl/src/Models/LagrangianParticleTracking/lagrangian_particle_advection.jl:177 [inlined]; cpu__advect_particles! at /home/alir/.julia/packages/KernelAbstractions/491pi/src/macros.jl:291 [inlined]; cpu__advect_particles! at ./none:0; __thread_run at /home/alir/.julia/packages/KernelAbstractions/491pi/src/cpu.jl:144; unknown function (ip: 0x7c0090512182); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; __run at /home/alir/.julia/packages/KernelAbstractions/491pi/src/cpu.jl:111; unknown function (ip: 0x7c009050feb3); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; #_#16 at /home/alir/.julia/packages/KernelAbstractions/491pi/src/cpu.jl:46; _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; jl_apply at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/julia.h:1982 [inlined]; do_apply at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/builtins.c:768; Kernel at /home/alir/.julia/packages/KernelAbstractions/491pi/src/cpu.jl:39; _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; advect_lagrangian_particles! at /home/alir/atdepth/Oceananigans.jl/src/Models/LagrangianParticleTracking/lagrangian_particle_advection.jl:193; step_lagrangian_particles! at /home/alir/atdepth/Oceananigans.jl/src/Models/LagrangianParticleTracking/LagrangianParticleTracking.jl:143 [inlined]; step_lagrangian_particles! at /home/alir/atdepth/Oceananigans.jl/src/Mo,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3852:2669,cache,cache,2669,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3852,1,['cache'],['cache']
Performance,"OffsetArrays.OffsetArray{Float64, 3, CUDA.CuDeviceArray{Float64, 3, 1}}, typeof(identity), RectilinearGrid{Float64, Periodic, Periodic, Bounded, Float64, Float64, Float64, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, Nothing}, typeof(condition_greater_3), Int64, Float64}}}}}); @ GPUCompiler ~/.julia/packages/GPUCompiler/07qaN/src/driver.jl:76; [9] cufunction_compile(job::GPUCompiler.CompilerJob); @ CUDA ~/.julia/packages/CUDA/DfvRa/src/compiler/execution.jl:346; [10] cached_compilation(cache::Dict{UInt64, Any}, job::GPUCompiler.CompilerJob, compiler::typeof(CUDA.cufunction_compile), linker::typeof(CUDA.cufunction_link)); @ GPUCompiler ~/.julia/packages/GPUCompiler/07qaN/src/cache.jl:90; [11] cufunction(f::typeof(CUDA.partial_mapreduce_grid), tt::Type{Tuple{typeof(identity), typeof(Base.add_sum), Nothing, CartesianIndices{3, Tuple{Base.OneTo{Int64}, Base.OneTo{Int64}, Base.OneTo{Int64}}}, CartesianIndices{3, Tuple{Base.OneTo{Int64}, Base.OneTo{Int64}, Base.OneTo{Int64}}}, Val{true}, Base.ReshapedArray{Float64, 4, SubArray{Float64, 3, CUDA.CuDeviceArray{Float64, 3, 1}, Tuple{UnitRange{Int64}, UnitRange{Int64}, UnitRange{Int64}}, false}, Tuple{Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64}, Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64}}}, Oceananigans.AbstractOperations.ConditionalOperation{Center, Center, Center, OffsetArrays.OffsetArray{Float64, 3, CUDA.CuDeviceArray{Float64, 3, 1}}, typeof(identity), RectilinearGrid{Float64, Periodic, Periodic, Bounded, Float64, Float64, Float64, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, Offs",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2701#issuecomment-1242894568:9361,cache,cache,9361,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2701#issuecomment-1242894568,1,['cache'],['cache']
Performance,"Often when running CI, the tests fail on tartarus because of a loading error (there are apparently some missing files),. example; ```; ERROR: LoadError: LoadError: SystemError: opening file ""/storage5/buildkite-agent/.julia-5513/compiled/v1.6/Oceananigans/hU93i_FjLMs.ji"": No such file or directory; ```. Retrying the test clears the error, but maybe we should look a bit into it so that we don't have to manually retry...",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2222:63,load,loading,63,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2222,3,"['Load', 'load']","['LoadError', 'loading']"
Performance,"Oh ok, this error on CPU initialization?. ```; ERROR: LoadError: Failed to precompile GPUArrays [0c68f7d7-f131-5f86-a1c3-88cf8149b2d7] to /storage7/buildkite-agent/.julia-1965/compiled/v1.5/GPUArrays/v5u0T_Ru6eV.ji.; --;   | Stacktrace:;   | [1] top-level scope at none:2;   | [2] eval at ./boot.jl:347 [inlined];   | in expression starting at /storage7/buildkite-agent/.julia-1965/packages/CUDA/wTQsK/src/CUDA.jl:5;   | WARNING: Error during initialization of module LinearAlgebra:;   | ErrorException(""could not load library ""libopenblas64_"";   | libopenblas64_.so: ELF load command past end of file"");   | ┌ Error: Error during initialization of module CHOLMOD;   | │ exception =;   | │ could not load library ""libcholmod"";   | │ libopenblas64_.so.0: ELF load command past end of file;   | │ Stacktrace:;   | │ [1] dlopen(::String, ::UInt32; throw_error::Bool) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Libdl/src/Libdl.jl:109;   | │ [2] dlopen at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Libdl/src/Libdl.jl:109 [inlined] (repeats 2 times);   | │ [3] __init__() at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/SuiteSparse/src/cholmod.jl:90;   | └ @ SuiteSparse.CHOLMOD /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/SuiteSparse/src/cholmod.jl:187;   | /storage7/buildkite-agent/julia-1.5.4/bin/julia: error while loading shared libraries: libLLVM-9jl.so: ELF load command past end of file; ```. I believe these happen occasionally when using `instantiate` and the solution is typically to restart the build?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1566#issuecomment-817009364:54,Load,LoadError,54,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1566#issuecomment-817009364,7,"['Load', 'load']","['LoadError', 'load', 'loading']"
Performance,Oh wow! Let me check the performance,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3127#issuecomment-1577385126:25,perform,performance,25,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3127#issuecomment-1577385126,1,['perform'],['performance']
Performance,"Oh yeah I think I found that having 256 threads per block gave decent performance (a long time ago...) which is what I've used for those thread-block layouts. We always used 1 thread in z (number of z blocks = Nz) so for 3D grids Tx = Ty = 16 so that we have 16^2 = 256 threads per block while for 2D grids we set Tx or Ty = 256 so that Tx*Ty = 256 and we still have 256 threads per block. Just ensuring that large 2D models can saturate the GPU. But yeah, it should be documented in there unless `launch_config` can perform just as well as the hand-picked thread-block layout then we can get rid of all this.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/645#issuecomment-592501548:70,perform,performance,70,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/645#issuecomment-592501548,2,['perform'],"['perform', 'performance']"
Performance,"Oh, it looks good, it would be interesting to perform some complex operation and see if we get a speedup",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3234#issuecomment-1694026789:46,perform,perform,46,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3234#issuecomment-1694026789,1,['perform'],['perform']
Performance,"Ok I think I figured it out. The `Float64` values are coming in from functions like `depthᶜᶜᶠ` and `height_above_bottomᶜᶜᶠ`. The main issue is actually the grid coordinates not being fully `Float32`. In particular, when coordinates like `grid.zᵃᵃᶜ` are ranges the reference and step size are `Float64`:. ```julia; julia> r = range(0.0f0, 1.0f0, 16); 0.0f0:0.06666667f0:1.0f0. julia> typeof(r); StepRangeLen{Float32, Float64, Float64, Int64}; ```. You can force it to be `Float32` via:. ```julia; julia> rr = StepRangeLen{Float32, Float32, Float32, Int}(r); 0.0f0:0.06666667f0:1.0f0. julia> typeof(rr); StepRangeLen{Float32, Float32, Float32, Int64}; ```. Doing this in `grid_coordinates.jl` fixes the MWE. Curiously this behavior of `range` is not mentioned in the docs for `range` or `StepRangeLen` but is discussed on the [Julia Discourse](https://discourse.julialang.org/t/the-type-of-a-range-step-defined-as-float32-changes-to-float64/27411). Well looks like it was briefly mentioned in the `StepRangeLen` docstring in 2019. The step size being twice the precision is supposed to help with rounding errors. But maybe on the GPU it can do more harm than good?. I can see how this leads to type promotion to `Float64` but I'm not totally sure how having a `Float64` reference and step size leads to illegal memory accesses. I'm also surprised that this issue never cropped up before. On another note, I wonder if this had any impact on performance. I'm curious to do some benchmarking before and after this change.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3870#issuecomment-2445532635:1438,perform,performance,1438,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3870#issuecomment-2445532635,1,['perform'],['performance']
Performance,"Ok here's something simple:. ```julia; using Oceananigans; using BenchmarkTools. grid = RectilinearGrid(CPU(), size=(128, 128, 1), x=(0, 2π), y=(0, 2π), z=(0, 1)); model = NonhydrostaticModel(; grid, advection=WENO()). function lots_of_steps!(model, Δt, steps=100); for _ = 1:steps; time_step!(model, Δt); end; end. @btime lots_of_steps!(model, 0.01); ```. Here's what I've done:. * Run this on fresh clone of `main`. This returns. ```julia; julia> include(""../simple_benchmark.jl""); [ Info: Precompiling Oceananigans [9e8cae18-63c1-5223-a75c-80ca9d6e9a09]; 20.460 s (144483404 allocations: 94.43 GiB); ```. * Restrict compat on KernelAbstractions to 0.7.2 and CUDAKernels to 0.3.3. This returns:. ```julia; julia> include(""../simple_benchmark.jl""); [ Info: Precompiling Oceananigans [9e8cae18-63c1-5223-a75c-80ca9d6e9a09]; 2.202 s (118604 allocations: 52.20 MiB); ```. I'm running on a single core, Mac M1. Here the performance loss is just 10x so I'll change the somewhat dramatic title of this issue.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2996#issuecomment-1481304842:917,perform,performance,917,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2996#issuecomment-1481304842,1,['perform'],['performance']
Performance,"Ok, a bit better now!. The following uses some lower-level Oceananigans functions and `KernelFunctionOperation`. I suspect this is a bit more performant, but I'm not sure. The main reason is that I think our broadcasting machinery has some overhead right now, so writing `field .= op` is not all that cheap (we can fix this, but it might take some work). @iuryt if you have the chance to benchmark different solutions, it'd be interesting to hear what works best!. The main downside of this solution is that we need to understand the staggered grid to implement it. Maybe not too onerous (@simone-silvestri thinks I should give users more credit), but part of me feels like we should be able to auto-magic our way around this. The main barrier to using abstract operations here is figuring out how to implement this function with a `ConditionalOperation` (and also having `<` as a valid `BinaryOperation`, eg figuring out #2169). I also like the following because it's the first known example of `auxiliary_fields` being used. Hooray for that! Also I realized that we can just use a linear stratification which is nice. I made the shear stronger to increase the drama. Working on this helped uncover a few wrinkles in the user API:. * `closure = ScalarDiffusivity(VerticallyImplicitTimeDiscretization(); ν, κ=κᵇ)` doesn't work (#2342); * `HydrostaticFreeSurfaceModel(; velocities=velocities)` doesn't work (#2341). ```julia; using Oceananigans; using Oceananigans.Units; using Oceananigans.Operators; using GLMakie. # A bit of code...; @inline f²(i, j, k, grid, f, args...) = @inbounds f(i, j, k, grid, args...)^2. @inline function Riᶜᶜᶜ(i, j, k, grid, U, b); N² = ℑzᵃᵃᶜ(i, j, k, grid, ∂zᶜᶜᶠ, b); S²u = ℑxzᶜᵃᶜ(i, j, k, grid, f², ∂zᶠᶜᶠ, U.u); S²v = ℑyzᵃᶜᶜ(i, j, k, grid, f², ∂zᶜᶠᶠ, U.v); S² = S²u + S²v; return ifelse(S² == 0, zero(eltype(grid)), N² / S²); end. grid = RectilinearGrid(size=128, z=(-100, 0), halo=3, topology=(Flat, Flat, Bounded)); fake_model = HydrostaticFreeSurfaceModel(; grid, trac",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2337#issuecomment-1066093967:142,perform,performant,142,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2337#issuecomment-1066093967,1,['perform'],['performant']
Performance,"Ok, so if we shouldn't be using KernelAbstractions for performance on CPU, then I think this means we should write our own CPU infrastructure stuff and rely on KA only for GPU? Is that what you recommend?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2996#issuecomment-1481472066:55,perform,performance,55,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2996#issuecomment-1481472066,1,['perform'],['performance']
Performance,"Ok. That makes sense, since CPU is not our goal; we can accept some loss of performance on CPU in order to simplify the code. The other question is why we are not implementing this in PencilArrays / PencilFFTs. Having an independent implementation may not be the best practice (we want to be good open source community members), but could be justified, maybe.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3279#issuecomment-1727562842:76,perform,performance,76,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3279#issuecomment-1727562842,1,['perform'],['performance']
Performance,"Ok. The case where I think `SumOfFields` could be needed is if one runs into parameter space issues if its embedded into a tendency kernel function on the GPU. This is a compilation issue that is very hard to fix (performance problems, on the other hand, should be fixable).",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3676#issuecomment-2269566739:214,perform,performance,214,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3676#issuecomment-2269566739,1,['perform'],['performance']
Performance,"Okay interesting!. I'm not sure why you would need `gcc`. Have you tried omitting that?. Also, did you try installing julia yourself? There could be some benefit in using the system Julia via `module load julia/1.9.2`, but this would mostly have to do with compilation speed (not execution speed) I think. If you have found discrepancy between the system Julia vs your own Julia install that'd be interesting to hear about... You could also let julia handle netcdf --- again, I'm not sure if this would have performance implications or not, but it might provide faster way to get up and running if you are simply trying to run scripts. Julia's package manager should be able to automatically detect and install netcdf appropriate for the system, I think.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2258884823:200,load,load,200,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2258884823,2,"['load', 'perform']","['load', 'performance']"
Performance,"Okay, sounds good!. Let's not focus *too much* on optimization prematurely, though if I understand you I think you are really talking about the obvious things, like avoiding memory allocation and transfers to/from the CPU to GPU.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/115#issuecomment-470793646:50,optimiz,optimization,50,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/115#issuecomment-470793646,1,['optimiz'],['optimization']
Performance,"On Julia 1.10 users are met with an ~~avalanche~~ tsunami of warnings. ```; warning: /Users/gregorywagner/.julia/packages/KernelAbstractions/WoCk1/src/extras/loopinfo.jl:28:0: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering; ```. For example we try. https://github.com/CliMA/Oceananigans.jl/blob/7291ada057afc9cfcefb2b6e9351cff8782d9217/src/Solvers/batched_tridiagonal_solver.jl#L133-L148. but this loop probably can't be unrolled because `Nx` is a runtime value, not a compile time constant. I don't know if we ever `@unroll` properly... Seems like the easiest thing is just to stop pretending that we `@unroll`. @jlk9",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3374:199,optimiz,optimizer,199,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3374,2,"['optimiz', 'perform']","['optimizer', 'perform']"
Performance,On Satori using stupidly large meshes gives 85% - 89% efficiency going from 1 to 2 GPU for the `multi_region_turbulence.jl` benchmark (Note `1440×600×48` is the size of the 1/4 degree simulation); Unfortunately the efficiency decreases on a larger number of GPUs... we definitely have to fix the scaling. #### Strong Scaling; | Grid size | Grid | GPUs | wall time | efficiency |; | -- | -- | -- | -- | -- |; | `1024×1024×100`| `RectilinearGrid` | 1 | 3.4 minutes | 100% |; | `1024×1024×100`| `MultiRegionGrid` | 2 | 1.9 minutes | 89.5% | ; | `1440×600×48`| `RectilinearGrid` | 1 | 1.4 minutes | 100% |; | `1440×600×48`| `MultiRegionGrid` | 2 | 49.2 seconds | 85.4% |; | `1440×600×48`| `MultiRegionGrid` | 3 | 38.8 seconds | 72.2% |. Going to smaller meshes than these hampers the efficiency incredibly. I think there might be a lot of low hanging fruits to optimize multi GPU,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2253#issuecomment-1116489219:857,optimiz,optimize,857,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2253#issuecomment-1116489219,1,['optimiz'],['optimize']
Performance,Only perform operations as needed depending on model dimension.,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/35:5,perform,perform,5,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/35,1,['perform'],['perform']
Performance,Optimizes fill_halo_regions_open.jl,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3792:0,Optimiz,Optimizes,0,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3792,1,['Optimiz'],['Optimizes']
Performance,"Our `Field` type may benefit from subtyping one of the many array types with named/dimensional axes out there in the julia ecosystem. This will permit some desirable behaviors for the manipulation, loading, output, and plotting of fields and slices of fields, and perhaps will also enable some nice behavior for online diagnostics. A good option could be `AbstractDimensionalArray` defined by. https://github.com/rafaqz/DimensionalData.jl. We'll have to figure out what functions and such we'd have to define to complete such an implementation.",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/457:198,load,loading,198,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/457,1,['load'],['loading']
Performance,"Our model constructor API asks users to pass several `NamedTuple`s for field-specific input like `forcing`, `boundary_conditions`, etc. For example:. https://github.com/CliMA/Oceananigans.jl/blob/b3ddbc84c8f35aaf5b93fbbfdb4cffcada5c6533/src/Models/NonhydrostaticModels/nonhydrostatic_model.jl#L108. We currently enforce that these are `NamedTuple` by typing the keyword argument. However, this leads to obscure `TypeError` that may not be all that helpful / interpretable. For example:. ```; ERROR: LoadError: TypeError: in keyword argument forcing, expected NamedTuple, got a value of type Oceananigans.Forcings.DiscreteForcing{NamedTuple{(:x₀, :z₀, :δᴸ, :R), NTuple{4, Float64}}, typeof(b_discrete_forcing_func)}; ```. A more user-friendly error might be an `ArgumentError` that says something like. ```; $argname must be `NamedTuple` but is $argtype instead! Check for missing commas or semicolons. A common typo is to write `(field=value)` rather than `(field=value,)` or `(; field=value)`.; ```. In particular, a super common typo is to write something like. ```julia; forcing = (u=u_forcing),; ```. rather than the two-character-different. ```julia; forcing = (; u=u_forcing),; ```. We can write a little utility for throwing an error message like this, and then just loop over the relevant `NamedTuple` inputs in every model constructor. We can also check that `keys(arg)` are contained in the fields of the model and emit a warning if there's a key that'll be unused. Noticed while working on a script with @raphaelouillon.",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2269:499,Load,LoadError,499,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2269,1,['Load'],['LoadError']
Performance,"Output needs to be arbitrary. We may need to perform on-line analysis and output the result (example: turbulent dissipation rate, time-averages, slices of fields, point values, etc). We should design an additional interface for Fields. The type of the field indicates the coordinates on which the field is defined, so we should design an interface that uses that information.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/93#issuecomment-468290310:45,perform,perform,45,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/93#issuecomment-468290310,1,['perform'],['perform']
Performance,"PJ Tuckman (@qwert2266) recently wrote a pretty sweet Julia script for doing automated parameter exploration with Slurm on Satori (see https://github.com/ali-ramadhan/JuicyMoons.jl/pull/14). The script itself can be found at: https://github.com/ali-ramadhan/JuicyMoons.jl/blob/pjt/enceladus-slurm/slurm/20201031ScriptCreator.jl (needs some refactoring and might have bugs). The automation was complicated by the fact that Satori only allows you 1 Slurm job (through which you can request 4 GPUs and cram 4 GPU simulations on one node) and there's a 12 hour time limit on all jobs. So the idea/hack we came up with was for the simulation scripts to touch a file to indicate they have checkpointed themselves and to touch another file to indicate they have reached steady state. The Julia script keeps creating and submitting Slurm scripts until all simulations have reached steady state (""checkpointed"" simulations are queued/scheduled again while simulation that have reached ""steady state"" are not queued/scheduled any more). The point of this issue is to discuss whether it makes sense to add an example/tutorial of automating parameter exploration with Slurm? The specific workflow discussed above is specific to Satori so it might not make sense to include it in the docs (might be more of an internal resource). We looked at ClusterManagers.jl but don't think it's super useful since we're working around only have 1 job, and we don't know what the next job will be until the 4 simulations crammed into the first job are done running. X-Ref: #1045 proposes adding example Slurm scripts would is definitely a good idea. cc @sandreza @suyashbire1 might be interested.",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1137:918,queue,queued,918,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1137,2,['queue'],['queued']
Performance,"PU │ 256 │ (Periodic, Bounded, Bounded) │ 25.185 ms │ 25.761 ms │ 25.702 ms │ 25.865 ms │ 29.56 KiB │ 629 │; │ GPU │ 256 │ (Periodic, Periodic, Bounded) │ 9.832 ms │ 10.689 ms │ 10.631 ms │ 10.849 ms │ 13.06 KiB │ 290 │; └───────────────┴─────┴───────────────────────────────┴───────────┴───────────┴───────────┴───────────┴───────────┴────────┘; ```. ### CPU to GPU speedup. ```; Fourier-tridiagonal Poisson solver CPU -> GPU speedup; ┌─────┬───────────────────────────────┬─────────┬─────────┬─────────┐; │ Ns │ Topologies │ speedup │ memory │ allocs │; ├─────┼───────────────────────────────┼─────────┼─────────┼─────────┤; │ 256 │ (Bounded, Bounded, Bounded) │ 50.4045 │ 21.5194 │ 32.4444 │; │ 256 │ (Bounded, Periodic, Bounded) │ 51.2039 │ 15.8992 │ 23.2963 │; │ 256 │ (Periodic, Bounded, Bounded) │ 52.4472 │ 15.8992 │ 23.2963 │; │ 256 │ (Periodic, Periodic, Bounded) │ 99.4371 │ 6.48062 │ 10.7407 │; └─────┴───────────────────────────────┴─────────┴─────────┴─────────┘; ```. ### Relative performance on the CPU. ```; Fourier-tridiagonal Poisson solver relative performance (CPU); ┌───────────────┬─────┬───────────────────────────────┬──────────┬──────────┬────────┐; │ Architectures │ Ns │ Topologies │ slowdown │ memory │ allocs │; ├───────────────┼─────┼───────────────────────────────┼──────────┼──────────┼────────┤; │ CPU │ 256 │ (Bounded, Bounded, Bounded) │ 1.58185 │ 1.0 │ 1.0 │; │ CPU │ 256 │ (Bounded, Periodic, Bounded) │ 1.24529 │ 0.922481 │ 1.0 │; │ CPU │ 256 │ (Periodic, Bounded, Bounded) │ 1.27117 │ 0.922481 │ 1.0 │; │ CPU │ 256 │ (Periodic, Periodic, Bounded) │ 1.0 │ 1.0 │ 1.0 │; └───────────────┴─────┴───────────────────────────────┴──────────┴──────────┴────────┘; ```. ### Relative performance on the GPU. ```; Fourier-tridiagonal Poisson solver relative performance (GPU); ┌───────────────┬─────┬───────────────────────────────┬──────────┬─────────┬─────────┐; │ Architectures │ Ns │ Topologies │ slowdown │ memory │ allocs │; ├───────────────┼─────┼──────────────────",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1403#issuecomment-786398050:8232,perform,performance,8232,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1403#issuecomment-786398050,1,['perform'],['performance']
Performance,"PUCompiler ~/.julia/packages/GPUCompiler/2CW9L/src/driver.jl:79; [10] compile; @ ~/.julia/packages/GPUCompiler/2CW9L/src/driver.jl:74 [inlined]; [11] #1145; @ ~/.julia/packages/CUDA/2kjXI/src/compiler/compilation.jl:250 [inlined]; [12] JuliaContext(f::CUDA.var""#1145#1148""{GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget, CUDA.CUDACompilerParams}}; kwargs::@Kwargs{}); @ GPUCompiler ~/.julia/packages/GPUCompiler/2CW9L/src/driver.jl:34; [13] JuliaContext(f::Function); @ GPUCompiler ~/.julia/packages/GPUCompiler/2CW9L/src/driver.jl:25; [14] compile(job::GPUCompiler.CompilerJob); @ CUDA ~/.julia/packages/CUDA/2kjXI/src/compiler/compilation.jl:249; [15] actual_compilation(cache::Dict{Any, CuFunction}, src::Core.MethodInstance, world::UInt64, cfg::GPUCompiler.CompilerConfig{GPUCompiler.PTXCompilerTarget, CUDA.CUDACompilerParams}, compiler::typeof(CUDA.compile), linker::typeof(CUDA.link)); @ GPUCompiler ~/.julia/packages/GPUCompiler/2CW9L/src/execution.jl:237; [16] cached_compilation(cache::Dict{Any, CuFunction}, src::Core.MethodInstance, cfg::GPUCompiler.CompilerConfig{GPUCompiler.PTXCompilerTarget, CUDA.CUDACompilerParams}, compiler::Function, linker::Function); @ GPUCompiler ~/.julia/packages/GPUCompiler/2CW9L/src/execution.jl:151; [17] macro expansion; @ ~/.julia/packages/CUDA/2kjXI/src/compiler/execution.jl:380 [inlined]; [18] macro expansion; @ ./lock.jl:267 [inlined]; [19] cufunction(f::typeof(Oceananigans.Models.NonhydrostaticModels.gpu_compute_Gu!), tt::Type{Tuple{KernelAbstractions.CompilerMetadata{KernelAbstractions.NDIteration.StaticSize{(10, 10, 10)}, KernelAbstractions.NDIteration.DynamicCheck, Nothing, Nothing, KernelAbstractions.NDIteration.NDRange{3, KernelAbstractions.NDIteration.StaticSize{(1, 1, 10)}, KernelAbstractions.NDIteration.StaticSize{(16, 16, 1)}, Nothing, Nothing}}, OffsetArrays.OffsetArray{Float64, 3, CuDeviceArray{Float64, 3, 1}}, ImmersedBoundaryGrid{Float64, Oceananigans.Grids.Periodic, Oceananigans.Grids.Periodic, Bounded, Rectilinear",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3646#issuecomment-2428001700:7852,cache,cache,7852,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3646#issuecomment-2428001700,1,['cache'],['cache']
Performance,"Pair{Base.PkgId, UInt64}}, source::String); @ Base ./loading.jl:1213; [12] top-level scope; @ none:1; [13] eval; @ ./boot.jl:360 [inlined]; [14] eval(x::Expr); @ Base.MainInclude ./client.jl:446; [15] top-level scope; @ none:1; in expression starting at /home/tomas/.julia/packages/CUDA/3VnCC/src/device/intrinsics/math.jl:5; in expression starting at /home/tomas/.julia/packages/CUDA/3VnCC/src/device/intrinsics.jl:22; in expression starting at /home/tomas/.julia/packages/CUDA/3VnCC/src/CUDA.jl:1; ERROR: LoadError: Failed to precompile CUDA [052768ef-5323-5732-b1bb-66c8b64840ba] to /home/tomas/.julia/compiled/v1.6/CUDA/jl_q4lPlx.; Stacktrace:; [1] error(s::String); @ Base ./error.jl:33; [2] compilecache(pkg::Base.PkgId, path::String, internal_stderr::Base.TTY, internal_stdout::Base.TTY); @ Base ./loading.jl:1360; [3] compilecache(pkg::Base.PkgId, path::String); @ Base ./loading.jl:1306; [4] _require(pkg::Base.PkgId); @ Base ./loading.jl:1021; [5] require(uuidkey::Base.PkgId); @ Base ./loading.jl:914; [6] require(into::Module, mod::Symbol); @ Base ./loading.jl:901; [7] include; @ ./Base.jl:386 [inlined]; [8] include_package_for_output(pkg::Base.PkgId, input::String, depot_path::Vector{String}, dl_load_path::Vector{String}, load_path::Vector{String}, concrete_deps::Vector{Pair{Base.PkgId, UInt64}}, source::String); @ Base ./loading.jl:1213; [9] top-level scope; @ none:1; [10] eval; @ ./boot.jl:360 [inlined]; [11] eval(x::Expr); @ Base.MainInclude ./client.jl:446; [12] top-level scope; @ none:1; in expression starting at /home/tomas/repos/Oceananigans.jl/src/Oceananigans.jl:1; ERROR: LoadError: Failed to precompile Oceananigans [9e8cae18-63c1-5223-a75c-80ca9d6e9a09] to /home/tomas/.julia/compiled/v1.6/Oceananigans/jl_psrPk0.; Stacktrace:; [1] error(s::String); @ Base ./error.jl:33; [2] compilecache(pkg::Base.PkgId, path::String, internal_stderr::Base.TTY, internal_stdout::Base.TTY); @ Base ./loading.jl:1360; [3] compilecache(pkg::Base.PkgId, path::String); @ Base ./loading",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1707#issuecomment-849206371:2294,load,loading,2294,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1707#issuecomment-849206371,1,['load'],['loading']
Performance,"Passing `architecture = CPU` instead of `architecture = CPU()` when constructing an `IncompressibleModel` can produce a rather punishing error message as @glwagner and I found out with @vchuravy. Would be very easy to do a quick check for this and save future users the potential headache. ```; ERROR: LoadError: type NamedTuple has no field grid; Stacktrace:; [1] getbc at /home/vchuravy/.julia/packages/Oceananigans/IbUoB/src/BoundaryConditions/field_boundary_conditions.jl:199 [inlined]; [2] getproperty at /home/vchuravy/.julia/packages/Oceananigans/IbUoB/src/BoundaryConditions/field_boundary_conditions.jl:197 [inlined]; [3] topology at /home/vchuravy/.julia/packages/Oceananigans/IbUoB/src/Fields/abstract_field.jl:117 [inlined]; [4] topology(::NamedTuple{(:x, :y, :z),Tuple{CoordinateBoundaryConditions{BoundaryCondition{Oceananigans.BoundaryConditions.Periodic,Nothing},BoundaryCondition{Oceananigans.BoundaryConditions.Periodic,Nothing}},CoordinateBoundaryConditions{BoundaryCondition{Oceananigans.BoundaryConditions.Periodic,Nothing},BoundaryCondition{Oceananigans.BoundaryConditions.Periodic,Nothing}},CoordinateBoundaryConditions{BoundaryCondition{Flux,Nothing},BoundaryCondition{Flux,Float64}}}}, ::Int64) at /home/vchuravy/.julia/packages/Oceananigans/IbUoB/src/Grids/Grids.jl:81; [5] NamedTuple{(:x, :y, :z),T} where T<:Tuple(::NamedTuple{(:x, :y, :z),Tuple{CoordinateBoundaryConditions{BoundaryCondition{Oceananigans.BoundaryConditions.Periodic,Nothing},BoundaryCondition{Oceananigans.BoundaryConditions.Periodic,Nothing}},CoordinateBoundaryConditions{BoundaryCondition{Oceananigans.BoundaryConditions.Periodic,Nothing},BoundaryCondition{Oceananigans.BoundaryConditions.Periodic,Nothing}},CoordinateBoundaryConditions{BoundaryCondition{Flux,Nothing},BoundaryCondition{Flux,Float64}}}}, ::Tuple{DataType,DataType,DataType}) at /home/vchuravy/.julia/packages/Oceananigans/IbUoB/src/BoundaryConditions/field_boundary_conditions.jl:85; [6] UVelocityBoundaryConditions(::NamedTuple{(:x, :y",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1118:302,Load,LoadError,302,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1118,1,['Load'],['LoadError']
Performance,"Pasted from a slack discussion:. Hi, everyone. Can someone please give me a little push in the right direction? I'm trying to understand the calls to calculate_tracer_diffusivity but I can't make sense of them.; The relevant code is here:. ```julia; function calculate_diffusivities!(K, arch, grid, closure::AnisotropicMinimumDissipation, buoyancy, U, C); workgroup, worksize = work_layout(grid, :xyz); barrier = Event(device(arch)); viscosity_kernel! = calculate_viscosity!(device(arch), workgroup, worksize); diffusivity_kernel! = calculate_tracer_diffusivity!(device(arch), workgroup, worksize); viscosity_event = viscosity_kernel!(K.νₑ, grid, closure, buoyancy, U, C, dependencies=barrier); events = [viscosity_event]; for (tracer_index, κₑ) in enumerate(K.κₑ); @inbounds c = C[tracer_index]; event = diffusivity_kernel!(κₑ, grid, closure, c, Val(tracer_index), U, dependencies=barrier); push!(events, event); end; wait(device(arch), MultiEvent(Tuple(events))); return nothing; end; @kernel function calculate_viscosity!(νₑ, grid, closure::AnisotropicMinimumDissipation, buoyancy, U, C); i, j, k = @index(Global, NTuple); @inbounds νₑ[i, j, k] = νᶜᶜᶜ(i, j, k, grid, closure, buoyancy, U, C); end; @kernel function calculate_tracer_diffusivity!(κₑ, grid, closure, c, tracer_index, U); i, j, k = @index(Global, NTuple); @inbounds κₑ[i, j, k] = κᶜᶜᶜ(i, j, k, grid, closure, c, tracer_index, U); end; ```. So it seems that `calculate_tracer_diffusivity!` is defined with the signature `κₑ, grid, closure, c, tracer_index, U`, but is called with the signature `device(arch), workgroup, worksize`.; I can see that the `diffusivity_kernel!` is somehow what actually performs the ""correct"" signature call in the line `event = diffusivity_kernel!(κₑ, grid, closure, c, Val(tracer_index), U, dependencies=barrier)`, but I honestly can't understand why that's the case or even how that works. Could someone please shed some light?. CC @glwagner",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1873:1663,perform,performs,1663,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1873,1,['perform'],['performs']
Performance,Perform some of the grid tests on GPU as well,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3054:0,Perform,Perform,0,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3054,1,['Perform'],['Perform']
Performance,Performance benchmarks section of the README is super out of date,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3684:0,Perform,Performance,0,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3684,1,['Perform'],['Performance']
Performance,Performance benchmarks section of the README wrongly says we don't support `Distributed`,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3758:0,Perform,Performance,0,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3758,1,['Perform'],['Performance']
Performance,"Performance benchmarks seem fine timing wise (model hasn't slowed down), but there seems to be some performance regression as we now perform tons of little memory allocations somewhere. I'll open an issue but this should be merged as it's not related to this PR. ---. CPU:; ```; ───────────────────────────────────────────────────────────────────────────────────────────────────────────; Forcing function benchmarks Time Allocations ; ────────────────────── ───────────────────────; Tot / % measured: 69.0s / 32.8% 14.0GiB / 55.9% . Section ncalls time %tot avg alloc %tot avg; ───────────────────────────────────────────────────────────────────────────────────────────────────────────; 128×128×128 with forcing (params) (CPU, Float64) 10 7.76s 34.2% 776ms 2.68GiB 34.2% 274MiB; 128×128×128 with forcing (consts) (CPU, Float64) 10 7.63s 33.7% 763ms 2.58GiB 32.9% 264MiB; 128×128×128 no forcing (CPU, Float64) 10 7.27s 32.1% 727ms 2.58GiB 32.9% 264MiB; ───────────────────────────────────────────────────────────────────────────────────────────────────────────; ```. GPU:; ```; ───────────────────────────────────────────────────────────────────────────────────────────────────────────; Forcing function benchmarks Time Allocations ; ────────────────────── ───────────────────────; Tot / % measured: 69.6s / 0.21% 9.23GiB / 0.30% . Section ncalls time %tot avg alloc %tot avg; ───────────────────────────────────────────────────────────────────────────────────────────────────────────; 128×128×128 with forcing (params) (GPU, Float64) 10 48.9ms 33.7% 4.89ms 9.62MiB 33.5% 0.96MiB; 128×128×128 with forcing (consts) (GPU, Float64) 10 48.1ms 33.2% 4.81ms 9.55MiB 33.3% 0.96MiB; 128×128×128 no forcing (GPU, Float64) 10 47.9ms 33.1% 4.79ms 9.55MiB 33.3% 0.96MiB; ───────────────────────────────────────────────────────────────────────────────────────────────────────────; ```",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/395#issuecomment-530378319:0,Perform,Performance,0,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/395#issuecomment-530378319,3,"['Perform', 'perform']","['Performance', 'perform', 'performance']"
Performance,Performance is fine now so merging in. Thanks @glwagner!,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/85#issuecomment-468063989:0,Perform,Performance,0,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/85#issuecomment-468063989,1,['Perform'],['Performance']
Performance,Performance of Preconditioned Conjugate Gradient solver,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2728:0,Perform,Performance,0,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2728,1,['Perform'],['Performance']
Performance,Performance testing,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3492:0,Perform,Performance,0,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3492,1,['Perform'],['Performance']
Performance,Performant elementwise CPU/GPU kernels,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/54:0,Perform,Performant,0,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/54,1,['Perform'],['Performant']
Performance,"Perhaps we could convert to this: https://github.com/DTolm/VkFFT which supports hardware-accelerated FFT on CUDA, Metal and lots of others. It looks like that library is more performant than `cuFFT` as well.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3288#issuecomment-1734214155:175,perform,performant,175,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3288#issuecomment-1734214155,1,['perform'],['performant']
Performance,"Periodic, Periodic, Bounded, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecisi...; 763 0 @Oceananigans/src/Advection/weno_fifth_order.jl 211 left_biased_interpolate_xᶠᵃᵃ(::Int64, ::Int64, ::Int64, ::RegularRectilinearGrid{Float64, Periodic, Periodic, Bounded, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecisi...; 768 0 @Oceananigans/src/Advection/topologically_conditional_interpolation.jl 36 _right_biased_interpolate_xᶠᵃᵃ(::Int64, ::Int64, ::Int64, ::RegularRectilinearGrid{Float64, Periodic, Periodic, Bounded, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePreci...; 768 0 @Oceananigans/src/Advection/upwind_biased_advective_fluxes.jl 49 overdub; 781 0 @Oceananigans/src/Simulations/run.jl 127 run!(sim::Simulation{NonhydrostaticModel{Oceananigans.TimeSteppers.QuasiAdamsBashforth2TimeStepper{Float64, NamedTuple{(:u, :v, :w, :b), Tuple{Field{Face, Center, Center, CPU, OffsetArrays.Offs...; 781 0 @Base/boot.jl 360 eval; 781 0 @Base/loading.jl 1116 include_string(mapexpr::typeof(identity), mod::Module, code::String, filename::String); 781 0 @Base/loading.jl 1170 _include(mapexpr::Function, mod::Module, _path::String); 781 0 @Base/Base.jl 386 include(mod::Module, _path::String); 781 0 @Base/client.jl 285 exec_options(opts::Base.JLOptions); 781 0 @Base/client.jl 485 _start(); 796 796 @Cassette/src/context.jl ? overdub; 821 0 @Oceananigans/src/Advection/topologically_conditional_interpolation.jl 36 _right_biased_interpolate_yᵃᶠᵃ(::Int64, ::Int64, ::Int64, ::RegularRectilinearGrid{Float64, Periodic, Periodic, Bounded, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePreci...; 821 0 @Oceananigans/src/Advection/upwind_biased_advective_fluxes.jl 85 overdub; 860 860 @KernelAbstractions/src/compiler/contract.jl 18 sub_float_contract; 860 0 @KernelAbstractions/src/compiler.jl 46 overdub; 873 0 @Oceananigans/src/Advection/weno_fifth_order.jl 148 overdub; 879 0 @Oceananigans/src/Operators/difference_",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1912#issuecomment-892297846:31141,load,loading,31141,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1912#issuecomment-892297846,1,['load'],['loading']
Performance,"Possibly, we should start using a merge queue that would disallow PRs from being merged unless tests pass.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3413#issuecomment-1867019151:40,queue,queue,40,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3413#issuecomment-1867019151,1,['queue'],['queue']
Performance,Potential performance improvement for upwind schemes,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/987:10,perform,performance,10,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/987,1,['perform'],['performance']
Performance,Potentially a race condition?,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2809:14,race condition,race condition,14,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2809,1,['race condition'],['race condition']
Performance,"Preferences are loaded by the `climacommon` module, which has seen a new release over the past few months. This release was to move to Julia 1.11 and there was no change with respect to the preferences. . In general, you shouldn't set any preference when running on the Caltech clusters because everything is set for you by the module system.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3897#issuecomment-2455064613:16,load,loaded,16,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3897#issuecomment-2455064613,1,['load'],['loaded']
Performance,"Pretty sure there's a performance regression on GPUs. 128^3 `Float64` GPU model slowed down by ~2.4x and 256^3 by ~2.9x while 64^3 didn't really slow down so maybe it's something that gets worse as the model size grows? Although 64^3 doesn't saturate the GPU. I'll think about this as I review the PR... ---. All benchmarks run on a Supercloud node with a V100 GPU. ### Master; ```; ──────────────────────────────────────────────────────────────────────────────────────; Static ocean benchmarks Time Allocations ; ────────────────────── ───────────────────────; Tot / % measured: 171s / 38.5% 14.9GiB / 0.44% . Section ncalls time %tot avg alloc %tot avg; ──────────────────────────────────────────────────────────────────────────────────────; 256×256×256 (CPU, Float32) 10 29.2s 44.4% 2.92s 733KiB 1.06% 73.3KiB; 256×256×256 (CPU, Float64) 10 28.8s 43.9% 2.88s 733KiB 1.06% 73.3KiB; 128×128×128 (CPU, Float32) 10 3.33s 5.07% 333ms 733KiB 1.06% 73.3KiB; 128×128×128 (CPU, Float64) 10 2.87s 4.36% 287ms 733KiB 1.06% 73.3KiB; 64× 64× 64 (CPU, Float32) 10 378ms 0.58% 37.8ms 733KiB 1.06% 73.3KiB; 64× 64× 64 (CPU, Float64) 10 314ms 0.48% 31.4ms 733KiB 1.06% 73.3KiB; 256×256×256 (GPU, Float32) 10 278ms 0.42% 27.8ms 7.71MiB 11.4% 789KiB; 256×256×256 (GPU, Float64) 10 270ms 0.41% 27.0ms 7.73MiB 11.4% 791KiB; 128×128×128 (GPU, Float64) 10 45.4ms 0.07% 4.54ms 7.73MiB 11.4% 791KiB; 128×128×128 (GPU, Float32) 10 43.1ms 0.07% 4.31ms 7.71MiB 11.4% 789KiB; 32× 32× 32 (CPU, Float32) 10 41.0ms 0.06% 4.10ms 733KiB 1.06% 73.3KiB; 32× 32× 32 (CPU, Float64) 10 41.0ms 0.06% 4.10ms 733KiB 1.06% 73.3KiB; 64× 64× 64 (GPU, Float64) 10 36.2ms 0.06% 3.62ms 7.84MiB 11.6% 803KiB; 64× 64× 64 (GPU, Float32) 10 36.2ms 0.06% 3.62ms 7.82MiB 11.6% 801KiB; 32× 32× 32 (GPU, Float32) 10 33.2ms 0.05% 3.32ms 7.70MiB 11.4% 789KiB; 32× 32× 32 (GPU, Float64) 10 32.2ms 0.05% 3.22ms 7.72MiB 11.4% 791KiB; ──────────────────────────────────────────────────────────────────────────────────────; ```. ### This PR; ```; ──────────────",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/452#issuecomment-541323374:22,perform,performance,22,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/452#issuecomment-541323374,1,['perform'],['performance']
Performance,Prevents race condition from multiple builds attempting to use/delete the same julia install,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3662:9,race condition,race condition,9,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3662,1,['race condition'],['race condition']
Performance,Prior of these PR constants (like Earth's radius) were multiply defined. Now all constants are gathered in a module loaded just after units. Closes #2981,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3045:116,load,loaded,116,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3045,1,['load'],['loaded']
Performance,"Probably as a result of PR #3847, on the `main` branch precompiation fails due to some method overwriting. This may be harmless as all tests passed anyways, e.g. see the log from the last test of PR #3847: https://buildkite.com/clima/oceananigans/builds/18103#0192adf8-82f6-48e6-b72e-131502fdfcfc/26-746. But maybe there are some performance implications?. Output when running `using Oceananigans`:; ```; julia> using Oceananigans; Precompiling Oceananigans; Info Given Oceananigans was explicitly requested, output will be shown live ; WARNING: Method definition _advective_tracer_flux_x(Any, Any, Any, Oceananigans.ImmersedBoundaries.ImmersedBoundaryGrid{FT, TX, TY, TZ, G, I, M, S, Arch} where Arch where S where M where I where G where TZ where TY where TX where FT, Oceananigans.Advection.FluxFormAdvection{N, FT, A, B, C} where C where B where A where FT where N, Any...) in module Advection at /home/alir/atdepth/Oceananigans.jl/src/Advection/immersed_advective_fluxes.jl:79 overwritten at /home/alir/atdepth/Oceananigans.jl/src/Advection/tracer_advection_operators.jl:11.; ERROR: Method overwriting is not permitted during Module precompilation. Use `__precompile__(false)` to opt-out of precompilation.; ? Oceananigans; [ Info: Precompiling Oceananigans [9e8cae18-63c1-5223-a75c-80ca9d6e9a09]; WARNING: Method definition _advective_tracer_flux_x(Any, Any, Any, Oceananigans.ImmersedBoundaries.ImmersedBoundaryGrid{FT, TX, TY, TZ, G, I, M, S, Arch} where Arch where S where M where I where G where TZ where TY where TX where FT, Oceananigans.Advection.FluxFormAdvection{N, FT, A, B, C} where C where B where A where FT where N, Any...) in module Advection at /home/alir/atdepth/Oceananigans.jl/src/Advection/immersed_advective_fluxes.jl:79 overwritten at /home/alir/atdepth/Oceananigans.jl/src/Advection/tracer_advection_operators.jl:11.; ERROR: Method overwriting is not permitted during Module precompilation. Use `__precompile__(false)` to opt-out of precompilation.; [ Info: Skipping preco",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3851:330,perform,performance,330,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3851,1,['perform'],['performance']
Performance,Probably race condition with apply regionally then. Cc @simone-silvestri,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2786#issuecomment-1298402412:9,race condition,race condition,9,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2786#issuecomment-1298402412,1,['race condition'],['race condition']
Performance,"Probably the easiest thing to do is to fork `LambertW.jl` and remove that warning. The rest seems ok, though a max iterations of 1000 seems a bit high if you want performance. It depends what you want, but as a hack you can return a NaN upon non-convergence rather than throwing a warning.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3438#issuecomment-1904993244:163,perform,performance,163,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3438#issuecomment-1904993244,1,['perform'],['performance']
Performance,"Profiling is a very good idea. It probably makes sense to use an integrated / application profiler (rather than simply timing functions), because WENO5 is itself composed of many small functions and we don't know which one is the bottleneck. I have never tried profiling on the GPU, but there's some info here: https://juliagpu.gitlab.io/CUDA.jl/development/profiling/. Specifically I think we need to install NSight: https://juliagpu.gitlab.io/CUDA.jl/development/profiling/#NVIDIA-Nsight-Systems",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1764#issuecomment-868829956:230,bottleneck,bottleneck,230,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1764#issuecomment-868829956,1,['bottleneck'],['bottleneck']
Performance,"Protoyping how we might dynamically append arbitrary forcing functions. This could provide a really powerful and concise API for configuring the model. Adding a sponge layer can be done in ~5 lines. I highly doubt this will be performant as is, especially as I'm appending closures with an arbitrary number of arguments. I'm not even sure if it's even possible to make this run fast and work on a GPU... We might have to compile something every time a new forcing function is created to move it away from `Main` and make it inline-able?. Initially the forcing is zero so Julia sees `Oceananigans.zero_func` which it presumably knows how to optimize away. But after adding an `add_ones(args...) = 1.0` forcing function it sees `(::getfield(Main, Symbol(""#new_Fu#3"")){typeof(add_ones),typeof(Oceananigans.zero_func)})` which lives in `Main`...",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/294:227,perform,performant,227,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/294,2,"['optimiz', 'perform']","['optimize', 'performant']"
Performance,"Quickly benchmarked stepping the simulation and as setup in `test/test_biogeochemistry_npzd.jl` (just realized this is incorrectly named) got:; ```; BenchmarkTools.Trial: 3359 samples with 1 evaluation.; Range (min … max): 1.162 ms … 11.420 ms ┊ GC (min … max): 0.00% … 62.52%; Time (median): 1.324 ms ┊ GC (median): 0.00%; Time (mean ± σ): 1.479 ms ± 842.222 μs ┊ GC (mean ± σ): 5.25% ± 7.80%. ▇█▆▄▃▂▂▁ ▁; ████████▇▆▆▅▄▁▃▁▁▃▃▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▄▆▆ █; 1.16 ms Histogram: log(frequency) by time 7.82 ms <. Memory estimate: 1.52 MiB, allocs estimate: 2522. ```; And in `test/test_biogeochemistry_tracer_based.jl`got:; ```; BenchmarkTools.Trial: 978 samples with 1 evaluation.; Range (min … max): 2.071 ms … 77.292 ms ┊ GC (min … max): 0.00% … 0.00%; Time (median): 3.359 ms ┊ GC (median): 0.00%; Time (mean ± σ): 5.091 ms ± 4.405 ms ┊ GC (mean ± σ): 5.49% ± 10.75%. ▄▂▃▃█▃ ; ███████▄▃▂▂▂▂▁▁▁▂▂▂▂▂▂▂▁▁▁▂▂▃▄▅▄▅▃▃▃▃▃▃▂▂▂▂▂▁▂▂▂▁▁▁▁▁▁▁▁▁▂ ▃; 2.07 ms Histogram: frequency by time 18.3 ms <. Memory estimate: 3.84 MiB, allocs estimate: 6731.; ```; So there seems to be a lot of room for optimization in how I've written `TracerBasedBiogeochemistry`. The difference when I just benchmarked `time_step!` was slightly less but still ∼2x slower. (Time to increment stop_iteration by 1 and run! after doing that for an initial step)",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2802#issuecomment-1311928286:1096,optimiz,optimization,1096,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2802#issuecomment-1311928286,1,['optimiz'],['optimization']
Performance,Race condition leading to NaNs with KernelAbstractions.jl on v0.31.0?,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/816:0,Race condition,Race condition,0,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/816,1,['Race condition'],['Race condition']
Performance,"Ran some multithreading benchmarks on Tartarus and Satori but the results weren't really different from previous benchmarks. Perhaps it's still good to merge in case this PR improves performance on other machines?. # Multithreading on Tartarus. ```; Oceananigans v0.44.1; Julia Version 1.5.2; Commit 539f3ce943 (2020-09-23 23:17 UTC); Platform Info:; OS: Linux (x86_64-pc-linux-gnu); CPU: Intel(R) Xeon(R) Silver 4214 CPU @ 2.20GHz; WORD_SIZE: 64; LIBM: libopenlibm; LLVM: libLLVM-9.0.1 (ORCJIT, cascadelake); GPU: TITAN V; ```. ```; Multithreading benchmarks; ┌──────┬─────────┬──────────┬──────────┬──────────┬──────────┬────────────┬──────────┐; │ size │ threads │ min │ median │ mean │ max │ memory │ allocs │; ├──────┼─────────┼──────────┼──────────┼──────────┼──────────┼────────────┼──────────┤; │ 512 │ 1 │ 38.207 s │ 38.207 s │ 38.207 s │ 38.207 s │ 294.28 KiB │ 1930 │; │ 512 │ 2 │ 31.129 s │ 31.129 s │ 31.129 s │ 31.129 s │ 158.41 MiB │ 10341843 │; │ 512 │ 4 │ 13.182 s │ 13.182 s │ 13.182 s │ 13.182 s │ 59.70 MiB │ 3877803 │; │ 512 │ 8 │ 7.637 s │ 7.637 s │ 7.637 s │ 7.637 s │ 32.84 MiB │ 2109253 │; │ 512 │ 16 │ 4.633 s │ 4.680 s │ 4.680 s │ 4.728 s │ 17.21 MiB │ 1062196 │; │ 512 │ 32 │ 3.950 s │ 3.955 s │ 3.955 s │ 3.960 s │ 9.64 MiB │ 517538 │; │ 512 │ 48 │ 3.908 s │ 4.012 s │ 4.012 s │ 4.115 s │ 10.23 MiB │ 472979 │; └──────┴─────────┴──────────┴──────────┴──────────┴──────────┴────────────┴──────────┘; ```. ```; Multithreading speedup; ┌──────┬─────────┬─────────┬─────────┬─────────┐; │ size │ threads │ speedup │ memory │ allocs │; ├──────┼─────────┼─────────┼─────────┼─────────┤; │ 512 │ 1 │ 1.0 │ 1.0 │ 1.0 │; │ 512 │ 2 │ 1.2274 │ 551.2 │ 5358.47 │; │ 512 │ 4 │ 2.8984 │ 207.742 │ 2009.22 │; │ 512 │ 8 │ 5.00278 │ 114.286 │ 1092.88 │; │ 512 │ 16 │ 8.1637 │ 59.8949 │ 550.361 │; │ 512 │ 32 │ 9.65944 │ 33.5302 │ 268.154 │; │ 512 │ 48 │ 9.52408 │ 35.6085 │ 245.067 │; └──────┴─────────┴─────────┴─────────┴─────────┘; ```. # Multithreading on Satori. ```; Oceananigans v0.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1120#issuecomment-732516353:183,perform,performance,183,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1120#issuecomment-732516353,1,['perform'],['performance']
Performance,"Re-posting from #3026... that PR solved performance problems with `NonhydrostaticModel`, but `HydrostaticFreeSurfaceModel` is still 2x slower roughly than when using KA 0.7.2. Here's a simple benchmark:. ```julia; using Oceananigans; using BenchmarkTools. grid = RectilinearGrid(CPU(), size=(128, 128, 1), x=(0, 2π), y=(0, 2π), z=(0, 1), topology=(Periodic, Periodic, Bounded)); model = HydrostaticFreeSurfaceModel(; grid, momentum_advection=WENO(), tracer_advection=WENO()); ϵ(x, y, z) = 2rand() - 1; set!(model, u=ϵ, v=ϵ). function lots_of_steps!(model, Δt, steps=100); for _ = 1:steps; time_step!(model, Δt); end; end. @btime lots_of_steps!(model, 0.01); ```. Results. ```; 10.220 s (85845109 allocations: 37.94 GiB) # main; 6.284 s (66184308 allocations: 16.31 GiB) # main with KA downgraded to 0.7.2; ```. cc @simone-silvestri",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2996#issuecomment-1482198037:40,perform,performance,40,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2996#issuecomment-1482198037,1,['perform'],['performance']
Performance,"Recently, I treid running this example with the change to the environment and now get an error. See below. This is more of a problem as no mp4 file is created. . @ali-ramadhan any suggestions on how to fix this? . One option is not using `Plots.jl`, but is that what people might want to consider?. ```; ERROR: LoadError: failed process: Process(`/home/fpoulin/.julia/artifacts/7f40eeb66d90d3026ae5fb68761c263b57adb840/bin/ffmpeg -v 16 -framerate 15 -i /tmp/jl_CtisJw/%06d.png -vf format=yuv420p -loop 0 -y /scratch/fpoulin/software/Oceananigans.jl/examples/one_dimensional_diffusion.mp4`, ProcessExited(1)) [1]. Stacktrace:; [1] pipeline_error; @ ./process.jl:525 [inlined]; [2] run(::Cmd; wait::Bool); @ Base ./process.jl:440; [3] run; @ ./process.jl:438 [inlined]; [4] (::FFMPEG.var""#4#6""{Cmd})(command_path::String); @ FFMPEG ~/.julia/packages/FFMPEG/OUpap/src/FFMPEG.jl:112; [5] (::JLLWrappers.var""#2#3""{FFMPEG.var""#4#6""{Cmd}, String})(); @ JLLWrappers ~/.julia/packages/JLLWrappers/bkwIo/src/runtime.jl:49; [6] withenv(::JLLWrappers.var""#2#3""{FFMPEG.var""#4#6""{Cmd}, String}, ::Pair{String, String}, ::Vararg{Pair{String, String}, N} where N); @ Base ./env.jl:161; [7] withenv_executable_wrapper(f::Function, executable_path::String, PATH::String, LIBPATH::String, adjust_PATH::Bool, adjust_LIBPATH::Bool); @ JLLWrappers ~/.julia/packages/JLLWrappers/bkwIo/src/runtime.jl:48; [8] #invokelatest#2; @ ./essentials.jl:708 [inlined]; [9] invokelatest; @ ./essentials.jl:706 [inlined]; [10] #ffmpeg#7; @ ~/.julia/packages/JLLWrappers/bkwIo/src/products/executable_generators.jl:7 [inlined]; [11] ffmpeg; @ ~/.julia/packages/JLLWrappers/bkwIo/src/products/executable_generators.jl:7 [inlined]; [12] #exe#2; @ ~/.julia/packages/FFMPEG/OUpap/src/FFMPEG.jl:111 [inlined]; [13] ffmpeg_exe; @ ~/.julia/packages/FFMPEG/OUpap/src/FFMPEG.jl:123 [inlined]; [14] buildanimation(anim::Animation, fn::String, is_animated_gif::Bool; fps::Int64, loop::Int64, variable_palette::Bool, verbose::Bool, show_msg::Bool); @",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1657#issuecomment-866922873:311,Load,LoadError,311,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1657#issuecomment-866922873,1,['Load'],['LoadError']
Performance,"RectilinearGrid{Float64, Bounded, Bounded, Flat, Float64, Float64, Float64, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}, Nothing}, NamedTuple{(:time, :iteration, :stage), Tuple{Float64, Int64, Int64}}, NamedTuple{(:u, :v, :w), Tuple{OffsetArrays.OffsetArray{Float64, 3, CUDA.CuDeviceArray{Float64, 3, 1}}, OffsetArrays.OffsetArray{Float64, 3, CUDA.CuDeviceArray{Float64, 3, 1}}, OffsetArrays.OffsetArray{Float64, 3, CUDA.CuDeviceArray{Float64, 3, 1}}}}}}}}); @ GPUCompiler C:\Users\parfe\.julia\packages\GPUCompiler\1FdJy\src\driver.jl:74; [9] cufunction_compile(job::GPUCompiler.CompilerJob); @ CUDA C:\Users\parfe\.julia\packages\CUDA\Uurn4\src\compiler\execution.jl:324; [10] cached_compilation(cache::Dict{UInt64, Any}, job::GPUCompiler.CompilerJob, compiler::typeof(CUDA.cufunction_compile), linker::typeof(CUDA.cufunction_link)); @ GPUCompiler C:\Users\parfe\.julia\packages\GPUCompiler\1FdJy\src\cache.jl:90; [11] cufunction(f::typeof(Oceananigans.BoundaryConditions.gpu__fill_south_and_north_halo!), tt::Type{Tuple{KernelAbstractions.CompilerMetadata{KernelAbstractions.NDIteration.StaticSize{(1024, 1)}, KernelAbstractions.NDIteration.DynamicCheck, Nothing, Nothing, KernelAbstractions.NDIteration.NDRange{2, KernelAbstractions.NDIteration.StaticSize{(64, 1)}, KernelAbstractions.NDIteration.StaticSize{(16, 16)}, Nothing, Nothing}}, Tuple{OffsetArrays.OffsetArray{Float64, 3, CUDA.CuDeviceArray{Float64, 3, 1}}, OffsetArrays.OffsetArray{Float64, 3, CUDA.CuDeviceArray{Float64, 3, 1}}, OffsetArrays.OffsetArray{Float64, 3, CUDA.CuDeviceArray{Float64, 3, 1}}}, Tuple{BoundaryCondition{Oceananigans.BoundaryConditions.Value, Float64}, BoundaryCondition{Oceananigans.BoundaryConditions.Open, Nothin",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2530:10033,cache,cache,10033,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2530,1,['cache'],['cache']
Performance,"Reductions on FieldTimeSeries are performed individually for each element by constructing two Fields and reducing one into another. Probably, the construction of the individual field is what is causing the loss in performance?; We do not necessarily need to do that, we can just wrap the data in a `ConditionalOperation`.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3750#issuecomment-2322204413:34,perform,performed,34,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3750#issuecomment-2322204413,2,['perform'],"['performance', 'performed']"
Performance,Refactor `Simulations` + simulations infrastructure to be loaded first,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3088:58,load,loaded,58,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3088,1,['load'],['loaded']
Performance,Regression test on the latitude longitude grid ; - Hydrostatic free surface model; - only tested the explicit free surface ; - both periodic and bounded latitude topology tested. Comments; - at the moment the data is generated on the fly at tests/test_regression.jl on the CPU() and tested on the GPU(); - explore the option to load the data in OceananigansArtifacts.jl,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2031:328,load,load,328,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2031,1,['load'],['load']
Performance,Related to #2179. If we don't then we can't load on CPU.,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2240:44,load,load,44,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2240,1,['load'],['load']
Performance,"Related to https://github.com/CliMA/Oceananigans.jl/issues/1465. When fixing the MWE I gave in https://github.com/CliMA/Oceananigans.jl/issues/1465 but outputting things with a NetCDFWriter I also get an error:. ```julia; ERROR: LoadError: type VerticallyStretchedRectilinearGrid has no field xC; Stacktrace:; [1] getproperty(::VerticallyStretchedRectilinearGrid{Float64,Periodic,Periodic,Bounded,OffsetArrays.OffsetArray{Float64,1,StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}},OffsetArrays.OffsetArray{Float64,1,Array{Float64,1}}}, ::Symbol) at ./Base.jl:33; [2] default_dimensions(::Dict{String,Field{X,Y,Z,OffsetArrays.OffsetArray{Float64,3,Array{Float64,3}},VerticallyStretchedRectilinearGrid{Float64,Periodic,Periodic,Bounded,OffsetArrays.OffsetArray{Float64,1,StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}},OffsetArrays.OffsetArray{Float64,1,Array{Float64,1}}},B} where B where Z where Y where X}, ::VerticallyStretchedRectilinearGrid{Float64,Periodic,Periodic,Bounded,OffsetArrays.OffsetArray{Float64,1,StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}},OffsetArrays.OffsetArray{Float64,1,Array{Float64,1}}}, ::FieldSlicer{Colon,Colon,Colon}) at /home/tomas/repos2/Oceananigans.jl/src/OutputWriters/netcdf_output_writer.jl:38; [3] NetCDFOutputWriter(::IncompressibleModel{Oceananigans.TimeSteppers.QuasiAdamsBashforth2TimeStepper{Float64,NamedTuple{(:u, :v, :w, :T, :S),Tuple{Field{Face,Center,Center,OffsetArrays.OffsetArray{Float64,3,Array{Float64,3}},VerticallyStretchedRectilinearGrid{Float64,Periodic,Periodic,Bounded,OffsetArrays.OffsetArray{Float64,1,StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}},OffsetArrays.OffsetArray{Float64,1,Array{Float64,1}}},NamedTuple{(:x, :y, :z),Tuple{CoordinateBoundaryConditions{BoundaryCondition{Oceananigans.BoundaryConditions.Periodic,Nothing},BoundaryCondition{Oceananigans.BoundaryConditions.Periodic,Nothing}},Coordi",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1466:229,Load,LoadError,229,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1466,1,['Load'],['LoadError']
Performance,"Release notes:. **Everyone should stop using Oceananigans v0.31.0 and upgrade to this version. Oceananigans now requires Julia 1.4 or later.**. This release fixes a major bug concerning a race condition making GPU simulation, especially large models, explode into NaNs in v0.31.0. It also restores the ""Examples"" section in the documentation and adds experimental support for higher order advection schemes.",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/821:188,race condition,race condition,188,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/821,1,['race condition'],['race condition']
Performance,Remove performance-/precompilation-time harmful `@eval`,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3556:7,perform,performance,7,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3556,1,['perform'],['performance']
Performance,Resolves #1113 . Needs new multi-threading benchmarks. Will do as part of #1088 (need to add nice multithreading benchmark script on `ar/benchmarks` branch).,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1120:27,multi-thread,multi-threading,27,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1120,1,['multi-thread'],['multi-threading']
Performance,Reusing the fill_halo optimization done in glw/performance,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2033:22,optimiz,optimization,22,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2033,2,"['optimiz', 'perform']","['optimization', 'performance']"
Performance,"Revert ""Fixes checkpointer GPU to CPU loading and writing fields with function boundary conditions""",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/807:38,load,loading,38,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/807,1,['load'],['loading']
Performance,"Rewrote the time stepping algorithm to perform operations element-wise which allows the MITgcm algorithm to fully utilize a GPU (for large enough problems). And the same code runs on the CPU. Essentially we have a massive triple for loop iterating over all the grid points and functions that e.g. calculate u-momentum advection at grid point (i,j,k). The code is ugly and unorganized right now but I just wanted to get something working. Now we can work on cleaning up the code and optimizing performance. You can see for yourselves how bad the native type element-wise operators are but the time stepping loop should clean up nicely. It's split up into 4-5 kernels as the GPU needs to synchronize between certain steps. I've had to make some compromises writing it for the GPU, mainly by stripping away all the abstractions as the GPU will only deal with native data types. But we can figure this stuff out, it can only get better from here. This time stepping will essentially replace what's already in place (and turn the code into a CPU/GPU capable package) so might as well merge and start working on integrating it (and getting some sort sort of GPU CI working). Resolves #49",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/54:39,perform,perform,39,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/54,3,"['optimiz', 'perform']","['optimizing', 'perform', 'performance']"
Performance,"Right now I'm just constructing grids with dimensions that are multiples of 16 so; ```julia; Tx, Ty = 16, 16 # Threads per block; Bx, By, Bz = Int(Nx/Tx), Int(Ny/Ty), Nz # Blocks in grid.; ```. @vchuravy suggested laying them out to fill out the _x_-direction first, then the _y_-direction, then the _z_-direction. That would also let us use grids with arbitrary sizes, i.e. not just multiples of 16. Not sure if it would affect performance for large problems. See https://github.com/vchuravy/GPUifyLoops.jl/pull/18#issuecomment-465150581. Update: Yeah I should have done this a long time ago. This issue is preventing us from running small grids (e.g. 1D column models) on the GPU and running certain tests.",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/64:429,perform,performance,429,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/64,1,['perform'],['performance']
Performance,"Right now only 1 benchmark job is run to check for performance regression, which is important seeing as sometimes we introduce rogue bugs that kill performance. cc @simonbyrne will this work with the Slurm CI/CliMA bot framework you've set up? Will be awesome to start using it. No MPI stuff yet but will probably add some soon.",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/280:51,perform,performance,51,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/280,2,['perform'],['performance']
Performance,Right now we are wasting memory and time computing free surface tendencies when `free_surface isa ImplicitFreeSurface` with `HydrostaticFreeSurfaceModel`. A minor optimization would neither allocate memory for the free surface tendencies or calculate them in this case.,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1941:163,optimiz,optimization,163,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1941,1,['optimiz'],['optimization']
Performance,"Right now we load cubed sphere grids from a file which is a Julia/JLD2.jl version of the binary files used by MITgcm. These grid files are currently stored at https://github.com/CliMA/OceananigansArtifacts.jl. The easiest way to use them is to load them as a data dependency with DataDeps.jl: https://github.com/CliMA/Oceananigans.jl/blob/master/test/data_dependencies.jl. This works well for the CS32 and CS96 grids, but for much larger grids like the CS510 the grid file is ~200 MiB uncompressed (~118 MiB compressed with JLD2's `compress=true`) which is bigger than GitHub's 100 MiB file size limit. 200 MiB for a grid file is also a bit cumbersome. Right now I'm thinking of hosting CS510 on the engaging cluster, although git lfs for OceananigansArtifacts.jl may be an option. It would be nice to be able to generate conformal cubed sphere grid files to make it easier for users to use cubed sphere grids, and also for the added flexibility of not being limited to three common resolutions (CS32, CS96, and CS510). It would also be good to keep the ability to load a cubed sphere grid from file since we may want to do this for other grids besides the cubed sphere in the future (lat-lon-cap or LLC grids?), and it would be useful to test that the grids we generate are indeed correct by comparing with the grid files. I'm opening this issue just to document what we know about conformal cubed sphere grid generation. It's not a particularly urgent issue. # Computing grid metrics. We already have some code that generates conformal cubed sphere grids with the coordinates, but they are missing the grid metrics (grid spacings and areas). @christophernhill has pointed out these MITgcm MATLAB scripts that may just be what we need to compute the grid metrics. http://wwwcvs.mitgcm.org/viewvc/MITgcm/MITgcm_contrib/high_res_cube/matlab-grid-generator/README?revision=1.1.1.1&view=markup; http://wwwcvs.mitgcm.org/viewvc/MITgcm/MITgcm_contrib/high_res_cube/matlab-grid-generator/calc_fvgrid.m?revis",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1586:13,load,load,13,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1586,2,['load'],['load']
Performance,"Right! The CPU -> GPU speed up has limited value, because the CPU is not a good reference / is not optimized. More useful would be a comparison with other models, somehow, but this is always challenging, especially for complicated models. Another useful piece of information would compare performance for difference GPUs. This blog post (which advertises the advantages of liquid cooled GPUs) is relevant and includes some Oceananigans benchmarks:. https://www.markiiisys.com/blog/liquid-cooled-workstation-advantageous-for-big-ocean-simulations-exceptional-for-small-tasks-oregon-state-mark-iii-nvidia-supermicro/",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3684#issuecomment-2272422585:99,optimiz,optimized,99,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3684#issuecomment-2272422585,2,"['optimiz', 'perform']","['optimized', 'performance']"
Performance,Running the example from #2389 it returned this error; `ERROR: LoadError: UndefVarError: div_Uc not defined`. Hope this suggestion makes sense.,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2390:63,Load,LoadError,63,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2390,1,['Load'],['LoadError']
Performance,"Running with `--check-bounds=yes` on the CPU provides a strong hint:. ```; at index [39914881, -59303136, 54]; ```. Yeah that'll do it lol. ---. ```; [ Info: Iteration 1...; [ Info: Iteration 2...; ERROR: LoadError: BoundsError: attempt to access 109×208×68 OffsetArray(::Array{Float64, 3}, -3:105, -3:204, -3:64) with eltype Float64 with indices -3:105×-3:204×-3:64 at index [39914881, -59303136, 54]; Stacktrace:; [1] throw_boundserror(A::OffsetArrays.OffsetArray{Float64, 3, Array{Float64, 3}}, I::Tuple{Int64, Int64, Int64}); @ Base ./abstractarray.jl:737; [2] checkbounds; @ ./abstractarray.jl:702 [inlined]; [3] getindex; @ ~/.julia/packages/OffsetArrays/hwmnB/src/OffsetArrays.jl:422 [inlined]; [4] getindex; @ ~/atdepth/Oceananigans.jl/src/Fields/field.jl:401 [inlined]; [5] _interpolate; @ ~/atdepth/Oceananigans.jl/src/Fields/interpolate.jl:295 [inlined]; [6] interpolate; @ ~/atdepth/Oceananigans.jl/src/Fields/interpolate.jl:245 [inlined]; [7] advect_particle; @ ~/atdepth/Oceananigans.jl/src/Models/LagrangianParticleTracking/lagrangian_particle_advection.jl:113 [inlined]; [8] macro expansion; @ ~/atdepth/Oceananigans.jl/src/Models/LagrangianParticleTracking/lagrangian_particle_advection.jl:177 [inlined]; [9] cpu__advect_particles!; @ ~/.julia/packages/KernelAbstractions/491pi/src/macros.jl:291 [inlined]; [10] cpu__advect_particles!(__ctx__::KernelAbstractions.CompilerMetadata{…}, particles::StructArrays.StructVector{…}, restitution::Float64, grid::LatitudeLongitudeGrid{…}, Δt::Float64, velocities::@NamedTuple{…}); @ Oceananigans.Models.LagrangianParticleTracking ./none:0; [11] __thread_run(tid::Int64, len::Int64, rem::Int64, obj::KernelAbstractions.Kernel{…}, ndrange::Nothing, iterspace::KernelAbstractions.NDIteration.NDRange{…}, args::Tuple{…}, dynamic::KernelAbstractions.NDIteration.NoDynamicCheck); @ KernelAbstractions ~/.julia/packages/KernelAbstractions/491pi/src/cpu.jl:144; [12] __run(obj::KernelAbstractions.Kernel{…}, ndrange::Nothing, iterspace::KernelAbstracti",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3852#issuecomment-2427969795:205,Load,LoadError,205,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3852#issuecomment-2427969795,1,['Load'],['LoadError']
Performance,"RvJ/ext/TaylorSeriesIAExt.jl:182; [2] include; @ ./Base.jl:495 [inlined]; [3] include_package_for_output(pkg::Base.PkgId, input::String, depot_path::Vector{String}, dl_load_path::Vector{String}, load_path::Vector{String}, concrete_deps::Vector{Pair{Base.PkgId, UInt128}}, source::Nothing); @ Base ./loading.jl:2222; [4] top-level scope; @ stdin:3; in expression starting at /Users/navid/.julia/packages/TaylorSeries/2qRvJ/ext/TaylorSeriesIAExt.jl:1; in expression starting at stdin:3; ┌ Error: Error during loading of extension TaylorSeriesIAExt of TaylorSeries, use `Base.retry_load_extensions()` to retry.; │ exception =; │ 1-element ExceptionStack:; │ Failed to precompile TaylorSeriesIAExt [ed7ef945-33a4-511e-97fe-2b89c7a130ca] to ""/Users/navid/.julia/compiled/v1.10/TaylorSeriesIAExt/jl_TNauRw"".; │ Stacktrace:; │ [1] error(s::String); │ @ Base ./error.jl:35; │ [2] compilecache(pkg::Base.PkgId, path::String, internal_stderr::IO, internal_stdout::IO, keep_loaded_modules::Bool); │ @ Base ./loading.jl:2468; │ [3] compilecache; │ @ ./loading.jl:2340 [inlined]; │ [4] (::Base.var""#968#969""{Base.PkgId})(); │ @ Base ./loading.jl:1974; │ [5] mkpidlock(f::Base.var""#968#969""{Base.PkgId}, at::String, pid::Int32; kwopts::@Kwargs{stale_age::Int64, wait::Bool}); │ @ FileWatching.Pidfile ~/julia-1.10/usr/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:93; │ [6] #mkpidlock#6; │ @ ~/julia-1.10/usr/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:88 [inlined]; │ [7] trymkpidlock(::Function, ::Vararg{Any}; kwargs::@Kwargs{stale_age::Int64}); │ @ FileWatching.Pidfile ~/julia-1.10/usr/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:111; │ [8] #invokelatest#2; │ @ ./essentials.jl:894 [inlined]; │ [9] invokelatest; │ @ ./essentials.jl:889 [inlined]; │ [10] maybe_cachefile_lock(f::Base.var""#968#969""{Base.PkgId}, pkg::Base.PkgId, srcpath::String; stale_age::Int64); │ @ Base ./loading.jl:2983; │ [11] maybe_cachefile_lock; │ @ ./loading.jl:2980 [inlined]; │ [12] _require(pkg::Base.PkgId",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3686#issuecomment-2272432528:1292,load,loading,1292,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3686#issuecomment-2272432528,1,['load'],['loading']
Performance,"S23_IS7_S24_IS7_ES24_IS7_ES4_EES22_IS7_Li1ES23_IS7_S24_IS7_ES24_IS7_ES4_EEvES25_IS22_IS7_Li2ES9_IS7_Li2ELi1EEES26_EvvE11NotImmersedI8truefuncES4_S7_E' uses too much parameter space (0x11f0 bytes, 0x1100 max).; ptxas fatal : Ptx assembly aborted due to errors; If you think this is a bug, please file an issue and attach /glade/scratch/tomasc/jl_uD0VONe1Cz.ptx; Stacktrace:; [1] error(s::String); @ Base ./error.jl:35; [2] compile(job::GPUCompiler.CompilerJob, ctx::LLVM.Context); @ CUDA /glade/work/tomasc/.julia/packages/CUDA/pCcGc/src/compiler/compilation.jl:208; [3] #1032; @ /glade/work/tomasc/.julia/packages/CUDA/pCcGc/src/compiler/compilation.jl:120 [inlined]; [4] JuliaContext(f::CUDA.var""#1032#1033""{GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget, CUDA.CUDACompilerParams}}); @ GPUCompiler /glade/work/tomasc/.julia/packages/GPUCompiler/NVLGB/src/driver.jl:37; [5] compile; @ /glade/work/tomasc/.julia/packages/CUDA/pCcGc/src/compiler/compilation.jl:119 [inlined]; [6] actual_compilation(cache::Dict{Any, Any}, src::Core.MethodInstance, world::UInt64, cfg::GPUCompiler.CompilerConfig{GPUCompiler.PTXCompilerTarget, CUDA.CUDACompilerParams}, compiler::typeof(CUDA.compile), linker::typeof(CUDA.link)); @ GPUCompiler /glade/work/tomasc/.julia/packages/GPUCompiler/NVLGB/src/execution.jl:125; [7] cached_compilation(cache::Dict{Any, Any}, src::Core.MethodInstance, cfg::GPUCompiler.CompilerConfig{GPUCompiler.PTXCompilerTarget, CUDA.CUDACompilerParams}, compiler::Function, linker::Function); @ GPUCompiler /glade/work/tomasc/.julia/packages/GPUCompiler/NVLGB/src/execution.jl:103; [8] macro expansion; @ /glade/work/tomasc/.julia/packages/CUDA/pCcGc/src/compiler/execution.jl:318 [inlined]; [9] macro expansion; @ ./lock.jl:223 [inlined]; [10] cufunction(f::typeof(CUDA.partial_mapreduce_grid), tt::Type{Tuple{typeof(identity), typeof(Base.add_sum), Nothing, CartesianIndices{3, Tuple{Base.OneTo{Int64}, Base.OneTo{Int64}, Base.OneTo{Int64}}}, CartesianIndices{3, Tuple{Base.OneTo{Int64",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3140:5085,cache,cache,5085,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3140,1,['cache'],['cache']
Performance,"Same tests performed on an **Immersed** grid with a random bathymetry (i.e. roughly 50% of the cells are immersed); on main:. ```julia ; julia> using NESAPOceananigans; julia> set_problem_size!(500, 500, 50). julia> trial1 = run_model_benchmark!(momentum_kernel_test, GPU();; use_benchmarktools = true,; bathymetry = random_bathymetry()); BenchmarkTools.Trial: 5 samples with 1 evaluation.; Range (min … max): 20.150 ms … 21.143 ms ┊ GC (min … max): 0.00% … 0.00%; Time (median): 20.174 ms ┊ GC (median): 0.00%; Time (mean ± σ): 20.384 ms ± 428.856 μs ┊ GC (mean ± σ): 0.00% ± 0.00%. █▁ ▁ ▁; ██▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█ ▁; 20.1 ms Histogram: frequency by time 21.1 ms <. Memory estimate: 482.09 KiB, allocs estimate: 407. julia> trial1 = run_model_benchmark!(tracer_kernel_test, GPU();; use_benchmarktools = true,; bathymetry = random_bathymetry()); BenchmarkTools.Trial: 5 samples with 1 evaluation.; Range (min … max): 9.614 ms … 10.272 ms ┊ GC (min … max): 0.00% … 0.00%; Time (median): 9.717 ms ┊ GC (median): 0.00%; Time (mean ± σ): 9.797 ms ± 271.120 μs ┊ GC (mean ± σ): 0.00% ± 0.00%. █ █ █ █ █; █▁█▁▁▁▁▁▁█▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█ ▁; 9.61 ms Histogram: frequency by time 10.3 ms <. Memory estimate: 89.45 KiB, allocs estimate: 320. ```. on this branch:; ```julia ; julia> using NESAPOceananigans; julia> set_problem_size!(500, 500, 50). julia> trial1 = run_model_benchmark!(momentum_kernel_test, GPU();; use_benchmarktools = true,; bathymetry = random_bathymetry()); trial1 = run_model_benchmark!(momentum_kernel_test, arch; use_benchmarktools = true, bottom_height = random_bathymetry()) = Trial(13.991 ms); BenchmarkTools.Trial: 5 samples with 1 evaluation.; Range (min … max): 13.991 ms … 22.167 ms ┊ GC (min … max): 0.00% … 0.00%; Time (median): 14.057 ms ┊ GC (median): 0.00%; Time (mean ± σ): 15.676 ms ± 3.629 ms ┊ GC (mean ± σ): 0.00% ± 0.00%. █; █▇▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▇ ▁; 14 ms Histogram: f",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3658#issuecomment-2243530643:11,perform,performed,11,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3658#issuecomment-2243530643,1,['perform'],['performed']
Performance,"See also. https://docs.julialang.org/en/v1/manual/variables-and-scoping/. to understand how scoping works, and how using the `const` keyword can help produce performant code when working with global variables. The last example at that link is:. ```julia; julia> const x = 1; 1. julia> f() = x; f (generic function with 1 method). julia> f(); 1. julia> x = 2; WARNING: redefining constant x; 2. julia> f(); 1; ```. which is illustrative.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/110#issuecomment-470672237:158,perform,performant,158,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/110#issuecomment-470672237,1,['perform'],['performant']
Performance,See: https://discourse.julialang.org/t/cuarrays-cudanative-psa-simplified-package-loading/27897,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/376:82,load,loading,82,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/376,1,['load'],['loading']
Performance,"Seems like the Distributed Poisson solver would work only when `grid.Nx == grid.Ny` and `ranks[1] == 1`. Should we maybe implement a transpose to allow different grid sizes in Nx and Ny and 2D parallelization in case of vertically stretched domains?. On the other hand, this might hinder the performance quite a bit...",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2074:292,perform,performance,292,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2074,1,['perform'],['performance']
Performance,Seems to work:. ```; [2021/06/25 18:04:55.066] INFO Writing Advection_schemes_relative_performance_(CPU).html...; Advection schemes relative performance (GPU); ┌───────────────┬────────────────────────┬──────────┬─────────┬─────────┐; │ Architectures │ Schemes │ slowdown │ memory │ allocs │; ├───────────────┼────────────────────────┼──────────┼─────────┼─────────┤; │ GPU │ CenteredFourthOrder │ 1.36629 │ 1.07711 │ 1.66944 │; │ GPU │ CenteredSecondOrder │ 1.0 │ 1.0 │ 1.0 │; │ GPU │ UpwindBiasedFifthOrder │ 1.53522 │ 1.11266 │ 1.9781 │; │ GPU │ UpwindBiasedThirdOrder │ 1.31322 │ 1.03505 │ 1.30432 │; │ GPU │ WENO5 │ 1.84272 │ 1.1889 │ 2.64008 │; ```,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1770#issuecomment-868903511:141,perform,performance,141,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1770#issuecomment-868903511,1,['perform'],['performance']
Performance,Set up Slurm pipeline for performance regression testing.,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/280:26,perform,performance,26,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/280,1,['perform'],['performance']
Performance,"Settled on the code below. ```julia; struct SpatialWindowAverage{F, S, D} ; field :: F; field_slicer :: S; dims :: D; end. SpatialWindowAverage(field; dims, field_slicer=FieldSlicer()) = SpatialWindowAverage(field, field_slicer, dims). function (wsa::SpatialWindowAverage)(model); compute!(wsa.field); window = view(data(wsa.field), wsa.field_slicer.i, wsa.field_slicer.j, wsa.field_slicer.k); return mean(window, dims=wsa.dims); end; ```. Which constructs an object fine, but it doesn't work when outputting. I can call `compute!` on it, but really it doesn't do anything. When including it to a `NetCDFWriter` I get this error:. <details>; <summary>Click to expand!</summary>. ```; ERROR: LoadError: Custom output Us needs dimensions!; Stacktrace:; [1] error(::String) at ./error.jl:33; [2] define_output_variable!(::NCDatasets.NCDataset{Nothing}, ::SpatialWindowAverage{Field{Face,Center,Center,OffsetArrays.OffsetArray{Float64,3,Array{Float64,3}},RegularCartesianGrid{Float64,Periodic,Periodic,Periodic,OffsetArrays.OffsetArray{Float64,1,StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}}},NamedTuple{(:x, :y, :z),Tuple{CoordinateBoundaryConditions{BoundaryCondition{Oceananigans.BoundaryConditions.Periodic,Nothing},BoundaryCondition{Oceananigans.BoundaryConditions.Periodic,Nothing}},CoordinateBoundaryConditions{BoundaryCondition{Oceananigans.BoundaryConditions.Periodic,Nothing},BoundaryCondition{Oceananigans.BoundaryConditions.Periodic,Nothing}},CoordinateBoundaryConditions{BoundaryCondition{Oceananigans.BoundaryConditions.Periodic,Nothing},BoundaryCondition{Oceananigans.BoundaryConditions.Periodic,Nothing}}}}},FieldSlicer{Colon,UnitRange{Int64},Colon},Tuple{Int64}}, ::String, ::Type{T} where T, ::Int64, ::Tuple{}, ::Dict{Any,Any}) at /home/tomas/repos/Oceananigans.jl/src/OutputWriters/netcdf_output_writer.jl:357; [3] NetCDFOutputWriter(::IncompressibleModel{Oceananigans.TimeSteppers.RungeKutta3TimeStepper{Float64,NamedTuple{(:u, :v, :w, :b),Tuple{Fie",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1392#issuecomment-783533170:691,Load,LoadError,691,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1392#issuecomment-783533170,1,['Load'],['LoadError']
Performance,"Should examples be written so that the ""part 2"" (load output and plot/process) is independent of ""part 1"" (setup and run)?",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1850:49,load,load,49,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1850,1,['load'],['load']
Performance,"Should run these benchmarks again once everything works on the GPU. Notes:; 1. Lat-lon grid seems slower than the single cubed sphere face, which is weird. Maybe the cost of computing the grid metrics on the fly is actually adding up to a significant overhead?; 2. Cubed sphere grid performs better than it should (less than 6x slower than 1 face), but allocates a lot of memory.; 3. Explicit vs. implicit free surface solver performance is problem-dependent so for the purposes of this benchmarks every implicit solver is forced to take 1 iteration. ```; Hydrostatic model benchmarks; ┌───────────────┬──────────────────────────────┬─────────────────────┬───────────┬───────────┬───────────┬───────────┬────────────┬─────────┬─────────┐; │ Architectures │ grid_types │ free_surface_types │ min │ median │ mean │ max │ memory │ allocs │ samples │; ├───────────────┼──────────────────────────────┼─────────────────────┼───────────┼───────────┼───────────┼───────────┼────────────┼─────────┼─────────┤; │ CPU │ RegularRectilinearGrid │ ExplicitFreeSurface │ 3.127 ms │ 3.632 ms │ 3.665 ms │ 4.225 ms │ 263.23 KiB │ 1726 │ 10 │; │ CPU │ RegularLatitudeLongitudeGrid │ ExplicitFreeSurface │ 9.765 ms │ 10.370 ms │ 10.428 ms │ 11.847 ms │ 290.50 KiB │ 1984 │ 10 │; │ CPU │ ConformalCubedSphereFaceGrid │ ExplicitFreeSurface │ 5.986 ms │ 9.676 ms │ 10.276 ms │ 16.990 ms │ 151.66 KiB │ 1994 │ 10 │; │ CPU │ ConformalCubedSphereGrid │ ExplicitFreeSurface │ 24.817 ms │ 28.235 ms │ 30.393 ms │ 45.743 ms │ 2.12 MiB │ 41751 │ 10 │; ├───────────────┼──────────────────────────────┼─────────────────────┼───────────┼───────────┼───────────┼───────────┼────────────┼─────────┼─────────┤; │ CPU │ RegularRectilinearGrid │ ImplicitFreeSurface │ 6.418 ms │ 6.925 ms │ 7.147 ms │ 9.625 ms │ 578.41 KiB │ 3545 │ 10 │; │ CPU │ RegularLatitudeLongitudeGrid │ ImplicitFreeSurface │ 15.913 ms │ 16.438 ms │ 17.028 ms │ 20.042 ms │ 656.92 KiB │ 4306 │ 10 │; │ CPU │ ConformalCubedSphereFaceGrid │ ImplicitFreeSurface │ 9.89",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1604:283,perform,performs,283,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1604,2,['perform'],"['performance', 'performs']"
Performance,Should we also expect the MG preconditioner to perform similarly to the FFT-based preconditioner? Those are relatively similar algorithms. How many CG iterations are we performing for either?,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2728#issuecomment-1238512157:47,perform,perform,47,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2728#issuecomment-1238512157,2,['perform'],"['perform', 'performing']"
Performance,"Should we also try things like. ```julia; node(i, j, k, grid, ::Nothing, ℓy, ℓz) = _node(i, j, k, grid, nothing, ℓy, ℓz)[1:2]; ```. I think we determined there could be a tiny performance loss but it would make the code a little simpler and also easier to read since we don't have to define every combination of locations for `_node`. I still want to clean up and streamline `xspacings` and `xnodes` (etc) so that they use `xnode` directly (rather than ""re-implementing"" the nodes) but that'll have to wait for another PR I think.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3355#issuecomment-1775504750:176,perform,performance,176,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3355#issuecomment-1775504750,1,['perform'],['performance']
Performance,"Since there's no performance benefit, I'm closing this. We can revisit `heuristic_workgroup` in the future.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1902#issuecomment-958088865:17,perform,performance,17,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1902#issuecomment-958088865,1,['perform'],['performance']
Performance,"Since we need the performance provided by KA 0.7, and we need to use KA 0.8+ on GPU, does that mean that we should invest in developing our own CPU infrastructure (replicating what KA 0.7 offered) to achieve that performance?. Another possibility is that we re-write much of the code base to avoid the performance pitfalls we are currently facing in order to get back to the level of performance we have with current code + KA 0.7. I believe the issue is basically an interaction between some of the abstractions / indirection we have developed and the compiler, so possibly rolling back that abstraction / indirection will bring us back to where we were previously.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2996#issuecomment-1481707882:18,perform,performance,18,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2996#issuecomment-1481707882,4,['perform'],['performance']
Performance,"So I just ran the performance benchmarks again (different CPU but same GPU) and the model has gotten slower since the last time we ran benchmarks (PR #116), by about 30~40%. Details pasted below but for the larger GPU models: 256^3 Float32 it went up from 85.9 ms to 119 ms, and for 128^3 Float32 it went up from 7.70 ms to 10.1 ms. These are wall clock time per model iteration. To be fair, we had no boundary conditions API last time we benchmarked. I won't worry about this for now though as we haven't begun to optimize for performance. ```; ──────────────────────────────────────────────────────────────────────────────────────────────────; Oceananigans.jl benchmarks Time Allocations ; ────────────────────── ───────────────────────; Tot / % measured: 734s / 72.9% 18.7GiB / 0.06% . Section ncalls time %tot avg alloc %tot avg; ──────────────────────────────────────────────────────────────────────────────────────────────────; 256x256x256 static ocean (CPU, Float32) 10 312s 58.4% 31.2s 60.2KiB 0.47% 6.02KiB; 256x256x256 static ocean (CPU, Float64) 10 196s 36.7% 19.6s 78.0KiB 0.62% 7.80KiB; 128x128x128 static ocean (CPU, Float32) 10 14.2s 2.66% 1.42s 60.2KiB 0.47% 6.02KiB; 128x128x128 static ocean (CPU, Float64) 10 7.64s 1.43% 764ms 78.0KiB 0.62% 7.80KiB; 256x256x256 static ocean (GPU, Float64) 10 1.36s 0.25% 136ms 1.60MiB 12.9% 164KiB; 256x256x256 static ocean (GPU, Float32) 10 1.19s 0.22% 119ms 1.36MiB 11.0% 139KiB; 64x 64x 64 static ocean (CPU, Float32) 10 950ms 0.18% 95.0ms 60.2KiB 0.47% 6.02KiB; 64x 64x 64 static ocean (CPU, Float64) 10 584ms 0.11% 58.4ms 78.0KiB 0.62% 7.80KiB; 128x128x128 static ocean (GPU, Float64) 10 116ms 0.02% 11.6ms 1.60MiB 12.9% 164KiB; 128x128x128 static ocean (GPU, Float32) 10 101ms 0.02% 10.1ms 1.36MiB 11.0% 139KiB; 32x 32x 32 static ocean (CPU, Float32) 10 94.6ms 0.02% 9.46ms 60.2KiB 0.47% 6.02KiB; 32x 32x 32 static ocean (CPU, Float64) 10 53.9ms 0.01% 5.39ms 78.0KiB 0.62% 7.80KiB; 64x 64x 64 static ocean (GPU, Float64) 10 11.6ms 0.00% 1.16ms",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/147#issuecomment-479474578:18,perform,performance,18,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/147#issuecomment-479474578,3,"['optimiz', 'perform']","['optimize', 'performance']"
Performance,"So I reduced my code to be just; ```; using Oceananigans; ```; and the code gave the following error; ```; [20610] signal (11.1): Segmentation fault; in expression starting at /glade/u/home/knudsenl/.julia/packages/CUDA_Runtime_jll/YgJCI/.pkg/platform_augmentation.jl:283; Allocations: 2907 (Pool: 2898; Big: 9); GC: 0; ERROR: LoadError: Failed to precompile CUDA_Runtime_jll [76a88914-d11a-5bdc-97e0-2f5a05c973a2] to ""/glade/u/home/knudsenl/.julia/compiled/v1.10/CUDA_Runtime_jll/jl_FIXMhf"".; Stacktrace:; [1] error(s::String); @ Base ./error.jl:35; [2] compilecache(pkg::Base.PkgId, path::String, internal_stderr::IO, internal_stdout::IO, keep_loaded_modules::Bool); @ Base ./loading.jl:2468; [3] compilecache; @ ./loading.jl:2340 [inlined]; [4] (::Base.var""#968#969""{Base.PkgId})(); @ Base ./loading.jl:1974; [5] mkpidlock(f::Base.var""#968#969""{Base.PkgId}, at::String, pid::Int32; kwopts::@Kwargs{stale_age::Int64, wait::Bool}); @ FileWatching.Pidfile /glade/u/apps/casper/23.10/spack/opt/spack/julia/1.10.2/gcc/7.5.0/apod/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:93; [6] #mkpidlock#6 ; @ /glade/u/apps/casper/23.10/spack/opt/spack/julia/1.10.2/gcc/7.5.0/apod/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:88 [inlined]; [7] trymkpidlock(::Function, ::Vararg{Any}; kwargs::@Kwargs{stale_age::Int64}); @ FileWatching.Pidfile /glade/u/apps/casper/23.10/spack/opt/spack/julia/1.10.2/gcc/7.5.0/apod/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:111; [8] #invokelatest#2; @ ./essentials.jl:894 [inlined]; [9] invokelatest; @ ./essentials.jl:889 [inlined]; [10] maybe_cachefile_lock(f::Base.var""#968#969""{Base.PkgId}, pkg::Base.PkgId, srcpath::String; stale_age::Int64); @ Base ./loading.jl:2983; [11] maybe_cachefile_lock; @ ./loading.jl:2980 [inlined]; [12] _require(pkg::Base.PkgId, env::String); @ Base ./loading.jl:1970; [13] __require_prelocked(uuidkey::Base.PkgId, env::String); @ Base ./loading.jl:1812; [14] #invoke_in_world#3; @ ./essentials.jl:926 [inlined]; [15] inv",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2231635812:327,Load,LoadError,327,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2231635812,4,"['Load', 'load']","['LoadError', 'loading']"
Performance,"So in this I've got a load of `update_tendencies!` being called, and adding `synchronize(device(architecture(model)))` at the end appears to have fixed this. To summarise: ; - `CUDA error: an illegal memory access was encountered (code 700, ERROR_ILLEGAL_ADDRESS)` error; - Resolved by manually synchronizing the device with `synchronize(device(architecture(model)))`",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3267#issuecomment-1721361808:22,load,load,22,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3267#issuecomment-1721361808,1,['load'],['load']
Performance,So there is a performance issue with the FFT? How does `PencilFFTs` scale when run stand-alone?,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1948#issuecomment-902786645:14,perform,performance,14,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1948#issuecomment-902786645,1,['perform'],['performance']
Performance,"So, I'm hitting some errors running the `baroclinic_adjustment` example with the ROCBackend. The only modification of this example script is the specification of the architecture as `GPU()`. It seems that this chokes on AMDGPU's `zeros`. I did not encounter this in the other work I was doing on Simone's branch; though the test we were using was not building with a recilinear grid. ```; $ julia --project=. ./bench/baroclinic_adjustment.jl ; ERROR: LoadError: Not implemented; Stacktrace:; [1] error(s::String); @ Base ./error.jl:35; [2] runtime_module(job::GPUCompiler.CompilerJob); @ GPUCompiler ~/.julia/packages/GPUCompiler/U36Ed/src/interface.jl:173; [3] build_runtime(job::GPUCompiler.CompilerJob); @ GPUCompiler ~/.julia/packages/GPUCompiler/U36Ed/src/rtlib.jl:101; [4] (::GPUCompiler.var""#136#138""{GPUCompiler.CompilerJob{GPUCompiler.GCNCompilerTarget, AMDGPU.Compiler.HIPCompilerParams}})(); @ GPUCompiler ~/.julia/packages/GPUCompiler/U36Ed/src/rtlib.jl:139; [5] lock(f::GPUCompiler.var""#136#138""{GPUCompiler.CompilerJob{GPUCompiler.GCNCompilerTarget, AMDGPU.Compiler.HIPCompilerParams}}, l::ReentrantLock); @ Base ./lock.jl:229; [6] macro expansion; @ GPUCompiler ~/.julia/packages/GPUCompiler/U36Ed/src/rtlib.jl:120 [inlined]; [7] load_runtime(job::GPUCompiler.CompilerJob); @ GPUCompiler ~/.julia/packages/GPUCompiler/U36Ed/src/utils.jl:92; [8] macro expansion; @ ~/.julia/packages/GPUCompiler/U36Ed/src/driver.jl:290 [inlined]; [9] emit_llvm(job::GPUCompiler.CompilerJob; libraries::Bool, toplevel::Bool, optimize::Bool, cleanup::Bool, only_entry::Bool, validate::Bool); @ GPUCompiler ~/.julia/packages/GPUCompiler/U36Ed/src/utils.jl:92; [10] emit_llvm; @ ~/.julia/packages/GPUCompiler/U36Ed/src/utils.jl:86 [inlined]; [11] codegen(output::Symbol, job::GPUCompiler.CompilerJob; libraries::Bool, toplevel::Bool, optimize::Bool, cleanup::Bool, strip::Bool, validate::Bool, only_entry::Bool, parent_job::Nothing); @ GPUCompiler ~/.julia/packages/GPUCompiler/U36Ed/src/driver.jl:129; [12] ",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3468#issuecomment-1935971273:451,Load,LoadError,451,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3468#issuecomment-1935971273,1,['Load'],['LoadError']
Performance,Some CATKE performance optimizations,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3453:11,perform,performance,11,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3453,2,"['optimiz', 'perform']","['optimizations', 'performance']"
Performance,"Some of the lessons over on https://github.com/CliMA/Oceananigans.jl/issues/2024 could warrant using an object after all. The reason is a bit technical... but because of how the compiler works, we have. ```julia; julia> using Oceananigans.AbstractOperations: Δx. julia> using Oceananigans. julia> grid = RegularRectilinearGrid(size=(2, 2, 2), extent=(1, 1, 1));. julia> u = XFaceField(grid);. julia> udx1 = u / Δx; BinaryOperation at (Face, Center, Center); ├── grid: RegularRectilinearGrid{Float64, Periodic, Periodic, Bounded}(Nx=2, Ny=2, Nz=2); │ └── domain: x ∈ [0.0, 1.0], y ∈ [0.0, 1.0], z ∈ [-1.0, 0.0]; └── tree: ; / at (Face, Center, Center);    ├── Field located at (Face, Center, Center);    └── Δxᶠᶜᵃ at (Face, Center, Center). julia> udx2 = u / Δx; BinaryOperation at (Face, Center, Center); ├── grid: RegularRectilinearGrid{Float64, Periodic, Periodic, Bounded}(Nx=2, Ny=2, Nz=2); │ └── domain: x ∈ [0.0, 1.0], y ∈ [0.0, 1.0], z ∈ [-1.0, 0.0]; └── tree: ; / at (Face, Center, Center);    ├── Field located at (Face, Center, Center);    └── Δxᶠᶜᵃ at (Face, Center, Center). julia> udx1 === udx2; false; ```. The reason we get different types is because of the different ""identity"" functions introduced in. https://github.com/CliMA/Oceananigans.jl/blob/6dcf6ffd1fdb2febaf0d20dfab85ef1d3f83d758/src/Operators/interpolation_utils.jl#L29-L66. which we do to avoid the compiler from detecting recursion during compilation. A few things to digest here but basically we want to cache the operations `u / Δx`, etc because reforming them for every calculation may incur high compilation costs.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2037#issuecomment-958994106:1484,cache,cache,1484,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2037#issuecomment-958994106,1,['cache'],['cache']
Performance,"Some pretty promising Lagrangian particle tracking benchmarks!. Couple of takeaways (all assuming a model with 128^3 grid points and QAB2 time stepping):; 1. **Low overhead**: You can advect up to ~100,000 particles on the CPU and up to ~10,000,000 particles on a (Titan V) GPU before the model slows down by more than 30%.; 2. **Great on GPUs**: Seems that the GPU is great for advecting millions of particles. You can advect ~100,000,000 particles and your model only slows down by a factor of 4x. In this scenario, the GPU is ~620x faster than a single CPU core.; 3. Calculated using `(t_100000000 - t_0) / 100000000`, advecting a single particle on the CPU takes ~110 ns while on the GPU it only takes ~0.127 ns. This seems a little too good to be true but I'll double check this. I'll start refactoring this PR using @glwagner's and @zhenwu0728's feedback, but I think it would be really great if we can keep this performance. # Benchmarks. ```; Oceananigans v0.44.1; Julia Version 1.5.2; Commit 539f3ce943 (2020-09-23 23:17 UTC); Platform Info:; OS: Linux (x86_64-pc-linux-gnu); CPU: Intel(R) Xeon(R) Silver 4214 CPU @ 2.20GHz; WORD_SIZE: 64; LIBM: libopenlibm; LLVM: libLLVM-9.0.1 (ORCJIT, cascadelake); GPU: TITAN V; ```. ```; Lagrangian particle tracking benchmarks; ┌───────────────┬─────────────┬────────────┬────────────┬────────────┬────────────┬────────────┬────────┐; │ Architectures │ N_particles │ min │ median │ mean │ max │ memory │ allocs │; ├───────────────┼─────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────┤; │ CPU │ 0 │ 361.749 ms │ 364.041 ms │ 364.293 ms │ 368.854 ms │ 293.44 KiB │ 1876 │; │ CPU │ 1 │ 375.030 ms │ 376.591 ms │ 377.959 ms │ 385.248 ms │ 297.16 KiB │ 1906 │; │ CPU │ 10 │ 377.251 ms │ 380.792 ms │ 387.560 ms │ 443.325 ms │ 297.16 KiB │ 1906 │; │ CPU │ 100 │ 378.867 ms │ 381.194 ms │ 381.328 ms │ 383.461 ms │ 297.16 KiB │ 1906 │; │ CPU │ 1000 │ 378.076 ms │ 384.114 ms │ 383.611 ms │ 388.507 ms │ 297.16 KiB │ 1906 │; │",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1091#issuecomment-732529975:919,perform,performance,919,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1091#issuecomment-732529975,1,['perform'],['performance']
Performance,"Some recent improvements in `CUDA.jl`:. https://juliagpu.org/post/2022-01-28-cuda_3.5_3.8/#preserving_array_indices. attempt to avoid promoting index types from 32-bit to 64-bit integers (64-bit integers are Julia's default). Throughout `Oceananigans` we use 64-bit integers by using the constant `1` for index calculations:. https://github.com/CliMA/Oceananigans.jl/blob/9f6d841393094b123c99287fab7157a65db4d513/src/Operators/difference_operators.jl#L7. According to that blog post, changing the above line. ```julia; @inline δxᶜᵃᵃ(i, j, k, grid, u) = @inbounds u[i+0x1, j, k] - u[i, j, k] ; ```. may decrease register pressure (perhaps dramatically, especially for wide stencils with lots of indices). `0x1` is the integer 1 with type `UInt8`:. ```julia; julia> i = 0x1; 0x01. julia> typeof(i); UInt8. julia> i == 1; true. julia> i === 1; false; ```. The amount of code that generates our stencils is not all that large. We find stencil calculations in. * `Operators`; * `Advection`; * `ImmersedBoundaries`. I also found a few stray index manipulations in random places like. https://github.com/CliMA/Oceananigans.jl/blob/9f6d841393094b123c99287fab7157a65db4d513/src/Models/HydrostaticFreeSurfaceModels/pcg_implicit_free_surface_solver.jl#L211. I think this issue also implies that we should strive to minimize the number of places where we do index calculations, to minimize the chances that we accidentally convert to `Int64`. We might also want to do some profiling to see the specific effect moving to `UInt8` indices might have. Perhaps we should benchmark two fully-loaded models, a `NonhydrostaticModel` LES with high-order advection, and a global-configuration of `HydrostaticFreeSurfaceModel` with an immersed boundary / continents, etc.",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2202:1574,load,loaded,1574,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2202,1,['load'],['loaded']
Performance,"Some thoughts:. The first case (`N=100`) doesn't look great but it shows that the MPData schemes do not differ very much whether we have 2 or 5 iteractions, and they are somewhere between 1st and 3rd order upwinding. The second case (`N=200`) looks better and I can't really see any difference between 2 and 5 iterations. The third case (`N=400`) has all the schemes performing well, even first order upwinding. Again we can't differentiate between the two MPData schemes and it's even hard to differentiate between 3rd and 5th order upwinding. I lesson that I learned from here is to use as much spatial resolution as we can as this does much more compared to the number of iterations. . @simone-silvestri : do you think we should do a similar comparison for 2D advection?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3434#issuecomment-1954430883:367,perform,performing,367,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3434#issuecomment-1954430883,1,['perform'],['performing']
Performance,"Something I don't understand about the vertical integrals: for performance, would we not want to launch `Nx` by `Ny` blocks, each with `Nz` threads? That way an entire column ends up in the shared memory of a block. Maybe I am missing something.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/249#issuecomment-496563988:63,perform,performance,63,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/249#issuecomment-496563988,1,['perform'],['performance']
Performance,"Something I don't understand about the vertical integrals: for performance, would we not want to launch `Nx` by `Ny` blocks, each with `Nz` threads? That way an entire column ends up in the shared memory of a block. Maybe I am missing something. Similarly, it would seem that for 3D computations we want to launch a 3D array of threads within each block to maximize the usage of within-block shared memory as we loop over stencils. To minimize memory access across blocks, we need the 3D thread block to be as large and as isotropic as possible. Again, maybe I am missing something... Might be to discuss with @vchuravy, since this is his bread-and-butter, as they say.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/249#issuecomment-496565439:63,perform,performance,63,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/249#issuecomment-496565439,1,['perform'],['performance']
Performance,"Something related with the boundary conditions of the loaded saved `b` perhaps?. E.g., . ```julia; julia> b_timeseries[10]; Field located at (Center, Center, Center); ├── data: OffsetArrays.OffsetArray{Float64, 3, Array{Float64, 3}}, size: (128, 1, 64); ├── grid: RegularRectilinearGrid{Float64, Bounded, Flat, Bounded}(Nx=128, Ny=1, Nz=64); └── boundary conditions: JLD2.ReconstructedTypes.var""##NamedTuple{(:x, :y, :z),Tuple}#261""(CoordinateBoundaryConditions{BoundaryCondition{Oceananigans.BoundaryConditions.Flux, Nothing}, BoundaryCondition{Oceananigans.BoundaryConditions.Flux, Nothing}}(BoundaryCondition: type=ZeroFlux, condition=nothing, BoundaryCondition: type=ZeroFlux, condition=nothing), CoordinateBoundaryConditions{Nothing, Nothing}(nothing, nothing), JLD2.ReconstructedTypes.var""##CoordinateBoundaryConditions{BoundaryCondition{Oceananigans.BoundaryConditions.Flux, Nothing},BoundaryCondition{Oceananigans.BoundaryConditions.Value,Oceananigans.BoundaryConditions.ContinuousBoundaryFunction{Center,Center,Nothing,64,Main.#bₛ,Nothing,Tuple{},Tuple{},Tuple{}}}}#262""()); ```",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1847#issuecomment-878829232:54,load,loaded,54,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1847#issuecomment-878829232,1,['load'],['loaded']
Performance,Sometimes different Buildkite jobs try to download the same file as the same time leading to freezes like in https://buildkite.com/clima/oceananigans/builds/3066#157d0809-a11d-476b-b78d-fc5b1a241286. This PR tries to avoid race conditions by just downloading the file once during the initialization stage. Doesn't always happen though so not sure when/how to merge.,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1804:223,race condition,race conditions,223,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1804,1,['race condition'],['race conditions']
Performance,Sometimes tests fail to load. Merely restarting them does the job. Did that. Let’s see.,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2733#issuecomment-1252866889:24,load,load,24,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2733#issuecomment-1252866889,1,['load'],['load']
Performance,"Somewhat of a small frivolous PR but it improves logging in Oceananigans with some fancy formatting which includes using colors for timestamps and log levels (using the same colors as the base logger) and the source of the message is underlined (see screenshot below). It can be used via the usual macros (`@debug`, `@info`, `@warn`, and `@error`) once Oceananigans is loaded via `using` or `import`. Although this might actually be undesirable as it hijacks the global logger... The logger is now turned on globally in the `Oceananigans.__init__` function. I'd like to start using it for new simulations as it's useful to have timestamps. ![image](https://user-images.githubusercontent.com/20099589/90137955-51510400-dd44-11ea-9bb9-9c407db19b99.png)",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/855:369,load,loaded,369,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/855,1,['load'],['loaded']
Performance,"Somewhere between this commit ; https://buildkite.com/clima/oceananigans-distributed/builds/3113#01917ace-fe81-401d-ba21-467037e6aead; and main, we switched from using `libmpitrampoline.so` in the distributed tests to `libmpi.so` downloaded from the artifacts. . Previously, the mpi trampoline was loading a CUDA-aware implementation of Open MPI, while the libmpi.so we use now is a ; MPICH implementation non CUDA-aware:; https://buildkite.com/clima/oceananigans-distributed/builds/4227#0192f70a-b947-4d38-bd1c-c2497a964de9. This makes our GPU distributed tests fail. ; I am wondering where this switch happened because I couldn't trace any changes to the code. @Sbozzolo, do you know if something changed in the `LocalPreferences.toml` in the Caltech cluster?",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3897:298,load,loading,298,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3897,1,['load'],['loading']
Performance,"Sorry I didn't realize I didn't get the full error message:; ```; [52922] signal (11.1): Segmentation fault; in expression starting at /glade/u/home/knudsenl/.julia/packages/Oceananigans/M82LU/src/Oceananigans.jl:129; Allocations: 618273 (Pool: 617476; Big: 797); GC: 1; ERROR: LoadError: Failed to precompile Oceananigans [9e8cae18-63c1-5223-a75c-80ca9d6e9a09] to ""/glade/u/home/knudsenl/.julia/compiled/v1.9/Oceananigans/jl_AMNEzH"".; Stacktrace:; [1] error(s::String); @ Base ./error.jl:35; [2] compilecache(pkg::Base.PkgId, path::String, internal_stderr::IO, internal_stdout::IO, keep_loaded_modules::Bool); @ Base ./loading.jl:2300; [3] compilecache; @ ./loading.jl:2167 [inlined]; [4] _require(pkg::Base.PkgId, env::String); @ Base ./loading.jl:1805; [5] _require_prelocked(uuidkey::Base.PkgId, env::String); @ Base ./loading.jl:1660; [6] macro expansion; @ ./loading.jl:1648 [inlined]; [7] macro expansion; @ ./lock.jl:267 [inlined]; [8] require(into::Module, mod::Symbol); @ Base ./loading.jl:1611; in expression starting at /glade/derecho/scratch/knudsenl/BottomBoundaryLayer/testcode.jl:1; ```; The versioninfo gave me the following as output:; ```; Julia Version 1.9.2; Commit e4ee485e90 (2023-07-05 09:39 UTC); Platform Info:; OS: Linux (x86_64-suse-linux); ""openSUSE Leap 15.4""; uname: Linux 5.14.21-150400.24.46-default #1 SMP PREEMPT_DYNAMIC Thu Feb 9 08:38:18 UTC 2023 (2d95137) x86_64 x86_64; CPU: Intel(R) Xeon(R) Gold 6240 CPU @ 2.60GHz: ; speed user nice sys idle irq; #1-72 2600 MHz 922553194 s 727867 s 363212152 s 5414020342 s 0 s; Memory: 370.290340423584 GB (301775.56640625 MB free); Uptime: 9.37897786e6 sec; Load Avg: 13.61 25.96 27.94; WORD_SIZE: 64; LIBM: libopenlibm; LLVM: libLLVM-14.0.6 (ORCJIT, cascadelake); Threads: 1 on 72 virtual cores; Environment:; LD_LIBRARY_PATH = /glade/u/apps/casper/23.10/spack/opt/spack/openmpi/4.1.6/oneapi/2023.2.1/dgcv/lib:/glade/u/apps/common/23.08/spack/opt/spack/cuda/12.2.1/lib64:/glade/u/apps/common/23.08/spack/opt/spack/cuda/12.2",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2233740720:278,Load,LoadError,278,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2233740720,7,"['Load', 'load']","['LoadError', 'loading']"
Performance,"Sorry about the confusion @tomchor! Opening a new issue might actually be a good idea since it's true we currently only support a subset of the 3^3 = 27 possible topologies (I went ahead and quoted your post in this new issue). I actually might avoid using the `Flat` topology right now since until it's fully tested (see e.g. #1023). You can substitute `Bounded` for `Flat` and your simulation should do the same thing except it will take up some extra memory and slow down your model by a tiny bit. The functional difference is that halo regions have non-zero size along `Bounded` dimensions (and derivatives are computed along `Bounded` dimensions which will evaluate to zero unless you have some weird boundary conditions). Yeah currently the four supported topologies are:; 1. `(Periodic, Periodic, Periodic)`; 2. `(Periodic, Periodic, Bounded)`; 3. `(Periodic, Bounded, Bounded)`; 4. `(Bounded, Bounded, Bounded)` (unfortunately only works on the CPU, see #1007). We could support all possible topology combinations. The only bottleneck is implementing pressure solver that work for all topologies (see #586). This is on my current list of things to do but is not super trivial (see https://github.com/JuliaGPU/CUDA.jl/issues/119 and possibly related #1170) so it's taking some time... 😅 . Hopefully I can refactor the pressure solver(s) to support all topologies soon but either way it might be good to be explicit about which topologies are actually supported.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1192#issuecomment-732260751:1032,bottleneck,bottleneck,1032,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1192#issuecomment-732260751,1,['bottleneck'],['bottleneck']
Performance,"Sorry but sometimes i must wait a long time to get gpu resources :; I created a TestOcean project with Oceananigans as dependency. I used Julia v1.4; and it works perfectly, thank you very much!. ~~~; module load julia/1.4.0 cuda; cd TestOcean; env JULIA_CUDA_USE_BINARYBUILDER=false julia --project; julia> using Oceananigans; [ Info: Precompiling Oceananigans [9e8cae18-63c1-5223-a75c-80ca9d6e9a09]; julia> using CUDA; julia> CUDA.has_cuda(); true; julia> using Oceananigans.Architectures: @hascuda; julia> @hascuda ""Hi""; ""Hi""; julia> grid = RegularCartesianGrid(size=(100, 100, 50), extent=(2π, 2π, 1)); RegularCartesianGrid{Float64, Periodic, Periodic, Bounded}; domain: x ∈ [-5.496153587253255e-18, 6.283185307179586], y ∈ [-5.496153587253255e-18, 6.283185307179586], z ∈ [-1.0, 1.7080354225002348e-17]; topology: (Periodic, Periodic, Bounded); resolution (Nx, Ny, Nz): (100, 100, 50); halo size (Hx, Hy, Hz): (1, 1, 1); grid spacing (Δx, Δy, Δz): (0.06283185307179587, 0.06283185307179587, 0.02). julia> model = IncompressibleModel(architecture=GPU(), grid=grid); IncompressibleModel{GPU, Float64}(time = 0.000 s, iteration = 0) ; ├── grid: RegularCartesianGrid{Float64, Periodic, Periodic, Bounded}(Nx=100, Ny=100, Nz=50); ├── tracers: (:T, :S); ├── closure: IsotropicDiffusivity{Float64,NamedTuple{(:T, :S),Tuple{Float64,Float64}}}; ├── buoyancy: SeawaterBuoyancy{Float64,LinearEquationOfState{Float64},Nothing,Nothing}; └── coriolis: Nothing. julia> simulation = Simulation(model, Δt=60, stop_time=3600); Simulation{IncompressibleModel{GPU, Float64}}; ├── Model clock: time = 0.000 s, iteration = 0 ; ├── Next time step (Int64): 1.000 min ; ├── Iteration interval: 1; ├── Stop criteria: Any[Oceananigans.Simulations.iteration_limit_exceeded, Oceananigans.Simulations.stop_time_exceeded, Oceananigans.Simulations.wall_time_limit_exceeded]; ├── Run time: 0.000 s, wall time limit: Inf; ├── Stop time: 1.000 hr, stop iteration: Inf; ├── Diagnostics: OrderedCollections.OrderedDict with no entrie",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1035#issuecomment-707702037:208,load,load,208,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1035#issuecomment-707702037,1,['load'],['load']
Performance,"Sorry that I didn't emphasize this but this is the initial error. ```; [ Info: Populate: populating indices.; ERROR: LoadError: `makedocs` encountered an error. Terminating build; Stacktrace:; [1] error(::String) at ./error.jl:33; [2] runner(::Type{Documenter.Builder.RenderDocument}, ::Documenter.Documents.Document) at /home/fpoulin/.julia/packages/Documenter/lul8Y/src/Builder.jl:255; [3] dispatch(::Type{Documenter.Builder.DocumentPipeline}, ::Documenter.Documents.Document) at /home/fpoulin/.julia/packages/Documenter/lul8Y/src/Utilities/Selectors.jl:170; [4] #2 at /home/fpoulin/.julia/packages/Documenter/lul8Y/src/Documenter.jl:249 [inlined]; [5] cd(::Documenter.var""#2#3""{Documenter.Documents.Document}, ::String) at ./file.jl:104; [6] #makedocs#1 at /home/fpoulin/.julia/packages/Documenter/lul8Y/src/Documenter.jl:248 [inlined]; [7] top-level scope at /home/fpoulin/software/Oceananigans.jl/docs/make.jl:137; [8] include(::Function, ::Module, ::String) at ./Base.jl:380; [9] include(::Module, ::String) at ./Base.jl:368; [10] exec_options(::Base.JLOptions) at ./client.jl:296; [11] _start() at ./client.jl:506; in expression starting at /home/fpoulin/software/Oceananigans.jl/docs/make.jl:137; ```",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1455#issuecomment-796866901:117,Load,LoadError,117,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1455#issuecomment-796866901,1,['Load'],['LoadError']
Performance,"Sorry, you are right. You use set_num_threads. See the constructors in. https://github.com/FourierFlows/FourierFlows.jl/blob/master/src/domains.jl. On Sat, Mar 9, 2019 at 8:55 AM Ali Ramadhan <notifications@github.com>; wrote:. > Weird, I see no mention of threads or multi-threading; >; > help?> plan_fft; >; > search: plan_fft plan_fft! plan_rfft plan_ifft plan_bfft plan_ifft! plan_bfft! plan_irfft plan_brfft; >; >; >; > plan_fft(A [, dims]; flags=FFTW.ESTIMATE, timelimit=Inf); >; >; >; > Pre-plan an optimized FFT along given dimensions (dims) of arrays matching the shape and type of A. (The first two arguments have the same meaning as for fft.) Returns an object P which; >; > represents the linear operator computed by the FFT, and which contains all of the information needed to compute fft(A, dims) quickly.; >; >; >; > To apply P to an array A, use P * A; in general, the syntax for applying plans is much like that of matrices. (A plan can only be applied to arrays of the same size as the A for which; >; > the plan was created.) You can also apply a plan with a preallocated output array Â by calling mul!(Â, plan, A). (For mul!, however, the input array A must be a complex floating-point; >; > array like the output Â.) You can compute the inverse-transform plan by inv(P) and apply the inverse plan with P \ Â (the inverse plan is cached and reused for subsequent calls to inv or; >; > \), and apply the inverse plan to a pre-allocated output array A with ldiv!(A, P, Â).; >; >; >; > The flags argument is a bitwise-or of FFTW planner flags, defaulting to FFTW.ESTIMATE. e.g. passing FFTW.MEASURE or FFTW.PATIENT will instead spend several seconds (or more) benchmarking; >; > different possible FFT algorithms and picking the fastest one; see the FFTW manual for more information on planner flags. The optional timelimit argument specifies a rough upper bound on; >; > the allowed planning time, in seconds. Passing FFTW.MEASURE or FFTW.PATIENT may cause the input array A to be ov",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/119#issuecomment-471179341:268,multi-thread,multi-threading,268,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/119#issuecomment-471179341,2,"['multi-thread', 'optimiz']","['multi-threading', 'optimized']"
Performance,"StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}}},CenteredSecondOrder,Nothing,Nothing,IsotropicDiffusivity{Float64,NamedTuple{(:T, :S),Tuple{Float64,Float64}}},NamedTuple{(:u, :v, :w),Tuple{OffsetArray{Float64,3,CuDeviceArray{Float64,3,CUDA.AS.Global}},OffsetArray{Float64,3,CuDeviceArray{Float64,3,CUDA.AS.Global}},OffsetArray{Float64,3,CuDeviceArray{Float64,3,CUDA.AS.Global}}}},NamedTuple{(:T, :S),Tuple{OffsetArray{Float64,3,CuDeviceArray{Float64,3,CUDA.AS.Global}},OffsetArray{Float64,3,CuDeviceArray{Float64,3,CUDA.AS.Global}}}},Nothing,NamedTuple{(:u, :v, :w, :T, :S),Tuple{var""#Fu#80"",var""#Fv#81"",var""#Fw#82"",typeof(Oceananigans.Forcing.zeroforcing),typeof(Oceananigans.Forcing.zeroforcing)}},OffsetArray{Float64,3,CuDeviceArray{Float64,3,CUDA.AS.Global}},NamedTuple{(:time, :iteration),Tuple{Float64,Int64}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:maxthreads,),Tuple{Int64}}}) at /home/ancellin/.julia/packages/GPUCompiler/4e9CU/src/cache.jl:0; [23] cufunction(::typeof(Cassette.overdub), ::Type{Tuple{Cassette.Context{nametype(CUDACtx),KernelAbstractions.CompilerMetadata{KernelAbstractions.NDIteration.StaticSize{(16, 16, 16)},KernelAbstractions.NDIteration.DynamicCheck,Nothing,Nothing,KernelAbstractions.NDIteration.NDRange{3,KernelAbstractions.NDIteration.StaticSize{(1, 1, 16)},KernelAbstractions.NDIteration.StaticSize{(16, 16, 1)},Nothing,Nothing}},Nothing,KernelAbstractions.var""##PassType#253"",Nothing,Cassette.DisableHooks},typeof(Oceananigans.TimeSteppers.gpu_calculate_Gu!),OffsetArray{Float64,3,CuDeviceArray{Float64,3,CUDA.AS.Global}},RegularCartesianGrid{Float64,Periodic,Periodic,Bounded,OffsetArray{Float64,1,StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}}},CenteredSecondOrder,Nothing,Nothing,IsotropicDiffusivity{Float64,NamedTuple{(:T, :S),Tuple{Float64,Float64}}},NamedTuple{(:u, :v, :w),Tuple{OffsetArray{Float64,3,CuDeviceArray{Float64,3,CUDA.AS.Global}},Offset",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/882:9814,cache,cache,9814,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/882,1,['cache'],['cache']
Performance,"StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}}},CenteredSecondOrder,Nothing,Nothing,IsotropicDiffusivity{Float64,NamedTuple{(:T, :S),Tuple{Float64,Float64}}},NamedTuple{(:u, :v, :w),Tuple{OffsetArray{Float64,3,CuDeviceArray{Float64,3,CUDA.AS.Global}},OffsetArray{Float64,3,CuDeviceArray{Float64,3,CUDA.AS.Global}},OffsetArray{Float64,3,CuDeviceArray{Float64,3,CUDA.AS.Global}}}},NamedTuple{(:T, :S),Tuple{OffsetArray{Float64,3,CuDeviceArray{Float64,3,CUDA.AS.Global}},OffsetArray{Float64,3,CuDeviceArray{Float64,3,CUDA.AS.Global}}}},Nothing,NamedTuple{(:u, :v, :w, :T, :S),Tuple{var""#Fu#80"",var""#Fv#81"",var""#Fw#82"",typeof(Oceananigans.Forcing.zeroforcing),typeof(Oceananigans.Forcing.zeroforcing)}},OffsetArray{Float64,3,CuDeviceArray{Float64,3,CUDA.AS.Global}},NamedTuple{(:time, :iteration),Tuple{Float64,Int64}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:maxthreads,),Tuple{Int64}}}) at /home/ancellin/.julia/packages/GPUCompiler/4e9CU/src/cache.jl:19; [17] + at ./int.jl:53 [inlined]; [18] hash_64_64 at ./hashing.jl:35 [inlined]; [19] hash_uint64 at ./hashing.jl:62 [inlined]; [20] hx at ./float.jl:568 [inlined]; [21] hash at ./float.jl:571 [inlined]; [22] cached_compilation(::typeof(CUDA._cufunction), ::GPUCompiler.FunctionSpec{typeof(Cassette.overdub),Tuple{Cassette.Context{nametype(CUDACtx),KernelAbstractions.CompilerMetadata{KernelAbstractions.NDIteration.StaticSize{(16, 16, 16)},KernelAbstractions.NDIteration.DynamicCheck,Nothing,Nothing,KernelAbstractions.NDIteration.NDRange{3,KernelAbstractions.NDIteration.StaticSize{(1, 1, 16)},KernelAbstractions.NDIteration.StaticSize{(16, 16, 1)},Nothing,Nothing}},Nothing,KernelAbstractions.var""##PassType#253"",Nothing,Cassette.DisableHooks},typeof(Oceananigans.TimeSteppers.gpu_calculate_Gu!),OffsetArray{Float64,3,CuDeviceArray{Float64,3,CUDA.AS.Global}},RegularCartesianGrid{Float64,Periodic,Periodic,Bounded,OffsetArray{Float64,1,StepRangeLen{Float64,Base.TwicePrec",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/882:7848,cache,cache,7848,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/882,1,['cache'],['cache']
Performance,"Still, we shouldn't get race condition; that's a bug.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2809#issuecomment-1308175859:24,race condition,race condition,24,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2809#issuecomment-1308175859,1,['race condition'],['race condition']
Performance,"Suggestion: If the filename loaded is, e.g., a `_part1.jld2` then it would be of interest to detail that and whether this is part1 out of X?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3859#issuecomment-2433313107:28,load,loaded,28,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3859#issuecomment-2433313107,1,['load'],['loaded']
Performance,"Sure!. ```; (Oceananigans) pkg> status; Project Oceananigans v0.53.0; Status `~/Documents/Bachelor Arbeit/Repos/Oceananigans.jl/Project.toml`; [79e6a3ab] Adapt v3.2.0; [052768ef] CUDA v2.4.1; [a8cc5b0e] Crayons v4.0.4; [7a1cc6ca] FFTW v1.3.2; [e9467ef8] GLMakie v0.1.30; [c27321d9] Glob v1.3.0; [033835bb] JLD2 v0.4.3; [63c18a36] KernelAbstractions v0.5.4; [da04e1cc] MPI v0.16.1; [442fdcdd] Measures v0.3.1; [85f8d34a] NCDatasets v0.11.3; [6fe1bfb0] OffsetArrays v1.6.2; [bac558e1] OrderedCollections v1.4.0; [4a48f351] PencilFFTs v0.12.2; [d330b81b] PyPlot v2.9.0; [1bc83da4] SafeTestsets v0.0.1; [d496a93d] SeawaterPolynomials v0.2.0; [09ab397b] StructArrays v0.5.0; [ade2ca70] Dates; [b77e0a4c] InteractiveUtils; [37e2e46d] LinearAlgebra; [56ddb016] Logging; [44cfe95a] Pkg; [de0858da] Printf; [9a3f8284] Random; [10745b16] Statistics; ```. ```; ERROR: LoadError: ArgumentError: length(size) must be 2.; Stacktrace:; [1] validate_tupled_argument(::Tuple{Int64,Int64,Int64}, ::Type{T} where T, ::String, ::Int64; greater_than::Int64) at /home/meck/Documents/Bachelor Arbeit/Repos/Oceananigans.jl/src/Grids/input_validation.jl:24; [2] validate_tupled_argument(::Tuple{Int64,Int64,Int64}, ::Type{T} where T, ::String, ::Int64) at /home/meck/Documents/Bachelor Arbeit/Repos/Oceananigans.jl/src/Grids/input_validation.jl:24; [3] validate_size(::Type{T} where T, ::Type{T} where T, ::Type{T} where T, ::Tuple{Int64,Int64,Int64}) at /home/meck/Documents/Bachelor Arbeit/Repos/Oceananigans.jl/src/Grids/input_validation.jl:48; [4] RegularRectilinearGrid(::Type{T} where T; size::Tuple{Int64,Int64,Int64}, x::Tuple{Int64,Int64}, y::Tuple{Int64,Int64}, z::Tuple{Int64,Int64}, extent::Nothing, topology::Tuple{DataType,DataType,DataType}, halo::Nothing) at /home/meck/Documents/Bachelor Arbeit/Repos/Oceananigans.jl/src/Grids/regular_rectilinear_grid.jl:161; [5] simulate_lid_driven_cavity(; Re::Int64, N::Int64, end_time::Int64) at /home/meck/Documents/Bachelor Arbeit/Repos/Oceananigans.jl/validation/lid_d",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1507#issuecomment-807380928:857,Load,LoadError,857,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1507#issuecomment-807380928,1,['Load'],['LoadError']
Performance,"Sure, the PR resolves the rounding issue caused by `previous_interval_stop_time` through the use of actuations (as inspired by its application [here](https://github.com/CliMA/Oceananigans.jl/blob/dc5dc28c6cf433dcb8a6668cef99e98309e6ead9/src/Utils/schedules.jl#L58-L63)).; Here is the part of the code showing these [changes](https://github.com/CliMA/Oceananigans.jl/pull/3721/commits/a52812b00eb38e712ed20c7a6db3cf2e0c3a7877#diff-532eb4a17264dc44a7cae7601aca768c39bfb08f0493561c333f24a3261d6a46R123-R130). Another important change is that ; ```julia. # Save averaging start time and the initial data collection time; wta.window_start_time = model.clock.time; wta.window_start_iteration = model.clock.iteration; wta.previous_collection_time = model.clock.time. wta.schedule.collecting = false; wta.schedule.actuations += 1; ```. occurs only when the window ends, i.e., when `end_of_window(wta.schedule, model.clock) == true`. In contrast, the [previous version](https://github.com/CliMA/Oceananigans.jl/pull/3721/commits/a52812b00eb38e712ed20c7a6db3cf2e0c3a7877#diff-532eb4a17264dc44a7cae7601aca768c39bfb08f0493561c333f24a3261d6a46L265-L268) triggered this only when the model wasn't collecting. > I'm wondering if it makes sense that this is hard or if we should actually consider a more fundamental redesign to make it more robust... I agree that a more fundamental redesign could improve robustness in the long term. That said, the current adjustments seem to resolve the issue for now (I'll look into why certain cases aren't passing the test). We can continue to monitor its performance and consider a redesign if further issues arise.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3721#issuecomment-2379285570:1579,perform,performance,1579,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3721#issuecomment-2379285570,1,['perform'],['performance']
Performance,Takes **longer than 6 hrs** atm... yikes!; https://buildkite.com/clima/oceananigans/builds/9087#01845448-a0ae-4e67-89df-6a4ec2ba9c9e. I believe the bottleneck is due to the nested `for`-loops in. https://github.com/CliMA/Oceananigans.jl/blob/99ad4c151095835c21ca899561429be30e8181cb/src/Solvers/sparse_approximate_inverse.jl#L61-L86,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2813:148,bottleneck,bottleneck,148,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2813,1,['bottleneck'],['bottleneck']
Performance,Testing with validation/mesoscale/baroclinic_adjustment.jl seems to indicate that the race condition is eliminated from the changes to the fill_halo_region! function,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1985:86,race condition,race condition,86,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1985,1,['race condition'],['race condition']
Performance,"Thank you @christophernhill , that is kind of you to offer and I will certainly take you up on your offer. . @hennyg888 has tried running things on 2 GPUs and has been getting an error at `Waitall!`, which is the first time that information is shared between GPUs. I hope we can look into this a bit more and then ask you about it next week. The last time that @ali-ramadhan successfully ran the distirbuted models on GPUs he found that the efficiency on 2 gpus was `50%`. Not great. We are trying to reproduce this as a starting point and when we do, it'll be nice to figure out what's causing the bottleneck. At the moment it's unknown to us but hope to start looking at this soon. We will certainly ask you about this next week!",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1654#issuecomment-868677525:599,bottleneck,bottleneck,599,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1654#issuecomment-868677525,1,['bottleneck'],['bottleneck']
Performance,"Thank you for all your comments. I will try those lines; Just one question. Is the sintax of. `ΞT = randn(size(T)...) *. shape`. correct? I am getting this error message:. > Warning: No xauth data; using fake authentication data for X11 forwarding.; [NVBLAS] NVBLAS_CONFIG_FILE environment variable is NOT set : relying on default config filename 'nvblas.conf'; [NVBLAS] Cannot open default config file 'nvblas.conf'; [NVBLAS] Config parsed; [NVBLAS] CPU Blas library need to be provided; ┌ Warning: You appear to be using MPI.jl with the default MPI binary on a cluster.; │ We recommend using the system-provided MPI, see the Configuration section of the MPI.jl docs.; └ @ MPI ~/.julia/packages/MPI/08SPr/deps/deps.jl:15; [ Info: Oceananigans will use 16 threads; ERROR: LoadError: syntax: invalid identifier name "".""; Stacktrace:; [1] top-level scope; @ /lustre/scratch5/.mdt0/fspereira/OCEANANIGANS/test/case09/c16_128_128m.jl:197; in expression starting at /lustre/scratch5/.mdt0/fspereira/OCEANANIGANS/test/case09/c16_128_128m.jl:197. Line 197 corresponds to the line above. I removed the `*.shape` and the simulations are now running. Is that ok?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2766#issuecomment-1268557962:772,Load,LoadError,772,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2766#issuecomment-1268557962,1,['Load'],['LoadError']
Performance,"Thank you for the links !. The Oceananigans src code is very clearly written so the reading is relatively OK (I could use some explanations on your closure usage). If it was up to me I would prefer an extended documentation of GPUifyLoops ;) I only understand how it works by reading what you do with it. . The GPU shared memory is basically a programmable cache while the cache of CPU can't be (easily) controlled. In both cases there is cache so, if you compute multiple partial derivatives of a given (set of) field(s) (d/dx,d/dy, d2/dx2,...) once a block has been fetched in the cache then the memory operations are cheap. Cache is also useful for performing tiny transpositions enabling fast access and vectorized (SIMD) CPU or GPU ops in both X,Y or Z directions. . I hope that obtaining an efficient code for both (multicore SIMD) CPUs and GPUs maybe possible adjusting the (recursive?) block sizes (i.e. controlling the data layout and adapt it to the computing target). I will try to use part of your code to rewrite the toy 2D CFD solver I have translated from Matlab (https://discourse.julialang.org/t/asynchronous-makie/27127/9?u=laurentplagne). Kudos again to your team for this inspiring package. Laurent",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/333#issuecomment-518372592:357,cache,cache,357,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/333#issuecomment-518372592,6,"['Cache', 'cache', 'perform']","['Cache', 'cache', 'performing']"
Performance,"Thank you for your feedback @maleadt! What is CUDAnative compilation? If you mean the precompilation phase when CUDAnative is first loaded, then it's not that as I start timing after all packages are loaded. I thought 6-7 minutes was normal/expected as @vchuravy et al. reported similar GPU compilation times for their shallow water model: https://github.com/JuliaLabs/ShallowWaterBench. I haven't done any rigorous benchmarking yet but out of those 6 minutes, ~1.5 minutes are spent on compiling code that creates CuFFT plans (the first plan takes 1.5 minutes then the others take <1 second). From watching the log I'm guessing the other 4.5 minutes are evenly split between setting up the model (creating CuArrays, initializing them, etc.) and the first time step (where the kernels are getting compiled presumably). I don't think we have that many kernels (just 5 bigger ones) but one of them; https://github.com/ali-ramadhan/Oceananigans.jl/blob/2b64d584c79ece0429f2421335ddb6bc0c6c6663/src/time_steppers.jl#L213; has several layers of inlining (it's inlining the majority of the functions in [operators/ops_regular_cartesian_grid_elementwise.jl](https://github.com/ali-ramadhan/Oceananigans.jl/blob/master/src/operators/ops_regular_cartesian_grid_elementwise.jl)) after which it probably balloons up to be a pretty big kernel. They also have tons of arguments crammed in as the structs I was passing weren't `isbitstype` (working on this #59). I should come back and update this issue once we do some proper benchmarking (note to self: nvprof seems like it's being deprecated in favor of Nsight). Caching kernels between sessions sounds tough but will definitely look into timing compilations in CUDAnative, might provide some insight on how to speed things up.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/66#issuecomment-466847765:132,load,loaded,132,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/66#issuecomment-466847765,2,['load'],['loaded']
Performance,"Thank you to everyone who took the time to respond to this issue! I am still unclear on what exactly was driving the error I was getting, but I was able to fix it by creating a new environment in a clean directory and running the following modules on Casper for the UCAR HPC resources:; ```; module --force purge; module --ignore-cache load ncarenv/23.10 gcc ncarcompilers netcdf; module --ignore-cache load cuda; module --ignore-cache load julia/1.10.2; ```; and then running my script. Thanks again for all the help!",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2256200159:330,cache,cache,330,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2256200159,6,"['cache', 'load']","['cache', 'load']"
Performance,"Thanks @ali-ramadhan and @navidcy . Adding `const U = 1.0` definitely seemed to help as now it has gone further. However, at this stage I believe it's complaining about the norm. Below is the beginning part of the output, in case this makes sense to others. ```; ERROR: LoadError: scalar getindex is disallowed; Stacktrace:; [1] error(::String) at ./error.jl:33; [2] assertscalar(::String) at /home/fpoulin/.julia/packages/GPUArrays/WV76E/src/host/indexing.jl:41; [3] getindex at /home/fpoulin/.julia/packages/GPUArrays/WV76E/src/host/indexing.jl:96 [inlined]; [4] getindex at /home/fpoulin/.julia/packages/OffsetArrays/lli7H/src/OffsetArrays.jl:300 [inlined]; [5] getindex at ./subarray.jl:257 [inlined]; [6] _getindex at ./abstractarray.jl:1100 [inlined]; [7] getindex at ./abstractarray.jl:1060 [inlined]; [8] iterate at ./abstractarray.jl:986 [inlined]; [9] iterate at ./abstractarray.jl:984 [inlined]; [10] generic_normInf(::SubArray{Float64,3,OffsetArrays.OffsetArray{Float64,3,CuArray{Float64,3}},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},false}) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/LinearAlgebra/src/generic.jl:445; [11] normInf at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/LinearAlgebra/src/generic.jl:536 [inlined]; [12] generic_norm2(::SubArray{Float64,3,OffsetArrays.OffsetArray{Float64,3,CuArray{Float64,3}},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},false}) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/LinearAlgebra/src/generic.jl:477; [13] norm2 at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/LinearAlgebra/src/generic.jl:538 [inlined]; [14] norm(::SubArray{Float64,3,OffsetArrays.OffsetArray{Float64,3,CuArray{Float64,3}},Tuple{UnitRange{Int64},UnitRange{Int64},UnitRange{Int64}},false}, ::Int64) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/LinearAlgebra/src/generic.jl:607; [15] norm(::SubArray{Float64,3",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1477#issuecomment-800208000:270,Load,LoadError,270,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1477#issuecomment-800208000,1,['Load'],['LoadError']
Performance,"Thanks @ali-ramadhan, that's helpful. It does seem that `Requires.jl` can solve this issue and satisfies @ali-ramadhan's desire for verticality as mentioned in his comment. We could perhaps isolate the output functionality into a submodule (eg, `Oceananigans.Output`) that is loaded only when `HDF5` (for example) is present. Ditto for `Oceananigans.Plotting`. I've changed the name of the issue to be more descriptive of the general issue we are discussing (heavy dependencies).",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/284#issuecomment-503157682:276,load,loaded,276,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/284#issuecomment-503157682,1,['load'],['loaded']
Performance,"Thanks @maleadt, that's very helpful!. In this PR, we haven't directly changed any kernel function signatures. However, this PR does pass more complicated objects into kernels (a wrapper around an `OffsetArray` called a ""`Field`"", rather than simply the `OffsetArray`). The primary changes in this PR are thus 1. _not_ to extract the underlying `OffsetArray` from a `Field`, and 2. writing an `adapt_structure` method for `Field`s. I suppose the translation that's performed by `adapt_structure` increases the number or arguments to the function `ptxcall_calculate_Gu__66`?. The changes made in this PR are not strictly necessary --- they are a convenience. If manually unwrapping `Field`s (the method we previously used) is necessitated by CUDA limitations, I think we can live with that. If I understand this issue correctly, we are facing a basic trade-off between (compiler?) performance and the use of convenient but complicated abstraction objects?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/746#issuecomment-655255987:465,perform,performed,465,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/746#issuecomment-655255987,2,['perform'],"['performance', 'performed']"
Performance,"Thanks @navidcy for the quick approval. Strangely, 3 tests have already failed. I looked at one and see that it can't find a file, see below. This is not actually related to this PR but I don't know how to fix it. ```; Precompiling project...;   | ✓ Oceananigans;   | 1 dependency successfully precompiled in 88 seconds (199 already precompiled);   | Testing Running tests...;   | ERROR: LoadError: LoadError: SystemError: opening file ""/data5/glwagner/.julia-7768/compiled/v1.6/Oceananigans/hU93i_huVsp.ji"": No such file or directory;  ; ```",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2598#issuecomment-1146595140:388,Load,LoadError,388,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2598#issuecomment-1146595140,2,['Load'],['LoadError']
Performance,"Thanks @simone-silvestri @glwagner! I encountered a different error on CPU recently that might be related. I ran a 3D (Bounded,Bounded,Bounded) simulation with immersed boundary on CPU. When using `FieldTimeSeries` to load the JLD2 output, I get a `BoundsError` message:. ```b = FieldTimeSeries(""moving_source_complex_topography.jld2"",""b"",grid=nothing); Error showing value of type FieldTimeSeries{Center, Center, Center, InMemory, Tuple{UnitRange{Int64}, UnitRange{Int64}, UnitRange{Int64}}, OffsetArrays.OffsetArray{Float64, 4, Array{Float64, 4}}, ImmersedBoundaryGrid{Float64, Bounded, Bounded, Bounded, RectilinearGrid{Float64, Bounded, Bounded, Bounded, Float64, Float64, Float64, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, CPU}, GridFittedBottom{OffsetArrays.OffsetMatrix{Float64, Matrix{Float64}}}, CPU}, Float64, FieldBoundaryConditions{Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, BoundaryCondition{Oceananigans.BoundaryConditions.Flux, Nothing}}, Vector{Float64}}:; ERROR: BoundsError: attempt to access 512×512×128×10 FieldTimeSeries{Center, Center, Center, InMemory, Tuple{UnitRange{Int64}, UnitRange{Int64}, UnitRange{Int64}}, OffsetArrays.OffsetArray{Float64, 4, Array{Float64, 4}}, ImmersedBoundaryGrid{Float64, Bounded, Bounded, Bounded, RectilinearGrid{Float64, Bounded, Bounded, Bounded, Float64, Float64, Float64, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePreci",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2479#issuecomment-1124607638:218,load,load,218,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2479#issuecomment-1124607638,1,['load'],['load']
Performance,Thanks @simone-silvestri for adding a GPU test for `FieldTimeSeries`! I've adapted it correctly now and tests pass locally so I think this PR is ready for review. Actually I should add a test that uses the new kwargs. Would do a test where multiple threads open the same `FieldTimeSeries` but don't think we have multi-threaded tests.,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3739#issuecomment-2397361158:313,multi-thread,multi-threaded,313,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3739#issuecomment-2397361158,1,['multi-thread'],['multi-threaded']
Performance,Thanks Simone! So I guess [this line](https://github.com/CliMA/Oceananigans.jl/blob/edef0fc49a1019bfba093ffa1c0bcce23535ac41/src/Solvers/sparse_preconditioners.jl#L70) explains why we only see a performance increase on GPU? I don't understand why ILU is hard to parallelise - would you mind explaining this?,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2753#issuecomment-1258832262:195,perform,performance,195,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2753#issuecomment-1258832262,1,['perform'],['performance']
Performance,"Thanks again @glwagner, unfortunately I still get the same error (ERROR: LoadError: UndefVarError: Relaxation not defined). I have been trying to figure out why, so far no success. Will keep you updated, thanks again!",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/789#issuecomment-656897248:73,Load,LoadError,73,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/789#issuecomment-656897248,1,['Load'],['LoadError']
Performance,"Thanks everyone for your feedback. @vchuravy , great to know that multi-threading is built in! . I agree that profiling would be a good way to determine why we get not great efficiency. I have not used perf but we can look into it. Also, do you know of benchmarking others have done using `KernelAbstractions` on threads that we could look at for comparison?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1861#issuecomment-880844335:66,multi-thread,multi-threading,66,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1861#issuecomment-880844335,1,['multi-thread'],['multi-threading']
Performance,"Thanks for clarifying. Yes, I was thinking about being able to perform fit in any dimension you choose and save outputs like this. Putting the heavy load on the GPU.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3279#issuecomment-2178922830:63,perform,perform,63,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3279#issuecomment-2178922830,2,"['load', 'perform']","['load', 'perform']"
Performance,"Thanks for doing this! Stacktraces are much longer now lol but might help with abstraction performance?. This is not ready to be merged though. The tests are failing because of some error when assigning the forcing functions in the `Model` constructor. I can have a closer look and figure out a fix. > Nice. It's just `ModelBoundaryConditions` and `StepperTemporaryFields` remaining. It's possible that `StepperTemporaryFields` isn't even required anymore. If so, I'll open a new PR removing them.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/261#issuecomment-498051659:91,perform,performance,91,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/261#issuecomment-498051659,1,['perform'],['performance']
Performance,"Thanks for looking through the time stepping code, this is great feedback!. I agree it would be nice to dispatch on the array type (or even the architecture, another reason to use types instead of symbols). No reason why `xC, xF, yC, ...` have no assigned type in `RegularCartesianGrid`, just oversight on my part. There will probably be a major refactor of the code's design soon-ish to get all this little stuff right. I remember fields initially had the array type be a parameter, e.g. `CellField{T}` then `data::T` but I hit some performance issues with that I think. I may have just been encountering the [Avoid fields with abstract containers](https://docs.julialang.org/en/v1/manual/performance-tips/index.html#Avoid-fields-with-abstract-containers-1) Julia performance tip, but I think making the parameter an `AbstractArray` as you're suggesting shouldn't sacrifice performance. And yes, the solver is not spectral. Will probably generically call it `PoissonSolver` when the next refactor happens (and the documentation would make it clear what it actually does).",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/45#issuecomment-462783213:534,perform,performance,534,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/45#issuecomment-462783213,4,['perform'],"['performance', 'performance-tips']"
Performance,Thanks for posting the solution. What does the `ignore-cache` flag do? And how did you get to it?,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2256203467:55,cache,cache,55,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2256203467,1,['cache'],['cache']
Performance,Thanks for running these tests. It would be nice to see how MPData performs in a 2D flow. . It's interesting that tweaking the number of iterations doesn't seem to make much difference. I'm a bit skeptical about this scheme though; it seems pretty diffusive and doesn't even beat third-order upwind which is way simpler. I might have introduced a bug although I went through the code again and it looks correct to me. Another possibility is that the high diffusivity is the trade-off for keeping the scheme positivity-preserving?; Godunov's theorem sets some tough standards -- it's not possible to obtain monotonic advection with a linear scheme beyond first order.,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3434#issuecomment-1954700946:67,perform,performs,67,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3434#issuecomment-1954700946,1,['perform'],['performs']
Performance,Thanks for taking a look @glwagner! I'll open a PR to make sure we're working with `PrescribedVelocityFields` correctly. And I'll also open a second PR to start looking at cubed sphere performance + getting it working on the GPU.,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1522#issuecomment-819603798:185,perform,performance,185,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1522#issuecomment-819603798,1,['perform'],['performance']
Performance,"Thanks for taking a look. Good point. `FreeSlip` is technically not correct either as it only applies to the velocity field. I've removed it as a boundary condition and instead specify a no-flux boundary condition. So now only doubly `Periodic` boundary conditions default to `nothing` as they're enforced via halo filling elsewhere in the code for now. The no-flux BCs will just add zeros but the performance hit should be negligible. I guess we only apply BCs in the z-dimension for now, so would be good to benchmark when x and y BCs are applied.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/288#issuecomment-505390113:398,perform,performance,398,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/288#issuecomment-505390113,1,['perform'],['performance']
Performance,"Thanks for the additional changes and it seems to be doing much better! Still three tests that fail. I looked ath the initialize environments and saw what I copied below. It seems to have probelms with Oceananigans?. ```. ✗ Oceananigans; --;   | 105 dependencies successfully precompiled in 112 seconds;   | 1 dependency errored. To see a full report either run `import Pkg; Pkg.precompile()` or load the package;   | Precompiling project...;   | ✗ Oceananigans;   | 0 dependencies successfully precompiled in 22 seconds. 105 already precompiled.;   |  ;   | ERROR: The following 1 direct dependency failed to precompile:;   |  ;   | Oceananigans [9e8cae18-63c1-5223-a75c-80ca9d6e9a09];   |  ;   | Failed to precompile Oceananigans [9e8cae18-63c1-5223-a75c-80ca9d6e9a09] to /storage5/buildkite-agent/.julia-9773/compiled/v1.8/Oceananigans/jl_yggB5x.;   | [NVBLAS] No Gpu available;   | [NVBLAS] NVBLAS_CONFIG_FILE environment variable is NOT set : relying on default config filename 'nvblas.conf';   | [NVBLAS] Cannot open default config file 'nvblas.conf';   | [NVBLAS] Config parsed;   | [NVBLAS] CPU Blas library need to be provided;   | ERROR: LoadError: syntax: missing comma or ) in argument list;   | Stacktrace:;   | [1] top-level scope;   | @ ~/builds/tartarus-1/clima/oceananigans/src/Coriolis/non_traditional_beta_plane.jl:75;   | [2] include(mod::Module, _path::String);   | @ Base ./Base.jl:419;   | [3] include(x::String);   | @ Oceananigans.Coriolis ~/builds/tartarus-1/clima/oceananigans/src/Coriolis/Coriolis.jl:1;   | [4] top-level scope;   | @ ~/builds/tartarus-1/clima/oceananigans/src/Coriolis/Coriolis.jl:28;   | [5] include(mod::Module, _path::String);   | @ Base ./Base.jl:419;   | [6] include(x::String);   | @ Oceananigans ~/builds/tartarus-1/clima/oceananigans/src/Oceananigans.jl:5;   | [7] top-level scope;   | @ ~/builds/tartarus-1/clima/oceananigans/src/Oceananigans.jl:230;   | [8] include;   | @ ./Base.jl:419 [inlined];   | [9] include_package_for_output(pkg::Base.Pk",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2877#issuecomment-1414372445:396,load,load,396,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2877#issuecomment-1414372445,1,['load'],['load']
Performance,Thanks for the feedback @ranjanan !; * Instead of using `const` we went with structs.; * CuArrays have worked quite well but still working on writing performant CPU/GPU shared kernels (hoping [GPUifyLoops.jl](https://github.com/vchuravy/GPUifyLoops.jl) will help here).; * Multicore parallelism is a bit lower priority right now but have had some pretty good results with DistributedArrays although I think MPIArrays will be the way forward for us here.,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2#issuecomment-462058793:150,perform,performant,150,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2#issuecomment-462058793,1,['perform'],['performant']
Performance,"Thanks for the suggestions!. @glwagner Lots of nice ideas and cool syntax! Really like the way the boundary conditions are implemented, I'll see if I can translate it to the halo regions. Interested in some of the design choices you made, hope you don't mind if I ask you about them in person. To me resolving this issue feels like implementing ghost cells for boundary conditions. I mean this is exactly what's happening for the periodic boundary conditions but it sounds like these halo regions are strictly for implementing periodic boundary conditions (without `incmod1` and `decmod1`) and distributed parallelization? But might be good to make sure this won't blow up in our faces when we get to implementing bathymetry and continental/land boundaries. @jm-c @christophernhill (See #92 for decision not to use ghost cells.). @vchuravy OffsetArrays.jl would be perfect for halo regions! Do you think this stuff would work for the GPU? I assume if e.g. `isbitstype(OffsetArray{Float64}) == true` then it can be used in a GPU kernel right out of the box? It's actually `false` but I might be checking/using it incorrectly. We might need to create our own `OffsetCuArray` maybe?. ```julia; # 1D grid with 10 cells (and 1 ghost cell on each end of the domain).; julia> y = OffsetArray{Float64}(undef, 0:11); OffsetArray(::Array{Float64,1}, 0:11) with eltype Float64 with indices 0:11:; 0.0; 0.0; 5.0e-324; 0.0; 0.0; 0.0; 0.0; 0.0; 0.0; 0.0; 0.0; 0.0. julia> typeof(y); OffsetArray{Float64,1,Array{Float64,1}}. julia> isbitstype(typeof(y)); false; ```. I think it's important to implement #67 ASAP before we start changing data structures and abstractions so we know exactly when we've introduced a performance bottleneck.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/104#issuecomment-469735462:1698,perform,performance,1698,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/104#issuecomment-469735462,2,"['bottleneck', 'perform']","['bottleneck', 'performance']"
Performance,Thanks for the useful link. That makes sense that it's all about efficient memory arrangement. Since it's a micro-optimization I'll mark it as wontfix and revisit when we have a model that runs on GPUs.,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/44#issuecomment-462555060:114,optimiz,optimization,114,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/44#issuecomment-462555060,1,['optimiz'],['optimization']
Performance,"Thanks for your answer, i made some progress by using the JULIA_CUDA_USE_BINARYBUILDER env variable.; The download problem is fixed and CUDA.jl tests passed on the gpu node but i still have some errors with Oceaningans. ```; module load julia cuda; env JULIA_CUDA_USE_BINARYBUILDER=false julia —project; julia> using Oceananigans; julia> grid = RegularCartesianGrid(size=(100, 100, 50), extent=(2π, 2π, 1)); RegularCartesianGrid{Float64, Periodic, Periodic, Bounded}; domain: x ∈ [-5.496153587253255e-18, 6.283185307179586], y ∈ [-5.496153587253255e-18, 6.283185307179586], z ∈ [-1.0, 1.7080354225002348e-17]; topology: (Periodic, Periodic, Bounded); resolution (Nx, Ny, Nz): (100, 100, 50); halo size (Hx, Hy, Hz): (1, 1, 1); grid spacing (Δx, Δy, Δz): (0.06283185307179587, 0.06283185307179587, 0.02). julia> model = IncompressibleModel(architecture=GPU(), grid=grid); ERROR: MethodError: no method matching plan_forward_transform(::CUDA.CuArray{Complex{Float64},3}, ::BoundaryCondition{Oceananigans.BoundaryConditions.Periodic,Nothing}, ::Array{Int64,1}); ```",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1035#issuecomment-707677672:232,load,load,232,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1035#issuecomment-707677672,1,['load'],['load']
Performance,"Thanks to some preliminary CSI profiling by @banex19 we have some timings for the time-stepping loop; ```; Function time_step!: 1 times (average runtime : 20186 ms); Function update_buoyancy!: 1 times (average runtime : 8631 ms); Function calculate_interior_source_terms!: 1 times (average runtime : 4315 ms); Function solve_poisson_3d_ppn_planned!: 1 times (average runtime : 1131 ms); Function update_velocities_and_tracers!: 1 times (average runtime : 728 ms); Function adams_bashforth_update_source_terms!: 1 times (average runtime : 526 ms); Function store_previous_source_terms!: 1 times (average runtime : 353 ms); Function calculate_source_term_divergence!: 1 times (average runtime : 295 ms); ```. I kind of overlooked this but the `update_buoyancy!` function is taking up 43% of the time stepping cycles because each grid point gets one thread that calculates the buoyancy for all grid points above them (quadratic time for no reason). Well, the reason I did it this way was to avoid having to synchronize between threads, but there is no need to synchronize. This should be changed so that each column gets a thread that is responsible for calculating the buoyancy in that column. So when launching this kernel with CUDA it should be called with `threads=(Tx, Ty), blocks=(Bx, By)`. This is probably only performant for large grids with `Nx*Ny >> 1024`. On the CPU this might speed up `update_buoyancy!` by ~10x and the entire time stepping by ~60%! Should speed up the GPU models as well. Note: The absolute times themselves don't mean much as the profiler is pretty sophisticated/invasive.",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/169:1316,perform,performant,1316,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/169,1,['perform'],['performant']
Performance,"Thanks very much @ali-ramadhan. I am now using version ""Oceananigans v0.30.0 #master (https://github.com/CliMA/Oceananigans.jl.git)"". . I only get an error when calling . `u_forcing = Relaxation(; rate=1/60, mask=GaussianMask{:x}(grid.Lx, grid.Lx/10), target=0.1)`. which returns ERROR: LoadError: MethodError: no method matching GaussianMask{:x,T} where T(::Float64, ::Float64)",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/789#issuecomment-657584582:287,Load,LoadError,287,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/789#issuecomment-657584582,1,['Load'],['LoadError']
Performance,"Thansk for the quick feedback. 1. I should say this is not as much of a concern as I found a work around. In my definition of b\tilde, I changed `f` to `model.coriolis.f` and received an error. Below is the beginning and it's huge so can't copy the whole thing. I will stick to my simple solution for the moment but want to point this out, in case there was a concern. ```; include(""inertially_unstable_jet.jl""); ERROR: LoadError: InvalidIRError: compiling kernel gpu__compute!(Cassette.Context{nametype(CUDACtx),KernelAbstractions.CompilerMetadata{KernelAbstractions.NDIteration.StaticSize{(1, 64, 64)},KernelAbstractions.NDIteration.DynamicCheck,Nothing,Nothing,KernelAbstractions.NDIteration.NDRange{3,KernelAbstractions.NDIteration.StaticSize{(1, 1, 64)},KernelAbstractions.NDIteration.StaticSize{(1, 64, 1)},Nothing,Nothing}},Nothing,KernelAbstractions.var""##PassType#253"",Nothing,Cassette.DisableHooks}, typeof(Oceananigans.Fields.gpu__compute!), OffsetArrays.OffsetArray{Float64,3,CuDeviceArray{Float64,3,1}}, Oceananigans.AbstractOperations.BinaryOperation{Center,Center,Center,typeof(-),OffsetArrays.OffsetArray{Float64,3,CuDeviceArray{Float64,3,1}},Oceananigans.Fields.FunctionField{Center,Center,Center,Nothing,Nothing,typeof(b̄),RegularRectilinearGrid{Float64,Flat,Bounded,Bounded,OffsetArrays.OffsetArray{Float64,1,StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}}}},typeof(identity),typeof(identity),typeof(identity),RegularRectilinearGrid{Float64,Flat,Bounded,Bounded,OffsetArrays.OffsetArray{Float64,1,StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}}}}) resulted in invalid LLVM IR; Reason: unsupported dynamic function invocation (call to overdub); Stacktrace:; [1] b̄ at /home/fpoulin/software/Oceananigans.jl/examples/inertially_unstable_jet.jl:39; [2] call_func at /home/fpoulin/software/Oceananigans.jl/src/Fields/function_field.jl:61; [3] getindex at /home/fpoulin/software/Oceananigans.jl/src/Fields/function_field.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1554#issuecomment-815099807:420,Load,LoadError,420,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1554#issuecomment-815099807,1,['Load'],['LoadError']
Performance,That code allows two things that would not be possible with `maximum(parent(f.data))`. - it returns a `ReducedField` instead of an Array if `dims != :`; - it is possible to perform a conditional reduction which means that we can remove immersed cells from the reduction. It can be done in another way though if it errors... when do you find the error?,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2727#issuecomment-1237038870:173,perform,perform,173,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2727#issuecomment-1237038870,1,['perform'],['perform']
Performance,"That is a huge leap forward @hennyg888 and great to see! Before we were at 50% and now we are at 75%. An increase of 50%, which is pretty huge all things considered. I like @christophernhill 's suggest of adding the version info. Yesterday when we talked the consensus was that one major problem was how we do buffering. As a silly experiment, what if we redo this without updating any halos, ever. Physically, it's going to be wrong but do we get another huge increase in the efficiency? If the efficiency gets close to 100% then in my mind that validate the hypothesis. If not, then that would signify there is another bottleneck that we need to hunt down.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1882#issuecomment-887431593:621,bottleneck,bottleneck,621,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1882#issuecomment-887431593,1,['bottleneck'],['bottleneck']
Performance,"That probably shouldn't have changed, can you file an issue on CUDA.jl/GPUArrays.jl? I'll have a look next week. The only change to `@allowscalar` that comes to mind is task/thread-safety, which does come at a certain performance cost (it now does a TLS lookup instead of a simple pointer check, but the cost of that should be negligible compared to the subsequent memory transfer).",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1514#issuecomment-817088189:218,perform,performance,218,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1514#issuecomment-817088189,1,['perform'],['performance']
Performance,That's awesome @simone-silvestri. Great work!. A couple of questions:. > - Design correct OutputWriters and OutputReaders for MultiRegionFields. Does this mean that the code with multi-grid doesn't have the ability to output things at the moment? Or does it output to one file per region/GPU?. > - Implement a multi-region version of the Nonhydrostatic pressure solver. So I take that a nonhydrostatic pressure solver for multi-grid isn't defined yet? Or is it just non-optimized (but will run)?,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2253#issuecomment-1116857309:470,optimiz,optimized,470,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2253#issuecomment-1116857309,1,['optimiz'],['optimized']
Performance,"That's great news @ali-ramadhan . I guess by looking at your code I can learn how to adapt it to `ShallowWaterModel`. Maybe I will start by doing some tests for the two-dimensional turbulence example, or has someone done that already? By that I mean checking scalabiilty.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/590#issuecomment-786696921:259,scalab,scalabiilty,259,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/590#issuecomment-786696921,1,['scalab'],['scalabiilty']
Performance,"That's not expected and sounds like a huge change! The only important change I can think of is when we started pre-calculating the immersed boundary (rather than allowing it to be a function):. https://github.com/CliMA/Oceananigans.jl/blob/3557db3692ff8e1978b126369230c9bacae42ae8/src/ImmersedBoundaries/grid_fitted_immersed_boundaries.jl#L70. That adds one 2D (x, y) field. The reason is because in the majority of cases this is more performant (we think). In a model with one tracer and AMD, we have something like 16 3D fields. So 25% is a huge amount of data equivalent to four 3D fields. Any ideas @simone-silvestri ?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2794#issuecomment-1298967328:435,perform,performant,435,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2794#issuecomment-1298967328,1,['perform'],['performant']
Performance,"That's right. Purely for simplicity we launch all the tendency kernels from 1:N, though for Face-fields in Bounded directions, we only require 2:N. In fact using tendencies only from 2:N could allow an optimization where we don't need to ""enforce"" no-penetration boundary conditions. It'd be hard to achieve this optimization though because users can write things like `parent(u) .= 1` so I'm not sure we can get away with this being guaranteed correct. This has never been a problem because we simply overwrite the boundary velocity and therefore simply discard the tendency at index 1. > The problem is if we try to integrate something like a radiation condition. Can you point me to where in the code this goes down?. > On bounded topology I don't think we ever want to integrate the tendency right? But it might be more complicated to do that. I think that's right that we don't need the tendency. This has been part of the algorithm since time immemorial and back in the mists of time it was indeed more complicated than worthwhile. The complication is that KernelAbstractions assumes indices start at 1... However, we now have a way of offsetting indices in kernels via our `KernelParameters`. So it's not very hard to do this anymore. I can give it a start. If we make this change, we also want to take a step back and look at all the kernels we are launching currently to make sure everything makes sense. For example, here is a question: while we don't want to integrate the velocity tendencies on boundaries, what do we do about diagnostics? Do we want to compute vorticity on the boundary, for example, if we are computing a vorticity diagnostic? It seems simpler if we don't, that way we don't have special cases...",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3810#issuecomment-2388758985:202,optimiz,optimization,202,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3810#issuecomment-2388758985,2,['optimiz'],['optimization']
Performance,"The ""long line"" issue is not just a problem for diff's by the way. I use a text editor (and many others do too) in which navigation is performed by the keyboard. Navigating to the middle of an extremely long line is annoying and time consuming (I could possible build in shortcuts that jump many words at at time for that... but I haven't). Code navigation is more efficient (for me) when the lines are short and I can navigate through the text by jumping lines.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/111#issuecomment-470523539:135,perform,performed,135,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/111#issuecomment-470523539,1,['perform'],['performed']
Performance,The Langmuir example was failing because it was loading the arrays with the halos to plot. Switching to using the MakieExt does the job better and cleaner.,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3860#issuecomment-2434352236:48,load,loading,48,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3860#issuecomment-2434352236,1,['load'],['loading']
Performance,"The Roquet’s approximation is perfectly sufficient for Oceananigans, because it will never be used for global calculations where local approximations are an issue. However I agree with everybody else that it would be best to use the same EOS in Ocenanigans and Climate_Ocean. in that case we should adopt TEOS-10. Be warned that it is quite inefficient through. So we may be hit performance-wise. Hard to tell without trying. Raffaele. > On Mar 9, 2020, at 11:48 AM, Gregory L. Wagner <notifications@github.com> wrote:; > ; > I also think it’s a good idea to use a full equation of state for all simulations sooner rather than later. It’s simpler: we won’t have to report constants of linearization everywhere. And setting up simulations will be easier.; > ; > I’ll defer to the modelers for whether Roquet’s approximation is an acceptable model for TEOS-10.; > ; > This package is relevant and we should consider contributing to it rather than implementing an equation of state somewhere in the Clima ecosystem:; > ; > https://github.com/gher-ulg/PhysOcean.jl <https://github.com/gher-ulg/PhysOcean.jl>; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub <https://github.com/climate-machine/Oceananigans.jl/issues/692?email_source=notifications&email_token=AK24ROIBMZMYD77AGEZNM3DRGUMURA5CNFSM4LEKJAAKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEOHZCYQ#issuecomment-596611426>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AK24ROONT4RO4YCCF6BTIIDRGUMURANCNFSM4LEKJAAA>.; >",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/692#issuecomment-596726895:379,perform,performance-wise,379,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/692#issuecomment-596726895,1,['perform'],['performance-wise']
Performance,The actual bottlenecks (as seen from the profile) are the transpose steps. The FFT steps are basically irrelevant.; This file shows how to setup and perform the transposes; https://github.com/CliMA/Oceananigans.jl/blob/ss/distributed-fft/test/test_distributed_transpose.jl,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3279#issuecomment-1986306179:11,bottleneck,bottlenecks,11,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3279#issuecomment-1986306179,2,"['bottleneck', 'perform']","['bottlenecks', 'perform']"
Performance,"The advection-like Coriolis scheme was a non-sequitur because `f` is a very regular field, so upwinding it was just decreasing performance without a significative increase in quality of the simulation. On the other hand, upwinding `u` is very much discouraged because the energy builds up rapidly (by upwinding the velocity the divergence of the reconstructed tangential velocity is not a direct interpolation of the divergence of the original velocity, which is a necessary condition to maintain the algorithm stable). The only thing I can think to increase the order of velocity interpolation in the Coriolis force is to use a centered high-order scheme to interpolate velocity, but that would not help with the noise since a centered scheme is dispersive in nature. . I converted this PR to implement a `WetPointCoriolisScheme` (described in [Numerical boundary layers and spurious residual flows](https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/JC091iC09p10621)). ; This is just a simple addition to an enstrophy conserving scheme where edge (""dry"") points are neglected in the interpolation of the velocity in the tangential direction. A comparison of the output of this scheme in a global 1 degree setup will follow",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2729#issuecomment-1252445272:127,perform,performance,127,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2729#issuecomment-1252445272,1,['perform'],['performance']
Performance,"The boundary condition functions should have the arguments `x, y, t, p` (no `z`!) so. ```julia; Qᵘ(x, y, t, p) = radius(x, y) < p.L ? imag(p.ρₐ / p.ρₒ * p.cᴰ * U(x, y, p.L) * abs(U(x, y, p.L))) : 0 # m² s⁻²; Qᵛ(x, y, t, p) = radius(x, y) < p.L ? - real(p.ρₐ / p.ρₒ * p.cᴰ * U(x, y, p.L) * abs(U(x, y, p.L))) : 0 # m² s⁻²; ```. You may also want to tag functions with `@inline` and use `ifelse` rather than the shortcircuiting ternary `? :` for performance:. ```julia; @inline d(x, y) = sqrt(x^2 + y^2); @inline U(x, y, L) = sin(π * d(x, y) / L) * exp(1im * angle(x + y*im)); @inline Qᵘ(x, y, t, p) = ifelse(d(x, y) < p.L, +imag(p.ρₐ / p.ρₒ * p.cᴰ * U(x, y, p.L) * abs(U(x, y, p.L))), 0.0) # m² s⁻²; @inline Qᵛ(x, y, t, p) = ifelse(d(x, y) < p.L, -real(p.ρₐ / p.ρₒ * p.cᴰ * U(x, y, p.L) * abs(U(x, y, p.L))), 0.0) # m² s⁻²; ```. Is `U` air speed? We sometimes recommend using the momentum flux itself as an input, rather than introducing a bulk formula (like a drag law) because this reduces the number of parameters in the problem (making it easier to understand and reproduce) --- but that's up to you. Here you could write. ```julia; @inline d(x, y) = sqrt(x^2 + y^2); @inline s(x, y, L) = sin(π * d(x, y) / L) * exp(1im * angle(x + y*im)); @inline Qᵘ(x, y, t, p) = p.τ₀ * ifelse(d(x, y) < p.L, +imag(s(x, y, p.L) * abs(s(x, y, p.L))), 0.0) # m² s⁻²; @inline Qᵛ(x, y, t, p) = p.τ₀ * ifelse(d(x, y) < p.L, -real(s(x, y, p.L) * abs(s(x, y, p.L))), 0.0) # m² s⁻²; ```. or something like that!",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2336#issuecomment-1066015300:444,perform,performance,444,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2336#issuecomment-1066015300,1,['perform'],['performance']
Performance,"The connectivity creates a ""cartesian grid"" of ranks which informs us which ranks are spatial neighbours. ; The more general way to arrange this grid, in absence of any other information is to wrap around the ranks as if the cartesian rank grid was `Periodic`. This is enough information for parsing the rank configuration, but not enough to perform fill halo regions. ; For this, we need a grid! Only with a grid, we will know if some directions are Bounded or Periodic, so, leveraging this information we can correctly assess which boundaries need communicating or not. Philosophically speaking, architecture provides the general rank layout while the grid concretizes this information ; leveraging the topology to decide whether to perform halo passing or not. Practically speaking the implementation leverages an `inject_halo_communication_boundary_conditions` implemented here:; https://github.com/CliMA/Oceananigans.jl/blob/34a3b930e0ace7df7dc7660f9c74f52315f41da7/src/DistributedComputations/halo_communication_bcs.jl#L14; previously only the connectivity was passed to this function, while this function should require also grid information.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3328#issuecomment-1758117187:342,perform,perform,342,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3328#issuecomment-1758117187,2,['perform'],['perform']
Performance,The default is not a concrete type which may cause performance issues when `FieldTimeSeries` is used as boundary condition or forcing in a kernel:. https://github.com/CliMA/Oceananigans.jl/blob/6c40d7e225c2127051b2703b9c62a8b18260e3a5/src/OutputReaders/field_time_series.jl#L373. I suggest `nothing` as the default. Users can still change it if they want. We could also use a `NamedTuple` instead of `Dict`. @ali-ramadhan I think you maybe were not thinking that we use `FieldTimeSeries` in kernels and models now (not just for reading output),MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3898:51,perform,performance,51,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3898,1,['perform'],['performance']
Performance,"The first step is to install the `buildkite-agent` utility on the local machine:. https://buildkite.com/organizations/clima/agents. Then a tag needs to be added to the `buildkite-agent.cfg` file (for example, when buildkite is installed with a system package manager the default configuration file is located at `/etc/buildkite-agent.cfg`). We use two tags (for example):. ```; tags=""queue=Oceananigans,architecture=GPU""; ```. Note that you may not be able to use a space after the comma between two tags. We also set the number of agents via the `spawn` keyword. To start the buildkite agent as a system process, type. ```; sudo systemctl start buildkite-agent; ```. To stop a buildkite agent running as a system process, type. ```; sudo systemctl stop buildkite-agent; ```. Alternatively, `buildkite-agent` can be run in the background via `tmux` or `screen`. When `buildkite-agent` is run as a system process, their status is viewed by typing. ```; sudo systemctl status buildkite-agent; ```",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1046:384,queue,queue,384,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1046,1,['queue'],['queue']
Performance,"The grids define their own `R_Earth`.; https://github.com/CliMA/Oceananigans.jl/blob/e881faee3de5f17cb382cd61e2b43c2069cadde7/src/Grids/latitude_longitude_grid.jl#L3. The `Coriolis` module defines another one... Perhaps we should have a module `Constants`, add everything in there and load it first? This potentially would come handy when we try to couple?",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2981:285,load,load,285,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2981,1,['load'],['load']
Performance,"The issue is a little tricky. Typically we expect abstract operations to be computed during time-stepping. In that case, the halos should be correctly filled. However, @navidcy expects that abstract operations should be correct at any time and does not expect to have to call fill halo regions. Thus for `compute!` to be more generally useful to users I think we do want this behavior. The problem is that fill halo regions can be expensive eg for distributed models. Therefore to both serve expected user behavior and provide a performant interface we perhaps have to add a flag to `compute!` like `fill_halo_regions=false` so that computation for output does not trigger extra calls to fill halo regions. Note @navidcy you can also use the simpler and more transparent . ```julia; parent(model.velocities.u) .= 1; ```. or just `fill!(model.velocities.u, 1)`. I think your result would be correct then. But still if we are setting to functions then we need `set!`.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3114#issuecomment-1559611530:529,perform,performant,529,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3114#issuecomment-1559611530,1,['perform'],['performant']
Performance,"The issue was that, prior to the update the fill_halo_region! function, the test in validation/mesoscale/baroclinic_adjustment.jl , which in particular, uses the hydrostatic model with an explicit free surface, would produce different answers when run twice after several thousand timesteps. This typically happens when there is a race condition in the code and this particular one was hard to find since it occurs only after several timesteps have occurred. The reason for ""potential"" in the title of the PR is that I do not understand why this race condition exists in the first place, since I have not seen this in any other Oceananigans model, including Hydrostatic + Implicit Free Surface or the NonHydrostatic Model",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1985#issuecomment-920473363:331,race condition,race condition,331,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1985#issuecomment-920473363,2,['race condition'],['race condition']
Performance,"The logic should be reversed, we should have to include ""compute_tendencies=false"" as an _optimization_. Rather than what's implemented here, which does the unsafe thing by default. The problem with this logic is that it makes it harder to implement new models. The optimizations should be the optional thing basically, if one is naive, then things should work even if they are slower than they could be",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3741#issuecomment-2313416515:266,optimiz,optimizations,266,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3741#issuecomment-2313416515,1,['optimiz'],['optimizations']
Performance,The module loading sequence I was running was before that didn't work were; ```; module --force purge; module load ncarenv/23.10 gcc/12.2.0; module load ncarcompilers/1.0.0; module load netcdf/4.9.2 openmpi/4.1.6 ; module load julia/1.9.2; module load cuda/12.2.1; ```; if needed I can see if I can find the versions of the modules that ended up working. There are no other major difference in how I load in the packages or Julia. Let me know if you have any questions,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2258742898:11,load,loading,11,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2258742898,7,['load'],"['load', 'loading']"
Performance,"The only parts of the solver algorithm that require communication between nodes are `norm` and `dot`, right?. The ""matrix product"" --- a Poisson operator for us --- has unavoidable communication as well. But, we should be able to keep this limited in scope and we only need 1 halo. The trickier part where I think there is room for significant optimization is in the development of an effective multi-GPU preconditioner that is also efficient in parallel.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3803#issuecomment-2386795222:344,optimiz,optimization,344,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3803#issuecomment-2386795222,1,['optimiz'],['optimization']
Performance,"The preconditioner we used at high resolution is an asymptotic approximation of the inverse matrix that assumes that the original matrix is diagonally dominant. This is an increasingly worst approximation as you increase the resolution, deteriorating the performance when cells size is decreased",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2753#issuecomment-1257981531:255,perform,performance,255,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2753#issuecomment-1257981531,1,['perform'],['performance']
Performance,"The problem is I haven't yet figured out where in `weno_interpolants.jl` the code relies on very heavy bounds checking, so running julia with or without `--check-inbounds=no` changes the performance radically. . It's probably not too hard to identify where the problem is, but I have been missing it and I also find myself in a moment in which I have very little time. On the other hand, this PR fixes a lot of problems, so all the next PRs are kind of contingent on this one. I can wait a little bit more to merge this...",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2603#issuecomment-1185571576:187,perform,performance,187,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2603#issuecomment-1185571576,1,['perform'],['performance']
Performance,"The problem is not only with the `Nothing`, it's that the reduced fields are effectively launched with a reduced grid size . for example the west boundary kernel launch for a `(Center, Center, Center)` field will be of size `Ny*Nz`; for a `(Center, Center, Nothing)` it will be of size `Ny`. For the moment I just filter the reduced fields and do them individually. I don't think it will create too much problem. This is only an optimization for time-stepping where we do not have too many reduced fields (only free surface I think)",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2335#issuecomment-1065523937:429,optimiz,optimization,429,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2335#issuecomment-1065523937,1,['optimiz'],['optimization']
Performance,"The problem is not with the reduction but with derivatives that act on reduced fields.; The reductions are not an issue, because by performing a reduction we know what operation leads us to a reduced field, so we can perform the reduction accordingly (for example we exclude immersed cells from reductions). . When performing a derivative we use all three indices regardless of the field being reduced or not. ; In this case we get a funky result where we are trying to evaluate a derivative at `k == 1` for a reduced field that does not necessarily live at `k == 1`. In other words, the assumption that the field lives at `k == 1` is wrong for a reduced field; the right solution, on the other hand, is not so clear cut:. If we assume that reduced fields ""lives"" nowhere in the reduced direction, then we can remove the k index and just perform an ""immersed-boundary-unaware"" derivative. On the other hand, if the reduced field lives on the whole reduced column (like for example an integral) then we need to be aware of the ""immersed column"", because if the whole column is immersed then the derivative should return a zero.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3588#issuecomment-2097124708:132,perform,performing,132,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3588#issuecomment-2097124708,4,['perform'],"['perform', 'performing']"
Performance,The problems seen in the global simulation were actually fixed by #2774. The test was performed without updating the code to the latest version. I will close this PR,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2778#issuecomment-1279612571:86,perform,performed,86,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2778#issuecomment-1279612571,1,['perform'],['performed']
Performance,"The regression tests take a long time just because there are quite a few tests. We recently added many new hydrostatic regression tests (10-20? or more?) where there used to be just a handful. The solver tests have always been a bottleneck but I think there are a few new solvers being tested which probably increased the test length? I'm not sure why `time_stepping_2` would have increased (possibly that was always a bit slow). I definitely agree that CI time is a problem, which is why we opened #1962 to move the CI over to Caltech's central cluster and to split tests into a ""nightly"" category (running every night rather than on every PR / commit), and a per-commit category. However, we haven't made much progress on that PR.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2139#issuecomment-1009058943:229,bottleneck,bottleneck,229,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2139#issuecomment-1009058943,1,['bottleneck'],['bottleneck']
Performance,"The results in [A simple algorithm to improve the performance of the WENO scheme on non-uniform grids](https://link.springer.com/article/10.1007/s10409-017-0715-2) show that WENO reverts to 2nd-order accuracy on stretched grids. So its not ""incorrect"", just less accurate. However, for any given problem, the convergence rate is only one aspect of the accuracy of a solution. It's possible that WENO5 is still more accurate than any other numerical scheme, even if the solution only _converges_ to the true solution at a 2nd order rate. It also seems likely that this is true for any reconstruction stencil. I'm not sure how much attention is paid to this issue in other codes.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1704#issuecomment-967312861:50,perform,performance,50,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1704#issuecomment-967312861,1,['perform'],['performance']
Performance,The results over at https://github.com/glwagner/multithreaded-stencils suggest that we could be leaving as much as 2x on the table for CPU performance by choosing a non-optimal group size for multithreading. This PR changes the group size on the CPU. @hennyg888 could be good to runs some benchmarks to see if this improves our CPU / multithreading situation!,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1902:139,perform,performance,139,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1902,1,['perform'],['performance']
Performance,"The second suggestion is what I am proposing. There are concomitant changes that must be in the function definition. Using `solver.storage` for both `RHS` and `solution` is fine --- I understand that the objective is to minimize memory usage, which seems to require a compromise in the code design. Changing the function signature of `solve_poisson_equation!`, however, costs nothing in performance or memory usage and will increase the utility and readability of the `solve_poisson_equation!` function. You could rename `solution` and `RHS` to `φ` and `R`, and explain what the function does by writing that it solves `Δφ = R` --- unless `solution` and `RHS` are easier to explain via the docstring for some other reason.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/594#issuecomment-577202115:387,perform,performance,387,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/594#issuecomment-577202115,1,['perform'],['performance']
Performance,"The solution is not too difficult, except for one annoying detail: if boundary conditions contain references to model fields, this means that `field.boundary_conditions` contains a circular reference. I guess we can work around this by giving boundary conditions a reference to field data, rather than the field itself. This preserves existing functionality and so its fine. We will, however, have to rewrite `VelocityFields` and `TracerFields` to a single `VelocityAndTracerFields` constructor, and perform the requisite gymnastics there. There are also challenges if users want to create `VelocityFields` and pass them to the model. I suppose we'll have to re-wrap the user-created data for that.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/971#issuecomment-708092162:500,perform,perform,500,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/971#issuecomment-708092162,1,['perform'],['perform']
Performance,The tests have sped up a lot (presumably because compile time is so much better with KA 0.9). This means that the docs build is a much bigger bottleneck. I think we're at 2h48m:. https://buildkite.com/clima/oceananigans/builds/11123#01879a9c-fde8-4691-a3f5-29ef6060e288. so we will see what we get down to.,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3075#issuecomment-1515556985:142,bottleneck,bottleneck,142,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3075#issuecomment-1515556985,1,['bottleneck'],['bottleneck']
Performance,"The tests in `test_cubed_spheres.jl` were not used (the file was not included). Therefore, the tests on vector rotation were not performed. This PR includes these tests in a new file on a coarse cubed sphere grid. ; @navidcy and @siddharthabishnu are there any tests in `test_cubed_spheres.jl` we want to salvage?. Another problem is the inclusion of `OrthogonalSphericalShellGrids` in the tests. This has caused a bit of problems because of circular dependency. . The inclusion of `OrthogonalSphericalShellGrids` in the tests is to have a non-trivial `OrthogonalSphericalShellGrid` in the tests. For the moment, however, it is used only in the vector rotation test (which was not performed anyways), which is covered by using a conformal cubed sphere. . For this reason thought to remove this dependency here since at the moment does not add anything, and I will open an issue to discuss which non-trivial OrthogonalSphericalShellGrid we want to build to test the OrthogonalSphericalShellGrid capabilities.",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3881:129,perform,performed,129,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3881,2,['perform'],['performed']
Performance,"The tests should run with `--check-bounds=yes` automatically. I think that when calling `Pkg.test()` the testing environment automatically activates the 0 optimization flag (`-O0`) and the bounds checking flag `--check-bounds=yes`, but I cannot find a reference at the moment.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3747#issuecomment-2318384101:155,optimiz,optimization,155,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3747#issuecomment-2318384101,1,['optimiz'],['optimization']
Performance,"The way I implemented it, the matrix ""constructors"" are saved in the solver without the variable diagonal term. Basically the constructors include all the terms except for the term that varies with the time step which is saved in `solver.diagonal`. ; i.e. the `Dᵢⱼₖ` in; ```; Axᵢ₊₁ ηᵢ₊₁ + Axᵢ ηᵢ₋₁ + Ayⱼ₊₁ ηⱼ₊₁ + Ayⱼ ηⱼ₋₁ + Azₖ₊₁ ηₖ₊₁ + Azₖ ηⱼ₋₁ ; - 2 ( Axᵢ₊₁ + Axᵢ + Ayⱼ₊₁ + Ayⱼ + Azₖ₊₁ + Azₖ ) ηᵢⱼₖ ; + ( Cᵢⱼₖ + Dᵢⱼₖ/Δt^2 ) ηᵢⱼₖ = b; ```. Every time that the time step changes this is the operation that is performed:; ```; constructors = deepcopy(solver.matrix_constructors); M = prod(solver.problem_size); update_diag!(constructors, arch, M, M, solver.diagonal, Δt, 0); solver.matrix = arch_sparse_matrix(arch, constructors) ; ```. So the matrix is constructed from a copy of the updated constructors. In this way there is no need to subtract anything to the diagonal because `solver.matrix_constructors` does not include the diagonal term. . `update_diag!` adds `Dᵢⱼₖ/Δt^2` to the diagonal elements. . If you want to do the something analogous with the linear operation, remove the time dependent term from it. Then add it separately with `update_diag!` every time the time step changes. Of course you want to make sure that you have elements in your diagonal, non trivial if `Cᵢⱼₖ` because a sparse matrix removes 0 elements. check out this function ; `ensure_diagonal_elements_are_present!(sparse_matrix)`",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2396#issuecomment-1179415403:509,perform,performed,509,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2396#issuecomment-1179415403,1,['perform'],['performed']
Performance,"The way examples are written now is that there is the there is a **part 1** that sets up and runs a simulation and saves output. Then, **part 2** loads the `.jld2`/`.nc` file and plots an animations or what not. However, the way part 2 is currently coded up in most examples *it requires that part 1 is run** as it references on things like `grid` or `fields` constructed by part 1. E.g., ; https://github.com/CliMA/Oceananigans.jl/blob/3c930b00f0ccfdcc20556a9b6ac888b63aeb58d2/examples/eady_turbulence.jl#L317-L324. Ideally, one would like to be able to run the ""load `.jld2` file and animate"" independently. That's the whole idea, right? At the moment the way I do it is that I have to run part 1 until **just before** the `run!(simulation)`, make sure I overwrite the `.jld2` placeholder file that is created with the one that has been populated by some other run previously, and then move on with loading output and animating. Thoughts?",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1850:146,load,loads,146,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1850,3,['load'],"['load', 'loading', 'loads']"
Performance,"Then you can just load in your ""bottom"" from the `ImmersedBoundaryGrid` and set that to; ```; bottom[ bottom .>0 ] .= NaN; bottom[ bottom .<0 ] .= 0.0; ```; then when you lift the variable; ```; ζ′ = @lift file[""timestepper/ζ′/ *string($iter][:, 1:end-1, 1] .+ bottom; ```; (it is not going to be in the exact same position since `ζ′` is at faces and bottom is at centers, but on a 1/4 degree resolution I don't think makes much of a change)",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2522#issuecomment-1128082953:18,load,load,18,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2522#issuecomment-1128082953,1,['load'],['load']
Performance,"There appears to be the possibility of a write/write race condition at the corners in this case. Discovered with @sandreza. No bug has yet been detected, however.",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1938:53,race condition,race condition,53,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1938,1,['race condition'],['race condition']
Performance,"There are some errors that occurred right away, and I copied one below. Is this because of something in this PR or because of the system perhaps?. ```. ERROR: The following 1 direct dependency failed to precompile:; --;   |  ;   | Oceananigans [9e8cae18-63c1-5223-a75c-80ca9d6e9a09];   |  ;   | Failed to precompile Oceananigans [9e8cae18-63c1-5223-a75c-80ca9d6e9a09] to /storage5/buildkite-agent/.julia-3746/compiled/v1.6/Oceananigans/jl_IfcPYz.;   | ERROR: LoadError: LoadError: SystemError: opening file ""/var/lib/buildkite-agent/builds/tartarus-7/clima/oceananigans/src/Advection/upwind_biased_first_order.jl"": No such file or directory;   | Stacktrace:;   | [1] systemerror(p::String, errno::Int32; extrainfo::Nothing);   | @ Base ./error.jl:168;   | [2] #systemerror#62;   | @ ./error.jl:167 [inlined];   | [3] systemerror;   | @ ./error.jl:167 [inlined];   | [4] open(fname::String; lock::Bool, read::Nothing, write::Nothing, create::Nothing, truncate::Nothing, append::Nothing);   | @ Base ./iostream.jl:293;   | [5] open;   | @ ./iostream.jl:282 [inlined];   | [6] open(f::Base.var""#326#327""{String}, args::String; kwargs::Base.Iterators.Pairs{Union{}, Union{}, Tuple{}, NamedTuple{(), Tuple{}}});   | @ Base ./io.jl:328;   | [7] open;   | @ ./io.jl:328 [inlined];   | [8] read;   | @ ./io.jl:434 [inlined];   | [9] _include(mapexpr::Function, mod::Module, _path::String);   | @ Base ./loading.jl:1166;   | [10] include(mod::Module, _path::String);   | @ Base ./Base.jl:386;   | [11] include(x::String);   | @ Oceananigans.Advection ~/builds/tartarus-7/clima/oceananigans/src/Advection/Advection.jl:1;   | [12] top-level scope;   | @ ~/builds/tartarus-7/clima/oceananigans/src/Advection/Advection.jl:43;   | [13] include(mod::Module, _path::String);   | @ Base ./Base.jl:386;   | [14] include(x::String);   | @ Oceananigans ~/builds/tartarus-7/clima/oceananigans/src/Oceananigans.jl:1;   | [15] top-level scope;   | @ ~/builds/tartarus-7/clima/oceananigans/src/Oceananigans.jl:173;   | [16] i",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1957#issuecomment-904165134:459,Load,LoadError,459,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1957#issuecomment-904165134,2,['Load'],['LoadError']
Performance,"There is a fair amount of scalar iteration right now, largely I think because we have array-like objects (Fields, and friends) that lack fully-featured broadcasting capabilities. This means writing something like `a .== 2` triggers scalar iteration on the GPU because it hits Base broadcasting. We _can_ fix the problem by fleshing out broadcasting a bit so `a .== 2` works ""correctly"" / sensibly, but we haven't prioritized it (we also didn't have broadcasting at all for Field until a month or two ago). Possibly, if changing a global via `allowscalar(true)` were not available we would have been forced to address this deficiency in our Field abstraction sooner. So that could have been a good thing depending on your perspective. It did allow us to sweep some things under the hood. On the other hand I don't think we have any performance issues; scalar iteration is only used on very small arrays for testing where we are completely dominated by compile times.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1740#issuecomment-864145884:831,perform,performance,831,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1740#issuecomment-864145884,1,['perform'],['performance']
Performance,"There is still a problem. This time with the vector size:. > ┌ Warning: You appear to be using MPI.jl with the default MPI binary on a cluster.; │ We recommend using the system-provided MPI, see the Configuration section of the MPI.jl docs.; └ @ MPI ~/.julia/packages/MPI/08SPr/deps/deps.jl:15; [NVBLAS] No Gpu available; [NVBLAS] NVBLAS_CONFIG_FILE environment variable is NOT set : relying on default config filename 'nvblas.conf'; [NVBLAS] Cannot open default config file 'nvblas.conf'; [NVBLAS] Config parsed; [NVBLAS] CPU Blas library need to be provided; [ Info: Oceananigans will use 16 threads; ERROR: LoadError: DimensionMismatch(""arrays could not be broadcast to a common size; got a dimension with lengths 129 and 128""); Stacktrace:; [1] _bcs1; @ ./broadcast.jl:501 [inlined]; [2] _bcs (repeats 3 times); @ ./broadcast.jl:495 [inlined]; [3] broadcast_shape; @ ./broadcast.jl:489 [inlined]; [4] combine_axes; @ ./broadcast.jl:484 [inlined]; [5] instantiate; @ ./broadcast.jl:266 [inlined]; [6] materialize(bc::Base.Broadcast.Broadcasted{Base.Broadcast.DefaultArrayStyle{3}, Nothing, typeof(*), Tuple{Array{Float64, 3}, Array{Float64, 3}}}); @ Base.Broadcast ./broadcast.jl:883; [7] top-level scope; @ /lustre/scratch5/.mdt0/fspereira/OCEANANIGANS/test/case13/c16_128_128m.jl:200. Also, shouldn't we set v (last line below, I added the commented v's):. `Random.seed!(1414). T = model.tracers.T; u, v, w = model.velocities. x, y, z = nodes(T, reshape=true); Lz = model.grid.Lz. shape = @. z / Lz * (1 + z / Lz); ΞT = randn(size(T)...) .* shape; Ξu = randn(size(u)...) .* shape; #Ξv = randn(size(v)...) .* shape; Ξw = randn(size(w)...) .* shape. Tᵢ = @. 20 + dTdz * z + dTdz * Lz * 1e-6 * ΞT; uᵢ = @. sqrt(abs(Qᵘ)) * 1e-3 * Ξu; #vᵢ = @. sqrt(abs(Qᵘ)) * 1e-3 * Ξv; wᵢ = @. sqrt(abs(Qᵘ)) * 1e-3 * Ξw. #set!(model, u=uᵢ, v=vᵢ, w=wᵢ, T=Tᵢ, S=35); set!(model, u=uᵢ, w=wᵢ, T=Tᵢ, S=35)`",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2766#issuecomment-1268613743:610,Load,LoadError,610,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2766#issuecomment-1268613743,1,['Load'],['LoadError']
Performance,"There seems to be some performance regression as we now perform tons of little memory allocations somewhere. Model still time steps as fast as it should so we probably haven't noticed it as we run large model sizes where this allocations are kind of hidden. Seems like mainly a CPU issue. CPU: ; ```; julia> using Oceananigans. julia> model = Model(N=(128, 128, 128), L=(1, 1, 1));. julia> @time time_step!(model, 1, 1); 16.614982 seconds (34.03 M allocations: 1.775 GiB, 6.86% gc time). julia> @time time_step!(model, 1, 1); 0.738576 seconds (1.31 M allocations: 264.089 MiB, 3.97% gc time); ```. GPU:; ```; julia> using Oceananigans. julia> model = Model(N=(256, 256, 256), L=(1, 1, 1), arch=GPU());. julia> @time time_step!(model, 1, 1);; 28.425740 seconds (71.04 M allocations: 3.383 GiB, 10.72% gc time). julia> @time time_step!(model, 1, 1);; 0.025893 seconds (8.90 k allocations: 978.531 KiB); ```",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/397:23,perform,performance,23,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/397,2,['perform'],"['perform', 'performance']"
Performance,"There was a bug in the earlier v4 releases so Oceananigans won't work with them (this has come up as CUDAv4.0 is the final version compatible with CUDA 10.2). ```julia; julia> using Oceananigans; ┌ Warning: The NVIDIA driver on this system only supports up to CUDA 10.2.0.; │ For performance reasons, it is recommended to upgrade to a driver that supports CUDA 11.2 or higher.; └ @ CUDA ~/.julia/packages/CUDA/ZdCxS/src/initialization.jl:71; julia> grid = RectilinearGrid(GPU(); size = (128, 128, 128), extent = (1, 1, 1)); mo128×128×128 RectilinearGrid{Float64, Periodic, Periodic, Bounded} on GPU with 3×3×3 halo; ├── Periodic x ∈ [0.0, 1.0) regularly spaced with Δx=0.0078125; ├── Periodic y ∈ [0.0, 1.0) regularly spaced with Δy=0.0078125; └── Bounded z ∈ [-1.0, 0.0] regularly spaced with Δz=0.0078125. julia> model = NonhydrostaticModel(; grid); ERROR: UndefVarError: `CUDABackend` not defined; Stacktrace:; [1] getproperty; @ ./Base.jl:31 [inlined]; [2] device(#unused#::GPU); @ Oceananigans.Architectures ~/.julia/packages/Oceananigans/f5Cpw/src/Architectures.jl:39; [3] launch!(::GPU, ::RectilinearGrid{Float64, Periodic, Periodic, Bounded, Float64, Float64, Float64, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, GPU}, ::Oceananigans.Utils.KernelParameters{:xy, (0, 0)}, ::typeof(Oceananigans.BoundaryConditions._fill_bottom_and_top_halo!), ::Tuple{OffsetArrays.OffsetArray{Float64, 3, CUDA.CuArray{Float64, 3, CUDA.Mem.DeviceBuffer}}, OffsetArrays.OffsetArray{Float64, 3, CUDA.CuArray{Float64, 3, CUDA.Mem.DeviceBuffer}}, OffsetArrays.OffsetArray{Float64, 3, CUDA.CuArray{Float64, 3, CUDA.Mem.DeviceBuffer}}}, ::Vararg{Any}; include_right_boundaries::Bool, r",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3402:280,perform,performance,280,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3402,1,['perform'],['performance']
Performance,"There were a bunch of bugs in `shallow_water_model.jl`. The reason for the error. > ERROR: LoadError: LoadError: LoadError: LoadError: UndefVarError: NoImmersedBoundary not defined. is because there was a line. ```julia; using Oceananigans.ImmersedBoundaries: NoImmersedBoundary; ```. in the file `src/Models/ShallowWaterModels/shallow_water_model.jl`. Since `NoImmersedBoundary` is not defined (there is no name `NoImmersedBoundary` in the module `ImmersedBoundaries`, or anywhere else in the code), this line threw an error. I am not sure if I found all the bugs in `shallow_water_model.jl`; more debugging may be necessary.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1663#issuecomment-843325766:91,Load,LoadError,91,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1663#issuecomment-843325766,4,['Load'],['LoadError']
Performance,"There's a bug in `adapt_advection_order` (or the constructor for `HydrostaticFreeSurfaceModel`, since this code:. ```julia; using Oceananigans; grid = RectilinearGrid(size=1, x=(0, 1), topology=(Periodic, Flat, Flat)); tracer_advection = (b=nothing, c=WENO()); model = HydrostaticFreeSurfaceModel(; grid, tracer_advection); ```. fails with. ```; ERROR: LoadError: MethodError: no method matching adapt_advection_order(::@NamedTuple{…}, ::Int64, ::RectilinearGrid{…}). Closest candidates are:; adapt_advection_order(::Nothing, ::Int64, ::Oceananigans.Grids.AbstractGrid); @ Oceananigans ~/Projects/Oceananigans.jl/src/Advection/adapt_advection_order.jl:60; adapt_advection_order(::UpwindBiased{B}, ::Int64, ::Oceananigans.Grids.AbstractGrid) where B; @ Oceananigans ~/Projects/Oceananigans.jl/src/Advection/adapt_advection_order.jl:74; adapt_advection_order(::WENO{B, FT, XT, YT, ZT}, ::Int64, ::Oceananigans.Grids.AbstractGrid) where {B, FT, XT, YT, ZT}; @ Oceananigans ~/Projects/Oceananigans.jl/src/Advection/adapt_advection_order.jl:92; ```",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3863:353,Load,LoadError,353,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3863,1,['Load'],['LoadError']
Performance,"There's some info in CUDA docs but it doesn't explicitly answer my question:. > For small integer powers (e.g., x2 or x3), explicit multiplication is almost certainly faster than the use of general exponentiation routines such as pow(). While compiler optimization improvements continually seek to narrow this gap, explicit multiplication (or the use of an equivalent purpose-built inline function or macro) can have a significant advantage. This advantage is increased when several powers of the same base are needed (e.g., where both x2 and x5 are calculated in close proximity), as this aids the compiler in its common sub-expression elimination (CSE) optimization. So maybe we shouldn't square at all here...",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1770#issuecomment-870017593:252,optimiz,optimization,252,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1770#issuecomment-870017593,2,['optimiz'],['optimization']
Performance,"These benchmarks:. https://github.com/CliMA/Oceananigans.jl/pull/2412. show a factor of 2 speed up on the CPU. In principle, the FFT is faster than a matrix multiply which may explain it? Or, perhaps the benchmarks test the wrong thing so we should re-run them. As explained on that PR, some performance is left on the table, because the fastest combination would use an FFT-based preconditioner with a matrix multiply to compute the LHS.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2635#issuecomment-1172613179:292,perform,performance,292,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2635#issuecomment-1172613179,1,['perform'],['performance']
Performance,"Thinking about this more, I think it would make sense to make an even more radical change. I think we should add `diagnostics`, `output_writers`, and `clock` to `Simulation`. . The `time_step!` function then performs a single time-step, whereas to run a simulation one should write `run!(simulation)`, which handles diagnostics, output writing, adaptive time-stepping, and logging in an integrated way. This orthogonalizes the design a bit: `diagnostics` and `output_writers` are not really aspects of a ""model"", if we use a narrow interpretation of a model as a discrete representation of a physical system. A single physical system might conceivably be associated with a wide range of disparate diagnostics and output, depending on what kind of science is being done. I think scripts become clearer. The user writes. ```julia; model = Model(; model_parameters...). simulation = Simulation(model; simulation_parameters...). simulation.diagnostics[:diag] = # something. run!(simulation); ```. As an example to illustrate why `Simulation` is orthogonal to `Model`, here's a possible clear and coherent usage of this separation:. ```julia; model = Model(; model_parameters...). set!(model; first_interesting_initial_condition...). first_simulation = Simulation(model, first_simulation_parameters...); first_simulation[:diag] = diag_specific_to_first_simulation; run!(first_simulation). set!(model; second_interesting_initial_condition...) # same physical model, but different starting initial condition... no new memory allocated, no recompilation --- fast. second_simulation = Simulation(model, second_simulation_parameters...); second_simulation[:diag] = diag_specific_to_second_simulation; run!(second_simulation); ```. We can use `run!(simulation, time_steps=nsteps)` to allow hand-coded user loops that achieve a functionality similar to what we have now.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/447#issuecomment-542294442:208,perform,performs,208,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/447#issuecomment-542294442,1,['perform'],['performs']
Performance,"This PR ""unrestricts"" (removes type annotations) from second and fourth derivative operators. These annotations are unnecessary, since the second and fourth derivative operators call first-derivative operators that perform appropriate dispatch. Also, the second vertical derivative operators are general (within the scope of current Oceananigans code) and should not be restricted. @jm-c does this fix your problem?",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1936:215,perform,perform,215,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1936,1,['perform'],['perform']
Performance,This PR (will) implement(s) pre-computed nonlinear turbulent diffusivities for large eddy simulation. It also performs some minor clean-up for the model constructor to prevent boiler plate proliferation.,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/312:110,perform,performs,110,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/312,1,['perform'],['performs']
Performance,"This PR adds a NaN checker that checks for NaNs in the `[1, 1, 1]` grid point of the velocity fields at every time step (which should be very cheap and not affect performance). When a NaN is detected, an `ErrorException` is thrown so when running in the REPL it'll error and return control to the REPL, while if running a script from the terminal, it will terminate the Julia session (seems like the behavior we all want). Resolves #938; Resolves #1196",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1198:163,perform,performance,163,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1198,1,['perform'],['performance']
Performance,"This PR adds a `TurbulenceClosures` module to `Oceananigans`. At the moment nothing in the `Oceananigans` algorithm is touched --- the module is simply loaded and exported. I suggest that we keep it this way, given the size of this PR, and work on integrating the code into `Oceananigans` in a future PR. A simple incremental test in that future PR will be to replace the diffusive operators with the ones here and ensure that the regression tests still pass. Ultimately `TurbulenceClosures` will provide code for all the different diffusive terms we want to provide for `Oceananigans` users. The current design is that a 'closure' must provide a diffusive flux divergence term for `Oceananigans`. These diffusive flux terms are. * `∇_κ_∇ϕ(i, j, k, grid, ϕ, closure, u, v, w, T, S)` for a scalar `ϕ`; * `∂ⱼ_2ν_Σ₁ⱼ(i, j, k, grid, closure, u, v, w, T, S)` for x momentum; * `∂ⱼ_2ν_Σ₂ⱼ(i, j, k, grid, closure, u, v, w, T, S)` for y momentum; * `∂ⱼ_2ν_Σ₃ⱼ(i, j, k, grid, closure, u, v, w, T, S)` for z momentum. This will have to be generalized if we wish to provide more sophisticated turbulence closures with, for example, backscatter. I am open to changing the name of the diffusive flux divergences. This PR introduces the closures:. * `ConstantSmagorinsky` (with no buoyancy modification --- yet); * `ConstantIsotropicDiffusivity`; * `DirectionalDiffusivity` (with different horizontal and vertical diffusivities --- for lack of a better term). There is also an abstraction --- we have `ScalarDiffusivity`s and `TensorDiffusivity`s. The `DirectionalDiffusivity` is an example of a tensor diffusivity. I would like to add docs before merging. Please review the code and let me know what can be improved while I work on docs, and suggest improvements to the doc strings. There are some unit tests included in this PR. Please take a look and suggest new ones. There is significant notation associated with this PR. I don't think we need to finalize the notation here, but comments are welcome. This PR m",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/234:152,load,loaded,152,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/234,1,['load'],['loaded']
Performance,"This PR adds a benchmarking script that times how long a single time step takes on the CPU and GPU using `Float32` and `Float64` for various model resolutions. It uses TimerOutputs.jl to nicely format the benchmarks. It also prints out CPU->GPU speedups and Float64->Float32 ""speedups"". It only executes the GPU benchmarks if executed on a CUDA-enabled machine. We can later extend it to time model initialization, different parts of the time stepping, etc. Right now it only benchmarks a simple ""static ocean"" configuration so no fancy forcing functions are used, but we can extend the number of scenarios/experiments we benchmark. The time stepping and Poisson solver still takes the same amount of time whether the ocean is static or active. Resolves #67. Well, kind of. It's not clear to me how to easily incorperate this with CI in a way that doesn't involve one of us eyeballing the text output. For now we should at least run this script every time we make a change that might potentially impact performance. Example output; ```; ──────────────────────────────────────────────────────────────────────────────────────────────────; Oceananigans.jl benchmarks Time Allocations; ────────────────────── ───────────────────────; Tot / % measured: 718s / 46.6% 17.2GiB / 0.02%. Section ncalls time %tot avg alloc %tot avg; ──────────────────────────────────────────────────────────────────────────────────────────────────; 256x256x256 static ocean (CPU, Float32) 10 168s 50.2% 16.8s 20.3KiB 0.73% 2.03KiB; 256x256x256 static ocean (CPU, Float64) 10 141s 42.3% 14.1s 20.3KiB 0.73% 2.03KiB; 128x128x128 static ocean (CPU, Float32) 10 12.4s 3.72% 1.24s 14.5KiB 0.52% 1.45KiB; 128x128x128 static ocean (CPU, Float64) 10 9.00s 2.69% 900ms 14.8KiB 0.54% 1.48KiB; 64x 64x 64 static ocean (CPU, Float32) 10 1.03s 0.31% 103ms 14.2KiB 0.51% 1.42KiB; 256x256x256 static ocean (GPU, Float64) 10 891ms 0.27% 89.1ms 333KiB 12.0% 33.3KiB; 256x256x256 static ocean (GPU, Float32) 10 859ms 0.26% 85.9ms 329KiB 11.9% 32",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/116:1003,perform,performance,1003,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/116,1,['perform'],['performance']
Performance,"This PR adds a near-global one degree setup to `validation/near_global_lat_lon`. This is mostly @sandreza and @simone-silvestri 's work. The setup spans -75S to 75N and is implemented on a regular `LatitudeLongitudeGrid` with 1 degree horizontal resolution and 50 vertical levels. It's WIP right now partly because @sandreza is still sorting out some difficulties with including a `IsopycnalSkewSymmetricDiffusivity` closure to this setup. In addition, I'm going to update the `closure_tuples.jl` implementation because this setup may require a tuple of 5 closures. As a teaser, the file `validation/near_global_lat_lon/one_degree_setups/plot_bathymetry.jl` produces. ![image](https://user-images.githubusercontent.com/15271942/164785636-5bbf0e2d-0e0c-458e-9c92-758afbaf96d3.png). The bathymetry file is small (not much larger than few dozen long text files), so I've uploaded it directly into the repo from this PR. If we don't want to do this, I can reopen this PR with the same `.jl` files and download the bathymetry from `OceananigansArtifacts.jl` instead. I'm thinking that we should add at least two one degree setups in this PR:. 1. A ""realistic"" setup with initial conditions and surface relaxation or fluxes loaded from file (which @sandreza is working on). 2. An ""idealized"" setup that uses idealized and constant momentum and temperature forcing (perhaps with `constant_salinity=35.0`). and possibly others. Based on this experience, we can go in a similar direction with the 1/4 degree setup.",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2463:1218,load,loaded,1218,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2463,1,['load'],['loaded']
Performance,"This PR adds grid spacings to NetCDF out, which is a step closer to solving https://github.com/CliMA/Oceananigans.jl/issues/1334. EDIT:. The goal here is to add grid metrics to NetCDF output. The are two main avenues to follow:. 1. We can follow Oceananigans nomenclature and conventions, which would make the output play more nicely with Oceananigans itself (and more generally in the Julia environment).; 2. We can follow standard community conventions, which would mean the output won't follow Oceananigans naming etc., but it would optimize its readability by other software. I think we should follow option 2, since if a user wants to work with the output in Oceananigans/Julia, then using JLD2 output is probably the right choice anyway. Given that most people in the community use Python, `xarray` and `xgcm` to analyze model output, I think we should optimize the output to work with that ecosystem out of the box. Based on the discussion in https://github.com/CliMA/Oceananigans.jl/issues/1334, it seems the preferred conventions to use are the [SGRID conventions](https://sgrid.github.io/sgrid/). For the more technical aspects, I'm planning on starting with `RectilinearGrids` and `LatLonGrids` in this PR since these are more straightforward. And then we can expand from there. I also think this should be presented to the user as an opt-in flag in `NetCDFWriter` constructor, as opposed to being included in every NetCDF output by default.",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2652:536,optimiz,optimize,536,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2652,2,['optimiz'],['optimize']
Performance,"This PR builds off #2536 and implements a distributed Poisson solver that users horizontal FFTs and a vertical tridiagonal solve, with more help from @jipolanco. When distributed in (x, y), this kind of solver is more expensive than a pure FFT-based solver, because it requires 4 additional transpositions + communication. For problems that are only distributed in x _or_ y (eg, slab decomposition), we can avoid the additional transpositions. ~~Implementing that optimization is TODO for this PR.~~. Some of the details are discussed on https://github.com/jipolanco/PencilFFTs.jl/issues/44. Future work, which would require abstracting the implementation of hydrostatic pressure in `NonhydrostaticModel` (and, for friendliness, forbidding the use of VerticallyImplicitTimeDiscretization), could in principle support a more efficient version of this solver with pencil decomposition in (y, z) or (x, z). This memory layout would increase performance for very large problems that require a 2D domain decomposition, since decomposing in (y, z) or (x, z) reduces the number of transposes needed by 4 over (x, y). This feature is easy to code, but might take some time to test. We've already noticed on #1910 that lumping hydrostatic and nonhydrostatic pressure produces different (perhaps lower quality) solutions. TODO:; - [x] Implement a more efficient algorithm for 1D ""slab"" decompositions; - [x] Add tests",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2538:464,optimiz,optimization,464,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2538,2,"['optimiz', 'perform']","['optimization', 'performance']"
Performance,This PR contains some improvements that allow better scaling on multiple devices. . In particular:; - Splitting the interior active cell map into `west` `east` `north` `south` and `interior` to allow for overlapping communication and computations with _active_ cells.; - Advancements for the `SplitExplicitFreeSurface` to reduce kernel launch latency,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3404:343,latency,latency,343,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3404,1,['latency'],['latency']
Performance,"This PR converts `Simulation.stop_criteria` to `Callback`s. This change allows users to more easily add their own custom stop criteria as callbacks. ### Example. We're interested in running a suite of turbulence simulations involving a transition to turbulence. For this purpose, we'd like to stop a simulation whenever the turbulent kinetic energy exceeds some multiple of its initial value (since I'm interested mainly in the _transition_ to turbulence, rather than the ensuing dynamics. After this PR, this can be implemented by writing. ```julia; # Build a simulation. function turbulent_kinetic_energy_threshold_exceeded(sim). mean_tke = mean(tke_operation) # tke_operation is an `AbstractOperation` that computes turbulent kinetic energy. if mean_tke > 1e-6 # arbitrary threshold for this example!; @info ""Simulation is stopping because the turbulent kinetic energy threshold has been exceeded.""; sim.running = false; end. return nothing; end. # Because this computation is relatively expensive, we perform it every 100 iterations; simulation.callbacks[:tke_threshold] = Callback(turbulent_kinetic_energy_threshold_exceeded, IterationInterval(100)); ```. Previously, it wasn't useful to modify `sim.running` because it would be overwritten immediately. So before this PR, the only way to stop a simulation is to throw an error. This PR _could_ also convert the `NaNChecker` to a stop criterion. In fact we should probably do that. But feedback is welcome beforehand.",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2048:1005,perform,perform,1005,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2048,1,['perform'],['perform']
Performance,"This PR fixes a bug in the `FourierTridiagonalPoissonSolver` that almost certainly meant it produced incorrect answers. There were two concurrent bugs that cancelled each other out, which allowed tests to pass:. 1. The Laplacian operator (previously called `∇²`, now called `∇²ᶜᶜᶜ`) was incorrect, because ""Face"" and ""Center"" superscripts were swapped. In other words, the Laplacian operator was correct for an object located at `(Face, Face, Face)`, rather than `(Center, Center, Center)` as it was being applied.; 2. `ΔzF` and `ΔzC` were also swapped in derivation of the `FourierTridiagonalPoissonSolver`. This means that both the docs and the code were incorrect. The tests passed because these two bugs effectively cancel each other out. This PR fixes both bugs. There still may be a lingering issue however, because the docs multiply the entire Poisson equation by the vertical grid spacing (which in the docs is written `ΔzC`, but should be `ΔzF`), including the source term. I didn't see immediately where to replace `ΔzC` with `ΔzF` in `solve_poisson_equation!`. The tests may pass anyways because there is no test that incompressibility is maintained on a stretched grid. I also cleaned up the Laplacian operators a bit, and the grid spacing operators, and added a convenience function `set_source_term!` so that users don't have to know about the special formulation that `FourierTridiagonalPoissonSolver` uses. TODO:. - [x] Update the docs; - [x] Test incompressibility on a stretched grid",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1541:135,concurren,concurrent,135,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1541,1,['concurren'],['concurrent']
Performance,"This PR implements `ComputedField`. The concept is that the results of applying an abstract operation should be stored in a field-like object, since it has a location. This PR, along with #930, tidies up the way that abstract operations are computed and stored, how averages of fields are computed and stored, and how averages of abstract operations are handled. A key new function is `AveragedField(op::AbstractOperation; kwargs...)`, which first creates a `ComputedField`, and then returns an average over that `ComputedField`. Another key feature is that `compute!(comp::ComputedField` calls `compute!(comp.operand)`. In addition, `compute!(op::AbstractOperation)` is defined for all abstract operations. This means that abstract operations which themselves depend on either `ComputedField`s or `AveragedField`s can be computed correctly. In the future, however, some optimization is warranted to avoid ""recomputing"" certain fields. Once both the `JLD2OutputWriter` and the `NetCDFOutputWriter` have full support for all types of `AbstractField`, we can nuke both `Average` and `Computation` from the source and move to this new interface.",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/931:871,optimiz,optimization,871,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/931,1,['optimiz'],['optimization']
Performance,"This PR implements a minor performance optimization that changes the 'default' boundary condition type to `BoundaryCondition{Flux, Nothing}`. Using this type elides the adding of 0's in the case of zero flux boundary conditions. The functionality for eliding computation in this case was already implemented; however it was not used by default. . This may address #397, but @ali-ramadhan we need to benchmark.",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/402:27,perform,performance,27,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/402,2,"['optimiz', 'perform']","['optimization', 'performance']"
Performance,"This PR implements a third-order Runge-Kutta time stepping method. The implementation is based off [Le and Moin (1991)](https://www.sciencedirect.com/science/article/pii/0021999191902157). It should be noted, however, that we do not use an implicit method for diffusion, and that the pressure correction is calculated every substep. It could be possible in the future to implement the optimization described in Le and Moin (1991) that reduces the number of times the pressure Poisson equation needs to be solved each time-step from 3 to 1, though this would reduce the accuracy of the scheme from third to second-order. So far, this PR extends the dynamics tests and incompressibility test to `RungeKutta3TimeStepper`. It also extends the time stepper convergence test to the `RungeKutta3TimeStepper`. I am open to changing the name of the time stepper. It may also be a good idea to add basic documentation. The time stepper is used by setting the keyword `timestepper=:RungeKutta3` in the constructor for `IncompressibleModel`. ~~I am not sure if checkpointing will work with this timestepper.~~. Checkpointing will not work with this timestepper. A more generic checkpointer awaits a future PR. Resolves #506",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/945:385,optimiz,optimization,385,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/945,1,['optimiz'],['optimization']
Performance,"This PR implements a validation experiment on a latitude-longitude grid that spans from 84 S to 84 N, thus ""nearly"" covering the globe. Here's some stats:. * 128 by 60 by 18 resolution, and therefore 2.8 deg grid spacing in the horizontal with 200 m spacing in the vertical; * Realistic bathymetry with solid northern wall; * Prescribed wind stress and temperature flux that relaxes sea surface temperature to a target distribution; * Convective adjustment vertical diffusivity and laplacian background horizontal and vertical diffusivities. ### Yet to be implemented for this setup:. * Annual cycle for wind stress and target sea surface temperature (derived from monthly averaged data, not implemented yet); * Bottom drag (requires capability for immersed boundary fluxes, not implemented); * Gent-McWilliams and skew diffusivity and Redi symmetric isopycnal diffusivity; * CATKE vertical diffusivity (may come in future PR). ## Some visualization. ### Setup: bathymetry, vertically-integrated lateral areas used in free surface solver, boundary conditions. ![image](https://user-images.githubusercontent.com/15271942/138298149-3123d9aa-6b93-4dc7-a811-4e4d0330d686.png). ### Solution after 1 day with 20 second time-step. ![image](https://user-images.githubusercontent.com/15271942/138451026-0d74946b-a5d0-428f-aa15-5743ab05a282.png). We need a bit more work (perhaps a better vertical mixing scheme and stretched grid) to obtain better results at long times. ## Notes. * The implicit free surface solver is the major performance bottleneck for this setup; * With a 20 second time-step and max iterations of 10 for free surface solver, 100 years can be simulated in about 21 hours",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2023:1520,perform,performance,1520,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2023,2,"['bottleneck', 'perform']","['bottleneck', 'performance']"
Performance,"This PR implements an optimization that (potentially) avoids redundant recompilation of fields when `compute!(output)` is called in `fetch_output`. This optimization was discussed on #955 . The idea is to use `model.clock.time` as a ""key"" which is checked prior to computation. Fields that opt-in to avoid recomputation then check if `model.clock.time == field.status.time`. If the two are equal, no computation is performed. This will hopefully speed up expressions that involve `AveragedField`s, `ComputedField`s, and `BuoyancyField`. Note that in the case that users specify scratch space for a field, they must opt-in to this optimization by passing `recompute_safely=false` to the field constructor. If scratch space is not specified (and we therefore know it is unique), recomputation is avoided by default. This PR also changes the keyword `computed_data` to `operand_data` in the constructor for `AveragedField(op::AbstractOperation)`. . Todo:. - [x] Tests. Resolves #955 ; Resolves #967",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/968:22,optimiz,optimization,22,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/968,4,"['optimiz', 'perform']","['optimization', 'performed']"
Performance,"This PR implements two advancements for CATKE. First is a reformulation of the shear production term. Previously this was computed using the velocity field at time-step `n` in the TKE tendency, like all other terms in the velocity and tracer explicit tendencies. However, [Burchard (2002)](https://www.sciencedirect.com/science/article/pii/S1463500302000094?casa_token=3cKiqlICvN8AAAAA:Hg3iGP-1q_UYh2We7maQRb9z9F5blbp8lHiH61T_Gep7y0DG8VYVRWwGvzvCDlbD2iANNHbLp4g) argue that the shear production term should be formulated to _exactly_ conserve total kinetic energy, which means that it's form actually depends on the time-stepping scheme being used. In our case, temporally-discrete shear production requires using the velocity field at time step `n` and `n+1`. To implement this scheme we have to add some features to the time stepping routine. In particular we have to allow closures to tell `HydrostaticFreeSurfaceModel` not to step forward certain tracers. Next, we step forward the TKE within `compute_diffusivities!`. Second, this PR also implements split-explicit substepping for CATKE's TKE. It seems that this is required because even though the new shear production discretization dramatically stabilizes CATKE (allowing long time-steps stably), it does not render CATKE's solutions completely insensitive to the time-step. So, for _accuracy_ (and performance) purposes, we are motivated to also add the capability to advance the TKE on a short time-step while the velocities and tracers advance on a slower time-step. This only matters for high vertical resolutions and very strong forcing. But when you do global simulations, there is strong forcing somewhere on the Earth, and we probably want to represented it accurately... More documentation to come.",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3585:1357,perform,performance,1357,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3585,1,['perform'],['performance']
Performance,"This PR increases our support for `Flat` dimensions of `RegularCartesianGrid`. When topology in a direction is `Flat`, the halo size is set to 0, the grid size to 1, and halo filling, interpolation, and differences are all elided. Two new examples in the docstring for `RegularCartesianGrid` illustrate it's use:. * A two-dimenisional, horizontally-periodic grid:. ```julia; julia> using Oceananigans. julia> grid = RegularCartesianGrid(size=(32, 32), extent=(2π, 4π), topology=(Periodic, Periodic, Flat)); RegularCartesianGrid{Float64, Periodic, Periodic, Flat}; domain: x ∈ [0.0, 6.283185307179586], y ∈ [0.0, 12.566370614359172], z ∈ [0.0, 0.0]; topology: (Periodic, Periodic, Flat); resolution (Nx, Ny, Nz): (32, 32, 1); halo size (Hx, Hy, Hz): (1, 1, 0); grid spacing (Δx, Δy, Δz): (0.19634954084936207, 0.39269908169872414, 0.0); ```. * A one-dimensional ""column"" grid:. ```julia; julia> using Oceananigans. julia> grid = RegularCartesianGrid(size=256, z=(-128, 0), topology=(Flat, Flat, Bounded)); RegularCartesianGrid{Float64, Flat, Flat, Bounded}; domain: x ∈ [0.0, 0.0], y ∈ [0.0, 0.0], z ∈ [-128.0, 0.0]; topology: (Flat, Flat, Bounded); resolution (Nx, Ny, Nz): (1, 1, 256); halo size (Hx, Hy, Hz): (0, 0, 1); grid spacing (Δx, Δy, Δz): (0.0, 0.0, 0.5); ```. In principle we can also eliminate the pressure solver for column models. I'll leave this optimization for a future PR. Resolves #902 ; Resolves #940 ; Resolves #1001; Resolves #35",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1003:1361,optimiz,optimization,1361,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1003,1,['optimiz'],['optimization']
Performance,"This PR integrates Tim's changes from the GPU hackathon. By using dynamic launch configurations and splitting up the interior source term calculation kernel (+ https://github.com/JuliaGPU/CUDAnative.jl/pull/417) we get a pretty sweet ~25% overall speedup (so the most expensive kernel which was a bottleneck is sped up by ~40%). Needs https://github.com/vchuravy/GPUifyLoops.jl/pull/90 to work for now. cc @maleadt Sorry it took so long for me to merge these improvements. Actually resolves #64 . ```; ──────────────────────────────────────────────────────────────────────────────────────; Oceananigans.jl static ocean bench... Time Allocations ; ────────────────────── ───────────────────────; Tot / % measured: 409s / 28.8% 15.0GiB / 0.50% . Section ncalls time %tot avg alloc %tot avg; ─────────────────────────────────────────────────────────────────────────────────────; 256×256×256 (CPU, Float64) 10 56.0s 47.7% 5.60s 292KiB 0.37% 29.2KiB; 256×256×256 (CPU, Float32) 10 47.2s 40.2% 4.72s 227KiB 0.29% 22.7KiB; 128×128×128 (CPU, Float32) 10 5.91s 5.03% 591ms 227KiB 0.29% 22.7KiB; 128×128×128 (CPU, Float64) 10 5.87s 5.00% 587ms 292KiB 0.37% 29.2KiB; 64× 64× 64 (CPU, Float64) 10 803ms 0.68% 80.3ms 292KiB 0.37% 29.2KiB; 64× 64× 64 (CPU, Float32) 10 724ms 0.62% 72.4ms 227KiB 0.29% 22.7KiB; 256×256×256 (GPU, Float64) 10 309ms 0.26% 30.9ms 9.83MiB 12.9% 0.98MiB; 256×256×256 (GPU, Float32) 10 239ms 0.20% 23.9ms 8.70MiB 11.4% 891KiB; 32× 32× 32 (CPU, Float64) 10 86.6ms 0.07% 8.66ms 292KiB 0.37% 29.2KiB; 32× 32× 32 (CPU, Float32) 10 61.0ms 0.05% 6.10ms 227KiB 0.29% 22.7KiB; 32× 32× 32 (GPU, Float64) 10 50.7ms 0.04% 5.07ms 9.83MiB 12.9% 0.98MiB; 64× 64× 64 (GPU, Float64) 10 49.7ms 0.04% 4.97ms 9.83MiB 12.9% 0.98MiB; 128×128×128 (GPU, Float64) 10 46.9ms 0.04% 4.69ms 9.83MiB 12.9% 0.98MiB; 32× 32× 32 (GPU, Float32) 10 44.8ms 0.04% 4.48ms 8.70MiB 11.4% 891KiB; 128×128×128 (GPU, Float32) 10 40.6ms 0.03% 4.06ms 8.70MiB 11.4% 891KiB; 64× 64× 64 (GPU, Float32) 10 32.6ms 0.03% 3.26ms 8.70MiB 11.",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/302:297,bottleneck,bottleneck,297,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/302,1,['bottleneck'],['bottleneck']
Performance,"This PR introduces a new ""Matrix based"" implicit solver `Oceananigans/Solvers/matrix_iterative_solver` that uses the package IterativeSolvers.jl to solve a linear system of equation based on a sparse matrix formulation. This has a couple of advantages in terms of performance; - there is no need to `fill_halo_regions!` on the residuals while iterating; - it is possible to implement efficient preconditioning techniques. the `MatrixIterativeSolver` accepts a tuple of coefficients `Ax, Ay, Az, C, D` as inputs and creates the associated matrix to solve the following ; <img src=""https://render.githubusercontent.com/render/math?math=Ax_{i%2B 1jk} \eta_{i%2B 1jk} %2B Ax_{ijk} \eta_{i-1jk} %2B Ay_{ij%2B 1k} \eta_{ij%2B 1k} %2B Ay_{ijk} \eta_{ij-1k} %2B Az_{ijk%2B 1} \eta_{ijk%2B 1} %2B Az_{ijk} \eta_{ijk-1} - 2 (Ax_{i%2B 1jk} %2B Ax_{ijk} %2B Ay_{ij%2B 1k} %2B Ay_{ijk} %2B Az_{ijk%2B 1} %2B Az_{ijk} ) \eta_{ijk} %2B ( C_{ijk} %2B D_{ijk} /\Delta t^2 ) \eta_{ijk} = b_{ijk}""> . the coefficients are specified as 3D arrays (also fields should be good). To solve for a `Center, Center, Center` value, `Ax` should be on `Face, Center, Center`, `Ay` should be on `Center, Face, Center`, `Az` on `Center, Center, Face` and `C` and `D` on `Center, Center, Center`. . `b` (the rhs) is specified as a 1D `Array` (or `CuArray` on GPUs). Example: coefficients to solve a Poisson equation of the form <img src=""https://render.githubusercontent.com/render/math?math=\nabla^2 \eta = b""> in a finite volume framework <img src=""https://render.githubusercontent.com/render/math?math=\sum_k A_k \nabla \eta = b \cdot V""> are shown in `test/test_matrix_poisson_solver.jl`. to construct the type, a part from the coefficients it is possible to specify; - the type of `iterative_solver` desired (`cg` is the default) ; - `reduced_dim::Tuple(Bool, Bool, Bool)` to have a lower-dimensional solve (a reduced dimension will mean that the matrix will be filled with the coefficients of index 1 in that direction, as an exa",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2105:264,perform,performance,264,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2105,1,['perform'],['performance']
Performance,"This PR introduces a new regression test (and renames ""golden master tests"" to ""regression tests"") based on Rayleigh-Benard convection that adds regression tests for `Value` boundary conditions, the forcing implementation and the salinity equation, by using salinity as a passive tracer in the test. The test avoids setting random initial conditions by loading both the initial model state and the comparison state from file. Note that loading the model state from file required writing a new `OutputWriter` that outputs and loads the ""source terms"", `G`. This implementation is included in `test_regression.jl`, but it may be worthwhile to integrate it into `output_writers.jl` at some point. The test runs on the CPU and GPU. However, similar to the thermal bubble tests, this test fails on the GPU. This PR also adds a file to `sandbox` to aid running and exploring solutions to Rayleigh-Benard and also demonstrates the user-specification of a forcing term.",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/239:353,load,loading,353,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/239,3,['load'],"['loading', 'loads']"
Performance,"This PR is a first stab at tackling #1493. It adds a `FieldTimeSeries` type that stores a time series of an entire field taken from a JLD2 file and a `FieldDataset` function that stores all the fields from a JLD2 file in a `Dict{String,FieldTimeSeries}`. There are two flavors: `FieldTimeSeries{InMemory}` where the data is loaded fully into memory into a 4D array, and `FieldTimeSeries{OnDisk}` which lazily loads different time snapshots from disk. Indexing linearly into a `FieldTimeSeries` returns a `Field`. Immediate TODO:; - [x] Test `architecture = GPU()`. Right now it's pretty barebones. Some more work on `Oceananigans.Fields` might be needed before we get a fully-featured `FieldTimeSeries`. Eventually it would be great to be able to be able to:; 1. Have `FieldTimeSeries` work with abstract operations like it was a `Field` broadcasting over time.; 2. Work with `FieldTimeSeries` as if it were a 4D array (with operations ignoring the halos).; 2. Have named dimensions, slick selectors, and references dimensions through DimensionalData.jl. Some things to think about:; 1. Since the aim is to work with abstract operations, `FieldTimeSeries` right now only work with JLD2 data that includes halos. Do we want to support loading data without halos?; 2. Do we want `FieldTimeSeries` to support reading from NetCDF? If the answer to (1) is yes then this shouldn't be hard to support.; 3. Right now function boundary conditions are lost in serialization. We should probably allow users to specify boundary conditions on `FieldTimeSeries`.",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1641:324,load,loaded,324,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1641,3,['load'],"['loaded', 'loading', 'loads']"
Performance,"This PR is an attempt to take performance benchmarking more seriously by keeping benchmark scripts up to date and tested. This will be nice so we can get an idea of whether performance has regressed by looking at build logs. More useful for looking at memory allocations as runtimes will vary depending on the CI server. We can still run the benchmark scripts from the terminal or REPL, and reduced versions are run as part of the test suite. This PR is just a start, I'm sure we'll tune this stuff as time goes on.",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/727:30,perform,performance,30,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/727,3,"['perform', 'tune']","['performance', 'tune']"
Performance,"This PR is created to solve the boundary condition race condition that occurs when trying to fill in the halo corner nodes. It is basically just an integration of PR #1985 that runs the halo filling sequentially to avoid unsynchronized execution and PR #1923 to fill left and right halos together which should increase performance. IMPORTANT NOTE:. This PR will fail the ocean large eddy simulation regression tests when lines (50-53) in BoundaryConditions/fill_halo_regions.jl are uncommented. The only thing that these lines do is a reordering of halo filling order such that the periodic boundary conditions are calculated after all the ""Bounded type"" boundary conditions. This is needed since periodic boundary conditions require previous evaluation of boundary conditions in other directions to fill the corner nodes (see PR #1985). It is possible that the ocean-large-eddy-simulation-regression-data has been generated with periodic boundary conditions (west-east, north-south) which are evaluated before the bounded ones (top-bottom) (see the fill_halo_regions! function in main). As such the issue with the test failing when uncommenting lines (50-53) would not be a bug in the code but a bug in the previous generated regression data. This error is quite small (as the corners would be updated based on the previous step ""bounded type"" boundary conditions) but still enough to make the test fail. (I leave this to you to confirm :)). Resolves #1179",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2035:51,race condition,race condition,51,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2035,2,"['perform', 'race condition']","['performance', 'race condition']"
Performance,"This PR makes a few improvements to reductions of `AbstractField` and `AbstractOperation`. ~~One change is to perform in-place reductions of `AbstractDataField` (fields backed by data) using the parent arrays (this is much faster as those arrays are contiguous).~~ (This doesn't work, obviously in hindsight) We also try to support allocating reductions of `AbstractOperations` like `maximum(a * b)`. These changes were inspired by the discussion on #2024 (though we don't resolve that issue here). To do:. - [x] Tests that allocating reductions of abstract operations work; - [x] Test that allocating reductions are correct (eg they only reduce over the interior of an array); ~~- [ ] Benchmark?~~",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2040:110,perform,perform,110,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2040,1,['perform'],['perform']
Performance,"This PR makes several improvements to CATKE:. 1. We add parameters to the TKE dissipation rate model, which is `ep = CD * e^(3/2) / L`. Previously `CD` was a constant; this PR makes `CD(Ri)` a function of the Richardson number `Ri`. We can interpret this physically as a relaxation of the assumption that the TKE dissipation length scale is equal (or scales identically with) the TKE transport mixing length. . 2. We simplify the stability function (cc @adelinehillier). Previously we used tanh; now we use piecewise linear, which is simpler and more performant (we are unsure if this matters, but we can use all the help we can get for performance). @adelinehillier also simplifies the formulation of the stability function. 3. We improve the numerics of implicit time-stepping for the TKE equation. Following [Patankar 1980](https://www.taylorfrancis.com/books/mono/10.1201/9781482234213/numerical-heat-transfer-fluid-flow-suhas-patankar) (a good summary can be found in [Burchard et al. 2003](https://reader.elsevier.com/reader/sd/pii/S0168927403001016?token=44F9FAFB8D6BF56C3B72B35E65525AE9851D1C87699B6AE41218916A030C54351702E29AA5E58EC81B8EF1F687777D4C&originRegion=us-east-1&originCreation=20220901171212)), we treat the buoyancy flux term in the TKE equation implicitly _when buoyancy flux is a sink of TKE_. In other words, when N^2 > 0 (stable stratification), buoyancy mixing reduces TKE. Treating the buoyancy flux implicitly in this case greatly reduces negative TKE due to temporal discretization errors (but does not solve it completely). More generally, treating _sink_ terms implicitly in a tracer conservation equation reduces the possibility of negative tracer concentrations. The possibility does not seem to be entirely eliminated, unfortunately, I think because of the complicating effects of implicit diffusion. We'll be recalibrating parameters as part of https://github.com/CliMA/ParameterEstimocean.jl/pull/291 and will report the results here.",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2597:551,perform,performant,551,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2597,2,['perform'],"['performance', 'performant']"
Performance,"This PR overhauls halo filling to improve performance, especially for models that are ""thin"" in one direction (meaning that halo filling in the ""fat"" directions is expensive). It combines halo filling for opposing boundaries when possible (this is almost always possible except when using value / gradient boundary conditions). Notably it implements kernels for filling periodic boundary conditions rather than using `view` plus broadcasting, leading to significant performance improvement and reduction in memory allocations. The main problem was periodic directions I think, but there were also issues for normal velocity components. There may be more improvements to be had there. I think we can also do a lot less halo filling for flux boundary conditions, but full optimization would require a bit of work to the turbulence closures. For the default model configuration, we launch about half as many kernels for halo filling now. Before this PR:. ```julia; [ Info: Oceananigans will use 8 threads; [ Info: Benchmarking model with RegularRectilinearGrid{Float64, Periodic, Periodic, Bounded}(Nx=64, Ny=64, Nz=1)...; 226.285 ms (404628 allocations: 164.96 MiB); [ Info: Benchmarking model with RegularRectilinearGrid{Float64, Periodic, Periodic, Bounded}(Nx=64, Ny=1, Nz=64)...; 517.198 ms (431298 allocations: 347.27 MiB); [ Info: Benchmarking model with RegularRectilinearGrid{Float64, Periodic, Periodic, Bounded}(Nx=1, Ny=64, Nz=64)...; 432.185 ms (292065 allocations: 315.60 MiB); ```. After this PR:. ```julia; gregorywagner:benchmark/ (glw/performance✗) $ julia --project benchmark_two_dimensional_models.jl [18:25:59]; [ Info: Oceananigans will use 8 threads; [ Info: Benchmarking model with RegularRectilinearGrid{Float64, Periodic, Periodic, Bounded}(Nx=64, Ny=64, Nz=1)...; 183.673 ms (340830 allocations: 107.52 MiB); [ Info: Benchmarking model with RegularRectilinearGrid{Float64, Periodic, Periodic, Bounded}(Nx=64, Ny=1, Nz=64)...; 229.704 ms (359167 allocations: 108.74 MiB); [ Info",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1923:42,perform,performance,42,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1923,3,"['optimiz', 'perform']","['optimization', 'performance']"
Performance,"This PR provides a convenience constructor for building `UniformStokesDrift` with four `Field`s for the Stokes shear and tendency, rather than functions. It also adds kernel functions for the field case, so users now have the choice between functions of `(z, t)`, `Field`s, or `nothing`. This is useful when calculating the Stokes profile is a relatively expensive or involved computation (eg, involving integration over a spectrum of waves). This permits two optimizations:. 1. Stationary Stokes shear profiles can be precomputed.; 2. Time-dependent Stokes shear profiles and Stokes tendencies can be computed in a `Callback`. This saves computation time for 3D runs because the Stokes profiles are 1D.; ; co-authored with @qingli411. TODO: . - [x] add a test",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2320:460,optimiz,optimizations,460,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2320,1,['optimiz'],['optimizations']
Performance,"This PR refactors and improves the documentation. Some of the things done in this PR:; 1. Split long pages in section. This was done to the physics section and the model setup section.; 2. Switch from PyPlot.jl to Plots.jl for plotting in examples. Each example now comes with an mp4 animation of the output. I believe this greatly improves the examples as users will have a better idea of what each example does. Surprisingly, the plotting code has been simplified. This will also allow us to run the example tests again (currently 5 or 6 broken tests).; 3. Updated performance benchmarks. CPU and GPU used (among other info from `versioninfo()`) is included to provide context for each benchmark.; 4. Added a page in the documentation called ""Using GPUs"" which gives some instructions on how to how to start using GPUs with Oceananigans (and Julia), how to tell if you have a compatible GPU, and some resources on where to get GPUs.; 5. Fixed the public API documentation to include more docstrings and include some docstrings that were left out when we moved certain bits into submodules.; 6. Numerous small fixes and improvements. Things left to do:; 1. Fix `@example` blocks in model setup docs.; 2. Add references.; 3. Generate documentation for private API as well. Resolves #466; Resolves #482; Resolves #483; Resolves #536",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/570:567,perform,performance,567,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/570,1,['perform'],['performance']
Performance,"This PR refactors the Poisson solvers so that:. 1. All our solvers (FFT-based, mixed FFT / tridiagonal, tridiagonal, and preconditioned conjugate gradient) have a consistent interface that's reminiscent of linear algebra solvers, eg we use the syntax. ```julia; solve!(x, solver, b); ```. which solves `A*x = b` for `x`, where `A` is described by `solver`. This resembles the syntax `ldiv!(x, A, b)` implemented by `LinearAlgebra.jl`. One exception is the mixed FFT / tridiagonal solver, which also permits `solve!(x, solver)` as an optimization. This extension is used because the ""source term"" is represented differently (eg in a way that depends on the vertical grid spacing) than the other Poisson solvers. The mixed solver also accepts `solve!(x, solver, b)` like the others; this will launch an extra kernel to copy and scale `b` appropriately. 2. The FFT-based solver supports the ""screened"" Poisson equation. ```; (∇² + m) ϕ = b; ```. via `solve!(x, solver, b, m)`, where `m` is a _constant_. This will allow us to solve the elliptic equation posed by an implicit time discretization of the free surface evolution equation. Todo:. - [x] Implement an FFT-based solver for the implicit free surface; - [x] Test demonstrating that the preconditioned conjugate gradient solver and FFT-based solver get the same answer.",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1869:533,optimiz,optimization,533,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1869,1,['optimiz'],['optimization']
Performance,This PR refactors the kernels for turbulence closures to pave the way for new closures that require knowledge of buoyancy and buoyancy gradients. It also performs some minor cleanup in `closure_operators.jl` and moves the turbulence closure implementations into a separate folder so that it is clear from the source structure which turbulence closures are implemented.,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/515:154,perform,performs,154,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/515,1,['perform'],['performs']
Performance,"This PR removes a bunch of scalar operations introduced by #3575 on the GPU and instead performs operations on CPU and converts to CuArrays afterwards. There are two issues that don't allow us to create the grid natively on the GPU but rather force us to create the grid on CPU and then convert it. 1. The metrics for the `OrthogonalSphericalShellGrid` use scalar operations; see the functions that are called at https://github.com/CliMA/Oceananigans.jl/blob/4c853b94e6c99949134981153250a4e2391bb0f1/src/Grids/orthogonal_spherical_shell_grid.jl#L780-L782. 2. When we create the `ConformalCubedSphreGrid`, to fill the metric and coordinate horizontal halos properly we use the same functionality that we use to fill halos for prognostic variables. To do that, we create Fields with the coordinate and metric values, call `fill_halo_regions!` on these fields, and then copy back the data from the Fields to the metrics. Copying the data from the metrics to the fields and vice-versa requires scalar operations, e.g., see ; https://github.com/CliMA/Oceananigans.jl/blob/7f12be3d82486b8be2923d8675d2e59c84efd722/src/MultiRegion/cubed_sphere_grid.jl#L280-L292. cc @siddharthabishnu",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3579:88,perform,performs,88,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3579,1,['perform'],['performs']
Performance,"This PR removes argument splatting in intermediate functions that are called to compute the hydrostatic free surface tendencies. Argument splatting was removed in a prior PR (that'd be great if someone can remember), but was reinstated in #3360. This PR re-removes splatting. It yields a 2x performance gain for a column model:. # `main`. ```julia; julia> include(""test_single_column_model.jl""); ┌ Info: Running a simulation of HydrostaticFreeSurfaceModel{CPU, RectilinearGrid}(time = 0 seconds, iteration = 0); │ ├── grid: 1×1×64 RectilinearGrid{Float64, Flat, Flat, Bounded} on CPU with 0×0×3 halo; │ ├── timestepper: QuasiAdamsBashforth2TimeStepper; │ ├── tracers: (b, e); │ ├── closure: CATKEVerticalDiffusivity{VerticallyImplicitTimeDiscretization}; │ ├── buoyancy: BuoyancyTracer with ĝ = NegativeZDirection(); └ └── coriolis: FPlane{Float64}...; 0.398369 seconds (727.30 k allocations: 706.103 MiB, 30.55% gc time); ```. # This PR; ```julia; julia> include(""test_single_column_model.jl""); ┌ Info: Running a simulation of HydrostaticFreeSurfaceModel{CPU, RectilinearGrid}(time = 0 seconds, iteration = 0); │ ├── grid: 1×1×64 RectilinearGrid{Float64, Flat, Flat, Bounded} on CPU with 0×0×3 halo; │ ├── timestepper: QuasiAdamsBashforth2TimeStepper; │ ├── tracers: (b, e); │ ├── closure: CATKEVerticalDiffusivity{VerticallyImplicitTimeDiscretization}; │ ├── buoyancy: BuoyancyTracer with ĝ = NegativeZDirection(); └ └── coriolis: FPlane{Float64}...; 0.214935 seconds (258.50 k allocations: 195.374 MiB, 10.57% gc time); ```. It also reduces memory allocation. . Note that the nonhydrostatic model was not changed (it does not splat).",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3477:291,perform,performance,291,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3477,1,['perform'],['performance']
Performance,"This PR removes the CubedSpheres module and re-implements CubedSphere grid using the MultiRegion module. Furthermore, it improves the OrthogonalSphericalShellGrid. ## Implemented in this PR; - uses Distances package which allowed us to clear up various utilities in the grid_utils.jl file regarding great circle distances on the sphere; - fix some bugs that didn't allow an OrthogonalSphericalShellGrid to be constructed with any topology; - better and more accurate show methods that also work on the GPU.; - construct ConformalCubedSphereGrid + tests; - introduces the Connectivity type for MultiRegionGrids + adds a default CubedSphereGrid connectivity; - adds halo filling functionality for tracers + velocities ( + tests); - introduced CubedSphereField abstraction; - introduces CubedSphereConnectivity + tests. ## Outstanding issues; (Several issues will be opened as soon as this PR is merged for the following); - Coordinate and metric halo fillings for the ConformalCubedSphereGrid are hardcoded. We need to re-implement this based on the connectivity of the grid so that it works (i) for any connectivity and (ii) for any CubedSpherePartition.; - Properties `ξₗ, ξᵣ, ηₗ, ηᵣ` should be taken out from OrthogonalSphericalShellGrid _or_, even better, be grouped together into a property `conformal_cubed_sphere` or something. This way, the OrthogonalSphericalShellGrid will be general and not necessarily tied to the conformal cubed sphere.; - Differentiate the OrthogonalSphericalShellGrid constructors from the cubed_sphere_panel constructors; - Shortcut for velocity halo filling; - Alleviate the need for multiple hallo filling passes for velocities (**important for performance/scaling**); - Add testing for MultiRegionGrids with `YPartition`; - Allow uneven x-y partition for ConformalCubedSphere",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2867:1678,perform,performance,1678,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2867,1,['perform'],['performance']
Performance,"This PR removes the PencilFFT library from Oceananigans and builds a distributed FFT solver using Oceananigans' inhouse transforms. This allows us to run on GPUs both periodic and bounded domains.; No stretched mesh is supported at the moment (that will come in a later PR). The transposition is performed through a custom `transpose` routine built for Oceananigans' fields that assumes ; - the starting configuration is always a _z-free_ configuration.; - the transpose directions are _z-free_ -> _y-free_ -> _x-free_ -> _y-free_ -> _x-free_; - the y-direction is integer divisible by the number of ranks that divide the x-direction; - the z-direction is integer divisible by the number of ranks that divide the y-direction. An additional assumption is that:; - if TY is Bounded, also TZ needs to be Bounded; - if TX is Bounded, also TY needs to be Bounded. All these assumptions can be relaxed in following PRs",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3279:296,perform,performed,296,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3279,1,['perform'],['performed']
Performance,"This PR reorganizes the diagnostics structs and introduces a new `VerticalProfile` diagnostic that can calculate vertical profiles efficiently on-the-fly on CPUs and GPUs. Product profiles and velocity covariance profiles are built on top of it. `HorizontallyAveragedVerticalProfile` would be a more accurate name, but is much longer. So I'm sticking with the ""convention"" that a profile is implied to be horizontally averaged. The `profile` can be passed to an output writer which can write it to disk. The horizontal averaging currently relies on a parallel reduction prefix sum algorithm that I hacked over a CUDAnative.jl example, although I do have a test for the diagnostic so it does work. The algorithm can be more efficient (see https://github.com/JuliaGPU/CuArrays.jl/issues/68). It allocates very minimal amounts of memory (less than `mean`) and benchmarks show that it is ~20x faster than what we were doing before (copying to CPU and calculating there) which is great but it's ~5x slower than optimal performance. As it does not allocate memory, we can now calculate vertical profiles even when running large models that fill up memory. Although I should mention that an intermediate array with a size of at least `1*Ny*Nz` is required for the parallel reduction step (so I'm using `poisson_solvers.storage` because it's a vanilla CuArray that can be overwritten). ```julia; N, H = 512, 1; T = N + 2H. a = rand(T, T, T) |> CuArray; h = zeros(N) |> CuArray; ```. What we were doing before:; ```julia; @benchmark CuArrays.@sync mean(Array(view(a, H:N+H, H:N+H, H:N+H)), dims=[1, 2]). BenchmarkTools.Trial: ; memory estimate: 1.01 GiB; allocs estimate: 250; --------------; minimum time: 684.013 ms (2.29% GC); median time: 712.570 ms (6.28% GC); mean time: 732.480 ms (8.79% GC); maximum time: 807.437 ms (16.95% GC); --------------; samples: 7; evals/sample: 1; ```. What this PR does:; ```julia; Nx, Ny, Nz = 512, 512, 512; C = rand(Nx, Ny, Nz) |> CuArray; Rx = zeros(Float64, 1, Ny, Nz) ",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/352:1014,perform,performance,1014,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/352,1,['perform'],['performance']
Performance,"This PR rewrites every kernel and kernel launcher to use `KernelAbstractions` rather than `GPUifyLoops`. A key new function is `launch!`:. https://github.com/CliMA/Oceananigans.jl/blob/7bc83752f72508d6147a8d2e940baf69eff762c0/src/Utils/kernel_launching.jl#L58-L69. This function launches a kernel over `layout`. Performance differences are a crucial question. To start, I've written a function `work_layout` to define the workgroup and worksize for each of our layouts:. https://github.com/CliMA/Oceananigans.jl/blob/7bc83752f72508d6147a8d2e940baf69eff762c0/src/Utils/kernel_launching.jl#L41-L54. This function may need to be improved for performance reasons. Hopefully this is the right way to use `KernelAbstractions`... cc @vchuravy, @leios",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/805:312,Perform,Performance,312,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/805,2,"['Perform', 'perform']","['Performance', 'performance']"
Performance,"This PR rewrites how we do benchmarking so it's easier to analyze, visualize, and compare benchmark results. It's also easier to add new benchmarks. And we have automated multithreading benchmarks (we can approach MPI benchmarking in a similar manner). We used to use TimerOutputs.jl for benchmarking but really it's made more for profiling. This PR; 1. switches to using the `BenchmarkGroup` from BenchmarkTools.jl which makes it easy to compare benchmarks (e.g. check for CPU -> GPU performance and for performance relative to a base case).; 2. Dataframes.jl and PrettyTables.jl are used to visualize benchmarks, which are prettier, and you can output HTML tables that can be easily embedded into documentation.; 3. PkgBenchmark.jl allows use to compare benchmarks between two commits or branches (right now only via `benchmark_regression.jl`) which should make it easy to automate checking for performance regressions between PRs and the master branch. The next big step for benchmarking would be to create a Buildkite benchmarks pipeline that can be triggered via GitHub comments (this would involve looking into https://github.com/buildkite/trigger-pipeline-action). I started embedding HTML tables into the docs but it's going to be hard for users to parse tables full of numbers. Plots and bar graphs might be much more useful here, but this might require a Buildkite pipeline to generate the figures to embed into the documentation. So I'll leave this important step for a different PR. I also want to add a script that generates a better version of the benchmark plots in the README, but this might have to wait for a future PR. It's a little rough around the edges but I think it's polished enough to be considered for merging. Feels like benchmarks share a lot of boilerplate but might try to reduce it in a future PR. Some fun benchmarking facts:; * GPU models using WENO-5 are 250x faster than CPU models using WENO-5!; * TEOS-10 slows down your CPU model by ~30%, but only ~3% for GPU mo",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1169:485,perform,performance,485,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1169,3,['perform'],['performance']
Performance,"This PR serves as an important step toward integrating the split-explicit free surface with `MultiRegionGrid`. It introduces an optional parameter, `extended_halos`, which allows users to control the behavior of halo filling during each substep. Setting `extended_halos` to `false` maintains the halo size to be the same as the original grid, diverging from the previous default behavior where `extended_halos = true` would automatically extend the halo to cover all split-explicit substeps, significantly enhancing computational performance. Although setting `extended_halos` to `false` may be less efficient for long simulations on high-resolution`MultiRegionGrids` grids, this feature is helpful for debugging purposes. Additionally, this PR addresses and resolves a couple of bugs related to the interaction between `ImmersedBoundaryGrid` and `MultiRegionGrid`. These modifications ensure that `ImmersedBoundaryGrid` now correctly wraps over `MultiRegionGrid`, reversing the prior implementation approach. With @simone-silvestri and @jm-c.",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3596:530,perform,performance,530,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3596,1,['perform'],['performance']
Performance,"This PR takes a stab at designing a non-invasive interface to running Oceananigans on multiple CPUs and GPUs, i.e. distributed parallelism with MPI. By non-invasive I mean that no existing code will have to change. It may not be the best solution but I really like it and I'm hoping we can discuss this design. I see no reason why it won't perform well. Vision of this PR:; 1. Oceananigans core code base wil be free of MPI and uses GPUifyLoops.jl as it already does so nothing changes there.; 2. Everything needed for distributed parallelism will live in the `Oceananigans.Distributed` submodule.; 3. Support for x, y, and z decompositions. In practice, choice of pressure solver may limit which decomposition we can use but they're all supported right now.; 4. Each rank will time step it's own submodel communicating with its neighbors as needed, i.e. in `fill_halo_regions!`. There is no ""master rank"".; 5. Halo communication is implemented by injecting `HaloCommunication` boundary conditions wherever a submodel shares a halo with another rank. This is then dispatched on so no need to modify existing code.; 6. With PR #589 we will be able to easily slide in a `DistributedPressureSolver` struct that can be used to dispatch on `solve_for_pressure!`. So again, no need to modify existing code. This way MPI does not invade the core code base making it easier to maintain, and there will be a very clear boundary between ""core Oceananigans"" and ""distributed parallelism functionality"" which I think will serve us well in the future as MPI seems to permeate deeply into other codes, making them hard to modify. The big thing that is missing is of course the distributed pressure solver, the hard thing to implement. This is where [DistributedTranspose.jl](https://github.com/leios/DistributedTranspose.jl) will come in handy. I also recently found [PencilFFTs.jl](https://github.com/jipolanco/PencilFFTs.jl) which also looks interesting. cc @leios. For testing purposes, I'm tempted to do the pre",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/590:340,perform,perform,340,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/590,1,['perform'],['perform']
Performance,"This PR updates `FieldTimeSeries` to be slightly ""backward compatible"" and capable of loading data from Oceananigans versions that did not have `Field.indices`, or which serialized stretched GPU grids with `CuArray`s. We might have to constantly maintain this... but hopefully eventually structures will be more stable, and eventually we can just delete the stuff added here.",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2419:86,load,loading,86,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2419,1,['load'],['loading']
Performance,"This PR would add a [""Strange splitting""](http://hplgit.github.io/fdm-book/doc/pub/book/sphinx/._book018.html) time stepper where the biogeochemical components of the tendencies are computed more frequently than the transport since biogeochemistry can often be much stiffer and so it is relatively common practice to step the bgc at different frequencies to the physics (e.g. NEMO-PISCES allows multiple euler substeps to be taken between each physics step). Strange splitting assumes the tendency can be written as a stiff and non-stiff part:; $\frac{\partial C}{\partial t} = \mathcal{A} + \mathcal{B}$,; where $\mathcal{A}$ is the advection (less stiff) and $\mathcal{B}$ is the BGC (more stiff) components.; And we basically take half a step with just the stiff component, then a full step with just the non-stiff component, and finally another step with the stiff component.; i.e. we have $C^n$ then:; $C^{n+1/2} = C^n + \int_{t_n}^{t_n+\Delta t/2}\mathcal{B}\left(C^n\right)dt,$; $C^* = C^{n+1/2} + \int_{t_n}^{t_n+\Delta t}\mathcal{A}\left(C^{n+1/2}, \vec{u^n}\right) dt, $; $C^{n+1} = C^* + \int_{t_n +\Delta t / 2}^{t_n+\Delta t}\mathcal{B}\left(C^*\right)dt.$. This is supposedly $\mathcal{O}(\Delta t ^2)$ from the splitting, so you can take the substeps with $\mathcal{O}(\Delta t ^2)$ schemes. To do this I had to implement quite a few changes so that you can optionally turn off bgc transitions in the normal tendency calculation, and then add some new functions to the time steppers to allow them to just compute the tendencies for, and step the bgc. This currently does not work, but I ran out of time to debug it. If there is interest in using this I would be happy to have another look at it. Another thought I had was that we could allow the biogeochemical sub-stepping to be performed by e.g. DifferentialEquations.jl timesteppers, but then I realised it wouldn't be that straight forward to make a wrapper for them to work in oceananigans so decided to leave it for another time.",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3888:1917,perform,performed,1917,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3888,1,['perform'],['performed']
Performance,This abstraction would also simplify the ECCO/SOSE LESbrary.jl setup although it's also messy due to other terms and me jamming in Interpolations.jl objects: https://github.com/CliMA/LESbrary.jl/blob/master/scripts/pilot_simulation.jl#L117-L145. It seems like some complex setups could benefit a lot from a background fields abstraction. It shouldn't affect performance since the two interaction terms should results in no-ops. > I think we would want this functionality to assume that the linear balances between background terms are somehow separately satisfied. Agree with this. Users are responsible for making sure that they impose sensible background fields (just like they are responsible for setting sensible fields).,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/960#issuecomment-701523300:358,perform,performance,358,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/960#issuecomment-701523300,1,['perform'],['performance']
Performance,"This can be implemented by implementing a new method for [`update_source_terms!`](https://github.com/climate-machine/Oceananigans.jl/blob/341c17410189528bb07cdcfc42e7e55fb81c730e/src/time_steppers.jl#L161) that dispatches on `forcing::Forcing{Tu} where Tu <: AbstractArray`. No changes are needed to the code except for this addition. When the user passes arrays to the `Forcing` constructor, the code will do the right thing as long as this method is implemented. I would note unless we write *a lot* of new code, the user will be essentially limited to either providing an array for *all* fields, or forcing for *all* fields. To allow the user to implement either forcing functions or forcing arrays requires writing 5! = 120 new methods (right?) which is not desirable. A convenience constructor for `Forcing` that detects when the user passes arrays for any of the fields will help users avoid accidentally specifying forcing arrays for some fields and functions for others. However, once we resolve the `isbitstype` problem (perhaps also using `FieldVectors` or `LabeledArrays` for `velocities` and `tracers` to retain high performance) we can introduce abstractions for the idea of an `equation` function for each field individually (which would also be stored in a `LabeledArray`), and permit more flexible equation specification by the user without inducing a combinatorial explosion of code length.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/110#issuecomment-470324259:1129,perform,performance,1129,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/110#issuecomment-470324259,1,['perform'],['performance']
Performance,"This changes the threading structure in a kernel launch to only launch over the i,j indices to avoid race conditions",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1944:101,race condition,race conditions,101,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1944,1,['race condition'],['race conditions']
Performance,"This code:. ```julia; using Oceananigans. grid = RectilinearGrid(size = (128, 128, 1),; halo = (4, 4, 1),; x = (0, 2π),; y = (0, 2π),; z = (0, 1),; topology = (Periodic, Periodic, Bounded)). momentum_advection = VectorInvariant(vorticity_scheme = WENO(order=5),; vertical_scheme = Centered(),; divergence_scheme = WENO(order=5)); buoyancy = nothing; tracers = nothing; free_surface = ExplicitFreeSurface(gravitational_acceleration=10); model_args = (; momentum_advection, buoyancy, tracers, free_surface). model = HydrostaticFreeSurfaceModel(; grid, model_args...); ```. throws an error:. ```julia; julia> include(""hydrostatic_turbulence.jl""); ERROR: LoadError: ArgumentError: The grid halo (4, 4, 1) must be at least equal to (4, 4, 4).; Note that an ImmersedBoundaryGrid requires an extra halo point in all; non-flat directions compared to a non-immersed boundary grid.; Stacktrace:; [1] validate_model_halo; @ ~/.julia/packages/Oceananigans/OMBY0/src/Models/Models.jl:55 [inlined]; ```. This seems like a bug to me, because a `Centered()` scheme in the vertical should only require one halo point.",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3699:651,Load,LoadError,651,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3699,1,['Load'],['LoadError']
Performance,"This code:. ```julia; using Oceananigans; using Oceananigans.Grids: with_halo. underlying_grid = RectilinearGrid(size=(3, 3, 3), x=(0, 2), y=(0, 1), z=(0, 1)); grid = ImmersedBoundaryGrid(underlying_grid, GridFittedBottom((x, y) -> x)). with_halo((4, 4, 4), grid); ```. produces. ```; ERROR: LoadError: ArgumentError: The dimensions of the immersed boundary (9, 9) do not match the grid size (11, 11); Stacktrace:; [1] validate_ib_size; @ ~/.julia/packages/Oceananigans/QYJpb/src/ImmersedBoundaries/grid_fitted_immersed_boundaries.jl:92 [inlined]; [2] ImmersedBoundaryGrid(grid::RectilinearGrid{Float64, Periodic, Periodic, Bounded, Float64, Float64, Float64, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, CPU}, ib::GridFittedBottom{OffsetArrays.OffsetMatrix{Float64, Matrix{Float64}}, Oceananigans.ImmersedBoundaries.CenterImmersedCondition}); @ Oceananigans.ImmersedBoundaries ~/.julia/packages/Oceananigans/QYJpb/src/ImmersedBoundaries/grid_fitted_immersed_boundaries.jl:85; [3] with_halo(halo::Tuple{Int64, Int64, Int64}, ibg::ImmersedBoundaryGrid{Float64, Periodic, Periodic, Bounded, RectilinearGrid{Float64, Periodic, Periodic, Bounded, Float64, Float64, Float64, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, CPU}, GridFittedBottom{OffsetArrays.OffsetMatrix{Float64, Matrix{Float64}}, Oceananigans.ImmersedBoundaries.CenterImmers",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2988:292,Load,LoadError,292,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2988,1,['Load'],['LoadError']
Performance,"This correctly adds the time-step wizard to `ShallowWaterModel`. . All the tests pass but there are two warnings that maybe should be adressed?. ```; ...; [2021/02/01 11:45:05.426] INFO Testing setting shallow water model fields...; [2021/02/01 11:45:12.752] WARN Performing scalar operations on GPU arrays: This is very slow, consider disallowing these operations with `allowscalar(false)` -@-> /home/fpoulin/.julia/packages/GPUArrays/WV76E/src/host/indexing.jl:43; ...; [2021/02/01 11:45:39.457] INFO Testing time-step wizard ShallowWaterModels [GPU(), ((Periodic, Periodic, Bounded), (Periodic, Bounded, Bounded), (Bounded, Bounded, Bounded))[1]]...; [2021/02/01 11:45:39.641] WARN You have used the default iteration_interval=1. This simulation will recalculate the time step every iteration which can be slow. -@-> /home/fpoulin/software/Oceananigans.jl/src/Simulations/simulation.jl:68. ```",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1328:264,Perform,Performing,264,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1328,1,['Perform'],['Performing']
Performance,This fixes a bug in `MultiRegion` for which `copyto!` is by default asynchronous. This is not a problem until very large meshes (which unfortunately is what we want with multi GPU) ; I still have to test performance... and maybe we also want another solution,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2547:204,perform,performance,204,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2547,1,['perform'],['performance']
Performance,"This fixes a bug in which halo regions were not filled for subgrid-scale LES diffusivities. . To do this robustly for various diffusivity structures with different nesting hierarchies, a recursive halo region filling functionality is implemented that dispatches on `bcs::NamedTuple{(:x, :y:, :z)}`, corresponding to a `FieldBoundaryCondition`. Thus, when `fill_halo_regions!(fields, bcs, grid)` is called with matching tuples for *both* `fields` and `bcs`, a simultaneous loop is done over the members of the two tuples. However, when `fill_halo_regions!(fields, bcs, grid)` is called and `bcs` is a `FieldBoundaryCondition`, a loop is performed over the members of `fields` *only*, and `bcs` is reused for every member of `fields`. Thus halo regions for fields embedded in nested structures (like diffusivities in `AnisotropicMinimumDissipation`) are all filled with a single call to `fill_halo_regions!`. For now, the code uses temperature boundary conditions to determine the diffusivity boundary conditions. This will work until we implement the changes proposed in #371.",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/385:636,perform,performed,636,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/385,1,['perform'],['performed']
Performance,"This has been figured out. We have; 1. `solve_poisson_3d_ppn(f, Nx, Ny, Nz, Δx, Δy, Δz)`; 2. `solve_poisson_3d_ppn!(g::RegularCartesianGrid, f::CellField, ϕ::CellField)`; 3. `solve_poisson_3d_ppn_planned!(ssp::SpectralSolverParameters, g::RegularCartesianGrid, f::CellField, ϕ::CellField)`. that work and pass multiple tests. The DCT is slower than the FFT (somewhat expected) and for some reason we couldn't get it to work with `FFTW.rfft` but those are optimizations we can figure out later.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/6#issuecomment-452732017:455,optimiz,optimizations,455,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/6#issuecomment-452732017,1,['optimiz'],['optimizations']
Performance,"This has implications in terms of code performance, if we have `set!(field)` somewhere in the code it will trigger automatically a `fill_halo_regions!` which we might not want as we are very careful to where we call `fill_halo_regions!`. There are two options then:; (1) remove all instances of `set!` in the internals and make sure to never use it; (2) implement a different interface that can be user-facing which triggers set! and fill_halo_regions! together. I would vote for (2)",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3010#issuecomment-1480334115:39,perform,performance,39,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3010#issuecomment-1480334115,1,['perform'],['performance']
Performance,"This is a (somewhat minor) convenience for parameter studies, because it means that if `u` was constructed with. ```julia; u_boundary_conditions = FieldBoundaryConditions(top = FluxBoundaryCondition(1.0)); ```. we can later write things like. ```julia; u.boundary_conditions.top = FluxBoundaryCondition(2.0); ```. to change the value of the boundary condition. Note that the _type_ of the boundary condition can't change. . Just opening this PR to see if tests pass with this change. If so it seem positive to me (I don't think `FieldBoundaryConditions` needs to be immutable for performance reasons, but please speak up if anyone thinks/knows otherwise).",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2063:580,perform,performance,580,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2063,1,['perform'],['performance']
Performance,"This is a breaking change for some scripts that load, e.g., Earth's gravity from the `BuoyancyModels` module. Let's aim to release a new version when both this PR and #2979 are merged.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3045#issuecomment-1492610370:48,load,load,48,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3045#issuecomment-1492610370,1,['load'],['load']
Performance,"This is a minimum working example that shows how we (@writingindy and I) have tried to use closure in the `VectorInvariantFormulation` of the `ShallowWaterModel` but get a `TaskFailedException` error, copied below. @simone-silvestri , do you have any idea as to how this can be fixed?. ```; using Oceananigans; using Oceananigans.Models.ShallowWaterModels: VectorInvariantFormulation; using Oceananigans.Advection: VelocityStencil, VorticityStencil; using Oceananigans.TurbulenceClosures. grid = RectilinearGrid(size = (16, 16), x = (0, 1), y = (0, 1), ; topology = (Periodic, Periodic, Flat)). model = ShallowWaterModel(grid = grid,; momentum_advection = WENO5(vector_invariant = VelocityStencil()),; tracers = (:A),; closure = HorizontalScalarBiharmonicDiffusivity(κ=1e-6),; gravitational_acceleration = 1.0,; formulation = VectorInvariantFormulation(); ). set!(model, h=1); simulation = Simulation(model, Δt = 0.001, stop_time = 10.0). run!(simulation); ```. Error: ; ```; ERROR: LoadError: TaskFailedException. nested task error: MethodError: no method matching getindex(::Nothing, ::Int64); Stacktrace:; [1] call; @ ~/.julia/packages/Cassette/34vIw/src/context.jl:456 [inlined]; [2] fallback; @ ~/.julia/packages/Cassette/34vIw/src/context.jl:454 [inlined]; [3] _overdub_fallback(::Any, ::Vararg{Any, N} where N); @ ~/.julia/packages/Cassette/34vIw/src/overdub.jl:586 [inlined]; [4] overdub; @ ~/.julia/packages/Cassette/34vIw/src/overdub.jl:586 [inlined]; [5] diffusive_flux_x(::Int64, ::Int64, ::Int64, ::RectilinearGrid{Float64, Periodic, Periodic, Flat, Float64, Float64, Float64, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float; ```",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2606:983,Load,LoadError,983,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2606,1,['Load'],['LoadError']
Performance,"This is a rather minimal example that tries to use `PrescribedVelocityFields` in the `ImmersedBoundaryMethod`,. ```; using Oceananigans; using Oceananigans.ImmersedBoundaries: ImmersedBoundaryGrid, GridFittedBoundary. grid = RegularRectilinearGrid(size=(16, 8),; y=(-1, 1), z=(-1, 0), ; topology=(Flat, Periodic, Bounded)). seamount(x, y, z) = z < - 1 + 0.1*exp(-y^2/0.25^2) . grid_with_seamount = ImmersedBoundaryGrid(grid, GridFittedBoundary(seamount)). U(x, y, z) = 0.; V(x, y, z) = 0.; W(x, y, z) = 0. velocities = PrescribedVelocityFields(u=U, v=V, w=W). model = HydrostaticFreeSurfaceModel(architecture = CPU(), ; grid = grid_with_seamount,; momentum_advection = CenteredSecondOrder(), ; free_surface = ImplicitFreeSurface(),; closure = nothing, ; tracers = :b,; velocities = velocities,; buoyancy = BuoyancyTracer()); ```. Unfortunately, it fails with the following error,. ```; ERROR: LoadError: MethodError: no method matching device(::Nothing); Closest candidates are:; device(::Oceananigans.Architectures.AbstractCPUArchitecture) at /home/fpoulin/.julia/packages/Oceananigans/X0YQn/src/Architectures.jl:50; device(::Oceananigans.Architectures.AbstractGPUArchitecture) at /home/fpoulin/.julia/packages/Oceananigans/X0YQn/src/Architectures.jl:51; Stacktrace:; [1] device_event(arch::Nothing); @ Oceananigans.Architectures ~/.julia/packages/Oceananigans/X0YQn/src/Architectures.jl:75; [2] mask_immersed_field!(field::Oceananigans.Fields.FunctionField{Face, Center, Center, Clock{Float64}, Nothing, typeof(U), ImmersedBoundaryGrid{Float64, Flat, Periodic, Bounded, RegularRectilinearGrid{Float64, Flat, Periodic, Bounded, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}}}}, GridFittedBoundary{Nothing, typeof(seamount)}}, Float64}, grid::ImmersedBoundaryGrid{Float64, Flat, Periodic, Bounded, RegularRectilinearGrid{Float64, Flat, Periodic, Bounded, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1958:893,Load,LoadError,893,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1958,1,['Load'],['LoadError']
Performance,"This is related to [this PR](https://github.com/CliMA/Oceananigans.jl/pull/2029). To summarize:. - I have a production-ready research code (that's far too complicated to post here) that does sines/cosines calculations for a background field; - If these sine/consine calculations are performed inside the background field function the code is about 100x slowed than if I perform these calculations outside the function, and just pass the pre-calculated sine and cosine; - I tried making a MWE to reproduce this behavior but failed, which indicates that it's not _just_ the trig functions that are causing the slowdown. It's the trig functions plus something else (I have no idea what). So that's the state of things. I'll try to start from my code and cut down on things one by one to try and post a small MWE whenever I have time. CC: @glwagner @navidcy @francispoulin",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2034:283,perform,performed,283,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2034,2,['perform'],"['perform', 'performed']"
Performance,"This is something that @glwagner helped me put together a while ago and I had not merged it yet. I created a new branch that does only this and tested it out. Where it ran before, I get some errors. 7 to be precise. It seems GPU related and wondered if anyone has any suggestions how I can fix these?. ```; Test Summary: | Pass Error Total; Oceananigans | 18 7 25; Shallow Water Models | 18 7 25; Model constructor errors | 2 2; (Periodic, Periodic, Bounded) model construction | 6 6; (Periodic, Bounded, Bounded) model construction | 6 6; (Bounded, Bounded, Bounded) model construction | No tests; Setting ShallowWaterModel fields | 4 4; Time-stepping ShallowWaterModels [GPU(), (Periodic, Periodic, Bounded)] | 1 1; Time-stepping ShallowWaterModels [GPU(), (Periodic, Bounded, Bounded)] | 1 1; Time-stepping ShallowWaterModels [GPU(), (Bounded, Bounded, Bounded)] | 1 1; Time-stepping ShallowWaterModels [GPU(), Nothing] | 1 1; Time-stepping ShallowWaterModels [GPU(), FPlane{Float64}] | 1 1; Time-stepping ShallowWaterModels [GPU(), BetaPlane{Float64}] | 1 1; Time-step Wizard ShallowWaterModels [GPU(), ((Periodic, Periodic, Bounded), (Periodic, Bounded, Bounded), (Bounded, Bounded, Bounded))[1]] | 1 1; ERROR: LoadError: Some tests did not pass: 18 passed, 0 failed, 7 errored, 0 broken.; in expression starting at /home/fpoulin/software/Oceananigans.jl/test/runtests.jl:77; error in running finalizer: CUDA.CuError(code=CUDA.cudaError_enum(0x000002cf), meta=nothing); error in running finalizer: CUDA.CuError(code=CUDA.cudaError_enum(0x000002cf), meta=nothing); error in running finalizer: CUDA.CuError(code=CUDA.cudaError_enum(0x000002cf), meta=nothing); error in running finalizer: CUDA.CuError(code=CUDA.cudaError_enum(0x000002cf), meta=nothing); error in running finalizer: CUDA.CuError(code=CUDA.cudaError_enum(0x000002cf), meta=nothing); error in running finalizer: CUDA.CuError(code=CUDA.cudaError_enum(0x000002cf), meta=nothing); error in running finalizer: CUDA.CuError(code=CUDA.cudaE",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1326:1216,Load,LoadError,1216,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1326,1,['Load'],['LoadError']
Performance,This is super exciting! Out of curiosity is there any/which Krylov solver is compatible on multiple GPUs? Seems to be an important bottleneck for our current `PreconditionedConjugateGradientSolver` approach.,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3803#issuecomment-2386649167:131,bottleneck,bottleneck,131,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3803#issuecomment-2386649167,1,['bottleneck'],['bottleneck']
Performance,This is the first step towards #2012. The goal is to test the method with a flat-bottom and compare performance / stability with respect to the other free-surface implementations.,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2013:100,perform,performance,100,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2013,1,['perform'],['performance']
Performance,"This is the same benchmark performed with `ImplicitFreeSurface`, by imposing a divergent velocity `u(x, y, z) = x / 10` to make sure the implicit solver iterates. Looking at the results it seems like it doesn't iterate too much... (probably WENO cleans up?) And it is very weird that the `RectilinearGrid` version is not affected by the FreeSurface calculation? (I have double checked that the free surface solver is correct). #### Strong Scaling; | Grid size | Grid | GPUs | wall time | efficiency |; | -- | -- | -- | -- | -- |; | `1440×600×48`| `RectilinearGrid` | 1 | 1.37 minutes | 100% |; | `1440×600×48`| `MultiRegionGrid` | 2 | 1.05 minutes | 65.2% |",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2253#issuecomment-1116777025:27,perform,performed,27,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2253#issuecomment-1116777025,1,['perform'],['performed']
Performance,"This is worrisome!. What is being plotted in the comparisons? It looks like 3D fields are being outputted, but the time series plots show some reduction of the 3D data. It certainly seems that the time-averaged output is incorrect, but only for certain types of output, which is puzzling. One thought is that it seems possible to write a utility / function that calculates time-averages directly from snapshots and compares the result with time-averages generated by `WindowedTimeAverage`. We could even write a function that takes in a field and a simulation, and then constructs the two output writers, runs the simulation, and performs the analysis. The first TKE example should pass, but dissipation should not. Perhaps even time-averaging the velocity field is not correct? We can then experiment with types of output to figure out what ingredient leads to a discrepancy. > Here the snapshot results are consistent with each other, but the time-averaged TKE results computed with ComputedFields actually match the snapshots, but not the time-averaged TKE results computed with KernelComputedFields!. I think I might have missed something --- in the very first example, was TKE computed using a `ComputedField` or `KernelComputedField`? Are the later results in this post consistent with the first posted results?. Is there any way that any of this has to do with time-step alignment?. Lastly, why is the window slightly different from the `TimeInterval`? What happens when the time-interval and averaging window are the same (which appears to be our default?)",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1517#issuecomment-809893411:630,perform,performs,630,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1517#issuecomment-809893411,1,['perform'],['performs']
Performance,"This kind of feature can't be implemented directly in `Forcing`, because all explicit tendency terms are evaluated in the same kernel. So we have to evaluate `forcing` in every cell that requires a tendency. It is possible to expose a feature that allows kernels to be launched over some subset of cells. Then users could implement this kind of optimization in their script by using an array to represent the forcing, and then by precomputing the forcing into that array using a kernel that only evaluates some subset of all the cells in a callback. Or something like that. If forcing functions are expensive there may be simpler ways to do performance optimization though. For example, we can recommend piecewise linear masking functions for sponge layers instead of tanh or exp.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3525#issuecomment-2028469830:345,optimiz,optimization,345,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3525#issuecomment-2028469830,3,"['optimiz', 'perform']","['optimization', 'performance']"
Performance,"This looks like a good start!. I think we should use multiples of 2 and 3 for the resolution in x and z (24, 48, 64, etc). This has some minor advantages for FFT performance, but more importantly is something of a convention that I think is nice to stick to. We may want to write a simple abstraction for specifying systems of reactions. I will think about that. What would be a good biochemical system to consider that involves a handful of reactive tracers?. @ali-ramadhan just to be clear, the diel vertical migration cycle is something that's computed offline? In other words, we are not modeling the vertical motion of individual plankton in this model. We can, however, model other tracers directly in Oceananigans, such as nutrient concentrations, CO2, and carbonates that contribute to dissolved inorganic carbon.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/749#issuecomment-629398620:162,perform,performance,162,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/749#issuecomment-629398620,1,['perform'],['performance']
Performance,This looks slightly slower than the solution on #1770 (slow down of 2.0x rather than 1.8x):. ```; Advection schemes relative performance (GPU); ┌───────────────┬────────────────────────┬──────────┬─────────┬─────────┐; │ Architectures │ Schemes │ slowdown │ memory │ allocs │; ├───────────────┼────────────────────────┼──────────┼─────────┼─────────┤; │ GPU │ CenteredFourthOrder │ 1.36394 │ 1.08012 │ 1.6807 │; │ GPU │ CenteredSecondOrder │ 1.0 │ 1.0 │ 1.0 │; │ GPU │ UpwindBiasedFifthOrder │ 1.53947 │ 1.08022 │ 1.6815 │; │ GPU │ UpwindBiasedThirdOrder │ 1.3124 │ 1.06337 │ 1.53834 │; │ GPU │ WENO5 │ 2.04368 │ 1.22539 │ 2.91485 │; └───────────────┴────────────────────────┴──────────┴─────────┴─────────┘; ```. I find this surprising. @hennyg888 can you confirm?,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1783#issuecomment-870041934:125,perform,performance,125,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1783#issuecomment-870041934,1,['perform'],['performance']
Performance,"This major PR changes the way difference operators work so that across boundaries (either the end of the domain in `Bounded` directions, or across immersed boundaries), differences always return 0 and interpolation always returns the value on the ""active"" or ""wet"" side of the domain. This change means the halos do not need to be filled for `Value / Gradient` boundary conditions. Instead, value and gradient boundary conditions will be enforced across `Bounded` the same way they are enforced across immersed boundaries. Moreover these boundary conditions will only be supported for `AbstractScalarDiffusivity`. This leads to considerable code reduction, because it unifies the algorithms for non-immersed and immersed boundaries. This may decrease compile times. In addition, the reduction in halo filling will hopefully lead to further performance increases, as we have found that halo filling is inefficient especially on the GPU and should be avoided when possible. Finally, avoiding halo filling is important for distributed models, since it eliminates the dependency that halos must be filled across `Bounded` before they are sent between processes across a `Communication` topology. Evaluating performance changes will be an important part of this PR. There's still a bit todo:. - [x] Refactor halo filling and `apply_flux_x` to enforce value / gradient boundary conditions; - [x] Implement homogeneous interpolation operators; - [ ] Possibly eliminate halo filling for `OpenBoundaryCondition`, leaving halo filling only for periodic and communication bcs",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2477:840,perform,performance,840,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2477,2,['perform'],['performance']
Performance,"This seems like a way forward, but I would wait for #2924 (that seems to be our bottleneck) to pass before changing CI",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3042#issuecomment-1498500356:80,bottleneck,bottleneck,80,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3042#issuecomment-1498500356,1,['bottleneck'],['bottleneck']
Performance,This seems reasonable. I'm not sure if there are performance implications. @navidcy @simone-silvestri any thoughts?,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3455#issuecomment-1927382247:49,perform,performance,49,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3455#issuecomment-1927382247,1,['perform'],['performance']
Performance,"This seems to work out-of-the box with our simple example, but I'm encountering errors when applying to one of my complex scripts. I'm getting errors of the kind `ERROR: LoadError: Custom output b_tot needs dimensions!`. And when I do specify the dimensions manually I get `Trying to write (1, 1, 16) elements while [17, 1] are expected`. So clearly we're missing something. Looking at the behavior of `AveragedField` I think we should not `dropdim` when doing the average anymore since `AveragedField` always returns a 3D array:. ```julia; julia> bmean = AveragedField(b, dims=(1, 2)); AveragedField over dims=(1, 2) located at (⋅, ⋅, Center) of Field located at (Center, Center, Center); ├── data: OffsetArrays.OffsetArray{Float64,3,Array{Float64,3}}, size: (1, 1, 20); ├── grid: RegularRectilinearGrid{Float64, Periodic, Bounded, Bounded}(Nx=1, Ny=1024, Nz=16); ├── dims: (1, 2); ├── operand: Field located at (Center, Center, Center); └── status: time=0.0. julia> bmean.data; 1×1×20 OffsetArray(::Array{Float64,3}, 1:1, 1:1, -1:18) with eltype Float64 with indices 1:1×1:1×-1:18:; [:, :, -1] =; 0.0. [:, :, 0] =; 0.0. [:, :, 1] =; 0.0. ... [:, :, 16] =; 0.0. [:, :, 17] =; 0.0. [:, :, 18] =; 0.0; ```. But even without using `dropdim` I still get an error. I think maybe the issue is because `WindowedSpatialAverage` is returning an Array, and not an OffsetArray. Any thoughts?. ```julia; julia> bwind = WindowedSpatialAverage(b, dims=(1, 2), field_slicer=FieldSlicer(j=3:7)); WindowedSpatialAverage{Field{Center,Center,Center,OffsetArrays.OffsetArray{Float64,3,Array{Float64,3}},RegularRectilinearGrid{Float64,Periodic,Bounded,Bounded,OffsetArrays.OffsetArray{Float64,1,StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}}},NamedTuple{(:x, :y, :z),Tuple{CoordinateBoundaryConditions{BoundaryCondition{Oceananigans.BoundaryConditions.Periodic,Nothing},BoundaryCondition{Oceananigans.BoundaryConditions.Periodic,Nothing}},CoordinateBoundaryConditions{BoundaryCondition{F",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1397#issuecomment-783777559:170,Load,LoadError,170,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1397#issuecomment-783777559,1,['Load'],['LoadError']
Performance,"This set of tests also fails at least in part because of the lack of high order WENO as in #1271 , but the plots seem to use PyPlot and I ended up just getting a bunch of blank plots. I presume we wanted to update the plotting to use something else?. But I think some of the failures are that the orders do not match the theory close enough. ```; Test Summary: | Pass Fail Total; tmp | 86 9 95; ERROR: LoadError: Some tests did not pass: 86 passed, 9 failed, 0 errored, 0 broken.; in expression starting at /home/fpoulin/software/Oceananigans.jl/validation/convergence_tests/one_dimensional_advection_schemes.jl:69; ```",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1272:402,Load,LoadError,402,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1272,1,['Load'],['LoadError']
Performance,This significantly reduced the time to first plot for Plots.jl: https://github.com/JuliaPlots/Plots.jl/pull/2544. I guess the idea is we don't care about performance when running our tests. Most of the time is spent compiling so if we can reduce compile time (at the cost of having sub-optimal code) then our tests will run faster.,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1076:154,perform,performance,154,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1076,1,['perform'],['performance']
Performance,"This validation script also fails because it can't find CenteredSecondOrder. ```; julia> include(""one_dimensional_cosine_advection_diffusion.jl""); ERROR: LoadError: UndefVarError: CenteredSecondOrder not defined; Stacktrace:; [1] #1 at ./none:0 [inlined]; [2] iterate at ./generator.jl:47 [inlined]; [3] collect(::Base.Generator{Array{Int64,1},var""#1#2""{Float64,Int64,Int64,Float64}}) at ./array.jl:686; [4] run_convergence_test(::Float64, ::Int64, ::Array{Int64,1}) at /home/fpoulin/software/Oceananigans.jl/validation/convergence_tests/one_dimensional_cosine_advection_diffusion.jl:28; [5] top-level scope at /home/fpoulin/software/Oceananigans.jl/validation/convergence_tests/one_dimensional_cosine_advection_diffusion.jl:38; [6] include(::String) at ./client.jl:457; [7] top-level scope at REPL[1]:1; in expression starting at /home/fpoulin/software/Oceananigans.jl/validation/convergence_tests/one_dimensional_cosine_advection_diffusion.jl:38; ```",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1273:154,Load,LoadError,154,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1273,1,['Load'],['LoadError']
Performance,"This week I plan to revisit this and hopefully figure out how to make the `ShallowWaterModel` stable and not experimental. Lots of nice work was started on the multi-layer version and I hope that we can continue to work towards that after we get the fluxes and the regression tests. I just wanted to confirm that people are still supportive of this before I dive in too deep. If yes, @simone-silvestri , would you have time to meet for 1 hour next week to fine tune things?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3050#issuecomment-1808221910:461,tune,tune,461,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3050#issuecomment-1808221910,1,['tune'],['tune']
Performance,Till now the distributed module was the last module loaded in Oceananigans :(; For this reason it depended on the Models module. It should definitely be the other way around!. Therefore in this PR:. - Reduced `MultiCPU` and `MultiGPU` to just one type `MultiArch`; - `MultiArch` can infer the underlying architecture by looking at its local grid which is built upon construction; - the local grid can accept rectilinear and lat-lon grids and uniform and stretched domains; - Made Models depend on Distributed and not the other way around; - deleted a bunch of files which were not required anymore. with this infrastructure in place the following steps will be; - make the `BoundaryConditions` module depend on `Distributed` (and not the other way around); - implement GPU halo passing,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2073:52,load,loaded,52,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2073,1,['load'],['loaded']
Performance,"To fill in a few more details for @hennyg888 --- _almost all_ multithreading in Oceananigans is achieved via [`KernelAbstractions.jl`](https://github.com/JuliaGPU/KernelAbstractions.jl). Improving efficiency for Oceananigans kernels likely means contributing to `KernelAbstractions.jl` (which @vchuravy may or may not be excited about :-D). More specifically, all tendency evaluations, non-communicative / non-periodic halo fills (periodic halo filling uses Base broadcasting and thus is not parallelized), integrals (like the hydrostatic pressure integral, or vertical velocity computation in `HydrostaticFreeSurfaceModel`), evaluation of diagnostics, and broadcasting with fields all use KernelAbstractions via the Oceananigans function `launch!`:. https://github.com/CliMA/Oceananigans.jl/blob/6e39d3fcc098c69ac207cc21be759cf6bd3ec604/src/Utils/kernel_launching.jl#L71-L90. The line . ```julia; event = loop!(args...; dependencies=dependencies, kwargs...); ```. launches a kernel, using [`KernelAbstractions` syntax](https://juliagpu.github.io/KernelAbstractions.jl/stable/#Quickstart-1). `event` is a token that can be ""waited"" on if we need to. So either we can improve multithreading by changing what happens when `loop!` is called --- or, possibly, by refining the dependency tree so that we can launch more kernels simultaneously. The second optimization is probably more important for small problems. You have mostly benchmarked fairly large problems so I don't we'd see much speed for them. But I'm not 100% sure.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1861#issuecomment-880826423:1350,optimiz,optimization,1350,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1861#issuecomment-880826423,1,['optimiz'],['optimization']
Performance,"To summarize the updates, we can make use of the type `ConditionalOperation` to perform conditional reductions with a mask. If a mask is not specified, the specific neutral value for that particular reduction will be used. Example:. ```; julia> grid = RectilinearGrid(arch, size = (4, 1, 1), extent = (1, 1, 1)); RectilinearGrid{Float64, Periodic, Periodic, Bounded}; architecture: CPU(); domain: x ∈ [0.0, 1.0], y ∈ [0.0, 1.0], z ∈ [-1.0, 0.0]; topology: (Periodic, Periodic, Bounded); size (Nx, Ny, Nz): (4, 1, 1); halo (Hx, Hy, Hz): (1, 1, 1); spacing in x: Regular, with spacing 0.25; spacing in y: Regular, with spacing 1.0; spacing in z: Regular, with spacing 1.0. julia> field = Field{Center, Center, Center}(grid); Field located at (Center, Center, Center); ├── data: OffsetArrays.OffsetArray{Float64, 3, Array{Float64, 3}}, size: (4, 1, 1); ├── grid: RectilinearGrid{Float64, Periodic, Periodic, Bounded}(Nx=4, Ny=1, Nz=1); └── boundary conditions: west=Periodic, east=Periodic, south=Periodic, north=Periodic, bottom=ZeroFlux, top=ZeroFlux, immersed=ZeroFlux. julia> set!(field, [1, 2, 3, 4]). julia> sum(field); 10.0. julia> sum(field, condition = field .> 2); 7.0. julia> sum(field, condition = field .> 2, mask = 100); 207.0. julia> sum(field, condition = (i, j, k, grid, field) -> grid.xᶜᵃᵃ[i] < 0.3); 1.0; ```. This type is used to automatically exclude the immersed region in reductions of `AbstractField{<:Any, <:Any, <:Any, <:ImmersedBoundaryGrid}`",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2097#issuecomment-1018860248:80,perform,perform,80,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2097#issuecomment-1018860248,1,['perform'],['perform']
Performance,Travis below seems to be queued but if you click on the travis link it shows that everything run ok... weird...,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1072#issuecomment-711074061:25,queue,queued,25,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1072#issuecomment-711074061,1,['queue'],['queued']
Performance,Tries to catch the race condition that produces #1767 .,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1784:19,race condition,race condition,19,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1784,1,['race condition'],['race condition']
Performance,"True, but the abstraction I was thinking about would be far more general! Users would define fluxes by writing code of the form. ```julia; average_flux = HorizontalAverage(κₑ * ∂z(T), frequency=10); ```. for example, where `κₑ` and `T` are eddy diffusivity and temperature fields. The operator `∂z(T)` will return an object `Derivative` and the operation `κₑ * ∂z(T)` will return a `BinaryOperation`. These objects will behave like fields in the sense that calling `getindex(obj, inds...)` will perform the necessary interpolation, difference, and arithmetic operations to calculate the values at the specified indices. In particular, running such `HorizontalAverage` diagnostic will compute the flux with a kernel (eg `compute!(scratch_array, binary_operation)`) and store it in some intermediate scratch array (like pressure), and then compute the horizontal average of that. Still not sure how to specify momentum fluxes, since those expressions are a bit more complicated. But we can provided aliases for those cases that utilize this abstraction system.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/428#issuecomment-536201187:495,perform,perform,495,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/428#issuecomment-536201187,1,['perform'],['perform']
Performance,True. I guess a performance loss due to type instability isn't super important --- we need to solve type instability anyways.,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/250#issuecomment-497061273:16,perform,performance,16,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/250#issuecomment-497061273,1,['perform'],['performance']
Performance,True. I guess we don't have a separate architecture for multi-threaded. I agree that `MPI_CPU` or `MPICPU` would be more precise. What do you think of `DistributedCPU` and `DistributedGPU`?,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1502#issuecomment-804952861:56,multi-thread,multi-threaded,56,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1502#issuecomment-804952861,1,['multi-thread'],['multi-threaded']
Performance,True. Should probably also link to https://docs.julialang.org/en/v1/manual/performance-tips/,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1478#issuecomment-804942693:75,perform,performance-tips,75,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1478#issuecomment-804942693,1,['perform'],['performance-tips']
Performance,"Try. ```julia; const width = 100kilometers; @inline ν(x, y, z, t) = ifelse(abs(y - Ny*kilometers/2) < Ny*kilometers/2 - width, 1, 10) # sponge layers; ```. The `const` is necessary here for GPU and will also improve performance on CPU. The reason is that we need to assure the compiler that the type of `width` will not change for GPU compilation to be possible. If that fails you may also need to change `kilometers` to `1e3`, or to define another `const`. However, I think that `kilometers` should already b `const` so this may not be necessary.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2359#issuecomment-1068643259:216,perform,performance,216,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2359#issuecomment-1068643259,1,['perform'],['performance']
Performance,"Trying to use OceanScalingTest.jl from @simone-silvestri as a benchmark for TTFTS (Time-To-First-Time-Step) I came across this delightful error:. ```; ERROR: LoadError: Evaluation into the closed module `Grids` breaks incremental compilation because the side effects will not be permanent. This is likely due to some other module mutating `Grids` with `eval` during precompilation - don't do this.; Stacktrace:; [1] eval; @ ./boot.jl:428 [inlined]; [2] allocate_metrics(grid::Oceananigans.Grids.LatitudeLongitudeGrid{Float64, Oceananigans.Grids.Periodic, Oceananigans.Grids.Bounded, Oceananigans.Grids.Bounded, Nothing, Nothing, Float64, Float64, OffsetArrays.OffsetVector{Float64, CUDA.CuArray{Float64, 1, CUDA.Mem.DeviceBuffer}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVector{Float64, CUDA.CuArray{Float64, 1, CUDA.Mem.DeviceBuffer}}, Oceananigans.DistributedComputations.Distributed{Oceananigans.Architectures.GPU, false, Oceananigans.DistributedComputations.Partition{Int64, Int64, Int64}, Tuple{Int64, Int64, Int64}, Int64, Tuple{Int64, Int64, Int64}, Oceananigans.DistributedComputations.RankConnectivity{Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing}, MPI.Comm, Vector{MPI.Request}, Base.RefValue{Int64}}}); @ Oceananigans.Grids ~/.julia/packages/Oceananigans/kBe5X/src/Grids/latitude_longitude_grid.jl:554; [3] with_precomputed_metrics(grid::Oceananigans.Grids.LatitudeLongitudeGrid{Float64, Oceananigans.Grids.Periodic, Oceananigans.Grids.Bounded, Oceananigans.Grids.Bounded, Nothing, Nothing, Float64, Float64, OffsetArrays.OffsetVector{Float64, CUDA.CuArray{Float64, 1, CUDA.Mem.DeviceBuffer}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3555:158,Load,LoadError,158,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3555,1,['Load'],['LoadError']
Performance,"Tuple{(), Tuple{}}}}, NamedTuple{(:u, :v, :w), Tuple{OffsetArrays.OffsetArray{Float64, 3, CUDA.CuDeviceArray{Float64, 3, 1}}, OffsetArrays.OffsetArray{Float64, 3, CUDA.CuDeviceArray{Float64, 3, 1}}, OffsetArrays.OffsetArray{Float64, 3, CUDA.CuDeviceArray{Float64, 3, 1}}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:a,), Tuple{Int64}}, Nothing, NamedTuple{(:u, :v, :w, :a), Tuple{Oceananigans.Forcings.ContinuousForcing{Face, Center, Center, Nothing, typeof(forc_u), Nothing, Tuple{Int64}, Tuple{typeof(Oceananigans.Operators.identity4)}}, typeof(Oceananigans.Forcings.zeroforcing), typeof(Oceananigans.Forcings.zeroforcing), Nothing}}, OffsetArrays.OffsetArray{Float64, 3, CUDA.CuDeviceArray{Float64, 3, 1}}, NamedTuple{(:time, :iteration, :stage), Tuple{Float64, Int64, Int64}}}}}}); @ GPUCompiler ~/.julia/packages/GPUCompiler/kb6yJ/src/driver.jl:76; [9] cufunction_compile(job::GPUCompiler.CompilerJob); @ CUDA ~/.julia/packages/CUDA/BbliS/src/compiler/execution.jl:347; [10] cached_compilation(cache::Dict{UInt64, Any}, job::GPUCompiler.CompilerJob, compiler::typeof(CUDA.cufunction_compile), linker::typeof(CUDA.cufunction_link)); @ GPUCompiler ~/.julia/packages/GPUCompiler/kb6yJ/src/cache.jl:90; [11] cufunction(f::typeof(Oceananigans.Models.NonhydrostaticModels.gpu_calculate_Gu!), tt::Type{Tuple{KernelAbstractions.CompilerMetadata{KernelAbstractions.NDIteration.StaticSize{(4, 4, 4)}, KernelAbstractions.NDIteration.DynamicCheck, Nothing, Nothing, KernelAbstractions.NDIteration.NDRange{3, KernelAbstractions.NDIteration.StaticSize{(1, 1, 4)}, KernelAbstractions.NDIteration.StaticSize{(16, 16, 1)}, Nothing, Nothing}}, OffsetArrays.OffsetArray{Float64, 3, CUDA.CuDeviceArray{Float64, 3, 1}}, RectilinearGrid{Float64, Periodic, Periodic, Bounded, Float64, Float64, Float64, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.Tw",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3025#issuecomment-1481807353:8717,cache,cache,8717,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3025#issuecomment-1481807353,1,['cache'],['cache']
Performance,"Underlying cause seems to be https://github.com/JuliaGPU/CUDA.jl/issues/1169 so we should probably not update CUDA.jl until it's fixed. We could also pin the current version of CUDA.jl (a change which should propagate to users I think?). cc @Yixiao-Zhang. ---. **Minimal working example**. ```julia; using Oceananigans. grid = RegularRectilinearGrid(topology=(Periodic, Bounded, Bounded), size=(85, 1320, 100), extent=(1, 1, 1)); model = NonhydrostaticModel(architecture=GPU(); grid); simulation = Simulation(model, Δt=1, stop_time=10, progress = sim -> @info ""iteration $(sim.model.clock.iteration)""). V = AveragedField(model.velocities.v, dims=1); simulation.output_writers[:zonal_averages] =; NetCDFOutputWriter(model, (; V), filepath=""zonal_averages.nc"", schedule=TimeInterval(1), verbose=true). run!(simulation); ```. produces this GPU compiler error. ```julia; ERROR: LoadError: InvalidIRError: compiling kernel broadcast_kernel(CUDA.CuKernelContext, SubArray{Float64, 3, CUDA.CuDeviceArray{Float64, 3, 1}, Tuple{UnitRange{Int64}, Base.Slice{Base.OneTo{Int64}}, Base.Slice{Base.OneTo{Int64}}}, false}, Base.Broadcast.Broadcasted{Nothing, Tuple{Base.OneTo{Int64}, Base.OneTo{Int64}, Base.OneTo{Int64}}, typeof(identity), Tuple{Base.Broadcast.Extruded{SubArray{Float64, 3, CUDA.CuDeviceArray{Float64, 3, 1}, Tuple{UnitRange{Int64}, Base.Slice{Base.OneTo{Int64}}, Base.Slice{Base.OneTo{Int64}}}, false}, Tuple{Bool, Bool, Bool}, Tuple{Int64, Int64, Int64}}}}, Int64) resulted in invalid LLVM IR; Reason: unsupported call to an unknown function (call to julia.gpu.state_getter); Stacktrace:; [1] kernel_state_pointer; @ ~/.julia/packages/GPUCompiler/j0ybe/src/irgen.jl:695; [2] kernel_state; @ ~/.julia/packages/CUDA/dNx3X/src/device/runtime.jl:31; [3] exception_flag; @ ~/.julia/packages/CUDA/dNx3X/src/device/runtime.jl:33; [4] signal_exception; @ ~/.julia/packages/CUDA/dNx3X/src/device/runtime.jl:36; [5] multiple call sites; @ unknown:0; Stacktrace:; [1] check_ir(job::GPUCompiler.CompilerJob{G",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1995:874,Load,LoadError,874,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1995,1,['Load'],['LoadError']
Performance,"Unfortunately it doesn't work for me:. ```julia; (base) tomas@np900:~/repos/Oceananigans.jl$ julia1.6 --project=docs/ -e 'using Pkg; Pkg.instantiate()'; (base) tomas@np900:~/repos/Oceananigans.jl$ julia1.6 --project=docs/ docs/make.jl; ERROR: LoadError: LoadError: LoadError: ArgumentError: Package SpecialFunctions [276daf66-3868-5448-9aa4-cd146d93841b] is required but does not seem to be installed:; - Run `Pkg.instantiate()` to install all recorded dependencies. Stacktrace:; [1] _require(pkg::Base.PkgId); @ Base ./loading.jl:990; [2] require(uuidkey::Base.PkgId); @ Base ./loading.jl:914; [3] require(into::Module, mod::Symbol); @ Base ./loading.jl:901; [4] include(mod::Module, _path::String); @ Base ./Base.jl:386; [5] include(x::String); @ CUDA ~/.julia/packages/CUDA/3VnCC/src/CUDA.jl:1; [6] top-level scope; @ ~/.julia/packages/CUDA/3VnCC/src/device/intrinsics.jl:22; [7] include(mod::Module, _path::String); @ Base ./Base.jl:386; [8] include(x::String); @ CUDA ~/.julia/packages/CUDA/3VnCC/src/CUDA.jl:1; [9] top-level scope; @ ~/.julia/packages/CUDA/3VnCC/src/CUDA.jl:46; [10] include; @ ./Base.jl:386 [inlined]; [11] include_package_for_output(pkg::Base.PkgId, input::String, depot_path::Vector{String}, dl_load_path::Vector{String}, load_path::Vector{String}, concrete_deps::Vector{Pair{Base.PkgId, UInt64}}, source::String); @ Base ./loading.jl:1213; [12] top-level scope; @ none:1; [13] eval; @ ./boot.jl:360 [inlined]; [14] eval(x::Expr); @ Base.MainInclude ./client.jl:446; [15] top-level scope; @ none:1; in expression starting at /home/tomas/.julia/packages/CUDA/3VnCC/src/device/intrinsics/math.jl:5; in expression starting at /home/tomas/.julia/packages/CUDA/3VnCC/src/device/intrinsics.jl:22; in expression starting at /home/tomas/.julia/packages/CUDA/3VnCC/src/CUDA.jl:1; ERROR: LoadError: Failed to precompile CUDA [052768ef-5323-5732-b1bb-66c8b64840ba] to /home/tomas/.julia/compiled/v1.6/CUDA/jl_q4lPlx.; Stacktrace:; [1] error(s::String); @ Base ./error.jl:33; [2] compile",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1707#issuecomment-849206371:243,Load,LoadError,243,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1707#issuecomment-849206371,6,"['Load', 'load']","['LoadError', 'loading']"
Performance,Unfortunately the previews clog up the [OceananigansDocumentation](https://github.com/CliMA/OceananigansDocumentation) repository and we need to perform manual cleanup often... I suggest we drop the doc previews for the moment. We could build the docs manually _or_ temporarily enable them for a particular PR that includes Doc changes and then disable them again.,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2870:145,perform,perform,145,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2870,1,['perform'],['perform']
Performance,Update Performance Benchmarks in Docs and README,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1676:7,Perform,Performance,7,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1676,1,['Perform'],['Performance']
Performance,Update performance benchmarks plot and discuss results,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/607:7,perform,performance,7,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/607,1,['perform'],['performance']
Performance,"Update: I have been able to reduce the error to be a procompiling error in oceananigans.jl:; ```; ERROR: LoadError: Failed to precompile Oceananigans [9e8cae18-63c1-5223-a75c-80ca9d6e9a09] to ""/glade/u/home/knudsenl/.julia/compiled/v1.9/Oceananigans/jl_AMNEzH"".; Stacktrace:; [1] error(s::String); @ Base ./error.jl:35; [2] compilecache(pkg::Base.PkgId, path::String, internal_stderr::IO, internal_stdout::IO, keep_loaded_modules::Bool); @ Base ./loading.jl:2300; [3] compilecache; @ ./loading.jl:2167 [inlined]; [4] _require(pkg::Base.PkgId, env::String); @ Base ./loading.jl:1805; [5] _require_prelocked(uuidkey::Base.PkgId, env::String); @ Base ./loading.jl:1660; [6] macro expansion; @ ./loading.jl:1648 [inlined]; [7] macro expansion; @ ./lock.jl:267 [inlined]; [8] require(into::Module, mod::Symbol); @ Base ./loading.jl:1611; in expression starting at /glade/derecho/scratch/knudsenl/BottomBoundaryLayer/testcode.jl:1; ```; I have been trying to make sure that everything is up to date, and I am running on Julia version 1.9.2. My code is just; ```; using Oceananingans; ```; as I have been trying to get the library to load properly. Does anyone have any experience with this error or does it make things any clearer?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2233710372:105,Load,LoadError,105,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2233710372,8,"['Load', 'load']","['LoadError', 'load', 'loading']"
Performance,"Upon closer inspection of the [REDFT01 (DCT-III) transform](http://www.fftw.org/fftw3_doc/1d-Real_002deven-DFTs-_0028DCTs_0029.html) in FFTW, I should have been dividing the first element _X₀_ by 2 in our DCT-III implementation using the IFFT. The correct IDCT is thus; ```julia; function idct_dim3_gpu!(f); Nx, Ny, Nz = size(f); ; bfactors = exp.(collect(1im*π*(0:Nz-1) / (2*Nz))); bfactors[1] *= 0.5. f .*= cu(repeat(reshape(bfactors, 1, 1, Nz), Nx, Ny, 1)); ifft!(f, 3); ; f .= cu(reshape(permutedims(cat(f[:, :, 1:Int(Nz/2)], f[:, :, end:-1:Int(Nz/2)+1]; dims=4), (1, 2, 4, 3)), Nx, Ny, Nz)); # @. f = real(f) # Don't do it here. We'll do it when assigning real(ϕ) to pNHS to save some measly FLOPS.; ; nothing; end; ```; This can all be optimized, but at least a working Poisson solver is a good step.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/56#issuecomment-465425101:742,optimiz,optimized,742,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/56#issuecomment-465425101,1,['optimiz'],['optimized']
Performance,"Using ; ```; grid = RectilinearGrid(arch, size=(Nx, Ny, Nz), halo=(4, 4, 4), x=(0, 2π), y=(0, 2π), z=(0, 1), topology=(Periodic, Periodic, Bounded)). bottom = zeros(Nx, Ny); bound = Int.(Ny/2-40:Ny/2+40); bottom[bound, bound] .= 0.5. grid = ImmersedBoundaryGrid(grid, GridFittedBottom(bottom)); mrg = MultiRegionGrid(grid, partition=XPartition(2), devices = 2); ```; and ; ```; u(x, y, z) = (x * z) / 10; ```. (with `ImplicitFreeSurface`) we get. #### Strong Scaling; | Grid size | Grid | GPUs | wall time | efficiency |; | -- | -- | -- | -- | -- |; | `1440×600×48`| `RectilinearGrid` | 1 | 1.85 minutes | 100% |; | `1440×600×48`| `MultiRegionGrid` | 2 | 1.12 minutes | 82.5% |. Scaling gets better?? at this point I a little confused...; Bottomline... ; there is still a bunch of optimization and more systematic benchmarking to be done, I'll merge this PR and then we can think about improving the scaling, the first things that come in mind are; - Ensure that all the `apply_regionally!` and `construct_regionally` calls are asynchronous. This might not be the case if there are memory copies inside function calls. That would serialize the execution of part of the code. To ensure this we require a more in-depth profiling using ***nsys***; - remove all `fill_halo_regions!` that are not `Periodic` or `Communication` which will allow asynchronous execution of halo filling across different direction (luckily already being done in #2477); - Bundle together the halo passing in a single boundary buffer to allow sending field tuples together (depends on #2509). Additional work to do on `MultiRegion` is ; - Perform more systematic benchmarking; - Design correct `OutputWriters` and `OutputReaders` for `MultiRegionFields`. Maybe not immediate priorities but definitely important; - Adapt `RungeKutta3` to `Multiregion` through `@apply_regionally`; - Implement a multi-region version of the Nonhydrostatic pressure solver; - Overlap computation and communication (for this we require _non-blocking",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2253#issuecomment-1116853178:781,optimiz,optimization,781,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2253#issuecomment-1116853178,1,['optimiz'],['optimization']
Performance,"Using Julia 1.7.3 and Oceananigans 0.77.1, this code:. ```julia; using Oceananigans. grid = RectilinearGrid(GPU(), size=(1, 1, 2), extent=(1, 1, 1)); buoyancy_bottom_bc = GradientBoundaryCondition(1e-5); buoyancy_bcs = FieldBoundaryConditions(bottom=buoyancy_bottom_bc); model = NonhydrostaticModel(; grid, tracers=:b, buoyancy=BuoyancyTracer(),; boundary_conditions=(; b=buoyancy_bcs)); simulation = Simulation(model; Δt=1, stop_iteration=2); run!(simulation); ```. produces:. ```julia; julia> include(""simple_model.jl""); ERROR: LoadError: InvalidIRError: compiling kernel #gpu__fill_bottom_and_top_halo!(KernelAbstractions.CompilerMetadata{KernelAbstractions.NDIteration.StaticSize{(1, 1)}, KernelAbstractions.NDIteration.DynamicCheck, Nothing, Nothing, KernelAbstractions.NDIteration.NDRange{2, KernelAbstractions.NDIteration.StaticSize{(1, 1)}, KernelAbstractions.NDIteration.StaticSize{(1, 1)}, Nothing, Nothing}}, NTuple{4, OffsetArrays.OffsetArray{Float64, 3, CUDA.CuDeviceArray{Float64, 3, 1}}}, Tuple{BoundaryCondition{Oceananigans.BoundaryConditions.Flux, Nothing}, BoundaryCondition{Oceananigans.BoundaryConditions.Flux, Nothing}, BoundaryCondition{Oceananigans.BoundaryConditions.Open, Nothing}, BoundaryCondition{Oceananigans.BoundaryConditions.Gradient, Float64}}, Tuple{BoundaryCondition{Oceananigans.BoundaryConditions.Flux, Nothing}, BoundaryCondition{Oceananigans.BoundaryConditions.Flux, Nothing}, BoundaryCondition{Oceananigans.BoundaryConditions.Open, Nothing}, BoundaryCondition{Oceananigans.BoundaryConditions.Flux, Nothing}}, Tuple{Tuple{Face, Center, Center}, Tuple{Center, Face, Center}, Tuple{Center, Center, Face}, Tuple{Center, Center, Center}}, RectilinearGrid{Float64, Periodic, Periodic, Bounded, Float64, Float64, Float64, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, In",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2709:530,Load,LoadError,530,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2709,1,['Load'],['LoadError']
Performance,"Using `BenchmarkGroup` from BenchmarkTools.jl, DataFrames.jl, and PrettyTables.jl we can develop a more powerful and elegant benchmarking framework, allowing us to easily perform more benchmarks and compare them. `BenchmarkGroup` is serializable so we can use BSON.jl or JLD2.jl to save multiple benchmarks or suites of benchmarks to disk and collect them afterwards. This would allow us to automate multi-threading benchmarks and run benchmarks for large models that almost fill memory (GPU garbage collection doesn't always seem to free memory in these cases?). Using `BenchmarkGroup` will also allow us to automate benchmarking between branches so we can easily discover performance regressions. Right now I do this manually. Here is a crude first working example: https://github.com/CliMA/Oceananigans.jl/blob/fd6e173042497464d78e4f03f4e850f55171c74a/compressible/benchmarks/benchmark_compressible_model.jl",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1088:171,perform,perform,171,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1088,3,"['multi-thread', 'perform']","['multi-threading', 'perform', 'performance']"
Performance,"Using `Cthulhu` by running the commands. ```julia; using Oceananigans, Cthulhu. model = Model(N=(32, 32, 32), L=(1, 1, 1)) . @descend time_step!(model, 1, 1); ```. and descending into the call to `store_previous_source_terms!` (I did this using the optimize flag and scrolling to invocation `%206`, then using the warn_type flag), reveals (I think) that for some reason julia cannot infer the types of arguments into any of the kernels. For example, the `Cthulhu` output from that exercise yields. ```; @ /Users/gregorywagner/Projects/Oceananigans.jl/src/time_steppers.jl:122 within `store_previous_source_terms!'; 15 ┄ %74 = (GPUifyLoops.isdevice)()::Core.Compiler.Const(false, false); │ %75 = !%74::Core.Compiler.Const(true, true); │ %75; │ %77 = (Base.getproperty)(grid, :Nx)::Any; ```. The type of `grid` is not inferred, and neither is the type of `grid.Nx`. None of the types of the arguments are inferred. I can fix this problem by explicitly typing all of the arguments except for the floating point type. However, I cannot fix the problem even if I specify that most of the arguments are `AbstractArray`. Is this causing performance problems? Is this the source of memory allocation during time-stepping?. @vchuravy @ali-ramadhan",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/251:249,optimiz,optimize,249,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/251,2,"['optimiz', 'perform']","['optimize', 'performance']"
Performance,Using short-circuiting logic inside GPU kernels would reduce performance rather than improve it.,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/987#issuecomment-1056924530:61,perform,performance,61,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/987#issuecomment-1056924530,1,['perform'],['performance']
Performance,Validation and performance benchmarks.,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/136:15,perform,performance,15,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/136,1,['perform'],['performance']
Performance,Verification tests comparing performance of different LES closures,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/441:29,perform,performance,29,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/441,1,['perform'],['performance']
Performance,"Wall-normal velocities can depend on `model_fields`:. https://github.com/CliMA/Oceananigans.jl/blob/5aafe8ee1d3e49a53906e7225a01fc18f1a5f165/src/BoundaryConditions/fill_halo_regions_normal_flow.jl#L15-L18. and wall-normal velocities are updated _after_ an RK3 substep, but _before_ the pressure solve:. https://github.com/CliMA/Oceananigans.jl/blob/5aafe8ee1d3e49a53906e7225a01fc18f1a5f165/src/TimeSteppers/pressure_correction.jl#L6-L10. Thus for some problems the wall-normal velocity fields are updated based on the predictor model fields (both the predictor velocity and the updated tracer fields) that result from an RK3 substep. This devious bug can be avoided simply by _not updating wall-normal velocity components on the boundary_ in the RK3 substep by changing the indexing in the rk3 substep as well as the worksize here:. https://github.com/CliMA/Oceananigans.jl/blob/5aafe8ee1d3e49a53906e7225a01fc18f1a5f165/src/TimeSteppers/runge_kutta_3.jl#L124. Then we don't have to fill halo regions before performing the pressure correction. The resulting algorithm is both more correct and computationally less expensive. Note that doing this could require a bit of gymnastics to get the indexing right in the rk3 substep kernel:. https://github.com/CliMA/Oceananigans.jl/blob/5aafe8ee1d3e49a53906e7225a01fc18f1a5f165/src/TimeSteppers/runge_kutta_3.jl#L178-L186",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1156:1007,perform,performing,1007,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1156,1,['perform'],['performing']
Performance,"We currently calculate the barotropic mode inside the AB2 step:. https://github.com/CliMA/Oceananigans.jl/blob/e27988039e65e9244a84d241b70c2dcdeac93309/src/Models/HydrostaticFreeSurfaceModels/hydrostatic_free_surface_ab2_step.jl#L18-L22. I'm wondering if it makes more sense to calculate this inside `update_state!`. Our algorithm calculates `update_state!` at the _end_ of a time-step, which means that after calling `time_step!(model)` the model auxiliary and prognostic state are all concurrent, which is useful for output. cc @sandreza @simone-silvestri",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2244:487,concurren,concurrent,487,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2244,1,['concurren'],['concurrent']
Performance,"We currently have a `Timeseries` diagnostic which accumulates the output of `some_function(model)` in a vector `data`. . It'd be nice to have a simple way to save a timeseries to file. The problem is that we don't want to duplicate data; eg we want to periodically append data to an array that was previously saved in a file. . Note that it is not possible to, for example, delete data within a `JLD2` file and replace it with something else:. https://github.com/JuliaIO/JLD2.jl/issues/38. @ali-ramadhan @suyashbire1 is this possible in NetCDF / `NCDatasets.jl`?. There are two other possibilities. One is to ""unspool"" the time series into a `group` within a `JLD2` file and write the data from each time stamp individually in the file. This allows new data to be appended to the group easily. I'm not sure how performant this would be, but perhaps its fine. The second is to simply delete the old file and replace it with a new one with the updated time series. This is the hack that I've been using so far. The downside is that we have to save each timeseries in their own file. Curious to hear if there are any other ideas for solving this problem.",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/502:811,perform,performant,811,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/502,1,['perform'],['performant']
Performance,"We have always launched a kernel for flux bcs! Our method is 1) fill halos to nullify diffusive fluxes across boundaries; 2) add the specified fluxes directly to boundary cells. Step 1 requires filling halos, which we have always done for flux bcs. Previously, we used broadcasting for flux bcs, similar to what we do for periodic bcs. When the halo size was greater than 1, the intent of the broadcast operation was violated, however, because zero diffusive flux was not guaranteed for biharmonic diffusivity. This PR corrects that, allowing biharmonic diffusivities to be used in bounded domains. Some informal benchmarks suggest that a KernelAbstractions kernel can be more efficient than using GPUArrays broadcasting. KernelAbstractions kernels are also multithreaded, unlike broadcast operations. Thus I would expect speed up from this change, rather than slowdown (but likely negligible speed up). But of course benchmarking is a good idea, and if there is a problem we should open an issue. Another advantage is that we can utilize KernelAbstractions dependency feature more fully if we get to the point where we do not use broadcasting for anything. We haven't optimized our dependency structure, however. Also, there are a few puzzles to be worked out regarding periodic boundary conditions before we can convert the periodic halo filling to kernels.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/904#issuecomment-686430220:1169,optimiz,optimized,1169,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/904#issuecomment-686430220,1,['optimiz'],['optimized']
Performance,"We have to debate the best choice... fate cannot decide it for us. I think split-explicit is probably the best choice for a default --- it's performant in all contexts, and potentially also the least complicated? I believe it will also handle nonlinearity better than an implicit formulation (which we don't support now, but could support in the future). @simone-silvestri can probably best advise. Not sure how the docstring diverged from the code.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3289#issuecomment-1734789419:141,perform,performant,141,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3289#issuecomment-1734789419,1,['perform'],['performant']
Performance,"We may able to provide an interface for specifying background tracer distributions and velocity fields (which perform advection only?) using `FunctionField`s and the existing advection operators. I think we would want this functionality to assume that the _linear_ balances between background terms are somehow separately satisfied. With that assumption we can include just the two nonlinear terms associated with 1) advection of the resolved field by the background field and 2) advection of the background field by the resolved fields. For this to be easily implemented we need to change the function signature of the advection operators for momentum from. ```julia; div_ũu(i, j, k, grid, advection, U); ```. to . ```julia; div_ũu(i, j, k, grid, advection, U, u); ```. which then enables us to include background terms via. ```julia; div_ũu(i, j, k, grid, advection, U_background, u) + div_ũu(i, j, k, grid, advection, U, u_background); ```. for example. Tracers are fine, since the current function is. ```julia; div_uc(i, j, k, grid, advection, U, c); ```. which will become. ```julia; div_uc(i, j, k, grid, advection, U_background, c) + div_uc(i, j, k, grid, advection, U, c_background); ```",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/960:110,perform,perform,110,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/960,1,['perform'],['perform']
Performance,We maybe want to optimize the construction of the `SparseInverse`.; I naively wrote a serial code to do that because I didn't know that `CUDA.jl` had implemented a GPU version of the `qr` factorization. Since the construction of the preconditioner is embarrassingly parallelizable we can shift it to the GPU easily,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2463#issuecomment-1119052363:17,optimiz,optimize,17,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2463#issuecomment-1119052363,1,['optimiz'],['optimize']
Performance,"We might need to implement the K-Profile boundary layer turbulence closure so that we can use it to compare the CATKE performance. . Ideally, we'd like to calibrate the K-Profile parametrisation using [OceanTurbulenceParameterEstimation.jl](http://github.com/CliMA/OceanTurbulenceParameterEstimation.jl) alongside with CATKE closure and then compare how the two perform under various scenarios. @glwagner, would it be easy to implement KPP, similarly as done in [OceanTurb.jl](https://github.com/glwagner/OceanTurb.jl)?. cc @rafferrari",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2149:118,perform,performance,118,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2149,2,['perform'],"['perform', 'performance']"
Performance,"We need a generic type that represents a reduction operation over a field that can be used both by the user to compute arbitrary reductions on the fly, and also for integration into `AbstractOperations`. One way to accomplish this is to define a new field that looks something like. ```julia; struct ReducedField{X, Y, Z, A, G, O, D, I} <: AbstractLocatedField{X, Y, Z, G}; data :: A; grid :: G; reduction! :: O; dims :: D; input :: I; end; ```. here `data` and `grid` are an array and grid similar to an ordinary `Field`, except that `data` is reduced along the tuple of dimensions in `dims`. The `input` is the 3D field / OffsetArray on which the reduction is performed. The function `reduction` performs the reduction; eg. ```julia; U = ReducedField(mean, model.velocities.u, dims=(1, 2)); ```. Returns an object `U` that represents the average of `model.velocities.u` over dims `(1, 2)` (x and y). We can also write wrappers / translators that allow dimensions to be referenced by name rather than number (integration with DimensionalData.jl would help in this respect). . Computing a reduction would look something like. ```julia; function compute!(reduced); zero_halo_regions!(parent(reduced.data), model.grid); reduced.reduction!(parent(reduced.data), parent(reduced.input), dims=reduced.dims); return nothing; end; ```. Note that `mean` requires a special implementation since we have to use `sum!` followed by normalization. But this is not too onerous; we can dispatch on that scenario, and other scenarios that require special implementation not covered by the generic version above. We can then integrate `ReducedField`s into abstract operations so we can calculate things like. ```julia; u, v, w = model.velocities. U = ReducedField(mean, u, dims=(1, 2)); V = ReducedField(mean, v, dims=(1, 2)). turbulent_kinetic_energy = @at (Cell, Cell, Cell) ( (u - U)^2 + (v - V)^2 + w^2 ) / 2; ```. This is a little tricky and requires the analysis of an operation tree to deduce which operations mu",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/525:662,perform,performed,662,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/525,2,['perform'],"['performed', 'performs']"
Performance,"We need an MPI tips / wiki section... been working on getting distributed simulations running on Engaging since yesterday (we need people to be able to start running distributed simulations immediately with minimal start up time). Here are a few things I learned:. * Remember to load a system MPI on your HPC, eg `module load openmpi`; * MPI may have to be installed / configured carefully --- after loading a system MPI. `MPIPreferences` can help:. ```julia; using MPIPreferences; MPIPreferences.use_system_binary(); ```. https://juliaparallel.org/MPI.jl/stable/configuration/. * Configuring SLURM correctly may also require some trial and error, because the documentation is unclear and there are a huge number of options. I can't figure out if the documentation is wrong, in fact, or if instead the engaging cluster does not support some options. Either way, a small test script like. ```julia; using MPI. MPI.Init(); @show MPI.Comm_rank(MPI.COMM_WORLD); @show MPI.Comm_size(MPI.COMM_WORLD); ```. Is helpful for debugging. Also, I recommend first trying to start jobs interactively before using `sbatch`. Interactive jobs launch faster, so we get a faster trial-error iteration. I'm still working on things, but it seems like another point may be salient:. * Initialize / precompile outside of a parallel job before trying to launch a parallel job via `mpiexec`. Here are some notes that may be specific to engaging:. I'm using `srun` to get a 4-GPU node to use interactively:. ```; srun -p my_partition --tasks-per-node=4 --cpus-per-task=32 --gres=gpu:4 --gpus-per-node=4 --mem=0 -t 01:00:00 -N 1; ```. it seems we have to use the option `gpus-per-node`, rather than setting the total number of GPUs. Also we redundantly also have to specify `gres=gpu:4` (`gres` stands for ""generalized resource""). Why `gres` can't tell what `gpus-per-node` is, I don't know. `-N 1` means 1 node. Finally we also have to specify ``--tasks-per-node=4` (matching `gres` _and_ the number of GPUs per node), otherwise",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3698#issuecomment-2389107436:279,load,load,279,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3698#issuecomment-2389107436,3,['load'],"['load', 'loading']"
Performance,"We need to think how we do this. We need a long run to produce a good figure. E.g., 100 years?. Should we run this somewhere and save output and then in this example show how we can load the output and plot?. Or should we run this somewhere for 95 years and save a checkpoint and then use this example to demonstrate how we can restart from a checkpoint and run for 5 years to produce animations/plots?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1085#issuecomment-1025239485:182,load,load,182,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1085#issuecomment-1025239485,1,['load'],['load']
Performance,"We should add a benchmark for a fully loaded model, e.g. WENO-5 + RK3 + multiple passive tracers + TEOS-10(?) + output writing + time averaging + ... With #1088 we can serialize such a `BenchmarkGroup` to disk and compare with results from `benchmark_static_ocean.jl` to automatically generate tables of slowdown values for barebones simulation -> fully loaded simulation.",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1089:38,load,loaded,38,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1089,2,['load'],['loaded']
Performance,"We should close this, but I'd like to leave an explanatory note in case others want to weigh in. This issue is really about ""vision"" than something missing with output writers. This issue really proposes that we think about JLD2 output differently (perhaps, in a way that's similar to how NetCDF views output). In that mode output would be somewhat independent of Oceananigans; if we output ""sliced"" data, then we could also output a ""sliced grid"" that represents the portion of the grid on which the data lives. One might then be able to load data and a grid ""simply"" (ie as `Array`s) and make sense of it without `using Oceananigans`. But since this issue was opened we have implemented `Field.indices` to move towards more full support for ""windowed"" or ""sliced"" fields (not _only_ when writing output), which sends us in a slightly different direction. Now a single output writer can be used to output fields that are differently sliced or windowed. We also support loading sliced fields with `FieldTimeSeries` (which is then able to correctly locate sliced fields on a grid), and doing further computations with those fields (we even envision supporting computations between fields that are sliced differently by computing index ""intersections""). Since we do save index information now when we slice a field, it is actually possible to manually slice a grid after data has been outputted. So savvy users can achieve this functionality with themselves if they (for some reason) are allergic to `FieldTimeSeries`.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1194#issuecomment-1115175994:539,load,load,539,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1194#issuecomment-1115175994,2,['load'],"['load', 'loading']"
Performance,"We should have some more sophisticated CPU and GPU benchmarks to make sure commits and pull requests don't surprisingly kill performance. Ideally along with CI (not sure if JuliaGPU's GitLab CI will let us do extensive benchmarking). Right now I just benchmark the global operators on the CPU, but I think a more meaningful benchmark would be something like average wall clock time per model time step between time steps 10-90 to avoid transients (+ no disk output) or something. Have a look at: https://github.com/KristofferC/TimerOutputs.jl; Also see: https://github.com/glwagner/CuBenchmarks",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/67:125,perform,performance,125,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/67,1,['perform'],['performance']
Performance,"We should incorporate the following validation tests which we can also use for performance benchmarking.; - [ ] Free convection (check heat budget, mixed layer depth, and turbulent kinetic energy); - [ ] Rayleigh–Bénard convection (compare Nusselt and Péclet numbers); - [x] Deep convection (regression test); - [x] Rising thermal bubble?; - [ ] Stress-driven flow (not clear which experiment we run)",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/136:79,perform,performance,79,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/136,1,['perform'],['performance']
Performance,We should run the [Performance Benchmarks](https://clima.github.io/OceananigansDocumentation/stable/benchmarks/) with the latest Oceananigans.jl version and on Julia v1.6.,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1676:19,Perform,Performance,19,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1676,1,['Perform'],['Performance']
Performance,"We think there is a race condition in the CI. Partly discussed on #3661 and also #3662, although one conclusions is that we should update to use the buildkite plugin (started on #3042)",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3751#issuecomment-2322236885:20,race condition,race condition,20,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3751#issuecomment-2322236885,1,['race condition'],['race condition']
Performance,"We use that function to initialize a correct array before performing global reductions (i.e. all zeros for sums, all 1s for prods and so on...) which are called when showing a field (if you put a semicolon after `myfield = CenterField(grid)` the error will disappear) Apparently they have changed the signature of the function in julia-1.8. Adapting Oceananigans to the new `initarray!` was on the table, I guess it's a good time to fix it",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2663#issuecomment-1185068632:58,perform,performing,58,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2663#issuecomment-1185068632,1,['perform'],['performing']
Performance,"We won't reduce computations, because these operators are not short-circuiting (this is what we want to maximize performance for these inner functions). Evaluating booleans isn't expensive. Short-circuiting logic can be expensive (or rather, prevent compiler optimizations) in hot inner loops. We've written the code so we don't use short-circuiting logic (`ifelse` rather than `if`, `&` rather than `&&`).",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2263#issuecomment-1047180293:113,perform,performance,113,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2263#issuecomment-1047180293,2,"['optimiz', 'perform']","['optimizations', 'performance']"
Performance,We've experienced a major performance regression. For v0.27.0:. ```,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/726:26,perform,performance,26,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/726,1,['perform'],['performance']
Performance,"We've seen issues noise in the vertical velocity along immersed boundaries in other configurations too. I've seen at least one result in which the noise is substantially mitigated by using the PCG solver, which avoids the approximations of the ""naive"" FFT solver. Unfortunately, we don't yet have a performant PCG-based solver cc @simone-silvestri @xkykai . I think it's interesting and also convenient that the noise is mitigated in turbulent cases or by the inclusion of bottom drag.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3526#issuecomment-2028935410:299,perform,performant,299,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3526#issuecomment-2028935410,1,['perform'],['performant']
Performance,"Weird, I see no mention of threads or multi-threading; ```; help?> plan_fft; search: plan_fft plan_fft! plan_rfft plan_ifft plan_bfft plan_ifft! plan_bfft! plan_irfft plan_brfft. plan_fft(A [, dims]; flags=FFTW.ESTIMATE, timelimit=Inf). Pre-plan an optimized FFT along given dimensions (dims) of arrays matching the shape and type of A. (The first two arguments have the same meaning as for fft.) Returns an object P which; represents the linear operator computed by the FFT, and which contains all of the information needed to compute fft(A, dims) quickly. To apply P to an array A, use P * A; in general, the syntax for applying plans is much like that of matrices. (A plan can only be applied to arrays of the same size as the A for which; the plan was created.) You can also apply a plan with a preallocated output array Â by calling mul!(Â, plan, A). (For mul!, however, the input array A must be a complex floating-point; array like the output Â.) You can compute the inverse-transform plan by inv(P) and apply the inverse plan with P \ Â (the inverse plan is cached and reused for subsequent calls to inv or; \), and apply the inverse plan to a pre-allocated output array A with ldiv!(A, P, Â). The flags argument is a bitwise-or of FFTW planner flags, defaulting to FFTW.ESTIMATE. e.g. passing FFTW.MEASURE or FFTW.PATIENT will instead spend several seconds (or more) benchmarking; different possible FFT algorithms and picking the fastest one; see the FFTW manual for more information on planner flags. The optional timelimit argument specifies a rough upper bound on; the allowed planning time, in seconds. Passing FFTW.MEASURE or FFTW.PATIENT may cause the input array A to be overwritten with zeros during plan creation. plan_fft! is the same as plan_fft but creates a plan that operates in-place on its argument (which must be an array of complex floating-point numbers). plan_ifft and so on are similar; but produce plans that perform the equivalent of the inverse transforms ifft and so",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/119#issuecomment-471179127:38,multi-thread,multi-threading,38,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/119#issuecomment-471179127,2,"['multi-thread', 'optimiz']","['multi-threading', 'optimized']"
Performance,Welcome @asapabedi thank you for opening this PR! I meant to merge it earlier. @glwagner That's what I thought but it seems that with both macros there is a very slight performance improvement. See benchmarks at: https://github.com/thabbott/JULES.jl/pull/22. The point of this was to finally close issue #13,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/434#issuecomment-536307796:169,perform,performance,169,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/434#issuecomment-536307796,1,['perform'],['performance']
Performance,"Well, it looks like this PR does not completely solve the problem, and the jury is out perhaps on whether it helps or not. I did learn a few things. It seems the race condition originates from an attempt to ""re-resolve"" the manifest during the test:. https://buildkite.com/clima/oceananigans/builds/17516#01920c32-4e89-4654-a4a8-9b077e51e87c/39-221. The puzzling part is how to solve the problem. I thought at first we could solve it by preventing the Manifest from being ""re-resolved"" --- perhaps by resolving it correctly during initialization? But actually, this isn't possible, since the problematic manifest in question is a temporary run that is created only for the tests (and its different from the project's manifest, because it also includes the test dependencies). This temporary test manifest can't be initialized as far as I can tell and is created independently for each test. Finally, it seems that some race conditions happen in a print statement (this is just from reading the error message). It all boils down eventually to these two lines:. https://github.com/JuliaLang/Pkg.jl/blob/5fbfa125045ce3e68ce10bf9fc1727bb3232c123/src/Operations.jl#L799-L800. The command its trying to run is. ```julia; ERROR: failed process: Process(`/net/ocean/home/data44/data5/glwagner/julia-1.10.5/bin/julia -C native -J/net/ocean/home/data44/data5/glwagner/julia-1.10.5/lib/julia/sys.so -O0 -g1 --color=yes -O0 --color=no --history-file=no --startup-file=no --project=/tmp/jl_f9Z8t1/Project.toml --eval 'append!(empty!(Base.DEPOT_PATH), [""/data5/glwagner/.julia-17516""]); --;   | append!(empty!(Base.DL_LOAD_PATH), String[]). cd(""/data5/glwagner/.julia-17516/packages/CUDA_Runtime_jll/YgJCI/.pkg""); --;   | include(""/data5/glwagner/.julia-17516/packages/CUDA_Runtime_jll/YgJCI/.pkg/select_artifacts.jl"");   | ' -t1 --startup-file=no x86_64-linux-gnu-libgfortran5-cxx11-julia_version+1.10.5`, ProcessSignaled(11)) [0]; ```. and supposedly the issue arises within the `select_artifacts.jl` call in `CUD",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3783#issuecomment-2362378199:162,race condition,race condition,162,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3783#issuecomment-2362378199,2,['race condition'],"['race condition', 'race conditions']"
Performance,"Well, there was a major bug associated with the fact that we were trying to avoid precomputed the diffusivities. Basically, diffusivities cannot be computed on the fly due to the ""predictor-corrector"" algorithm we use for implicit time stepping. Opting to precompute diffusivities solves this problem, at the cost of three additional 3D model fields when using `TKEBasedVerticalDiffusivity`. I think this is not a huge price to pay considering that models with 3 velocities and n tracers have (3 + n) * 3 fields already; thus in the simplest case (one buoyancy tracer and one TKE tracer) we incur ~20% (3/15) additional memory allocation from diffusivity precomputation. We can optimize memory allocation further in the future as well, potentially. On the upside, things are looking pretty good now. From the wind mixing validation:. ![image](https://user-images.githubusercontent.com/15271942/118858897-14dba700-b886-11eb-93b2-b4be1a6ea8ed.png)",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1664#issuecomment-844323514:678,optimiz,optimize,678,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1664#issuecomment-844323514,1,['optimiz'],['optimize']
Performance,"What do you mean by 'fetched'? Are you referring to memory movement from main memory to cache?. I don't know much about cache optimization, but I think a guideline is that nearby operations should be nearby in memory. For example, when looping over elements of an array `u[i]`, your machine will use a linear forecast to bring `u[i-1], u[i-2], u[i+1], u[i+2]` into the cache simultaneous to `u[i]`. This idea leads to the 'struct of arrays' optimization when running loops over the elements of many arrays simultaneously. This may be useful:. https://software.intel.com/en-us/articles/memory-layout-transformations",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/44#issuecomment-462541310:88,cache,cache,88,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/44#issuecomment-462541310,5,"['cache', 'optimiz']","['cache', 'optimization']"
Performance,"What does J-M think?; John. On Mon, Mar 9, 2020, 3:07 PM rafferrari <notifications@github.com> wrote:. > The Roquet’s approximation is perfectly sufficient for Oceananigans,; > because it will never be used for global calculations where local; > approximations are an issue. However I agree with everybody else that it; > would be best to use the same EOS in Ocenanigans and Climate_Ocean. in that; > case we should adopt TEOS-10. Be warned that it is quite inefficient; > through. So we may be hit performance-wise. Hard to tell without trying.; >; > Raffaele; >; >; >; > > On Mar 9, 2020, at 11:48 AM, Gregory L. Wagner <notifications@github.com>; > wrote:; > >; > > I also think it’s a good idea to use a full equation of state for all; > simulations sooner rather than later. It’s simpler: we won’t have to report; > constants of linearization everywhere. And setting up simulations will be; > easier.; > >; > > I’ll defer to the modelers for whether Roquet’s approximation is an; > acceptable model for TEOS-10.; > >; > > This package is relevant and we should consider contributing to it; > rather than implementing an equation of state somewhere in the Clima; > ecosystem:; > >; > > https://github.com/gher-ulg/PhysOcean.jl <; > https://github.com/gher-ulg/PhysOcean.jl>; > > —; > > You are receiving this because you were mentioned.; > > Reply to this email directly, view it on GitHub <; > https://github.com/climate-machine/Oceananigans.jl/issues/692?email_source=notifications&email_token=AK24ROIBMZMYD77AGEZNM3DRGUMURA5CNFSM4LEKJAAKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEOHZCYQ#issuecomment-596611426>,; > or unsubscribe <; > https://github.com/notifications/unsubscribe-auth/AK24ROONT4RO4YCCF6BTIIDRGUMURANCNFSM4LEKJAAA; > >.; > >; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/climate-machine/Oceananigans.jl/issues/692?email_source=notifications&email_token=AKXUEQUJLIMSIC2HX",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/692#issuecomment-596864018:499,perform,performance-wise,499,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/692#issuecomment-596864018,1,['perform'],['performance-wise']
Performance,What's are the possible causes of the performance difference? What code should we focus on if we want to close this performance gap in the future?,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2603#issuecomment-1194187328:38,perform,performance,38,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2603#issuecomment-1194187328,2,['perform'],['performance']
Performance,"What's the motivation for using a macro rather than multiple dispatch? To my eye it looks like your macro is basically performing multiple dispatch with an `if` statement. But Julia has multiple dispatch built in; we don't have to implement it ourselves. An argument against macros is that they make the code more obscure. It's harder to figure out what is happening because you have to find the definition of the macro. . Come to think of it, the user can also just define a forcing function that indexes into some constant array. Why is this not a good solution?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/110#issuecomment-470540114:119,perform,performing,119,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/110#issuecomment-470540114,1,['perform'],['performing']
Performance,When I ran this locally I got a load of `@test_broken` passing for computed fields which is quite strange,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3482#issuecomment-2180941031:32,load,load,32,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3482#issuecomment-2180941031,1,['load'],['load']
Performance,"When I run the following MWE:. ```julia; using Oceananigans; using Oceananigans.Units; using Oceananigans.ImmersedBoundaries: ImmersedBoundaryGrid, GridFittedBottom. Lz = 50; grid_base = RectilinearGrid(size=(50, 50, 6), x=(-2000, 2000), y=(-1500, 2500), z=(0, Lz)). bathymetry(x, y) = Lz/2 * exp(-(x/500)^2 - (y/500)^2); grid = ImmersedBoundaryGrid(grid_base, GridFittedBottom(bathymetry)). const cᴰᶻ = 3e-3; @inline τˣᶻ_drag(x, y, z, t, u, v, w) = -cᴰᶻ * u * √(u^2 + v^2); τˣᶻ_BC = FluxBoundaryCondition(τˣᶻ_drag, field_dependencies = (:u, :v, :w)). u_bcs = FieldBoundaryConditions(immersed=ImmersedBoundaryCondition(bottom = τˣᶻ_BC)). model = HydrostaticFreeSurfaceModel(; grid, boundary_conditions = (; u=u_bcs)). simulation = Simulation(model, Δt=1, stop_iteration=10); run!(simulation); ```. I get the following error:. ```; ERROR: LoadError: TaskFailedException. nested task error: TaskFailedException; ; nested task error: MethodError: no method matching field_arguments(::Int64, ::Int64, ::Int64, ::ImmersedBoundaryGrid{Float64, Periodic, Periodic, Bounded, RectilinearGrid{Float64, Periodic, Periodic, Bounded, Float64, Float64, Float64, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}}}, CPU}, GridFittedBottom{OffsetArrays.OffsetMatrix{Float64, Matrix{Float64}}, Oceananigans.ImmersedBoundaries.CenterImmersedCondition}, CPU}, ::NamedTuple{(:u, :v, :w, :T, :S, :η), Tuple{Field{Face, Center, Center, Nothing, ImmersedBoundaryGrid{Float64, Periodic, Periodic, Bounded, RectilinearGrid{Float64, Periodic, Periodic, Bounded, Float64, Float64, Float64, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}}}, OffsetArrays.Of",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2711:838,Load,LoadError,838,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2711,1,['Load'],['LoadError']
Performance,"When I try to write a `Field` that's reduced over 3 dimensions to a NetCDF file I get an error. For the example the MWE below. ```julia; using Oceananigans. grid = RectilinearGrid(size=(8,8,8,), extent=(1,1,1)); model = NonhydrostaticModel(grid=grid). u, v, w = model.velocities; u_mean = Field(Average(u)); outputs = (; u_mean,). filename = ""mwe.nc""; writer_nc = NetCDFOutputWriter(model, outputs;; filename = ""avgnc.nc"",; schedule = IterationInterval(1),; overwrite_existing = true,). simulation = Simulation(model, Δt=1, stop_iteration=5); simulation.output_writers[:fields] = writer_nc. run!(simulation); ```. produces this error:. ```; ERROR: LoadError: MethodError: no method matching Float32(::Array{Float32, 0}); Closest candidates are:; (::Type{T})(::AbstractChar) where T<:Union{AbstractChar, Number} at char.jl:50; (::Type{T})(::Base.TwicePrecision) where T<:Number at twiceprecision.jl:266; (::Type{T})(::Complex) where T<:Real at complex.jl:44; ...; Stacktrace:; [1] setindex!(v::NCDatasets.Variable{Float32, 1, NCDatasets.NCDataset{Nothing}}, data::Array{Float32, 0}, indexes::Int64); @ NCDatasets ~/.julia/packages/NCDatasets/dXXHC/src/variable.jl:308; [2] setindex!(v::NCDatasets.CFVariable{Float32, 1, NCDatasets.Variable{Float32, 1, NCDatasets.NCDataset{Nothing}}, NCDatasets.Attributes{NCDatasets.NCDataset{Nothing}}, NamedTuple{(:fillvalue, :missing_values, :scale_factor, :add_offset, :calendar, :time_origin, :time_factor), Tuple{Nothing, Tuple{}, Nothing, Nothing, Nothing, Nothing, Nothing}}}, data::Array{Float32, 0}, indexes::Int64); @ NCDatasets ~/.julia/packages/NCDatasets/dXXHC/src/cfvariable.jl:765; [3] save_output!(ds::NCDatasets.NCDataset{Nothing}, output::Field{Nothing, Nothing, Nothing, Reduction{typeof(Statistics.mean!), Field{Face, Center, Center, Nothing, RectilinearGrid{Float64, Periodic, Periodic, Bounded, Float64, Float64, Float64, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2857:648,Load,LoadError,648,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2857,1,['Load'],['LoadError']
Performance,"When I use a function to set an `OpenBoundaryCondition` on a `HydrostaticFreeSurfaceModel` I get an error and I don't understand why. Here's a MWE:. ```julia; using Oceananigans; grid = RectilinearGrid(topology = (Bounded, Flat, Bounded), size = (4, 4), extent = (1, 1)). u₀ = 1; @inline u_func(z, t) = u₀. u_bcs = FieldBoundaryConditions(east = OpenBoundaryCondition(u_func), west = OpenBoundaryCondition(u_func)). model = HydrostaticFreeSurfaceModel(; grid, boundary_conditions = (; u = u_bcs,)); set!(model, u = u₀); time_step!(model, 0.1); ```. This gives me the error:. ```; ERROR: LoadError: TaskFailedException. nested task error: MethodError: objects of type Oceananigans.BoundaryConditions.ContinuousBoundaryFunction{Nothing, Center, Center, Oceananigans.BoundaryConditions.LeftBoundary, typeof(u_func), Nothing, Tuple{}, Tuple{}, Tuple{}} are not callable; Stacktrace:; [1] getbc; @ ~/.julia/packages/Oceananigans/OHYQj/src/BoundaryConditions/boundary_condition.jl:115 [inlined]; [2] _fill_west_halo!; @ ~/.julia/packages/Oceananigans/OHYQj/src/BoundaryConditions/fill_halo_regions_open.jl:34 [inlined]; [3] #25; @ ~/.julia/packages/Oceananigans/OHYQj/src/BoundaryConditions/fill_halo_regions.jl:260 [inlined]; [4] ntuple; @ ./ntuple.jl:50 [inlined]; [5] cpu__fill_west_and_east_halo!; @ ~/.julia/packages/KernelAbstractions/HAcqg/src/macros.jl:287 [inlined]; [6] __thread_run(tid::Int64, len::Int64, rem::Int64, obj::KernelAbstractions.Kernel{…}, ndrange::Nothing, iterspace::KernelAbstractions.NDIteration.NDRange{…}, args::Tuple{…}, dynamic::KernelAbstractions.NDIteration.DynamicCheck); @ KernelAbstractions ~/.julia/packages/KernelAbstractions/HAcqg/src/cpu.jl:115; [7] (::KernelAbstractions.var""#18#21""{…})(); @ KernelAbstractions ~/.julia/packages/KernelAbstractions/HAcqg/src/cpu.jl:90; ```; which points to this line https://github.com/CliMA/Oceananigans.jl/blob/d4bcc095be66c7b5c98a462106285a6f6d341fe1/src/BoundaryConditions/boundary_condition.jl#L115; which is a fallback method.",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3628:587,Load,LoadError,587,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3628,1,['Load'],['LoadError']
Performance,"When attempting to run the problem in #1916 I noticed something alarming: models that are 2D in ""xz"" are far less performant than models that are 2D in ""xy"" on both the CPU and the GPU. On the CPU, 2D models in ""xz"" have alarmingly high memory allocation (allocations also seem high for ""xy"", something it seems we've lost track of). I tested this on a number of branches / versions and found similar behavior on all them, so it doesn't seem there's been a performance regression (at least not recently). I haven't tested any branches requiring julia 1.5 --- I'd like to try that too. To illustrate this I put together a small benchmarking script:. ```julia; using Oceananigans; using Oceananigans.TimeSteppers: time_step!; using BenchmarkTools. xy_grid = RegularRectilinearGrid(size = (512, 512, 1), extent = (2π, 2π, 2π), topology = (Periodic, Periodic, Bounded)); xz_grid = RegularRectilinearGrid(size = (512, 1, 512), extent = (2π, 2π, 2π), topology = (Periodic, Periodic, Bounded)); yz_grid = RegularRectilinearGrid(size = (1, 512, 512), extent = (2π, 2π, 2π), topology = (Periodic, Periodic, Bounded)). function ten_steps!(model); for i = 1:10; time_step!(model, 1e-6); end; return nothing; end. for arch in (CPU(), GPU()); for grid in (; xy_grid,; xz_grid,; yz_grid,; ). model = NonhydrostaticModel(architecture = arch,; timestepper = :RungeKutta3,; advection = UpwindBiasedFifthOrder(),; grid = grid,; buoyancy = nothing,; tracers = nothing). @info ""Benchmarking $model...""; @btime ten_steps!($model); end; end; ```. The results are alarming: on the CPU we find. * `xy_grid`: 616.285 ms (429912 allocations: 114.29 MiB); * `xz_grid`: 4.638 s (944291 allocations: 2.63 GiB); * `yz_grid`: 3.240 s (405223 allocations: 2.60 GiB). Notice the `xz` configuration is 7 times slower than the `xy` configuration. In addition to that, allocations are through the roof --- 200 MiB of allocation per time-step?? (the benchmark tests 10 time-steps). On the GPU we find. * `xy_grid`: 48.438 ms (128139 alloc",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1919:114,perform,performant,114,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1919,2,['perform'],"['performance', 'performant']"
Performance,"When creating a background field, I can add it to its respective perturbations to create a total field. However, this fails when writing to file. Here's a MWE:. ```julia; using Oceananigans; using Oceananigans.AbstractOperations: compute!; using Oceananigans.Utils; using Oceananigans.OutputWriters. grid = RegularCartesianGrid(size=(16, 1, 16), extent=(1, 1, 1)). U(x, y, z, t) = 0.2 * z. model = IncompressibleModel(grid = grid, background_fields = (u=U,)). U_tot = model.background_fields.velocities.u + model.velocities.u. simulation = Simulation(model, Δt=2,; iteration_interval=5,; stop_iteration=5,). outputs = (u = model.velocities.u, U_tot = U_tot,); simulation.output_writers[:simple_output] = NetCDFOutputWriter(model, outputs,; filepath = ""mwe.nc"",; schedule = TimeInterval(20minutes),; mode = ""c""); run!(simulation); ```. This fails in the last line with. > ERROR: LoadError: type BinaryOperation has no field data. Is this expected behavior? It would be very nice to be able to do this!",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1308:878,Load,LoadError,878,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1308,1,['Load'],['LoadError']
Performance,"When using `with_halos = true` in the `JLD2OutputWriter` works well, however in `NetCDFOutputWriter` it crashes with the following error: . ```; [ Info: Initializing simulation...; Iteration: 0000, time: 0 seconds, Δt: 11 seconds, wall time: 0 seconds; ERROR: LoadError: DimensionMismatch: new dimensions (156, 156, 22, 1) must be consistent with array size 24336; Stacktrace:; [1] (::Base.var""#throw_dmrsa#328"")(dims::NTuple{4, Int64}, len::Int64); @ Base ./reshapedarray.jl:41; [2] reshape(a::Array{Float64, 3}, dims::NTuple{4, Int64}); @ Base ./reshapedarray.jl:45; [3] setindex_disk!(::NCDatasets.Variable{Float64, 4, NCDataset{Nothing}}, ::Array{Float64, 3}, ::Function, ::Vararg{Any}); @ DiskArrays /home/datawork-lops-drakkarcom/SIMULATION-OUTPUTS/ICE-CHANEL/.julia/packages/DiskArrays/bZBJE/src/diskarray.jl:56; [4] setindex!; @ /home/datawork-lops-drakkarcom/SIMULATION-OUTPUTS/ICE-CHANEL/.julia/packages/DiskArrays/bZBJE/src/diskarray.jl:229 [inlined]; [5] setindex!(::CommonDataModel.CFVariable{…}, ::Array{…}, ::Colon, ::Colon, ::Colon, ::UnitRange{…}); @ CommonDataModel /home/datawork-lops-drakkarcom/SIMULATION-OUTPUTS/ICE-CHANEL/.julia/packages/CommonDataModel/pO4st/src/cfvariable.jl:419; [6] save_output!(ds::NCDataset{…}, output::Field{…}, model::HydrostaticFreeSurfaceModel{…}, ow::NetCDFOutputWriter{…}, time_index::Int64, name::String); @ Oceananigans.OutputWriters /home/datawork-lops-drakkarcom/SIMULATION-OUTPUTS/ICE-CHANEL/.julia/packages/Oceananigans/17XSY/src/OutputWriters/netcdf_output_writer.jl:479; [7] write_output!(ow::NetCDFOutputWriter{…}, model::HydrostaticFreeSurfaceModel{…}); @ Oceananigans.OutputWriters /home/datawork-lops-drakkarcom/SIMULATION-OUTPUTS/ICE-CHANEL/.julia/packages/Oceananigans/17XSY/src/OutputWriters/netcdf_output_writer.jl:518; [8] initialize!(sim::Simulation{…}); @ Oceananigans.Simulations /home/datawork-lops-drakkarcom/SIMULATION-OUTPUTS/ICE-CHANEL/.julia/packages/Oceananigans/17XSY/src/Simulations/run.jl:212; [9] time_step!(sim::Simul",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3260#issuecomment-2007187819:260,Load,LoadError,260,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3260#issuecomment-2007187819,1,['Load'],['LoadError']
Performance,"While `Array{Float32}` is neat because user output is smaller, it's a bad default because it often leads to confusing results, like the fact that outputing `Field(Integral(c))` is different (possibly by `eps(Float32)`) than computing `Integral(c_output)` in post processing when `Field(Integral(c))` is close to 0. Thus @xkykai and I propose to make the default `Array{Float64}`. We could regard outputting with `Array{Float32}` as an ""optimization"" which is premature as a default.",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2890:436,optimiz,optimization,436,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2890,1,['optimiz'],['optimization']
Performance,Width; Max : 16x; Current : 16x; Bridge Chip; Type : N/A; Firmware : N/A; Replays Since Reset : 0; Replay Number Rollovers : 0; Tx Throughput : 0 KB/s; Rx Throughput : 0 KB/s; Atomic Caps Inbound : N/A; Atomic Caps Outbound : N/A; Fan Speed : N/A; Performance State : P0; Clocks Throttle Reasons; Idle : Active; Applications Clocks Setting : Not Active; SW Power Cap : Not Active; HW Slowdown : Not Active; HW Thermal Slowdown : Not Active; HW Power Brake Slowdown : Not Active; Sync Boost : Not Active; SW Thermal Slowdown : Not Active; Display Clock Setting : Not Active; FB Memory Usage; Total : 32768 MiB; Reserved : 267 MiB; Used : 0 MiB; Free : 32500 MiB; BAR1 Memory Usage; Total : 32768 MiB; Used : 2 MiB; Free : 32766 MiB; Compute Mode : Default; Utilization; Gpu : 0 %; Memory : 0 %; Encoder : 0 %; Decoder : 0 %; Encoder Stats; Active Sessions : 0; Average FPS : 0; Average Latency : 0; FBC Stats; Active Sessions : 0; Average FPS : 0; Average Latency : 0; Ecc Mode; Current : Enabled; Pending : Enabled; ECC Errors; Volatile; Single Bit; Device Memory : 0; Register File : 0; L1 Cache : 0; L2 Cache : 0; Texture Memory : N/A; Texture Shared : N/A; CBU : N/A; Total : 0; Double Bit; Device Memory : 0; Register File : 0; L1 Cache : 0; L2 Cache : 0; Texture Memory : N/A; Texture Shared : N/A; CBU : 0; Total : 0; Aggregate; Single Bit; Device Memory : 0; Register File : 0; L1 Cache : 0; L2 Cache : 0; Texture Memory : N/A; Texture Shared : N/A; CBU : N/A; Total : 0; Double Bit; Device Memory : 0; Register File : 0; L1 Cache : 0; L2 Cache : 0; Texture Memory : N/A; Texture Shared : N/A; CBU : 0; Total : 0; Retired Pages; Single Bit ECC : 0; Double Bit ECC : 0; Pending Page Blacklist : No; Remapped Rows : N/A; Temperature; GPU Current Temp : 41 C; GPU Shutdown Temp : 90 C; GPU Slowdown Temp : 87 C; GPU Max Operating Temp : 83 C; GPU Target Temperature : N/A; Memory Current Temp : 44 C; Memory Max Operating Temp : 85 C; Power Readings; Power Management : Supported; Power Draw : 44.,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2922#issuecomment-1437515895:21405,Cache,Cache,21405,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2922#issuecomment-1437515895,8,['Cache'],['Cache']
Performance,Will merge and keep a look out for race conditions on Buildkite.,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1804#issuecomment-872544140:35,race condition,race conditions,35,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1804#issuecomment-872544140,1,['race condition'],['race conditions']
Performance,"With @amontoison. We'd like to implement a Poisson solver that uses [`Krylov`](https://github.com/JuliaSmoothOptimizers/Krylov.jl) under the hood instead of our custom (preconditioned) conjugate gradient solver. This will open the door to [more solvers (that may be more appropriate for our pressure Poisson equation than conjugate gradient)](https://arxiv.org/abs/2310.01757) like conjugate residual, etc. To make this work we need to overload some of Krylov's operators for Oceananigans `Field`:. * [`kaxpby!(n, s, x, dx, y, dy)`](https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/200a9cd01c3bba906b82eaa87103aa2881983e5f/src/krylov_utils.jl#L326C1-L326C8) which performs `y = y + s * x` where `n` is the total length (eg `Nx * Ny * Nz`), `s` is the output, `dx` and `dy` are strides (irrelevant for us); * [`kaxpby!(n, s, x, dx, t, y, dy)`](https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/200a9cd01c3bba906b82eaa87103aa2881983e5f/src/krylov_utils.jl#L326C1-L326C8) which performs `y = t * y + s * x` where `n` is the total length (eg `Nx * Ny * Nz`), `s` is the output, `dx` and `dy` are strides (irrelevant for us); * [`kdot`](https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/200a9cd01c3bba906b82eaa87103aa2881983e5f/src/krylov_utils.jl#L310C1-L310C5); * [`knrm2`](https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/200a9cd01c3bba906b82eaa87103aa2881983e5f/src/krylov_utils.jl#L316C1-L316C6); * Either `kcopy!` or `copyto!`",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3803:671,perform,performs,671,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3803,2,['perform'],['performs']
Performance,"With the code being (slightly) optimized, we probably need a way to track performance across PRs and make sure we don't lose performance due to reasons we do not have much control over (for example changes in dependencies). Over at [SpeedyWeather.jl](https://github.com/SpeedyWeather/SpeedyWeather.jl/issues/464) they are thinking to do the same and the package [PkgBenchmark.jl](https://github.com/JuliaCI/PkgBenchmark.jl) was suggested as a way to simplify this implementation. The question here is what would be the suitable candidate for a performance test, we could start with ; - a non-hydrostatic simulation (no immersed boundaries); - a hydrostatic simulation on rectilinear; - same on latitude-longitude and on immersed boundary; - a near-global ocean at a quarter of a degree ; - a near-global ocean at a quarter of a degree on 4 GPUs. The tests do not have to be enforced but can run nightly (or once per week) on the main branch, with the possibility of performing the tests before merging sensitive PRs",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3492:31,optimiz,optimized,31,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3492,5,"['optimiz', 'perform']","['optimized', 'performance', 'performing']"
Performance,With this PR we can now apply boundary conditions on `x` and `y`. I still need to add tests and benchmark performance (nothing should change). One discussion point: Should we even add support for `x` boundary conditions seeing as we only support `Periodic` boundary conditions in the `x` direction?,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/475:106,perform,performance,106,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/475,1,['perform'],['performance']
Performance,"Working on implementing halo regions with OffsetArrays.jl right now. Got a minimal working example running with GPUifyLoops so it should be a pretty straightforward upgrade from here. The performance benchmarks will be interesting. Will open a PR soon. Some notes for myself:; * The binary and NetCDF output writers will have to be modified to avoid writing out the halo regions.; * We'll just need a kernel that fills in the halo regions at the end of each time step.; * The ""halo size"" will be part of the `Grid` and we'll probably have to make sure that the model runs with a large enough halo, e.g. halo of size 1 is enough for second-order advection schemes, but you need a halo of size 2 for fourth-order advection schemes.; * This should also make https://github.com/climate-machine/Oceananigans.jl/issues/57 redundant.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/104#issuecomment-480235335:188,perform,performance,188,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/104#issuecomment-480235335,1,['perform'],['performance']
Performance,"Working with @elise-palethorpe we see that the preconditioned conjugate gradient solver is much slower than expected. See #2654. In particular, benchmarks on solving the Poisson equation on a doubly bounded domain on the [ep/pcg-with-multigrid](https://github.com/CliMA/Oceananigans.jl/tree/ep/pcg-with-multigrid) branch give:. ```Julia; julia> include(""validation/elliptic_solvers/doubly_bounded_poisson.jl""). julia> include(""doubly_bounded_poisson.jl""); [ Info: Solving the Poisson equation with an FFT-based solver...; 123.083 μs (93 allocations: 17.56 KiB); [ Info: Solving the Poisson equation with a conjugate gradient iterative solver...; 64.748 ms (80482 allocations: 25.93 MiB); [ Info: Solving the Poisson equation with the Algebraic Multigrid solver...; 9.491 ms (498 allocations: 8.46 MiB); [ Info: Solving the Poisson equation with a conjugate gradient preconditioned iterative solver w/ AMG as preconditioner...; 47.891 ms (12771 allocations: 111.97 MiB); ```. We'd expect the PCG to perform similarly to MG and MG-preconditioned PGC to perform better. There is definitely some issue with memory allocations but, possibly, something else?",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2728:998,perform,perform,998,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2728,2,['perform'],['perform']
Performance,"Would be good to do some profiling (probably with a system profiler like perf), to understand where time is spent. The kernels using KernelAbstractions are automatically multi-threaded.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1861#issuecomment-880765565:170,multi-thread,multi-threaded,170,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1861#issuecomment-880765565,1,['multi-thread'],['multi-threaded']
Performance,Would be good to set this up with Slurm CI (PR #280) and start doing continuous performance testing to ensure we don't make mistakes that slow the model down.,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/380:80,perform,performance,80,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/380,1,['perform'],['performance']
Performance,"Yeah I guess we're pretty restricted in our thread-block layouts although it should be easy to generate them for all the grids we usually use. The important thing is that we can run on a wide variety of useful grid sizes, instead of being restricted to multiples of 16 in the horizontal like we are right now. It's also important to keep the kernel indexing intuitive, e.g. this kernel does things point-wise so it gets a triple for-loop, another kernel does things column-wise so it gets a double for-loop and a third unrolled inner loop, another only acts on the surface and bottom, etc. So maybe it's not worth introducing `Tz` (which might complicate the kernels) unless we know it'll improve performance.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/249#issuecomment-496545455:697,perform,performance,697,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/249#issuecomment-496545455,1,['perform'],['performance']
Performance,Yeah I saw that comment. My understanding is that `-O1` is a good compromise between performance and latency but in our case we just want to reduce latency as performance during testing doesn't matter. But I guess `-O0` didn't really do much. Perhaps the optimizations (might only be in Julia 1.6+) mostly affect compilation when loading/importing packages. Should we still consider merging this? Once tests run I can do a comparison to see how many minutes (or seconds?) `-O0` saves us?,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1078#issuecomment-712840423:85,perform,performance,85,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1078#issuecomment-712840423,6,"['latency', 'load', 'optimiz', 'perform']","['latency', 'loading', 'optimizations', 'performance']"
Performance,"Yeah I think that's kind of what I was thinking, much more for the GPU though. Dunno what the final element-wise kernels will look like but if the kernels are shared with the CPU and GPU then I'm not sure how memory will be managed (would be nice to avoid handling GPU shared memory manually). I think you're right, we just want a ""struct of arrays"" optimization. For what it's worth @christophernhill says that the MITgcm already does this, and it doesn't seem to noticeably boost performance.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/44#issuecomment-462545133:350,optimiz,optimization,350,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/44#issuecomment-462545133,2,"['optimiz', 'perform']","['optimization', 'performance']"
Performance,"Yeah filling halo regions as part of `set!` seems like the right thing to do. I guess we don't use `set!` for any performance critical code, but in case we do in the future might be nice to add an optional kwarg `set!(field, fill_halos=true)` in case someone wants to use `fill_halos=false` to shave 0-6 kernel launches.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1150#issuecomment-724252513:114,perform,performance,114,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1150#issuecomment-724252513,1,['perform'],['performance']
Performance,"Yeah it'll be slow but I figure we can always optimize later... The velocities stored are defined as averages over the cell faces (well the velocities are normal to the cell faces) while tracer variables (e.g. temperature T, salinity S, and pressure p) are defined as cell or volume averages. The difference is reflected in the different operators, e.g. [div_flux](https://github.com/ali-ramadhan/OceanLES.jl/blob/69d1b5794e297245289be993d254ec795469061a/src/operators.jl#L46) vs. [u_dot_u](https://github.com/ali-ramadhan/OceanLES.jl/blob/69d1b5794e297245289be993d254ec795469061a/src/operators.jl#L59), which I guess is why the δ operators aren't exactly derivatives as much as they are difference operators used by the actual divergence and curl operators.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/4#issuecomment-437340933:46,optimiz,optimize,46,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/4#issuecomment-437340933,1,['optimiz'],['optimize']
Performance,"Yeah the plot is from v0.5.x or something lol. Not sure if CPU -> GPU speedups are as relevant anymore? Well, maybe a lot of people are still on CPUs. The speedup can be much larger with WENO, but it's all very CPU and GPU dependent. I've seen ~8x and ~2000x speedups for the same benchmark. Also not sure if there are any good metrics to quantify performance. But @simone-silvestri's scaling plots would probably be great to include?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3684#issuecomment-2272351495:348,perform,performance,348,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3684#issuecomment-2272351495,1,['perform'],['performance']
Performance,Yeah this is pretty worrying... I'm pretty sure this is the cause of https://github.com/CliMA/Oceananigans.jl/issues/1420 which has been open for a while so this slowdown must have been around for a while (and just flew under the radar). [Profiling](https://docs.julialang.org/en/v1/manual/profile/) might help pinpoint the bottleneck.,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1919#issuecomment-891426665:324,bottleneck,bottleneck,324,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1919#issuecomment-891426665,1,['bottleneck'],['bottleneck']
Performance,"Yes, from what I could test so far the CPU performance seems roughly; equivalent between versions. Although it would be good if someone else; tried to validate that as well. On Fri, Jun 25, 2021, 09:16 Gregory L. Wagner ***@***.***>; wrote:. > I'm sorry, I misinterpreted the results @ali-ramadhan; > <https://github.com/ali-ramadhan> posted. I thought that; > CenteredSecondOrder was 1.0x slower with julia 1.6 than with 1.5 (and; > that small slowdowns were observed for the other schemes, which is why I; > recommended testing the biharmonic scheme.) Now I understand that these; > results are all for julia 1.6; we are comparing the results with previously; > obtained benchmarks (not posted) for julia 1.5.; >; > Looking at @tomchor <https://github.com/tomchor> and @ali-ramadhan; > <https://github.com/ali-ramadhan>'s results then it looks like; > simulations with WENO5 are running approximately 6-8 times slower on julia; > 1.6 than it was on julia 1.5, while other advection schemes (and closures); > are unchanged --- correct?; >; > Is the *CPU* performance of WENO5 roughly equivalent between julia 1.5; > and julia 1.6?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/CliMA/Oceananigans.jl/issues/1764#issuecomment-868677634>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ADEX5KV46VENYCZPAGUMK4LTUSTVVANCNFSM47I24R7Q>; > .; >",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1764#issuecomment-868684330:43,perform,performance,43,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1764#issuecomment-868684330,2,['perform'],['performance']
Performance,Yes. It is! And this example should be optimized. But until then I’m merging this so the developers have a slightly easier life ;),MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3168#issuecomment-1616796587:39,optimiz,optimized,39,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3168#issuecomment-1616796587,1,['optimiz'],['optimized']
Performance,"Yesterday I tried running the diffusion example on a server and it run but there was a lot of output that appeared when doing simple plotting, see below. This is not an oceananigans thing as much as a `Plots.jl` thing. We found out that adding the line `ENV[""GKSwstype""]=""nul""` seemed to resolve the issue. Do people know how common a problem this is? If not then nothing to do but if it does happen a lot I wonder whether addin this line in the examples, or somewhere, might be helpful?. ```; julia> plt = plot(1:10); qt.qpa.xcb: could not connect to display ; qt.qpa.plugin: Could not load the Qt platform plugin ""xcb"" in """" even though it was found.; This application failed to start because no Qt platform plugin could be initialized. Reinstalling the application may fix this problem. Available platform plugins are: linuxfb, minimal, offscreen, vnc, xcb. connect: Connection refused; GKS: can't connect to GKS socket application. GKS: Open failed in routine OPEN_WS; GKS: GKS not in proper state. GKS must be either in the state WSOP or WSAC in routine ACTIVATE_WS; GKS: GKS not in proper state. GKS must be either in the state WSAC or SGOP in routine FILLAREA; GKS: GKS not in proper state. GKS must be either in the state WSAC or SGOP in routine FILLAREA; GKS: GKS not in proper state. GKS must be either in the state WSAC or SGOP in routine POLYLINE; GKS: GKS not in proper state. GKS must be either in the state WSAC or SGOP in routine POLYLINE; GKS: GKS not in proper state. GKS must be either in the state WSAC or SGOP in routine POLYLINE; GKS: GKS not in proper state. GKS must be either in the state WSAC or SGOP in routine POLYLINE; GKS: GKS not in proper state. GKS must be either in the state WSAC or SGOP in routine POLYLINE; GKS: GKS not in proper state. GKS must be either in the state WSAC or SGOP in routine POLYLINE; GKS: GKS not in proper state. GKS must be either in the state WSAC or SGOP in routine POLYLINE; GKS: GKS not in proper state. GKS must be either in the state WSA",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1657:587,load,load,587,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1657,1,['load'],['load']
Performance,"Yesterday, @ali-ramadhan and I discussed a new idea for improving the abstraction of equations. The idea is to abstract a `RightHandSide` consisting of a tuple of `Flux`es and `VolumeTerm`s or `SourceTerm`s. Each `Flux` or `VolumeTerm` type would define a `getindex` method, and carry around the references needed to execute that `getindex` method on CPU or GPU. This would greatly simplify the time-stepping routines, which currently involve long function signatures. There'd be no need for 'unpacking', because each term in the equation would perform unpacking upon instantiation. It would also probably be easier for users to extend / add terms to an equation. As an example, we can consider a simple implementation. ```julia; struct AdvectiveFlux{S, D, C, G}; u :: D; v :: D; w :: D; c :: C; grid :: G; scheme :: S; end. getindex(adv::AdvectiveFlux{Centered}, i, j, k) = # centered advective flux calculation. struct IsotropicDiffusiveFlux{N, D, G}; ν :: N; ψ :: D; grid :: G; end. getindex(diff::IsotropicDiffusiveFlux, i, j, k) = # calculates flux due to isotropic diffusion by ν. struct RightHandSide{F, V}; fluxes :: F; volume_terms :: V; end. advection = AdvectiveFlux(velocities..., tracers.c); diffusion = IsotropicDiffusiveFlux(ν, tracers.c). tracer_rhs = RightHandSide((advection, diffusion), nothing); ```. We'd have functions that look something like. ```julia; function x_flux_divergence(i, j, k, grid, fluxes...); incoming_flux = add_fluxes(i, j, k, grid, fluxes...); outgoing_flux = add_fluxes(i+1, j, k, grid, fluxes...); return (incoming_flux - outgoing_flux) * grid.Ax / grid.V; end; ```. ... for example. Obviously questions of performance are paramount, though in the case that _everything_ is inlined I think there is hope. A downside of this approach is that we can't use shared memory stencils on the GPU. Shared memory stencils on the GPU require _functions_ for all terms that avoid carrying around internal references to data (since we need to be able to pass them referen",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/259#issuecomment-600078394:545,perform,perform,545,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/259#issuecomment-600078394,1,['perform'],['perform']
Performance,"Yet another optimization for upwind stencil computations.; The pattern in Oceananigans to perform upwind reconstruction is roughly:; ```julia; R_left = _left_reconstruction(.....); R_right = _right_reconstruction(.....). return ifelse(u > 0, u * R_left, u * R_right); ``` . This means that we are always performing the reconstruction twice. This is not a huge problem for linear reconstruction schemes (`UpwindBiased`) but leads to register blowup for `WENO` schemes that are extremely heavy to compute. ; This PR aims to push the `left` - `right` choice _inside_ the reconstruction function by realizing that the only difference between left and right reconstruction is how the data is organized in the stencil.; In this way, only one reconstruction is required significantly reducing register pressure, and consequently, computation time.; This follows the same pattern found in [SpeedyWeather.jl](https://github.com/SpeedyWeather/SpeedyWeather.jl/blob/4fd8c045fbc493f94b70f121cabf8f8fc3c15f66/src/dynamics/vertical_advection.jl#L124-L148). Some benchmarks are implemented in the [NESAPOceananigans.jl](https://github.com/simone-silvestri/NESAPOceananigans.jl) repository. . Here are some timing tests on main with a **NON-Immersed** grid (launching julia with `julia --project=""environments/main"" --check-bounds=no`); ```julia ; julia> using NESAPOceananigans; julia> set_problem_size!(500, 500, 50). julia> trial1 = run_model_benchmark!(momentum_kernel_test, GPU();; use_benchmarktools = true); BenchmarkTools.Trial: 5 samples with 1 evaluation.; Range (min … max): 21.916 ms … 22.784 ms ┊ GC (min … max): 0.00% … 0.00%; Time (median): 22.036 ms ┊ GC (median): 0.00%; Time (mean ± σ): 22.144 ms ± 363.318 μs ┊ GC (mean ± σ): 0.00% ± 0.00%. ██ ██ █; ██▁▁▁▁▁▁██▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█ ▁; 21.9 ms Histogram: frequency by time 22.8 ms <. Memory estimate: 245.86 KiB, allocs estimate: 407. julia> trial1 = run_model_benchmark!(tracer_kernel_test, GPU();; use_benchmarktools =",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3658:12,optimiz,optimization,12,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3658,3,"['optimiz', 'perform']","['optimization', 'perform', 'performing']"
Performance,"You can use both `@inline` and `@inbounds`; they mean different things. `@inline` is a compiler directive to inline a function (important for performance when a function is called within inner loops). We put `@inline` in front of functions; eg `@inline f(x) = ...`. `@inbounds` elides bounds checking when an array / field is indexed into. We need `@inbounds` in front of any indexing operation that occurs in a loop (eg `u[i, j, k]`).",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1827#issuecomment-875676924:142,perform,performance,142,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1827#issuecomment-875676924,1,['perform'],['performance']
Performance,You should be able to load any NetCDF file using xarray and it gives you a nice and powerful interface for analyzing and manipulating the data. Probably not the best example as it's a little old and messy (I've started using Makie more often) but here's one example: https://github.com/CliMA/LESbrary.jl/blob/3595ff2e1db6d5e6898b6ea84335fdb9dbd23b15/src/make_lesbrary_plots.py,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1334#issuecomment-772770432:22,load,load,22,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1334#issuecomment-772770432,1,['load'],['load']
Performance,Zero-ed initial guess to preconditioner for CG solver improves performance compared to using previous `z`,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2655:63,perform,performance,63,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2655,1,['perform'],['performance']
Performance,"[10] maybe_cachefile_lock(f::Base.var""#968#969""{Base.PkgId}, pkg::Base.PkgId, srcpath::String; stale_age::Int64); │ @ Base ./loading.jl:2983; │ [11] maybe_cachefile_lock; │ @ ./loading.jl:2980 [inlined]; │ [12] _require(pkg::Base.PkgId, env::Nothing); │ @ Base ./loading.jl:1970; │ [13] __require_prelocked(uuidkey::Base.PkgId, env::Nothing); │ @ Base ./loading.jl:1812; │ [14] #invoke_in_world#3; │ @ ./essentials.jl:926 [inlined]; │ [15] invoke_in_world; │ @ ./essentials.jl:923 [inlined]; │ [16] _require_prelocked; │ @ ./loading.jl:1803 [inlined]; │ [17] _require_prelocked; │ @ ./loading.jl:1802 [inlined]; │ [18] run_extension_callbacks(extid::Base.ExtensionId); │ @ Base ./loading.jl:1295; │ [19] run_extension_callbacks(pkgid::Base.PkgId); │ @ Base ./loading.jl:1330; │ [20] run_package_callbacks(modkey::Base.PkgId); │ @ Base ./loading.jl:1164; │ [21] _tryrequire_from_serialized(modkey::Base.PkgId, path::String, ocachepath::String, sourcepath::String, depmods::Vector{Any}); │ @ Base ./loading.jl:1487; │ [22] _require_search_from_serialized(pkg::Base.PkgId, sourcepath::String, build_id::UInt128); │ @ Base ./loading.jl:1574; │ [23] _require(pkg::Base.PkgId, env::String); │ @ Base ./loading.jl:1938; │ [24] __require_prelocked(uuidkey::Base.PkgId, env::String); │ @ Base ./loading.jl:1812; │ [25] #invoke_in_world#3; │ @ ./essentials.jl:926 [inlined]; │ [26] invoke_in_world; │ @ ./essentials.jl:923 [inlined]; │ [27] _require_prelocked(uuidkey::Base.PkgId, env::String); │ @ Base ./loading.jl:1803; │ [28] macro expansion; │ @ ./loading.jl:1790 [inlined]; │ [29] macro expansion; │ @ ./lock.jl:267 [inlined]; │ [30] __require(into::Module, mod::Symbol); │ @ Base ./loading.jl:1753; │ [31] #invoke_in_world#3; │ @ ./essentials.jl:926 [inlined]; │ [32] invoke_in_world; │ @ ./essentials.jl:923 [inlined]; │ [33] require(into::Module, mod::Symbol); │ @ Base ./loading.jl:1746; │ [34] eval; │ @ ./boot.jl:385 [inlined]; │ [35] eval_user_input(ast::Any, backend::REPL.REPLBackend, mod::Module",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3686#issuecomment-2272432528:3058,load,loading,3058,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3686#issuecomment-2272432528,1,['load'],['loading']
Performance,"[2]; Stacktrace:; [1] getindex; @ ./array.jl:805 [inlined]; [2] g(a::Vector{Float64}, first::Bool); @ Main ./REPL[11]:1; [3] top-level scope; @ REPL[12]:1. julia> g(a, false); ERROR: BoundsError: attempt to access 1-element Vector{Float64} at index [2]; Stacktrace:; [1] getindex; @ ./array.jl:805 [inlined]; [2] g(a::Vector{Float64}, first::Bool); @ Main ./REPL[11]:1; [3] top-level scope; @ REPL[13]:1; ```. `ifelse` is _not_ short-circuiting --- _both_ branches are executed, even though only the correct value is returned:. ```julia; julia> b = rand(2); 2-element Vector{Float64}:; 0.5340042876487958; 0.7031634999748222. julia> g(b, true); 0.5340042876487958. julia> g(b, false); 0.7031634999748222; ```. It's easier for the compiler to optimize code that involves `ifelse`, especially on the GPU. The reason is that it's allowed to execute all code on both branches. If we use short-circuiting logic, then I guess many optimizations are not possible, because execution on one branch or another must be completely excluded. Some of this is discussed here: https://discourse.julialang.org/t/multiplying-by-booleans-faster-than-if-else/64117. > What about @inline?. This a little hazier. The compiler decides based on a heuristic whether or not to ""inline"" a function (meaning, rather than compiling code for a function independently and jumping to that code at the right moment, it combines the function code with the code that calls the function). We want to inline everything basically, so that every tendency evaluation involves evaluating one giant function. Inlining lets LLVM magic optimize our code to the highest degree (at least that's my impression). For whatever reason the compiler often decides _not_ to inline our functions unless we specifically annotate them. So it seems we probably need to add `@inline` to every ""hot"" function that's called in a kernel, at every grid point (like a forcing function or boundary condition function). But I suggest you benchmark yourself and see!",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2336#issuecomment-1066115583:2716,optimiz,optimize,2716,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2336#issuecomment-1066115583,1,['optimiz'],['optimize']
Performance,"[3] _permute_index at /home/alir/Oceananigans.jl/src/Solvers/index_permutations.jl:11; [4] permute_index at /home/alir/Oceananigans.jl/src/Solvers/index_permutations.jl:24; [5] macro expansion at /home/alir/Oceananigans.jl/src/Solvers/solve_for_pressure.jl:39; [6] gpu_calculate_pressure_right_hand_side! at /home/alir/.julia/packages/KernelAbstractions/xslEz/src/macros.jl:80; [7] overdub at /home/alir/.julia/packages/Cassette/158rp/src/overdub.jl:0; Stacktrace:; [1] check_ir(::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDA.CUDACompilerParams}, ::LLVM.Module) at /home/alir/.julia/packages/GPUCompiler/GKp4B/src/validation.jl:123; [2] macro expansion at /home/alir/.julia/packages/GPUCompiler/GKp4B/src/driver.jl:241 [inlined]; [3] macro expansion at /home/alir/.julia/packages/TimerOutputs/dVnaw/src/TimerOutput.jl:206 [inlined]; [4] codegen(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDA.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, validate::Bool, only_entry::Bool) at /home/alir/.julia/packages/GPUCompiler/GKp4B/src/driver.jl:239; [5] compile(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDA.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, validate::Bool, only_entry::Bool) at /home/alir/.julia/packages/GPUCompiler/GKp4B/src/driver.jl:39; [6] compile at /home/alir/.julia/packages/GPUCompiler/GKp4B/src/driver.jl:35 [inlined]; [7] _cufunction(::GPUCompiler.FunctionSpec{typeof(Cassette.overdub),Tuple{Cassette.Context{nametype(CUDACtx),KernelAbstractions.CompilerMetadata{KernelAbstractions.NDIteration.StaticSize{(16, 16, 16)},KernelAbstractions.NDIteration.DynamicCheck,Nothing,Nothing,KernelAbstractions.NDIteration.NDRange{3,KernelAbstractions.NDIteration.StaticSize{(1, 1, 16)},KernelAbstractions.NDIteration.StaticSize{(16, 16, 1)},Nothing,Nothing}},Nothing,KernelAbstractions.var""##PassType#253"",Nothing,Cassette.DisableHooks},typeof(Oc",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/828#issuecomment-700320323:4053,optimiz,optimize,4053,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/828#issuecomment-700320323,1,['optimiz'],['optimize']
Performance,"[inlined]; [6] checked_cuModuleLoadDataEx(_module::Base.RefValue{Ptr{CUDA.CUmod_st}}, image::Ptr{UInt8}, numOptions::Int64, options::Vector{CUDA.CUjit_option_enum}, optionValues::Vector{Ptr{Nothing}}); @ CUDA ~/.julia/packages/CUDA/2kjXI/lib/cudadrv/module.jl:17; [7] CUDA.CuModule(data::Vector{UInt8}, options::Dict{CUDA.CUjit_option_enum, Any}); @ CUDA ~/.julia/packages/CUDA/2kjXI/lib/cudadrv/module.jl:60; [8] CuModule; @ ~/.julia/packages/CUDA/2kjXI/lib/cudadrv/module.jl:49 [inlined]; [9] link(job::GPUCompiler.CompilerJob, compiled::@NamedTuple{image::Vector{UInt8}, entry::String}); @ CUDA ~/.julia/packages/CUDA/2kjXI/src/compiler/compilation.jl:409; [10] actual_compilation(cache::Dict{Any, CUDA.CuFunction}, src::Core.MethodInstance, world::UInt64, cfg::GPUCompiler.CompilerConfig{GPUCompiler.PTXCompilerTarget, CUDA.CUDACompilerParams}, compiler::typeof(CUDA.compile), linker::typeof(CUDA.link)); @ GPUCompiler ~/.julia/packages/GPUCompiler/2CW9L/src/execution.jl:262; [11] cached_compilation(cache::Dict{Any, CUDA.CuFunction}, src::Core.MethodInstance, cfg::GPUCompiler.CompilerConfig{GPUCompiler.PTXCompilerTarget, CUDA.CUDACompilerParams}, compiler::Function, linker::Function); @ GPUCompiler ~/.julia/packages/GPUCompiler/2CW9L/src/execution.jl:151; [12] macro expansion; @ ~/.julia/packages/CUDA/2kjXI/src/compiler/execution.jl:380 [inlined]; [13] macro expansion; @ ./lock.jl:267 [inlined]; [14] cufunction(f::typeof(Oceananigans.Models.NonhydrostaticModels.gpu__update_hydrostatic_pressure!), tt::Type{Tuple{KernelAbstractions.CompilerMetadata{…}, OffsetArrays.OffsetArray{…}, LatitudeLongitudeGrid{…}, Buoyancy{…}, @NamedTuple{…}}}; kwargs::@Kwargs{always_inline::Bool, maxthreads::Int64}); @ CUDA ~/.julia/packages/CUDA/2kjXI/src/compiler/execution.jl:375; [15] macro expansion; @ ~/.julia/packages/CUDA/2kjXI/src/compiler/execution.jl:112 [inlined]; [16] (::KernelAbstractions.Kernel{…})(::Field{…}, ::Vararg{…}; ndrange::Nothing, workgroupsize::Nothing); @ CUDA.CUDAKernels ~/.",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3870:2456,cache,cache,2456,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3870,1,['cache'],['cache']
Performance,"[inlined]; [6] checked_cuModuleLoadDataEx(_module::Base.RefValue{Ptr{CUDA.CUmod_st}}, image::Ptr{UInt8}, numOptions::Int64, options::Vector{CUDA.CUjit_option_enum}, optionValues::Vector{Ptr{Nothing}}); @ CUDA ~/.julia/packages/CUDA/z3j2H/lib/cudadrv/module.jl:17; [7] CUDA.CuModule(data::Vector{UInt8}, options::Dict{CUDA.CUjit_option_enum, Any}); @ CUDA ~/.julia/packages/CUDA/z3j2H/lib/cudadrv/module.jl:60; [8] CuModule; @ ~/.julia/packages/CUDA/z3j2H/lib/cudadrv/module.jl:49 [inlined]; [9] link(job::GPUCompiler.CompilerJob, compiled::@NamedTuple{image::Vector{UInt8}, entry::String}); @ CUDA ~/.julia/packages/CUDA/z3j2H/src/compiler/compilation.jl:413; [10] actual_compilation(cache::Dict{Any, CUDA.CuFunction}, src::Core.MethodInstance, world::UInt64, cfg::GPUCompiler.CompilerConfig{GPUCompiler.PTXCompilerTarget, CUDA.CUDACompilerParams}, compiler::typeof(CUDA.compile), linker::typeof(CUDA.link)); @ GPUCompiler ~/.julia/packages/GPUCompiler/2CW9L/src/execution.jl:262; [11] cached_compilation(cache::Dict{Any, CUDA.CuFunction}, src::Core.MethodInstance, cfg::GPUCompiler.CompilerConfig{GPUCompiler.PTXCompilerTarget, CUDA.CUDACompilerParams}, compiler::Function, linker::Function); @ GPUCompiler ~/.julia/packages/GPUCompiler/2CW9L/src/execution.jl:151; [12] macro expansion; @ ~/.julia/packages/CUDA/z3j2H/src/compiler/execution.jl:380 [inlined]; [13] macro expansion; @ ./lock.jl:267 [inlined]; [14] cufunction(f::typeof(Oceananigans.Models.NonhydrostaticModels.gpu__update_hydrostatic_pressure!), tt::Type{Tuple{KernelAbstractions.CompilerMetadata{…}, OffsetArrays.OffsetArray{…}, LatitudeLongitudeGrid{…}, Buoyancy{…}, @NamedTuple{…}}}; kwargs::@Kwargs{always_inline::Bool, maxthreads::Int64}); @ CUDA ~/.julia/packages/CUDA/z3j2H/src/compiler/execution.jl:375; [15] macro expansion; @ ~/.julia/packages/CUDA/z3j2H/src/compiler/execution.jl:112 [inlined]; [16] (::KernelAbstractions.Kernel{…})(::Field{…}, ::Vararg{…}; ndrange::Nothing, workgroupsize::Nothing); @ CUDA.CUDAKernels ~/.",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3785:2183,cache,cache,2183,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3785,1,['cache'],['cache']
Performance,[inlined]; include_string at ./loading.jl:2076; _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; _include at ./loading.jl:2136; include at ./client.jl:489; unknown function (ip: 0x7c00f54ff855); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; jl_apply at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/julia.h:1982 [inlined]; do_call at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:126; eval_value at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:223; eval_stmt_value at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:174 [inlined]; eval_body at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:617; jl_interpret_toplevel_thunk at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:775; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:934; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:877; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:877; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:877; ijl_toplevel_eval_in at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:985; eval at ./boot.jl:385 [inlined]; eval_user_input at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:150; repl_backend_loop at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/usr/share/julia/stdlib/v1.10/REPL,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3852:5611,cache,cache,5611,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3852,1,['cache'],['cache']
Performance,"\Users\kab\.julia\packages\KernelAbstractions\Zcyra\src\extras\loopinfo.jl:28:0: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering; warning: C:\Users\kab\.julia\packages\KernelAbstractions\Zcyra\src\extras\loopinfo.jl:28:0: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering; warning: C:\Users\kab\.julia\packages\KernelAbstractions\Zcyra\src\extras\loopinfo.jl:28:0: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering; warning: C:\Users\kab\.julia\packages\KernelAbstractions\Zcyra\src\extras\loopinfo.jl:28:0: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering; warning: C:\Users\kab\.julia\packages\KernelAbstractions\Zcyra\src\extras\loopinfo.jl:28:0: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering; warning: C:\Users\kab\.julia\packages\KernelAbstractions\Zcyra\src\extras\loopinfo.jl:28:0: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering; NonhydrostaticModel{CPU, RectilinearGrid}(time = 0 seconds, iteration = 0); ├── grid: 128×128×1 RectilinearGrid{Float64, Periodic, Periodic, Flat} on CPU with 3×3×0 halo; ├── timestepper: RungeKutta3TimeStepper; ├── tracers: (); ├── closure: ScalarDiffusivity{ExplicitTimeDiscretization}(ν=1.0e-5); ├── buoyancy: Nothing; └── coriolis: Nothing",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3494:3027,optimiz,optimizer,3027,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3494,6,"['optimiz', 'perform']","['optimizer', 'perform']"
Performance,"] is required but does not seem to be installed:; - Run `Pkg.instantiate()` to install all recorded dependencies. Stacktrace:; [1] _require(pkg::Base.PkgId); @ Base ./loading.jl:990; [2] require(uuidkey::Base.PkgId); @ Base ./loading.jl:914; [3] require(into::Module, mod::Symbol); @ Base ./loading.jl:901; [4] include(mod::Module, _path::String); @ Base ./Base.jl:386; [5] include(x::String); @ CUDA ~/.julia/packages/CUDA/3VnCC/src/CUDA.jl:1; [6] top-level scope; @ ~/.julia/packages/CUDA/3VnCC/src/device/intrinsics.jl:22; [7] include(mod::Module, _path::String); @ Base ./Base.jl:386; [8] include(x::String); @ CUDA ~/.julia/packages/CUDA/3VnCC/src/CUDA.jl:1; [9] top-level scope; @ ~/.julia/packages/CUDA/3VnCC/src/CUDA.jl:46; [10] include; @ ./Base.jl:386 [inlined]; [11] include_package_for_output(pkg::Base.PkgId, input::String, depot_path::Vector{String}, dl_load_path::Vector{String}, load_path::Vector{String}, concrete_deps::Vector{Pair{Base.PkgId, UInt64}}, source::String); @ Base ./loading.jl:1213; [12] top-level scope; @ none:1; [13] eval; @ ./boot.jl:360 [inlined]; [14] eval(x::Expr); @ Base.MainInclude ./client.jl:446; [15] top-level scope; @ none:1; in expression starting at /home/tomas/.julia/packages/CUDA/3VnCC/src/device/intrinsics/math.jl:5; in expression starting at /home/tomas/.julia/packages/CUDA/3VnCC/src/device/intrinsics.jl:22; in expression starting at /home/tomas/.julia/packages/CUDA/3VnCC/src/CUDA.jl:1; ERROR: LoadError: Failed to precompile CUDA [052768ef-5323-5732-b1bb-66c8b64840ba] to /home/tomas/.julia/compiled/v1.6/CUDA/jl_q4lPlx.; Stacktrace:; [1] error(s::String); @ Base ./error.jl:33; [2] compilecache(pkg::Base.PkgId, path::String, internal_stderr::Base.TTY, internal_stdout::Base.TTY); @ Base ./loading.jl:1360; [3] compilecache(pkg::Base.PkgId, path::String); @ Base ./loading.jl:1306; [4] _require(pkg::Base.PkgId); @ Base ./loading.jl:1021; [5] require(uuidkey::Base.PkgId); @ Base ./loading.jl:914; [6] require(into::Module, mod::Symbol); @ Ba",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1707#issuecomment-849206371:1350,load,loading,1350,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1707#issuecomment-849206371,1,['load'],['loading']
Performance,] signal (11.1): Segmentation fault; in expression starting at /home/alir/atdepth/Oceananigans.jl/particles_error.jl:35; advect_particle at /home/alir/atdepth/Oceananigans.jl/src/Models/LagrangianParticleTracking/lagrangian_particle_advection.jl:0 [inlined]; macro expansion at /home/alir/atdepth/Oceananigans.jl/src/Models/LagrangianParticleTracking/lagrangian_particle_advection.jl:177 [inlined]; cpu__advect_particles! at /home/alir/.julia/packages/KernelAbstractions/491pi/src/macros.jl:291 [inlined]; cpu__advect_particles! at ./none:0; __thread_run at /home/alir/.julia/packages/KernelAbstractions/491pi/src/cpu.jl:144; unknown function (ip: 0x7c0090512182); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; __run at /home/alir/.julia/packages/KernelAbstractions/491pi/src/cpu.jl:111; unknown function (ip: 0x7c009050feb3); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; #_#16 at /home/alir/.julia/packages/KernelAbstractions/491pi/src/cpu.jl:46; _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; jl_apply at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/julia.h:1982 [inlined]; do_apply at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/builtins.c:768; Kernel at /home/alir/.julia/packages/KernelAbstractions/491pi/src/cpu.jl:39; _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; advect_lagrangian_particles! at /home/alir/atdepth/Oceananigans.j,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3852:2392,cache,cache,2392,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3852,1,['cache'],['cache']
Performance,_apply at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/julia.h:1982 [inlined]; do_call at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:126; eval_value at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:223; eval_stmt_value at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:174 [inlined]; eval_body at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:617; jl_interpret_toplevel_thunk at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:775; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:934; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:877; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:877; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:877; ijl_toplevel_eval_in at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:985; eval at ./boot.jl:385 [inlined]; eval_user_input at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:150; repl_backend_loop at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:246; #start_repl_backend#46 at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:231; start_repl_backend at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:228; _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; #run_repl#59 at /cache/build/builder-amdci4-4/julialan,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3852:6163,cache,cache,6163,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3852,1,['cache'],['cache']
Performance,"_bfft plan_ifft! plan_bfft! plan_irfft plan_brfft; >; >; >; > plan_fft(A [, dims]; flags=FFTW.ESTIMATE, timelimit=Inf); >; >; >; > Pre-plan an optimized FFT along given dimensions (dims) of arrays matching the shape and type of A. (The first two arguments have the same meaning as for fft.) Returns an object P which; >; > represents the linear operator computed by the FFT, and which contains all of the information needed to compute fft(A, dims) quickly.; >; >; >; > To apply P to an array A, use P * A; in general, the syntax for applying plans is much like that of matrices. (A plan can only be applied to arrays of the same size as the A for which; >; > the plan was created.) You can also apply a plan with a preallocated output array Â by calling mul!(Â, plan, A). (For mul!, however, the input array A must be a complex floating-point; >; > array like the output Â.) You can compute the inverse-transform plan by inv(P) and apply the inverse plan with P \ Â (the inverse plan is cached and reused for subsequent calls to inv or; >; > \), and apply the inverse plan to a pre-allocated output array A with ldiv!(A, P, Â).; >; >; >; > The flags argument is a bitwise-or of FFTW planner flags, defaulting to FFTW.ESTIMATE. e.g. passing FFTW.MEASURE or FFTW.PATIENT will instead spend several seconds (or more) benchmarking; >; > different possible FFT algorithms and picking the fastest one; see the FFTW manual for more information on planner flags. The optional timelimit argument specifies a rough upper bound on; >; > the allowed planning time, in seconds. Passing FFTW.MEASURE or FFTW.PATIENT may cause the input array A to be overwritten with zeros during plan creation.; >; >; >; > plan_fft! is the same as plan_fft but creates a plan that operates in-place on its argument (which must be an array of complex floating-point numbers). plan_ifft and so on are similar; >; > but produce plans that perform the equivalent of the inverse transforms ifft and so on.; >; >; >; > help?> plan_fft!;",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/119#issuecomment-471179341:1350,cache,cached,1350,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/119#issuecomment-471179341,1,['cache'],['cached']
Performance,"_require_prelocked; │ @ ./loading.jl:1803 [inlined]; │ [17] _require_prelocked; │ @ ./loading.jl:1802 [inlined]; │ [18] run_extension_callbacks(extid::Base.ExtensionId); │ @ Base ./loading.jl:1295; │ [19] run_extension_callbacks(pkgid::Base.PkgId); │ @ Base ./loading.jl:1330; │ [20] run_package_callbacks(modkey::Base.PkgId); │ @ Base ./loading.jl:1164; │ [21] _tryrequire_from_serialized(modkey::Base.PkgId, path::String, ocachepath::String, sourcepath::String, depmods::Vector{Any}); │ @ Base ./loading.jl:1487; │ [22] _require_search_from_serialized(pkg::Base.PkgId, sourcepath::String, build_id::UInt128); │ @ Base ./loading.jl:1574; │ [23] _require(pkg::Base.PkgId, env::String); │ @ Base ./loading.jl:1938; │ [24] __require_prelocked(uuidkey::Base.PkgId, env::String); │ @ Base ./loading.jl:1812; │ [25] #invoke_in_world#3; │ @ ./essentials.jl:926 [inlined]; │ [26] invoke_in_world; │ @ ./essentials.jl:923 [inlined]; │ [27] _require_prelocked(uuidkey::Base.PkgId, env::String); │ @ Base ./loading.jl:1803; │ [28] macro expansion; │ @ ./loading.jl:1790 [inlined]; │ [29] macro expansion; │ @ ./lock.jl:267 [inlined]; │ [30] __require(into::Module, mod::Symbol); │ @ Base ./loading.jl:1753; │ [31] #invoke_in_world#3; │ @ ./essentials.jl:926 [inlined]; │ [32] invoke_in_world; │ @ ./essentials.jl:923 [inlined]; │ [33] require(into::Module, mod::Symbol); │ @ Base ./loading.jl:1746; │ [34] eval; │ @ ./boot.jl:385 [inlined]; │ [35] eval_user_input(ast::Any, backend::REPL.REPLBackend, mod::Module); │ @ REPL ~/julia-1.10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:150; │ [36] repl_backend_loop(backend::REPL.REPLBackend, get_module::Function); │ @ REPL ~/julia-1.10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:246; │ [37] start_repl_backend(backend::REPL.REPLBackend, consumer::Any; get_module::Function); │ @ REPL ~/julia-1.10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:231; │ [38] run_repl(repl::REPL.AbstractREPL, consumer::Any; backend_on_current_task::Bool, backend::Any); │ @ REPL ",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3686#issuecomment-2272432528:3557,load,loading,3557,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3686#issuecomment-2272432528,1,['load'],['loading']
Performance,"_v(x, y, z, t, v, p) = -p.σ * (v - p.α*p.u₀); @inline sponge_w(x, y, z, t, w, p) = -p.σ * (w - p.α*p.u₀); @inline sponge_b(x, y, z, t, b, p) = -p.σ * (b - p.α*p.u₀). Fᵤ = Forcing(sponge_u, field_dependencies = :u, parameters = (; σ=1, u₀=1, α=4e-5)); Fᵥ = Forcing(sponge_v, field_dependencies = :v, parameters = (; σ=1, u₀=1, α=4e-5)); Fw = Forcing(sponge_w, field_dependencies = :w, parameters = (; σ=1, u₀=1, α=4e-5)); Fb = Forcing(sponge_b, field_dependencies = :b, parameters = (; σ=1, u₀=1, α=4e-5)). model = NonhydrostaticModel(; grid,; advection = WENO(grid=grid, order=5),; tracers = (:b, :τ1, :τ2),; closure = SmagorinskyLilly(C=0.1),; background_fields = (b=B_field,),; forcing = (u=Fᵤ, v=Fᵥ, w=Fw, b=Fb),; ); @info model. simulation = Simulation(model, Δt=1, stop_iteration=10). run!(simulation); ```. This (and way more complex examples) runs fine on the CPU but when I run that on the GPU I get:. ```; [ Info: Executing initial time step...; ERROR: LoadError: CUDA error: device kernel image is invalid (code 200, ERROR_INVALID_IMAGE). Stacktrace:; [1] CUDA.CuModule(data::Vector{UInt8}, options::Dict{CUDA.CUjit_option_enum, Any}); @ CUDA /glade/work/tomasc/.julia/packages/CUDA/Ey3w2/lib/cudadrv/module.jl:58; [2] CuModule; @ /glade/work/tomasc/.julia/packages/CUDA/Ey3w2/lib/cudadrv/module.jl:23 [inlined]; [3] cufunction_link(job::GPUCompiler.CompilerJob, compiled::NamedTuple{(:image, :entry, :external_gvars), Tuple{Vector{UInt8}, String, Vector{String}}}); @ CUDA /glade/work/tomasc/.julia/packages/CUDA/Ey3w2/src/compiler/execution.jl:481; [4] cached_compilation(cache::Dict{UInt64, Any}, job::GPUCompiler.CompilerJob, compiler::typeof(CUDA.cufunction_compile), linker::typeof(CUDA.cufunction_link)); @ GPUCompiler /glade/work/tomasc/.julia/packages/GPUCompiler/qdoh1/src/cache.jl:95; [5] cufunction(f::typeof(Cassette.overdub), tt::Type{Tuple{Cassette.Context{nametype(CUDACtx), Nothing, Nothing, KernelAbstractions.var""##PassType#312"", Nothing, Cassette.DisableHooks}, typeof(Oc",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2869:1543,Load,LoadError,1543,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2869,1,['Load'],['LoadError']
Performance,"_x p = \partial_x p_h' + \partial_x p_n $$. This decomposition is advantageous for two reasons. First, in a hydrostatic model the vertical momentum equation reduces to the equation for $p_h'$. This means that switching from a hydrostatic to non-hydrostatic model is particularly simple given this decomposition. Second --- and this must be evaluated --- it's possible to carefully control the evaluation of the hydrostatic integral so that a resting stratified fluid remains at rest, even in the presence of complex bathymetry. When we use the ""MITgcm algorithm"" we achieve this perfectly, even with partial cell bathymetry. This PR proposes to eliminate the pressure decomposition so that there is only one pressure. In principle, this has a computational advantage because the hydrostatic pressure integral does not need to be evaluated (in practice, this computation has a negligible cost). It also reduces the number of memory loads that take place in the momentum advection kernels (though these are typically domained by advection scheme, so this may not matter except for centered advection schemes). Also in principle, it would allow 3D domain decompositions for distributed computations, in addition to 2D (but again, these are rarely used because typical ocean domains are shallow and wide, rather than deep and narrow). Having a single pressure also simplifies diagnostics. Finally, and perhaps most importantly, we can avoid allocating memory for an additional 3D field. In the absolute best case scenario of a model with no tracers and pure implicit dissipation, this means we go from 14 3D fields (9 for prognostic momentum + tendencies, 4 (?) for nonhydrostatic pressure including scratch variables for FFTs, and 1 for hydrostatic pressure) to 13 3D fields. So it saves about 7%. In more typical situations with LES closure and one active tracer, the savings is more marginal: we go from 19 3D fields to 18 3D fields, and thus have 5% more memory. Note also that more scratch variables",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3080:1932,load,loads,1932,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3080,1,['load'],['loads']
Performance,"`0` is almost always safe. But neither case is really an issue. The main issues are when the entirety of a heavy kernel (like one that calculates a tendency) may be promoted to higher precision. ~`fill(var, 0)`~ `fill!(var, 0)` is cheap and unlikely to affect performance. That said it's just more precise to write `fill(var, zero(eltype(var)))` (this is _exactly_ what you are trying to do) and therefore the preferred way to write it.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3271#issuecomment-1723671988:260,perform,performance,260,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3271#issuecomment-1723671988,1,['perform'],['performance']
Performance,`@propagate_inbounds` implies `@inline` and GPUifyLoops will force inline everything anyway on the GPU. So we only need inline annotations for CPU performance,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/164#issuecomment-481796246:147,perform,performance,147,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/164#issuecomment-481796246,1,['perform'],['performance']
Performance,`Callable` tridiagonal coefficients produces catastrophic loss of performance,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3047:66,perform,performance,66,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3047,1,['perform'],['performance']
Performance,"`HydrostaticFreeSurfaceModel` has two options for `free_surface`: `ExplicitFreeSurface` and `ImplicitFreeSurface`. Unfortunately, `ImplicitFreeSurface` using a `PreconditionedConjugateGradientSolver` incurs a significant cost (even relative to the cost of a _three-dimensional_ direct non hydrostatic pressure solve!), while `ExplicitFreeSurface` severely limits time-step with realistic `gravitational_acceleration` and velocity scales. For grids that are regular and rectilinear in the horizontal, there is a cure: we can implement a direct solve using the FFT based pressure solver either of the 2D barotropic poisson pressure equation, or for the 2D implicit free surface step. It will be interesting to compare the total cost of the barotropic integral + 2D solve, which requires integrals to extra the barotropic transport, with the 3D solve used by `IncompressibleModel`. The vertical FFT is _asymptotically_ more costly than the couple vertical integrals required for a barotropic pressure solve; but on the other hand the FFTs are highly optimized.",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1727:1047,optimiz,optimized,1047,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1727,1,['optimiz'],['optimized']
Performance,"```; julia> using Distributed. help?> addprocs; search: addprocs. addprocs(manager::ClusterManager; kwargs...) -> List of process identifiers. Launches worker processes via the specified cluster manager. For example, Beowulf clusters are supported via a custom cluster manager implemented in the package ClusterManagers.jl. The number of seconds a newly launched worker waits for connection establishment from the master can be specified via variable; JULIA_WORKER_TIMEOUT in the worker process's environment. Relevant only when using TCP/IP as transport. To launch workers without blocking the REPL, or the containing function if launching workers programmatically, execute addprocs in its own task. Examples; ≡≡≡≡≡≡≡≡≡≡. # On busy clusters, call `addprocs` asynchronously; t = @async addprocs(...). # Utilize workers as and when they come online; if nprocs() > 1 # Ensure at least one new worker is available; .... # perform distributed execution; end. # Retrieve newly launched worker IDs, or any error messages; if istaskdone(t) # Check if `addprocs` has completed to ensure `fetch` doesn't block; if nworkers() == N; new_pids = fetch(t); else; fetch(t); end; end. ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────. addprocs(machines; tunnel=false, sshflags=``, max_parallel=10, kwargs...) -> List of process identifiers. Add worker processes on remote machines via SSH. Configuration is done with keyword arguments (see below). In particular, the exename keyword; can be used to specify the path to the julia binary on the remote machine(s). machines is a vector of ""machine specifications"" which are given as strings of the form [user@]host[:port] [bind_addr[:port]]. user defaults to; current user and port to the standard SSH port. If [bind_addr[:port]] is specified, other workers will connect to this worker at the specified; bind_addr and port. It is possible to launch multiple processes on a r",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3135#issuecomment-1579765636:919,perform,perform,919,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3135#issuecomment-1579765636,1,['perform'],['perform']
Performance,"```; using Oceananigans.Diagnostics: accurate_cell_advection_timescale; using Oceananigans.Utils: cell_advection_timescale; wizard = TimeStepWizard(cfl=0.5,Δt=10.0, max_change=1.1, max_Δt=1minute, cell_advection_timescale = accurate_cell_advection_timescale); ```; gives the following error right now-. ```; MethodError: no method matching TimeStepWizard(; cfl=0.5, Δt=10.0, max_change=1.1, max_Δt=60.0, cell_advection_timescale=Oceananigans.Diagnostics.accurate_cell_advection_timescale); Closest candidates are:; TimeStepWizard(; cfl, diffusive_cfl, max_change, min_change, max_Δt, min_Δt, Δt) at C:\Users\91766\.julia\packages\Oceananigans\zj42o\src\Simulations\time_step_wizard.jl:23 got unsupported keyword argument ""cell_advection_timescale""; TimeStepWizard(::T, ::T, ::T, ::T, ::T, ::T, ::T) where T at C:\Users\91766\.julia\packages\Oceananigans\zj42o\src\Simulations\time_step_wizard.jl:2 got unsupported keyword arguments ""cfl"", ""Δt"", ""max_change"", ""max_Δt"", ""cell_advection_timescale"". Stacktrace:; [1] kwerr(::NamedTuple{(:cfl, :Δt, :max_change, :max_Δt, :cell_advection_timescale),Tuple{Float64,Float64,Float64,Float64,typeof(accurate_cell_advection_timescale)}}, ::Type{T} where T) at .\error.jl:157; [2] top-level scope at In[15]:1; [3] include_string(::Function, ::Module, ::String, ::String) at .\loading.jl:1091; ```",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1741#issuecomment-862929408:1314,load,loading,1314,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1741#issuecomment-862929408,1,['load'],['loading']
Performance,"```Julia; [ Info: ... simulation initialization complete (1.471 minutes); [ Info: Executing initial time step...; ERROR: LoadError: TaskFailedException. nested task error: TaskFailedException. nested task error: type Tuple has no field surface_TKE_flux; Stacktrace:; [1] getproperty(x::Tuple{CATKEVerticalDiffusivity{VerticallyImplicitTimeDiscretization, Float64, Oceananigans.TurbulenceClosures.CATKEVerticalDiffusivities.MixingLength{Float64}, Oceananigans.TurbulenceClosures.CATKEVerticalDiffusivities.SurfaceTKEFlux{Float64}}, AnisotropicDiffusivity{VerticallyImplicitTimeDiscretization, Float64, Float64, Float64, NamedTuple{(:b, :c, :e), Tuple{Float64, Float64, Float64}}, NamedTuple{(:b, :c, :e), Tuple{Float64, Float64, Float64}}, NamedTuple{(:b, :c, :e), Tuple{Float64, Float64, Float64}}}}, f::Symbol); @ Base ./Base.jl:33; [2] call; @ ~/.julia/packages/Cassette/1lyEM/src/context.jl:456 [inlined]; [3] fallback; @ ~/.julia/packages/Cassette/1lyEM/src/context.jl:454 [inlined]; [4] overdub; @ ~/.julia/packages/Cassette/1lyEM/src/context.jl:279 [inlined]; [5] overdub; @ ~/.julia/packages/Oceananigans/H39qI/src/TurbulenceClosures/turbulence_closure_implementations/CATKEVerticalDiffusivities/surface_TKE_flux.jl:39 [inlined]; [6] getbc(::BoundaryCondition{Oceananigans.BoundaryConditions.Flux, Oceananigans.BoundaryConditions.DiscreteBoundaryFunction{Oceananigans.TurbulenceClosures.CATKEVerticalDiffusivities.TKETopBoundaryConditionParameters{NamedTuple{(:b, :c, :e), Tuple{BoundaryCondition{Oceananigans.BoundaryConditions.Flux, Oceananigans.BoundaryConditions.DiscreteBoundaryFunction{NamedTuple{(:Ly, :Lz, :Qᵇ, :y_shutoff, :τ, :μ, :ΔB, :H, :h, :y_sponge, :λt), NTuple{11, Float64}}, typeof(buoyancy_flux)}}, BoundaryCondition{Oceananigans.BoundaryConditions.Flux, Nothing}, BoundaryCondition{Oceananigans.BoundaryConditions.Flux, Nothing}}}, NamedTuple{(:u, :v), Tuple{BoundaryCondition{Oceananigans.BoundaryConditions.Flux, Oceananigans.BoundaryConditions.DiscreteBoundaryFunction{Name",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2225#issuecomment-1030718913:121,Load,LoadError,121,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2225#issuecomment-1030718913,1,['Load'],['LoadError']
Performance,"```Julia; [ Info: Running the simulation...; ERROR: LoadError: KeyError: key ""timestepper/Gⁿ/w"" not found; Stacktrace:; [1] pathize(g::JLD2.Group{JLD2.JLDFile{JLD2.MmapIO}}, name::String, create::Bool); @ JLD2 /g/data/v45/nc3020/.julia/packages/JLD2/5iijr/src/groups.jl:70; [2] getindex(g::JLD2.Group{JLD2.JLDFile{JLD2.MmapIO}}, name::String); @ JLD2 /g/data/v45/nc3020/.julia/packages/JLD2/5iijr/src/groups.jl:95; [3] getindex(f::JLD2.JLDFile{JLD2.MmapIO}, name::String); @ JLD2 /g/data/v45/nc3020/.julia/packages/JLD2/5iijr/src/JLD2.jl:379; [4] set_time_stepper_tendencies!(timestepper::Oceananigans.TimeSteppers.QuasiAdamsBashforth2TimeStepper{Float64, NamedTuple{(:u, :v, :η, :b, :e, :c), Tuple{Field{Face, Center, Center, Nothing, RectilinearGrid{Float64, Periodic, Bounded, Bounded, Float64, Float64, Float64, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}}}, GPU}, Float64, OffsetArrays.OffsetArray{Float64, 3, CUDA.CuArray{Float64, 3}}, FieldBoundaryConditions{BoundaryCondition{Oceananigans.BoundaryConditions.Periodic, Nothing}, BoundaryCondition{Oceananigans.BoundaryConditions.Periodic, Nothing}, BoundaryCondition{Oceananigans.BoundaryConditions.Flux, Nothing}, BoundaryCondition{Oceananigans.BoundaryConditions.Flux, Nothing}, BoundaryCondition{Oceananigans.BoundaryConditions.Flux, Nothing}, BoundaryCondition{Oceananigans.BoundaryConditions.Flux, Nothing}, BoundaryCondition{Oceananigans.BoundaryConditions.Flux, Nothing}}, Nothing}, Field{Center, Face, Center, Nothing, RectilinearGrid{Float64, Periodic, Bounded, Bounded, Float64, Float64, Float64, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}}}, OffsetAr",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2223:52,Load,LoadError,52,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2223,1,['Load'],['LoadError']
Performance,"```Julia; julia> using Oceananigans, GLMakie; [ Info: Oceananigans will use 8 threads; [ Info: Precompiling TaylorSeriesIAExt [ed7ef945-33a4-511e-97fe-2b89c7a130ca]; ERROR: LoadError: UndefVarError: `IntervalBox` not defined; Stacktrace:; [1] top-level scope; @ ~/.julia/packages/TaylorSeries/2qRvJ/ext/TaylorSeriesIAExt.jl:182; [2] include; @ ./Base.jl:495 [inlined]; [3] include_package_for_output(pkg::Base.PkgId, input::String, depot_path::Vector{String}, dl_load_path::Vector{String}, load_path::Vector{String}, concrete_deps::Vector{Pair{Base.PkgId, UInt128}}, source::Nothing); @ Base ./loading.jl:2222; [4] top-level scope; @ stdin:3; in expression starting at /Users/navid/.julia/packages/TaylorSeries/2qRvJ/ext/TaylorSeriesIAExt.jl:1; in expression starting at stdin:3; ┌ Error: Error during loading of extension TaylorSeriesIAExt of TaylorSeries, use `Base.retry_load_extensions()` to retry.; │ exception =; │ 1-element ExceptionStack:; │ Failed to precompile TaylorSeriesIAExt [ed7ef945-33a4-511e-97fe-2b89c7a130ca] to ""/Users/navid/.julia/compiled/v1.10/TaylorSeriesIAExt/jl_TNauRw"".; │ Stacktrace:; │ [1] error(s::String); │ @ Base ./error.jl:35; │ [2] compilecache(pkg::Base.PkgId, path::String, internal_stderr::IO, internal_stdout::IO, keep_loaded_modules::Bool); │ @ Base ./loading.jl:2468; │ [3] compilecache; │ @ ./loading.jl:2340 [inlined]; │ [4] (::Base.var""#968#969""{Base.PkgId})(); │ @ Base ./loading.jl:1974; │ [5] mkpidlock(f::Base.var""#968#969""{Base.PkgId}, at::String, pid::Int32; kwopts::@Kwargs{stale_age::Int64, wait::Bool}); │ @ FileWatching.Pidfile ~/julia-1.10/usr/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:93; │ [6] #mkpidlock#6; │ @ ~/julia-1.10/usr/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:88 [inlined]; │ [7] trymkpidlock(::Function, ::Vararg{Any}; kwargs::@Kwargs{stale_age::Int64}); │ @ FileWatching.Pidfile ~/julia-1.10/usr/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:111; │ [8] #invokelatest#2; │ @ ./essentials.jl:894 [inlined",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3686#issuecomment-2272432528:173,Load,LoadError,173,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3686#issuecomment-2272432528,3,"['Load', 'load']","['LoadError', 'loading']"
Performance,"```[DengQ@localhost bin]$ ./julia; _; _ _ _(_)_ | Documentation: https://docs.julialang.org; (_) | (_) (_) |; _ _ _| |_ __ _ | Type ""?"" for help, ""]?"" for Pkg help.; | | | | | | |/ _` | |; | | |_| | | | (_| | | Version 1.5.3 (2020-11-09); _/ |\__'_|_|_|\__'_| | Official https://julialang.org/ release; |__/ |. julia> pkg""st --manifest""; ERROR: LoadError: UndefVarError: @pkg_str not defined; in expression starting at REPL[1]:1",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1601#issuecomment-824054281:345,Load,LoadError,345,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1601#issuecomment-824054281,1,['Load'],['LoadError']
Performance,"```julia; (base) gregorywagner:Oceananigans.jl/ (main✗) $ JULIA_NUM_THREADS=6 julia --project race_condition_test.jl; [ Info: Oceananigans will use 6 threads; [ Info: Initializing simulation...; [ Info: ... simulation initialization complete (78.718 ms); [ Info: Executing initial time step...; [ Info: ... initial time step complete (6.593 seconds).; [ Info: Simulation is stopping. Model iteration 100 has hit or exceeded simulation stop iteration 100.; (parent(simulation.model.velocities.u))[1, 1, :] = [1.9561509021434433, 1.9561509021434433, 1.9560395693473134, 1.9560395693473134]; Test Failed at /Users/gregorywagner/Projects/test/Oceananigans.jl/race_condition_test.jl:17; Expression: (parent(simulation.model.velocities.u))[1, 1, 2] == (parent(simulation.model.velocities.u))[1, 1, 3]; Evaluated: 1.9561509021434433 == 1.9560395693473134; ERROR: LoadError: There was an error during testing; in expression starting at /Users/gregorywagner/Projects/test/Oceananigans.jl/race_condition_test.jl:17; ```. bingo",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2809#issuecomment-1308175571:856,Load,LoadError,856,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2809#issuecomment-1308175571,1,['Load'],['LoadError']
Performance,```julia; CuArrays.allowscalar(false); ```; to disable slow fallback methods for CuArrays so we don't accidentally perform some slow operation.,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/82:115,perform,perform,115,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/82,1,['perform'],['perform']
Performance,```julia; ERROR: LoadError: cannot write a pointer to JLD file; ```,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/420:17,Load,LoadError,17,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/420,1,['Load'],['LoadError']
Performance,"```julia; using Oceananigans; using Oceananigans.AbstractOperations; using Oceananigans.Fields. grid = RegularCartesianGrid(size=(1, 1, 1), extent=(1, 1, 1)) . model = IncompressibleModel(architecture=GPU(), grid=grid). u, v, w = model.velocities. Σˣˣ = ∂x(u); Σʸʸ = ∂y(v); Σᶻᶻ = ∂z(w). Σˣʸ = (∂y(u) + ∂x(v)) / 2 ; Σˣᶻ = (∂z(u) + ∂x(w)) / 2 ; Σʸᶻ = (∂z(v) + ∂y(w)) / 2 . ϵ = model.closure.ν * 2 * (Σˣˣ^2 + Σʸʸ^2 + Σᶻᶻ^2 + 2 * (Σˣʸ^2 + Σˣᶻ^2 + Σʸᶻ^2)). ϵ_field = ComputedField(ϵ). compute!(ϵ_field); ```. produces. ```; julia> include(""complex_output.jl""); [ Info: Precompiling Oceananigans [9e8cae18-63c1-5223-a75c-80ca9d6e9a09]; [ Info: Oceananigans will use 24 threads; ERROR: LoadError: InvalidIRError: compiling kernel gpu__compute!(Cassette.Context{nametype(CUDACtx),KernelAbstractions.CompilerMetadata{KernelAbstractions.NDIteration.StaticSize{(1, 1, 1)},KernelAbstractions.NDIteration.DynamicCheck,Nothing,Nothing,KernelAbstractions.NDIteration.NDRange{3,KernelAbstractions.NDIteration.StaticSize{(1, 1, 1)},KernelAbstractions.NDIteration.StaticSize{(1, 1, 1)},Nothing,Nothing}},Nothing,KernelAbstractions.var""##PassType#253"",Nothing,Cassette.DisableHooks}, typeof(Oceananigans.Fields.gpu__compute!), OffsetArrays.OffsetArray{Float64,3,CUDA.CuDeviceArray{Float64,3,1}}, Oceananigans.AbstractOperations.BinaryOperation{Cell,Cell,Cell,typeof(*),Float64,Oceananigans.AbstractOperations.MultiaryOperation{Cell,Cell,Cell,4,typeof(+),Tuple{Oceananigans.AbstractOperations.BinaryOperation{Cell,Cell,Cell,typeof(^),Oceananigans.AbstractOperations.Derivative{Cell,Cell,Cell,typeof(Oceananigans.Operators.∂xᶜᵃᵃ),OffsetArrays.OffsetArray{Float64,3,CUDA.CuDeviceArray{Float64,3,1}},typeof(identity),RegularCartesianGrid{Float64,Periodic,Periodic,Bounded,OffsetArrays.OffsetArray{Float64,1,StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}}}},Int64,typeof(identity),typeof(identity),typeof(identity),RegularCartesianGrid{Float64,Periodic,Periodic,Bounded,OffsetArrays.OffsetArr",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1241#issuecomment-738834053:679,Load,LoadError,679,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1241#issuecomment-738834053,1,['Load'],['LoadError']
Performance,"```julia; using Oceananigans; using Oceananigans.Units; using Oceananigans.TurbulenceClosures: CATKEVerticalDiffusivity. grid = RectilinearGrid(size=128, z=(-128, 0), topology=(Flat, Flat, Bounded)). closure = (VerticalScalarDiffusivity(VerticallyImplicitTimeDiscretization(), κ=1e-4),; CATKEVerticalDiffusivity()). model = HydrostaticFreeSurfaceModel(; grid, closure,; tracers = (:b, :e),; buoyancy = BuoyancyTracer()). bᵢ(z) = 1e-5 * z; set!(model, b = bᵢ); simulation = Simulation(model, Δt=1minute, stop_iteration=10). run!(simulation); ```. generates the error. ```julia; julia> include(""tupled_vertical_diffusion.jl""); [ Info: Initializing simulation...; [ Info: ... simulation initialization complete (2.367 seconds); [ Info: Executing initial time step...; ERROR: LoadError: type Tuple has no field κe; Stacktrace:; [1] getproperty; @ ./Base.jl:37 [inlined]; [2] time_step_catke_equation!(model::HydrostaticFreeSurfaceModel{…}); @ Oceananigans.TurbulenceClosures.TKEBasedVerticalDiffusivities ~/Projects/Oceananigans.jl/src/TurbulenceClosures/turbulence_closure_implementations/TKEBasedVerticalDiffusivities/time_step_catke_equation.jl:25; [3] compute_diffusivities!(diffusivities::@NamedTuple{…}, closure::CATKEVerticalDiffusivity{…}, model::HydrostaticFreeSurfaceModel{…}; parameters::Symbol); @ Oceananigans.TurbulenceClosures.TKEBasedVerticalDiffusivities ~/Projects/Oceananigans.jl/src/TurbulenceClosures/turbulence_closure_implementations/TKEBasedVerticalDiffusivities/catke_vertical_diffusivity.jl:197; [4] compute_diffusivities!; @ ~/Projects/Oceananigans.jl/src/TurbulenceClosures/turbulence_closure_implementations/TKEBasedVerticalDiffusivities/catke_vertical_diffusivity.jl:181 [inlined]; [5] #compute_diffusivities!#24; @ ~/Projects/Oceananigans.jl/src/TurbulenceClosures/closure_tuples.jl:79 [inlined]; [6] compute_diffusivities!; @ ~/Projects/Oceananigans.jl/src/TurbulenceClosures/closure_tuples.jl:76 [inlined]; [7] update_state!(model::HydrostaticFreeSurfaceModel{…}, grid::Re",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3804:772,Load,LoadError,772,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3804,1,['Load'],['LoadError']
Performance,"`const` matters when a variable is referenced as a global variable in a function; something like. ```julia; const a = 2; f(x) = a * x; ```. `a` is not an argument to `f(x)`; it's value is taken from the global scope. In that case it needs to be `const`, or `f(x)` cannot be compiled on the GPU. When numbers are added to explicit data structures --- which is what happens when they are inserted into the `parameters` kwarg in the constructor for `BoundaryCondition` or `Forcing` --- then it's irrelevant whether a variable is declared `const`. This is because in that case the variable is explicitly an argument to the function (eg via the argument `p` in `boundary_condition(x, y, t, p)`). In fact, this is the purpose of the `parameters` kwarg --- to avoid having to use `const` (which is annoying or inconvenient, and has compilation / performance pitfalls). However the API has not developed enough to completely avoid `const` in all cases (eg for `BackgroundFields`, or for masking / target functions in things like `Relaxation`). So we still wrestle with it from time to time.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1863#issuecomment-881683697:839,perform,performance,839,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1863#issuecomment-881683697,1,['perform'],['performance']
Performance,`outputinfo(filename)` for inspecting output without loading data,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3859:53,load,loading,53,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3859,1,['load'],['loading']
Performance,"`time_step_precomputations!` is. ```julia; function time_step_precomputations!(diffusivities, pressures, velocities, tracers, model). fill_halo_regions!(merge(velocities, tracers), model.boundary_conditions.solution, model.architecture,; model.grid, boundary_condition_function_arguments(model)...). calculate_diffusivities!(diffusivities, model.architecture, model.grid, model.closure, model.buoyancy,; velocities, tracers). fill_halo_regions!(diffusivities, model.boundary_conditions.diffusivities, model.architecture, model.grid). @launch(device(model.architecture), config=launch_config(model.grid, :xy),; update_hydrostatic_pressure!(pressures.pHY′, model.grid, model.buoyancy, tracers)). fill_halo_regions!(pressures.pHY′, model.boundary_conditions.pressure, model.architecture, model.grid). return nothing; end; ```. To implement the optimizations discussed in this issue, we need to also consider the calculation of hydrostatic pressure and nonlinear diffusivities to intertwine communication with interior tendency computation. Can this be done abstractly perhaps via some combination of launch configurations and macro specifications to `@loop_xyz`? This would allow us to exert control over the ""region"" of interior source term computation from the ""outside"", while keeping our kernels intact. Notice that the ""pre-computation"" of nonlinear diffusivities and the isolation of the hydrostatic pressure both add communication steps. We should monitor whether these become significantly suboptimal algorithms in the presence of expensive communication. We can easily combine hydrostatic pressure with nonhydrostatic pressure with no loss of performance (probably a small performance increase, in fact). We can also in principle calculate nonlinear diffusivities ""in-line"", though when we tried this previously we were unable to achieve good performance. Also, ""in-line"" calculation of diffusivities makes the application of diffusivity boundary conditions much more difficult (or impossible).",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/615#issuecomment-583379290:3129,perform,performance,3129,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/615#issuecomment-583379290,3,['perform'],['performance']
Performance,"a/libquadmath.0.dylib' (no such file), '/Applications/Julia-1.7.app/Contents/Resources/julia/bin/../lib/libquadmath.0.dylib' (no such file), '/usr/local/lib/libquadmath.0.dylib' (no such file), '/usr/lib/libquadmath.0.dylib' (no such file); Stacktrace:; [1] dlopen(s::String, flags::UInt32; throw_error::Bool); @ Base.Libc.Libdl ./libdl.jl:117; [2] dlopen(s::String, flags::UInt32); @ Base.Libc.Libdl ./libdl.jl:117; [3] macro expansion; @ ~/.julia/packages/JLLWrappers/QpMQW/src/products/library_generators.jl:54 [inlined]; [4] __init__(); @ MPICH_jll ~/.julia/packages/MPICH_jll/dhUyI/src/wrappers/aarch64-apple-darwin-libgfortran5.jl:32; [5] _include_from_serialized(path::String, depmods::Vector{Any}); @ Base ./loading.jl:768; [6] _require_search_from_serialized(pkg::Base.PkgId, sourcepath::String); @ Base ./loading.jl:854; [7] _require(pkg::Base.PkgId); @ Base ./loading.jl:1097; [8] require(uuidkey::Base.PkgId); @ Base ./loading.jl:1013; [9] require(into::Module, mod::Symbol); @ Base ./loading.jl:997; [10] top-level scope; @ ~/.julia/packages/MPI/08SPr/deps/deps.jl:8; [11] include(mod::Module, _path::String); @ Base ./Base.jl:418; [12] include(x::String); @ MPI ~/.julia/packages/MPI/08SPr/src/MPI.jl:1; [13] top-level scope; @ ~/.julia/packages/MPI/08SPr/src/MPI.jl:36; [14] include; @ ./Base.jl:418 [inlined]; [15] include_package_for_output(pkg::Base.PkgId, input::String, depot_path::Vector{String}, dl_load_path::Vector{String}, load_path::Vector{String}, concrete_deps::Vector{Pair{Base.PkgId, UInt64}}, source::String); @ Base ./loading.jl:1318; [16] top-level scope; @ none:1; [17] eval; @ ./boot.jl:373 [inlined]; [18] eval(x::Expr); @ Base.MainInclude ./client.jl:453; [19] top-level scope; @ none:1; during initialization of module MPICH_jll; in expression starting at /Users/sean/.julia/packages/MPI/08SPr/deps/deps.jl:1; ERROR: LoadError: Failed to precompile MPI [da04e1cc-30fd-572f-bb4f-1f8673147195] to /Users/sean/.julia/compiled/v1.7/MPI/jl_AfEwik.; Stacktrace:; [1] er",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2480:3428,load,loading,3428,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2480,1,['load'],['loading']
Performance,"a/packages/CUDA/3VnCC/src/device/intrinsics/math.jl:5; in expression starting at /home/tomas/.julia/packages/CUDA/3VnCC/src/device/intrinsics.jl:22; in expression starting at /home/tomas/.julia/packages/CUDA/3VnCC/src/CUDA.jl:1; ERROR: LoadError: Failed to precompile CUDA [052768ef-5323-5732-b1bb-66c8b64840ba] to /home/tomas/.julia/compiled/v1.6/CUDA/jl_q4lPlx.; Stacktrace:; [1] error(s::String); @ Base ./error.jl:33; [2] compilecache(pkg::Base.PkgId, path::String, internal_stderr::Base.TTY, internal_stdout::Base.TTY); @ Base ./loading.jl:1360; [3] compilecache(pkg::Base.PkgId, path::String); @ Base ./loading.jl:1306; [4] _require(pkg::Base.PkgId); @ Base ./loading.jl:1021; [5] require(uuidkey::Base.PkgId); @ Base ./loading.jl:914; [6] require(into::Module, mod::Symbol); @ Base ./loading.jl:901; [7] include; @ ./Base.jl:386 [inlined]; [8] include_package_for_output(pkg::Base.PkgId, input::String, depot_path::Vector{String}, dl_load_path::Vector{String}, load_path::Vector{String}, concrete_deps::Vector{Pair{Base.PkgId, UInt64}}, source::String); @ Base ./loading.jl:1213; [9] top-level scope; @ none:1; [10] eval; @ ./boot.jl:360 [inlined]; [11] eval(x::Expr); @ Base.MainInclude ./client.jl:446; [12] top-level scope; @ none:1; in expression starting at /home/tomas/repos/Oceananigans.jl/src/Oceananigans.jl:1; ERROR: LoadError: Failed to precompile Oceananigans [9e8cae18-63c1-5223-a75c-80ca9d6e9a09] to /home/tomas/.julia/compiled/v1.6/Oceananigans/jl_psrPk0.; Stacktrace:; [1] error(s::String); @ Base ./error.jl:33; [2] compilecache(pkg::Base.PkgId, path::String, internal_stderr::Base.TTY, internal_stdout::Base.TTY); @ Base ./loading.jl:1360; [3] compilecache(pkg::Base.PkgId, path::String); @ Base ./loading.jl:1306; [4] _require(pkg::Base.PkgId); @ Base ./loading.jl:1021; [5] require(uuidkey::Base.PkgId); @ Base ./loading.jl:914; [6] require(into::Module, mod::Symbol); @ Base ./loading.jl:901; in expression starting at /home/tomas/repos/Oceananigans.jl/docs/make.jl:8; ```",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1707#issuecomment-849206371:2638,load,loading,2638,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1707#issuecomment-849206371,7,"['Load', 'load']","['LoadError', 'loading']"
Performance,aac8ad0ee5); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; jl_apply at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/julia.h:1982 [inlined]; do_call at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:126; eval_value at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:223; eval_stmt_value at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:174 [inlined]; eval_body at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:617; jl_interpret_toplevel_thunk at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:775; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:934; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:877; ijl_toplevel_eval_in at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:985; eval at ./boot.jl:385 [inlined]; include_string at ./loading.jl:2076; _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; _include at ./loading.jl:2136; include at ./Base.jl:495; jfptr_include_46447.1 at /orcd/data/raffaele/001/glwagner/Software/julia-1.10.5/lib/julia/sys.so (unknown line); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; exec_options at ./client.jl:318; _start at ./client.jl:552; jfptr__start_82798.1 at /orcd/data/raffaele/001/glwagner/Software/julia-1.10.5/lib/julia/sys.so (unknown line); _jl_invoke at /cac,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3878:4693,cache,cache,4693,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3878,1,['cache'],['cache']
Performance,"ackages/KernelAbstractions/jAutM/src/macros.jl:80; [4] overdub at /home/glwagner/.julia/packages/Cassette/158rp/src/overdub.jl:0; Reason: unsupported dynamic function invocation (call to overdub); Stacktrace:; [1] macro expansion at /home/glwagner/.julia/packages/Oceananigans/cLFd3/src/Fields/computed_field.jl:86; [2] gpu__compute! at /home/glwagner/.julia/packages/KernelAbstractions/jAutM/src/macros.jl:80; [3] overdub at /home/glwagner/.julia/packages/Cassette/158rp/src/overdub.jl:0; Stacktrace:; [1] check_ir(::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDA.CUDACompilerParams}, ::LLVM.Module) at /home/glwagner/.julia/packages/GPUCompiler/uTpNx/src/validation.jl:123; [2] macro expansion at /home/glwagner/.julia/packages/GPUCompiler/uTpNx/src/driver.jl:239 [inlined]; [3] macro expansion at /home/glwagner/.julia/packages/TimerOutputs/ZmKD7/src/TimerOutput.jl:206 [inlined]; [4] codegen(::Symbol, ::GPUCompiler.CompilerJob; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, validate::Bool, only_entry::Bool) at /home/glwagner/.julia/packages/GPUCompiler/uTpNx/src/driver.jl:237; [5] compile(::Symbol, ::GPUCompiler.CompilerJob; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, validate::Bool, only_entry::Bool) at /home/glwagner/.julia/packages/GPUCompiler/uTpNx/src/driver.jl:39; [6] compile at /home/glwagner/.julia/packages/GPUCompiler/uTpNx/src/driver.jl:35 [inlined]; [7] cufunction_compile(::GPUCompiler.FunctionSpec; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:maxthreads,),Tuple{Int64}}}) at /home/glwagner/.julia/packages/CUDA/YeS8q/src/compiler/execution.jl:310; [8] check_cache(::Dict{UInt64,Any}, ::Any, ::Any, ::GPUCompiler.FunctionSpec{typeof(Cassette.overdub),Tuple{Cassette.Context{nametype(CUDACtx),KernelAbstractions.CompilerMetadata{KernelAbstractions.NDIteration.StaticSize{(1, 1, 1)},KernelAbstractions.NDIteration.DynamicCheck,Nothing,Nothing,KernelAbstractions.NDIteration.NDRange{3,KernelA",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1241#issuecomment-738834053:10887,optimiz,optimize,10887,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1241#issuecomment-738834053,1,['optimiz'],['optimize']
Performance,"actOperation that computes Ri in advance; u, v, w = model.velocities; b = model.tracers.b; Ri_op = @at (Center, Center, Center) ∂z(b) / (∂z(u)^2 + ∂z(v)^2). """""" Compute the Richardson number and store in `Ri`. """"""; function compute_Ri!(sim); Ri .= Ri_op. # Zero NaNs; Ri_parent = parent(Ri); parent(Ri_parent)[isnan.(Ri_parent)] .= 0. fill_halo_regions!(Ri, sim.model.architecture); return nothing; end. simulation.callbacks[:compute_Ri] = Callback(compute_Ri!) # uses default IterationInterval(1). fields = []; function grab_fields!(sim); uⁿ = Array(interior(u, 1, 1, :)); vⁿ = Array(interior(v, 1, 1, :)); bⁿ = Array(interior(b, 1, 1, :)); iter = iteration(sim); t = time(sim); push!(fields, (; t, iter, u=uⁿ, v=vⁿ, b=bⁿ)); return nothing; end. simulation.callbacks[:grabber] = Callback(grab_fields!, TimeInterval(10minutes)). N² = 1e-4; S² = 1e-3 # Ri = 0.1; z₀ = -50; Δzu = 4; Δzb = 16. step(x, c, w) = 1/2 * (1 + tanh((x - c) / w)) # smooth step function. uᵢ(x, y, z) = Δzu * sqrt(S²) * step(z, z₀, Δzu); bᵢ(x, y, z) = Δzb * N² * step(z, z₀, Δzb). set!(model, u=uᵢ, b=bᵢ). run!(simulation). z = znodes(Center, grid); fig = Figure(); ax_u = Axis(fig[1, 1], ylabel=""z (m)"", xlabel=""u (m s⁻¹)""); ax_b = Axis(fig[1, 2], ylabel=""z (m)"", xlabel=""b (m s⁻²)""); slider = Slider(fig[2, :], range=1:length(fields), startvalue=1); n = slider.value; uⁿ = @lift fields[$n].u; bⁿ = @lift fields[$n].b. lines!(ax_u, uⁿ, z); lines!(ax_b, bⁿ, z). title = @lift ""Diffusing shear layer at t = "" * prettytime(fields[$n].t); Label(fig[0, :], title). display(fig). record(fig, ""pacanowski_philander_diffusion.gif"", 1:length(fields), framerate=12) do nn; @info ""Drawing frame $nn of $(length(fields))...""; n[] = nn; end; ```. produces. ![pacanowski_philander_diffusion](https://user-images.githubusercontent.com/15271942/158040409-1157f273-89b7-429e-b9f1-2bde8a62d00b.gif). My main concern is that our broadcasting may not be performant when used online during a simulation. Possibly, we can fix that or make it better.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2337#issuecomment-1065996051:2722,perform,performant,2722,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2337#issuecomment-1065996051,1,['perform'],['performant']
Performance,"ading.jl:1295; │ [19] run_extension_callbacks(pkgid::Base.PkgId); │ @ Base ./loading.jl:1330; │ [20] run_package_callbacks(modkey::Base.PkgId); │ @ Base ./loading.jl:1164; │ [21] _tryrequire_from_serialized(modkey::Base.PkgId, path::String, ocachepath::String, sourcepath::String, depmods::Vector{Any}); │ @ Base ./loading.jl:1487; │ [22] _require_search_from_serialized(pkg::Base.PkgId, sourcepath::String, build_id::UInt128); │ @ Base ./loading.jl:1574; │ [23] _require(pkg::Base.PkgId, env::String); │ @ Base ./loading.jl:1938; │ [24] __require_prelocked(uuidkey::Base.PkgId, env::String); │ @ Base ./loading.jl:1812; │ [25] #invoke_in_world#3; │ @ ./essentials.jl:926 [inlined]; │ [26] invoke_in_world; │ @ ./essentials.jl:923 [inlined]; │ [27] _require_prelocked(uuidkey::Base.PkgId, env::String); │ @ Base ./loading.jl:1803; │ [28] macro expansion; │ @ ./loading.jl:1790 [inlined]; │ [29] macro expansion; │ @ ./lock.jl:267 [inlined]; │ [30] __require(into::Module, mod::Symbol); │ @ Base ./loading.jl:1753; │ [31] #invoke_in_world#3; │ @ ./essentials.jl:926 [inlined]; │ [32] invoke_in_world; │ @ ./essentials.jl:923 [inlined]; │ [33] require(into::Module, mod::Symbol); │ @ Base ./loading.jl:1746; │ [34] eval; │ @ ./boot.jl:385 [inlined]; │ [35] eval_user_input(ast::Any, backend::REPL.REPLBackend, mod::Module); │ @ REPL ~/julia-1.10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:150; │ [36] repl_backend_loop(backend::REPL.REPLBackend, get_module::Function); │ @ REPL ~/julia-1.10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:246; │ [37] start_repl_backend(backend::REPL.REPLBackend, consumer::Any; get_module::Function); │ @ REPL ~/julia-1.10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:231; │ [38] run_repl(repl::REPL.AbstractREPL, consumer::Any; backend_on_current_task::Bool, backend::Any); │ @ REPL ~/julia-1.10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:389; │ [39] run_repl(repl::REPL.AbstractREPL, consumer::Any); │ @ REPL ~/julia-1.10/usr/share/julia/stdlib/v1.10/REPL/src/RE",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3686#issuecomment-2272432528:3740,load,loading,3740,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3686#issuecomment-2272432528,1,['load'],['loading']
Performance,"ading.jl:2980 [inlined]; [12] _require(pkg::Base.PkgId, env::String); @ Base ./loading.jl:1970; [13] __require_prelocked(uuidkey::Base.PkgId, env::String); @ Base ./loading.jl:1812; [14] #invoke_in_world#3; @ ./essentials.jl:926 [inlined]; [15] invoke_in_world; @ ./essentials.jl:923 [inlined]; [16] _require_prelocked(uuidkey::Base.PkgId, env::String); @ Base ./loading.jl:1803; [17] macro expansion; @ ./loading.jl:1790 [inlined]; [18] macro expansion; @ ./lock.jl:267 [inlined]; [19] __require(into::Module, mod::Symbol); @ Base ./loading.jl:1753; [20] #invoke_in_world#3; @ ./essentials.jl:926 [inlined]; [21] invoke_in_world; @ ./essentials.jl:923 [inlined]; [22] require(into::Module, mod::Symbol); @ Base ./loading.jl:1746; [23] include; @ ./Base.jl:495 [inlined]; [24] include_package_for_output(pkg::Base.PkgId, input::String, depot_path::Vector{String}, dl_load_path::Vector{String}, load_path::Vector{String}, concrete_deps::Vector{Pair{Base.PkgId, UInt128}}, source::String); @ Base ./loading.jl:2222; [25] top-level scope; @ stdin:3; in expression starting at /glade/u/home/knudsenl/.julia/packages/CUDA/Tl08O/src/CUDA.jl:1; in expression starting at stdin:3; ERROR: LoadError: Failed to precompile CUDA [052768ef-5323-5732-b1bb-66c8b64840ba] to ""/glade/u/home/knudsenl/.julia/compiled/v1.10/CUDA/jl_zRopeZ"".; Stacktrace:; [1] error(s::String); @ Base ./error.jl:35; [2] compilecache(pkg::Base.PkgId, path::String, internal_stderr::IO, internal_stdout::IO, keep_loaded_modules::Bool); @ Base ./loading.jl:2468; [3] compilecache; @ ./loading.jl:2340 [inlined]; [4] (::Base.var""#968#969""{Base.PkgId})(); @ Base ./loading.jl:1974; [5] mkpidlock(f::Base.var""#968#969""{Base.PkgId}, at::String, pid::Int32; kwopts::@Kwargs{stale_age::Int64, wait::Bool}); @ FileWatching.Pidfile /glade/u/apps/casper/23.10/spack/opt/spack/julia/1.10.2/gcc/7.5.0/apod/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:93; [6] #mkpidlock#6; @ /glade/u/apps/casper/23.10/spack/opt/spack/julia/1.10.2/gcc/7.5.0/ap",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2231635812:2750,load,loading,2750,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2231635812,1,['load'],['loading']
Performance,"ading.jl:2980 [inlined]; [12] _require(pkg::Base.PkgId, env::String); @ Base ./loading.jl:1970; [13] __require_prelocked(uuidkey::Base.PkgId, env::String); @ Base ./loading.jl:1812; [14] #invoke_in_world#3; @ ./essentials.jl:926 [inlined]; [15] invoke_in_world; @ ./essentials.jl:923 [inlined]; [16] _require_prelocked(uuidkey::Base.PkgId, env::String); @ Base ./loading.jl:1803; [17] macro expansion; @ ./loading.jl:1790 [inlined]; [18] macro expansion; @ ./lock.jl:267 [inlined]; [19] __require(into::Module, mod::Symbol); @ Base ./loading.jl:1753; [20] #invoke_in_world#3; @ ./essentials.jl:926 [inlined]; [21] invoke_in_world; @ ./essentials.jl:923 [inlined]; [22] require(into::Module, mod::Symbol); @ Base ./loading.jl:1746; [23] include; @ ./Base.jl:495 [inlined]; [24] include_package_for_output(pkg::Base.PkgId, input::String, depot_path::Vector{String}, dl_load_path::Vector{String}, load_path::Vector{String}, concrete_deps::Vector{Pair{Base.PkgId, UInt128}}, source::String); @ Base ./loading.jl:2222; [25] top-level scope; @ stdin:3; in expression starting at /glade/u/home/knudsenl/.julia/packages/Oceananigans/M82LU/src/Oceananigans.jl:1; in expression starting at stdin:3; ERROR: LoadError: Failed to precompile Oceananigans [9e8cae18-63c1-5223-a75c-80ca9d6e9a09] to ""/glade/u/home/knudsenl/.julia/compiled/v1.10/Oceananigans/jl_k7YOZN"".; Stacktrace:; [1] error(s::String); @ Base ./error.jl:35; [2] compilecache(pkg::Base.PkgId, path::String, internal_stderr::IO, internal_stdout::IO, keep_loaded_modules::Bool); @ Base ./loading.jl:2468; [3] compilecache; @ ./loading.jl:2340 [inlined]; [4] (::Base.var""#968#969""{Base.PkgId})(); @ Base ./loading.jl:1974; [5] mkpidlock(f::Base.var""#968#969""{Base.PkgId}, at::String, pid::Int32; kwopts::@Kwargs{stale_age::Int64, wait::Bool}); @ FileWatching.Pidfile /glade/u/apps/casper/23.10/spack/opt/spack/julia/1.10.2/gcc/7.5.0/apod/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:93; [6] #mkpidlock#6; @ /glade/u/apps/casper/23.10/spack/opt",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2231635812:5331,load,loading,5331,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2231635812,1,['load'],['loading']
Performance,agrangianParticleTracking/LagrangianParticleTracking.jl:143 [inlined]; step_lagrangian_particles! at /home/alir/atdepth/Oceananigans.jl/src/Models/HydrostaticFreeSurfaceModels/HydrostaticFreeSurfaceModels.jl:107 [inlined]; #time_step!#8 at /home/alir/atdepth/Oceananigans.jl/src/TimeSteppers/quasi_adams_bashforth_2.jl:124; time_step! at /home/alir/atdepth/Oceananigans.jl/src/TimeSteppers/quasi_adams_bashforth_2.jl:76; unknown function (ip: 0x7c00a0f12fbd); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; top-level scope at /home/alir/atdepth/Oceananigans.jl/particles_error.jl:37; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:925; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:877; ijl_toplevel_eval_in at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:985; eval at ./boot.jl:385 [inlined]; include_string at ./loading.jl:2076; _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; _include at ./loading.jl:2136; include at ./client.jl:489; unknown function (ip: 0x7c00f54ff855); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; jl_apply at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/julia.h:1982 [inlined]; do_call at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:126; eval_value at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:223; eval_stmt_value at /cache/build/builder-amdci4-4/julialang/julia-relea,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3852:4530,cache,cache,4530,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3852,1,['cache'],['cache']
Performance,"ait::Bool}); @ FileWatching.Pidfile /glade/u/apps/casper/23.10/spack/opt/spack/julia/1.10.2/gcc/7.5.0/apod/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:93; [6] #mkpidlock#6; @ /glade/u/apps/casper/23.10/spack/opt/spack/julia/1.10.2/gcc/7.5.0/apod/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:88 [inlined]; [7] trymkpidlock(::Function, ::Vararg{Any}; kwargs::@Kwargs{stale_age::Int64}); @ FileWatching.Pidfile /glade/u/apps/casper/23.10/spack/opt/spack/julia/1.10.2/gcc/7.5.0/apod/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:111; [8] #invokelatest#2; @ ./essentials.jl:894 [inlined]; [9] invokelatest; @ ./essentials.jl:889 [inlined]; [10] maybe_cachefile_lock(f::Base.var""#968#969""{Base.PkgId}, pkg::Base.PkgId, srcpath::String; stale_age::Int64); @ Base ./loading.jl:2983; [11] maybe_cachefile_lock; @ ./loading.jl:2980 [inlined]; [12] _require(pkg::Base.PkgId, env::String); @ Base ./loading.jl:1970; [13] __require_prelocked(uuidkey::Base.PkgId, env::String); @ Base ./loading.jl:1812; [14] #invoke_in_world#3; @ ./essentials.jl:926 [inlined]; [15] invoke_in_world; @ ./essentials.jl:923 [inlined]; [16] _require_prelocked(uuidkey::Base.PkgId, env::String); @ Base ./loading.jl:1803; [17] macro expansion; @ ./loading.jl:1790 [inlined]; [18] macro expansion; @ ./lock.jl:267 [inlined]; [19] __require(into::Module, mod::Symbol); @ Base ./loading.jl:1753; [20] #invoke_in_world#3; @ ./essentials.jl:926 [inlined]; [21] invoke_in_world; @ ./essentials.jl:923 [inlined]; [22] require(into::Module, mod::Symbol); @ Base ./loading.jl:1746; [23] include; @ ./Base.jl:495 [inlined]; [24] include_package_for_output(pkg::Base.PkgId, input::String, depot_path::Vector{String}, dl_load_path::Vector{String}, load_path::Vector{String}, concrete_deps::Vector{Pair{Base.PkgId, UInt128}}, source::String); @ Base ./loading.jl:2222; [25] top-level scope; @ stdin:3; in expression starting at /glade/u/home/knudsenl/.julia/packages/Oceananigans/M82LU/src/Oceananigans.jl:1; in expression s",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2231635812:4499,load,loading,4499,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2231635812,1,['load'],['loading']
Performance,alang/julia-release-1-dot-10/src/interpreter.c:223; eval_stmt_value at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:174 [inlined]; eval_body at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:617; jl_interpret_toplevel_thunk at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:775; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:934; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:877; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:877; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:877; ijl_toplevel_eval_in at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:985; eval at ./boot.jl:385 [inlined]; eval_user_input at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:150; repl_backend_loop at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:246; #start_repl_backend#46 at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:231; start_repl_backend at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:228; _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; #run_repl#59 at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:389; run_repl at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:375; jfptr_run_repl_91805.1 at /home/alir/.julia/,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3852:6405,cache,cache,6405,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3852,1,['cache'],['cache']
Performance,"alculation that reproduces the issue:. ```julia; using Oceananigans; using CUDA: has_cuda_gpu. arch = has_cuda_gpu() ? GPU() : CPU(); grid_base = RectilinearGrid(arch,; size=(4, 4, 4),; extent = (1, 1, 1),; halo=(4, 4, 4)). bathymetry(x, y) = 1; grid = ImmersedBoundaryGrid(grid_base, GridFittedBottom(bathymetry)). model = NonhydrostaticModel(; grid). @inline function vector_projection_aaa(i, j, k, grid, ϕˣ, ϕᶻ, params); return @inbounds ϕˣ[i,j,k]*params.xdirection + ϕᶻ[i,j,k]*params.zdirection; end. using Oceananigans.AbstractOperations: ∂x, ∂y, ∂z; u, v, w = model.velocities; dudx_tilt = ∂x(u) + 1e-7; dudz_tilt = ∂z(u). using Oceananigans.Grids: Center, Face; dudz_op = KernelFunctionOperation{Center, Center, Face}(vector_projection_aaa, model.grid,; dudx_tilt, dudz_tilt,; (xdirection=0.9,; zdirection=0.1)); dudz = Field(Average(dudz_op, dims=(1,2))); compute!(dudz); ```. This gives me:. ```; ERROR: LoadError: Failed to compile PTX code (ptxas exited with code 255); Invocation arguments: --generate-line-info --verbose --gpu-name sm_70 --output-file /glade/scratch/tomasc/jl_6zqZEpG5Yv.cubin /glade/scratch/tomasc/jl_uD0VONe1Cz.ptx; ptxas /glade/scratch/tomasc/jl_uD0VONe1Cz.ptx, line 3031; error : Entry function '_Z22partial_mapreduce_grid8identity7add_sumv16CartesianIndicesILi3E5TupleI5OneToI5Int64ES3_IS4_ES3_IS4_EEES1_ILi3ES2_IS3_IS4_ES3_IS4_ES3_IS4_EEE3ValILitrueEE13ReshapedArrayI7Float64Li4E8SubArrayIS7_Li3E13CuDeviceArrayIS7_Li3ELi1EES2_I9UnitRangeIS4_ES10_IS4_ES10_IS4_EELinfalseEES2_I27SignedMultiplicativeInverseIS4_ES11_IS4_EEE20ConditionalOperationI6CenterS13_4Face15BinaryOperationIS13_S13_S14_1_S15_IS13_S13_S14_S16_23KernelFunctionOperationIS13_S13_S14_20ImmersedBoundaryGridIS7_8PeriodicS19_7Bounded15RectilinearGridIS7_S19_S19_S20_S7_S7_S7_11OffsetArrayIS7_Li1E12StepRangeLenIS7_14TwicePrecisionIS7_ES24_IS7_ES4_EES22_IS7_Li1ES23_IS7_S24_IS7_ES24_IS7_ES4_EES22_IS7_Li1ES23_IS7_S24_IS7_ES24_IS7_ES4_EEvE16GridFittedBottomIS22_IS7_Li2ES9_IS7_Li2ELi1EEE23CenterImmerse",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3140:1140,Load,LoadError,1140,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3140,1,['Load'],['LoadError']
Performance,"all to the Julia runtime (call to jl_f_getfield); Stacktrace:; [1] overdub at /home/alir/.julia/packages/Cassette/158rp/src/overdub.jl:586; [2] multiple call sites at unknown:0; Stacktrace:; [1] check_ir(::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDA.CUDACompilerParams}, ::LLVM.Module) at /home/alir/.julia/packages/GPUCompiler/4e9CU/src/validation.jl:123; [2] macro expansion at /home/alir/.julia/packages/GPUCompiler/4e9CU/src/driver.jl:241 [inlined]; [3] macro expansion at /home/alir/.julia/packages/TimerOutputs/dVnaw/src/TimerOutput.jl:206 [inlined]; [4] codegen(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDA.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, validate::Bool, only_entry::Bool) at /home/alir/.julia/packages/GPUCompiler/4e9CU/src/driver.jl:239; [5] compile(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDA.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, validate::Bool, only_entry::Bool) at /home/alir/.julia/packages/GPUCompiler/4e9CU/src/driver.jl:39; [6] compile at /home/alir/.julia/packages/GPUCompiler/4e9CU/src/driver.jl:35 [inlined]; [7] _cufunction(::GPUCompiler.FunctionSpec{typeof(Cassette.overdub),Tuple{Cassette.Context{nametype(CUDACtx),KernelAbstractions.CompilerMetadata{KernelAbstractions.NDIteration.StaticSize{(16, 16, 16)},KernelAbstractions.NDIteration.DynamicCheck,Nothing,Nothing,KernelAbstractions.NDIteration.NDRange{3,KernelAbstractions.NDIteration.StaticSize{(1, 1, 16)},KernelAbstractions.NDIteration.StaticSize{(16, 16, 1)},Nothing,Nothing}},Nothing,KernelAbstractions.var""##PassType#253"",Nothing,Cassette.DisableHooks},typeof(Oceananigans.Solvers.gpu_calculate_pressure_right_hand_side!),CUDA.CuDeviceArray{Complex{Float64},3,CUDA.AS.Global},Oceananigans.Solvers.HorizontallyPeriodic,GPU,RegularCartesianGrid{Float64,Periodic,Periodic,Bounded,OffsetArrays.OffsetArray{Float64,1,StepRangeLen{Float64,Base",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/828:5419,optimiz,optimize,5419,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/828,1,['optimiz'],['optimize']
Performance,"ally we can add Lagrangian averaging as an option for this model. Note that this PR is related to https://github.com/CliMA/Oceananigans.jl/pull/3638. The rationale for having this PR in addition to #3638 is that for most LES are doubly periodic, in which case there's not much advantage in having the Lagrangian averaging that's implemented in #3638. In these cases I'd argue the model implemented here is more efficient, so having both types of models is probably desirable. Another note is that, once this PR is merged, it should be very straightforward to implement a scale-_dependent_ version, which works better close to boundaries. The model seems to be working. Here's an animation of an unforced 3D turbulence simulation on a 32³ grid. Left is vorticity and right is the strain rate modulus squared:. https://github.com/CliMA/Oceananigans.jl/assets/13205162/7131a99d-df6c-4883-850d-d4a87988cdb7. Note that the value of the calculated Smag coefficient $c_s$ (which I'm showing at the top of the plots above) is about 0.17, which is very close to the theoretical value obtained by Lilly of 0.16. I'm opening this as a draft PR for now because there are some things that need doing:. - [x] Generalize the filter for stretched grids. For now it assumes a regular grid for simplicity, but it's trivial to generalize.; - [x] Optimize the calculation of the coefficient. At the moment I'm creating four extra fields in order to calculate the Smag coefficient: `LM`, `MM` are 3D fields; and `LM_avg` and `MM_avg` are 1D or 2D. What I'm doing is to first calculate $L_{ij} M_{ij}$ and $M_{ij} M_{ij}$ pointwise, and then `LM_avg` and `MM_avg` receive their averages. We should be able to calculate everything without needing `LM`, `MM` are 3D fields, I just couldn't figure out how yet :); - [ ] Write docs; - [x] Write tests; - [x] Validate that model is working as intended. CC @glwagner @simone-silvestri @xkykai @whitleyv . Feel free to add more things to the to-do list that I may have forgotten.",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3642:2203,Optimiz,Optimize,2203,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3642,1,['Optimiz'],['Optimize']
Performance,amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:877; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:877; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:877; ijl_toplevel_eval_in at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:985; eval at ./boot.jl:385 [inlined]; eval_user_input at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:150; repl_backend_loop at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:246; #start_repl_backend#46 at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:231; start_repl_backend at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:228; _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; #run_repl#59 at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:389; run_repl at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:375; jfptr_run_repl_91805.1 at /home/alir/.julia/juliaup/julia-1.10.5+0.x64.linux.gnu/lib/julia/sys.so (unknown line); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; #1013 at ./client.jl:432; jfptr_YY.1013_82772.1 at /home/alir/.julia/juliaup/julia-1.10.5+0.x64.linux.gnu/lib/julia/sys.so (unknown line); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cac,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3852:6944,cache,cache,6944,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3852,1,['cache'],['cache']
Performance,an_particle_advection.jl:193; step_lagrangian_particles! at /home/alir/atdepth/Oceananigans.jl/src/Models/LagrangianParticleTracking/LagrangianParticleTracking.jl:143 [inlined]; step_lagrangian_particles! at /home/alir/atdepth/Oceananigans.jl/src/Models/HydrostaticFreeSurfaceModels/HydrostaticFreeSurfaceModels.jl:107 [inlined]; #time_step!#8 at /home/alir/atdepth/Oceananigans.jl/src/TimeSteppers/quasi_adams_bashforth_2.jl:124; time_step! at /home/alir/atdepth/Oceananigans.jl/src/TimeSteppers/quasi_adams_bashforth_2.jl:76; unknown function (ip: 0x7c00a0f12fbd); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; top-level scope at /home/alir/atdepth/Oceananigans.jl/particles_error.jl:37; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:925; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:877; ijl_toplevel_eval_in at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:985; eval at ./boot.jl:385 [inlined]; include_string at ./loading.jl:2076; _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; _include at ./loading.jl:2136; include at ./client.jl:489; unknown function (ip: 0x7c00f54ff855); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; jl_apply at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/julia.h:1982 [inlined]; do_call at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:126; eval_value at /cache/build/builder-amdci4-4/julialang/julia-re,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3852:4423,cache,cache,4423,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3852,1,['cache'],['cache']
Performance,"ananigans.jl ecosystem. 1. I see Oceananigans.jl as a general-purpose package for fluid dynamics even though we mostly apply it to ocean problems. With both incompressible and compressible models, Oceananigans.jl would appeal to a larger audience and may be used to investigate a greater range of problems.; 2. One potential use of the `CompressibleModel` is to simulate a compressible ocean (with pressure as a prognostic variable) in which sound waves artificially slowed down for practical purposes. There were some discussions around this idea and @johncmarshall54 might still be interested.; 3. With PR #590, Oceananigans.jl will support distributed parallelism via MPI. While incompressible models (and anelastic models) don't scale that well to many nodes due to the need to solve an elliptic Poission equation globally across all ranks, a compressible model is completely local and might easily scale well on many GPUs. So even if the scalability of incompressible models on many GPUs is dissapointing, we may get a very scalable compressible model almost for free. Actually, the efficiency of the Oceananigans MPI algorithm might be best tested using a compressible model.; 4. Since distributed FFTs aren't generally available on GPUs (CuFFT only goes up to 16 GPUs), CUDA-aware MPI for incompressible models might take some time and effort to support once PR #590 is merged. However, CUDA-aware MPI should work out of the box for compressible models as there are no FFTs to worry about.; 5. Due to the need for a fast pressure solver for incompressible models, we are not considering more general grids beyond the vertically stretched Cartesian grid. The compressible model does not have this limitation and can easily make use of a more general Cartesian grid (stretching in all dimensions).; 6. The incompressible model is limited to a certain number of topologies, particularly on the GPU, due to the pressure solver. A compressible model would work with all possible topologies out of t",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1079:3252,scalab,scalability,3252,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1079,2,['scalab'],"['scalability', 'scalable']"
Performance,"ananigans.jl/test/test_enzyme.jl:285 [inlined]; [64] macro expansion; @ ~/.julia/juliaup/julia-1.10.5+0.aarch64.apple.darwin14/share/julia/stdlib/v1.10/Test/src/Test.jl:1577 [inlined]; [65] top-level scope; @ ~/Projects/Oceananigans.jl/test/test_enzyme.jl:265; [66] include(fname::String); @ Base.MainInclude ./client.jl:489; [67] top-level scope; @ REPL[6]:1; [68] eval; @ ./boot.jl:385 [inlined]; [69] eval_user_input(ast::Any, backend::REPL.REPLBackend, mod::Module); @ REPL ~/.julia/juliaup/julia-1.10.5+0.aarch64.apple.darwin14/share/julia/stdlib/v1.10/REPL/src/REPL.jl:150; [70] repl_backend_loop(backend::REPL.REPLBackend, get_module::Function); @ REPL ~/.julia/juliaup/julia-1.10.5+0.aarch64.apple.darwin14/share/julia/stdlib/v1.10/REPL/src/REPL.jl:246; [71] start_repl_backend(backend::REPL.REPLBackend, consumer::Any; get_module::Function); @ REPL ~/.julia/juliaup/julia-1.10.5+0.aarch64.apple.darwin14/share/julia/stdlib/v1.10/REPL/src/REPL.jl:231; [72] run_repl(repl::REPL.AbstractREPL, consumer::Any; backend_on_current_task::Bool, backend::Any); @ REPL ~/.julia/juliaup/julia-1.10.5+0.aarch64.apple.darwin14/share/julia/stdlib/v1.10/REPL/src/REPL.jl:389; [73] run_repl(repl::REPL.AbstractREPL, consumer::Any); @ REPL ~/.julia/juliaup/julia-1.10.5+0.aarch64.apple.darwin14/share/julia/stdlib/v1.10/REPL/src/REPL.jl:375; [74] (::Base.var""#1013#1015""{Bool, Bool, Bool})(REPL::Module); @ Base ./client.jl:432; [75] #invokelatest#2; @ ./essentials.jl:892 [inlined]; [76] invokelatest; @ ./essentials.jl:889 [inlined]; [77] run_main_repl(interactive::Bool, quiet::Bool, banner::Bool, history_file::Bool, color_set::Bool); @ Base ./client.jl:416; [78] exec_options(opts::Base.JLOptions); @ Base ./client.jl:333; Test Summary: | Error Total Time; Enzyme with CATKEVerticalDiffusivity | 1 1 7m42.2s; ERROR: LoadError: Some tests did not pass: 0 passed, 0 failed, 1 errored, 0 broken.; in expression starting at /Users/gregorywagner/Projects/Oceananigans.jl/test/test_enzyme.jl:264; ```. @wsmoses",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3837#issuecomment-2400785596:153199,Load,LoadError,153199,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3837#issuecomment-2400785596,1,['Load'],['LoadError']
Performance,"ang/julia-release-1-dot-10/src/julia.h:1982 [inlined]; jl_f__call_latest at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/builtins.c:812; #invokelatest#2 at ./essentials.jl:892 [inlined]; invokelatest at ./essentials.jl:889 [inlined]; run_main_repl at ./client.jl:416; exec_options at ./client.jl:333; _start at ./client.jl:552; jfptr__start_82798.1 at /home/alir/.julia/juliaup/julia-1.10.5+0.x64.linux.gnu/lib/julia/sys.so (unknown line); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; jl_apply at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/julia.h:1982 [inlined]; true_main at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/jlapi.c:582; jl_repl_entrypoint at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/jlapi.c:731; main at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/cli/loader_exe.c:58; unknown function (ip: 0x7c00f758ce07); __libc_start_main at /usr/lib/libc.so.6 (unknown line); unknown function (ip: 0x4010b8); Allocations: 67298744 (Pool: 67235612; Big: 63132); GC: 66; fish: Job 1, 'julia --project' terminated by signal SIGSEGV (Address boundary error); ```. GPU illegal memory access:. ```; [ Info: Skipping precompilation since __precompile__(false). Importing Oceananigans [9e8cae18-63c1-5223-a75c-80ca9d6e9a09].; [ Info: Iteration 1...; [ Info: Iteration 2...; [ Info: Iteration 3...; [ Info: Iteration 4...; [ Info: Iteration 5...; [ Info: Iteration 6...; [ Info: Iteration 7...; [ Info: Iteration 8...; [ Info: Iteration 9...; [ Info: Iteration 10...; [ Info: Iteration 11...; [ Info: Iteration 12...; [ Info: Iteration 13...; [ Info: Iteration 14...; [ Info: Iteration 15...; [ Info: Iteration 16...; [ Info: Iteration 17...; [ Info: Iteration 18...; [ Info: Iteration 19...; [ Info: Iteration 20...; [ Info: Iteration 21",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3852:9059,cache,cache,9059,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3852,1,['cache'],['cache']
Performance,"anigans; using Oceananigans.Units. const Nx = 128; const Lx = 500; const Nz = 32; const Lz = 100. topo = (Periodic, Periodic, Bounded); S = 1.5; zF(k) = Lz*(1 + tanh(S * ( (k - 1) / Nz - 1)) / tanh(S)); grid_ver = VerticallyStretchedRectilinearGrid(topology=topo,; architecture = CPU(),; size=(Nx, 1, Nz),; x=(0, Lx), y=(0, 6*Lx/Nx), zF=zF,). model = IncompressibleModel(grid_ver = grid_ver,; ). wizard = TimeStepWizard(Δt=0.1); print_progress(sim) = @info ""iteration: $(sim.model.clock.iteration), time: $(prettytime(sim.model.clock.time))""; simulation = Simulation(model, Δt=wizard,; stop_time=10days,; progress=print_progress,; iteration_interval=2,; stop_iteration=10,; ); #----. fields = model.velocities; simulation.output_writers[:fields] =; NetCDFOutputWriter(model, fields, filepath = ""wenegrat2020.nc"",; schedule = TimeInterval(5minutes),; mode = ""c""). run!(simulation); ```. and the error:. ```julia; ERROR: LoadError: type VerticallyStretchedRectilinearGrid has no field xC; Stacktrace:; [1] getproperty(::VerticallyStretchedRectilinearGrid{Float64,Periodic,Periodic,Bounded,OffsetArrays.OffsetArray{Float64,1,StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}},OffsetArrays.OffsetArray{Float64,1,Array{Float64,1}}}, ::Symbol) at ./Base.jl:33; [2] default_dimensions(::Dict{String,Field{X,Y,Z,OffsetArrays.OffsetArray{Float64,3,Array{Float64,3}},VerticallyStretchedRectilinearGrid{Float64,Periodic,Periodic,Bounded,OffsetArrays.OffsetArray{Float64,1,StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}},OffsetArrays.OffsetArray{Float64,1,Array{Float64,1}}},B} where B where Z where Y where X}, ::VerticallyStretchedRectilinearGrid{Float64,Periodic,Periodic,Bounded,OffsetArrays.OffsetArray{Float64,1,StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}},OffsetArrays.OffsetArray{Float64,1,Array{Float64,1}}}, ::FieldSlicer{Colon,Colon,Colon}) at /home/tomas/repos2/Oceananigans.jl/src/OutputWriters/netcdf",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1466#issuecomment-799790013:1245,Load,LoadError,1245,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1466#issuecomment-799790013,1,['Load'],['LoadError']
Performance,"ar""#968#969""{Base.PkgId}, at::String, pid::Int32; kwopts::@Kwargs{stale_age::Int64, wait::Bool}); @ FileWatching.Pidfile /glade/u/apps/casper/23.10/spack/opt/spack/julia/1.10.2/gcc/7.5.0/apod/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:93; [6] #mkpidlock#6 ; @ /glade/u/apps/casper/23.10/spack/opt/spack/julia/1.10.2/gcc/7.5.0/apod/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:88 [inlined]; [7] trymkpidlock(::Function, ::Vararg{Any}; kwargs::@Kwargs{stale_age::Int64}); @ FileWatching.Pidfile /glade/u/apps/casper/23.10/spack/opt/spack/julia/1.10.2/gcc/7.5.0/apod/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:111; [8] #invokelatest#2; @ ./essentials.jl:894 [inlined]; [9] invokelatest; @ ./essentials.jl:889 [inlined]; [10] maybe_cachefile_lock(f::Base.var""#968#969""{Base.PkgId}, pkg::Base.PkgId, srcpath::String; stale_age::Int64); @ Base ./loading.jl:2983; [11] maybe_cachefile_lock; @ ./loading.jl:2980 [inlined]; [12] _require(pkg::Base.PkgId, env::String); @ Base ./loading.jl:1970; [13] __require_prelocked(uuidkey::Base.PkgId, env::String); @ Base ./loading.jl:1812; [14] #invoke_in_world#3; @ ./essentials.jl:926 [inlined]; [15] invoke_in_world; @ ./essentials.jl:923 [inlined]; [16] _require_prelocked(uuidkey::Base.PkgId, env::String); @ Base ./loading.jl:1803; [17] macro expansion; @ ./loading.jl:1790 [inlined]; [18] macro expansion; @ ./lock.jl:267 [inlined]; [19] __require(into::Module, mod::Symbol); @ Base ./loading.jl:1753; [20] #invoke_in_world#3; @ ./essentials.jl:926 [inlined]; [21] invoke_in_world; @ ./essentials.jl:923 [inlined]; [22] require(into::Module, mod::Symbol); @ Base ./loading.jl:1746; [23] include; @ ./Base.jl:495 [inlined]; [24] include_package_for_output(pkg::Base.PkgId, input::String, depot_path::Vector{String}, dl_load_path::Vector{String}, load_path::Vector{String}, concrete_deps::Vector{Pair{Base.PkgId, UInt128}}, source::String); @ Base ./loading.jl:2222; [25] top-level scope; @ stdin:3; in expression starting at /glade/u/h",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2231635812:1832,load,loading,1832,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2231635812,1,['load'],['loading']
Performance,"argument list;   | Stacktrace:;   | [1] top-level scope;   | @ ~/builds/tartarus-1/clima/oceananigans/src/Coriolis/non_traditional_beta_plane.jl:75;   | [2] include(mod::Module, _path::String);   | @ Base ./Base.jl:419;   | [3] include(x::String);   | @ Oceananigans.Coriolis ~/builds/tartarus-1/clima/oceananigans/src/Coriolis/Coriolis.jl:1;   | [4] top-level scope;   | @ ~/builds/tartarus-1/clima/oceananigans/src/Coriolis/Coriolis.jl:28;   | [5] include(mod::Module, _path::String);   | @ Base ./Base.jl:419;   | [6] include(x::String);   | @ Oceananigans ~/builds/tartarus-1/clima/oceananigans/src/Oceananigans.jl:5;   | [7] top-level scope;   | @ ~/builds/tartarus-1/clima/oceananigans/src/Oceananigans.jl:230;   | [8] include;   | @ ./Base.jl:419 [inlined];   | [9] include_package_for_output(pkg::Base.PkgId, input::String, depot_path::Vector{String}, dl_load_path::Vector{String}, load_path::Vector{String}, concrete_deps::Vector{Pair{Base.PkgId, UInt64}}, source::Nothing);   | @ Base ./loading.jl:1554;   | [10] top-level scope;   | @ stdin:1;   | in expression starting at /var/lib/buildkite-agent/builds/tartarus-1/clima/oceananigans/src/Coriolis/non_traditional_beta_plane.jl:75;   | in expression starting at /var/lib/buildkite-agent/builds/tartarus-1/clima/oceananigans/src/Coriolis/Coriolis.jl:1;   | in expression starting at /var/lib/buildkite-agent/builds/tartarus-1/clima/oceananigans/src/Oceananigans.jl:1;   | in expression starting at stdin:1;   | Stacktrace:;   | [1] pkgerror(msg::String);   | @ Pkg.Types /storage5/buildkite-agent/julia-1.8.2/share/julia/stdlib/v1.8/Pkg/src/Types.jl:67;   | [2] precompile(ctx::Pkg.Types.Context, pkgs::Vector{String}; internal_call::Bool, strict::Bool, warn_loaded::Bool, already_instantiated::Bool, kwargs::Base.Pairs{Symbol, Union{}, Tuple{}, NamedTuple{(), Tuple{}}});   | @ Pkg.API /storage5/buildkite-agent/julia-1.8.2/share/julia/stdlib/v1.8/Pkg/src/API.jl:1432;   | [3] precompile;   | @ /storage5/buildkite-agent/julia-1.8.2/share/",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2877#issuecomment-1414372445:2186,load,loading,2186,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2877#issuecomment-1414372445,1,['load'],['loading']
Performance,"arseArrays`; [10745b16] Statistics `@stdlib/Statistics`; [4607b0f0] SuiteSparse `@stdlib/SuiteSparse`; [fa267f1f] TOML `@stdlib/TOML`; [a4e569a6] Tar `@stdlib/Tar`; [8dfed614] Test `@stdlib/Test`; [cf7118a7] UUIDs `@stdlib/UUIDs`; [4ec0a83e] Unicode `@stdlib/Unicode`; [e66e0078] CompilerSupportLibraries_jll `@stdlib/CompilerSupportLibraries_jll`; [deac9b47] LibCURL_jll `@stdlib/LibCURL_jll`; [29816b5a] LibSSH2_jll `@stdlib/LibSSH2_jll`; [c8ffd9c3] MbedTLS_jll `@stdlib/MbedTLS_jll`; [14a3606d] MozillaCACerts_jll `@stdlib/MozillaCACerts_jll`; [05823500] OpenLibm_jll `@stdlib/OpenLibm_jll`; [efcefdf7] PCRE2_jll `@stdlib/PCRE2_jll`; [83775a58] Zlib_jll `@stdlib/Zlib_jll`; [8e850ede] nghttp2_jll `@stdlib/nghttp2_jll`; [3f19e933] p7zip_jll `@stdlib/p7zip_jll`; Precompiling project...; ✗ RecipesPipeline; ✗ Plots; 0 dependencies successfully precompiled in 7 seconds (205 already precompiled); 2 dependencies errored. To see a full report either run `import Pkg; Pkg.precompile()` or load the packages; Testing Running tests...; ┌ Warning: You appear to be using MPI.jl with the default MPI binary on a cluster.; │ We recommend using the system-provided MPI, see the Configuration section of the MPI.jl docs.; └ @ MPI /g/data/v45/nc3020/.julia/packages/MPI/08SPr/deps/deps.jl:15; [ Info: Oceananigans will use 48 threads; [2023/02/21 06:36:13.652] WARN allowscalar([true]) is deprecated, use `allowscalar() do end` or `@allowscalar` to denote exactly which operations can use scalar operations. -@-> /g/data/v45/nc3020/OC.jl/test/dependencies_for_runtests.jl:71; [2023/02/21 06:36:18.673] WARN allowscalar([true]) is deprecated, use `allowscalar() do end` or `@allowscalar` to denote exactly which operations can use scalar operations. -@-> /g/data/v45/nc3020/OC.jl/test/dependencies_for_runtests.jl:71; [2023/02/21 06:36:18.728] WARN Over-writing registration of the datadep -@-> /g/data/v45/nc3020/.julia/packages/DataDeps/ae6dT/src/registration.jl:15; [2023/02/21 06:36:18.729] WARN Over-writi",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2922#issuecomment-1437515895:10505,load,load,10505,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2922#issuecomment-1437515895,1,['load'],['load']
Performance,"as `err` and call `code_typed(err; interactive = true)` to introspect the erronous code with Cthulhu.jl; Stacktrace:; [1] check_ir(job::GPUCompiler.CompilerJob{GPUCompiler.MetalCompilerTarget, Metal.MetalCompilerParams}, args::LLVM.Module); @ GPUCompiler ~/.julia/packages/GPUCompiler/Cp7sE/src/validation.jl:147; [2] macro expansion; @ ~/.julia/packages/GPUCompiler/Cp7sE/src/driver.jl:440 [inlined]; [3] macro expansion; @ ~/.julia/packages/TimerOutputs/RsWnF/src/TimerOutput.jl:253 [inlined]; [4] macro expansion; @ ~/.julia/packages/GPUCompiler/Cp7sE/src/driver.jl:439 [inlined]; [5] emit_llvm(job::GPUCompiler.CompilerJob; libraries::Bool, toplevel::Bool, optimize::Bool, cleanup::Bool, only_entry::Bool, validate::Bool); @ GPUCompiler ~/.julia/packages/GPUCompiler/Cp7sE/src/utils.jl:89; [6] emit_llvm; @ ~/.julia/packages/GPUCompiler/Cp7sE/src/utils.jl:83 [inlined]; [7] codegen(output::Symbol, job::GPUCompiler.CompilerJob; libraries::Bool, toplevel::Bool, optimize::Bool, cleanup::Bool, strip::Bool, validate::Bool, only_entry::Bool, parent_job::Nothing); @ GPUCompiler ~/.julia/packages/GPUCompiler/Cp7sE/src/driver.jl:129; [8] codegen; @ ~/.julia/packages/GPUCompiler/Cp7sE/src/driver.jl:110 [inlined]; [9] compile(target::Symbol, job::GPUCompiler.CompilerJob; libraries::Bool, toplevel::Bool, optimize::Bool, cleanup::Bool, strip::Bool, validate::Bool, only_entry::Bool); @ GPUCompiler ~/.julia/packages/GPUCompiler/Cp7sE/src/driver.jl:106; [10] compile; @ ~/.julia/packages/GPUCompiler/Cp7sE/src/driver.jl:98 [inlined]; [11] #45; @ ~/.julia/packages/Metal/lnkVP/src/compiler/compilation.jl:57 [inlined]; [12] JuliaContext(f::Metal.var""#45#46""{GPUCompiler.CompilerJob{GPUCompiler.MetalCompilerTarget, Metal.MetalCompilerParams}}); @ GPUCompiler ~/.julia/packages/GPUCompiler/Cp7sE/src/driver.jl:47; [13] compile(job::GPUCompiler.CompilerJob); @ Metal ~/.julia/packages/Metal/lnkVP/src/compiler/compilation.jl:56; [14] actual_compilation(cache::Dict{Any, Any}, src::Core.MethodInstance, wor",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2618#issuecomment-1731573822:36640,optimiz,optimize,36640,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2618#issuecomment-1731573822,1,['optimiz'],['optimize']
Performance,"as created.) You can also apply a plan with a preallocated output array Â by calling mul!(Â, plan, A). (For mul!, however, the input array A must be a complex floating-point; >; > array like the output Â.) You can compute the inverse-transform plan by inv(P) and apply the inverse plan with P \ Â (the inverse plan is cached and reused for subsequent calls to inv or; >; > \), and apply the inverse plan to a pre-allocated output array A with ldiv!(A, P, Â).; >; >; >; > The flags argument is a bitwise-or of FFTW planner flags, defaulting to FFTW.ESTIMATE. e.g. passing FFTW.MEASURE or FFTW.PATIENT will instead spend several seconds (or more) benchmarking; >; > different possible FFT algorithms and picking the fastest one; see the FFTW manual for more information on planner flags. The optional timelimit argument specifies a rough upper bound on; >; > the allowed planning time, in seconds. Passing FFTW.MEASURE or FFTW.PATIENT may cause the input array A to be overwritten with zeros during plan creation.; >; >; >; > plan_fft! is the same as plan_fft but creates a plan that operates in-place on its argument (which must be an array of complex floating-point numbers). plan_ifft and so on are similar; >; > but produce plans that perform the equivalent of the inverse transforms ifft and so on.; >; >; >; > help?> plan_fft!; >; > search: plan_fft! plan_ifft! plan_bfft! plan_fft plan_rfft plan_ifft plan_bfft plan_irfft plan_brfft; >; >; >; > plan_fft!(A [, dims]; flags=FFTW.ESTIMATE, timelimit=Inf); >; >; >; > Same as plan_fft, but operates in-place on A.; >; >; > But I just learned that you can apply the inverse plan with P \ Â which is; > cool!; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/climate-machine/Oceananigans.jl/issues/119#issuecomment-471179127>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AOkIBrJ-I9C04VxTFehp8CvR6NOT2nSeks5vU71agaJpZM4bmrZ0>; > .; >",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/119#issuecomment-471179341:2269,perform,perform,2269,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/119#issuecomment-471179341,1,['perform'],['perform']
Performance,"at the moment for chunked arrays the user can change with `set!`. For the above example of `fts3` which has a chunk size of 4:. ```; julia> fts3[1]; 17×16×10 Field{Face, Center, Center} on LatitudeLongitudeGrid on CPU; ├── grid: 16×16×10 LatitudeLongitudeGrid{Float64, Bounded, Bounded, Bounded} on CPU with 3×3×3 halo and with precomputed metrics; ├── boundary conditions: Nothing; └── data: 23×22×16 OffsetArray(view(::Array{Float64, 4}, :, :, :, 1), -2:20, -2:19, -2:13) with eltype Float64 with indices -2:20×-2:19×-2:13; └── max=2.0, min=2.0, mean=2.0. julia> fts3[7]; ERROR: BoundsError: attempt to access 23×22×16×4 Array{Float64, 4} at index [1:23, 1:22, 1:16, 7]. julia> set!(fts3, 7:10). julia> fts3[7]; 17×16×10 Field{Face, Center, Center} on LatitudeLongitudeGrid on CPU; ├── grid: 16×16×10 LatitudeLongitudeGrid{Float64, Bounded, Bounded, Bounded} on CPU with 3×3×3 halo and with precomputed metrics; ├── boundary conditions: Nothing; └── data: 23×22×16 OffsetArray(view(::Array{Float64, 4}, :, :, :, 1), -2:20, -2:19, -2:13) with eltype Float64 with indices -2:20×-2:19×-2:13; └── max=14.0, min=14.0, mean=14.0. julia> fts3[1]; ERROR: BoundsError: attempt to access 23×22×16×4 Array{Float64, 4} at index [1:23, 1:22, 1:16, -5]; Stacktrace:; ```. So at the moment the chunked field does not ""auto-load"". I am not sure whether it is better to autoload or not.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3233#issuecomment-1694023533:1310,load,load,1310,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3233#issuecomment-1694023533,1,['load'],['load']
Performance,"ata.github.io/netcdf4-python/) but I'd rather not have to use PyCall...; > ; > _Originally posted by @ali-ramadhan in https://github.com/climate-machine/Oceananigans.jl/issues/31#issuecomment-468288154_. Some more comments:; * Right now NetCDF output is much slower than expected. We have asynchronous output writing #137 but spending 2-3 minutes to write out the fields of a 256³ grid is ridiculous.; * Compression doesn't seem to work in NetCDF.jl: See https://github.com/JuliaGeo/NetCDF.jl/issues/87; * We might want to share this output writing feature with CliMA.jl. See https://github.com/climate-machine/CLIMA/issues/114; * @simonbyrne and @charleskawczynski suggested looking at [HDF5.jl](https://github.com/JuliaIO/HDF5.jl). As NetCDF4 is built on HDF5, we should be able to generate valid NetCDF files with it.; * Whatever we end up doing, @christophernhill makes a good point that we should produce portable NetCDF files as this is what all our potential users would expect and want.; * Thinking more long-term @lcw says that IO performance is a hard problem and thinks we are going to want something that plays well on clusters (e.g., something MPI IO based). Some extra features we might want in the short-term:; - [ ] Option to only write out a specific subset of the model state, e.g. surface velocities only, or a vertical temperature slice. This would really speed up IO if you don't need full 3D fields.; - [ ] Select which fields to write out to NetCDF.; - [ ] Include diagnostics in NetCDF output. I believe this will be possible if we resolve the item above as output writing occurs right after diagnostics are run, so diagnostic fields can just be included as one of the fields to write out.; - [ ] Option to create one NetCDF file for each iteration, or to combine all output into one NeCDF file. I think the main point of this issue is that we should decide how to output NetCDF. Packages we could choose from are NCDatasets.jl and HDF5.jl. Or we could go with something else.",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/145:1626,perform,performance,1626,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/145,1,['perform'],['performance']
Performance,"atch/tomasc/jl_uD0VONe1Cz.ptx; Stacktrace:; [1] error(s::String); @ Base ./error.jl:35; [2] compile(job::GPUCompiler.CompilerJob, ctx::LLVM.Context); @ CUDA /glade/work/tomasc/.julia/packages/CUDA/pCcGc/src/compiler/compilation.jl:208; [3] #1032; @ /glade/work/tomasc/.julia/packages/CUDA/pCcGc/src/compiler/compilation.jl:120 [inlined]; [4] JuliaContext(f::CUDA.var""#1032#1033""{GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget, CUDA.CUDACompilerParams}}); @ GPUCompiler /glade/work/tomasc/.julia/packages/GPUCompiler/NVLGB/src/driver.jl:37; [5] compile; @ /glade/work/tomasc/.julia/packages/CUDA/pCcGc/src/compiler/compilation.jl:119 [inlined]; [6] actual_compilation(cache::Dict{Any, Any}, src::Core.MethodInstance, world::UInt64, cfg::GPUCompiler.CompilerConfig{GPUCompiler.PTXCompilerTarget, CUDA.CUDACompilerParams}, compiler::typeof(CUDA.compile), linker::typeof(CUDA.link)); @ GPUCompiler /glade/work/tomasc/.julia/packages/GPUCompiler/NVLGB/src/execution.jl:125; [7] cached_compilation(cache::Dict{Any, Any}, src::Core.MethodInstance, cfg::GPUCompiler.CompilerConfig{GPUCompiler.PTXCompilerTarget, CUDA.CUDACompilerParams}, compiler::Function, linker::Function); @ GPUCompiler /glade/work/tomasc/.julia/packages/GPUCompiler/NVLGB/src/execution.jl:103; [8] macro expansion; @ /glade/work/tomasc/.julia/packages/CUDA/pCcGc/src/compiler/execution.jl:318 [inlined]; [9] macro expansion; @ ./lock.jl:223 [inlined]; [10] cufunction(f::typeof(CUDA.partial_mapreduce_grid), tt::Type{Tuple{typeof(identity), typeof(Base.add_sum), Nothing, CartesianIndices{3, Tuple{Base.OneTo{Int64}, Base.OneTo{Int64}, Base.OneTo{Int64}}}, CartesianIndices{3, Tuple{Base.OneTo{Int64}, Base.OneTo{Int64}, Base.OneTo{Int64}}}, Val{true}, Base.ReshapedArray{Float64, 4, SubArray{Float64, 3, CUDA.CuDeviceArray{Float64, 3, 1}, Tuple{UnitRange{Int64}, UnitRange{Int64}, UnitRange{Int64}}, false}, Tuple{Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64}, Base.MultiplicativeInverses.SignedMultiplicativeIn",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3140:5410,cache,cache,5410,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3140,1,['cache'],['cache']
Performance,atdepth/Oceananigans.jl/src/TimeSteppers/quasi_adams_bashforth_2.jl:76; unknown function (ip: 0x7c00a0f12fbd); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; top-level scope at /home/alir/atdepth/Oceananigans.jl/particles_error.jl:37; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:925; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:877; ijl_toplevel_eval_in at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:985; eval at ./boot.jl:385 [inlined]; include_string at ./loading.jl:2076; _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; _include at ./loading.jl:2136; include at ./client.jl:489; unknown function (ip: 0x7c00f54ff855); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; jl_apply at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/julia.h:1982 [inlined]; do_call at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:126; eval_value at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:223; eval_stmt_value at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:174 [inlined]; eval_body at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:617; jl_interpret_toplevel_thunk at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:775; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3852:4897,load,loading,4897,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3852,1,['load'],['loading']
Performance,"ate_pressure_right_hand_side! at /home/alir/.julia/packages/KernelAbstractions/xslEz/src/macros.jl:80; [7] overdub at /home/alir/.julia/packages/Cassette/158rp/src/overdub.jl:0; Stacktrace:; [1] check_ir(::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDA.CUDACompilerParams}, ::LLVM.Module) at /home/alir/.julia/packages/GPUCompiler/GKp4B/src/validation.jl:123; [2] macro expansion at /home/alir/.julia/packages/GPUCompiler/GKp4B/src/driver.jl:241 [inlined]; [3] macro expansion at /home/alir/.julia/packages/TimerOutputs/dVnaw/src/TimerOutput.jl:206 [inlined]; [4] codegen(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDA.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, validate::Bool, only_entry::Bool) at /home/alir/.julia/packages/GPUCompiler/GKp4B/src/driver.jl:239; [5] compile(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDA.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, validate::Bool, only_entry::Bool) at /home/alir/.julia/packages/GPUCompiler/GKp4B/src/driver.jl:39; [6] compile at /home/alir/.julia/packages/GPUCompiler/GKp4B/src/driver.jl:35 [inlined]; [7] _cufunction(::GPUCompiler.FunctionSpec{typeof(Cassette.overdub),Tuple{Cassette.Context{nametype(CUDACtx),KernelAbstractions.CompilerMetadata{KernelAbstractions.NDIteration.StaticSize{(16, 16, 16)},KernelAbstractions.NDIteration.DynamicCheck,Nothing,Nothing,KernelAbstractions.NDIteration.NDRange{3,KernelAbstractions.NDIteration.StaticSize{(1, 1, 16)},KernelAbstractions.NDIteration.StaticSize{(16, 16, 1)},Nothing,Nothing}},Nothing,KernelAbstractions.var""##PassType#253"",Nothing,Cassette.DisableHooks},typeof(Oceananigans.Solvers.gpu_calculate_pressure_right_hand_side!),CUDA.CuDeviceArray{Complex{Float64},3,CUDA.AS.Global},Oceananigans.Solvers.HorizontallyPeriodic,GPU,RegularCartesianGrid{Float64,Periodic,Periodic,Bounded,OffsetArrays.OffsetArray{Float64,1,StepRangeLen{Float64,Base",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/828#issuecomment-700320323:4328,optimiz,optimize,4328,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/828#issuecomment-700320323,1,['optimiz'],['optimize']
Performance,ations/halo_communication.jl:101 [inlined]; #fill_halo_regions!#37 at /orcd/data/raffaele/001/glwagner/Oceananigans.jl/src/DistributedComputations/halo_communication.jl:90 [inlined]; fill_halo_regions! at /orcd/data/raffaele/001/glwagner/Oceananigans.jl/src/DistributedComputations/halo_communication.jl:87; unknown function (ip: 0x2aaac8ad0ee5); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; jl_apply at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/julia.h:1982 [inlined]; do_call at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:126; eval_value at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:223; eval_stmt_value at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:174 [inlined]; eval_body at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:617; jl_interpret_toplevel_thunk at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:775; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:934; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:877; ijl_toplevel_eval_in at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:985; eval at ./boot.jl:385 [inlined]; include_string at ./loading.jl:2076; _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; _include at ./loading.jl:2136; include at ./Base.jl:495; jfptr_include_46447.1 at /orcd/data/raffaele/001/glwagner/Software/julia-1.10.5/lib/julia/sys.so (unknown line); _jl_invoke at /cache/build/builder-amdci4-4/julialang/juli,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3878:4357,cache,cache,4357,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3878,1,['cache'],['cache']
Performance,"ays.jl:12; &nbsp; | [13] include(::Function, ::Module, ::String) at ./Base.jl:380; &nbsp; | [14] include(::Module, ::String) at ./Base.jl:368; &nbsp; | [15] top-level scope at none:2; &nbsp; | [16] eval at ./boot.jl:347 [inlined]; &nbsp; | [17] eval(::Expr) at ./client.jl:467; &nbsp; | [18] top-level scope at ./none:3; &nbsp; | in expression starting at /storage7/buildkite-agent/.julia-2581/packages/PencilArrays/DTEhf/src/Pencils/Pencils.jl:7; &nbsp; | in expression starting at /storage7/buildkite-agent/.julia-2581/packages/PencilArrays/DTEhf/src/PencilArrays.jl:12; &nbsp; | ERROR: LoadError: Failed to precompile PencilArrays [0e08944d-e94e-41b1-9406-dcf66b6a9d2e] to /storage7/buildkite-agent/.julia-2581/compiled/v1.5/PencilArrays/yKKUy_zV1Ut.ji.; &nbsp; | Stacktrace:; &nbsp; | [1] error(::String) at ./error.jl:33; &nbsp; | [2] compilecache(::Base.PkgId, ::String) at ./loading.jl:1305; &nbsp; | [3] _require(::Base.PkgId) at ./loading.jl:1030; &nbsp; | [4] require(::Base.PkgId) at ./loading.jl:928; &nbsp; | [5] require(::Module, ::Symbol) at ./loading.jl:923; &nbsp; | [6] include(::Function, ::Module, ::String) at ./Base.jl:380; &nbsp; | [7] include(::Module, ::String) at ./Base.jl:368; &nbsp; | [8] top-level scope at none:2; &nbsp; | [9] eval at ./boot.jl:347 [inlined]; &nbsp; | [10] eval(::Expr) at ./client.jl:467; &nbsp; | [11] top-level scope at ./none:3; &nbsp; | in expression starting at /storage7/buildkite-agent/.julia-2581/packages/PencilFFTs/Xwxei/src/PencilFFTs.jl:11; &nbsp; | ERROR: could not load library ""/storage7/buildkite-agent/julia-1.5.4/lib/julia/sys.so""; &nbsp; | /storage7/buildkite-agent/julia-1.5.4/lib/julia/sys.so: ELF load command past end of file; &nbsp; | ERROR: could not load library ""/storage7/buildkite-agent/julia-1.5.4/lib/julia/sys.so""; &nbsp; | /storage7/buildkite-agent/julia-1.5.4/lib/julia/sys.so: ELF load command past end of file; &nbsp; | ERROR: could not load library ""/storage7/buildkite-agent/julia-1.5.4/lib/julia/sys.so""; &nbsp; |",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1662#issuecomment-843325731:5631,load,loading,5631,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1662#issuecomment-843325731,1,['load'],['loading']
Performance,"b8d4d1272c3e7ccbf7d5/lib/./libquadmath.0.dylib' (no such file), '/Applications/Julia-1.7.app/Contents/Resources/julia/lib/julia/libquadmath.0.dylib' (no such file), '/Applications/Julia-1.7.app/Contents/Resources/julia/bin/../lib/libquadmath.0.dylib' (no such file), '/usr/local/lib/libquadmath.0.dylib' (no such file), '/usr/lib/libquadmath.0.dylib' (no such file); Stacktrace:; [1] dlopen(s::String, flags::UInt32; throw_error::Bool); @ Base.Libc.Libdl ./libdl.jl:117; [2] dlopen(s::String, flags::UInt32); @ Base.Libc.Libdl ./libdl.jl:117; [3] macro expansion; @ ~/.julia/packages/JLLWrappers/QpMQW/src/products/library_generators.jl:54 [inlined]; [4] __init__(); @ MPICH_jll ~/.julia/packages/MPICH_jll/dhUyI/src/wrappers/aarch64-apple-darwin-libgfortran5.jl:32; [5] _include_from_serialized(path::String, depmods::Vector{Any}); @ Base ./loading.jl:768; [6] _require_search_from_serialized(pkg::Base.PkgId, sourcepath::String); @ Base ./loading.jl:854; [7] _require(pkg::Base.PkgId); @ Base ./loading.jl:1097; [8] require(uuidkey::Base.PkgId); @ Base ./loading.jl:1013; [9] require(into::Module, mod::Symbol); @ Base ./loading.jl:997; [10] top-level scope; @ ~/.julia/packages/MPI/08SPr/deps/deps.jl:8; [11] include(mod::Module, _path::String); @ Base ./Base.jl:418; [12] include(x::String); @ MPI ~/.julia/packages/MPI/08SPr/src/MPI.jl:1; [13] top-level scope; @ ~/.julia/packages/MPI/08SPr/src/MPI.jl:36; [14] include; @ ./Base.jl:418 [inlined]; [15] include_package_for_output(pkg::Base.PkgId, input::String, depot_path::Vector{String}, dl_load_path::Vector{String}, load_path::Vector{String}, concrete_deps::Vector{Pair{Base.PkgId, UInt64}}, source::String); @ Base ./loading.jl:1318; [16] top-level scope; @ none:1; [17] eval; @ ./boot.jl:373 [inlined]; [18] eval(x::Expr); @ Base.MainInclude ./client.jl:453; [19] top-level scope; @ none:1; during initialization of module MPICH_jll; in expression starting at /Users/sean/.julia/packages/MPI/08SPr/deps/deps.jl:1; ERROR: LoadError: Failed to",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2480:3302,load,loading,3302,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2480,1,['load'],['loading']
Performance,"be good to suppress them somehow. This package might help: https://github.com/JuliaIO/Suppressor.jl. What I see on a CPU:; ```; WARNING: Method definition recurse(Cassette.Context{N, M, T, P, B, H} where H<:Union{Cassette.DisableHooks, Nothing} where B<:Union{Nothing, Base.IdDict{Module, Base.Dict{Symbol, Cassette.BindingMeta}}} where P<:Cassette.AbstractPass where T<:Union{Nothing, Cassette.Tag{N, X, E} where E where X where N<:Cassette.AbstractContextName} where M where N<:Cassette.AbstractContextName, Any...) in module Cassette at /home/alir/.julia/packages/Cassette/IwsFs/src/overdub.jl:550 overwritten in module GPUifyLoops at /home/alir/.julia/packages/Cassette/IwsFs/src/overdub.jl:550.; WARNING: Method definition overdub(Cassette.Context{N, M, T, P, B, H} where H<:Union{Cassette.DisableHooks, Nothing} where B<:Union{Nothing, Base.IdDict{Module, Base.Dict{Symbol, Cassette.BindingMeta}}} where P<:Cassette.AbstractPass where T<:Union{Nothing, Cassette.Tag{N, X, E} where E where X where N<:Cassette.AbstractContextName} where M where N<:Cassette.AbstractContextName, Any...) in module Cassette at /home/alir/.julia/packages/Cassette/IwsFs/src/overdub.jl:537 overwritten in module GPUifyLoops at /home/alir/.julia/packages/Cassette/IwsFs/src/overdub.jl:537.; ERROR: LoadError: Could not find CUDA driver library; Stacktrace:; [1] top-level scope at /home/alir/.julia/packages/CUDAdrv/ADRHQ/src/CUDAdrv.jl:33; [2] top-level scope at none:2; in expression starting at /home/alir/.julia/packages/CUDAdrv/ADRHQ/src/CUDAdrv.jl:27; ┌ Warning: Replacing docs for `Core.nothing :: Union{}` in module `Oceananigans`; └ @ Base.Docs docs/Docs.jl:223; ┌ Warning: Replacing docs for `Core.nothing :: Union{}` in module `Oceananigans`; └ @ Base.Docs docs/Docs.jl:223; ┌ Warning: Replacing docs for `Core.nothing :: Union{}` in module `Oceananigans`; └ @ Base.Docs docs/Docs.jl:223; ┌ Warning: Replacing docs for `Core.nothing :: Union{}` in module `Oceananigans`; └ @ Base.Docs docs/Docs.jl:223; ```",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/366:1518,Load,LoadError,1518,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/366,1,['Load'],['LoadError']
Performance,"belms with Oceananigans?. ```. ✗ Oceananigans; --;   | 105 dependencies successfully precompiled in 112 seconds;   | 1 dependency errored. To see a full report either run `import Pkg; Pkg.precompile()` or load the package;   | Precompiling project...;   | ✗ Oceananigans;   | 0 dependencies successfully precompiled in 22 seconds. 105 already precompiled.;   |  ;   | ERROR: The following 1 direct dependency failed to precompile:;   |  ;   | Oceananigans [9e8cae18-63c1-5223-a75c-80ca9d6e9a09];   |  ;   | Failed to precompile Oceananigans [9e8cae18-63c1-5223-a75c-80ca9d6e9a09] to /storage5/buildkite-agent/.julia-9773/compiled/v1.8/Oceananigans/jl_yggB5x.;   | [NVBLAS] No Gpu available;   | [NVBLAS] NVBLAS_CONFIG_FILE environment variable is NOT set : relying on default config filename 'nvblas.conf';   | [NVBLAS] Cannot open default config file 'nvblas.conf';   | [NVBLAS] Config parsed;   | [NVBLAS] CPU Blas library need to be provided;   | ERROR: LoadError: syntax: missing comma or ) in argument list;   | Stacktrace:;   | [1] top-level scope;   | @ ~/builds/tartarus-1/clima/oceananigans/src/Coriolis/non_traditional_beta_plane.jl:75;   | [2] include(mod::Module, _path::String);   | @ Base ./Base.jl:419;   | [3] include(x::String);   | @ Oceananigans.Coriolis ~/builds/tartarus-1/clima/oceananigans/src/Coriolis/Coriolis.jl:1;   | [4] top-level scope;   | @ ~/builds/tartarus-1/clima/oceananigans/src/Coriolis/Coriolis.jl:28;   | [5] include(mod::Module, _path::String);   | @ Base ./Base.jl:419;   | [6] include(x::String);   | @ Oceananigans ~/builds/tartarus-1/clima/oceananigans/src/Oceananigans.jl:5;   | [7] top-level scope;   | @ ~/builds/tartarus-1/clima/oceananigans/src/Oceananigans.jl:230;   | [8] include;   | @ ./Base.jl:419 [inlined];   | [9] include_package_for_output(pkg::Base.PkgId, input::String, depot_path::Vector{String}, dl_load_path::Vector{String}, load_path::Vector{String}, concrete_deps::Vector{Pair{Base.PkgId, UInt64}}, source::Nothing);   | @ Base ./loadi",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2877#issuecomment-1414372445:1148,Load,LoadError,1148,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2877#issuecomment-1414372445,1,['Load'],['LoadError']
Performance,"bstractOperations; using Oceananigans.TurbulenceClosures; using Oceananigans.Advection: CenteredFourthOrder; using Oceananigans.Utils: TimeInterval; using CUDA. # # Making grid; Nx = 32; Ny = 32; Nz = 30; Δy = 250.0; Δx = 250.0; Δz = 2.5; Lx = Δx*Nx; Ly = Δy*Ny; Lz = Δz*Nz; grid = RectilinearGrid(GPU(), size = (Nx, Ny, Nz),x = (0, Lx), y = (0, Ly), z = (0, Lz)). kappaH = 5e5 # [m4/s]; kappaV = 5e-5 # [m2/s]; vertical_closure = VerticalScalarDiffusivity(ν=kappaV, κ=kappaV); horizontal_closure = HorizontalScalarBiharmonicDiffusivity(κ=kappaH, ν=kappaH); closures = (vertical_closure, horizontal_closure). model = NonhydrostaticModel(advection = CenteredFourthOrder(),; timestepper = :RungeKutta3,; grid = grid,; tracers = (:T, :S),; coriolis = FPlane(f=1e-4),; closure = closures,; ); ; # Running; Δt = 60.; simulation = Simulation(model, Δt=Δt, stop_iteration=0); simulation.stop_iteration += 1; run!(simulation). ```. and get the following error:. ```; ERROR: LoadError: InvalidIRError: compiling kernel #gpu_calculate_Gu!(KernelAbstractions.CompilerMetadata{KernelAbstractions.NDIteration.StaticSize{(32, 32, 30)}, KernelAbstractions.NDIteration.DynamicCheck, Nothing, Nothing, KernelAbstractions.NDIteration.NDRange{3, KernelAbstractions.NDIteration.StaticSize{(2, 2, 30)}, KernelAbstractions.NDIteration.StaticSize{(16, 16, 1)}, Nothing, Nothing}}, OffsetArrays.OffsetArray{Float64, 3, CuDeviceArray{Float64, 3, 1}}, RectilinearGrid{Float64, Periodic, Periodic, Bounded, Float64, Float64, Float64, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, Nothing}, CenteredFourthOrder, FPlane{Float64}, Nothing, Tuple{ScalarBiharmonicDiffusivity{Oceananigans.TurbulenceCl",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2591:1150,Load,LoadError,1150,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2591,1,['Load'],['LoadError']
Performance,build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:925; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:877; ijl_toplevel_eval_in at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:985; eval at ./boot.jl:385 [inlined]; include_string at ./loading.jl:2076; _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; _include at ./loading.jl:2136; include at ./client.jl:489; unknown function (ip: 0x7c00f54ff855); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; jl_apply at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/julia.h:1982 [inlined]; do_call at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:126; eval_value at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:223; eval_stmt_value at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:174 [inlined]; eval_body at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:617; jl_interpret_toplevel_thunk at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:775; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:934; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:877; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:877; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:877; ijl_toplevel_eval_in at /cache/build/builder-amdci4-4/julialang/julia-releas,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3852:5297,cache,cache,5297,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3852,1,['cache'],['cache']
Performance,builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:877; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:877; ijl_toplevel_eval_in at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:985; eval at ./boot.jl:385 [inlined]; eval_user_input at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:150; repl_backend_loop at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:246; #start_repl_backend#46 at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:231; start_repl_backend at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:228; _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; #run_repl#59 at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:389; run_repl at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:375; jfptr_run_repl_91805.1 at /home/alir/.julia/juliaup/julia-1.10.5+0.x64.linux.gnu/lib/julia/sys.so (unknown line); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; #1013 at ./client.jl:432; jfptr_YY.1013_82772.1 at /home/alir/.julia/juliaup/julia-1.10.5+0.x64.linux.gnu/lib/julia/sys.so (unknown line); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; jl_apply at /cache/build/bu,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3852:7053,cache,cache,7053,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3852,1,['cache'],['cache']
Performance,"bulations I also decided to add some simple 1D tests. Here I list the tests but will post a followup comment for each test with a figure or animation. Hopefully together these tests act as a starting point to start believing that the `CompressibleModel` indeed does work as expected. 1. Periodic advection of a square waveform; 2. Inviscid Burgers equation developing a shock; 3. Shock tube problem (Sod, 1978); 4. Rising thermal bubble with entropy and energy (Wicker & Skamarock, 1998); 5. Rising thermal bubble with 3 different gas species (entropy and energy); 6. Density current (Straka et al., 1993); 7. Dry convection. See comments below for movies and eyeball norms. The four dry rising thermal bubble simulations are used for regression testing. # GPU performance benchmarks. Preliminary benchmarks show a 75~80x speedup for large models when comparing a single CPU core to a single Titan V GPU on Tartarus. Not as good as the incompressible model as some of the functions that diagnose temperature and pressure need some optimizing, especially in the case of multiple gases. ```; Compressible model benchmarks ; ┌──────┬──────┬───────────┬───────────┬────────────┬────────────┬────────────┬────────────┬────────────┬────────┐; │ Arch │ Size │ Gases │ ThermoVar │ min │ median │ mean │ max │ memory │ allocs │; ├──────┼──────┼───────────┼───────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────┤; │ CPU │ 32³ │ DryEarth │ Energy │ 37.682 ms │ 38.012 ms │ 37.982 ms │ 38.199 ms │ 646.33 KiB │ 4499 │; │ CPU │ 32³ │ DryEarth │ Entropy │ 32.325 ms │ 32.920 ms │ 32.928 ms │ 33.628 ms │ 646.33 KiB │ 4499 │; │ CPU │ 32³ │ DryEarth3 │ Energy │ 52.473 ms │ 52.815 ms │ 52.896 ms │ 53.413 ms │ 816.50 KiB │ 5635 │; │ CPU │ 32³ │ DryEarth3 │ Entropy │ 69.928 ms │ 70.388 ms │ 70.402 ms │ 71.224 ms │ 816.50 KiB │ 5635 │; │ CPU │ 192³ │ DryEarth │ Energy │ 7.438 s │ 7.438 s │ 7.438 s │ 7.438 s │ 646.33 KiB │ 4499 │; │ CPU │ 192³ │ DryEarth │ Entropy │ 6.501 s │ 6.501 s",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1079:7559,optimiz,optimizing,7559,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1079,1,['optimiz'],['optimizing']
Performance,"but I haven't been able to figure it out. First off, here's the command I'm using to run the script:. ```; $ /glade/work/tomasc/.julia/bin/mpiexecjl --project -n 2 julia --project distributed_nonhydrostatic_model_mpi.jl 8 8 8 1 2 1; ```. Which should (if I understand correctly) create a simulation on an 8x8x8 grid that partitioned into 2 in the y direction. However, I always get this error:. ```; [2022/04/12 16:43:26.454] INFO Setting up distributed nonhydrostatic model with N=(8, 8, 8) grid points and ranks=(1, 2, 1) on rank 0...; [2022/04/12 16:43:26.473] INFO Setting up distributed nonhydrostatic model with N=(8, 8, 8) grid points and ranks=(1, 2, 1) on rank 1...; [2022/04/12 16:43:56.216] INFO Warming up distributed nonhydrostatic model on rank 0...; [2022/04/12 16:43:56.216] INFO Warming up distributed nonhydrostatic model on rank 1...; ERROR: ERROR: LoadError: LoadError: DimensionMismatch(DimensionMismatch(""arrays could not be broadcast to a common size; got a dimension with lengths 8 and 4""); Stacktrace:; [1] ""arrays could not be broadcast to a common size; got a dimension with lengths 8 and 4""); Stacktrace:; [1] _bcs1; @ ./broadcast.jl:501 [inlined]; [2] _bcs1; @ ./broadcast.jl:501 [inlined]; [2] _bcs(shape::_bcs(shape::Tuple{Base.OneTo{Int64}, Base.OneTo{Int64}, Base.OneTo{Int64}}, newshape::Tuple{Base.OneTo{Int64}, Base.OneTo{Int64}, Base.OneTo{Int64}}); @ Base.Broadcast ./broadcast.jl:495; [3] broadcast_shape; @ ./broadcast.jl:489 [inlined]; [4] combine_axes; @ ./broadcast.jl:484 [inlined]; [5] _axes; @ ./broadcast.jl:209 [inlined]; [6] axes; @ ./broadcast.jl:207 [inlined]; [7] _unwrap_pa(bc::Tuple{Base.OneTo{Int64}, Base.OneTo{Int64}, Base.OneTo{Int64}}, newshape::Tuple{Base.OneTo{Int64}, Base.OneTo{Int64}, Base.OneTo{Int64}}); @ Base.Broadcast.Broadcasted{PencilArrays.PencilArrayStyle{3}, Nothing, typeof(/), Tuple{Base.Broadcast.Broadcasted{PencilArrays.PencilArrayStyle{3}, Nothing, typeof(-), Tuple{PencilArrays.PencilArrayBroadcastable{ComplexF64, 3, P",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2433:1215,Load,LoadError,1215,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2433,2,['Load'],['LoadError']
Performance,"butions to tendencies and does not involve communication. The function `time_step_precomputations!` is. ```julia; function time_step_precomputations!(diffusivities, pressures, velocities, tracers, model). fill_halo_regions!(merge(velocities, tracers), model.boundary_conditions.solution, model.architecture,; model.grid, boundary_condition_function_arguments(model)...). calculate_diffusivities!(diffusivities, model.architecture, model.grid, model.closure, model.buoyancy,; velocities, tracers). fill_halo_regions!(diffusivities, model.boundary_conditions.diffusivities, model.architecture, model.grid). @launch(device(model.architecture), config=launch_config(model.grid, :xy),; update_hydrostatic_pressure!(pressures.pHY′, model.grid, model.buoyancy, tracers)). fill_halo_regions!(pressures.pHY′, model.boundary_conditions.pressure, model.architecture, model.grid). return nothing; end; ```. To implement the optimizations discussed in this issue, we need to also consider the calculation of hydrostatic pressure and nonlinear diffusivities to intertwine communication with interior tendency computation. Can this be done abstractly perhaps via some combination of launch configurations and macro specifications to `@loop_xyz`? This would allow us to exert control over the ""region"" of interior source term computation from the ""outside"", while keeping our kernels intact. Notice that the ""pre-computation"" of nonlinear diffusivities and the isolation of the hydrostatic pressure both add communication steps. We should monitor whether these become significantly suboptimal algorithms in the presence of expensive communication. We can easily combine hydrostatic pressure with nonhydrostatic pressure with no loss of performance (probably a small performance increase, in fact). We can also in principle calculate nonlinear diffusivities ""in-line"", though when we tried this previously we were unable to achieve good performance. Also, ""in-line"" calculation of diffusivities makes the application o",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/615#issuecomment-583379290:2321,optimiz,optimizations,2321,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/615#issuecomment-583379290,1,['optimiz'],['optimizations']
Performance,"c). ```; Topologies relative performance (GPU); ┌───────────────┬─────┬────────────────────────────────┬──────────┬──────────┬─────────┐; │ Architectures │ Ns │ Topologies │ slowdown │ memory │ allocs │; ├───────────────┼─────┼────────────────────────────────┼──────────┼──────────┼─────────┤; │ GPU │ 192 │ (Bounded, Bounded, Bounded) │ 2.22277 │ 0.965821 │ 1.70133 │; │ GPU │ 192 │ (Bounded, Bounded, Periodic) │ 1.82308 │ 0.980729 │ 1.496 │; │ GPU │ 192 │ (Bounded, Periodic, Bounded) │ 1.82349 │ 0.984497 │ 1.4736 │; │ GPU │ 192 │ (Bounded, Periodic, Periodic) │ 2.26462 │ 0.989389 │ 1.24018 │; │ GPU │ 192 │ (Periodic, Bounded, Bounded) │ 1.82237 │ 0.984695 │ 1.47467 │; │ GPU │ 192 │ (Periodic, Bounded, Periodic) │ 1.54676 │ 0.992331 │ 1.25653 │; │ GPU │ 192 │ (Periodic, Periodic, Bounded) │ 1.30183 │ 0.99405 │ 1.22631 │; │ GPU │ 192 │ (Periodic, Periodic, Periodic) │ 1.0 │ 1.0 │ 1.0 │; └───────────────┴─────┴────────────────────────────────┴──────────┴──────────┴─────────┘; ```. # Performance vs. main branch. ## Main branch. ```; Topologies benchmarks; ┌───────────────┬─────┬────────────────────────────────┬───────────┬───────────┬───────────┬───────────┬────────────┬────────┐; │ Architectures │ Ns │ Topologies │ min │ median │ mean │ max │ memory │ allocs │; ├───────────────┼─────┼────────────────────────────────┼───────────┼───────────┼───────────┼───────────┼────────────┼────────┤; │ CPU │ 192 │ (Periodic, Bounded, Bounded) │ 1.922 s │ 1.922 s │ 1.967 s │ 2.058 s │ 363.61 KiB │ 2163 │; │ CPU │ 192 │ (Periodic, Periodic, Bounded) │ 2.143 s │ 2.144 s │ 2.145 s │ 2.146 s │ 317.33 KiB │ 1807 │; │ CPU │ 192 │ (Periodic, Periodic, Periodic) │ 1.791 s │ 1.793 s │ 1.793 s │ 1.794 s │ 277.77 KiB │ 1661 │; │ GPU │ 192 │ (Periodic, Bounded, Bounded) │ 32.188 ms │ 37.447 ms │ 36.936 ms │ 37.557 ms │ 985.94 KiB │ 13476 │; │ GPU │ 192 │ (Periodic, Periodic, Bounded) │ 11.051 ms │ 11.114 ms │ 11.148 ms │ 11.533 ms │ 807.44 KiB │ 10746 │; │ GPU │ 192 │ (Periodic, Periodic, Periodi",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1338#issuecomment-773394296:10941,Perform,Performance,10941,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1338#issuecomment-773394296,1,['Perform'],['Performance']
Performance,c/DistributedComputations/halo_communication.jl:193; unknown function (ip: 0x2aaac8aefb2e); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; #fill_halo_regions!#38 at /orcd/data/raffaele/001/glwagner/Oceananigans.jl/src/DistributedComputations/halo_communication.jl:114; fill_halo_regions! at /orcd/data/raffaele/001/glwagner/Oceananigans.jl/src/DistributedComputations/halo_communication.jl:101 [inlined]; #fill_halo_regions!#37 at /orcd/data/raffaele/001/glwagner/Oceananigans.jl/src/DistributedComputations/halo_communication.jl:90 [inlined]; fill_halo_regions! at /orcd/data/raffaele/001/glwagner/Oceananigans.jl/src/DistributedComputations/halo_communication.jl:87; unknown function (ip: 0x2aaac8ad0ee5); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; jl_apply at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/julia.h:1982 [inlined]; do_call at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:126; eval_value at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:223; eval_stmt_value at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:174 [inlined]; eval_body at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:617; jl_interpret_toplevel_thunk at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:775; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:934; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:877; ijl_toplevel_eval_in at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3878:3851,cache,cache,3851,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3878,1,['cache'],['cache']
Performance,c/REPL.jl:246; #start_repl_backend#46 at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:231; start_repl_backend at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:228; _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; #run_repl#59 at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:389; run_repl at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:375; jfptr_run_repl_91805.1 at /home/alir/.julia/juliaup/julia-1.10.5+0.x64.linux.gnu/lib/julia/sys.so (unknown line); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; #1013 at ./client.jl:432; jfptr_YY.1013_82772.1 at /home/alir/.julia/juliaup/julia-1.10.5+0.x64.linux.gnu/lib/julia/sys.so (unknown line); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; jl_apply at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/julia.h:1982 [inlined]; jl_f__call_latest at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/builtins.c:812; #invokelatest#2 at ./essentials.jl:892 [inlined]; invokelatest at ./essentials.jl:889 [inlined]; run_main_repl at ./client.jl:416; exec_options at ./client.jl:333; _start at ./client.jl:552; jfptr__start_82798.1 at /home/alir/.julia/juliaup/julia-1.10.5+0.x64.linux.gnu/lib/julia/sys.so (unknown line); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:28,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3852:7624,cache,cache,7624,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3852,1,['cache'],['cache']
Performance,c:617; jl_interpret_toplevel_thunk at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:775; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:934; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:877; ijl_toplevel_eval_in at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:985; eval at ./boot.jl:385 [inlined]; include_string at ./loading.jl:2076; _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; _include at ./loading.jl:2136; include at ./Base.jl:495; jfptr_include_46447.1 at /orcd/data/raffaele/001/glwagner/Software/julia-1.10.5/lib/julia/sys.so (unknown line); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; exec_options at ./client.jl:318; _start at ./client.jl:552; jfptr__start_82798.1 at /orcd/data/raffaele/001/glwagner/Software/julia-1.10.5/lib/julia/sys.so (unknown line); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; jl_apply at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/julia.h:1982 [inlined]; true_main at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/jlapi.c:582; jl_repl_entrypoint at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/jlapi.c:731; main at julia (unknown line); __libc_start_main at /lib64/libc.so.6 (unknown line); unknown function (ip: 0x4010b8); Allocations: 26236174 (Pool: 26209699; Big: 26475); GC: 35; ```. I'll test CPU then try to see if this situation is tested.,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3878:5447,cache,cache,5447,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3878,6,['cache'],['cache']
Performance,"cache; @ ./loading.jl:2340 [inlined]; [4] (::Base.var""#968#969""{Base.PkgId})(); @ Base ./loading.jl:1974; [5] mkpidlock(f::Base.var""#968#969""{Base.PkgId}, at::String, pid::Int32; kwopts::@Kwargs{stale_age::Int64, wait::Bool}); @ FileWatching.Pidfile /glade/u/apps/casper/23.10/spack/opt/spack/julia/1.10.2/gcc/7.5.0/apod/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:93; [6] #mkpidlock#6 ; @ /glade/u/apps/casper/23.10/spack/opt/spack/julia/1.10.2/gcc/7.5.0/apod/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:88 [inlined]; [7] trymkpidlock(::Function, ::Vararg{Any}; kwargs::@Kwargs{stale_age::Int64}); @ FileWatching.Pidfile /glade/u/apps/casper/23.10/spack/opt/spack/julia/1.10.2/gcc/7.5.0/apod/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:111; [8] #invokelatest#2; @ ./essentials.jl:894 [inlined]; [9] invokelatest; @ ./essentials.jl:889 [inlined]; [10] maybe_cachefile_lock(f::Base.var""#968#969""{Base.PkgId}, pkg::Base.PkgId, srcpath::String; stale_age::Int64); @ Base ./loading.jl:2983; [11] maybe_cachefile_lock; @ ./loading.jl:2980 [inlined]; [12] _require(pkg::Base.PkgId, env::String); @ Base ./loading.jl:1970; [13] __require_prelocked(uuidkey::Base.PkgId, env::String); @ Base ./loading.jl:1812; [14] #invoke_in_world#3; @ ./essentials.jl:926 [inlined]; [15] invoke_in_world; @ ./essentials.jl:923 [inlined]; [16] _require_prelocked(uuidkey::Base.PkgId, env::String); @ Base ./loading.jl:1803; [17] macro expansion; @ ./loading.jl:1790 [inlined]; [18] macro expansion; @ ./lock.jl:267 [inlined]; [19] __require(into::Module, mod::Symbol); @ Base ./loading.jl:1753; [20] #invoke_in_world#3; @ ./essentials.jl:926 [inlined]; [21] invoke_in_world; @ ./essentials.jl:923 [inlined]; [22] require(into::Module, mod::Symbol); @ Base ./loading.jl:1746; [23] include; @ ./Base.jl:495 [inlined]; [24] include_package_for_output(pkg::Base.PkgId, input::String, depot_path::Vector{String}, dl_load_path::Vector{String}, load_path::Vector{String}, concrete_deps::Vector{Pair{Base.P",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2231635812:1703,load,loading,1703,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2231635812,1,['load'],['loading']
Performance,"casper/23.10/spack/opt/spack/julia/1.10.2/gcc/7.5.0/apod/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:88 [inlined]; [7] trymkpidlock(::Function, ::Vararg{Any}; kwargs::@Kwargs{stale_age::Int64}); @ FileWatching.Pidfile /glade/u/apps/casper/23.10/spack/opt/spack/julia/1.10.2/gcc/7.5.0/apod/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:111; [8] #invokelatest#2; @ ./essentials.jl:894 [inlined]; [9] invokelatest; @ ./essentials.jl:889 [inlined]; [10] maybe_cachefile_lock(f::Base.var""#968#969""{Base.PkgId}, pkg::Base.PkgId, srcpath::String; stale_age::Int64); @ Base ./loading.jl:2983; [11] maybe_cachefile_lock; @ ./loading.jl:2980 [inlined]; [12] _require(pkg::Base.PkgId, env::String); @ Base ./loading.jl:1970; [13] __require_prelocked(uuidkey::Base.PkgId, env::String); @ Base ./loading.jl:1812; [14] #invoke_in_world#3; @ ./essentials.jl:926 [inlined]; [15] invoke_in_world; @ ./essentials.jl:923 [inlined]; [16] _require_prelocked(uuidkey::Base.PkgId, env::String); @ Base ./loading.jl:1803; [17] macro expansion; @ ./loading.jl:1790 [inlined]; [18] macro expansion; @ ./lock.jl:267 [inlined]; [19] __require(into::Module, mod::Symbol); @ Base ./loading.jl:1753; [20] #invoke_in_world#3; @ ./essentials.jl:926 [inlined]; [21] invoke_in_world; @ ./essentials.jl:923 [inlined]; [22] require(into::Module, mod::Symbol); @ Base ./loading.jl:1746; [23] include; @ ./Base.jl:495 [inlined]; [24] include_package_for_output(pkg::Base.PkgId, input::String, depot_path::Vector{String}, dl_load_path::Vector{String}, load_path::Vector{String}, concrete_deps::Vector{Pair{Base.PkgId, UInt128}}, source::String); @ Base ./loading.jl:2222; [25] top-level scope; @ stdin:3; in expression starting at /glade/u/home/knudsenl/.julia/packages/CUDA/Tl08O/src/CUDA.jl:1; in expression starting at stdin:3; ERROR: LoadError: Failed to precompile CUDA [052768ef-5323-5732-b1bb-66c8b64840ba] to ""/glade/u/home/knudsenl/.julia/compiled/v1.10/CUDA/jl_zRopeZ"".; Stacktrace:; [1] error(s::String); @ Base ./",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2231635812:2116,load,loading,2116,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2231635812,1,['load'],['loading']
Performance,"casper/23.10/spack/opt/spack/julia/1.10.2/gcc/7.5.0/apod/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:88 [inlined]; [7] trymkpidlock(::Function, ::Vararg{Any}; kwargs::@Kwargs{stale_age::Int64}); @ FileWatching.Pidfile /glade/u/apps/casper/23.10/spack/opt/spack/julia/1.10.2/gcc/7.5.0/apod/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:111; [8] #invokelatest#2; @ ./essentials.jl:894 [inlined]; [9] invokelatest; @ ./essentials.jl:889 [inlined]; [10] maybe_cachefile_lock(f::Base.var""#968#969""{Base.PkgId}, pkg::Base.PkgId, srcpath::String; stale_age::Int64); @ Base ./loading.jl:2983; [11] maybe_cachefile_lock; @ ./loading.jl:2980 [inlined]; [12] _require(pkg::Base.PkgId, env::String); @ Base ./loading.jl:1970; [13] __require_prelocked(uuidkey::Base.PkgId, env::String); @ Base ./loading.jl:1812; [14] #invoke_in_world#3; @ ./essentials.jl:926 [inlined]; [15] invoke_in_world; @ ./essentials.jl:923 [inlined]; [16] _require_prelocked(uuidkey::Base.PkgId, env::String); @ Base ./loading.jl:1803; [17] macro expansion; @ ./loading.jl:1790 [inlined]; [18] macro expansion; @ ./lock.jl:267 [inlined]; [19] __require(into::Module, mod::Symbol); @ Base ./loading.jl:1753; [20] #invoke_in_world#3; @ ./essentials.jl:926 [inlined]; [21] invoke_in_world; @ ./essentials.jl:923 [inlined]; [22] require(into::Module, mod::Symbol); @ Base ./loading.jl:1746; [23] include; @ ./Base.jl:495 [inlined]; [24] include_package_for_output(pkg::Base.PkgId, input::String, depot_path::Vector{String}, dl_load_path::Vector{String}, load_path::Vector{String}, concrete_deps::Vector{Pair{Base.PkgId, UInt128}}, source::String); @ Base ./loading.jl:2222; [25] top-level scope; @ stdin:3; in expression starting at /glade/u/home/knudsenl/.julia/packages/Oceananigans/M82LU/src/Oceananigans.jl:1; in expression starting at stdin:3; ERROR: LoadError: Failed to precompile Oceananigans [9e8cae18-63c1-5223-a75c-80ca9d6e9a09] to ""/glade/u/home/knudsenl/.julia/compiled/v1.10/Oceananigans/jl_k7YOZN"".; Stacktrace:;",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2231635812:4697,load,loading,4697,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2231635812,1,['load'],['loading']
Performance,cation.jl:114; fill_halo_regions! at /orcd/data/raffaele/001/glwagner/Oceananigans.jl/src/DistributedComputations/halo_communication.jl:101 [inlined]; #fill_halo_regions!#37 at /orcd/data/raffaele/001/glwagner/Oceananigans.jl/src/DistributedComputations/halo_communication.jl:90 [inlined]; fill_halo_regions! at /orcd/data/raffaele/001/glwagner/Oceananigans.jl/src/DistributedComputations/halo_communication.jl:87; unknown function (ip: 0x2aaac8ad0ee5); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; jl_apply at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/julia.h:1982 [inlined]; do_call at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:126; eval_value at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:223; eval_stmt_value at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:174 [inlined]; eval_body at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:617; jl_interpret_toplevel_thunk at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:775; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:934; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:877; ijl_toplevel_eval_in at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:985; eval at ./boot.jl:385 [inlined]; include_string at ./loading.jl:2076; _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; _include at ./loading.jl:2136; include at ./Base.jl:495; jfptr_include_46447.1 at /orcd/data/raffaele/001/glwagner/Softwa,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3878:4248,cache,cache,4248,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3878,1,['cache'],['cache']
Performance,"cc/7.5.0/apod/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:88 [inlined]; [7] trymkpidlock(::Function, ::Vararg{Any}; kwargs::@Kwargs{stale_age::Int64}); @ FileWatching.Pidfile /glade/u/apps/casper/23.10/spack/opt/spack/julia/1.10.2/gcc/7.5.0/apod/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:111; [8] #invokelatest#2; @ ./essentials.jl:894 [inlined]; [9] invokelatest; @ ./essentials.jl:889 [inlined]; [10] maybe_cachefile_lock(f::Base.var""#968#969""{Base.PkgId}, pkg::Base.PkgId, srcpath::String; stale_age::Int64); @ Base ./loading.jl:2983; [11] maybe_cachefile_lock; @ ./loading.jl:2980 [inlined]; [12] _require(pkg::Base.PkgId, env::String); @ Base ./loading.jl:1970; [13] __require_prelocked(uuidkey::Base.PkgId, env::String); @ Base ./loading.jl:1812; [14] #invoke_in_world#3; @ ./essentials.jl:926 [inlined]; [15] invoke_in_world; @ ./essentials.jl:923 [inlined]; [16] _require_prelocked(uuidkey::Base.PkgId, env::String); @ Base ./loading.jl:1803; [17] macro expansion; @ ./loading.jl:1790 [inlined]; [18] macro expansion; @ ./lock.jl:267 [inlined]; [19] __require(into::Module, mod::Symbol); @ Base ./loading.jl:1753; [20] #invoke_in_world#3; @ ./essentials.jl:926 [inlined]; [21] invoke_in_world; @ ./essentials.jl:923 [inlined]; [22] require(into::Module, mod::Symbol); @ Base ./loading.jl:1746; [23] include; @ ./Base.jl:495 [inlined]; [24] include_package_for_output(pkg::Base.PkgId, input::String, depot_path::Vector{String}, dl_load_path::Vector{String}, load_path::Vector{String}, concrete_deps::Vector{Pair{Base.PkgId, UInt128}}, source::String); @ Base ./loading.jl:2222; [25] top-level scope; @ stdin:3; in expression starting at /glade/u/home/knudsenl/.julia/packages/CUDA/Tl08O/src/CUDA.jl:1; in expression starting at stdin:3; ERROR: LoadError: Failed to precompile CUDA [052768ef-5323-5732-b1bb-66c8b64840ba] to ""/glade/u/home/knudsenl/.julia/compiled/v1.10/CUDA/jl_zRopeZ"".; Stacktrace:; [1] error(s::String); @ Base ./error.jl:35; [2] compilecache(pkg::Base.Pkg",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2231635812:2159,load,loading,2159,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2231635812,1,['load'],['loading']
Performance,"cc/7.5.0/apod/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:88 [inlined]; [7] trymkpidlock(::Function, ::Vararg{Any}; kwargs::@Kwargs{stale_age::Int64}); @ FileWatching.Pidfile /glade/u/apps/casper/23.10/spack/opt/spack/julia/1.10.2/gcc/7.5.0/apod/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:111; [8] #invokelatest#2; @ ./essentials.jl:894 [inlined]; [9] invokelatest; @ ./essentials.jl:889 [inlined]; [10] maybe_cachefile_lock(f::Base.var""#968#969""{Base.PkgId}, pkg::Base.PkgId, srcpath::String; stale_age::Int64); @ Base ./loading.jl:2983; [11] maybe_cachefile_lock; @ ./loading.jl:2980 [inlined]; [12] _require(pkg::Base.PkgId, env::String); @ Base ./loading.jl:1970; [13] __require_prelocked(uuidkey::Base.PkgId, env::String); @ Base ./loading.jl:1812; [14] #invoke_in_world#3; @ ./essentials.jl:926 [inlined]; [15] invoke_in_world; @ ./essentials.jl:923 [inlined]; [16] _require_prelocked(uuidkey::Base.PkgId, env::String); @ Base ./loading.jl:1803; [17] macro expansion; @ ./loading.jl:1790 [inlined]; [18] macro expansion; @ ./lock.jl:267 [inlined]; [19] __require(into::Module, mod::Symbol); @ Base ./loading.jl:1753; [20] #invoke_in_world#3; @ ./essentials.jl:926 [inlined]; [21] invoke_in_world; @ ./essentials.jl:923 [inlined]; [22] require(into::Module, mod::Symbol); @ Base ./loading.jl:1746; [23] include; @ ./Base.jl:495 [inlined]; [24] include_package_for_output(pkg::Base.PkgId, input::String, depot_path::Vector{String}, dl_load_path::Vector{String}, load_path::Vector{String}, concrete_deps::Vector{Pair{Base.PkgId, UInt128}}, source::String); @ Base ./loading.jl:2222; [25] top-level scope; @ stdin:3; in expression starting at /glade/u/home/knudsenl/.julia/packages/Oceananigans/M82LU/src/Oceananigans.jl:1; in expression starting at stdin:3; ERROR: LoadError: Failed to precompile Oceananigans [9e8cae18-63c1-5223-a75c-80ca9d6e9a09] to ""/glade/u/home/knudsenl/.julia/compiled/v1.10/Oceananigans/jl_k7YOZN"".; Stacktrace:; [1] error(s::String); @ Base ./error.jl:35",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2231635812:4740,load,loading,4740,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2231635812,1,['load'],['loading']
Performance,"ce to provide but I can describe the problem as I've seen it. Basically, very rare, subtle irregularities have been observed on the GPU when using `HydrostaticFreeSurfaceModel` in a `Periodic, Bounded, Bounded` configuration. I think that it is possible the main issue is an interaction (a read-write race condition) associated with both impenetrable boundary conditions and periodic boundary conditions that affects the 8 corner points. The race condition affects model trajectories via the Coriolis force (which is the only term as far as I know that touches the 8 ""corner"" points affected by this race condition). Because the race condition only manifests when a `Coriolis` or `VectorInvariant` stencil touches corner points, it may not affect _most_ `Periodic, Periodic, Bounded` models, which could explain why we haven't caught it. The reason it doesn't affect those models is because this race condition would only affect the corner points of `w`, which are not touched when using an `FPlane` Coriolis model. However, it's possible (I'm not sure) that the race condition could affect models using `NonTraditionalFPlane` in `Periodic, Periodic, Bounded` configurations. More generally, it will also affect models that are bounded in the `y`-direction, because in those models the corner points of the `y`-velocity are affected and also invoked when using `FPlane` or `BetaPlane` coriolis. That's as much as I know. It's very hard to gather information about this bug because it's so rare are subtle. In other words, only one grid point among 10,000 iterations might be affected, and the errors induced are very small. To find this issue, we have to run tens of thousands of iterations of identical models on the GPU, and then compare some statistic of the model (ideally the entire velocity field, but @sandreza has gotten away just comparing something like `[maximum(abs, u), maximum(abs, v), maximum(abs, w)]`). If there's no race condition, identical models should produce identical results.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1985#issuecomment-921143865:1330,race condition,race condition,1330,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1985#issuecomment-921143865,2,['race condition'],['race condition']
Performance,"ceananigans/s0kF6/src/models.jl:186; [21] (::getfield(Core, Symbol(""#kw#Type"")))(::NamedTuple{(:float_type, :grid, :closure, :architecture),Tuple{DataType,RegularCartesianGrid{Float64,StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}},ConstantIsotropicDiffusivity{Float64},GPU}}, ::Type{Model}) at ./none:0; [22] #BasicModel#23(::Tuple{Int64,Int64,Int64}, ::Tuple{Int64,Int64,Int64}, ::Float64, ::Float64, ::Type, ::Base.Iterators.Pairs{Symbol,GPU,Tuple{Symbol},NamedTuple{(:architecture,),Tuple{GPU}}}, ::typeof(BasicModel)) at /home/tomaschor/.julia/packages/Oceananigans/s0kF6/src/models.jl:125; [23] (::getfield(Oceananigans, Symbol(""#kw##BasicModel"")))(::NamedTuple{(:N, :L, :architecture, :ν, :κ),Tuple{Tuple{Int64,Int64,Int64},Tuple{Int64,Int64,Int64},GPU,Float64,Float64}}, ::typeof(BasicModel)) at ./none:0; [24] top-level scope at /home/tomaschor/oceantest/test1.jl:5; [25] include at ./boot.jl:328 [inlined]; [26] include_relative(::Module, ::String) at ./loading.jl:1094; [27] include(::Module, ::String) at ./Base.jl:31; [28] include(::String) at ./client.jl:431; [29] top-level scope at REPL[1]:1; in expression starting at /home/tomaschor/oceantest/test1.jl:5; ```. Here's the output of `nvidia-smi`:; ```; +-----------------------------------------------------------------------------+; | NVIDIA-SMI 390.129 Driver Version: 390.129 |; |-------------------------------+----------------------+----------------------+; | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |; | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |; |===============================+======================+======================|; | 0 NVS 310 Off | 00000000:03:00.0 N/A | N/A |; | 30% 40C P8 N/A / N/A | 634MiB / 962MiB | N/A Default |; +-------------------------------+----------------------+----------------------+; ; +-----------------------------------------------------------------------------+; | Processes: GPU Memory |; | GPU PID Type Process nam",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/451:4405,load,loading,4405,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/451,1,['load'],['loading']
Performance,"cf94725c20106029e4/src/Models/HydrostaticFreeSurfaceModels/HydrostaticFreeSurfaceModels.jl#L92-L96) should return also the barotropic velocities when using a split-explicit free surface. ***(3)***. I would like to flatten out the design of the `SplitExplicitFreeSurface` by; - removing the [`SplitExplicitState`](https://github.com/CliMA/Oceananigans.jl/blob/d66ed9b8b7c4def36260fccf94725c20106029e4/src/Models/HydrostaticFreeSurfaceModels/split_explicit_free_surface.jl#L136-L161), and have the barotropic velocities and the mean fields part of the main `free_surface` type and all the remaining fields required for specific timesteppers in the [`timestepper`](https://github.com/CliMA/Oceananigans.jl/blob/d66ed9b8b7c4def36260fccf94725c20106029e4/src/Models/HydrostaticFreeSurfaceModels/split_explicit_free_surface.jl#L259-L260) which at the moment is very simple but can be redesigned to contain all time-stepping specific information; - removing [`SplitExplicitSettings`](https://github.com/CliMA/Oceananigans.jl/blob/d66ed9b8b7c4def36260fccf94725c20106029e4/src/Models/HydrostaticFreeSurfaceModels/split_explicit_free_surface.jl#L253-L256) (I don't know why there is a `settings_kwargs` there but it looks odd and it's probably a sign that this type is not well designed). This leads to a `SplitExplicitFreeSurface` which will look like; ```julia; struct SplitExplicitFreeSurface{H, U, M, FT, K , S, T} <: AbstractFreeSurface{H, FT}; η :: H; barotropic_velocities :: U # A namedtuple with U, V ; filtered_state :: M # A namedtuple with η, U, V averaged throughout the substepping; gravitational_acceleration :: FT; kernel_parameters :: K; substepping :: S # Either `FixedSubstepNumber` or `FixedTimeStepSize`; timestepper :: T # redesigned to contain all auxiliary field and settings necessary to the particular timestepping; end; ```; These changes will not affect the performance nor the functioning of the algorithm, and (if there are none required) should not even affect the user interface.",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3873:3428,perform,performance,3428,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3873,1,['perform'],['performance']
Performance,che/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:877; ijl_toplevel_eval_in at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:985; eval at ./boot.jl:385 [inlined]; include_string at ./loading.jl:2076; _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; _include at ./loading.jl:2136; include at ./client.jl:489; unknown function (ip: 0x7c00f54ff855); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; jl_apply at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/julia.h:1982 [inlined]; do_call at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:126; eval_value at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:223; eval_stmt_value at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:174 [inlined]; eval_body at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:617; jl_interpret_toplevel_thunk at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:775; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:934; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:877; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:877; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:877; ijl_toplevel_eval_in at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:985; eval at ./boot.jl:385 [inlined]; eval_user_input at /cache/build/builder-,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3852:5397,cache,cache,5397,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3852,1,['cache'],['cache']
Performance,"ckages/GPUArrays/PkHCM/src/GPUArrays.jl:25; ERROR: LoadError: Failed to precompile GPUArrays [0c68f7d7-f131-5f86-a1c3-88cf8149b2d7] to /Users/truedichotomy/.julia/compiled/v1.5/GPUArrays/v5u0T_IyCmP.ji.; Stacktrace:; [1] error(::String) at ./error.jl:33; [2] compilecache(::Base.PkgId, ::String) at ./loading.jl:1290; [3] _require(::Base.PkgId) at ./loading.jl:1030; [4] require(::Base.PkgId) at ./loading.jl:928; [5] require(::Module, ::Symbol) at ./loading.jl:923; [6] include(::Function, ::Module, ::String) at ./Base.jl:380; [7] include(::Module, ::String) at ./Base.jl:368; [8] top-level scope at none:2; [9] eval at ./boot.jl:331 [inlined]; [10] eval(::Expr) at ./client.jl:467; [11] top-level scope at ./none:3; in expression starting at /Users/truedichotomy/.julia/packages/CUDA/7vLVC/src/CUDA.jl:5; ERROR: LoadError: Failed to precompile CUDA [052768ef-5323-5732-b1bb-66c8b64840ba] to /Users/truedichotomy/.julia/compiled/v1.5/CUDA/oWw5k_IyCmP.ji.; Stacktrace:; [1] error(::String) at ./error.jl:33; [2] compilecache(::Base.PkgId, ::String) at ./loading.jl:1290; [3] _require(::Base.PkgId) at ./loading.jl:1030; [4] require(::Base.PkgId) at ./loading.jl:928; [5] require(::Module, ::Symbol) at ./loading.jl:923; [6] include(::Function, ::Module, ::String) at ./Base.jl:380; [7] include(::Module, ::String) at ./Base.jl:368; [8] top-level scope at none:2; [9] eval at ./boot.jl:331 [inlined]; [10] eval(::Expr) at ./client.jl:467; [11] top-level scope at ./none:3; in expression starting at /Users/truedichotomy/.julia/packages/Oceananigans/LW3v4/src/Oceananigans.jl:70; ERROR: Failed to precompile Oceananigans [9e8cae18-63c1-5223-a75c-80ca9d6e9a09] to /Users/truedichotomy/.julia/compiled/v1.5/Oceananigans/hU93i_IyCmP.ji.; Stacktrace:; [1] error(::String) at ./error.jl:33; [2] compilecache(::Base.PkgId, ::String) at ./loading.jl:1290; [3] _require(::Base.PkgId) at ./loading.jl:1030; [4] require(::Base.PkgId) at ./loading.jl:928; [5] require(::Module, ::Symbol) at ./loading.jl:923; ```",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/854:2163,load,loading,2163,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/854,8,['load'],['loading']
Performance,"ckend(backend::REPL.REPLBackend, consumer::Any; get_module::Function); @ REPL /rds/user/js2430/hpc-work/julia-1.9.2/share/julia/stdlib/v1.9/REPL/src/REPL.jl:234; [24] run_repl(repl::REPL.AbstractREPL, consumer::Any; backend_on_current_task::Bool, backend::Any); @ REPL /rds/user/js2430/hpc-work/julia-1.9.2/share/julia/stdlib/v1.9/REPL/src/REPL.jl:379; [25] run_repl(repl::REPL.AbstractREPL, consumer::Any); @ REPL /rds/user/js2430/hpc-work/julia-1.9.2/share/julia/stdlib/v1.9/REPL/src/REPL.jl:365; [26] (::Base.var""#1017#1019""{Bool, Bool, Bool})(REPL::Module); @ Base ./client.jl:421; [27] #invokelatest#2; @ ./essentials.jl:816 [inlined]; [28] invokelatest; @ ./essentials.jl:813 [inlined]; [29] run_main_repl(interactive::Bool, quiet::Bool, banner::Bool, history_file::Bool, color_set::Bool); @ Base ./client.jl:405; [30] exec_options(opts::Base.JLOptions); @ Base ./client.jl:322; [31] _start(); @ Base ./client.jl:522; LoadError: CUDA error: an illegal memory access was encountered (code 700, ERROR_ILLEGAL_ADDRESS); in expression starting at /rds/user/js2430/hpc-work/Eady/eady.jl:133; > (stacktrace); (user); CUDA; + throw_api_error ~/.julia/packages/CUDA/35NC6/lib/cudadrv/libcuda.jl:27; + [inlined]; CUDA; + cuOccupancyMaxPotentialBlockSize ~/.julia/packages/CUDA/35NC6/lib/utils/call.jl:26; + #launch_configuration#875 ~/.julia/packages/CUDA/35NC6/lib/cudadrv/occupancy.jl:63; + [inlined]; v CUDA; + cuOccupancyMaxPotentialBlockSize ~/.julia/packages/CUDA/35NC6/lib/utils/call.jl:26; + #launch_configuration#875 ~/.julia/packages/CUDA/35NC6/lib/cudadrv/occupancy.jl:63; + [inlined]; CUDA; + #mapreducedim!#1119 ~/.julia/packages/CUDA/35NC6/src/mapreduce.jl:236; + [inlined]; GPUArrays; > + #_mapreduce#31 ~/.julia/packages/GPUArrays/5XhED/src/host/mapreduce.jl:69; v + [inlined]; GPUArrays; + #_mapreduce#31 ~/.julia/packages/GPUArrays/5XhED/src/host/mapreduce.jl:69; + [inlined]; Oceananigans.Solvers; + solve! ~/.julia/packages/Oceananigans/mwXt0/src/Solvers/fourier_tridiagonal_poisson_s",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3267:2070,Load,LoadError,2070,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3267,1,['Load'],['LoadError']
Performance,"com/ranocha""><code>@​ranocha</code></a> made their first contribution in <a href=""https://redirect.github.com/julia-actions/setup-julia/pull/209"">julia-actions/setup-julia#209</a></li>; </ul>; <p><strong>Full Changelog</strong>: <a href=""https://github.com/julia-actions/setup-julia/compare/v1.9.6...v2.0.0"">https://github.com/julia-actions/setup-julia/compare/v1.9.6...v2.0.0</a></p>; <h2>v1.9.6: Fix Apple Silicon installation</h2>; <h2>What's Changed</h2>; <ul>; <li>Fix the Apple Silicon (macOS <code>aarch64</code> / <code>arm64</code>) URLs for Julia nightly by <a href=""https://github.com/benlorenz""><code>@​benlorenz</code></a> in <a href=""https://redirect.github.com/julia-actions/setup-julia/pull/220"">julia-actions/setup-julia#220</a></li>; <li>put quotes on readme version examples by <a href=""https://github.com/IanButterworth""><code>@​IanButterworth</code></a> in <a href=""https://redirect.github.com/julia-actions/setup-julia/pull/203"">julia-actions/setup-julia#203</a></li>; </ul>; <h3>Deps &amp; CI</h3>; <ul>; <li>Bump actions/cache from 3 to 4 by <a href=""https://github.com/dependabot""><code>@​dependabot</code></a> in <a href=""https://redirect.github.com/julia-actions/setup-julia/pull/215"">julia-actions/setup-julia#215</a></li>; <li>Bump <code>@​types/node</code> from 20.10.6 to 20.11.16 by <a href=""https://github.com/dependabot""><code>@​dependabot</code></a> in <a href=""https://redirect.github.com/julia-actions/setup-julia/pull/213"">julia-actions/setup-julia#213</a></li>; <li>Bump ts-jest from 29.1.1 to 29.1.2 by <a href=""https://github.com/dependabot""><code>@​dependabot</code></a> in <a href=""https://redirect.github.com/julia-actions/setup-julia/pull/212"">julia-actions/setup-julia#212</a></li>; <li>Bump nock from 13.4.0 to 13.5.1 by <a href=""https://github.com/dependabot""><code>@​dependabot</code></a> in <a href=""https://redirect.github.com/julia-actions/setup-julia/pull/211"">julia-actions/setup-julia#211</a></li>; <li>Bump semver from 7.5.4 to 7.6.0 by <a href=",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3544:2921,cache,cache,2921,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3544,1,['cache'],['cache']
Performance,"combinations of boundary conditions instead, which would reduce the number of possibilities. For example, we might have just doubly periodic in (x, y) plus flux in z on all fields, or singly-periodic in x and flux in (y, z) on all fields. Agree that `CoordinateBoundaryConditions` might be a weird name but yeah, maximum flexibility would be very powerful. Maybe the common use case isn't to impose each of the 30 boundary conditions one-by-one but we can just have nice helper functions/abstractions like; ```julia; model.boundary conditions += HorizontallyPeriodic(); ```. > @ali-ramadhan do you mean with regards to performance? I'm not sure. With multiple dispatch being core to julia it seems this scenario is not uncommon (30+ may not be very large). I'm still pretty new to Julia so yeah don't know if that will be an issue, especially on the GPU. Only way to find out is to try and benchmark! Maybe you're right and 30+ isn't a lot. @vchuravy any idea on whether 30+ parameterized types for a struct is too many? Would this hurt performance on the GPU?. > The function calc does not actually impose a boundary condition --- the imposition of boundary condition depends on, for example, the viscosity and diffusivity, and is a property of the equation (or turbulent closure) being implemented. Again for example, the K-Profile-Parameterization includes a modification of how a flux boundary condition is implemented. In other words, the ""specification of flux"" is separate from the ""imposition of a boundary condition"". The former is determined by the user. The latter is determined by the model/governing equation. I see. So if no parameterizations are being used, are the boundary conditions actually being _imposed_ then? Even with KPP, isn't the boundary condition still being _imposed_ only to later be modified by KPP?. I still feel like `bc.calc()` feels obscure, I'm not sure why a boundary condition should have to calculated. Perhaps it's just semantics but it would be nice to see bo",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/118#issuecomment-472241993:1079,perform,performance,1079,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/118#issuecomment-472241993,1,['perform'],['performance']
Performance,"comparison of run wall time for the bickley jet `128²` between this PR and main (on the GPU). ; ; This is running the code till 200 seconds; | | Periodic | Periodic | Bounded | Bounded | Immersed | Immersed | ; | ---- | ---- | ---- | ---- | ---- | ---- | ---- |; | **Scheme** | this PR | Main | this PR | Main | this PR | Main |; Upwind 5 | 33.8967 | 64.0005 | 50.0858 | 47.4800 | 140.9554 | 107.9684 | ; WENO 5 VI | 28.4259 | 30.8202 | 37.8422 | 34.2834 | 123.7893 | 122.7207 |; WENO 5 FF | 27.9952 | 31.9765 | 32.2018 | 36.7740 | 111.0842 | 107.0792 |; Vector Inv | 28.9798 | 27.7792 | 37.5503 | 33.1336 | 125.4214 | 119.6501 |. Looking at these timings, everything seems in the ballpark (except for the immersed boundary) so I am not sure what might be the issue... I guess we would need an in depth profiling to understand this?. I say we merge. The benefit from time step increase covers for the performance decrease. And finding the root of such a decrease might be very difficult",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2603#issuecomment-1192981334:901,perform,performance,901,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2603#issuecomment-1192981334,1,['perform'],['performance']
Performance,"csLLVM; ✓ Colors; ✓ GPUArraysCore; ✓ NVTX. ✓ GPUArrays; ✓ KernelAbstractions; ✓ PrettyTables; ✓ GPUCompiler; ✓ DataFrames; ✗ CUDA; 61 dependencies successfully precompiled in 190 seconds. 5 already precompiled. The following 1 direct dependency failed to precompile:. CUDA [052768ef-5323-5732-b1bb-66c8b64840ba]. Failed to precompile CUDA [052768ef-5323-5732-b1bb-66c8b64840ba] to ""/glade/u/home/knudsenl/.julia/compiled/v1.10/CUDA/jl_UQIv2i"".; [45592] signal (11.1): Segmentation fault; in expression starting at /glade/u/home/knudsenl/.julia/packages/CUDA/Tl08O/src/CUDA.jl:25; Allocations: 2907 (Pool: 2898; Big: 9); GC: ERROR: LoadError: Failed to precompile CUDA [052768ef-5323-5732-b1bb-66c8b64840ba] to ""/glade/u/home/knudsenl/.julia/compiled/v1.10/CUDA/jl_CUC33l"".; Stacktrace:; [1] error(s::String); @ Base ./error.jl:35; [2] compilecache(pkg::Base.PkgId, path::String, internal_stderr::IO, internal_stdout::IO, keep_loaded_modules::Bool); @ Base ./loading.jl:2468; [3] compilecache; @ ./loading.jl:2340 [inlined]; [4] (::Base.var""#968#969""{Base.PkgId})(); @ Base ./loading.jl:1974; [5] mkpidlock(f::Base.var""#968#969""{Base.PkgId}, at::String, pid::Int32; kwopts::@Kwargs{stale_age::Int64, wait::Bool}); @ FileWatching.Pidfile /glade/u/apps/casper/23.10/spack/opt/spack/julia/1.10.2/gcc/7.5.0/apod/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:93; [6] #mkpidlock#6; @ /glade/u/apps/casper/23.10/spack/opt/spack/julia/1.10.2/gcc/7.5.0/apod/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:88 [inlined]; [7] trymkpidlock(::Function, ::Vararg{Any}; kwargs::@Kwargs{stale_age::Int64}); @ FileWatching.Pidfile /glade/u/apps/casper/23.10/spack/opt/spack/julia/1.10.2/gcc/7.5.0/apod/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:111; [8] #invokelatest#2; @ ./essentials.jl:894 [inlined]; [9] invokelatest; @ ./essentials.jl:889 [inlined]; [10] maybe_cachefile_lock(f::Base.var""#968#969""{Base.PkgId}, pkg::Base.PkgId, srcpath::String; stale_age::Int64); @ Base ./loading.jl:2983; [1",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2245919472:5241,load,loading,5241,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2245919472,1,['load'],['loading']
Performance,"cudadrv/libcuda.jl:37 [inlined]; [3] cuStreamGetCaptureInfo; @ ~/.julia/packages/CUDA/2kjXI/lib/utils/call.jl:34 [inlined]; [4] capture_status(stream::CUDA.CuStream); @ CUDA ~/.julia/packages/CUDA/2kjXI/lib/cudadrv/graph.jl:174; [5] is_capturing (repeats 2 times); @ ~/.julia/packages/CUDA/2kjXI/lib/cudadrv/graph.jl:179 [inlined]; [6] checked_cuModuleLoadDataEx(_module::Base.RefValue{Ptr{CUDA.CUmod_st}}, image::Ptr{UInt8}, numOptions::Int64, options::Vector{CUDA.CUjit_option_enum}, optionValues::Vector{Ptr{Nothing}}); @ CUDA ~/.julia/packages/CUDA/2kjXI/lib/cudadrv/module.jl:17; [7] CUDA.CuModule(data::Vector{UInt8}, options::Dict{CUDA.CUjit_option_enum, Any}); @ CUDA ~/.julia/packages/CUDA/2kjXI/lib/cudadrv/module.jl:60; [8] CuModule; @ ~/.julia/packages/CUDA/2kjXI/lib/cudadrv/module.jl:49 [inlined]; [9] link(job::GPUCompiler.CompilerJob, compiled::@NamedTuple{image::Vector{UInt8}, entry::String}); @ CUDA ~/.julia/packages/CUDA/2kjXI/src/compiler/compilation.jl:409; [10] actual_compilation(cache::Dict{Any, CUDA.CuFunction}, src::Core.MethodInstance, world::UInt64, cfg::GPUCompiler.CompilerConfig{GPUCompiler.PTXCompilerTarget, CUDA.CUDACompilerParams}, compiler::typeof(CUDA.compile), linker::typeof(CUDA.link)); @ GPUCompiler ~/.julia/packages/GPUCompiler/2CW9L/src/execution.jl:262; [11] cached_compilation(cache::Dict{Any, CUDA.CuFunction}, src::Core.MethodInstance, cfg::GPUCompiler.CompilerConfig{GPUCompiler.PTXCompilerTarget, CUDA.CUDACompilerParams}, compiler::Function, linker::Function); @ GPUCompiler ~/.julia/packages/GPUCompiler/2CW9L/src/execution.jl:151; [12] macro expansion; @ ~/.julia/packages/CUDA/2kjXI/src/compiler/execution.jl:380 [inlined]; [13] macro expansion; @ ./lock.jl:267 [inlined]; [14] cufunction(f::typeof(Oceananigans.Models.NonhydrostaticModels.gpu__update_hydrostatic_pressure!), tt::Type{Tuple{KernelAbstractions.CompilerMetadata{…}, OffsetArrays.OffsetArray{…}, LatitudeLongitudeGrid{…}, Buoyancy{…}, @NamedTuple{…}}}; kwargs::@Kwargs{always_inl",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3870:2135,cache,cache,2135,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3870,1,['cache'],['cache']
Performance,"cudadrv/libcuda.jl:37 [inlined]; [3] cuStreamGetCaptureInfo; @ ~/.julia/packages/CUDA/z3j2H/lib/utils/call.jl:34 [inlined]; [4] capture_status(stream::CUDA.CuStream); @ CUDA ~/.julia/packages/CUDA/z3j2H/lib/cudadrv/graph.jl:174; [5] is_capturing (repeats 2 times); @ ~/.julia/packages/CUDA/z3j2H/lib/cudadrv/graph.jl:179 [inlined]; [6] checked_cuModuleLoadDataEx(_module::Base.RefValue{Ptr{CUDA.CUmod_st}}, image::Ptr{UInt8}, numOptions::Int64, options::Vector{CUDA.CUjit_option_enum}, optionValues::Vector{Ptr{Nothing}}); @ CUDA ~/.julia/packages/CUDA/z3j2H/lib/cudadrv/module.jl:17; [7] CUDA.CuModule(data::Vector{UInt8}, options::Dict{CUDA.CUjit_option_enum, Any}); @ CUDA ~/.julia/packages/CUDA/z3j2H/lib/cudadrv/module.jl:60; [8] CuModule; @ ~/.julia/packages/CUDA/z3j2H/lib/cudadrv/module.jl:49 [inlined]; [9] link(job::GPUCompiler.CompilerJob, compiled::@NamedTuple{image::Vector{UInt8}, entry::String}); @ CUDA ~/.julia/packages/CUDA/z3j2H/src/compiler/compilation.jl:413; [10] actual_compilation(cache::Dict{Any, CUDA.CuFunction}, src::Core.MethodInstance, world::UInt64, cfg::GPUCompiler.CompilerConfig{GPUCompiler.PTXCompilerTarget, CUDA.CUDACompilerParams}, compiler::typeof(CUDA.compile), linker::typeof(CUDA.link)); @ GPUCompiler ~/.julia/packages/GPUCompiler/2CW9L/src/execution.jl:262; [11] cached_compilation(cache::Dict{Any, CUDA.CuFunction}, src::Core.MethodInstance, cfg::GPUCompiler.CompilerConfig{GPUCompiler.PTXCompilerTarget, CUDA.CUDACompilerParams}, compiler::Function, linker::Function); @ GPUCompiler ~/.julia/packages/GPUCompiler/2CW9L/src/execution.jl:151; [12] macro expansion; @ ~/.julia/packages/CUDA/z3j2H/src/compiler/execution.jl:380 [inlined]; [13] macro expansion; @ ./lock.jl:267 [inlined]; [14] cufunction(f::typeof(Oceananigans.Models.NonhydrostaticModels.gpu__update_hydrostatic_pressure!), tt::Type{Tuple{KernelAbstractions.CompilerMetadata{…}, OffsetArrays.OffsetArray{…}, LatitudeLongitudeGrid{…}, Buoyancy{…}, @NamedTuple{…}}}; kwargs::@Kwargs{always_inl",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3785:1862,cache,cache,1862,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3785,1,['cache'],['cache']
Performance,"d = Int.(Ny/2-40:Ny/2+40); bottom[bound, bound] .= 0.5. grid = ImmersedBoundaryGrid(grid, GridFittedBottom(bottom)); mrg = MultiRegionGrid(grid, partition=XPartition(2), devices = 2); ```; and ; ```; u(x, y, z) = (x * z) / 10; ```. (with `ImplicitFreeSurface`) we get. #### Strong Scaling; | Grid size | Grid | GPUs | wall time | efficiency |; | -- | -- | -- | -- | -- |; | `1440×600×48`| `RectilinearGrid` | 1 | 1.85 minutes | 100% |; | `1440×600×48`| `MultiRegionGrid` | 2 | 1.12 minutes | 82.5% |. Scaling gets better?? at this point I a little confused...; Bottomline... ; there is still a bunch of optimization and more systematic benchmarking to be done, I'll merge this PR and then we can think about improving the scaling, the first things that come in mind are; - Ensure that all the `apply_regionally!` and `construct_regionally` calls are asynchronous. This might not be the case if there are memory copies inside function calls. That would serialize the execution of part of the code. To ensure this we require a more in-depth profiling using ***nsys***; - remove all `fill_halo_regions!` that are not `Periodic` or `Communication` which will allow asynchronous execution of halo filling across different direction (luckily already being done in #2477); - Bundle together the halo passing in a single boundary buffer to allow sending field tuples together (depends on #2509). Additional work to do on `MultiRegion` is ; - Perform more systematic benchmarking; - Design correct `OutputWriters` and `OutputReaders` for `MultiRegionFields`. Maybe not immediate priorities but definitely important; - Adapt `RungeKutta3` to `Multiregion` through `@apply_regionally`; - Implement a multi-region version of the Nonhydrostatic pressure solver; - Overlap computation and communication (for this we require _non-blocking_ `fill_halo_regions!` with returning `events`); - Optimize the distributed implicit solver (maybe using `PETSc.jl`? Might not be needed when the implicit solve fits in one GPU)",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2253#issuecomment-1116853178:1612,Perform,Perform,1612,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2253#issuecomment-1116853178,2,"['Optimiz', 'Perform']","['Optimize', 'Perform']"
Performance,"d = RegularCartesianGrid(size=(16, 16, 16), extent=(1, 1, 1)); model = IncompressibleModel(architecture=GPU(), grid=grid); time_step!(model, 1); [ Info: Precompiling Oceananigans [9e8cae18-63c1-5223-a75c-80ca9d6e9a09]; CUDA-enabled GPU(s) detected:; CUDA.CuDevice(0); CUDA.CuDevice(1); CUDA.CuDevice(2); CUDA.CuDevice(3); ┌ Warning: Performing scalar operations on GPU arrays: This is very slow, consider disallowing these operations with `allowscalar(false)`; └ @ GPUArrays ~/.julia/packages/GPUArrays/4W5rW/src/host/indexing.jl:43; ```. # Julia 1.5.0. ```; _; _ _ _(_)_ | Documentation: https://docs.julialang.org; (_) | (_) (_) |; _ _ _| |_ __ _ | Type ""?"" for help, ""]?"" for Pkg help.; | | | | | | |/ _` | |; | | |_| | | | (_| | | Version 1.5.0 (2020-08-01); _/ |\__'_|_|_|\__'_| | Official https://julialang.org/ release; |__/ |. julia> using Oceananigans; grid = RegularCartesianGrid(size=(16, 16, 16), extent=(1, 1, 1)); model = IncompressibleModel(architecture=GPU(), grid=grid); time_step!(model, 1); ┌ Warning: Performing scalar operations on GPU arrays: This is very slow, consider disallowing these operations with `allowscalar(false)`; └ @ GPUArrays ~/.julia/packages/GPUArrays/4W5rW/src/host/indexing.jl:43; ERROR: InvalidIRError: compiling kernel gpu_calculate_pressure_right_hand_side!(Cassette.Context{nametype(CUDACtx),KernelAbstractions.CompilerMetadata{KernelAbstractions.NDIteration.StaticSize{(16, 16, 16)},KernelAbstractions.NDIteration.DynamicCheck,Nothing,Nothing,KernelAbstractions.NDIteration.NDRange{3,KernelAbstractions.NDIteration.StaticSize{(1, 1, 16)},KernelAbstractions.NDIteration.StaticSize{(16, 16, 1)},Nothing,Nothing}},Nothing,KernelAbstractions.var""##PassType#253"",Nothing,Cassette.DisableHooks}, typeof(Oceananigans.Solvers.gpu_calculate_pressure_right_hand_side!), CUDA.CuDeviceArray{Complex{Float64},3,CUDA.AS.Global}, Oceananigans.Solvers.HorizontallyPeriodic, GPU, RegularCartesianGrid{Float64,Periodic,Periodic,Bounded,OffsetArrays.OffsetArray{Float64,1,S",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/828:2836,Perform,Performing,2836,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/828,1,['Perform'],['Performing']
Performance,"d style = ""text-align: right; "">4.0899</td>; <td style = ""text-align: right; "">5.05926</td>; </tr>; <tr>; <td style = ""text-align: right; "">CPU</td>; <td style = ""text-align: right; "">128</td>; <td style = ""text-align: right; "">(2, 3)</td>; <td style = ""text-align: right; "">1.87883</td>; <td style = ""text-align: right; "">2.18814</td>; <td style = ""text-align: right; "">2.50863</td>; </tr>; <tr>; <td style = ""text-align: right; "">CPU</td>; <td style = ""text-align: right; "">128</td>; <td style = ""text-align: right; "">(2, 5)</td>; <td style = ""text-align: right; "">2.20522</td>; <td style = ""text-align: right; "">2.70298</td>; <td style = ""text-align: right; "">3.16129</td>; </tr>; </table>. <table>; <caption style = ""text-align: center; "">Arbitrary tracers relative performance (GPU)</caption>; <tr class = ""header headerLastRow"">; <th style = ""text-align: right; "">Architectures</th>; <th style = ""text-align: right; "">Ns</th>; <th style = ""text-align: right; "">tracers</th>; <th style = ""text-align: right; "">slowdown</th>; <th style = ""text-align: right; "">memory</th>; <th style = ""text-align: right; "">allocs</th>; </tr>; <tr>; <td style = ""text-align: right; "">GPU</td>; <td style = ""text-align: right; "">128</td>; <td style = ""text-align: right; "">(0, 0)</td>; <td style = ""text-align: right; "">1.0</td>; <td style = ""text-align: right; "">1.0</td>; <td style = ""text-align: right; "">1.0</td>; </tr>; <tr>; <td style = ""text-align: right; "">GPU</td>; <td style = ""text-align: right; "">128</td>; <td style = ""text-align: right; "">(0, 1)</td>; <td style = ""text-align: right; "">1.10818</td>; <td style = ""text-align: right; "">1.19745</td>; <td style = ""text-align: right; "">1.16449</td>; </tr>; <tr>; <td style = ""text-align: right; "">GPU</td>; <td style = ""text-align: right; "">128</td>; <td style = ""text-align: right; "">(0, 2)</td>; <td style = ""text-align: right; "">1.23007</td>; <td style = ""text-align: right; "">1.43936</td>; <td style = ""text-align: right; "">1.34169</td>; </tr>; <tr>; ",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1169#issuecomment-725471594:6273,perform,performance,6273,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1169#issuecomment-725471594,1,['perform'],['performance']
Performance,"d, apparently, `Σ² = S_ij S_ij` (if we believe we understand the notation here):. https://github.com/CliMA/Oceananigans.jl/blob/32c5c5a2d0f441a4b663866d511807d0f9413c90/src/TurbulenceClosures/turbulence_closure_implementations/smagorinsky_lilly.jl#L97. This means that our formula is identical to Pressel et al (2015), but the constant is wrong: Pressel proposes `Cs=0.17`, and we have a default of `Cs=0.23`:. https://github.com/CliMA/Oceananigans.jl/blob/32c5c5a2d0f441a4b663866d511807d0f9413c90/src/TurbulenceClosures/turbulence_closure_implementations/smagorinsky_lilly.jl#L63-L64. Amusingly, . ```julia; julia> 0.165 * sqrt(2); 0.23334523779156072; ```. which is probably not a coincidence. However, I'm not sure how an extra factor of `sqrt(2)` snuck into our constant. As for _clarification_, I don't have much to offer. Perhaps the constant was taken from some reference that used a different formulation than either us or Pressel et al. 2015. Nobody has submitted a validation test for this closure so I don't think we know how it performs. As a historical note, the paper cited by both Pressel et al (2015) and us is Lilly (1962), which does indeed use the same formulation:. ![image](https://user-images.githubusercontent.com/15271942/127507507-c7d11dfc-733a-4472-8216-dca4e5844b3f.png). where. ![image](https://user-images.githubusercontent.com/15271942/127507737-94aa05d4-6fa5-4d38-9a9d-b1bbcc8a1e79.png). @tomchor I can't tell if the formula you've pasted is actually different from ours (or what the definition of `|S|` is). Where does it come from?. It'd be fine to change the constant because there's no validation test. So the best we can do is theorize, and theorization on this issue suggests changing the default to `C=0.17`. If one wanted to set up a validation test, it could be nice to reproduce [Compte-Bellot and Corrsin (1964)](https://www.cambridge.org/core/journals/journal-of-fluid-mechanics/article/abs/simple-eulerian-time-correlation-of-fulland-narrowband-velocity-si",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1907#issuecomment-889189927:2134,perform,performs,2134,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1907#issuecomment-889189927,1,['perform'],['performs']
Performance,"de; @ ./Base.jl:495 [inlined]; [3] include_package_for_output(pkg::Base.PkgId, input::String, depot_path::Vector{String}, dl_load_path::Vector{String}, load_path::Vector{String}, concrete_deps::Vector{Pair{Base.PkgId, UInt128}}, source::Nothing); @ Base ./loading.jl:2222; [4] top-level scope; @ stdin:3; in expression starting at /Users/navid/.julia/packages/TaylorSeries/2qRvJ/ext/TaylorSeriesIAExt.jl:1; in expression starting at stdin:3; ┌ Error: Error during loading of extension TaylorSeriesIAExt of TaylorSeries, use `Base.retry_load_extensions()` to retry.; │ exception =; │ 1-element ExceptionStack:; │ Failed to precompile TaylorSeriesIAExt [ed7ef945-33a4-511e-97fe-2b89c7a130ca] to ""/Users/navid/.julia/compiled/v1.10/TaylorSeriesIAExt/jl_TNauRw"".; │ Stacktrace:; │ [1] error(s::String); │ @ Base ./error.jl:35; │ [2] compilecache(pkg::Base.PkgId, path::String, internal_stderr::IO, internal_stdout::IO, keep_loaded_modules::Bool); │ @ Base ./loading.jl:2468; │ [3] compilecache; │ @ ./loading.jl:2340 [inlined]; │ [4] (::Base.var""#968#969""{Base.PkgId})(); │ @ Base ./loading.jl:1974; │ [5] mkpidlock(f::Base.var""#968#969""{Base.PkgId}, at::String, pid::Int32; kwopts::@Kwargs{stale_age::Int64, wait::Bool}); │ @ FileWatching.Pidfile ~/julia-1.10/usr/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:93; │ [6] #mkpidlock#6; │ @ ~/julia-1.10/usr/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:88 [inlined]; │ [7] trymkpidlock(::Function, ::Vararg{Any}; kwargs::@Kwargs{stale_age::Int64}); │ @ FileWatching.Pidfile ~/julia-1.10/usr/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:111; │ [8] #invokelatest#2; │ @ ./essentials.jl:894 [inlined]; │ [9] invokelatest; │ @ ./essentials.jl:889 [inlined]; │ [10] maybe_cachefile_lock(f::Base.var""#968#969""{Base.PkgId}, pkg::Base.PkgId, srcpath::String; stale_age::Int64); │ @ Base ./loading.jl:2983; │ [11] maybe_cachefile_lock; │ @ ./loading.jl:2980 [inlined]; │ [12] _require(pkg::Base.PkgId, env::Nothing); │ @ Base ./loading.jl:1970",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3686#issuecomment-2272432528:1335,load,loading,1335,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3686#issuecomment-2272432528,1,['load'],['loading']
Performance,"ded)); ```. `Hmmm, how small was the time step? `. I tried a few different values ranging from 1 sec to 60 sec. . I found that the `NaN` issue has to do with the `AnisotropicBiharmonicDiffusivity`. The following works:. ```; closure = AnisotropicDiffusivity(νh=kappaH_tmp, κh=kappaH_tmp, κz = kappaV, νz = kappaV); ```. but not this: . ```; closure = (AnisotropicDiffusivity(νh=0, κh=0, κz = kappaV, νz = kappaV),; AnisotropicBiharmonicDiffusivity(νh=kappaH, κh=kappaH)); ```. Also, `topology = (Periodic, Periodic, Bounded)` runs fine now, but `topology = (Periodic, Bounded, Bounded)` gives the following error:. ```; Loaded all modules; Starting the simulation...; ┌ Warning: You have used the default iteration_interval=1. This simulation will recalculate the time step every iteration which can be slow.; └ @ Oceananigans.Simulations ~/.julia/packages/Oceananigans/gCqmh/src/Simulations/simulation.jl:68; ERROR: LoadError: type VerticallyStretchedRectilinearGrid has no field Δz; Stacktrace:; [1] getproperty(::VerticallyStretchedRectilinearGrid{Float64,Periodic,Bounded,Bounded,OffsetArrays.OffsetArray{Float64,1,StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}},OffsetArrays.OffsetArray{Float64,1,CuArray{Float64,1}}}, ::Symbol) at ./Base.jl:33; [2] cell_advection_timescale(::CuArray{Float64,3}, ::CuArray{Float64,3}, ::CuArray{Float64,3}, ::VerticallyStretchedRectilinearGrid{Float64,Periodic,Bounded,Bounded,OffsetArrays.OffsetArray{Float64,1,StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}},OffsetArrays.OffsetArray{Float64,1,CuArray{Float64,1}}}) at /home/guptam/.julia/packages/Oceananigans/gCqmh/src/Utils/cell_advection_timescale.jl:9; [3] cell_advection_timescale(::IncompressibleModel{Oceananigans.TimeSteppers.RungeKutta3TimeStepper{Float64,NamedTuple{(:u, :v, :w, :T, :S),Tuple{Field{Face,Center,Center,OffsetArrays.OffsetArray{Float64,3,CuArray{Float64,3}},VerticallyStretchedRectilinearGrid{Float64,Periodic,Bounded,",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1429#issuecomment-792994007:1296,Load,LoadError,1296,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1429#issuecomment-792994007,1,['Load'],['LoadError']
Performance,"ditions.Periodic,Nothing}},CoordinateBoundaryConditions{BoundaryCondition{Flux,Nothing},BoundaryCondition{Flux,Nothing}}}}}}},NamedTuple{(:u, :v, :w, :T, :S),NTuple{5,typeof(Oceananigans.Forcings.zeroforcing)}},CenteredFourthOrder,Oceananigans.Solvers.PressureSolver{Oceananigans.Solvers.HorizontallyPeriodic,CPU,NamedTuple{(:kx², :ky², :kz²),Tuple{Array{Float64,3},Array{Float64,3},Array{Float64,3}}},Array{Complex{Float64},3},NamedTuple{(:FFTxy!, :DCTz!, :IFFTxy!, :IDCTz!),Tuple{FFTW.cFFTWPlan{Complex{Float64},-1,true,3,Array{Int64,1}},FFTW.r2rFFTWPlan{Complex{Float64},(5,),true,3,Int64},AbstractFFTs.ScaledPlan{Complex{Float64},FFTW.cFFTWPlan{Complex{Float64},1,true,3,Array{Int64,1}},Float64},FFTW.r2rFFTWPlan{Complex{Float64},(4,),true,3,Int64}}},Nothing},Tuple{Nothing,Nothing},NamedTuple{(:velocities, :tracers),Tuple{NamedTuple{(:u, :v, :w),Tuple{Oceananigans.Fields.ZeroField,Oceananigans.Fields.ZeroField,Oceananigans.Fields.ZeroField}},NamedTuple{(:T, :S),Tuple{Oceananigans.Fields.ZeroField,Oceananigans.Fields.ZeroField}}}}},TimeStepWizard{Float64},Array{Any,1},Float64,Int64,Float64,Float64,OrderedCollections.OrderedDict{Symbol,Oceananigans.AbstractDiagnostic},OrderedCollections.OrderedDict{Symbol,Oceananigans.AbstractOutputWriter},typeof(progress),Int64,Nothing}) at /home/guptam/.julia/packages/Oceananigans/nKAWY/src/Simulations/run.jl:102; [21] top-level scope at /central/home/guptam/ocean_floes/turb-closures/expt/biharmonic_1e5_4thOrder_kappaV_profile1_Z_2p5.jl:135; [22] include(::Function, ::Module, ::String) at ./Base.jl:380; [23] include(::Module, ::String) at ./Base.jl:368; [24] exec_options(::Base.JLOptions) at ./client.jl:296; [25] _start() at ./client.jl:506; in expression starting at /central/home/guptam/ocean_floes/turb-closures/expt/biharmonic_1e5_4thOrder_kappaV_profile1_Z_2p5.jl:135; Loaded all modules; ```. which occurs because `TimeStepWizard` doesn't know how to compute the maximum diffusivity when diffusivity is a function (it shouldn't even try!)",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1104:25643,Load,Loaded,25643,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1104,1,['Load'],['Loaded']
Performance,does it change the performance?,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2843#issuecomment-1332805489:19,perform,performance,19,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2843#issuecomment-1332805489,1,['perform'],['performance']
Performance,"duce the parameter size for the velocity tendencies by only passing them the T and S tracers as required (see the first, nonworking, step towards this [here](https://github.com/jagoosw/Oceananigans.jl/tree/reduce-gpu-params)) I don't see how this problem can be solved in the `tracer_tendency` function since tracer forcing/boundaries may depend on any number of fields. I also realize that the choice to pass all the tracers may be required for something else I've missed. I can have a go at overhauling the `tracer_tendency` function etc. based on [this](https://github.com/JuliaGPU/CUDA.jl/issues/267#issuecomment-1218097841) suggestion and [this](https://github.com/CliMA/Oceananigans.jl/issues/722) but am not sure if its necessarily the best/nicest solution?. Edit: Possibly could just remove a lot of the information from the fields using the `adapt_structure` method from [PR #1057](https://github.com/CliMA/Oceananigans.jl/pull/1057)?. Thanks!. ```; LoadError: Failed to compile PTX code (ptxas exited with code 255); ptxas /tmp/jl_4JwMaF.ptx, line 4214; error : Entry function '_Z29julia_gpu_calculate_Gu__1315516CompilerMetadataI10StaticSizeI10_3__3__33_E12DynamicCheckvv7NDRangeILi3ES0_I10_1__1__33_ES0_I11_16__16__1_EvvEE11OffsetArrayI7Float64Li3E13CuDeviceArrayIS4_Li3ELi1EEE15RectilinearGridIS4_8PeriodicS7_7BoundedS4_S4_S3_IS4_Li1ES5_IS4_Li1ELi1EEES3_IS4_Li1E12StepRangeLenIS4_14TwicePrecisionIS4_ES10_IS4_E5Int64EES3_IS4_Li1ES9_IS4_S10_IS4_ES10_IS4_ES11_EES3_IS4_Li1ES5_IS4_Li1ELi1EEEvE22UpwindBiasedFifthOrder6FPlaneIS4_Ev17ScalarDiffusivityI26ExplicitTimeDiscretization27ThreeDimensionalFormulation3___10NamedTupleI57__b___NO____NH____P___Z___D___DD___DOM___DIC___ALK___OXY_5TupleIS17_S17_S17_S17_S17_S17_S17_S17_S17_S17_S17_EEE17BoundaryConditionI4FluxvE8BuoyancyI14BuoyancyTracer10ZDirectionES18_I23__velocities___tracers_S19_IS18_I12__u___v___w_S19_I9ZeroFieldIS11_Li3EES25_IS11_Li3EES25_IS11_Li3EEEES18_I57__b___NO____NH____P___Z___D___DD___DOM___DIC___ALK___OXY_S19_IS25_IS11_L",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2700:1568,Load,LoadError,1568,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2700,1,['Load'],['LoadError']
Performance,"e Julia runtime (call to jl_f_tuple); ```. Might have something to do with the new tuple/named tuple syntax but I thought Julia 1.4 -> 1.5 wasn't supposed to introduce any breaking changes? https://julialang.org/blog/2020/08/julia-1.5-highlights/#implicit_keyword_argument_values. # Julia 1.4.2. ```; _; _ _ _(_)_ | Documentation: https://docs.julialang.org; (_) | (_) (_) |; _ _ _| |_ __ _ | Type ""?"" for help, ""]?"" for Pkg help.; | | | | | | |/ _` | |; | | |_| | | | (_| | | Version 1.4.2 (2020-05-23); _/ |\__'_|_|_|\__'_| | Official https://julialang.org/ release; |__/ |. julia> using Oceananigans; grid = RegularCartesianGrid(size=(16, 16, 16), extent=(1, 1, 1)); model = IncompressibleModel(architecture=GPU(), grid=grid); time_step!(model, 1); [ Info: Precompiling Oceananigans [9e8cae18-63c1-5223-a75c-80ca9d6e9a09]; CUDA-enabled GPU(s) detected:; CUDA.CuDevice(0); CUDA.CuDevice(1); CUDA.CuDevice(2); CUDA.CuDevice(3); ┌ Warning: Performing scalar operations on GPU arrays: This is very slow, consider disallowing these operations with `allowscalar(false)`; └ @ GPUArrays ~/.julia/packages/GPUArrays/4W5rW/src/host/indexing.jl:43; ```. # Julia 1.5.0. ```; _; _ _ _(_)_ | Documentation: https://docs.julialang.org; (_) | (_) (_) |; _ _ _| |_ __ _ | Type ""?"" for help, ""]?"" for Pkg help.; | | | | | | |/ _` | |; | | |_| | | | (_| | | Version 1.5.0 (2020-08-01); _/ |\__'_|_|_|\__'_| | Official https://julialang.org/ release; |__/ |. julia> using Oceananigans; grid = RegularCartesianGrid(size=(16, 16, 16), extent=(1, 1, 1)); model = IncompressibleModel(architecture=GPU(), grid=grid); time_step!(model, 1); ┌ Warning: Performing scalar operations on GPU arrays: This is very slow, consider disallowing these operations with `allowscalar(false)`; └ @ GPUArrays ~/.julia/packages/GPUArrays/4W5rW/src/host/indexing.jl:43; ERROR: InvalidIRError: compiling kernel gpu_calculate_pressure_right_hand_side!(Cassette.Context{nametype(CUDACtx),KernelAbstractions.CompilerMetadata{KernelAbstractions.ND",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/828:2148,Perform,Performing,2148,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/828,1,['Perform'],['Performing']
Performance,e at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; #fill_halo_regions!#38 at /orcd/data/raffaele/001/glwagner/Oceananigans.jl/src/DistributedComputations/halo_communication.jl:114; fill_halo_regions! at /orcd/data/raffaele/001/glwagner/Oceananigans.jl/src/DistributedComputations/halo_communication.jl:101 [inlined]; #fill_halo_regions!#37 at /orcd/data/raffaele/001/glwagner/Oceananigans.jl/src/DistributedComputations/halo_communication.jl:90 [inlined]; fill_halo_regions! at /orcd/data/raffaele/001/glwagner/Oceananigans.jl/src/DistributedComputations/halo_communication.jl:87; unknown function (ip: 0x2aaac8ad0ee5); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; jl_apply at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/julia.h:1982 [inlined]; do_call at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:126; eval_value at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:223; eval_stmt_value at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:174 [inlined]; eval_body at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:617; jl_interpret_toplevel_thunk at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:775; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:934; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:877; ijl_toplevel_eval_in at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:985; eval at ./boot.jl:385 [inlined]; include_string at ./loading.jl:2076; _jl_invoke at /,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3878:3941,cache,cache,3941,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3878,1,['cache'],['cache']
Performance,"e base making it easier to maintain, and there will be a very clear boundary between ""core Oceananigans"" and ""distributed parallelism functionality"" which I think will serve us well in the future as MPI seems to permeate deeply into other codes, making them hard to modify. The big thing that is missing is of course the distributed pressure solver, the hard thing to implement. This is where [DistributedTranspose.jl](https://github.com/leios/DistributedTranspose.jl) will come in handy. I also recently found [PencilFFTs.jl](https://github.com/jipolanco/PencilFFTs.jl) which also looks interesting. cc @leios. For testing purposes, I'm tempted to do the pressure solve via an `MPI.Gather` onto rank 0 where it can be solved locally then an `MPI.Scatter` to pass the pressure to all ranks. Super inefficient but might be good to ensure that the `DistributedModel` can reproduce existing regression tests. Performance issues:; * Right now `MPI.Isend`, `MPI.Recv!`, and `MPI.SendRecv!` all expect send and receive buffers to be contiguous in memory I believe. To get around this I allocate memory for these buffers, but this is definitely not performant. @vchuravy suggested that we may be able to send and receive into strided buffers, so will look into this. cc @simonbyrne maybe you know more about this?. Quality of life features we may want in the future (which might effect design choices):; * Distributed diagnostics: these will be pretty expensive no matter how we implement them due to the extra reduction step (`MPI.Gather`) required across all ranks. I wonder if it's even worth thinking about them much. If we really need things like a `DistributedHorizontalAverage` then we can look into that.; * Distributed output writers: I wonder if we should add e.g. a distributed NetCDF output writer or if each rank just writes out its own output and we have a utility function that post-processes the output files and merges them at the end (this seems easier than writing another output writer).",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/590:2484,perform,performant,2484,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/590,1,['perform'],['performant']
Performance,"e change we could make would be instead write. ```Julia; ""Calculate the right-hand-side of the u-momentum equation at I, j, k.""; u_eqn(args..., F::Function i, j, k) = stuff + F(u, v, w, T, S, Nx, Ny, Nz, Δx, Δy, Δz, i, j, k); u_eqn(args..., F::AbstractArray i, j, k) = stuff + F[i, j, k]; u_eqn(args..., F::Nothing, i, j, k) = stuff. ""Store previous value of the source term and calculate current source term.""; function update_source_terms!(::Val{Dev}, fCor, χ, ρ₀, κh, κv, 𝜈h, 𝜈v, Nx, Ny, Nz, Δx, Δy, Δz,; u, v, w, T, S, pHY′, Gu, Gv, Gw, GT, GS, Gpu, Gpv, Gpw, GpT, GpS, F) where Dev; ; # ...; # u-momentum equation; @inbounds Gu[i, j, k] = u_eqn(args..., F.u, i, j, k); # ...; ```; We could write even less code if we created an abstraction for the right hand side, something like. ```Julia; struct Equation{TF}; G::Function; F::TF; end. (eq::Equation{TF})(args..., i, j, k) where TF <: Function = eq.G(args..., i, j, k) + eq.F(args..., i, j, k); (eq::Equation{TF})(args..., i, j, k) where TF <: AbstractArray = eq.G(args..., i, j, k) + eq.F(args..., i, j, k); (eq::Equation{TF})(args..., i, j, k) where TF <: Nothing = eq.G(args..., i, j, k) . u_eqn = Equation(Gu, Fu). ...; @inbounds Gu[i, j, k] = u_eqn(args..., i, j, k); ```. We can then load all the equations we have into a `FieldVector` or `LabeledArray` to make things even better and do something like . ```julia; @loop for k in (1:Nz; blockIdx().z); @loop for j in (1:Ny; (blockIdx().y - 1) * blockDim().y + threadIdx().y); @loop for i in (1:Nx; (blockIdx().x - 1) * blockDim().x + threadIdx().x); for (Gφ, i) in enumerate(G); φ_eqn = equation[i]; Gφ[i, j, k] = φ_eqn(args... i, j, k); end; end; end; end; ```. With a time-stepping kernel of that form we can easily add and subtract tracers, equations, sub grid closure variables, etc. I think the inner loop gets unrolled when the array is static, so the compiled code is no different from what we currently have. . This is somewhere down the line hopefully. Maybe v0.6... or 1.0. Heh.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/110#issuecomment-470624169:1867,load,load,1867,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/110#issuecomment-470624169,1,['load'],['load']
Performance,"e easier for users to extend / add terms to an equation. As an example, we can consider a simple implementation. ```julia; struct AdvectiveFlux{S, D, C, G}; u :: D; v :: D; w :: D; c :: C; grid :: G; scheme :: S; end. getindex(adv::AdvectiveFlux{Centered}, i, j, k) = # centered advective flux calculation. struct IsotropicDiffusiveFlux{N, D, G}; ν :: N; ψ :: D; grid :: G; end. getindex(diff::IsotropicDiffusiveFlux, i, j, k) = # calculates flux due to isotropic diffusion by ν. struct RightHandSide{F, V}; fluxes :: F; volume_terms :: V; end. advection = AdvectiveFlux(velocities..., tracers.c); diffusion = IsotropicDiffusiveFlux(ν, tracers.c). tracer_rhs = RightHandSide((advection, diffusion), nothing); ```. We'd have functions that look something like. ```julia; function x_flux_divergence(i, j, k, grid, fluxes...); incoming_flux = add_fluxes(i, j, k, grid, fluxes...); outgoing_flux = add_fluxes(i+1, j, k, grid, fluxes...); return (incoming_flux - outgoing_flux) * grid.Ax / grid.V; end; ```. ... for example. Obviously questions of performance are paramount, though in the case that _everything_ is inlined I think there is hope. A downside of this approach is that we can't use shared memory stencils on the GPU. Shared memory stencils on the GPU require _functions_ for all terms that avoid carrying around internal references to data (since we need to be able to pass them references to shared memory blocks, rather than references to global memory. This is a fair rewrite of the code internals. For example, each term in the `u_velocity_tendency`:. https://github.com/climate-machine/Oceananigans.jl/blob/8e3c27504be68ca06bacc7502cd6095ae390f8c6/src/TimeSteppers/velocity_and_tracer_tendencies.jl#L24. would get it's own type. It's worth brainstorming ways to implement such abstraction incrementally so we might avoid possibly time-consuming total-code-demolishment. This is mostly food for thought at this point. I don't think we should take action without substantial consideration.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/259#issuecomment-600078394:1650,perform,performance,1650,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/259#issuecomment-600078394,1,['perform'],['performance']
Performance,"e), '/Applications/Julia-1.7.app/Contents/Resources/julia/lib/julia/libquadmath.0.dylib' (no such file), '/Applications/Julia-1.7.app/Contents/Resources/julia/bin/../lib/libquadmath.0.dylib' (no such file), '/usr/local/lib/libquadmath.0.dylib' (no such file), '/usr/lib/libquadmath.0.dylib' (no such file); Stacktrace:; [1] dlopen(s::String, flags::UInt32; throw_error::Bool); @ Base.Libc.Libdl ./libdl.jl:117; [2] dlopen(s::String, flags::UInt32); @ Base.Libc.Libdl ./libdl.jl:117; [3] macro expansion; @ ~/.julia/packages/JLLWrappers/QpMQW/src/products/library_generators.jl:54 [inlined]; [4] __init__(); @ MPICH_jll ~/.julia/packages/MPICH_jll/dhUyI/src/wrappers/aarch64-apple-darwin-libgfortran5.jl:32; [5] _include_from_serialized(path::String, depmods::Vector{Any}); @ Base ./loading.jl:768; [6] _require_search_from_serialized(pkg::Base.PkgId, sourcepath::String); @ Base ./loading.jl:854; [7] _require(pkg::Base.PkgId); @ Base ./loading.jl:1097; [8] require(uuidkey::Base.PkgId); @ Base ./loading.jl:1013; [9] require(into::Module, mod::Symbol); @ Base ./loading.jl:997; [10] top-level scope; @ ~/.julia/packages/MPI/08SPr/deps/deps.jl:8; [11] include(mod::Module, _path::String); @ Base ./Base.jl:418; [12] include(x::String); @ MPI ~/.julia/packages/MPI/08SPr/src/MPI.jl:1; [13] top-level scope; @ ~/.julia/packages/MPI/08SPr/src/MPI.jl:36; [14] include; @ ./Base.jl:418 [inlined]; [15] include_package_for_output(pkg::Base.PkgId, input::String, depot_path::Vector{String}, dl_load_path::Vector{String}, load_path::Vector{String}, concrete_deps::Vector{Pair{Base.PkgId, UInt64}}, source::String); @ Base ./loading.jl:1318; [16] top-level scope; @ none:1; [17] eval; @ ./boot.jl:373 [inlined]; [18] eval(x::Expr); @ Base.MainInclude ./client.jl:453; [19] top-level scope; @ none:1; during initialization of module MPICH_jll; in expression starting at /Users/sean/.julia/packages/MPI/08SPr/deps/deps.jl:1; ERROR: LoadError: Failed to precompile MPI [da04e1cc-30fd-572f-bb4f-1f8673147195] to /U",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2480:3362,load,loading,3362,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2480,1,['load'],['loading']
Performance,"e. I guess you're trying to couple an agent-based larvae model to an ocean model?. Oceananigans.jl should have the features you need I think: mainly Lagrangian particle tracking to advect the larvae around and arbitrary forced passive tracers for the odor from the reef?. I've actually been meaning to set up an example of an agent-based model coupled to Oceananigans.jl (simulating interacting ""microbes"" in the ocean). It's not fully complete yet but might give you some ideas of how to use Lagrangian particles as agents: https://github.com/CliMA/Oceananigans.jl/pull/1244. > If that works, I will implement a continuity equation to simulate the odor which comes from the reef and which is supposed to help the larvae to find back to the home reef. What is the continuity equation for the odor? It should be easy to advect the odor around as a passive tracer but just curious what the equation looks like. > Since I have found Oceananigans, which is also a CFD tool, I was wondering what you think the differences are between that and WaterLily.jl? Given my description of the project above, would you tend to use either of them?. I'm not super familiar with WaterLily.jl but it's definitely a very nice package (and a great README)!. I think WaterLily.jl has a more mature immersed boundary implementation if you need to have complex boundaries and is auto-diff friendly if you need to do some optimization/inference. @weymouth might be able to elaborate!. Oceananigans.jl has native support for Lagrangian particle tracking and might have more mature diagnostics (e.g. if you need to compute vorticity or other statistics) and more mature output writers (among other quality of life features). The examples in the docs should give a pretty good of the current set of mature features. E.g. this example shows how to add in a passive tracer for plankton and evolve it according to some (continuity?) equation: https://clima.github.io/OceananigansDocumentation/stable/generated/convecting_plankton/",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1438#issuecomment-793113703:1547,optimiz,optimization,1547,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1438#issuecomment-793113703,1,['optimiz'],['optimization']
Performance,"e.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global},Forcing{typeof(Oceananigans.zero_func),typeof(Oceananigans.zero_func),typeof(Oceananigans.zero_func),typeof(Oceananigans.zero_func),typeof(FS)}}}) at /data5/glwagner/.julia/packages/CUDAnative/9rZcJ/src/execution.jl:347; [10] cufunction(::Function, ::Type) at /data5/glwagner/.julia/packages/CUDAnative/9rZcJ/src/execution.jl:347; [11] macro expansion at /data5/glwagner/.julia/packages/GPUifyLoops/hBRid/src/GPUifyLoops.jl:113 [inlined]; [12] macro expansion at ./gcutils.jl:87 [inlined]; [13] #launch#46(::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64,Vararg{Int64,N} where N},Tuple{Symbol,Symbol},NamedTuple{(:threads, :blocks),Tuple{Tuple{Int64,Int64},Tuple{Int64,Int64,Int64}}}}, ::Function, ::GPUifyLoops.CUDA, ::typeof(Oceananigans.calculate_interior_source_terms!), ::RegularCartesianGrid{Float64,StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}}, ::Vararg{Any,N} where N) at /data5/glwagner/.julia/packages/GPUifyLoops/hBRid/src/GPUifyLoops.jl:110; [14] #launch at ./none:0 [inlined]; [15] macro expansion at /data5/glwagner/.julia/packages/GPUifyLoops/hBRid/src/GPUifyLoops.jl:54 [inlined]; [16] time_step!(::Model{GPU,RegularCartesianGrid{Float64,StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}},Oceananigans.TurbulenceClosures.ConstantAnisotropicDiffusivity{Float64},Float64}, ::Int64, ::Float64) at /archive1/glwagner/Projects/Oceananigans.jl/src/time_steppers.jl:74; [17] top-level scope at util.jl:156; [18] include at ./boot.jl:326 [inlined]; [19] include_relative(::Module, ::String) at ./loading.jl:1038; [20] include(::Module, ::String) at ./sysimg.jl:29; [21] include(::String) at ./client.jl:403; [22] top-level scope at none:0; in expression starting at /archive1/glwagner/Projects/Oceananigans.jl/sandbox/simple_forcing.jl:35; ```",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/248#issuecomment-496489468:7545,load,loading,7545,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/248#issuecomment-496489468,1,['load'],['loading']
Performance,"e.var""#968#969""{Base.PkgId})(); @ Base ./loading.jl:1974; [5] mkpidlock(f::Base.var""#968#969""{Base.PkgId}, at::String, pid::Int32; kwopts::@Kwargs{stale_age::Int64, wait::Bool}); @ FileWatching.Pidfile /glade/u/apps/casper/23.10/spack/opt/spack/julia/1.10.2/gcc/7.5.0/apod/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:93; [6] #mkpidlock#6 ; @ /glade/u/apps/casper/23.10/spack/opt/spack/julia/1.10.2/gcc/7.5.0/apod/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:88 [inlined]; [7] trymkpidlock(::Function, ::Vararg{Any}; kwargs::@Kwargs{stale_age::Int64}); @ FileWatching.Pidfile /glade/u/apps/casper/23.10/spack/opt/spack/julia/1.10.2/gcc/7.5.0/apod/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:111; [8] #invokelatest#2; @ ./essentials.jl:894 [inlined]; [9] invokelatest; @ ./essentials.jl:889 [inlined]; [10] maybe_cachefile_lock(f::Base.var""#968#969""{Base.PkgId}, pkg::Base.PkgId, srcpath::String; stale_age::Int64); @ Base ./loading.jl:2983; [11] maybe_cachefile_lock; @ ./loading.jl:2980 [inlined]; [12] _require(pkg::Base.PkgId, env::String); @ Base ./loading.jl:1970; [13] __require_prelocked(uuidkey::Base.PkgId, env::String); @ Base ./loading.jl:1812; [14] #invoke_in_world#3; @ ./essentials.jl:926 [inlined]; [15] invoke_in_world; @ ./essentials.jl:923 [inlined]; [16] _require_prelocked(uuidkey::Base.PkgId, env::String); @ Base ./loading.jl:1803; [17] macro expansion; @ ./loading.jl:1790 [inlined]; [18] macro expansion; @ ./lock.jl:267 [inlined]; [19] __require(into::Module, mod::Symbol); @ Base ./loading.jl:1753; [20] #invoke_in_world#3; @ ./essentials.jl:926 [inlined]; [21] invoke_in_world; @ ./essentials.jl:923 [inlined]; [22] require(into::Module, mod::Symbol); @ Base ./loading.jl:1746; [23] include; @ ./Base.jl:495 [inlined]; [24] include_package_for_output(pkg::Base.PkgId, input::String, depot_path::Vector{String}, dl_load_path::Vector{String}, load_path::Vector{String}, concrete_deps::Vector{Pair{Base.PkgId, UInt128}}, source::String); @ Base ./loadi",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2231635812:1751,load,loading,1751,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2231635812,1,['load'],['loading']
Performance,e/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; advect_lagrangian_particles! at /home/alir/atdepth/Oceananigans.jl/src/Models/LagrangianParticleTracking/lagrangian_particle_advection.jl:193; step_lagrangian_particles! at /home/alir/atdepth/Oceananigans.jl/src/Models/LagrangianParticleTracking/LagrangianParticleTracking.jl:143 [inlined]; step_lagrangian_particles! at /home/alir/atdepth/Oceananigans.jl/src/Models/HydrostaticFreeSurfaceModels/HydrostaticFreeSurfaceModels.jl:107 [inlined]; #time_step!#8 at /home/alir/atdepth/Oceananigans.jl/src/TimeSteppers/quasi_adams_bashforth_2.jl:124; time_step! at /home/alir/atdepth/Oceananigans.jl/src/TimeSteppers/quasi_adams_bashforth_2.jl:76; unknown function (ip: 0x7c00a0f12fbd); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; top-level scope at /home/alir/atdepth/Oceananigans.jl/particles_error.jl:37; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:925; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:877; ijl_toplevel_eval_in at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:985; eval at ./boot.jl:385 [inlined]; include_string at ./loading.jl:2076; _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; _include at ./loading.jl:2136; include at ./client.jl:489; unknown function (ip: 0x7c00f54ff855); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3852:4135,cache,cache,4135,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3852,1,['cache'],['cache']
Performance,"e9a09]; ERROR: LoadError: MPI.jl not properly configured, please run `Pkg.build(""MPI"")`.; Stacktrace:; [1] error(s::String); @ Base ./error.jl:33; [2] top-level scope; @ ~/.julia/packages/MPI/08SPr/src/MPI.jl:38; [3] include; @ ./Base.jl:418 [inlined]; [4] include_package_for_output(pkg::Base.PkgId, input::String, depot_path::Vector{String}, dl_load_path::Vector{String}, load_path::Vector{String}, concrete_deps::Vector{Pair{Base.PkgId, UInt64}}, source::String); @ Base ./loading.jl:1318; [5] top-level scope; @ none:1; [6] eval; @ ./boot.jl:373 [inlined]; [7] eval(x::Expr); @ Base.MainInclude ./client.jl:453; [8] top-level scope; @ none:1; in expression starting at /Users/sean/.julia/packages/MPI/08SPr/src/MPI.jl:1. caused by: LoadError: InitError: could not load library ""/Users/sean/.julia/artifacts/48a9a608db31268626d8b8d4d1272c3e7ccbf7d5/lib/libmpifort.12.dylib""; dlopen(/Users/sean/.julia/artifacts/48a9a608db31268626d8b8d4d1272c3e7ccbf7d5/lib/libmpifort.12.dylib, 0x0001): Library not loaded: @rpath/libquadmath.0.dylib; Referenced from: /Users/sean/.julia/artifacts/48a9a608db31268626d8b8d4d1272c3e7ccbf7d5/lib/libmpifort.12.dylib; Reason: tried: '/Users/sean/.julia/artifacts/48a9a608db31268626d8b8d4d1272c3e7ccbf7d5/lib/./libquadmath.0.dylib' (no such file), '/Users/sean/.julia/artifacts/48a9a608db31268626d8b8d4d1272c3e7ccbf7d5/lib/./libquadmath.0.dylib' (no such file), '/Applications/Julia-1.7.app/Contents/Resources/julia/lib/julia/libquadmath.0.dylib' (no such file), '/Applications/Julia-1.7.app/Contents/Resources/julia/bin/../lib/libquadmath.0.dylib' (no such file), '/usr/local/lib/libquadmath.0.dylib' (no such file), '/usr/lib/libquadmath.0.dylib' (no such file); Stacktrace:; [1] dlopen(s::String, flags::UInt32; throw_error::Bool); @ Base.Libc.Libdl ./libdl.jl:117; [2] dlopen(s::String, flags::UInt32); @ Base.Libc.Libdl ./libdl.jl:117; [3] macro expansion; @ ~/.julia/packages/JLLWrappers/QpMQW/src/products/library_generators.jl:54 [inlined]; [4] __init__(); @ MPIC",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2480:1978,load,loaded,1978,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2480,1,['load'],['loaded']
Performance,"e::Bool, cleanup::Bool, strip::Bool, validate::Bool, only_entry::Bool, parent_job::Nothing); @ GPUCompiler ~/.julia/packages/GPUCompiler/U36Ed/src/driver.jl:129; [12] codegen; @ ~/.julia/packages/GPUCompiler/U36Ed/src/driver.jl:110 [inlined]; [13] compile(target::Symbol, job::GPUCompiler.CompilerJob; libraries::Bool, toplevel::Bool, optimize::Bool, cleanup::Bool, strip::Bool, validate::Bool, only_entry::Bool); @ GPUCompiler ~/.julia/packages/GPUCompiler/U36Ed/src/driver.jl:106; [14] compile; @ ~/.julia/packages/GPUCompiler/U36Ed/src/driver.jl:98 [inlined]; [15] #40; @ ~/.julia/packages/AMDGPU/FdIJi/src/compiler/codegen.jl:140 [inlined]; [16] JuliaContext(f::AMDGPU.Compiler.var""#40#41""{GPUCompiler.CompilerJob{GPUCompiler.GCNCompilerTarget, AMDGPU.Compiler.HIPCompilerParams}}); @ GPUCompiler ~/.julia/packages/GPUCompiler/U36Ed/src/driver.jl:47; [17] hipcompile(job::GPUCompiler.CompilerJob); @ AMDGPU.Compiler ~/.julia/packages/AMDGPU/FdIJi/src/compiler/codegen.jl:139; [18] actual_compilation(cache::Dict{Any, AMDGPU.HIP.HIPFunction}, src::Core.MethodInstance, world::UInt64, cfg::GPUCompiler.CompilerConfig{GPUCompiler.GCNCompilerTarget, AMDGPU.Compiler.HIPCompilerParams}, compiler::typeof(AMDGPU.Compiler.hipcompile), linker::typeof(AMDGPU.Compiler.hiplink)); @ GPUCompiler ~/.julia/packages/GPUCompiler/U36Ed/src/execution.jl:125; [19] cached_compilation(cache::Dict{Any, AMDGPU.HIP.HIPFunction}, src::Core.MethodInstance, cfg::GPUCompiler.CompilerConfig{GPUCompiler.GCNCompilerTarget, AMDGPU.Compiler.HIPCompilerParams}, compiler::Function, linker::Function); @ GPUCompiler ~/.julia/packages/GPUCompiler/U36Ed/src/execution.jl:103; [20] macro expansion; @ ~/.julia/packages/AMDGPU/FdIJi/src/compiler/codegen.jl:107 [inlined]; [21] macro expansion; @ ./lock.jl:267 [inlined]; [22] hipfunction(f::GPUArrays.var""#6#7"", tt::Type{Tuple{AMDGPU.ROCKernelContext, AMDGPU.Device.ROCDeviceArray{Float64, 3, 1}, Float64}}; kwargs::@Kwargs{name::Nothing}); @ AMDGPU.Compiler ~/.julia/packages/AMD",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3468#issuecomment-1935971273:2838,cache,cache,2838,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3468#issuecomment-1935971273,2,['cache'],['cache']
Performance,"e:; >; > @benchmark CuArrays.@sync mean(Array(view(a, H:N+H, H:N+H, H:N+H)), dims=[1, 2]); >; > BenchmarkTools.Trial:; > memory estimate: 1.01 GiB; > allocs estimate: 250; > --------------; > minimum time: 684.013 ms (2.29% GC); > median time: 712.570 ms (6.28% GC); > mean time: 732.480 ms (8.79% GC); > maximum time: 807.437 ms (16.95% GC); > --------------; > samples: 7; > evals/sample: 1; >; > What this PR does:; >; > Nx, Ny, Nz = 512, 512, 512; > C = rand(Nx, Ny, Nz) |> CuArray; > Rx = zeros(Float64, 1, Ny, Nz) |> CuArray; > Rxy = zeros(Float64, 1, 1, Nz) |> CuArray; > @benchmark CuArrays.@sync @cuda threads=Nx blocks=(Ny, Nz) shmem=2*Nx*sizeof(eltype(C)) gpu_accumulate_xy!(Rxy, Rx, C, +); >; > BenchmarkTools.Trial:; > memory estimate: 2.88 KiB; > allocs estimate: 64; > --------------; > minimum time: 39.129 ms (0.00% GC); > median time: 39.245 ms (0.00% GC); > mean time: 39.248 ms (0.00% GC); > maximum time: 39.374 ms (0.00% GC); > --------------; > samples: 128; > evals/sample: 1; >; > Probably optimal performance:; >; > @benchmark CuArrays.@sync mean(a, dims=[1, 2]); >; > BenchmarkTools.Trial:; > memory estimate: 8.56 KiB; > allocs estimate: 220; > --------------; > minimum time: 7.426 ms (0.00% GC); > median time: 7.526 ms (0.00% GC); > mean time: 7.527 ms (0.00% GC); > maximum time: 8.817 ms (0.00% GC); > --------------; > samples: 663; > evals/sample: 1; >; > Resolves #186; > <https://github.com/climate-machine/Oceananigans.jl/issues/186>; > ------------------------------; > You can view, comment on, or merge this pull request online at:; >; > https://github.com/climate-machine/Oceananigans.jl/pull/352; > Commit Summary; >; > - Organize diagnostics a bit; > - Nuke super ancient and useless field summary diagnostic; > - Also nuke velocity divergence checker. We test this.; > - Use Dicts in NaN checker and give a sensible default; > - Add test for NaN checker; > - Architecture-dispatching zeros function for any 3D shape; > - Parallel cumulative sum CUDA kernel",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/352#issuecomment-520187010:2809,perform,performance,2809,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/352#issuecomment-520187010,1,['perform'],['performance']
Performance,eModels/HydrostaticFreeSurfaceModels.jl:107 [inlined]; #time_step!#8 at /home/alir/atdepth/Oceananigans.jl/src/TimeSteppers/quasi_adams_bashforth_2.jl:124; time_step! at /home/alir/atdepth/Oceananigans.jl/src/TimeSteppers/quasi_adams_bashforth_2.jl:76; unknown function (ip: 0x7c00a0f12fbd); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; top-level scope at /home/alir/atdepth/Oceananigans.jl/particles_error.jl:37; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:925; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:877; ijl_toplevel_eval_in at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:985; eval at ./boot.jl:385 [inlined]; include_string at ./loading.jl:2076; _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; _include at ./loading.jl:2136; include at ./client.jl:489; unknown function (ip: 0x7c00f54ff855); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; jl_apply at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/julia.h:1982 [inlined]; do_call at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:126; eval_value at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:223; eval_stmt_value at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:174 [inlined]; eval_body at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:617; jl_interpret_toplevel_th,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3852:4697,cache,cache,4697,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3852,1,['cache'],['cache']
Performance,"ean/.julia/artifacts/48a9a608db31268626d8b8d4d1272c3e7ccbf7d5/lib/./libquadmath.0.dylib' (no such file), '/Users/sean/.julia/artifacts/48a9a608db31268626d8b8d4d1272c3e7ccbf7d5/lib/./libquadmath.0.dylib' (no such file), '/Applications/Julia-1.7.app/Contents/Resources/julia/lib/julia/libquadmath.0.dylib' (no such file), '/Applications/Julia-1.7.app/Contents/Resources/julia/bin/../lib/libquadmath.0.dylib' (no such file), '/usr/local/lib/libquadmath.0.dylib' (no such file), '/usr/lib/libquadmath.0.dylib' (no such file); Stacktrace:; [1] dlopen(s::String, flags::UInt32; throw_error::Bool); @ Base.Libc.Libdl ./libdl.jl:117; [2] dlopen(s::String, flags::UInt32); @ Base.Libc.Libdl ./libdl.jl:117; [3] macro expansion; @ ~/.julia/packages/JLLWrappers/QpMQW/src/products/library_generators.jl:54 [inlined]; [4] __init__(); @ MPICH_jll ~/.julia/packages/MPICH_jll/dhUyI/src/wrappers/aarch64-apple-darwin-libgfortran5.jl:32; [5] _include_from_serialized(path::String, depmods::Vector{Any}); @ Base ./loading.jl:768; [6] _require_search_from_serialized(pkg::Base.PkgId, sourcepath::String); @ Base ./loading.jl:854; [7] _require(pkg::Base.PkgId); @ Base ./loading.jl:1097; [8] require(uuidkey::Base.PkgId); @ Base ./loading.jl:1013; [9] require(into::Module, mod::Symbol); @ Base ./loading.jl:997; [10] top-level scope; @ ~/.julia/packages/MPI/08SPr/deps/deps.jl:8; [11] include(mod::Module, _path::String); @ Base ./Base.jl:418; [12] include(x::String); @ MPI ~/.julia/packages/MPI/08SPr/src/MPI.jl:1; [13] top-level scope; @ ~/.julia/packages/MPI/08SPr/src/MPI.jl:36; [14] include; @ ./Base.jl:418 [inlined]; [15] include_package_for_output(pkg::Base.PkgId, input::String, depot_path::Vector{String}, dl_load_path::Vector{String}, load_path::Vector{String}, concrete_deps::Vector{Pair{Base.PkgId, UInt64}}, source::String); @ Base ./loading.jl:1318; [16] top-level scope; @ none:1; [17] eval; @ ./boot.jl:373 [inlined]; [18] eval(x::Expr); @ Base.MainInclude ./client.jl:453; [19] top-level scope; @ non",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2480:3147,load,loading,3147,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2480,1,['load'],['loading']
Performance,"eananigans in its dependencies:; │ - If you have Benchmarks checked out for development and have; │ added Oceananigans as a dependency but haven't updated your primary; │ environment's manifest file, try `Pkg.resolve()`.; │ - Otherwise you may need to report an issue with Benchmarks; └ Loading Oceananigans into Benchmarks from project dependency, future warnings for Benchmarks are suppressed.; ```; It'd be nice to fix that eventually. * The output is kind of annoying:. ```julia; [2022/03/13 13:17:49.875] INFO Benchmarking weak scaling nonhydrostatic model with Slab decomposition [N=(128, 128, 32), ranks=(1, 2, 1)]...; Invalid MIT-MAGIC-COOKIE-1 keyInvalid MIT-MAGIC-COOKIE-1 keyNo protocol specified; [ Info: Oceananigans will use 24 threads; [ Info: Oceananigans will use 24 threads; ┌ Warning: Package Benchmarks does not have Oceananigans in its dependencies:; │ - If you have Benchmarks checked out for development and have; │ added Oceananigans as a dependency but haven't updated your primary; │ environment's manifest file, try `Pkg.resolve()`.; │ - Otherwise you may need to report an issue with Benchmarks; └ Loading Oceananigans into Benchmarks from project dependency, future warnings for Benchmarks are suppressed.; ┌ Warning: Package Benchmarks does not have Oceananigans in its dependencies:; │ - If you have Benchmarks checked out for development and have; │ added Oceananigans as a dependency but haven't updated your primary; │ environment's manifest file, try `Pkg.resolve()`.; │ - Otherwise you may need to report an issue with Benchmarks; └ Loading Oceananigans into Benchmarks from project dependency, future warnings for Benchmarks are suppressed.; [ Info: Oceananigans will use 24 threads; [ Info: Oceananigans will use 24 threads; ```; We don't need to be told `N` times how many threads Oceananigans is using. * I think the `README.md` could be clearer and the scripts could have better names (eg which script runs the benchmark versus which script sets up the model)",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2338#issuecomment-1066145590:1476,Load,Loading,1476,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2338#issuecomment-1066145590,2,['Load'],['Loading']
Performance,"eananigans; Precompiling Oceananigans; 1 dependency successfully precompiled in 11 seconds. 143 already precompiled.; [ Info: Oceananigans will use 8 threads. julia> grid = RectilinearGrid(size=(1, 8, 8), extent=(1, 1, 1)); 1×8×8 RectilinearGrid{Float64, Periodic, Periodic, Bounded} on CPU with 3×3×3 halo; ├── Periodic x ∈ [0.0, 1.0) regularly spaced with Δx=1.0; ├── Periodic y ∈ [0.0, 1.0) regularly spaced with Δy=0.125; └── Bounded z ∈ [-1.0, 0.0] regularly spaced with Δz=0.125. julia> model = NonhydrostaticModel(; grid); warning: /Users/navid/.julia/packages/KernelAbstractions/WoCk1/src/extras/loopinfo.jl:28:0: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering; warning: /Users/navid/.julia/packages/KernelAbstractions/WoCk1/src/extras/loopinfo.jl:28:0: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering; warning: /Users/navid/.julia/packages/KernelAbstractions/WoCk1/src/extras/loopinfo.jl:28:0: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering; warning: /Users/navid/.julia/packages/KernelAbstractions/WoCk1/src/extras/loopinfo.jl:28:0: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering; warning: /Users/navid/.julia/packages/KernelAbstractions/WoCk1/src/extras/loopinfo.jl:28:0: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering; warning: /Users/navid/.julia/packages/KernelAbstractions/WoCk1/src/extras/loopin",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3403#issuecomment-1872469814:1123,optimiz,optimizer,1123,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3403#issuecomment-1872469814,2,"['optimiz', 'perform']","['optimizer', 'perform']"
Performance,ease-1-dot-10/src/julia.h:1982 [inlined]; do_call at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:126; eval_value at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:223; eval_stmt_value at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:174 [inlined]; eval_body at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:617; jl_interpret_toplevel_thunk at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:775; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:934; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:877; ijl_toplevel_eval_in at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:985; eval at ./boot.jl:385 [inlined]; include_string at ./loading.jl:2076; _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; _include at ./loading.jl:2136; include at ./Base.jl:495; jfptr_include_46447.1 at /orcd/data/raffaele/001/glwagner/Software/julia-1.10.5/lib/julia/sys.so (unknown line); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; exec_options at ./client.jl:318; _start at ./client.jl:552; jfptr__start_82798.1 at /orcd/data/raffaele/001/glwagner/Software/julia-1.10.5/lib/julia/sys.so (unknown line); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; jl_apply at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/julia.h:1982 [,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3878:4967,cache,cache,4967,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3878,1,['cache'],['cache']
Performance,"ecache; @ ./loading.jl:2340 [inlined]; [4] (::Base.var""#968#969""{Base.PkgId})(); @ Base ./loading.jl:1974; [5] mkpidlock(f::Base.var""#968#969""{Base.PkgId}, at::String, pid::Int32; kwopts::@Kwargs{stale_age::Int64, wait::Bool}); @ FileWatching.Pidfile /glade/u/apps/casper/23.10/spack/opt/spack/julia/1.10.2/gcc/7.5.0/apod/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:93; [6] #mkpidlock#6; @ /glade/u/apps/casper/23.10/spack/opt/spack/julia/1.10.2/gcc/7.5.0/apod/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:88 [inlined]; [7] trymkpidlock(::Function, ::Vararg{Any}; kwargs::@Kwargs{stale_age::Int64}); @ FileWatching.Pidfile /glade/u/apps/casper/23.10/spack/opt/spack/julia/1.10.2/gcc/7.5.0/apod/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:111; [8] #invokelatest#2; @ ./essentials.jl:894 [inlined]; [9] invokelatest; @ ./essentials.jl:889 [inlined]; [10] maybe_cachefile_lock(f::Base.var""#968#969""{Base.PkgId}, pkg::Base.PkgId, srcpath::String; stale_age::Int64); @ Base ./loading.jl:2983; [11] maybe_cachefile_lock; @ ./loading.jl:2980 [inlined]; [12] _require(pkg::Base.PkgId, env::String); @ Base ./loading.jl:1970; [13] __require_prelocked(uuidkey::Base.PkgId, env::String); @ Base ./loading.jl:1812; [14] #invoke_in_world#3; @ ./essentials.jl:926 [inlined]; [15] invoke_in_world; @ ./essentials.jl:923 [inlined]; [16] _require_prelocked(uuidkey::Base.PkgId, env::String); @ Base ./loading.jl:1803; [17] macro expansion; @ ./loading.jl:1790 [inlined]; [18] macro expansion; @ ./lock.jl:267 [inlined]; [19] __require(into::Module, mod::Symbol); @ Base ./loading.jl:1753; [20] #invoke_in_world#3; @ ./essentials.jl:926 [inlined]; [21] invoke_in_world; @ ./essentials.jl:923 [inlined]; [22] require(into::Module, mod::Symbol); @ Base ./loading.jl:1746; [23] include; @ ./Base.jl:495 [inlined]; [24] include_package_for_output(pkg::Base.PkgId, input::String, depot_path::Vector{String}, dl_load_path::Vector{String}, load_path::Vector{String}, concrete_deps::Vector{Pair{Base.P",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2231635812:4284,load,loading,4284,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2231635812,1,['load'],['loading']
Performance,"ed an issue about this but I can't seem to find it so maybe not? Either way this has been on my mind lately (and would help quite a bit with OceanParameterizations.jl) so I thought I'd write down some thoughts. # Plan?. The main feature would be the ability to construct `Field`s from output. Then the machinery of `Oceananigans.AbstractOperations` is available! Some extra quality-of-life features would allow for a more xarray-like experience such as named axis behavior (#457) through https://github.com/rafaqz/DimensionalData.jl. Maybe the easiest thing would be to first create a new field, `TimeSeriesField` or something with a `DimensionalData.DimArray` under the hood, that stores (x, y, z, t) data from a file and works with abstract operations?. # Some thoughts. 1. It's probably easiest to add support for constructing fields from JLD2 first then NetCDF.; 2. Complex abstract operations on the GPU can fail but most data analysis is done on the CPU so #1241 probably won't be an issue.; 3. While there is some overlap with xgcm support (#1334) these two issues are quite orthogonal. Why not both!; 4. We might have to find a way of lazily loading data from disk with DimensionalData.jl so we can analyze large datasets without loading the complete dataset into memory, but this could come later.; 5. One of the big features of xarray is that together with Dask you can analyze huge datasets that do not fit in memory using many cores quite quickly. This is probably not possible with Julia right now (maybe with Dagger.jl in the future), but it seems that this might be an edge case. Most users will probably be able to get by with lazily loading data from disk?. # Example?. I was thinking what would be a good example would be to showcase this feature. The example could show how to go from math -> setup -> simulation -> analysis -> results. So it would have to be pretty simple but with enough complexity to showcase the power of the framework. So probably no closing of TKE budgets...",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1493:2023,load,loading,2023,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1493,3,['load'],['loading']
Performance,"ed boundary that might cause this?. ```; [ Info: Initializing simulation...; [ Info: [0.00%], iteration: 0, time: 0.000; [ Info: ... simulation initialization complete (1.209 seconds); [ Info: Executing initial time step...; ERROR: CUDA error: an illegal memory access was encountered (code 700, ERROR_ILLEGAL_ADDRESS); Stacktrace:; [1] throw_api_error(res::CUDA.cudaError_enum); @ CUDA ~/.julia/packages/CUDA/DL5Zo/lib/cudadrv/error.jl:105; [2] query; @ ~/.julia/packages/CUDA/DL5Zo/lib/cudadrv/stream.jl:102 [inlined]; [3] synchronize(stream::CUDA.CuStream; blocking::Bool); @ CUDA ~/.julia/packages/CUDA/DL5Zo/lib/cudadrv/stream.jl:117; [4] synchronize (repeats 2 times); @ ~/.julia/packages/CUDA/DL5Zo/lib/cudadrv/stream.jl:117 [inlined]; [5] top-level scope; @ ~/.julia/packages/CUDA/DL5Zo/src/initialization.jl:54. caused by: LoadError: CUDA error: an illegal memory access was encountered (code 700, ERROR_ILLEGAL_ADDRESS); Stacktrace:; [1] throw_api_error(res::CUDA.cudaError_enum); @ CUDA ~/.julia/packages/CUDA/DL5Zo/lib/cudadrv/error.jl:105; [2] macro expansion; @ ~/.julia/packages/CUDA/DL5Zo/lib/cudadrv/error.jl:115 [inlined]; [3] cuCtxSynchronize(); @ CUDA ~/.julia/packages/CUDA/DL5Zo/lib/utils/call.jl:26; [4] device_synchronize; @ ~/.julia/packages/CUDA/DL5Zo/lib/cudadrv/context.jl:319 [inlined]; [5] CUDA.CuModule(data::Vector{UInt8}, options::Dict{CUDA.CUjit_option_enum, Any}); @ CUDA ~/.julia/packages/CUDA/DL5Zo/lib/cudadrv/module.jl:41; [6] CuModule; @ ~/.julia/packages/CUDA/DL5Zo/lib/cudadrv/module.jl:23 [inlined]; [7] cufunction_link(job::GPUCompiler.CompilerJob, compiled::NamedTuple{(:image, :entry, :external_gvars), Tuple{Vector{UInt8}, String, Vector{String}}}); @ CUDA ~/.julia/packages/CUDA/DL5Zo/src/compiler/execution.jl:442; [8] cached_compilation(cache::Dict{UInt64, Any}, job::GPUCompiler.CompilerJob, compiler::typeof(CUDA.cufunction_compile), linker::typeof(CUDA.cufunction_link)); @ GPUCompiler ~/.julia/packages/GPUCompiler/fG3xK/src/cache.jl:94; ...; ```",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2479:2223,cache,cache,2223,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2479,2,['cache'],['cache']
Performance,"ed to-do some rewriting of that to unpack like `getbc(bc::BoundaryCondition, ...) = getbc(bc.condition, ...)` and then modify all of the existing `getbc` methods. We can also either i) materialize whatever the user provides into a full boundary condition or ii) build `BoundaryCondition` on the fly inside the kernel (it would get compiled away probably anyways). We don't need to modify the existing `getbc` methods. > For e.g. gradient boundary conditions it feels like the ""boundary condition value"" being the gradient at the boundary has about as much meaning as the boundary condition value for an open boundary condition being the external state and then we do some matching to get the internal solution to approximate it. I agree, I think it's acceptable if the `condition` corresponds to the prescribed external state only, and the halo regions are determined by a calculation that additionally involves a scheme + the internal state. The main difference is where the matching calculation is performed --- either in `getbc`, or in `fill_halo_regions`. I don't think either choice is ""harder"" than the other. The difference and points to consider regard code clarity. That said, I see the advantages of generalizing `Open` rather than adding a new classification. Where does that put us? It looks like this PR has more code than we need (19 files changed?), if all we need to do is generalize `Open`. To generalize `Open`, we should only need to change a few files. Should we close this PR and start over? . I'd suggest starting from the very simple place of showing that one can provide a non-trivial external state, with no ""matching scheme"" (or whatever we want to call it), using a sponge layer. I think having that example will be very useful for demonstrating the advantage of different schemes. We may want two examples --- perhaps one constant inflow / constant outflow, and another example with a time-varying inflow/outflow. For the second example, a nice case might be to use an anal",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3482#issuecomment-1992386703:1109,perform,performed,1109,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3482#issuecomment-1992386703,1,['perform'],['performed']
Performance,"ed/v1.7/MPI/jl_AfEwik.; Stacktrace:; [1] error(s::String); @ Base ./error.jl:33; [2] compilecache(pkg::Base.PkgId, path::String, internal_stderr::IO, internal_stdout::IO, ignore_loaded_modules::Bool); @ Base ./loading.jl:1466; [3] compilecache(pkg::Base.PkgId, path::String); @ Base ./loading.jl:1410; [4] _require(pkg::Base.PkgId); @ Base ./loading.jl:1120; [5] require(uuidkey::Base.PkgId); @ Base ./loading.jl:1013; [6] require(into::Module, mod::Symbol); @ Base ./loading.jl:997; [7] include(mod::Module, _path::String); @ Base ./Base.jl:418; [8] include(x::String); @ Oceananigans ~/.julia/packages/Oceananigans/jmNfq/src/Oceananigans.jl:5; [9] top-level scope; @ ~/.julia/packages/Oceananigans/jmNfq/src/Oceananigans.jl:190; [10] include; @ ./Base.jl:418 [inlined]; [11] include_package_for_output(pkg::Base.PkgId, input::String, depot_path::Vector{String}, dl_load_path::Vector{String}, load_path::Vector{String}, concrete_deps::Vector{Pair{Base.PkgId, UInt64}}, source::Nothing); @ Base ./loading.jl:1318; [12] top-level scope; @ none:1; [13] eval; @ ./boot.jl:373 [inlined]; [14] eval(x::Expr); @ Base.MainInclude ./client.jl:453; [15] top-level scope; @ none:1; in expression starting at /Users/sean/.julia/packages/Oceananigans/jmNfq/src/Distributed/Distributed.jl:1; in expression starting at /Users/sean/.julia/packages/Oceananigans/jmNfq/src/Oceananigans.jl:1; ERROR: Failed to precompile Oceananigans [9e8cae18-63c1-5223-a75c-80ca9d6e9a09] to /Users/sean/.julia/compiled/v1.7/Oceananigans/jl_sx8KhM.; Stacktrace:; [1] error(s::String); @ Base ./error.jl:33; [2] compilecache(pkg::Base.PkgId, path::String, internal_stderr::IO, internal_stdout::IO, ignore_loaded_modules::Bool); @ Base ./loading.jl:1466; [3] compilecache(pkg::Base.PkgId, path::String); @ Base ./loading.jl:1410; [4] _require(pkg::Base.PkgId); @ Base ./loading.jl:1120; [5] require(uuidkey::Base.PkgId); @ Base ./loading.jl:1013; [6] require(into::Module, mod::Symbol); @ Base ./loading.jl:997. Any help would be greatly",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2480:5386,load,loading,5386,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2480,1,['load'],['loading']
Performance,"ed]; [22] require(into::Module, mod::Symbol); @ Base ./loading.jl:1746; [23] include; @ ./Base.jl:495 [inlined]; [24] include_package_for_output(pkg::Base.PkgId, input::String, depot_path::Vector{String}, dl_load_path::Vector{String}, load_path::Vector{String}, concrete_deps::Vector{Pair{Base.PkgId, UInt128}}, source::String); @ Base ./loading.jl:2222; [25] top-level scope; @ stdin:3; in expression starting at /glade/u/home/knudsenl/.julia/packages/Oceananigans/M82LU/src/Oceananigans.jl:1; in expression starting at stdin:3; ERROR: LoadError: Failed to precompile Oceananigans [9e8cae18-63c1-5223-a75c-80ca9d6e9a09] to ""/glade/u/home/knudsenl/.julia/compiled/v1.10/Oceananigans/jl_k7YOZN"".; Stacktrace:; [1] error(s::String); @ Base ./error.jl:35; [2] compilecache(pkg::Base.PkgId, path::String, internal_stderr::IO, internal_stdout::IO, keep_loaded_modules::Bool); @ Base ./loading.jl:2468; [3] compilecache; @ ./loading.jl:2340 [inlined]; [4] (::Base.var""#968#969""{Base.PkgId})(); @ Base ./loading.jl:1974; [5] mkpidlock(f::Base.var""#968#969""{Base.PkgId}, at::String, pid::Int32; kwopts::@Kwargs{stale_age::Int64, wait::Bool}); @ FileWatching.Pidfile /glade/u/apps/casper/23.10/spack/opt/spack/julia/1.10.2/gcc/7.5.0/apod/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:93; [6] #mkpidlock#6; @ /glade/u/apps/casper/23.10/spack/opt/spack/julia/1.10.2/gcc/7.5.0/apod/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:88 [inlined]; [7] trymkpidlock(::Function, ::Vararg{Any}; kwargs::@Kwargs{stale_age::Int64}); @ FileWatching.Pidfile /glade/u/apps/casper/23.10/spack/opt/spack/julia/1.10.2/gcc/7.5.0/apod/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:111; [8] #invokelatest#2; @ ./essentials.jl:894 [inlined]; [9] invokelatest; @ ./essentials.jl:889 [inlined]; [10] maybe_cachefile_lock(f::Base.var""#968#969""{Base.PkgId}, pkg::Base.PkgId, srcpath::String; stale_age::Int64); @ Base ./loading.jl:2983; [11] maybe_cachefile_lock; @ ./loading.jl:2980 [inlined]; [12] _require(pkg::Bas",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2231635812:5990,load,loading,5990,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2231635812,1,['load'],['loading']
Performance,"ed]; │ [12] _require(pkg::Base.PkgId, env::Nothing); │ @ Base ./loading.jl:1970; │ [13] __require_prelocked(uuidkey::Base.PkgId, env::Nothing); │ @ Base ./loading.jl:1812; │ [14] #invoke_in_world#3; │ @ ./essentials.jl:926 [inlined]; │ [15] invoke_in_world; │ @ ./essentials.jl:923 [inlined]; │ [16] _require_prelocked; │ @ ./loading.jl:1803 [inlined]; │ [17] _require_prelocked; │ @ ./loading.jl:1802 [inlined]; │ [18] run_extension_callbacks(extid::Base.ExtensionId); │ @ Base ./loading.jl:1295; │ [19] run_extension_callbacks(pkgid::Base.PkgId); │ @ Base ./loading.jl:1330; │ [20] run_package_callbacks(modkey::Base.PkgId); │ @ Base ./loading.jl:1164; │ [21] _tryrequire_from_serialized(modkey::Base.PkgId, path::String, ocachepath::String, sourcepath::String, depmods::Vector{Any}); │ @ Base ./loading.jl:1487; │ [22] _require_search_from_serialized(pkg::Base.PkgId, sourcepath::String, build_id::UInt128); │ @ Base ./loading.jl:1574; │ [23] _require(pkg::Base.PkgId, env::String); │ @ Base ./loading.jl:1938; │ [24] __require_prelocked(uuidkey::Base.PkgId, env::String); │ @ Base ./loading.jl:1812; │ [25] #invoke_in_world#3; │ @ ./essentials.jl:926 [inlined]; │ [26] invoke_in_world; │ @ ./essentials.jl:923 [inlined]; │ [27] _require_prelocked(uuidkey::Base.PkgId, env::String); │ @ Base ./loading.jl:1803; │ [28] macro expansion; │ @ ./loading.jl:1790 [inlined]; │ [29] macro expansion; │ @ ./lock.jl:267 [inlined]; │ [30] __require(into::Module, mod::Symbol); │ @ Base ./loading.jl:1753; │ [31] #invoke_in_world#3; │ @ ./essentials.jl:926 [inlined]; │ [32] invoke_in_world; │ @ ./essentials.jl:923 [inlined]; │ [33] require(into::Module, mod::Symbol); │ @ Base ./loading.jl:1746; │ [34] eval; │ @ ./boot.jl:385 [inlined]; │ [35] eval_user_input(ast::Any, backend::REPL.REPLBackend, mod::Module); │ @ REPL ~/julia-1.10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:150; │ [36] repl_backend_loop(backend::REPL.REPLBackend, get_module::Function); │ @ REPL ~/julia-1.10/usr/share/julia/stdlib/v1.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3686#issuecomment-2272432528:3257,load,loading,3257,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3686#issuecomment-2272432528,1,['load'],['loading']
Performance,"elAbstractions v0.7.2; [da04e1cc] MPI v0.19.2; [85f8d34a] NCDatasets v0.12.4; [6fe1bfb0] OffsetArrays v1.12.6; [bac558e1] OrderedCollections v1.4.1; [0e08944d] PencilArrays v0.17.5; [4a48f351] PencilFFTs v0.14.0; [6038ab10] Rotations v1.3.1; [d496a93d] SeawaterPolynomials v0.2.3; [09ab397b] StructArrays v0.6.11; [bc48ee85] Tullio v0.3.4; [ade2ca70] Dates; [b77e0a4c] InteractiveUtils; [37e2e46d] LinearAlgebra; [56ddb016] Logging; [44cfe95a] Pkg; [de0858da] Printf; [9a3f8284] Random; [2f01184e] SparseArrays; [10745b16] Statistics. (Oceananigans) pkg> precompile; Precompiling project...; ✗ Oceananigans; 0 dependencies successfully precompiled in 11 seconds (99 already precompiled). ERROR: The following 1 direct dependency failed to precompile:. Oceananigans [9e8cae18-63c1-5223-a75c-80ca9d6e9a09]. Failed to precompile Oceananigans [9e8cae18-63c1-5223-a75c-80ca9d6e9a09] to /Users/navid/.julia/compiled/v1.6/Oceananigans/jl_Z5b4Xf.; ERROR: LoadError: LoadError: LoadError: InitError: UndefVarError: libamgxsh not defined; Stacktrace:; [1] getproperty; @ ./Base.jl:26 [inlined]; [2] __init__(); @ AMGX ~/.julia/packages/AMGX/GFHHN/src/AMGX.jl:30; [3] _include_from_serialized(path::String, depmods::Vector{Any}); @ Base ./loading.jl:696; [4] _require_search_from_serialized(pkg::Base.PkgId, sourcepath::String); @ Base ./loading.jl:782; [5] _require(pkg::Base.PkgId); @ Base ./loading.jl:1020; [6] require(uuidkey::Base.PkgId); @ Base ./loading.jl:936; [7] require(into::Module, mod::Symbol); @ Base ./loading.jl:923; [8] include(mod::Module, _path::String); @ Base ./Base.jl:384; [9] include(x::String); @ Oceananigans.Solvers ~/Research/OC.jl/src/Solvers/Solvers.jl:1; [10] top-level scope; @ ~/Research/OC.jl/src/Solvers/Solvers.jl:48; [11] include(mod::Module, _path::String); @ Base ./Base.jl:384; [12] include(x::String); @ Oceananigans ~/Research/OC.jl/src/Oceananigans.jl:5; [13] top-level scope; @ ~/Research/OC.jl/src/Oceananigans.jl:195; [14] include; @ ./Base.jl:384 [inlined]; [15]",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2688#issuecomment-1217694987:1872,Load,LoadError,1872,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2688#issuecomment-1217694987,3,['Load'],['LoadError']
Performance,"eldBoundaryConditions{BoundaryCondition{Oceananigans.BoundaryConditions.Periodic, Nothing}, BoundaryCondition{Oceananigans.BoundaryConditions.Periodic, Nothing}, BoundaryCondition{Oceananigans.BoundaryConditions.Periodic, Nothing}, BoundaryCondition{Oceananigans.BoundaryConditions.Periodic, Nothing}, Nothing, Nothing, BoundaryCondition{Oceananigans.BoundaryConditions.Flux, Nothing}}, Nothing}, Field{Center, Center, Center, Nothing, RectilinearGrid{Float64, Periodic, Periodic, Bounded, Float64, Float64, OffsetArrays.OffsetVector{Float64, Vector{Float64}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVector{Float64, Vector{Float64}}, CPU}, Tuple{Colon, Colon, Colon}, OffsetArrays.OffsetArray{Float64, 3, Array{Float64, 3}}, Float64, FieldBoundaryConditions{BoundaryCondition{Oceananigans.BoundaryConditions.Periodic, Nothing}, BoundaryCondition{Oceananigans.BoundaryConditions.Periodic, Nothing}, BoundaryCondition{Oceananigans.BoundaryConditions.Periodic, Nothing}, BoundaryCondition{Oceananigans.BoundaryConditions.Periodic, Nothing}, BoundaryCondition{Oceananigans.BoundaryConditions.Flux, Nothing}, BoundaryCondition{Oceananigans.BoundaryConditions.Flux, Nothing}, BoundaryCondition{Oceananigans.BoundaryConditions.Flux, Nothing}}, Nothing}}}, Nothing}, Float64, Float64, OrderedCollections.OrderedDict{Symbol, Oceananigans.AbstractDiagnostic}, OrderedCollections.OrderedDict{Symbol, Oceananigans.AbstractOutputWriter}, OrderedCollections.OrderedDict{Symbol, Callback}}); @ Oceananigans.Simulations ~/.julia/packages/Oceananigans/JIHfS/src/Simulations/run.jl:88; [10] top-level scope; @ In[2]:61; [11] eval; @ ./boot.jl:373 [inlined]; [12] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String); @ Base ./loading.jl:1196. ```",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2395#issuecomment-1083606906:117622,load,loading,117622,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2395#issuecomment-1083606906,1,['load'],['loading']
Performance,"em like it'd be nice to have a macro that a) prints normally for non-mpi and b) prints just from rank 0 if using MPI. That way we only have to change `arch` when switching from single-process runs to distributed runs.; * We need `Base.summary(::MultiArch)` so logs don't get mutilated; * How should reductions behave? Should `mean` over all dims reduce everything, or should we do a local reduction (as we do now?) Perhaps we want a special `mean` for `DistributedField`, and `mean(interior(field))` can still be used for local reductions? We already have to redefine any reductions on `Field`, so it makes sense that we further extend for `Field` on `MultiArch`; * `ReducedField` across the partition is also hard. We can reduce only locally, which could be fine for output in some cases. But if `ReducedField` are used in `AbstractOperation` we clearly need to gather and scatter for that to work. Probably the right thing is to implement gather and scatter by default, and then to add features for ""local `ReducedField` that could maybe be used to optimize I/O performance for the biggest problems. Note: a macro / logger manipulation that avoids ""extra"" logging for distributed simulations might actually be essential because Oceananigans is pretty chatty:. ```; ┌ Warning: defaulting to uniform WENO scheme with Float64 precision, use WENO5(grid = grid) if this was not intended; └ @ Oceananigans.Advection ~/Projects/Oceananigans.jl/src/Advection/weno_fifth_order.jl:144; ┌ Warning: defaulting to uniform WENO scheme with Float64 precision, use WENO5(grid = grid) if this was not intended; └ @ Oceananigans.Advection ~/Projects/Oceananigans.jl/src/Advection/weno_fifth_order.jl:144; [ Info: Initializing simulation...; [ Info: Initializing simulation...; [ Info: Iteration: 0, time: 0 seconds; [ Info: Rank 1: max|ζ|: 7.80e+01, max(e): 2.46e-01; [ Info: Rank 0: max|ζ|: 7.58e+01, max(e): 2.31e-01; [ Info: ... simulation initialization complete (9.536 seconds); [ Info: Executing initial time s",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2349:2130,optimiz,optimize,2130,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2349,2,"['optimiz', 'perform']","['optimize', 'performance']"
Performance,"emoving `v` as a background creates oscillations. @glwagner have you checked? If not, let's wait for the video to see what happens... > * I reduced the domain aspect ratio to 400 x 100 because based on the visualization it seemed the domain didn't need to be so wide. This lets us increase the resolution and reduce the diffusivity, which is neat. It's a bit more turbulent now. Good move :+1: . > * Note using a tuple for `ĝ` rather than `Array` means it can be used as a parameter on the GPU, so that's probably preferred. Arrays are needed only if we need to mutate elements or perform linear algebra.; > * Don't import CUDA because the example wasn't GPU friendly anyways (if you like, we can make it GPU friendly but I don't think it should be ""partially"" GPU friendly since it just makes the code more complicated). I tried to make it 100% GPU friendly actually (although I didn't test it). You already pointed out one flaw, which is the use of an array instead of a tuple for the unit vertical vector. > * When I try to run the example multiple times I get `ERROR: LoadError: NetCDF error: Permission denied (NetCDF error code: 13)`. How can we avoid this error? I think it's important that users can easily change parameters and re-run without having to manually delete a file; this is key to productivity. This only happens with me when the NetCDF file is open by another program (generally python in my case) while Julia is trying to write to it. Was that the case? If so, that's the usual behavior afaik and I don't think we can do anything on our end except inform users about it. > * This is a great inexpensive example. I do wonder if we should make it 3D with an LES closure?. If we can get away with the computational cost I see no problem there. I just made this one 2D because it runs faster. > * Can we tilt the visualization? It's disorienting to be looking at the flow at an angle of 3 degrees. I tried to do that but couldn't make it work in Julia. Feel free to give it a shot!",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2333#issuecomment-1065266138:1356,Load,LoadError,1356,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2333#issuecomment-1065266138,1,['Load'],['LoadError']
Performance,"empt to access 1-element Vector{Float64} at index [2]; Stacktrace:; [1] getindex; @ ./array.jl:805 [inlined]; [2] g(a::Vector{Float64}, first::Bool); @ Main ./REPL[11]:1; [3] top-level scope; @ REPL[12]:1. julia> g(a, false); ERROR: BoundsError: attempt to access 1-element Vector{Float64} at index [2]; Stacktrace:; [1] getindex; @ ./array.jl:805 [inlined]; [2] g(a::Vector{Float64}, first::Bool); @ Main ./REPL[11]:1; [3] top-level scope; @ REPL[13]:1; ```. `ifelse` is _not_ short-circuiting --- _both_ branches are executed, even though only the correct value is returned:. ```julia; julia> b = rand(2); 2-element Vector{Float64}:; 0.5340042876487958; 0.7031634999748222. julia> g(b, true); 0.5340042876487958. julia> g(b, false); 0.7031634999748222; ```. It's easier for the compiler to optimize code that involves `ifelse`, especially on the GPU. The reason is that it's allowed to execute all code on both branches. If we use short-circuiting logic, then I guess many optimizations are not possible, because execution on one branch or another must be completely excluded. Some of this is discussed here: https://discourse.julialang.org/t/multiplying-by-booleans-faster-than-if-else/64117. > What about @inline?. This a little hazier. The compiler decides based on a heuristic whether or not to ""inline"" a function (meaning, rather than compiling code for a function independently and jumping to that code at the right moment, it combines the function code with the code that calls the function). We want to inline everything basically, so that every tendency evaluation involves evaluating one giant function. Inlining lets LLVM magic optimize our code to the highest degree (at least that's my impression). For whatever reason the compiler often decides _not_ to inline our functions unless we specifically annotate them. So it seems we probably need to add `@inline` to every ""hot"" function that's called in a kernel, at every grid point (like a forcing function or boundary condition functi",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2336#issuecomment-1066115583:2049,optimiz,optimizations,2049,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2336#issuecomment-1066115583,1,['optimiz'],['optimizations']
Performance,"ent/.julia-2581/packages/PencilArrays/DTEhf/src/PencilArrays.jl:12; &nbsp; | [13] include(::Function, ::Module, ::String) at ./Base.jl:380; &nbsp; | [14] include(::Module, ::String) at ./Base.jl:368; &nbsp; | [15] top-level scope at none:2; &nbsp; | [16] eval at ./boot.jl:347 [inlined]; &nbsp; | [17] eval(::Expr) at ./client.jl:467; &nbsp; | [18] top-level scope at ./none:3; &nbsp; | in expression starting at /storage7/buildkite-agent/.julia-2581/packages/PencilArrays/DTEhf/src/Pencils/Pencils.jl:7; &nbsp; | in expression starting at /storage7/buildkite-agent/.julia-2581/packages/PencilArrays/DTEhf/src/PencilArrays.jl:12; &nbsp; | ERROR: LoadError: Failed to precompile PencilArrays [0e08944d-e94e-41b1-9406-dcf66b6a9d2e] to /storage7/buildkite-agent/.julia-2581/compiled/v1.5/PencilArrays/yKKUy_zV1Ut.ji.; &nbsp; | Stacktrace:; &nbsp; | [1] error(::String) at ./error.jl:33; &nbsp; | [2] compilecache(::Base.PkgId, ::String) at ./loading.jl:1305; &nbsp; | [3] _require(::Base.PkgId) at ./loading.jl:1030; &nbsp; | [4] require(::Base.PkgId) at ./loading.jl:928; &nbsp; | [5] require(::Module, ::Symbol) at ./loading.jl:923; &nbsp; | [6] include(::Function, ::Module, ::String) at ./Base.jl:380; &nbsp; | [7] include(::Module, ::String) at ./Base.jl:368; &nbsp; | [8] top-level scope at none:2; &nbsp; | [9] eval at ./boot.jl:347 [inlined]; &nbsp; | [10] eval(::Expr) at ./client.jl:467; &nbsp; | [11] top-level scope at ./none:3; &nbsp; | in expression starting at /storage7/buildkite-agent/.julia-2581/packages/PencilFFTs/Xwxei/src/PencilFFTs.jl:11; &nbsp; | ERROR: could not load library ""/storage7/buildkite-agent/julia-1.5.4/lib/julia/sys.so""; &nbsp; | /storage7/buildkite-agent/julia-1.5.4/lib/julia/sys.so: ELF load command past end of file; &nbsp; | ERROR: could not load library ""/storage7/buildkite-agent/julia-1.5.4/lib/julia/sys.so""; &nbsp; | /storage7/buildkite-agent/julia-1.5.4/lib/julia/sys.so: ELF load command past end of file; &nbsp; | ERROR: could not load library ""/storage",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1662#issuecomment-843325731:5574,load,loading,5574,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1662#issuecomment-843325731,1,['load'],['loading']
Performance,"ently, this recursion starts with. https://github.com/CliMA/Oceananigans.jl/blob/3c86d8f37a3bf5ff050e233f7b946685f9057c26/src/TurbulenceClosures/closure_tuples.jl#L45-L47. which calls itself and terminates at the end points. https://github.com/CliMA/Oceananigans.jl/blob/3c86d8f37a3bf5ff050e233f7b946685f9057c26/src/TurbulenceClosures/closure_tuples.jl#L33-L34. and. https://github.com/CliMA/Oceananigans.jl/blob/3c86d8f37a3bf5ff050e233f7b946685f9057c26/src/TurbulenceClosures/closure_tuples.jl#L36-L38. However, this pattern does _not_ compile on the GPU (which is why we hard code the 2- and 3-tuple cases to support these on the GPU). The reason is a compiler heuristic that aborts inlining when self-recursion is encountered (eg a function is called within itself). To avoid this, I think we can use an ""outer-inner"" form whereby the outer function. ```julia; ∇_dot_qᶜ(i, j, k, grid::AbstractGrid, closures::Tuple, c, iᶜ, clock, Ks, args...); ```. unpacks one element, calls itself,. ```julia; ∇_dot_qᶜ(i, j, k, grid, closures[1], c, iᶜ, clock, Ks[1], args...); ```. and handles the rest of the elements with an inner function. ```julia; inner_∇_dot_qᶜ(i, j, k, grid, closures[2:end], c, iᶜ, clock, Ks[2:end], args...); ```. Or, something like that... getting this right might require a little trial and error. This is similar to [a pattern implemented in `ClimaCore.jl`](https://github.com/CliMA/ClimaCore.jl/blob/f804a86de772437e93c82d2c3dfc56920a94d433/src/interface.jl#L31):. ```julia; @inline column(x, inds...) = x; @inline column(tup::Tuple, inds...) = column_args(tup, inds...). # Recursively call column() on broadcast arguments in a way that is statically reducible by the optimizer; # see Base.Broadcast.preprocess_args; @inline column_args(args::Tuple, inds...) =; (column(args[1], inds...), column_args(Base.tail(args), inds...)...); @inline column_args(args::Tuple{Any}, inds...) = (column(args[1], inds...),); @inline column_args(args::Tuple{}, inds...) = (); ```. cc @jakebolewski",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2101:1856,optimiz,optimizer,1856,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2101,1,['optimiz'],['optimizer']
Performance,"ependencies successfully precompiled in 41 seconds; 4 dependencies errored. To see a full report either run `import Pkg; Pkg.precompile()` or load the packages. (2) - using Oceananigans is looking for MPI, but to my knowledge if I want to run on my computer, it shouldn't need MPI, like MITgcm using serial instead of parallel. But I don't know where to change the setting? This is what happens when I call using Oceananigans:. julia> using Oceananigans; [ Info: Precompiling Oceananigans [9e8cae18-63c1-5223-a75c-80ca9d6e9a09]; ERROR: LoadError: MPI.jl not properly configured, please run `Pkg.build(""MPI"")`.; Stacktrace:; [1] error(s::String); @ Base ./error.jl:33; [2] top-level scope; @ ~/.julia/packages/MPI/08SPr/src/MPI.jl:38; [3] include; @ ./Base.jl:418 [inlined]; [4] include_package_for_output(pkg::Base.PkgId, input::String, depot_path::Vector{String}, dl_load_path::Vector{String}, load_path::Vector{String}, concrete_deps::Vector{Pair{Base.PkgId, UInt64}}, source::String); @ Base ./loading.jl:1318; [5] top-level scope; @ none:1; [6] eval; @ ./boot.jl:373 [inlined]; [7] eval(x::Expr); @ Base.MainInclude ./client.jl:453; [8] top-level scope; @ none:1; in expression starting at /Users/sean/.julia/packages/MPI/08SPr/src/MPI.jl:1. caused by: LoadError: InitError: could not load library ""/Users/sean/.julia/artifacts/48a9a608db31268626d8b8d4d1272c3e7ccbf7d5/lib/libmpifort.12.dylib""; dlopen(/Users/sean/.julia/artifacts/48a9a608db31268626d8b8d4d1272c3e7ccbf7d5/lib/libmpifort.12.dylib, 0x0001): Library not loaded: @rpath/libquadmath.0.dylib; Referenced from: /Users/sean/.julia/artifacts/48a9a608db31268626d8b8d4d1272c3e7ccbf7d5/lib/libmpifort.12.dylib; Reason: tried: '/Users/sean/.julia/artifacts/48a9a608db31268626d8b8d4d1272c3e7ccbf7d5/lib/./libquadmath.0.dylib' (no such file), '/Users/sean/.julia/artifacts/48a9a608db31268626d8b8d4d1272c3e7ccbf7d5/lib/./libquadmath.0.dylib' (no such file), '/Applications/Julia-1.7.app/Contents/Resources/julia/lib/julia/libquadmath.0.dylib' (no",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2480:1453,load,loading,1453,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2480,1,['load'],['loading']
Performance,"eration=10). run!(simulation); ```. This (and way more complex examples) runs fine on the CPU but when I run that on the GPU I get:. ```; [ Info: Executing initial time step...; ERROR: LoadError: CUDA error: device kernel image is invalid (code 200, ERROR_INVALID_IMAGE). Stacktrace:; [1] CUDA.CuModule(data::Vector{UInt8}, options::Dict{CUDA.CUjit_option_enum, Any}); @ CUDA /glade/work/tomasc/.julia/packages/CUDA/Ey3w2/lib/cudadrv/module.jl:58; [2] CuModule; @ /glade/work/tomasc/.julia/packages/CUDA/Ey3w2/lib/cudadrv/module.jl:23 [inlined]; [3] cufunction_link(job::GPUCompiler.CompilerJob, compiled::NamedTuple{(:image, :entry, :external_gvars), Tuple{Vector{UInt8}, String, Vector{String}}}); @ CUDA /glade/work/tomasc/.julia/packages/CUDA/Ey3w2/src/compiler/execution.jl:481; [4] cached_compilation(cache::Dict{UInt64, Any}, job::GPUCompiler.CompilerJob, compiler::typeof(CUDA.cufunction_compile), linker::typeof(CUDA.cufunction_link)); @ GPUCompiler /glade/work/tomasc/.julia/packages/GPUCompiler/qdoh1/src/cache.jl:95; [5] cufunction(f::typeof(Cassette.overdub), tt::Type{Tuple{Cassette.Context{nametype(CUDACtx), Nothing, Nothing, KernelAbstractions.var""##PassType#312"", Nothing, Cassette.DisableHooks}, typeof(Oceananigans.Models.NonhydrostaticModels.gpu_calculate_Gu!), KernelAbstractions.CompilerMetadata{KernelAbstractions.NDIteration.StaticSize{(8, 8, 6)}, KernelAbstractions.NDIteration.DynamicCheck, Nothing, Nothing, KernelAbstractions.NDIteration.NDRange{3, KernelAbstractions.NDIteration.StaticSize{(1, 1, 6)}, KernelAbstractions.NDIteration.StaticSize{(16, 16, 1)}, Nothing, Nothing}}, OffsetArrays.OffsetArray{Float64, 3, CUDA.CuDeviceArray{Float64, 3, 1}}, RectilinearGrid{Float64, Periodic, Periodic, Bounded, Float64, Float64, OffsetArrays.OffsetVector{Float64, CUDA.CuDeviceVector{Float64, 1}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2869:2374,cache,cache,2374,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2869,1,['cache'],['cache']
Performance,eredFourthOrder │ 17.309 ms │ 17.419 ms │ 18.107 ms │ 24.384 ms │ 2.71 MiB │ 27650 │ 10 │ ; │ GPU │ CenteredSecondOrder │ 10.369 ms │ 11.588 ms │ 11.472 ms │ 11.642 ms │ 2.53 MiB │ 16296 │ 10 │; │ GPU │ UpwindBiasedFifthOrder │ 19.561 ms │ 19.675 ms │ 20.975 ms │ 32.694 ms │ 2.77 MiB │ 32028 │ 10 │ ; │ GPU │ UpwindBiasedThirdOrder │ 16.131 ms │ 16.211 ms │ 16.806 ms │ 22.239 ms │ 2.68 MiB │ 25594 │ 10 │; │ GPU │ WENO5 │ 382.916 ms │ 385.558 ms │ 385.368 ms │ 386.709 ms │ 13.21 MiB │ 715860 │ 10 │; └───────────────┴────────────────────────┴────────────┴────────────┴────────────┴────────────┴───────────┴────────┴─────────┘; ```. ```; Advection schemes CPU to GPU speedup ; ┌────────────────────────┬─────────┬─────────┬─────────┐ ; │ Schemes │ speedup │ memory │ allocs │ ; ├────────────────────────┼─────────┼─────────┼─────────┤ ; │ CenteredFourthOrder │ 88.7159 │ 1.6849 │ 13.1918 │ ; │ CenteredSecondOrder │ 89.3514 │ 1.57709 │ 7.77481 │ ; │ UpwindBiasedFifthOrder │ 114.4 │ 1.72647 │ 15.2805 │ ; │ UpwindBiasedThirdOrder │ 98.3274 │ 1.66538 │ 12.2109 │ ; │ WENO5 │ 16.4404 │ 8.22094 │ 341.536 │ ; └────────────────────────┴─────────┴─────────┴─────────┘; ```. # Turbulence closure benchmarks. ```; Turbulence closures relative performance (GPU); ┌───────────────┬──────────────────────────────────┬──────────┬─────────┬─────────┐; │ Architectures │ Closures │ slowdown │ memory │ allocs │; ├───────────────┼──────────────────────────────────┼──────────┼─────────┼─────────┤; │ GPU │ AnisotropicBiharmonicDiffusivity │ 1.5313 │ 1.03189 │ 1.54697 │; │ GPU │ AnisotropicDiffusivity │ 1.05623 │ 1.00582 │ 1.01779 │; │ GPU │ AnisotropicMinimumDissipation │ 1.46265 │ 1.19908 │ 1.26817 │; │ GPU │ IsotropicDiffusivity │ 1.13134 │ 1.00607 │ 1.07995 │; │ GPU │ Nothing │ 1.0 │ 1.0 │ 1.0 │; │ GPU │ SmagorinskyLilly │ 1.41905 │ 1.30373 │ 1.18683 │; │ GPU │ TwoDimensionalLeith │ 1.11312 │ 1.06941 │ 1.06147 │; └───────────────┴──────────────────────────────────┴──────────┴─────────┴─────────┘; ```,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1764#issuecomment-868470173:2876,perform,performance,2876,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1764#issuecomment-868470173,1,['perform'],['performance']
Performance,"erent advection schemes and time steppers to help us decide what to implement. I hope this PR can serve as test-driven development: Oceananigans should be able to reproduce the results of this PR, at which point it becomes a true verification experiment. `weno.jl` defines `advective_tracer_flux` functions (`weno5_flux`) so it should work nicely with the existing `Oceananigans.Operators`. At some point in the future I'd like to follow this PR up with a 2D advection test using the Munk gyre solution. Should also test momentum advection. Of course, can't always generalize 1D advection results to 3D turbulence simulations... Things to do to integrate WENO-5:; 1. Design an abstraction for selecting advection schemes.; 2. Extend to multi-dimensional advection scheme.; 3. Extend to a momentum advection scheme as well.; 4. Decide whether we want WENO-3 and/or WENO-7 (or even higher-order advection schemes). Note: On extending to multi-dimensional advection, we can perform the 1D WENO interpolation along each dimension separately to come up with a multidimensional advection scheme. This is what most packages do in practice as true multidimensional would involve huge stencils (and some numerical quadrature?) so it's not worth it for the small increase in accuracy. Doing it dimension-wise might be fine at lower order like WENO <= 7. I should cite the appropriate papers for these claims. This relatively recent paper might be of interest to us: Buchmüller & Helzel (2014), [Improved Accuracy of High-Order WENO Finite Volume Methods on Cartesian Grids](https://doi.org/10.1007/s10915-014-9825-1). Resolves #481; Resolves #934. ---; Not sure if it'll generalize to 3D but seems that you get better accuracy even with half the number of grid points (per dimension?). And if we also upgrade time stepper from AB2 to something like LSRK54 then we should be able to take ~3x longer time steps. ![Gaussian_SecondOrderCentered_AB3_N64_CFL0 60](https://user-images.githubusercontent.com/20099589/72",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/592:1227,perform,perform,1227,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/592,1,['perform'],['perform']
Performance,"ering optimization strategies that our computations are probably memory-limited, rather than compute-limited. In other words, we think the process of transferring data from global memory to local thread memory is a bottleneck for our computations (we can really only know this through profiling a particular application, however, since all models are different...) Storing intermediate components of the tendency terms would probably create more memory accesses overall (since rather than immediately using intermediate results for subsequent calculations, we would have to send them to global memory, and then back, to complete the evaluation of a tendency) --- and thus could slow down tendency evaluations that are performed 1-3 times per time-step. For example, our best idea for speeding up tendency evaluations is to better manage memory movement using GPU shared memory (unfortunately, we haven't had the time to explore such optimization strategies...). I think there may be other ways to optimize diagnostics calculations, however. # Fusing `ComputedField` kernels. One possibility to speed up diagnostics is to ""fuse"" kernels for different `ComputedField` diagnostics. The kernel for a `ComputedField` is. https://github.com/CliMA/Oceananigans.jl/blob/9b52f3f911d26a66c75f1c3cb58fdd0a1cecb131/src/Fields/computed_field.jl#L112-L115. where `operand` is an `AbstractOperation`. But different `ComputedField`s may somehow depend on the same underlying data in memory. Thus if the kernels for differnet `ComputedField`s are fused into one, we overlap memory accesses for different computations. Our computations are usually memory-limited... so its possible this strategy could produce significant speed ups. For example, for two `ComputedField`s we might have something like. ```julia; function compute!(field1, field2); # calls _compute_two(field1.data, field2.data, field1.operand, field2.operand); end; ```. and the kernel. ```julia; @kernel function _compute_two!(data1, data2, operand1, op",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1483#issuecomment-800567837:1475,optimiz,optimize,1475,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1483#issuecomment-800567837,1,['optimiz'],['optimize']
Performance,"est/regression_tests/shallow_water_bickley_jet_regression.jl:94; Expression: all(test_fields.v .≈ truth_fields.v); Stacktrace:; [1] run_shallow_water_regression(arch::GPU, formulation::ConservativeFormulation; regenerate_data::Bool); @ Main /g/data/v45/nc3020/OC.jl/test/regression_tests/shallow_water_bickley_jet_regression.jl:94; [2] macro expansion; @ /g/data/v45/nc3020/OC.jl/test/test_shallow_water_regression.jl:12 [inlined]; [3] macro expansion; @ /g/data/v45/nc3020/julia/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1151 [inlined]; [4] macro expansion; @ /g/data/v45/nc3020/OC.jl/test/test_shallow_water_regression.jl:11 [inlined]; [5] macro expansion; @ /g/data/v45/nc3020/julia/usr/share/julia/stdlib/v1.6/Test/src/Test.jl:1151 [inlined]; [6] top-level scope; @ /g/data/v45/nc3020/OC.jl/test/test_shallow_water_regression.jl:6; Test Summary: | Pass Fail Total; Oceananigans | 4 2 6; Shallow Water Regression | 4 2 6; Shallow Water Bickley jet simulation [GPU, VectorInvariantFormulation] | 2 1 3; Shallow Water Bickley jet simulation [GPU, ConservativeFormulation] | 2 1 3; ERROR: LoadError: Some tests did not pass: 4 passed, 2 failed, 0 errored, 0 broken.; in expression starting at /g/data/v45/nc3020/OC.jl/test/runtests.jl:3; ERROR: Package Oceananigans errored during testing. (Oceananigans) pkg> st; Project Oceananigans v0.76.5; Status `/g/data/v45/nc3020/OC.jl/Project.toml`; [79e6a3ab] Adapt v3.3.3; [052768ef] CUDA v3.10.0; [72cfdca4] CUDAKernels v0.3.3; [a8cc5b0e] Crayons v4.1.1; [7445602f] CubedSphere v0.2.0; [ffbed154] DocStringExtensions v0.8.6; [7a1cc6ca] FFTW v1.4.6; [c27321d9] Glob v1.3.0; [40713840] IncompleteLU v0.2.0; [42fd0dbc] IterativeSolvers v0.9.2; [033835bb] JLD2 v0.4.22; [63c18a36] KernelAbstractions v0.7.2; [da04e1cc] MPI v0.19.2; [85f8d34a] NCDatasets v0.12.4; [6fe1bfb0] OffsetArrays v1.11.2; [bac558e1] OrderedCollections v1.4.1; [0e08944d] PencilArrays v0.17.2; [4a48f351] PencilFFTs v0.13.6; [6038ab10] Rotations v1.3.1; [1bc83da4] SafeTestsets v0.0.1",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2922#issuecomment-1437515895:17355,Load,LoadError,17355,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2922#issuecomment-1437515895,1,['Load'],['LoadError']
Performance,et_toplevel_thunk at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:775; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:934; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:877; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:877; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:877; ijl_toplevel_eval_in at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:985; eval at ./boot.jl:385 [inlined]; eval_user_input at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:150; repl_backend_loop at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:246; #start_repl_backend#46 at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:231; start_repl_backend at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:228; _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; #run_repl#59 at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:389; run_repl at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:375; jfptr_run_repl_91805.1 at /home/alir/.julia/juliaup/julia-1.10.5+0.x64.linux.gnu/lib/julia/sys.so (unknown line); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; #1013 ,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3852:6680,cache,cache,6680,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3852,1,['cache'],['cache']
Performance,"eviceArray{Float64, 3, 1}}, OffsetArrays.OffsetArray{Float64, 3, CUDA.CuDeviceArray{Float64, 3, 1}}}}, NamedTuple{(), Tuple{}}, NamedTuple{(:a,), Tuple{Int64}}, Nothing, NamedTuple{(:u, :v, :w, :a), Tuple{Oceananigans.Forcings.ContinuousForcing{Face, Center, Center, Nothing, typeof(forc_u), Nothing, Tuple{Int64}, Tuple{typeof(Oceananigans.Operators.identity4)}}, typeof(Oceananigans.Forcings.zeroforcing), typeof(Oceananigans.Forcings.zeroforcing), Nothing}}, OffsetArrays.OffsetArray{Float64, 3, CUDA.CuDeviceArray{Float64, 3, 1}}, NamedTuple{(:time, :iteration, :stage), Tuple{Float64, Int64, Int64}}}}}}); @ GPUCompiler ~/.julia/packages/GPUCompiler/kb6yJ/src/driver.jl:76; [9] cufunction_compile(job::GPUCompiler.CompilerJob); @ CUDA ~/.julia/packages/CUDA/BbliS/src/compiler/execution.jl:347; [10] cached_compilation(cache::Dict{UInt64, Any}, job::GPUCompiler.CompilerJob, compiler::typeof(CUDA.cufunction_compile), linker::typeof(CUDA.cufunction_link)); @ GPUCompiler ~/.julia/packages/GPUCompiler/kb6yJ/src/cache.jl:90; [11] cufunction(f::typeof(Oceananigans.Models.NonhydrostaticModels.gpu_calculate_Gu!), tt::Type{Tuple{KernelAbstractions.CompilerMetadata{KernelAbstractions.NDIteration.StaticSize{(4, 4, 4)}, KernelAbstractions.NDIteration.DynamicCheck, Nothing, Nothing, KernelAbstractions.NDIteration.NDRange{3, KernelAbstractions.NDIteration.StaticSize{(1, 1, 4)}, KernelAbstractions.NDIteration.StaticSize{(16, 16, 1)}, Nothing, Nothing}}, OffsetArrays.OffsetArray{Float64, 3, CUDA.CuDeviceArray{Float64, 3, 1}}, RectilinearGrid{Float64, Periodic, Periodic, Bounded, Float64, Float64, Float64, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, Nothing}, Cen",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3025#issuecomment-1481807353:8909,cache,cache,8909,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3025#issuecomment-1481807353,1,['cache'],['cache']
Performance,"expansion at /home/fpoulin/.julia/packages/CUDA/0p5fn/lib/utils/call.jl:39 [inlined]; │ [10] macro expansion at /home/fpoulin/.julia/packages/CUDA/0p5fn/lib/cudadrv/libcuda.jl:35 [inlined]; │ [11] macro expansion at /home/fpoulin/.julia/packages/CUDA/0p5fn/lib/cudadrv/error.jl:102 [inlined]; │ [12] cuDeviceGetCount(::Base.RefValue{Int32}) at /home/fpoulin/.julia/packages/CUDA/0p5fn/lib/utils/call.jl:93; │ [13] length at /home/fpoulin/.julia/packages/CUDA/0p5fn/lib/cudadrv/devices.jl:111 [inlined]; │ [14] iterate at /home/fpoulin/.julia/packages/CUDA/0p5fn/lib/cudadrv/devices.jl:106 [inlined] (repeats 2 times); │ [15] iterate at ./iterators.jl:139 [inlined]; │ [16] iterate at ./iterators.jl:138 [inlined]; │ [17] __init__() at /home/fpoulin/software/Oceananigans.jl/src/Oceananigans.jl:178; │ [18] _include_from_serialized(::String, ::Array{Any,1}) at ./loading.jl:697; │ [19] _require_search_from_serialized(::Base.PkgId, ::String) at ./loading.jl:782; │ [20] _require(::Base.PkgId) at ./loading.jl:1007; │ [21] require(::Base.PkgId) at ./loading.jl:928; │ [22] require(::Module, ::Symbol) at ./loading.jl:923; │ [23] eval(::Module, ::Any) at ./boot.jl:331; │ [24] eval_user_input(::Any, ::REPL.REPLBackend) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/REPL/src/REPL.jl:134; │ [25] repl_backend_loop(::REPL.REPLBackend) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/REPL/src/REPL.jl:195; │ [26] start_repl_backend(::REPL.REPLBackend, ::Any) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/REPL/src/REPL.jl:180; │ [27] run_repl(::REPL.AbstractREPL, ::Any; backend_on_current_task::Bool) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/REPL/src/REPL.jl:292; │ [28] run_repl(::REPL.AbstractREPL, ::Any) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/REPL/src/REPL.jl:288; │ [29] (::Base.var""#807#809""{Bool,Bool,Bool,Bool})(::Module) at ./client.jl:399; │ [30] #inv",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1189:2297,load,loading,2297,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1189,1,['load'],['loading']
Performance,"extName, Any...) in module Cassette at /data5/glwagner/.julia/packages/Cassette/xggAf/src/overdub.jl:521 overwritten in module GPUifyLoops at /data5/glwagner/.julia/packages/Cassette/xggAf/src/overdub.jl:521.; WARNING: Method definition overdub(Cassette.Context{N, M, T, P, B, H} where H<:Union{Cassette.DisableHooks, Nothing} where B<:Union{Nothing, Base.IdDict{Module, Base.Dict{Symbol, Cassette.BindingMeta}}} where P<:Cassette.AbstractPass where T<:Union{Nothing, Cassette.Tag{N, X, E} where E where X where N<:Cassette.AbstractContextName} where M where N<:Cassette.AbstractContextName, Any...) in module Cassette at /data5/glwagner/.julia/packages/Cassette/xggAf/src/overdub.jl:508 overwritten in module GPUifyLoops at /data5/glwagner/.julia/packages/Cassette/xggAf/src/overdub.jl:508. CUDA-enabled GPU(s) detected:; CuDevice(0): Quadro P6000; [ Info: Building the CUDAnative run-time library for your sm_61 device, this might take a while...; ERROR: LoadError: InvalidIRError: compiling #12(RegularCartesianGrid{Float64,StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}}, PlanetaryConstants{Float64}, LinearEquationOfState{Float64}, Oceananigans.TurbulenceClosures.ConstantAnisotropicDiffusivity{Float64}, CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global}, CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global}, CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global}, CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global}, CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global}, CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global}, CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global}, CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global}, CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global}, CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global}, CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global}, Forcing{typeof(Oceananigans.zero_func),typeof(Oceananigans.zero_func),typeof(Oceananigans.zero_func),typeof(Oceananigans",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/248#issuecomment-496489468:1689,Load,LoadError,1689,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/248#issuecomment-496489468,1,['Load'],['LoadError']
Performance,"fn/lib/utils/call.jl:39 [inlined]; │ [10] macro expansion at /home/fpoulin/.julia/packages/CUDA/0p5fn/lib/cudadrv/libcuda.jl:35 [inlined]; │ [11] macro expansion at /home/fpoulin/.julia/packages/CUDA/0p5fn/lib/cudadrv/error.jl:102 [inlined]; │ [12] cuDeviceGetCount(::Base.RefValue{Int32}) at /home/fpoulin/.julia/packages/CUDA/0p5fn/lib/utils/call.jl:93; │ [13] length at /home/fpoulin/.julia/packages/CUDA/0p5fn/lib/cudadrv/devices.jl:111 [inlined]; │ [14] iterate at /home/fpoulin/.julia/packages/CUDA/0p5fn/lib/cudadrv/devices.jl:106 [inlined] (repeats 2 times); │ [15] iterate at ./iterators.jl:139 [inlined]; │ [16] iterate at ./iterators.jl:138 [inlined]; │ [17] __init__() at /home/fpoulin/software/Oceananigans.jl/src/Oceananigans.jl:178; │ [18] _include_from_serialized(::String, ::Array{Any,1}) at ./loading.jl:697; │ [19] _require_search_from_serialized(::Base.PkgId, ::String) at ./loading.jl:782; │ [20] _require(::Base.PkgId) at ./loading.jl:1007; │ [21] require(::Base.PkgId) at ./loading.jl:928; │ [22] require(::Module, ::Symbol) at ./loading.jl:923; │ [23] eval(::Module, ::Any) at ./boot.jl:331; │ [24] eval_user_input(::Any, ::REPL.REPLBackend) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/REPL/src/REPL.jl:134; │ [25] repl_backend_loop(::REPL.REPLBackend) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/REPL/src/REPL.jl:195; │ [26] start_repl_backend(::REPL.REPLBackend, ::Any) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/REPL/src/REPL.jl:180; │ [27] run_repl(::REPL.AbstractREPL, ::Any; backend_on_current_task::Bool) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/REPL/src/REPL.jl:292; │ [28] run_repl(::REPL.AbstractREPL, ::Any) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/REPL/src/REPL.jl:288; │ [29] (::Base.var""#807#809""{Bool,Bool,Bool,Bool})(::Module) at ./client.jl:399; │ [30] #invokelatest#1 at ./essentials.jl:710 [inlined]; │ [31",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1189:2348,load,loading,2348,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1189,1,['load'],['loading']
Performance,"forcing::NamedTuple{(), Tuple{}}, closure::Nothing, boundary_conditions::NamedTuple{(), Tuple{}}, tracers::Tuple{Symbol, Symbol}, timestepper::Symbol, background_fields::NamedTuple{(), Tuple{}}, particles::Nothing, velocities::Nothing, pressures::Nothing, diffusivities::Nothing, pressure_solver::Nothing, immersed_boundary::Nothing); @ Oceananigans.Models.IncompressibleModels ~/.julia/packages/Oceananigans/IxOwr/src/Models/IncompressibleModels/incompressible_model.jl:142; [28] top-level scope; @ REPL[4]:1; [29] top-level scope; @ ~/.julia/packages/CUDA/3VnCC/src/initialization.jl:81; ```. # Environment. ```julia; julia> versioninfo(); Julia Version 1.6.1; Commit 6aaedecc44 (2021-04-23 05:59 UTC); Platform Info:; OS: Linux (powerpc64le-unknown-linux-gnu); CPU: unknown; WORD_SIZE: 64; LIBM: libopenlibm; LLVM: libLLVM-11.0.1 (ORCJIT, pwr9); ```. ```julia; julia> CUDA.versioninfo(); CUDA toolkit 10.1.243, local installation; CUDA driver 10.2.0; NVIDIA driver 440.64.0. Libraries: ; - CUBLAS: 10.2.2; - CURAND: 10.1.1; - CUFFT: 10.1.1; - CUSOLVER: 10.2.0; - CUSPARSE: 10.3.0; - CUPTI: 12.0.0; - NVML: 10.0.0+440.64.0; ┌ Warning: Could not find or load CUDNN; run with JULIA_DEBUG=CUDA for more details.; └ @ CUDA ~/.julia/packages/CUDA/3VnCC/deps/bindeps.jl:354; - CUDNN: missing; ┌ Warning: Could not find or load CUTENSOR; run with JULIA_DEBUG=CUDA for more details.; └ @ CUDA ~/.julia/packages/CUDA/3VnCC/deps/bindeps.jl:409; - CUTENSOR: missing. Toolchain:; - Julia: 1.6.1; - LLVM: 11.0.1; - PTX ISA support: 3.2, 4.0, 4.1, 4.2, 4.3, 5.0, 6.0, 6.1, 6.3, 6.4; - Device support: sm_30, sm_32, sm_35, sm_37, sm_50, sm_52, sm_53, sm_60, sm_61, sm_62, sm_70, sm_72, sm_75. 2 devices:; 0: Tesla V100-SXM2-32GB (sm_70, 29.284 GiB / 31.749 GiB available); 1: Tesla V100-SXM2-32GB (sm_70, 24.457 GiB / 31.749 GiB available); ```. ```julia; (@v1.6) pkg> st; Status `~/.julia/environments/v1.6/Project.toml`; [052768ef] CUDA v3.2.1; [13f3f980] CairoMakie v0.5.2; [9e8cae18] Oceananigans v0.58.1; ```",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1706:7087,load,load,7087,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1706,2,['load'],['load']
Performance,"g 1 direct dependency failed to precompile:. Oceananigans [9e8cae18-63c1-5223-a75c-80ca9d6e9a09]. Failed to precompile Oceananigans [9e8cae18-63c1-5223-a75c-80ca9d6e9a09] to /Users/navid/.julia/compiled/v1.6/Oceananigans/jl_Z5b4Xf.; ERROR: LoadError: LoadError: LoadError: InitError: UndefVarError: libamgxsh not defined; Stacktrace:; [1] getproperty; @ ./Base.jl:26 [inlined]; [2] __init__(); @ AMGX ~/.julia/packages/AMGX/GFHHN/src/AMGX.jl:30; [3] _include_from_serialized(path::String, depmods::Vector{Any}); @ Base ./loading.jl:696; [4] _require_search_from_serialized(pkg::Base.PkgId, sourcepath::String); @ Base ./loading.jl:782; [5] _require(pkg::Base.PkgId); @ Base ./loading.jl:1020; [6] require(uuidkey::Base.PkgId); @ Base ./loading.jl:936; [7] require(into::Module, mod::Symbol); @ Base ./loading.jl:923; [8] include(mod::Module, _path::String); @ Base ./Base.jl:384; [9] include(x::String); @ Oceananigans.Solvers ~/Research/OC.jl/src/Solvers/Solvers.jl:1; [10] top-level scope; @ ~/Research/OC.jl/src/Solvers/Solvers.jl:48; [11] include(mod::Module, _path::String); @ Base ./Base.jl:384; [12] include(x::String); @ Oceananigans ~/Research/OC.jl/src/Oceananigans.jl:5; [13] top-level scope; @ ~/Research/OC.jl/src/Oceananigans.jl:195; [14] include; @ ./Base.jl:384 [inlined]; [15] include_package_for_output(pkg::Base.PkgId, input::String, depot_path::Vector{String}, dl_load_path::Vector{String}, load_path::Vector{String}, concrete_deps::Vector{Pair{Base.PkgId, UInt64}}, source::Nothing); @ Base ./loading.jl:1235; [16] top-level scope; @ none:1; [17] eval; @ ./boot.jl:360 [inlined]; [18] eval(x::Expr); @ Base.MainInclude ./client.jl:446; [19] top-level scope; @ none:1; during initialization of module AMGX; in expression starting at /Users/navid/Research/OC.jl/src/Solvers/multigrid_solver.jl:5; in expression starting at /Users/navid/Research/OC.jl/src/Solvers/Solvers.jl:1; in expression starting at /Users/navid/Research/OC.jl/src/Oceananigans.jl:1; ```. Was it smooth for you?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2688#issuecomment-1217694987:3146,load,loading,3146,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2688#issuecomment-1217694987,1,['load'],['loading']
Performance,"g) at ./Base.jl:380; &nbsp; | [14] include(::Module, ::String) at ./Base.jl:368; &nbsp; | [15] top-level scope at none:2; &nbsp; | [16] eval at ./boot.jl:347 [inlined]; &nbsp; | [17] eval(::Expr) at ./client.jl:467; &nbsp; | [18] top-level scope at ./none:3; &nbsp; | in expression starting at /storage7/buildkite-agent/.julia-2581/packages/PencilArrays/DTEhf/src/Pencils/Pencils.jl:7; &nbsp; | in expression starting at /storage7/buildkite-agent/.julia-2581/packages/PencilArrays/DTEhf/src/PencilArrays.jl:12; &nbsp; | ERROR: LoadError: Failed to precompile PencilArrays [0e08944d-e94e-41b1-9406-dcf66b6a9d2e] to /storage7/buildkite-agent/.julia-2581/compiled/v1.5/PencilArrays/yKKUy_zV1Ut.ji.; &nbsp; | Stacktrace:; &nbsp; | [1] error(::String) at ./error.jl:33; &nbsp; | [2] compilecache(::Base.PkgId, ::String) at ./loading.jl:1305; &nbsp; | [3] _require(::Base.PkgId) at ./loading.jl:1030; &nbsp; | [4] require(::Base.PkgId) at ./loading.jl:928; &nbsp; | [5] require(::Module, ::Symbol) at ./loading.jl:923; &nbsp; | [6] include(::Function, ::Module, ::String) at ./Base.jl:380; &nbsp; | [7] include(::Module, ::String) at ./Base.jl:368; &nbsp; | [8] top-level scope at none:2; &nbsp; | [9] eval at ./boot.jl:347 [inlined]; &nbsp; | [10] eval(::Expr) at ./client.jl:467; &nbsp; | [11] top-level scope at ./none:3; &nbsp; | in expression starting at /storage7/buildkite-agent/.julia-2581/packages/PencilFFTs/Xwxei/src/PencilFFTs.jl:11; &nbsp; | ERROR: could not load library ""/storage7/buildkite-agent/julia-1.5.4/lib/julia/sys.so""; &nbsp; | /storage7/buildkite-agent/julia-1.5.4/lib/julia/sys.so: ELF load command past end of file; &nbsp; | ERROR: could not load library ""/storage7/buildkite-agent/julia-1.5.4/lib/julia/sys.so""; &nbsp; | /storage7/buildkite-agent/julia-1.5.4/lib/julia/sys.so: ELF load command past end of file; &nbsp; | ERROR: could not load library ""/storage7/buildkite-agent/julia-1.5.4/lib/julia/sys.so""; &nbsp; | /storage7/buildkite-agent/julia-1.5.4/lib/julia/sys.so: ELF l",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1662#issuecomment-843325731:5693,load,loading,5693,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1662#issuecomment-843325731,1,['load'],['loading']
Performance,"g; [de0858da] Printf; [9a3f8284] Random; [2f01184e] SparseArrays; [10745b16] Statistics. (Oceananigans) pkg> precompile; Precompiling project...; ✗ Oceananigans; 0 dependencies successfully precompiled in 11 seconds (99 already precompiled). ERROR: The following 1 direct dependency failed to precompile:. Oceananigans [9e8cae18-63c1-5223-a75c-80ca9d6e9a09]. Failed to precompile Oceananigans [9e8cae18-63c1-5223-a75c-80ca9d6e9a09] to /Users/navid/.julia/compiled/v1.6/Oceananigans/jl_Z5b4Xf.; ERROR: LoadError: LoadError: LoadError: InitError: UndefVarError: libamgxsh not defined; Stacktrace:; [1] getproperty; @ ./Base.jl:26 [inlined]; [2] __init__(); @ AMGX ~/.julia/packages/AMGX/GFHHN/src/AMGX.jl:30; [3] _include_from_serialized(path::String, depmods::Vector{Any}); @ Base ./loading.jl:696; [4] _require_search_from_serialized(pkg::Base.PkgId, sourcepath::String); @ Base ./loading.jl:782; [5] _require(pkg::Base.PkgId); @ Base ./loading.jl:1020; [6] require(uuidkey::Base.PkgId); @ Base ./loading.jl:936; [7] require(into::Module, mod::Symbol); @ Base ./loading.jl:923; [8] include(mod::Module, _path::String); @ Base ./Base.jl:384; [9] include(x::String); @ Oceananigans.Solvers ~/Research/OC.jl/src/Solvers/Solvers.jl:1; [10] top-level scope; @ ~/Research/OC.jl/src/Solvers/Solvers.jl:48; [11] include(mod::Module, _path::String); @ Base ./Base.jl:384; [12] include(x::String); @ Oceananigans ~/Research/OC.jl/src/Oceananigans.jl:5; [13] top-level scope; @ ~/Research/OC.jl/src/Oceananigans.jl:195; [14] include; @ ./Base.jl:384 [inlined]; [15] include_package_for_output(pkg::Base.PkgId, input::String, depot_path::Vector{String}, dl_load_path::Vector{String}, load_path::Vector{String}, concrete_deps::Vector{Pair{Base.PkgId, UInt64}}, source::Nothing); @ Base ./loading.jl:1235; [16] top-level scope; @ none:1; [17] eval; @ ./boot.jl:360 [inlined]; [18] eval(x::Expr); @ Base.MainInclude ./client.jl:446; [19] top-level scope; @ none:1; during initialization of module AMGX; in expression",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2688#issuecomment-1217694987:2368,load,loading,2368,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2688#issuecomment-1217694987,1,['load'],['loading']
Performance,"ggest leaving a caution statement on the simulation tips page. But I agree it has to be less strongly worded...; > ; > I'll try to provide a MWE that reproduces the behavior, but I'm currently having trouble getting my hands on some GPU, so I'm not sure how fast I can do that. Ok, no rush!. Trig functions aren't generically slower on GPUs than CPUs. On CPUs I think our code is fairly non-optimal right now, so various sources of overhead (eg non-optimal threading) can ""hide"" slow operations on the CPU. On the GPU we are more efficient, so overall speed might depend more sensitively on user code when it's injected. (I'd also argue that the beginning of this section is a bit misleading in how it claims we ""try to optimize"" internal source code. In fact, we have performed almost no performance optimization, and this is an important topic for future work.). I found this reference for the cost of various floating point operations on the CPU:. https://latkin.org/blog/2014/11/09/a-simple-benchmark-of-various-math-operations/. We could reproduce this chart on a GPU with CUDA.jl if we want to provide some useful information to users. I think if we're talking about a _constant_ (the current case), then precomputation hardly harms code complexity (both examples are equally readable to me). Precomputing an _array_ is another story (for example, a forcing function or boundary condition that depends on `sin(x)`). This lesson is definitely not restricted to trigonometric functions or the GPU. The basic principle here is that _there is a trade-off_ between precomputing a potentially expensive operation, and performing it on-the-fly. For constants, precomputation is harmless. For arrays, on-the-fly computation has significant benefits, both for code readability and also possibly for performance (in memory-bound computations). Enlightening users on 1) the existence of this trade-off and 2) how to use benchmarking to find the optimal solution for their problem would probably be useful.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2029#issuecomment-952107151:2130,perform,performing,2130,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2029#issuecomment-952107151,2,['perform'],"['performance', 'performing']"
Performance,gions!#38 at /orcd/data/raffaele/001/glwagner/Oceananigans.jl/src/DistributedComputations/halo_communication.jl:114; fill_halo_regions! at /orcd/data/raffaele/001/glwagner/Oceananigans.jl/src/DistributedComputations/halo_communication.jl:101 [inlined]; #fill_halo_regions!#37 at /orcd/data/raffaele/001/glwagner/Oceananigans.jl/src/DistributedComputations/halo_communication.jl:90 [inlined]; fill_halo_regions! at /orcd/data/raffaele/001/glwagner/Oceananigans.jl/src/DistributedComputations/halo_communication.jl:87; unknown function (ip: 0x2aaac8ad0ee5); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; jl_apply at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/julia.h:1982 [inlined]; do_call at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:126; eval_value at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:223; eval_stmt_value at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:174 [inlined]; eval_body at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:617; jl_interpret_toplevel_thunk at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:775; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:934; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:877; ijl_toplevel_eval_in at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:985; eval at ./boot.jl:385 [inlined]; include_string at ./loading.jl:2076; _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; _include at ./load,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3878:4143,cache,cache,4143,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3878,1,['cache'],['cache']
Performance,"grid); u, v, w = model.velocities. uu = @at (Center, Center, Center) u * u. # output; BinaryOperation at (Center, Center, Center); ├── grid: RegularRectilinearGrid{Float64, Periodic, Periodic, Bounded}(Nx=1, Ny=1, Nz=1); │ └── domain: x ∈ [0.0, 1.0], y ∈ [0.0, 1.0], z ∈ [-1.0, 0.0]; └── tree: ; * at (Center, Center, Center) via ℑxᶜᵃᵃ;    ├── Field located at (Face, Center, Center);    └── Field located at (Face, Center, Center); ```. which would compute `u * u` at `(Face, Center, Center)`, and subsequently interpolate to cell centers. The object `uu` would then be defined at cell centers. The main issue of this design in terms of functionality is that it produces expression trees that interpolate ""too eagerly"". A common example is a turbulent kinetic energy computation:. ```julia; U = AveragedField(u, dims=(1, 2)); V = AveragedField(v, dims=(1, 2)). tke = @at (Center, Center, Center) ((u - U)^2 + (v - V)^2 + w^2); ```. Inspection of this object reveals that while `u - U` does not interpolate, the squaring `(u - U)^2` would be performed at cell centers:. ```julia; julia> tke.args[1].b; 2. julia> tke.args[1].a; BinaryOperation at (Center, Center, Center); ├── grid: RegularRectilinearGrid{Float64, Periodic, Periodic, Bounded}(Nx=1, Ny=1, Nz=1); │ └── domain: x ∈ [0.0, 1.0], y ∈ [0.0, 1.0], z ∈ [-1.0, 0.0]; └── tree: ; - at (Center, Center, Center) via identity;    ├── Field located at (Face, Center, Center);    └── AveragedField over dims=(1, 2) located at (⋅, ⋅, Center) of Field located at (Face, Center, Center); ```. This isn't what we want (usually): instead, we want both `u - U` and subsequent squaring in `(u - U)^2` to be performed at `(Face, Center, Center)`. This PR addresses this issue by getting rid of the third interpolation in `BinaryOperation`, and changing how locations for `BinaryOperation` are inferred. Now, when locations are specified via `@at`, they are taken as a ""suggestion"" that only acts if the two elements of the binary operation have different _c",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1599:1723,perform,performed,1723,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1599,1,['perform'],['performed']
Performance,"gs{stale_age::Int64}); │ @ FileWatching.Pidfile ~/julia-1.10/usr/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:111; │ [8] #invokelatest#2; │ @ ./essentials.jl:894 [inlined]; │ [9] invokelatest; │ @ ./essentials.jl:889 [inlined]; │ [10] maybe_cachefile_lock(f::Base.var""#968#969""{Base.PkgId}, pkg::Base.PkgId, srcpath::String; stale_age::Int64); │ @ Base ./loading.jl:2983; │ [11] maybe_cachefile_lock; │ @ ./loading.jl:2980 [inlined]; │ [12] _require(pkg::Base.PkgId, env::Nothing); │ @ Base ./loading.jl:1970; │ [13] __require_prelocked(uuidkey::Base.PkgId, env::Nothing); │ @ Base ./loading.jl:1812; │ [14] #invoke_in_world#3; │ @ ./essentials.jl:926 [inlined]; │ [15] invoke_in_world; │ @ ./essentials.jl:923 [inlined]; │ [16] _require_prelocked; │ @ ./loading.jl:1803 [inlined]; │ [17] _require_prelocked; │ @ ./loading.jl:1802 [inlined]; │ [18] run_extension_callbacks(extid::Base.ExtensionId); │ @ Base ./loading.jl:1295; │ [19] run_extension_callbacks(pkgid::Base.PkgId); │ @ Base ./loading.jl:1330; │ [20] run_package_callbacks(modkey::Base.PkgId); │ @ Base ./loading.jl:1164; │ [21] _tryrequire_from_serialized(modkey::Base.PkgId, path::String, ocachepath::String, sourcepath::String, depmods::Vector{Any}); │ @ Base ./loading.jl:1487; │ [22] _require_search_from_serialized(pkg::Base.PkgId, sourcepath::String, build_id::UInt128); │ @ Base ./loading.jl:1574; │ [23] _require(pkg::Base.PkgId, env::String); │ @ Base ./loading.jl:1938; │ [24] __require_prelocked(uuidkey::Base.PkgId, env::String); │ @ Base ./loading.jl:1812; │ [25] #invoke_in_world#3; │ @ ./essentials.jl:926 [inlined]; │ [26] invoke_in_world; │ @ ./essentials.jl:923 [inlined]; │ [27] _require_prelocked(uuidkey::Base.PkgId, env::String); │ @ Base ./loading.jl:1803; │ [28] macro expansion; │ @ ./loading.jl:1790 [inlined]; │ [29] macro expansion; │ @ ./lock.jl:267 [inlined]; │ [30] __require(into::Module, mod::Symbol); │ @ Base ./loading.jl:1753; │ [31] #invoke_in_world#3; │ @ ./essentials.jl:926 [inlined]; │ [32",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3686#issuecomment-2272432528:2820,load,loading,2820,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3686#issuecomment-2272432528,1,['load'],['loading']
Performance,"gy cascade, and further assuming that the cut-off filter is in the inertial range. I think all these assumptions are valid in this simulation, so we expect the dynamically-calculated value of the Smagorinsky coefficient (the black line) to approach the value 0.16 as time goes on. https://github.com/CliMA/Oceananigans.jl/assets/13205162/4049e7cf-452e-4883-a709-a675cf12277c. Although the match is not exact (the value it approaches is ~0.17), I think this is close enough. That said, I'm planning on also implementing a boundary layer validation along similar lines, which we can use to validate the model in the same fashion as [Bou-Zeid et al. (2005)](https://dx.doi.org/10.1063/1.1839152). One thing to note is that the current implementation appears to be very slow. While the simulation with the `SmagorinskyLilly` closure runs on my laptop in 10 seconds, it takes 4 minutes for the simulation with the `ScaleInvariantSmagorinsky`. I know the dynamic model will be slower given the extra computations, but such a difference seems large to me, so I'm hoping something can be changed here to improve performance:. ```julia; ┌ Info: Running; └ closure = SmagorinskyLilly: C=0.16, Cb=1.0, Pr=1.0; [ Info: Initializing simulation...; [ Info: ... simulation initialization complete (2.738 seconds); [ Info: Executing initial time step...; [ Info: ... initial time step complete (4.852 seconds).; [ Info: Simulation is stopping after running for 11.657 seconds.; [ Info: Simulation time 1.333 minutes equals or exceeds stop time 1.333 minutes.; ┌ Info: Running; └ closure = ScaleInvariantSmagorinsky: averaging=Averaging over directions Colon(), Pr=1.0; [ Info: Initializing simulation...; [ Info: ... simulation initialization complete (2.195 seconds); [ Info: Executing initial time step...; [ Info: ... initial time step complete (2.812 seconds).; [ Info: Simulation is stopping after running for 3.735 minutes.; [ Info: Simulation time 1.333 minutes equals or exceeds stop time 1.333 minutes.; ```",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3642#issuecomment-2212623170:1963,perform,performance,1963,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3642#issuecomment-2212623170,1,['perform'],['performance']
Performance,"h @simone-silvestri:. If you want to adapt your script to multiregion you have to:; - define a multiregion grid with: `grid = MultiRegionGrid(grid, partition = XPartition(n_gpus_you_want_to_use), devices = n_gpus_you_want_to_use)` . The multiregion grid supersedes the immersed boundary grid, i.e. if you are using an immersed boundary grid then `grid = MultiRegionGrid(ibg; kwargs...)` , not the other way around.; - if you are using any array for forcing or boundary condition, you have to adapt it to the multiregion paradigm as follows: `using Oceananigans.MultiRegion: multi_region_object_from_array; my_adapted_array = multi_region_object_from_array(my_array, grid)`. MultiRegion works only on single node multi GPU, so all the GPUs should be accessible from a single process in the node. You can check the number of GPUs available by logging in a node and typing nvidia-smi , if you want to split your grid on specific devices (let’s say GPU 0 and 3), you can pass `devices = (0, 3)` to the `MultiRegionGrid` constructor. There is another thing that you have to take care of: the pressure solve is performed on one GPU only so both the storage and source term (a field of complex values of the size of the full grid) reside on 1 GPU only (usually the one corresponding to the last region). This means that if your grid is 100M points, 2.98 GB will have to be reserved for the solver’s auxiliary fields; ```julia; julia> sizeof(complex(zeros(Int(100e6)))) / 1024 / 1024 / 1024 * 2; 2.9802322387695312; ```; So make sure you have that space available. (When I have time I ll try to find a solution to run truly parallel pressure solvers, for both nonhydrostatic and hydrostatic models). In terms of outputs, we make use of `reconstruct_global_field` , a function used to reconstruct a global field from a `MultiRegionField` on the CPU. It is used by the output writers to spit out the full field. It is a slow procedure though, so I would advise against performing too many involved diagnostics.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2795#issuecomment-1301096875:1182,perform,performed,1182,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2795#issuecomment-1301096875,2,['perform'],"['performed', 'performing']"
Performance,"h::Vector{String}, load_path::Vector{String}, concrete_deps::Vector{Pair{Base.PkgId, UInt64}}, source::String); @ Base ./loading.jl:1318; [16] top-level scope; @ none:1; [17] eval; @ ./boot.jl:373 [inlined]; [18] eval(x::Expr); @ Base.MainInclude ./client.jl:453; [19] top-level scope; @ none:1; during initialization of module MPICH_jll; in expression starting at /Users/sean/.julia/packages/MPI/08SPr/deps/deps.jl:1; ERROR: LoadError: Failed to precompile MPI [da04e1cc-30fd-572f-bb4f-1f8673147195] to /Users/sean/.julia/compiled/v1.7/MPI/jl_AfEwik.; Stacktrace:; [1] error(s::String); @ Base ./error.jl:33; [2] compilecache(pkg::Base.PkgId, path::String, internal_stderr::IO, internal_stdout::IO, ignore_loaded_modules::Bool); @ Base ./loading.jl:1466; [3] compilecache(pkg::Base.PkgId, path::String); @ Base ./loading.jl:1410; [4] _require(pkg::Base.PkgId); @ Base ./loading.jl:1120; [5] require(uuidkey::Base.PkgId); @ Base ./loading.jl:1013; [6] require(into::Module, mod::Symbol); @ Base ./loading.jl:997; [7] include(mod::Module, _path::String); @ Base ./Base.jl:418; [8] include(x::String); @ Oceananigans ~/.julia/packages/Oceananigans/jmNfq/src/Oceananigans.jl:5; [9] top-level scope; @ ~/.julia/packages/Oceananigans/jmNfq/src/Oceananigans.jl:190; [10] include; @ ./Base.jl:418 [inlined]; [11] include_package_for_output(pkg::Base.PkgId, input::String, depot_path::Vector{String}, dl_load_path::Vector{String}, load_path::Vector{String}, concrete_deps::Vector{Pair{Base.PkgId, UInt64}}, source::Nothing); @ Base ./loading.jl:1318; [12] top-level scope; @ none:1; [13] eval; @ ./boot.jl:373 [inlined]; [14] eval(x::Expr); @ Base.MainInclude ./client.jl:453; [15] top-level scope; @ none:1; in expression starting at /Users/sean/.julia/packages/Oceananigans/jmNfq/src/Distributed/Distributed.jl:1; in expression starting at /Users/sean/.julia/packages/Oceananigans/jmNfq/src/Oceananigans.jl:1; ERROR: Failed to precompile Oceananigans [9e8cae18-63c1-5223-a75c-80ca9d6e9a09] to /Users/sean/.j",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2480:4857,load,loading,4857,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2480,1,['load'],['loading']
Performance,"haven't been able to solve it. ```julia; using Oceananigans. grid_base = RectilinearGrid(CPU(), size = (4, 4, 4), extent = (1,1,1)). # The lines immediately below work; using Oceananigans.Grids: boundary_node; boundary_node_ccf = KernelFunctionOperation{Center, Center, Face}(boundary_node, grid_base, Center(), Center(), Face()); c = CenterField(grid_base); compute!(Field(Average(c, condition=(boundary_node_ccf .== true)))). # The last line here doesn't work; grid = ImmersedBoundaryGrid(grid_base, GridFittedBottom((x, y) -> 1/2)); c_ib = CenterField(grid); compute!(Field(Average(c_ib, condition=(boundary_node_ccf .== true)))); ```. The first computation above (which doesn't use any immersed boundaries) works, although it's odd that I have to set `condition=(boundary_node_ccf .== true)`, since it fails if I simply set `condition=boundary_node_ccf`. However, the last line, which does use `ImmersedBoundaryGrid`s, fails with the following error:. ```; ERROR: LoadError: MethodError: condition_operand(::typeof(identity), ::Oceananigans.AbstractOperations.GridMetricOperation{Center, Center, Center, ImmersedBoundaryGrid{Float64, Periodic, Periodic, Bounded, RectilinearGrid{Float64, Periodic, Periodic, Bounded, Float64, Float64, Float64, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, CPU}, GridFittedBottom{OffsetArrays.OffsetMatrix{Float64, Matrix{Float64}}, Oceananigans.ImmersedBoundaries.CenterImmersedCondition}, Nothing, CPU}, Float64, typeof(Oceananigans.Operators.Vᶜᶜᶜ)}, ::Array{Bool, 3}, ::Int64) is ambiguous. Candidates:; condition_operand(func::Function, operand::Oceananigans.Fields.AbstractField, condition::AbstractArray, mask); @ Oceananigan",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3439:1281,Load,LoadError,1281,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3439,1,['Load'],['LoadError']
Performance,"he current design. A ""boundary condition"" is a flux, gradient, or value of a field (`u`, `v`, `w`, `T`, `S`) at the end (`left`, `right`) in a given direction (`x`, `y`, `z`). ### Pros. * Generality: any boundary condition can be specified; * Similarity to the mathematics: this design corresponds to how boundary conditions are specified in PDEs; * Extensibility: the design accommodates changes in *types* of boundary conditions or changes in dimensionality. (Adding new solution variables requires code modification, but that is true throughout the code). ### Concerns. * It's more complicated than allowing only 3-4 possibilities to users; as @johncmarshall54 says:. > It's not obvious to me what a field boundary condition is, vs a coordinate boundary condition.; Fundamentally we are applying boundary conditions to u, v, w, T, and S fields, and that's it. It all seems a bit too complicated. * The hierarchical dependence of this design on parameterized types creates a lot of possibilities for different boundary conditions to be specified, and this is a concern for performance (@ali-ramadhan). * We aren't sure what to call the function (currently `bc.calc`) whose purpose is to return the value of the boundary condition (either the field value, flux, gradient, or whatever) at a given grid point and simulation time. ### Other miscellaneous thoughts. * This PR implements a backend / abstraction system for specifying boundary conditions. For users we can add as much sugar on top as we want. * As we have discussed, the way we implement boundary conditions is intimately connected to the way that we specify equations. This can be seen in the current code (though it is commented out) in that [the lines that specify a no-slip condition depend on the ""vertical viscosity""](https://github.com/climate-machine/Oceananigans.jl/blob/7fb355d77df98e37a80b3e796291fa2856d3e1ce/src/time_steppers.jl#L293). Adding other diffusive terms (hyperdiffusivity, Leith diffusivity, various turbulent diffu",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/118#issuecomment-472372026:1184,perform,performance,1184,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/118#issuecomment-472372026,1,['perform'],['performance']
Performance,"he issue, we'd like to have an issue open... I don't have much hard evidence to provide but I can describe the problem as I've seen it. Basically, very rare, subtle irregularities have been observed on the GPU when using `HydrostaticFreeSurfaceModel` in a `Periodic, Bounded, Bounded` configuration. I think that it is possible the main issue is an interaction (a read-write race condition) associated with both impenetrable boundary conditions and periodic boundary conditions that affects the 8 corner points. The race condition affects model trajectories via the Coriolis force (which is the only term as far as I know that touches the 8 ""corner"" points affected by this race condition). Because the race condition only manifests when a `Coriolis` or `VectorInvariant` stencil touches corner points, it may not affect _most_ `Periodic, Periodic, Bounded` models, which could explain why we haven't caught it. The reason it doesn't affect those models is because this race condition would only affect the corner points of `w`, which are not touched when using an `FPlane` Coriolis model. However, it's possible (I'm not sure) that the race condition could affect models using `NonTraditionalFPlane` in `Periodic, Periodic, Bounded` configurations. More generally, it will also affect models that are bounded in the `y`-direction, because in those models the corner points of the `y`-velocity are affected and also invoked when using `FPlane` or `BetaPlane` coriolis. That's as much as I know. It's very hard to gather information about this bug because it's so rare are subtle. In other words, only one grid point among 10,000 iterations might be affected, and the errors induced are very small. To find this issue, we have to run tens of thousands of iterations of identical models on the GPU, and then compare some statistic of the model (ideally the entire velocity field, but @sandreza has gotten away just comparing something like `[maximum(abs, u), maximum(abs, v), maximum(abs, w)]`). If the",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1985#issuecomment-921143865:1163,race condition,race condition,1163,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1985#issuecomment-921143865,1,['race condition'],['race condition']
Performance,"his design in terms of functionality is that it produces expression trees that interpolate ""too eagerly"". A common example is a turbulent kinetic energy computation:. ```julia; U = AveragedField(u, dims=(1, 2)); V = AveragedField(v, dims=(1, 2)). tke = @at (Center, Center, Center) ((u - U)^2 + (v - V)^2 + w^2); ```. Inspection of this object reveals that while `u - U` does not interpolate, the squaring `(u - U)^2` would be performed at cell centers:. ```julia; julia> tke.args[1].b; 2. julia> tke.args[1].a; BinaryOperation at (Center, Center, Center); ├── grid: RegularRectilinearGrid{Float64, Periodic, Periodic, Bounded}(Nx=1, Ny=1, Nz=1); │ └── domain: x ∈ [0.0, 1.0], y ∈ [0.0, 1.0], z ∈ [-1.0, 0.0]; └── tree: ; - at (Center, Center, Center) via identity;    ├── Field located at (Face, Center, Center);    └── AveragedField over dims=(1, 2) located at (⋅, ⋅, Center) of Field located at (Face, Center, Center); ```. This isn't what we want (usually): instead, we want both `u - U` and subsequent squaring in `(u - U)^2` to be performed at `(Face, Center, Center)`. This PR addresses this issue by getting rid of the third interpolation in `BinaryOperation`, and changing how locations for `BinaryOperation` are inferred. Now, when locations are specified via `@at`, they are taken as a ""suggestion"" that only acts if the two elements of the binary operation have different _concrete_ locations such that a difference needs to be resolved. In other cases (such as a binary operation between fields at common locations, or a binary operation between a field and a number), the location of the members of the operation is preserved. In this way, `BinaryOperations` are ""stubborn"". . We thus have results like. ```julia; julia> *((Center, Center, Center), u, u); BinaryOperation at (Face, Center, Center); ├── grid: RegularRectilinearGrid{Float64, Periodic, Periodic, Bounded}(Nx=1, Ny=1, Nz=1); │ └── domain: x ∈ [0.0, 1.0], y ∈ [0.0, 1.0], z ∈ [-1.0, 0.0]; └── tree: ; * at (Face, Center, Cen",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1599:2333,perform,performed,2333,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1599,1,['perform'],['performed']
Performance,"hmm good to know about possible performance issues, we should be on the lookout. I suspect that (one of the) segfaults is an OOB error that got masked by `@inbounds`, running that now locally.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1514#issuecomment-809493617:32,perform,performance,32,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1514#issuecomment-809493617,1,['perform'],['performance']
Performance,https://github.com/ali-ramadhan/Oceananigans.jl/blob/a41c604e9360ba79ce11efe6d4d6370bf79a3cc6/src/model.jl#L1. It would make model construction much more flexible. Would this hinder performance?,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/42:182,perform,performance,182,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/42,1,['perform'],['performance']
Performance,"https://painterqubits.github.io/Unitful.jl/stable/. The obvious use case would be to replace the units (e.g. `minutes` and `years`) in `Oceananigans.Utils` with ""proper units"" from Unitful.jl. Not sure if we want to use it absolutely everywhere. Would be cool and there should be no performance hit, but might take quite a bit of refactoring. Could also be useful for defining constants in examples and scripts? Might be cool to play around with it a bit. Apparently @alanedelman is gonna be making fun of us for not using it.",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1116:283,perform,performance,283,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1116,1,['perform'],['performance']
Performance,"i-ramadhan thanks for looking into that!. I think a convergence test with coarse horizontal resolution and varying vertical resolution is probably a good idea, perhaps producing a plot like Fig 11 in Vreugdenhil and Taylor. In my reading, `∆y_w^+` is the vertical spacing in 'wall units', which is the physical spacing divided by `nu/u_\tau`, where `nu` is viscosity and `u_\tau` is the friction velocity defined in eq 16. In Table II, values for both `∆y_w^+` and `∆y_c^+` (grid spacing at wall and at domain center) are given. It looks like many of the runs may be feasible. Here's some code:. ```julia; y(j, Ny, Sf, h) = h / tanh(Sf) * tanh(Sf*(2*(j-1)/Ny - 1)) ; Δy(j, Ny, Sf, h) = y(j+1, Ny, Sf, h) - y(j, Ny, Sf, h). N_unstretched(Ny_VT, Sf) = ceil(Int, 2 / Δy(2, Ny_VT, Sf, 1)) . @show N_unstretched(49, 3); @show N_unstretched(65, 2.5); @show N_unstretched(97, 3); @show N_unstretched(129, 2); ```. It looks like some of their runs are feasible, since. ```julia; julia> N_unstretched(49, 3) # runs 2-4; 1141. julia> N_unstretched(65, 2.5) # run 5; 768. julia> N_unstretched(97, 3) # runs 7-11; 2710. julia> N_unstretched(129, 2) # run 12; 805; ```. Runs B-G (which are included to demonstrate resolutions that are apparently inadequate) may also be feasible. One concern about the suitability of stratified Couette flow is mentioned at the end of the paper... > Stratified plane Couette flow is a challenging test case; for the LES model because it has a linearly stable laminar; state which introduces requirement 1. The results of Abkar; and Moin13 suggest that AMD LES performs even better in; other stratified wall-bounded flows. Balancing all these concerns; is key to using the AMD model in LES of wall-bounded; stratified flow. Nevertheless the AMD model is able to capture; turbulent intermittency and mean and turbulent flowproperties; in stratified plane Couette flow. @johncmarshall54 we are planning to implement stretched vertical grid functionality after the LES is implemented.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/310#issuecomment-510240238:1583,perform,performs,1583,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/310#issuecomment-510240238,1,['perform'],['performs']
Performance,"ia/packages/KernelAbstractions/WoCk1/src/extras/loopinfo.jl:28:0: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering; warning: /Users/navid/.julia/packages/KernelAbstractions/WoCk1/src/extras/loopinfo.jl:28:0: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering; warning: /Users/navid/.julia/packages/KernelAbstractions/WoCk1/src/extras/loopinfo.jl:28:0: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering; warning: /Users/navid/.julia/packages/KernelAbstractions/WoCk1/src/extras/loopinfo.jl:28:0: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering; warning: /Users/navid/.julia/packages/KernelAbstractions/WoCk1/src/extras/loopinfo.jl:28:0: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering; warning: /Users/navid/.julia/packages/KernelAbstractions/WoCk1/src/extras/loopinfo.jl:28:0: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering; warning: /Users/navid/.julia/packages/KernelAbstractions/WoCk1/src/extras/loopinfo.jl:28:0: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering; NonhydrostaticModel{CPU, RectilinearGrid}(time = 0 seconds, iteration = 0); ├── ",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3403#issuecomment-1872469814:3347,optimiz,optimizer,3347,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3403#issuecomment-1872469814,2,"['optimiz', 'perform']","['optimizer', 'perform']"
Performance,ia/packages/KernelAbstractions/WoCk1/src/extras/loopinfo.jl:28:0: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering; warning: /Users/navid/.julia/packages/KernelAbstractions/WoCk1/src/extras/loopinfo.jl:28:0: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering; warning: /Users/navid/.julia/packages/KernelAbstractions/WoCk1/src/extras/loopinfo.jl:28:0: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering; warning: /Users/navid/.julia/packages/KernelAbstractions/WoCk1/src/extras/loopinfo.jl:28:0: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering; warning: /Users/navid/.julia/packages/KernelAbstractions/WoCk1/src/extras/loopinfo.jl:28:0: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering; warning: /Users/navid/.julia/packages/KernelAbstractions/WoCk1/src/extras/loopinfo.jl:28:0: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering; warning: /Users/navid/.julia/packages/KernelAbstractions/WoCk1/src/extras/loopinfo.jl:28:0: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering; warning: /Users/navid/.julia/packages/KernelAbstractions/WoCk1/src/extras/loopin,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3403#issuecomment-1872469814:1679,optimiz,optimizer,1679,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3403#issuecomment-1872469814,12,"['optimiz', 'perform']","['optimizer', 'perform']"
Performance,ia/packages/MPI/TKXAj/src/pointtopoint.jl:70 [inlined]; send_south_halo at /orcd/data/raffaele/001/glwagner/Oceananigans.jl/src/DistributedComputations/halo_communication.jl:317; #fill_south_and_north_halo!#50 at /orcd/data/raffaele/001/glwagner/Oceananigans.jl/src/DistributedComputations/halo_communication.jl:263; fill_south_and_north_halo! at /orcd/data/raffaele/001/glwagner/Oceananigans.jl/src/DistributedComputations/halo_communication.jl:250; unknown function (ip: 0x2aaac8afa8b6); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; #fill_halo_event!#40 at /orcd/data/raffaele/001/glwagner/Oceananigans.jl/src/DistributedComputations/halo_communication.jl:208; fill_halo_event! at /orcd/data/raffaele/001/glwagner/Oceananigans.jl/src/DistributedComputations/halo_communication.jl:193; unknown function (ip: 0x2aaac8aefb2e); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; #fill_halo_regions!#38 at /orcd/data/raffaele/001/glwagner/Oceananigans.jl/src/DistributedComputations/halo_communication.jl:114; fill_halo_regions! at /orcd/data/raffaele/001/glwagner/Oceananigans.jl/src/DistributedComputations/halo_communication.jl:101 [inlined]; #fill_halo_regions!#37 at /orcd/data/raffaele/001/glwagner/Oceananigans.jl/src/DistributedComputations/halo_communication.jl:90 [inlined]; fill_halo_regions! at /orcd/data/raffaele/001/glwagner/Oceananigans.jl/src/DistributedComputations/halo_communication.jl:87; unknown function (ip: 0x2aaac8ad0ee5); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; jl_apply at /cache/build/builder-amdci4-4/julial,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3878:2972,cache,cache,2972,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3878,1,['cache'],['cache']
Performance,"ia\packages\KernelAbstractions\Zcyra\src\extras\loopinfo.jl:28:0: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering; warning: C:\Users\kab\.julia\packages\KernelAbstractions\Zcyra\src\extras\loopinfo.jl:28:0: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering; warning: C:\Users\kab\.julia\packages\KernelAbstractions\Zcyra\src\extras\loopinfo.jl:28:0: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering; warning: C:\Users\kab\.julia\packages\KernelAbstractions\Zcyra\src\extras\loopinfo.jl:28:0: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering; warning: C:\Users\kab\.julia\packages\KernelAbstractions\Zcyra\src\extras\loopinfo.jl:28:0: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering; warning: C:\Users\kab\.julia\packages\KernelAbstractions\Zcyra\src\extras\loopinfo.jl:28:0: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering; warning: C:\Users\kab\.julia\packages\KernelAbstractions\Zcyra\src\extras\loopinfo.jl:28:0: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering; NonhydrostaticModel{CPU, RectilinearGrid}(time = 0 seconds, iteration = 0); ├── ",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3494:2749,optimiz,optimizer,2749,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3494,2,"['optimiz', 'perform']","['optimizer', 'perform']"
Performance,ia\packages\KernelAbstractions\Zcyra\src\extras\loopinfo.jl:28:0: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering; warning: C:\Users\kab\.julia\packages\KernelAbstractions\Zcyra\src\extras\loopinfo.jl:28:0: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering; warning: C:\Users\kab\.julia\packages\KernelAbstractions\Zcyra\src\extras\loopinfo.jl:28:0: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering; warning: C:\Users\kab\.julia\packages\KernelAbstractions\Zcyra\src\extras\loopinfo.jl:28:0: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering; warning: C:\Users\kab\.julia\packages\KernelAbstractions\Zcyra\src\extras\loopinfo.jl:28:0: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering; warning: C:\Users\kab\.julia\packages\KernelAbstractions\Zcyra\src\extras\loopinfo.jl:28:0: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering; warning: C:\Users\kab\.julia\packages\KernelAbstractions\Zcyra\src\extras\loopinfo.jl:28:0: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering; warning: C:\Users\kab\.julia\packages\KernelAbstractions\Zcyra\src\extras\loopin,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3494:1359,optimiz,optimizer,1359,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3494,10,"['optimiz', 'perform']","['optimizer', 'perform']"
Performance,"iceArray{Float64,3,CUDA.AS.Global}},OffsetArray{Float64,3,CuDeviceArray{Float64,3,CUDA.AS.Global}},OffsetArray{Float64,3,CuDeviceArray{Float64,3,CUDA.AS.Global}}}},NamedTuple{(:T, :S),Tuple{OffsetArray{Float64,3,CuDeviceArray{Float64,3,CUDA.AS.Global}},OffsetArray{Float64,3,CuDeviceArray{Float64,3,CUDA.AS.Global}}}},Nothing,NamedTuple{(:u, :v, :w, :T, :S),Tuple{ParameterizedForcing{var""#Fu_func#83"",NamedTuple{(:τ,),Tuple{Int64}}},ParameterizedForcing{var""#Fv_func#84"",NamedTuple{(:τ,),Tuple{Int64}}},ParameterizedForcing{var""#Fw_func#85"",NamedTuple{(:τ,),Tuple{Int64}}},typeof(Oceananigans.Forcing.zeroforcing),typeof(Oceananigans.Forcing.zeroforcing)}},OffsetArray{Float64,3,CuDeviceArray{Float64,3,CUDA.AS.Global}},NamedTuple{(:time, :iteration),Tuple{Float64,Int64}}}}; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:maxthreads,),Tuple{Int64}}}) at /home/ancellin/.julia/packages/CUDA/d6WNR/src/compiler/execution.jl:310; [13] #87 at /home/ancellin/.julia/packages/GPUCompiler/4e9CU/src/cache.jl:21 [inlined]; [14] get!(::GPUCompiler.var""#87#88""{Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:maxthreads,),Tuple{Int64}}},typeof(CUDA._cufunction),GPUCompiler.FunctionSpec{typeof(Cassette.overdub),Tuple{Cassette.Context{nametype(CUDACtx),KernelAbstractions.CompilerMetadata{KernelAbstractions.NDIteration.StaticSize{(16, 16, 16)},KernelAbstractions.NDIteration.DynamicCheck,Nothing,Nothing,KernelAbstractions.NDIteration.NDRange{3,KernelAbstractions.NDIteration.StaticSize{(1, 1, 16)},KernelAbstractions.NDIteration.StaticSize{(16, 16, 1)},Nothing,Nothing}},Nothing,KernelAbstractions.var""##PassType#253"",Nothing,Cassette.DisableHooks},typeof(Oceananigans.TimeSteppers.gpu_calculate_Gu!),OffsetArray{Float64,3,CuDeviceArray{Float64,3,CUDA.AS.Global}},RegularCartesianGrid{Float64,Periodic,Periodic,Bounded,OffsetArray{Float64,1,StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}}},CenteredSecondOrder,Nothing,Nothing,IsotropicDiff",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/882:41970,cache,cache,41970,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/882,1,['cache'],['cache']
Performance,igans.jl/examples/geostrophic_adjustment.jl:92; jl_gc_pool_alloc at /buildworker/worker/package_linux64/build/src/gc.c:1148; jl_gc_alloc_ at /buildworker/worker/package_linux64/build/src/julia_internal.h:277 [inlined]; jl_gc_alloc at /buildworker/worker/package_linux64/build/src/gc.c:3150; jl_gc_alloc_buf at /buildworker/worker/package_linux64/build/src/julia_internal.h:304 [inlined]; array_resize_buffer at /buildworker/worker/package_linux64/build/src/array.c:686; jl_array_grow_at_end at /buildworker/worker/package_linux64/build/src/array.c:875 [inlined]; jl_array_grow_end at /buildworker/worker/package_linux64/build/src/array.c:939; jl_array_sizehint at /buildworker/worker/package_linux64/build/src/array.c:1139; sizehint! at ./array.jl:1103 [inlined]; BitSet at ./bitset.jl:18; BitSet at ./bitset.jl:29 [inlined]; construct_ssa! at ./compiler/ssair/slot2ssa.jl:780; slot2reg at ./compiler/ssair/driver.jl:127 [inlined]; run_passes at ./compiler/ssair/driver.jl:134; optimize at ./compiler/optimize.jl:174; typeinf at ./compiler/typeinfer.jl:33; abstract_call_method_with_const_args at ./compiler/abstractinterpretation.jl:266; abstract_call_gf_by_type at ./compiler/abstractinterpretation.jl:134; abstract_call_known at ./compiler/abstractinterpretation.jl:904; abstract_call at ./compiler/abstractinterpretation.jl:926; abstract_call at ./compiler/abstractinterpretation.jl:911; abstract_eval at ./compiler/abstractinterpretation.jl:1005; typeinf_local at ./compiler/abstractinterpretation.jl:1270; typeinf_nocycle at ./compiler/abstractinterpretation.jl:1326; typeinf at ./compiler/typeinfer.jl:12; abstract_call_method_with_const_args at ./compiler/abstractinterpretation.jl:266; abstract_call_gf_by_type at ./compiler/abstractinterpretation.jl:134; abstract_call_known at ./compiler/abstractinterpretation.jl:904; abstract_call at ./compiler/abstractinterpretation.jl:926; abstract_call at ./compiler/abstractinterpretation.jl:911; abstract_eval at ./compiler/abstractinterpretation.jl,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1513#issuecomment-809634040:1729,optimiz,optimize,1729,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1513#issuecomment-809634040,1,['optimiz'],['optimize']
Performance,"ile), '/Users/sean/.julia/artifacts/48a9a608db31268626d8b8d4d1272c3e7ccbf7d5/lib/./libquadmath.0.dylib' (no such file), '/Applications/Julia-1.7.app/Contents/Resources/julia/lib/julia/libquadmath.0.dylib' (no such file), '/Applications/Julia-1.7.app/Contents/Resources/julia/bin/../lib/libquadmath.0.dylib' (no such file), '/usr/local/lib/libquadmath.0.dylib' (no such file), '/usr/lib/libquadmath.0.dylib' (no such file); Stacktrace:; [1] dlopen(s::String, flags::UInt32; throw_error::Bool); @ Base.Libc.Libdl ./libdl.jl:117; [2] dlopen(s::String, flags::UInt32); @ Base.Libc.Libdl ./libdl.jl:117; [3] macro expansion; @ ~/.julia/packages/JLLWrappers/QpMQW/src/products/library_generators.jl:54 [inlined]; [4] __init__(); @ MPICH_jll ~/.julia/packages/MPICH_jll/dhUyI/src/wrappers/aarch64-apple-darwin-libgfortran5.jl:32; [5] _include_from_serialized(path::String, depmods::Vector{Any}); @ Base ./loading.jl:768; [6] _require_search_from_serialized(pkg::Base.PkgId, sourcepath::String); @ Base ./loading.jl:854; [7] _require(pkg::Base.PkgId); @ Base ./loading.jl:1097; [8] require(uuidkey::Base.PkgId); @ Base ./loading.jl:1013; [9] require(into::Module, mod::Symbol); @ Base ./loading.jl:997; [10] top-level scope; @ ~/.julia/packages/MPI/08SPr/deps/deps.jl:8; [11] include(mod::Module, _path::String); @ Base ./Base.jl:418; [12] include(x::String); @ MPI ~/.julia/packages/MPI/08SPr/src/MPI.jl:1; [13] top-level scope; @ ~/.julia/packages/MPI/08SPr/src/MPI.jl:36; [14] include; @ ./Base.jl:418 [inlined]; [15] include_package_for_output(pkg::Base.PkgId, input::String, depot_path::Vector{String}, dl_load_path::Vector{String}, load_path::Vector{String}, concrete_deps::Vector{Pair{Base.PkgId, UInt64}}, source::String); @ Base ./loading.jl:1318; [16] top-level scope; @ none:1; [17] eval; @ ./boot.jl:373 [inlined]; [18] eval(x::Expr); @ Base.MainInclude ./client.jl:453; [19] top-level scope; @ none:1; during initialization of module MPICH_jll; in expression starting at /Users/sean/.julia/packag",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2480:3246,load,loading,3246,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2480,1,['load'],['loading']
Performance,"imilar).; >; > What does the jargon ""sounding"" mean?; >; > Merger means we need to think carefully about how to reduce boilerplate to; > minimize (within reason) the cost of maintaining two parallel models as we; > develop their shared subcomponents.; >; > I wonder if splitting off the output writers and diagnostics into a; > Simulation type that wraps AbstractModel may help. In this paradigm, a; > ""Model"" is reduced to numerics + physics specification. This would be easy; > to implement (while I think an Equation abstraction would be relatively; > difficult due to myriad difficult design problems, including the; > abstraction of tuples of terms with heterogeneous function signatures; > numerical aspects of the equation, implicit vs explicit treatment of terms,; > etc).; > The Simulation type can then be used to ""run"" simulations over multiple; > time steps, eg; >; > simulation = Simulation(model, Δt=1.0, end_time=8day, output=output_writers, diagnostics=diagnostics); > run!(simulation); >; > and is tasked with managing things like output writing, diagnostics; > calculation, adaptive time-stepping, and progress logging. Each Model; > then simply needs to define a function that performs a single time-step to; > interface with Simulation. This is discussed further in #447; > <https://github.com/climate-machine/Oceananigans.jl/issues/447>. Such a; > orthogonalization of the code means we can develop the Simulation; > abstraction without having to worry about updating each AbstractModel; > individually.; >; > —; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub; > <https://github.com/climate-machine/Oceananigans.jl/issues/605?email_source=notifications&email_token=AKXUEQRA7COBFTVWBSDE3ATRALQCRA5CNFSM4KNLLK52YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEKLF25Y#issuecomment-580279671>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AKXUEQSOSWBJGZJKTTSGCZ3RALQCRANCNFSM4KNLLK5Q>; > .; >",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/605#issuecomment-580309805:2284,perform,performs,2284,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/605#issuecomment-580309805,1,['perform'],['performs']
Performance,"imulation initialization complete (112.172 ms); [ Info: Executing initial time step...; [ Info: ... initial time step complete (3.376 seconds).; [ Info: Simulation is stopping after running for 7.945 seconds.; [ Info: Model iteration 200 equals or exceeds stop iteration 200.; ```. I should make it clear that `CFL=10` is not large enough to make particles move out of the domain. `CFL` should be larger than `Nx`, because the distance at which a particle moves in one time step needs to larger than the domain size \(not the grid size\). That means $u \Delta t > L_x$, which is equivalent to `CFL > Nx`. Using a `CFL` of `51` (`Nx = 50` in this case) reproduces the error on a CPU:; ```; [ Info: Initializing simulation...; [ Info: ... simulation initialization complete (108.842 ms); [ Info: Executing initial time step...; [ Info: ... initial time step complete (3.303 seconds).; ERROR: LoadError: BoundsError: attempt to access 56×56×56 OffsetArray(::Array{Float64, 3}, -2:53, -2:53, -2:53) with eltype Float64 with indices -2:53×-2:53×-2:53 at index [54, 49, 1]; Stacktrace:; [1] throw_boundserror(A::OffsetArrays.OffsetArray{Float64, 3, Array{Float64, 3}}, I::Tuple{Int64, Int64, Int64}); @ Base ./abstractarray.jl:744; [2] checkbounds; @ ./abstractarray.jl:709 [inlined]; [3] getindex; @ ~/.julia/packages/OffsetArrays/0MOrf/src/OffsetArrays.jl:420 [inlined]; [4] getindex; @ ~/Documents/IdealizedOceanWorlds.jl/OceananigansMemeoryIssue.jl/dev/Oceananigans/src/Fields/field.jl:399 [inlined]; [5] getindex; @ ~/Documents/IdealizedOceanWorlds.jl/OceananigansMemeoryIssue.jl/dev/Oceananigans/src/Utils/sum_of_arrays.jl:23 [inlined]; [6] _interpolate; @ ~/Documents/IdealizedOceanWorlds.jl/OceananigansMemeoryIssue.jl/dev/Oceananigans/src/Fields/interpolate.jl:148 [inlined]; [7] interpolate; @ ~/Documents/IdealizedOceanWorlds.jl/OceananigansMemeoryIssue.jl/dev/Oceananigans/src/Fields/interpolate.jl:197 [inlined]; [8] advect_particle; @ ~/Documents/IdealizedOceanWorlds.jl/OceananigansMemeoryIs",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3320#issuecomment-1773032135:1048,Load,LoadError,1048,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3320#issuecomment-1773032135,1,['Load'],['LoadError']
Performance,"in general. We currently handle this only during time-stepping --- there is no guarantee, for example, that the diffusivities or ghost cell values are consistent with the velocity field if the velocity field is set by the function `set!` (in fact, the diffusivities are *always* inconsistent with the current velocity field due to the fact that they are calculated *prior* to taking a time-step). If we want to guarantee such a consistency, we can develop the concept of a model ""state"" and apply it to both ocean and atmospheric models (eg a function `update_state!(model)`, or something similar). What does the jargon ""sounding"" mean?. Merger means we need to think carefully about how to reduce boilerplate to minimize (within reason) the cost of maintaining two parallel models as we develop their shared subcomponents. I wonder if splitting off the output writers and diagnostics into a `Simulation` type that wraps `AbstractModel` may help. In this paradigm, a ""Model"" is reduced to numerics + physics specification. This would be easy to implement (while I think an `Equation` abstraction would be relatively difficult due to myriad difficult design problems, including the abstraction of tuples of terms with heterogeneous function signatures, numerical aspects of the equation, implicit vs explicit treatment of terms, etc).; The `Simulation` type can then be used to ""run"" simulations over multiple time steps, eg. ```julia; simulation = Simulation(model, Δt=1.0, end_time=8day, output=output_writers, diagnostics=diagnostics); run!(simulation); ```. and is tasked with managing things like output writing, diagnostics calculation, adaptive time-stepping, and progress logging. Each `Model` then simply needs to define a function that performs a single time-step to interface with `Simulation`. This is discussed further in #447. Such a orthogonalization of the code means we can develop the `Simulation` abstraction without having to worry about updating each `AbstractModel` individually.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/605#issuecomment-580279671:2063,perform,performs,2063,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/605#issuecomment-580279671,1,['perform'],['performs']
Performance,ined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; jl_apply at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/julia.h:1982 [inlined]; do_call at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:126; eval_value at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:223; eval_stmt_value at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:174 [inlined]; eval_body at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:617; jl_interpret_toplevel_thunk at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:775; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:934; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:877; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:877; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:877; ijl_toplevel_eval_in at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:985; eval at ./boot.jl:385 [inlined]; eval_user_input at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:150; repl_backend_loop at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:246; #start_repl_backend#46 at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:231; start_repl_backend at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:228; _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amd,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3852:6055,cache,cache,6055,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3852,1,['cache'],['cache']
Performance,"ing in our current project.; > ; > My last attempt at improving the NetCDF solved many of the issues with the package JuliaGeo/NetCDF.jl#61 but was not merged because of conflicts with other bugfix PRs. However, might be source of inspiration if someone wants to do a rewrite.; > ; > Regarding write performance, I would be very interested to see examples where NetCDF.jl performs worse than e.g. python-netcdf4, since most of the time should be spent in the same NetCDF C library. I have been using the package extensively and did not experience it to be slower than comparable packages.; > ; > I you are worried about the robustness of NetCDF.jl, you should not even look at ZarrNative.jl, since it is still very young and rather a prototype.; > ; > I would be very happy to discuss the issues further, maybe in a call? Would also be interested to learn about your project which seems to be very cool. Thanks so much for working on NetCDF.jl! I didn't mean to sound ungrateful about NetCDF.jl's performance. We were just debating which package to use. With https://github.com/JuliaGeo/NetCDF.jl/issues/87 fixed, I think we'll be happy for a long time. The `compress=9` bug explains why the IO was slow. @glwagner has suggested that for a project of our scale we'd want to help and contribute to the packages we use. We definitely want to stick with NetCDF as it's the _de facto_ standard in the climate, atmospheric, and ocean sciences. A discussion might be helpful down the line. With faster IO I think we're happy now and we're still figuring how to do IO long-term. > My feeling is that if you want to write NetCDF files through the HDF API that it will be more work, though I never tried.; > ; > Regarding NetCDF.jl & NCDatasets.jl, I feel that the statements in the OP that NetCDF.jl is not being maintained and that NCDatasets.jl grew out of bugs not being fixed is are a bit of a misrepresentation.; > ; > For installations and dependency reduction, hopefully the new HDF5 release, which wil",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/145#issuecomment-476298847:1934,perform,performance,1934,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/145#issuecomment-476298847,1,['perform'],['performance']
Performance,"ing models we could just export `accurate_cell_advection_timescale` so that adaptive time-stepping with accurate CFL is opt-in, e.g. via an `AccurateTimeStepWizard` or something. Motivation for using Tullio.jl:. Originally I tried just using `mapreduce` but couldn't get it to work without a nested `mapreduce` which would trigger tons of kernel launches: https://discourse.julialang.org/t/is-it-possible-to-do-a-mapreduce-with-multiple-arrays-while-broadcasting-over-so/57433. Then I found out about Tullio.jl (super neat package we might want to use more) which allows us to compute CFL and reduce it down elegantly, however, reducing to one scalar does not currently work so I have to reduce the 3D CFL calculation to a 1D vector: https://github.com/mcabbott/Tullio.jl/issues/91. Apparently optimizing reductions on the GPU is non-trivial but can be done: https://developer.download.nvidia.com/compute/cuda/1.1-Beta/x86_website/projects/reduction/doc/reduction.pdf Not sure if an optimized reduction kernel is available in Julia/CUDA.jl but maybe they will be in the future. @maleadt or @vchuravy might know... ```; julia> model.architecture; CPU(). julia> size(grid); (32, 32, 32). julia> @btime time_step!(model, 1); 5.977 ms (2090 allocations: 343.97 KiB). julia> @btime cfl(model); 127.870 μs (10 allocations: 2.36 KiB); ```. ```; julia> model.architecture; GPU(). julia> size(grid); (32, 32, 32). julia> @btime time_step!(model, 1); 2.632 ms (7058 allocations: 820.19 KiB). julia> @btime cfl(model); 1.294 ms (257 allocations: 25.36 KiB); ```. ```; julia> model.architecture; CPU(). julia> size(grid); (256, 256, 256). julia> @btime time_step!(model, 1); 3.378 s (2090 allocations: 343.97 KiB). julia> @btime cfl(model); 134.737 ms (10 allocations: 4.16 KiB); ```. ```; julia> model.architecture; GPU(). julia> size(grid); (256, 256, 256). julia> @btime time_step!(model, 1); 22.507 ms (7066 allocations: 1.09 MiB). julia> @btime cfl(model); 128.361 ms (257 allocations: 41.53 KiB); ```. Resol",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1497:1561,optimiz,optimized,1561,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1497,1,['optimiz'],['optimized']
Performance,"inish this discussion I think.; > ; > Some notes:; > ; > * The same code on CPUs isn't even close to have the same slowdown. So there's definitely something going on here for GPUs; > * I have struggled with this for quite some time until I found the culprit, so if we can't immediately find and fix the cause I'd suggest leaving a caution statement on the simulation tips page. But I agree it has to be less strongly worded...; > ; > I'll try to provide a MWE that reproduces the behavior, but I'm currently having trouble getting my hands on some GPU, so I'm not sure how fast I can do that. Ok, no rush!. Trig functions aren't generically slower on GPUs than CPUs. On CPUs I think our code is fairly non-optimal right now, so various sources of overhead (eg non-optimal threading) can ""hide"" slow operations on the CPU. On the GPU we are more efficient, so overall speed might depend more sensitively on user code when it's injected. (I'd also argue that the beginning of this section is a bit misleading in how it claims we ""try to optimize"" internal source code. In fact, we have performed almost no performance optimization, and this is an important topic for future work.). I found this reference for the cost of various floating point operations on the CPU:. https://latkin.org/blog/2014/11/09/a-simple-benchmark-of-various-math-operations/. We could reproduce this chart on a GPU with CUDA.jl if we want to provide some useful information to users. I think if we're talking about a _constant_ (the current case), then precomputation hardly harms code complexity (both examples are equally readable to me). Precomputing an _array_ is another story (for example, a forcing function or boundary condition that depends on `sin(x)`). This lesson is definitely not restricted to trigonometric functions or the GPU. The basic principle here is that _there is a trade-off_ between precomputing a potentially expensive operation, and performing it on-the-fly. For constants, precomputation is harmless.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2029#issuecomment-952107151:1232,optimiz,optimize,1232,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2029#issuecomment-952107151,1,['optimiz'],['optimize']
Performance,"ion; @ ~/.julia/packages/GPUCompiler/Cp7sE/src/driver.jl:440 [inlined]; [3] macro expansion; @ ~/.julia/packages/TimerOutputs/RsWnF/src/TimerOutput.jl:253 [inlined]; [4] macro expansion; @ ~/.julia/packages/GPUCompiler/Cp7sE/src/driver.jl:439 [inlined]; [5] emit_llvm(job::GPUCompiler.CompilerJob; libraries::Bool, toplevel::Bool, optimize::Bool, cleanup::Bool, only_entry::Bool, validate::Bool); @ GPUCompiler ~/.julia/packages/GPUCompiler/Cp7sE/src/utils.jl:89; [6] emit_llvm; @ ~/.julia/packages/GPUCompiler/Cp7sE/src/utils.jl:83 [inlined]; [7] codegen(output::Symbol, job::GPUCompiler.CompilerJob; libraries::Bool, toplevel::Bool, optimize::Bool, cleanup::Bool, strip::Bool, validate::Bool, only_entry::Bool, parent_job::Nothing); @ GPUCompiler ~/.julia/packages/GPUCompiler/Cp7sE/src/driver.jl:129; [8] codegen; @ ~/.julia/packages/GPUCompiler/Cp7sE/src/driver.jl:110 [inlined]; [9] compile(target::Symbol, job::GPUCompiler.CompilerJob; libraries::Bool, toplevel::Bool, optimize::Bool, cleanup::Bool, strip::Bool, validate::Bool, only_entry::Bool); @ GPUCompiler ~/.julia/packages/GPUCompiler/Cp7sE/src/driver.jl:106; [10] compile; @ ~/.julia/packages/GPUCompiler/Cp7sE/src/driver.jl:98 [inlined]; [11] #45; @ ~/.julia/packages/Metal/lnkVP/src/compiler/compilation.jl:57 [inlined]; [12] JuliaContext(f::Metal.var""#45#46""{GPUCompiler.CompilerJob{GPUCompiler.MetalCompilerTarget, Metal.MetalCompilerParams}}); @ GPUCompiler ~/.julia/packages/GPUCompiler/Cp7sE/src/driver.jl:47; [13] compile(job::GPUCompiler.CompilerJob); @ Metal ~/.julia/packages/Metal/lnkVP/src/compiler/compilation.jl:56; [14] actual_compilation(cache::Dict{Any, Any}, src::Core.MethodInstance, world::UInt64, cfg::GPUCompiler.CompilerConfig{GPUCompiler.MetalCompilerTarget, Metal.MetalCompilerParams}, compiler::typeof(Metal.compile), linker::typeof(Metal.link)); @ GPUCompiler ~/.julia/packages/GPUCompiler/Cp7sE/src/execution.jl:125; [15] cached_compilation(cache::Dict{Any, Any}, src::Core.MethodInstance, cfg::GPUCompiler.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2618#issuecomment-1731573822:36980,optimiz,optimize,36980,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2618#issuecomment-1731573822,1,['optimiz'],['optimize']
Performance,"it::Bool}); @ FileWatching.Pidfile /glade/u/apps/casper/23.10/spack/opt/spack/julia/1.10.2/gcc/7.5.0/apod/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:93; [6] #mkpidlock#6 ; @ /glade/u/apps/casper/23.10/spack/opt/spack/julia/1.10.2/gcc/7.5.0/apod/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:88 [inlined]; [7] trymkpidlock(::Function, ::Vararg{Any}; kwargs::@Kwargs{stale_age::Int64}); @ FileWatching.Pidfile /glade/u/apps/casper/23.10/spack/opt/spack/julia/1.10.2/gcc/7.5.0/apod/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:111; [8] #invokelatest#2; @ ./essentials.jl:894 [inlined]; [9] invokelatest; @ ./essentials.jl:889 [inlined]; [10] maybe_cachefile_lock(f::Base.var""#968#969""{Base.PkgId}, pkg::Base.PkgId, srcpath::String; stale_age::Int64); @ Base ./loading.jl:2983; [11] maybe_cachefile_lock; @ ./loading.jl:2980 [inlined]; [12] _require(pkg::Base.PkgId, env::String); @ Base ./loading.jl:1970; [13] __require_prelocked(uuidkey::Base.PkgId, env::String); @ Base ./loading.jl:1812; [14] #invoke_in_world#3; @ ./essentials.jl:926 [inlined]; [15] invoke_in_world; @ ./essentials.jl:923 [inlined]; [16] _require_prelocked(uuidkey::Base.PkgId, env::String); @ Base ./loading.jl:1803; [17] macro expansion; @ ./loading.jl:1790 [inlined]; [18] macro expansion; @ ./lock.jl:267 [inlined]; [19] __require(into::Module, mod::Symbol); @ Base ./loading.jl:1753; [20] #invoke_in_world#3; @ ./essentials.jl:926 [inlined]; [21] invoke_in_world; @ ./essentials.jl:923 [inlined]; [22] require(into::Module, mod::Symbol); @ Base ./loading.jl:1746; [23] include; @ ./Base.jl:495 [inlined]; [24] include_package_for_output(pkg::Base.PkgId, input::String, depot_path::Vector{String}, dl_load_path::Vector{String}, load_path::Vector{String}, concrete_deps::Vector{Pair{Base.PkgId, UInt128}}, source::String); @ Base ./loading.jl:2222; [25] top-level scope; @ stdin:3; in expression starting at /glade/u/home/knudsenl/.julia/packages/CUDA/Tl08O/src/CUDA.jl:1; in expression starting at stdin",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2231635812:1918,load,loading,1918,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2231635812,1,['load'],['loading']
Performance,"itle=""Incompressible Model Benchmarks: CPU/GPU""); > benchmark/benchmark_incompressible_model.jl:benchmarks_pretty_table(df, title=""Incompressible model benchmarks""); > benchmark/benchmark_incompressible_model.jl: benchmarks_pretty_table(df_Δ, title=""Incompressible model CPU to GPU speedup""); > benchmark/strong_scaling_incompressible_model.jl: @info ""Benchmarking distributed incompressible model strong scaling with $(typeof(decomposition)) decomposition [N=($Nx, $Ny, $Nz), ranks=($Rx, $Ry, $Rz)]...""; > benchmark/strong_scaling_incompressible_model.jl:benchmarks_pretty_table(df, title=""Incompressible model strong scaling benchmark""); > benchmark/strong_scaling_incompressible_model.jl:benchmarks_pretty_table(df_Δ, title=""Incompressible model strong scaling speedup""); > benchmark/README.md:Running the `benchmark_regression.jl` script will run the incompressible model tests on the current branch and on the master branch for comparison. This is useful to test whether the current branch slows down the code or introduces any performance regression.; > benchmark/benchmark_vertically_stretched_incompressible_model.jl:benchmarks_pretty_table(df, title=""Vertically-stretched incompressible model benchmarks""); > benchmark/benchmark_vertically_stretched_incompressible_model.jl: benchmarks_pretty_table(df_Δ, title=""Vertically-stretched incompressible model CPU to GPU speedup""); > benchmark/strong_scaling_incompressible_model_single.jl:@info ""Setting up distributed incompressible model with N=($Nx, $Ny, $Nz) grid points and ranks=($Rx, $Ry, $Rz) ($decomposition decomposition) on rank $local_rank...""; > benchmark/strong_scaling_incompressible_model_single.jl:@info ""Warming up distributed incompressible model on rank $local_rank...""; > benchmark/strong_scaling_incompressible_model_single.jl:@info ""Benchmarking distributed incompressible model on rank $local_rank...""; > docs/src/physics/incompressible_model.md:# Incompressible model; > docs/src/numerical_implementation/time_stepping.md",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1870#issuecomment-882147932:1370,perform,performance,1370,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1870#issuecomment-882147932,1,['perform'],['performance']
Performance,"ivity` (with different horizontal and vertical diffusivities --- for lack of a better term). There is also an abstraction --- we have `ScalarDiffusivity`s and `TensorDiffusivity`s. The `DirectionalDiffusivity` is an example of a tensor diffusivity. I would like to add docs before merging. Please review the code and let me know what can be improved while I work on docs, and suggest improvements to the doc strings. There are some unit tests included in this PR. Please take a look and suggest new ones. There is significant notation associated with this PR. I don't think we need to finalize the notation here, but comments are welcome. This PR makes a significant contribution to `Oceananigans`' suite of operators: we introduce difference and interpolation operators that act on functions, thereby permitting the *composition* of such operations. The notation for a difference operator denoted by `δ` would be. `δx_caa` or `δx_faa` . where the three letters `caa` denote that differencing is performed at the location ""x cell, any y location, any z location"". The three letter format is an unambiguous specification of the location of a field in three dimensions on a staggered grid. Note that in this PR I have only defined partial derivative operators in this format, rather than difference operators, as we currently only accommodate regular grids. An interpolation operator is denoted by ""blacktriangleright"", so that. `▶x_caa`. denotes the interpolation of a field from `caa` to `faa`. Note that the destination of the operation is implied. Using composition, we also define double interpolations, such as. `▶xy_ffa`,. which interpolates a field located at `ffa` (x face, y face, and any z location) to `cca`. . There are 12 such double interpolation operators and they are essential for specifying eddy diffusivity closures based on strain rate. I think that using composition to its fullest will ultimately reduce the code length, make the code easier to maintain, and will probably make t",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/234:2349,perform,performed,2349,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/234,1,['perform'],['performed']
Performance,"ization.jl:50; │ [8] (::CUDA.var""#697#cache_fptr!#11"")() at /home/fpoulin/.julia/packages/CUDA/0p5fn/lib/utils/call.jl:31; │ [9] macro expansion at /home/fpoulin/.julia/packages/CUDA/0p5fn/lib/utils/call.jl:39 [inlined]; │ [10] macro expansion at /home/fpoulin/.julia/packages/CUDA/0p5fn/lib/cudadrv/libcuda.jl:35 [inlined]; │ [11] macro expansion at /home/fpoulin/.julia/packages/CUDA/0p5fn/lib/cudadrv/error.jl:102 [inlined]; │ [12] cuDeviceGetCount(::Base.RefValue{Int32}) at /home/fpoulin/.julia/packages/CUDA/0p5fn/lib/utils/call.jl:93; │ [13] length at /home/fpoulin/.julia/packages/CUDA/0p5fn/lib/cudadrv/devices.jl:111 [inlined]; │ [14] iterate at /home/fpoulin/.julia/packages/CUDA/0p5fn/lib/cudadrv/devices.jl:106 [inlined] (repeats 2 times); │ [15] iterate at ./iterators.jl:139 [inlined]; │ [16] iterate at ./iterators.jl:138 [inlined]; │ [17] __init__() at /home/fpoulin/software/Oceananigans.jl/src/Oceananigans.jl:178; │ [18] _include_from_serialized(::String, ::Array{Any,1}) at ./loading.jl:697; │ [19] _require_search_from_serialized(::Base.PkgId, ::String) at ./loading.jl:782; │ [20] _require(::Base.PkgId) at ./loading.jl:1007; │ [21] require(::Base.PkgId) at ./loading.jl:928; │ [22] require(::Module, ::Symbol) at ./loading.jl:923; │ [23] eval(::Module, ::Any) at ./boot.jl:331; │ [24] eval_user_input(::Any, ::REPL.REPLBackend) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/REPL/src/REPL.jl:134; │ [25] repl_backend_loop(::REPL.REPLBackend) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/REPL/src/REPL.jl:195; │ [26] start_repl_backend(::REPL.REPLBackend, ::Any) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/REPL/src/REPL.jl:180; │ [27] run_repl(::REPL.AbstractREPL, ::Any; backend_on_current_task::Bool) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/REPL/src/REPL.jl:292; │ [28] run_repl(::REPL.AbstractREPL, ::Any) at /buildworker/worker/package_linux64/build/usr/sha",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1189:2162,load,loading,2162,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1189,1,['load'],['loading']
Performance,"ized to the JLD2 file, and cannot be read outside of Julia. You might want to serialize the grid when checkpointing to easily restore from a checkpoint file. But when saving the grid to a JLD2 output file, which may be read by a language other than Julia, the grid properties should be saved in a language-agnostic manner. Same for boundary conditions. So I changed the way structs are saved to disk for both the JLD2 output writer and the checkpointer. It's all done recursively via multiple dispatch so it should be flexible enough to work for all current `Model` properties and it should accomodate future changes to `Model` with minor changes. When saving stuff to disk like a JLD2 file, `saveproperty!` is used, which converts Julia objects to language-agnostic objects. When checkpointing, `serializeproperty!` is used, which serializes objects, with fields and boundary conditions require special treatment. We checkpoint structs that are important for timestepping. Diagnostics and output writers are not checkpointed, as they are not essential and can be added in any time after model constructions. But if one or more boundary conditions contain a function, `model.boundary_conditions` are not serialized and must be manually restored. There is one mess bits associated with restoring from a checkpoint:; * Fields cannot be passed to the `Model` constructor. When restoring fields we want to avoid loading a field from disk and allocating `Model` memory for it at the same time, as we won't be able to restore models whose memory footprint exceed ~half the CPU/GPU memory. Thus restoring fields is treated as a special case (see `restore_fields!`). It is done after model creation where fields are read from disk and used to fill up existing model fields. Unfortunately, `model.timestepper` doesn't fit the pattern and is treated as an extra special case. @glwagner Let me know what you think. This PR has been open for a while so I'd like to merge it ASAP and work on more pressing issues.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/326#issuecomment-524941619:1541,load,loading,1541,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/326#issuecomment-524941619,1,['load'],['loading']
Performance,"jl:1753; [20] #invoke_in_world#3; @ ./essentials.jl:926 [inlined]; [21] invoke_in_world; @ ./essentials.jl:923 [inlined]; [22] require(into::Module, mod::Symbol); @ Base ./loading.jl:1746; [23] include; @ ./Base.jl:495 [inlined]; [24] include_package_for_output(pkg::Base.PkgId, input::String, depot_path::Vector{String}, dl_load_path::Vector{String}, load_path::Vector{String}, concrete_deps::Vector{Pair{Base.PkgId, UInt128}}, source::String); @ Base ./loading.jl:2222; [25] top-level scope; @ stdin:3; in expression starting at /glade/u/home/knudsenl/.julia/packages/Oceananigans/M82LU/src/Oceananigans.jl:1; in expression starting at stdin:3; ERROR: LoadError: Failed to precompile Oceananigans [9e8cae18-63c1-5223-a75c-80ca9d6e9a09] to ""/glade/u/home/knudsenl/.julia/compiled/v1.10/Oceananigans/jl_k7YOZN"".; Stacktrace:; [1] error(s::String); @ Base ./error.jl:35; [2] compilecache(pkg::Base.PkgId, path::String, internal_stderr::IO, internal_stdout::IO, keep_loaded_modules::Bool); @ Base ./loading.jl:2468; [3] compilecache; @ ./loading.jl:2340 [inlined]; [4] (::Base.var""#968#969""{Base.PkgId})(); @ Base ./loading.jl:1974; [5] mkpidlock(f::Base.var""#968#969""{Base.PkgId}, at::String, pid::Int32; kwopts::@Kwargs{stale_age::Int64, wait::Bool}); @ FileWatching.Pidfile /glade/u/apps/casper/23.10/spack/opt/spack/julia/1.10.2/gcc/7.5.0/apod/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:93; [6] #mkpidlock#6; @ /glade/u/apps/casper/23.10/spack/opt/spack/julia/1.10.2/gcc/7.5.0/apod/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:88 [inlined]; [7] trymkpidlock(::Function, ::Vararg{Any}; kwargs::@Kwargs{stale_age::Int64}); @ FileWatching.Pidfile /glade/u/apps/casper/23.10/spack/opt/spack/julia/1.10.2/gcc/7.5.0/apod/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:111; [8] #invokelatest#2; @ ./essentials.jl:894 [inlined]; [9] invokelatest; @ ./essentials.jl:889 [inlined]; [10] maybe_cachefile_lock(f::Base.var""#968#969""{Base.PkgId}, pkg::Base.PkgId, srcpath::String; stale_ag",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2231635812:5873,load,loading,5873,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2231635812,1,['load'],['loading']
Performance,"jl:93; │ [6] #mkpidlock#6; │ @ ~/julia-1.10/usr/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:88 [inlined]; │ [7] trymkpidlock(::Function, ::Vararg{Any}; kwargs::@Kwargs{stale_age::Int64}); │ @ FileWatching.Pidfile ~/julia-1.10/usr/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:111; │ [8] #invokelatest#2; │ @ ./essentials.jl:894 [inlined]; │ [9] invokelatest; │ @ ./essentials.jl:889 [inlined]; │ [10] maybe_cachefile_lock(f::Base.var""#968#969""{Base.PkgId}, pkg::Base.PkgId, srcpath::String; stale_age::Int64); │ @ Base ./loading.jl:2983; │ [11] maybe_cachefile_lock; │ @ ./loading.jl:2980 [inlined]; │ [12] _require(pkg::Base.PkgId, env::Nothing); │ @ Base ./loading.jl:1970; │ [13] __require_prelocked(uuidkey::Base.PkgId, env::Nothing); │ @ Base ./loading.jl:1812; │ [14] #invoke_in_world#3; │ @ ./essentials.jl:926 [inlined]; │ [15] invoke_in_world; │ @ ./essentials.jl:923 [inlined]; │ [16] _require_prelocked; │ @ ./loading.jl:1803 [inlined]; │ [17] _require_prelocked; │ @ ./loading.jl:1802 [inlined]; │ [18] run_extension_callbacks(extid::Base.ExtensionId); │ @ Base ./loading.jl:1295; │ [19] run_extension_callbacks(pkgid::Base.PkgId); │ @ Base ./loading.jl:1330; │ [20] run_package_callbacks(modkey::Base.PkgId); │ @ Base ./loading.jl:1164; │ [21] _tryrequire_from_serialized(modkey::Base.PkgId, path::String, ocachepath::String, sourcepath::String, depmods::Vector{Any}); │ @ Base ./loading.jl:1487; │ [22] _require_search_from_serialized(pkg::Base.PkgId, sourcepath::String, build_id::UInt128); │ @ Base ./loading.jl:1574; │ [23] _require(pkg::Base.PkgId, env::String); │ @ Base ./loading.jl:1938; │ [24] __require_prelocked(uuidkey::Base.PkgId, env::String); │ @ Base ./loading.jl:1812; │ [25] #invoke_in_world#3; │ @ ./essentials.jl:926 [inlined]; │ [26] invoke_in_world; │ @ ./essentials.jl:923 [inlined]; │ [27] _require_prelocked(uuidkey::Base.PkgId, env::String); │ @ Base ./loading.jl:1803; │ [28] macro expansion; │ @ ./loading.jl:1790 [inlined]; │ [29] macro expans",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3686#issuecomment-2272432528:2646,load,loading,2646,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3686#issuecomment-2272432528,1,['load'],['loading']
Performance,"jll; ✓ JuliaNVTXCallbacks_jll; ✓ MacroTools; ✓ LLVMExtra_jll; ✓ CUDA_Driver_jll; ✓ Random123; ✓ DataStructures; ✓ StringManipulation; ✓ FixedPointNumbers; ✓ SortingAlgorithms; ✗ CUDA_Runtime_jll; ✓ ColorTypes; ✓ LLVM; ✓ LLVM → BFloat16sExt; ✓ StaticArrays; ✓ Adapt → AdaptStaticArraysExt; ✓ StaticArrays → StaticArraysStatisticsExt; ✓ UnsafeAtomicsLLVM; ✓ Colors; ✓ GPUArraysCore; ✓ NVTX. ✓ GPUArrays; ✓ KernelAbstractions; ✓ PrettyTables; ✓ GPUCompiler; ✓ DataFrames; ✗ CUDA; 61 dependencies successfully precompiled in 190 seconds. 5 already precompiled. The following 1 direct dependency failed to precompile:. CUDA [052768ef-5323-5732-b1bb-66c8b64840ba]. Failed to precompile CUDA [052768ef-5323-5732-b1bb-66c8b64840ba] to ""/glade/u/home/knudsenl/.julia/compiled/v1.10/CUDA/jl_UQIv2i"".; [45592] signal (11.1): Segmentation fault; in expression starting at /glade/u/home/knudsenl/.julia/packages/CUDA/Tl08O/src/CUDA.jl:25; Allocations: 2907 (Pool: 2898; Big: 9); GC: ERROR: LoadError: Failed to precompile CUDA [052768ef-5323-5732-b1bb-66c8b64840ba] to ""/glade/u/home/knudsenl/.julia/compiled/v1.10/CUDA/jl_CUC33l"".; Stacktrace:; [1] error(s::String); @ Base ./error.jl:35; [2] compilecache(pkg::Base.PkgId, path::String, internal_stderr::IO, internal_stdout::IO, keep_loaded_modules::Bool); @ Base ./loading.jl:2468; [3] compilecache; @ ./loading.jl:2340 [inlined]; [4] (::Base.var""#968#969""{Base.PkgId})(); @ Base ./loading.jl:1974; [5] mkpidlock(f::Base.var""#968#969""{Base.PkgId}, at::String, pid::Int32; kwopts::@Kwargs{stale_age::Int64, wait::Bool}); @ FileWatching.Pidfile /glade/u/apps/casper/23.10/spack/opt/spack/julia/1.10.2/gcc/7.5.0/apod/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:93; [6] #mkpidlock#6; @ /glade/u/apps/casper/23.10/spack/opt/spack/julia/1.10.2/gcc/7.5.0/apod/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:88 [inlined]; [7] trymkpidlock(::Function, ::Vararg{Any}; kwargs::@Kwargs{stale_age::Int64}); @ FileWatching.Pidfile /glade/u/apps/casper/23.10/sp",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2245919472:4875,Load,LoadError,4875,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2245919472,1,['Load'],['LoadError']
Performance,"julia-3746/compiled/v1.6/Oceananigans/jl_IfcPYz.;   | ERROR: LoadError: LoadError: SystemError: opening file ""/var/lib/buildkite-agent/builds/tartarus-7/clima/oceananigans/src/Advection/upwind_biased_first_order.jl"": No such file or directory;   | Stacktrace:;   | [1] systemerror(p::String, errno::Int32; extrainfo::Nothing);   | @ Base ./error.jl:168;   | [2] #systemerror#62;   | @ ./error.jl:167 [inlined];   | [3] systemerror;   | @ ./error.jl:167 [inlined];   | [4] open(fname::String; lock::Bool, read::Nothing, write::Nothing, create::Nothing, truncate::Nothing, append::Nothing);   | @ Base ./iostream.jl:293;   | [5] open;   | @ ./iostream.jl:282 [inlined];   | [6] open(f::Base.var""#326#327""{String}, args::String; kwargs::Base.Iterators.Pairs{Union{}, Union{}, Tuple{}, NamedTuple{(), Tuple{}}});   | @ Base ./io.jl:328;   | [7] open;   | @ ./io.jl:328 [inlined];   | [8] read;   | @ ./io.jl:434 [inlined];   | [9] _include(mapexpr::Function, mod::Module, _path::String);   | @ Base ./loading.jl:1166;   | [10] include(mod::Module, _path::String);   | @ Base ./Base.jl:386;   | [11] include(x::String);   | @ Oceananigans.Advection ~/builds/tartarus-7/clima/oceananigans/src/Advection/Advection.jl:1;   | [12] top-level scope;   | @ ~/builds/tartarus-7/clima/oceananigans/src/Advection/Advection.jl:43;   | [13] include(mod::Module, _path::String);   | @ Base ./Base.jl:386;   | [14] include(x::String);   | @ Oceananigans ~/builds/tartarus-7/clima/oceananigans/src/Oceananigans.jl:1;   | [15] top-level scope;   | @ ~/builds/tartarus-7/clima/oceananigans/src/Oceananigans.jl:173;   | [16] include;   | @ ./Base.jl:386 [inlined];   | [17] include_package_for_output(pkg::Base.PkgId, input::String, depot_path::Vector{String}, dl_load_path::Vector{String}, load_path::Vector{String}, concrete_deps::Vector{Pair{Base.PkgId, UInt64}}, source::Nothing);   | @ Base ./loading.jl:1235;   | [18] top-level scope;   | @ none:1;   | [19] eval;   | @ ./boot.jl:360 [inlined];   | [20] eval(x::Expr);",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1957#issuecomment-904165134:1395,load,loading,1395,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1957#issuecomment-904165134,1,['load'],['loading']
Performance,"kages/CUDA/0p5fn/lib/utils/call.jl:31; │ [9] macro expansion at /home/fpoulin/.julia/packages/CUDA/0p5fn/lib/utils/call.jl:39 [inlined]; │ [10] macro expansion at /home/fpoulin/.julia/packages/CUDA/0p5fn/lib/cudadrv/libcuda.jl:35 [inlined]; │ [11] macro expansion at /home/fpoulin/.julia/packages/CUDA/0p5fn/lib/cudadrv/error.jl:102 [inlined]; │ [12] cuDeviceGetCount(::Base.RefValue{Int32}) at /home/fpoulin/.julia/packages/CUDA/0p5fn/lib/utils/call.jl:93; │ [13] length at /home/fpoulin/.julia/packages/CUDA/0p5fn/lib/cudadrv/devices.jl:111 [inlined]; │ [14] iterate at /home/fpoulin/.julia/packages/CUDA/0p5fn/lib/cudadrv/devices.jl:106 [inlined] (repeats 2 times); │ [15] iterate at ./iterators.jl:139 [inlined]; │ [16] iterate at ./iterators.jl:138 [inlined]; │ [17] __init__() at /home/fpoulin/software/Oceananigans.jl/src/Oceananigans.jl:178; │ [18] _include_from_serialized(::String, ::Array{Any,1}) at ./loading.jl:697; │ [19] _require_search_from_serialized(::Base.PkgId, ::String) at ./loading.jl:782; │ [20] _require(::Base.PkgId) at ./loading.jl:1007; │ [21] require(::Base.PkgId) at ./loading.jl:928; │ [22] require(::Module, ::Symbol) at ./loading.jl:923; │ [23] eval(::Module, ::Any) at ./boot.jl:331; │ [24] eval_user_input(::Any, ::REPL.REPLBackend) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/REPL/src/REPL.jl:134; │ [25] repl_backend_loop(::REPL.REPLBackend) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/REPL/src/REPL.jl:195; │ [26] start_repl_backend(::REPL.REPLBackend, ::Any) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/REPL/src/REPL.jl:180; │ [27] run_repl(::REPL.AbstractREPL, ::Any; backend_on_current_task::Bool) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/REPL/src/REPL.jl:292; │ [28] run_repl(::REPL.AbstractREPL, ::Any) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/REPL/src/REPL.jl:288; │ [29] (::Base.var""#807#809""{Bool,Bool,Bo",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1189:2246,load,loading,2246,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1189,1,['load'],['loading']
Performance,"kgId, path::String, internal_stderr::IO, internal_stdout::IO, keep_loaded_modules::Bool); @ Base ./loading.jl:2468; [3] compilecache; @ ./loading.jl:2340 [inlined]; [4] (::Base.var""#968#969""{Base.PkgId})(); @ Base ./loading.jl:1974; [5] mkpidlock(f::Base.var""#968#969""{Base.PkgId}, at::String, pid::Int32; kwopts::@Kwargs{stale_age::Int64, wait::Bool}); @ FileWatching.Pidfile /glade/u/apps/casper/23.10/spack/opt/spack/julia/1.10.2/gcc/7.5.0/apod/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:93; [6] #mkpidlock#6; @ /glade/u/apps/casper/23.10/spack/opt/spack/julia/1.10.2/gcc/7.5.0/apod/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:88 [inlined]; [7] trymkpidlock(::Function, ::Vararg{Any}; kwargs::@Kwargs{stale_age::Int64}); @ FileWatching.Pidfile /glade/u/apps/casper/23.10/spack/opt/spack/julia/1.10.2/gcc/7.5.0/apod/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:111; [8] #invokelatest#2; @ ./essentials.jl:894 [inlined]; [9] invokelatest; @ ./essentials.jl:889 [inlined]; [10] maybe_cachefile_lock(f::Base.var""#968#969""{Base.PkgId}, pkg::Base.PkgId, srcpath::String; stale_age::Int64); @ Base ./loading.jl:2983; [11] maybe_cachefile_lock; @ ./loading.jl:2980 [inlined]; [12] _require(pkg::Base.PkgId, env::String); @ Base ./loading.jl:1970; [13] __require_prelocked(uuidkey::Base.PkgId, env::String); @ Base ./loading.jl:1812; [14] #invoke_in_world#3; @ ./essentials.jl:926 [inlined]; [15] invoke_in_world; @ ./essentials.jl:923 [inlined]; [16] _require_prelocked(uuidkey::Base.PkgId, env::String); @ Base ./loading.jl:1803; [17] macro expansion; @ ./loading.jl:1790 [inlined]; [18] macro expansion; @ ./lock.jl:267 [inlined]; [19] __require(into::Module, mod::Symbol); @ Base ./loading.jl:1753; [20] #invoke_in_world#3; @ ./essentials.jl:926 [inlined]; [21] invoke_in_world; @ ./essentials.jl:923 [inlined]; [22] require(into::Module, mod::Symbol); @ Base ./loading.jl:1746; in expression starting at /glade/derecho/scratch/knudsenl/BottomBoundaryLayer/testcode.jl:1; ```",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2231635812:6897,load,loading,6897,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2231635812,8,['load'],['loading']
Performance,"kgId, path::String, internal_stderr::IO, internal_stdout::IO, keep_loaded_modules::Bool); @ Base ./loading.jl:2468; [3] compilecache; @ ./loading.jl:2340 [inlined]; [4] (::Base.var""#968#969""{Base.PkgId})(); @ Base ./loading.jl:1974; [5] mkpidlock(f::Base.var""#968#969""{Base.PkgId}, at::String, pid::Int32; kwopts::@Kwargs{stale_age::Int64, wait::Bool}); @ FileWatching.Pidfile /glade/u/apps/casper/23.10/spack/opt/spack/julia/1.10.2/gcc/7.5.0/apod/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:93; [6] #mkpidlock#6; @ /glade/u/apps/casper/23.10/spack/opt/spack/julia/1.10.2/gcc/7.5.0/apod/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:88 [inlined]; [7] trymkpidlock(::Function, ::Vararg{Any}; kwargs::@Kwargs{stale_age::Int64}); @ FileWatching.Pidfile /glade/u/apps/casper/23.10/spack/opt/spack/julia/1.10.2/gcc/7.5.0/apod/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:111; [8] #invokelatest#2; @ ./essentials.jl:894 [inlined]; [9] invokelatest; @ ./essentials.jl:889 [inlined]; [10] maybe_cachefile_lock(f::Base.var""#968#969""{Base.PkgId}, pkg::Base.PkgId, srcpath::String; stale_age::Int64); @ Base ./loading.jl:2983; [11] maybe_cachefile_lock; @ ./loading.jl:2980 [inlined]; [12] _require(pkg::Base.PkgId, env::String); @ Base ./loading.jl:1970; [13] __require_prelocked(uuidkey::Base.PkgId, env::String); @ Base ./loading.jl:1812; [14] #invoke_in_world#3; @ ./essentials.jl:926 [inlined]; [15] invoke_in_world; @ ./essentials.jl:923 [inlined]; [16] _require_prelocked(uuidkey::Base.PkgId, env::String); @ Base ./loading.jl:1803; [17] macro expansion; @ ./loading.jl:1790 [inlined]; [18] macro expansion; @ ./lock.jl:267 [inlined]; [19] __require(into::Module, mod::Symbol); @ Base ./loading.jl:1753; [20] #invoke_in_world#3; @ ./essentials.jl:926 [inlined]; [21] invoke_in_world; @ ./essentials.jl:923 [inlined]; [22] require(into::Module, mod::Symbol); @ Base ./loading.jl:1746; in expression starting at /glade/derecho/scratch/knudsenl/BottomBoundaryLayer/testcode.jl:3; ```",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2245919472:6226,load,loading,6226,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2245919472,8,['load'],['loading']
Performance,l at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:126; eval_value at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:223; eval_stmt_value at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:174 [inlined]; eval_body at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:617; jl_interpret_toplevel_thunk at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:775; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:934; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:877; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:877; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:877; ijl_toplevel_eval_in at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:985; eval at ./boot.jl:385 [inlined]; eval_user_input at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:150; repl_backend_loop at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:246; #start_repl_backend#46 at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:231; start_repl_backend at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:228; _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; #run_repl#59 at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:389; run_repl at /cache/build/builder,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3852:6270,cache,cache,6270,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3852,1,['cache'],['cache']
Performance,l scope at /home/alir/atdepth/Oceananigans.jl/particles_error.jl:37; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:925; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:877; ijl_toplevel_eval_in at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:985; eval at ./boot.jl:385 [inlined]; include_string at ./loading.jl:2076; _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; _include at ./loading.jl:2136; include at ./client.jl:489; unknown function (ip: 0x7c00f54ff855); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; jl_apply at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/julia.h:1982 [inlined]; do_call at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:126; eval_value at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:223; eval_stmt_value at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:174 [inlined]; eval_body at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:617; jl_interpret_toplevel_thunk at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:775; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:934; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:877; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:877; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3852:5195,cache,cache,5195,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3852,1,['cache'],['cache']
Performance,"l, cleanup::Bool, validate::Bool, strip::Bool, only_entry::Bool, parent_job::Nothing); @ GPUCompiler ~/.julia/packages/GPUCompiler/2CW9L/src/driver.jl:100; [8] codegen; @ ~/.julia/packages/GPUCompiler/2CW9L/src/driver.jl:82 [inlined]; [9] compile(target::Symbol, job::GPUCompiler.CompilerJob; kwargs::@Kwargs{}); @ GPUCompiler ~/.julia/packages/GPUCompiler/2CW9L/src/driver.jl:79; [10] compile; @ ~/.julia/packages/GPUCompiler/2CW9L/src/driver.jl:74 [inlined]; [11] #1145; @ ~/.julia/packages/CUDA/2kjXI/src/compiler/compilation.jl:250 [inlined]; [12] JuliaContext(f::CUDA.var""#1145#1148""{GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget, CUDA.CUDACompilerParams}}; kwargs::@Kwargs{}); @ GPUCompiler ~/.julia/packages/GPUCompiler/2CW9L/src/driver.jl:34; [13] JuliaContext(f::Function); @ GPUCompiler ~/.julia/packages/GPUCompiler/2CW9L/src/driver.jl:25; [14] compile(job::GPUCompiler.CompilerJob); @ CUDA ~/.julia/packages/CUDA/2kjXI/src/compiler/compilation.jl:249; [15] actual_compilation(cache::Dict{Any, CuFunction}, src::Core.MethodInstance, world::UInt64, cfg::GPUCompiler.CompilerConfig{GPUCompiler.PTXCompilerTarget, CUDA.CUDACompilerParams}, compiler::typeof(CUDA.compile), linker::typeof(CUDA.link)); @ GPUCompiler ~/.julia/packages/GPUCompiler/2CW9L/src/execution.jl:237; [16] cached_compilation(cache::Dict{Any, CuFunction}, src::Core.MethodInstance, cfg::GPUCompiler.CompilerConfig{GPUCompiler.PTXCompilerTarget, CUDA.CUDACompilerParams}, compiler::Function, linker::Function); @ GPUCompiler ~/.julia/packages/GPUCompiler/2CW9L/src/execution.jl:151; [17] macro expansion; @ ~/.julia/packages/CUDA/2kjXI/src/compiler/execution.jl:380 [inlined]; [18] macro expansion; @ ./lock.jl:267 [inlined]; [19] cufunction(f::typeof(Oceananigans.Models.NonhydrostaticModels.gpu_compute_Gu!), tt::Type{Tuple{KernelAbstractions.CompilerMetadata{KernelAbstractions.NDIteration.StaticSize{(10, 10, 10)}, KernelAbstractions.NDIteration.DynamicCheck, Nothing, Nothing, KernelAbstractions.NDIteration.ND",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3646#issuecomment-2428001700:7536,cache,cache,7536,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3646#issuecomment-2428001700,1,['cache'],['cache']
Performance,"lArrays/DTEhf/src/PencilArrays.jl:12; &nbsp; | ERROR: LoadError: Failed to precompile PencilArrays [0e08944d-e94e-41b1-9406-dcf66b6a9d2e] to /storage7/buildkite-agent/.julia-2581/compiled/v1.5/PencilArrays/yKKUy_zV1Ut.ji.; &nbsp; | Stacktrace:; &nbsp; | [1] error(::String) at ./error.jl:33; &nbsp; | [2] compilecache(::Base.PkgId, ::String) at ./loading.jl:1305; &nbsp; | [3] _require(::Base.PkgId) at ./loading.jl:1030; &nbsp; | [4] require(::Base.PkgId) at ./loading.jl:928; &nbsp; | [5] require(::Module, ::Symbol) at ./loading.jl:923; &nbsp; | [6] include(::Function, ::Module, ::String) at ./Base.jl:380; &nbsp; | [7] include(::Module, ::String) at ./Base.jl:368; &nbsp; | [8] top-level scope at none:2; &nbsp; | [9] eval at ./boot.jl:347 [inlined]; &nbsp; | [10] eval(::Expr) at ./client.jl:467; &nbsp; | [11] top-level scope at ./none:3; &nbsp; | in expression starting at /storage7/buildkite-agent/.julia-2581/packages/PencilFFTs/Xwxei/src/PencilFFTs.jl:11; &nbsp; | ERROR: could not load library ""/storage7/buildkite-agent/julia-1.5.4/lib/julia/sys.so""; &nbsp; | /storage7/buildkite-agent/julia-1.5.4/lib/julia/sys.so: ELF load command past end of file; &nbsp; | ERROR: could not load library ""/storage7/buildkite-agent/julia-1.5.4/lib/julia/sys.so""; &nbsp; | /storage7/buildkite-agent/julia-1.5.4/lib/julia/sys.so: ELF load command past end of file; &nbsp; | ERROR: could not load library ""/storage7/buildkite-agent/julia-1.5.4/lib/julia/sys.so""; &nbsp; | /storage7/buildkite-agent/julia-1.5.4/lib/julia/sys.so: ELF load command past end of file; &nbsp; | ERROR: could not load library ""/storage7/buildkite-agent/julia-1.5.4/lib/julia/sys.so""; &nbsp; | /storage7/buildkite-agent/julia-1.5.4/lib/julia/sys.so: ELF load command past end of file; &nbsp; | ERROR: could not load library ""/storage7/buildkite-agent/julia-1.5.4/lib/julia/sys.so""; &nbsp; | /storage7/buildkite-agent/julia-1.5.4/lib/julia/sys.so: ELF load command past end of file; &nbsp; | 🚨 Error: The command exited with status ",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1662#issuecomment-843325731:6162,load,load,6162,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1662#issuecomment-843325731,1,['load'],['load']
Performance,lang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; #run_repl#59 at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:389; run_repl at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:375; jfptr_run_repl_91805.1 at /home/alir/.julia/juliaup/julia-1.10.5+0.x64.linux.gnu/lib/julia/sys.so (unknown line); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; #1013 at ./client.jl:432; jfptr_YY.1013_82772.1 at /home/alir/.julia/juliaup/julia-1.10.5+0.x64.linux.gnu/lib/julia/sys.so (unknown line); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; jl_apply at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/julia.h:1982 [inlined]; jl_f__call_latest at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/builtins.c:812; #invokelatest#2 at ./essentials.jl:892 [inlined]; invokelatest at ./essentials.jl:889 [inlined]; run_main_repl at ./client.jl:416; exec_options at ./client.jl:333; _start at ./client.jl:552; jfptr__start_82798.1 at /home/alir/.julia/juliaup/julia-1.10.5+0.x64.linux.gnu/lib/julia/sys.so (unknown line); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; jl_apply at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/julia.h:1982 [inlined]; true_main at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/jlapi.c:582; jl_repl_entrypoint at /cache/bu,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3852:7964,cache,cache,7964,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3852,1,['cache'],['cache']
Performance,"lang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; jl_apply at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/julia.h:1982 [inlined]; jl_f__call_latest at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/builtins.c:812; #invokelatest#2 at ./essentials.jl:892 [inlined]; invokelatest at ./essentials.jl:889 [inlined]; run_main_repl at ./client.jl:416; exec_options at ./client.jl:333; _start at ./client.jl:552; jfptr__start_82798.1 at /home/alir/.julia/juliaup/julia-1.10.5+0.x64.linux.gnu/lib/julia/sys.so (unknown line); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; jl_apply at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/julia.h:1982 [inlined]; true_main at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/jlapi.c:582; jl_repl_entrypoint at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/jlapi.c:731; main at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/cli/loader_exe.c:58; unknown function (ip: 0x7c00f758ce07); __libc_start_main at /usr/lib/libc.so.6 (unknown line); unknown function (ip: 0x4010b8); Allocations: 67298744 (Pool: 67235612; Big: 63132); GC: 66; fish: Job 1, 'julia --project' terminated by signal SIGSEGV (Address boundary error); ```. GPU illegal memory access:. ```; [ Info: Skipping precompilation since __precompile__(false). Importing Oceananigans [9e8cae18-63c1-5223-a75c-80ca9d6e9a09].; [ Info: Iteration 1...; [ Info: Iteration 2...; [ Info: Iteration 3...; [ Info: Iteration 4...; [ Info: Iteration 5...; [ Info: Iteration 6...; [ Info: Iteration 7...; [ Info: Iteration 8...; [ Info: Iteration 9...; [ Info: Iteration 10...; [ Info: Iteration 11...; [ Info: Iteration 12...; [ Info: Iteration 1",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3852:8869,cache,cache,8869,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3852,1,['cache'],['cache']
Performance,"ld; @ ./essentials.jl:923 [inlined]; [22] require(into::Module, mod::Symbol); @ Base ./loading.jl:1746; [23] include; @ ./Base.jl:495 [inlined]; [24] include_package_for_output(pkg::Base.PkgId, input::String, depot_path::Vector{String}, dl_load_path::Vector{String}, load_path::Vector{String}, concrete_deps::Vector{Pair{Base.PkgId, UInt128}}, source::String); @ Base ./loading.jl:2222; [25] top-level scope; @ stdin:3; in expression starting at /glade/u/home/knudsenl/.julia/packages/CUDA/Tl08O/src/CUDA.jl:1; in expression starting at stdin:3; ERROR: LoadError: Failed to precompile CUDA [052768ef-5323-5732-b1bb-66c8b64840ba] to ""/glade/u/home/knudsenl/.julia/compiled/v1.10/CUDA/jl_zRopeZ"".; Stacktrace:; [1] error(s::String); @ Base ./error.jl:35; [2] compilecache(pkg::Base.PkgId, path::String, internal_stderr::IO, internal_stdout::IO, keep_loaded_modules::Bool); @ Base ./loading.jl:2468; [3] compilecache; @ ./loading.jl:2340 [inlined]; [4] (::Base.var""#968#969""{Base.PkgId})(); @ Base ./loading.jl:1974; [5] mkpidlock(f::Base.var""#968#969""{Base.PkgId}, at::String, pid::Int32; kwopts::@Kwargs{stale_age::Int64, wait::Bool}); @ FileWatching.Pidfile /glade/u/apps/casper/23.10/spack/opt/spack/julia/1.10.2/gcc/7.5.0/apod/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:93; [6] #mkpidlock#6; @ /glade/u/apps/casper/23.10/spack/opt/spack/julia/1.10.2/gcc/7.5.0/apod/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:88 [inlined]; [7] trymkpidlock(::Function, ::Vararg{Any}; kwargs::@Kwargs{stale_age::Int64}); @ FileWatching.Pidfile /glade/u/apps/casper/23.10/spack/opt/spack/julia/1.10.2/gcc/7.5.0/apod/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:111; [8] #invokelatest#2; @ ./essentials.jl:894 [inlined]; [9] invokelatest; @ ./essentials.jl:889 [inlined]; [10] maybe_cachefile_lock(f::Base.var""#968#969""{Base.PkgId}, pkg::Base.PkgId, srcpath::String; stale_age::Int64); @ Base ./loading.jl:2983; [11] maybe_cachefile_lock; @ ./loading.jl:2980 [inlined]; [12] _require(pkg::Bas",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2231635812:3377,load,loading,3377,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2231635812,1,['load'],['loading']
Performance,lder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:877; ijl_toplevel_eval_in at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:985; eval at ./boot.jl:385 [inlined]; eval_user_input at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:150; repl_backend_loop at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:246; #start_repl_backend#46 at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:231; start_repl_backend at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:228; _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; #run_repl#59 at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:389; run_repl at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:375; jfptr_run_repl_91805.1 at /home/alir/.julia/juliaup/julia-1.10.5+0.x64.linux.gnu/lib/julia/sys.so (unknown line); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; #1013 at ./client.jl:432; jfptr_YY.1013_82772.1 at /home/alir/.julia/juliaup/julia-1.10.5+0.x64.linux.gnu/lib/julia/sys.so (unknown line); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; jl_apply at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/julia.h:1982 [inlined]; jl_f__call_latest at /cache/build/b,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3852:7147,cache,cache,7147,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3852,1,['cache'],['cache']
Performance,"le.jl:88 [inlined]; │ [7] trymkpidlock(::Function, ::Vararg{Any}; kwargs::@Kwargs{stale_age::Int64}); │ @ FileWatching.Pidfile ~/julia-1.10/usr/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:111; │ [8] #invokelatest#2; │ @ ./essentials.jl:894 [inlined]; │ [9] invokelatest; │ @ ./essentials.jl:889 [inlined]; │ [10] maybe_cachefile_lock(f::Base.var""#968#969""{Base.PkgId}, pkg::Base.PkgId, srcpath::String; stale_age::Int64); │ @ Base ./loading.jl:2983; │ [11] maybe_cachefile_lock; │ @ ./loading.jl:2980 [inlined]; │ [12] _require(pkg::Base.PkgId, env::Nothing); │ @ Base ./loading.jl:1970; │ [13] __require_prelocked(uuidkey::Base.PkgId, env::Nothing); │ @ Base ./loading.jl:1812; │ [14] #invoke_in_world#3; │ @ ./essentials.jl:926 [inlined]; │ [15] invoke_in_world; │ @ ./essentials.jl:923 [inlined]; │ [16] _require_prelocked; │ @ ./loading.jl:1803 [inlined]; │ [17] _require_prelocked; │ @ ./loading.jl:1802 [inlined]; │ [18] run_extension_callbacks(extid::Base.ExtensionId); │ @ Base ./loading.jl:1295; │ [19] run_extension_callbacks(pkgid::Base.PkgId); │ @ Base ./loading.jl:1330; │ [20] run_package_callbacks(modkey::Base.PkgId); │ @ Base ./loading.jl:1164; │ [21] _tryrequire_from_serialized(modkey::Base.PkgId, path::String, ocachepath::String, sourcepath::String, depmods::Vector{Any}); │ @ Base ./loading.jl:1487; │ [22] _require_search_from_serialized(pkg::Base.PkgId, sourcepath::String, build_id::UInt128); │ @ Base ./loading.jl:1574; │ [23] _require(pkg::Base.PkgId, env::String); │ @ Base ./loading.jl:1938; │ [24] __require_prelocked(uuidkey::Base.PkgId, env::String); │ @ Base ./loading.jl:1812; │ [25] #invoke_in_world#3; │ @ ./essentials.jl:926 [inlined]; │ [26] invoke_in_world; │ @ ./essentials.jl:923 [inlined]; │ [27] _require_prelocked(uuidkey::Base.PkgId, env::String); │ @ Base ./loading.jl:1803; │ [28] macro expansion; │ @ ./loading.jl:1790 [inlined]; │ [29] macro expansion; │ @ ./lock.jl:267 [inlined]; │ [30] __require(into::Module, mod::Symbol); │ @ Base ./loadi",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3686#issuecomment-2272432528:2741,load,loading,2741,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3686#issuecomment-2272432528,1,['load'],['loading']
Performance,lease-1-dot-10/src/interpreter.c:126; eval_value at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:223; eval_stmt_value at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:174 [inlined]; eval_body at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:617; jl_interpret_toplevel_thunk at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:775; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:934; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:877; ijl_toplevel_eval_in at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:985; eval at ./boot.jl:385 [inlined]; include_string at ./loading.jl:2076; _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; _include at ./loading.jl:2136; include at ./Base.jl:495; jfptr_include_46447.1 at /orcd/data/raffaele/001/glwagner/Software/julia-1.10.5/lib/julia/sys.so (unknown line); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; exec_options at ./client.jl:318; _start at ./client.jl:552; jfptr__start_82798.1 at /orcd/data/raffaele/001/glwagner/Software/julia-1.10.5/lib/julia/sys.so (unknown line); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; jl_apply at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/julia.h:1982 [inlined]; true_main at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/jlapi.c:582,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3878:5076,cache,cache,5076,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3878,1,['cache'],['cache']
Performance,"ler/eJOtJ/src/utils.jl:62; [8] emit_llvm(job::GPUCompiler.CompilerJob, method_instance::Any, world::UInt64); @ GPUCompiler ~/.julia/packages/GPUCompiler/eJOtJ/src/utils.jl:60; [9] cufunction_compile(job::GPUCompiler.CompilerJob); @ CUDA ~/.julia/packages/CUDA/3VnCC/src/compiler/execution.jl:300; [10] check_cache; @ ~/.julia/packages/GPUCompiler/eJOtJ/src/cache.jl:47 [inlined]; [11] cached_compilation; @ ~/.julia/packages/GPUArrays/Z5nPF/src/host/broadcast.jl:57 [inlined]; [12] cached_compilation(cache::Dict{UInt64, Any}, job::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget, CUDA.CUDACompilerParams, GPUCompiler.FunctionSpec{GPUArrays.var""#broadcast_kernel#16"", Tuple{CUDA.CuKernelContext, CUDA.CuDeviceArray{Float64, 3, 1}, Base.Broadcast.Broadcasted{Nothing, Tuple{Base.OneTo{Int64}, Base.OneTo{Int64}, Base.OneTo{Int64}}, typeof(identity), Tuple{Int64}}, Int64}}}, compiler::typeof(CUDA.cufunction_compile), linker::typeof(CUDA.cufunction_link)); @ GPUCompiler ~/.julia/packages/GPUCompiler/eJOtJ/src/cache.jl:0; [13] cufunction(f::GPUArrays.var""#broadcast_kernel#16"", tt::Type{Tuple{CUDA.CuKernelContext, CUDA.CuDeviceArray{Float64, 3, 1}, Base.Broadcast.Broadcasted{Nothing, Tuple{Base.OneTo{Int64}, Base.OneTo{Int64}, Base.OneTo{Int64}}, typeof(identity), Tuple{Int64}}, Int64}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{}, Union{}, Tuple{}, NamedTuple{(), Tuple{}}}); @ CUDA ~/.julia/packages/CUDA/3VnCC/src/compiler/execution.jl:289; [14] cufunction; @ ~/.julia/packages/CUDA/3VnCC/src/compiler/execution.jl:283 [inlined]; [15] macro expansion; @ ~/.julia/packages/CUDA/3VnCC/src/compiler/execution.jl:102 [inlined]; [16] #launch_heuristic#286; @ ~/.julia/packages/CUDA/3VnCC/src/gpuarrays.jl:17 [inlined]; [17] launch_heuristic; @ ~/.julia/packages/CUDA/3VnCC/src/gpuarrays.jl:17 [inlined]; [18] copyto!; @ ~/.julia/packages/GPUArrays/Z5nPF/src/host/broadcast.jl:63 [inlined]; [19] copyto!; @ ~/.julia/packages/GPUArrays/Z5nPF/src/host/broadcast.jl:73 [inlined]; [20] mat",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1706:2191,cache,cache,2191,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1706,1,['cache'],['cache']
Performance,"lerJob; libraries::Bool, toplevel::Bool, optimize::Bool, cleanup::Bool, strip::Bool, validate::Bool, only_entry::Bool); @ GPUCompiler ~/.julia/packages/GPUCompiler/Cp7sE/src/driver.jl:106; [10] compile; @ ~/.julia/packages/GPUCompiler/Cp7sE/src/driver.jl:98 [inlined]; [11] #45; @ ~/.julia/packages/Metal/lnkVP/src/compiler/compilation.jl:57 [inlined]; [12] JuliaContext(f::Metal.var""#45#46""{GPUCompiler.CompilerJob{GPUCompiler.MetalCompilerTarget, Metal.MetalCompilerParams}}); @ GPUCompiler ~/.julia/packages/GPUCompiler/Cp7sE/src/driver.jl:47; [13] compile(job::GPUCompiler.CompilerJob); @ Metal ~/.julia/packages/Metal/lnkVP/src/compiler/compilation.jl:56; [14] actual_compilation(cache::Dict{Any, Any}, src::Core.MethodInstance, world::UInt64, cfg::GPUCompiler.CompilerConfig{GPUCompiler.MetalCompilerTarget, Metal.MetalCompilerParams}, compiler::typeof(Metal.compile), linker::typeof(Metal.link)); @ GPUCompiler ~/.julia/packages/GPUCompiler/Cp7sE/src/execution.jl:125; [15] cached_compilation(cache::Dict{Any, Any}, src::Core.MethodInstance, cfg::GPUCompiler.CompilerConfig{GPUCompiler.MetalCompilerTarget, Metal.MetalCompilerParams}, compiler::Function, linker::Function); @ GPUCompiler ~/.julia/packages/GPUCompiler/Cp7sE/src/execution.jl:103; [16] macro expansion; @ ~/.julia/packages/Metal/lnkVP/src/compiler/execution.jl:162 [inlined]; [17] macro expansion; @ ./lock.jl:267 [inlined]; [18] mtlfunction(f::typeof(Oceananigans.Models.HydrostaticFreeSurfaceModels.gpu_compute_hydrostatic_free_surface_Gu!), tt::Type{Tuple{KernelAbstractions.CompilerMetadata{KernelAbstractions.NDIteration.StaticSize{(3, 3, 50)}, KernelAbstractions.NDIteration.DynamicCheck, Nothing, Nothing, KernelAbstractions.NDIteration.NDRange{3, KernelAbstractions.NDIteration.StaticSize{(1, 1, 50)}, KernelAbstractions.NDIteration.StaticSize{(16, 16, 1)}, Nothing, Nothing}}, OffsetArrays.OffsetArray{Float32, 3, Metal.MtlDeviceArray{Float32, 3, 1}}, RectilinearGrid{Float32, Periodic, Periodic, Bounded, Float32, Float",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2618#issuecomment-1731573822:37939,cache,cache,37939,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2618#issuecomment-1731573822,1,['cache'],['cache']
Performance,lid_driven_cavity & thermal_bubble: ERROR: LoadError: ArgumentError: length(size) must be 2.,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1507:43,Load,LoadError,43,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1507,1,['Load'],['LoadError']
Performance,lin/software/Oceananigans.jl/examples/geostrophic_adjustment.jl:92; jl_gc_pool_alloc at /buildworker/worker/package_linux64/build/src/gc.c:1148; jl_gc_alloc_ at /buildworker/worker/package_linux64/build/src/julia_internal.h:277 [inlined]; jl_gc_alloc at /buildworker/worker/package_linux64/build/src/gc.c:3150; jl_gc_alloc_buf at /buildworker/worker/package_linux64/build/src/julia_internal.h:304 [inlined]; array_resize_buffer at /buildworker/worker/package_linux64/build/src/array.c:686; jl_array_grow_at_end at /buildworker/worker/package_linux64/build/src/array.c:875 [inlined]; jl_array_grow_end at /buildworker/worker/package_linux64/build/src/array.c:939; jl_array_sizehint at /buildworker/worker/package_linux64/build/src/array.c:1139; sizehint! at ./array.jl:1103 [inlined]; BitSet at ./bitset.jl:18; BitSet at ./bitset.jl:29 [inlined]; construct_ssa! at ./compiler/ssair/slot2ssa.jl:780; slot2reg at ./compiler/ssair/driver.jl:127 [inlined]; run_passes at ./compiler/ssair/driver.jl:134; optimize at ./compiler/optimize.jl:174; typeinf at ./compiler/typeinfer.jl:33; abstract_call_method_with_const_args at ./compiler/abstractinterpretation.jl:266; abstract_call_gf_by_type at ./compiler/abstractinterpretation.jl:134; abstract_call_known at ./compiler/abstractinterpretation.jl:904; abstract_call at ./compiler/abstractinterpretation.jl:926; abstract_call at ./compiler/abstractinterpretation.jl:911; abstract_eval at ./compiler/abstractinterpretation.jl:1005; typeinf_local at ./compiler/abstractinterpretation.jl:1270; typeinf_nocycle at ./compiler/abstractinterpretation.jl:1326; typeinf at ./compiler/typeinfer.jl:12; abstract_call_method_with_const_args at ./compiler/abstractinterpretation.jl:266; abstract_call_gf_by_type at ./compiler/abstractinterpretation.jl:134; abstract_call_known at ./compiler/abstractinterpretation.jl:904; abstract_call at ./compiler/abstractinterpretation.jl:926; abstract_call at ./compiler/abstractinterpretation.jl:911; abstract_eval at ./compiler/abstr,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1513#issuecomment-809634040:1706,optimiz,optimize,1706,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1513#issuecomment-809634040,1,['optimiz'],['optimize']
Performance,lined]; cpu__advect_particles! at ./none:0; __thread_run at /home/alir/.julia/packages/KernelAbstractions/491pi/src/cpu.jl:144; unknown function (ip: 0x7c0090512182); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; __run at /home/alir/.julia/packages/KernelAbstractions/491pi/src/cpu.jl:111; unknown function (ip: 0x7c009050feb3); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; #_#16 at /home/alir/.julia/packages/KernelAbstractions/491pi/src/cpu.jl:46; _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; jl_apply at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/julia.h:1982 [inlined]; do_apply at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/builtins.c:768; Kernel at /home/alir/.julia/packages/KernelAbstractions/491pi/src/cpu.jl:39; _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; advect_lagrangian_particles! at /home/alir/atdepth/Oceananigans.jl/src/Models/LagrangianParticleTracking/lagrangian_particle_advection.jl:193; step_lagrangian_particles! at /home/alir/atdepth/Oceananigans.jl/src/Models/LagrangianParticleTracking/LagrangianParticleTracking.jl:143 [inlined]; step_lagrangian_particles! at /home/alir/atdepth/Oceananigans.jl/src/Models/HydrostaticFreeSurfaceModels/HydrostaticFreeSurfaceModels.jl:107 [inlined]; #time_step!#8 at /home/alir/atdepth/Oceananigans.jl/src/TimeSteppers/quasi_adams_bashforth_2.jl:124; time_step! at /home,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3852:2868,cache,cache,2868,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3852,1,['cache'],['cache']
Performance,lined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; jl_apply at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/julia.h:1982 [inlined]; do_call at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:126; eval_value at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:223; eval_stmt_value at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:174 [inlined]; eval_body at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:617; jl_interpret_toplevel_thunk at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:775; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:934; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:877; ijl_toplevel_eval_in at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:985; eval at ./boot.jl:385 [inlined]; include_string at ./loading.jl:2076; _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; _include at ./loading.jl:2136; include at ./Base.jl:495; jfptr_include_46447.1 at /orcd/data/raffaele/001/glwagner/Software/julia-1.10.5/lib/julia/sys.so (unknown line); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; exec_options at ./client.jl:318; _start at ./client.jl:552; jfptr__start_82798.1 at /orcd/data/raffaele/001/glwagner/Software/julia-1.10.5/lib/julia/sys.so (unknown line); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /ca,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3878:4800,cache,cache,4800,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3878,1,['cache'],['cache']
Performance,"linux64/build/usr/share/julia/stdlib/v1.5/SuiteSparse/src/cholmod.jl:90; &nbsp; | └ @ SuiteSparse.CHOLMOD /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/SuiteSparse/src/cholmod.jl:187; &nbsp; | /storage7/buildkite-agent/julia-1.5.4/bin/julia: error while loading shared libraries: libLLVM-9jl.so: ELF load command past end of file; &nbsp; | ERROR: LoadError: LoadError: IOError: write: broken pipe (EPIPE); &nbsp; | Stacktrace:; &nbsp; | [1] uv_write(::Base.PipeEndpoint, ::Ptr{UInt8}, ::UInt64) at ./stream.jl:951; &nbsp; | [2] unsafe_write(::Base.PipeEndpoint, ::Ptr{UInt8}, ::UInt64) at ./stream.jl:1005; &nbsp; | [3] write(::Base.PipeEndpoint, ::String) at ./strings/io.jl:183; &nbsp; | [4] create_expr_cache(::String, ::String, ::Array{Pair{Base.PkgId,UInt64},1}, ::Base.UUID) at ./loading.jl:1194; &nbsp; | [5] compilecache(::Base.PkgId, ::String) at ./loading.jl:1286; &nbsp; | [6] _require(::Base.PkgId) at ./loading.jl:1030; &nbsp; | [7] require(::Base.PkgId) at ./loading.jl:928; &nbsp; | [8] require(::Module, ::Symbol) at ./loading.jl:923; &nbsp; | [9] include(::Function, ::Module, ::String) at ./Base.jl:380; &nbsp; | [10] include at ./Base.jl:368 [inlined]; &nbsp; | [11] include(::String) at /storage7/buildkite-agent/.julia-2581/packages/PencilArrays/DTEhf/src/PencilArrays.jl:1; &nbsp; | [12] top-level scope at /storage7/buildkite-agent/.julia-2581/packages/PencilArrays/DTEhf/src/PencilArrays.jl:12; &nbsp; | [13] include(::Function, ::Module, ::String) at ./Base.jl:380; &nbsp; | [14] include(::Module, ::String) at ./Base.jl:368; &nbsp; | [15] top-level scope at none:2; &nbsp; | [16] eval at ./boot.jl:347 [inlined]; &nbsp; | [17] eval(::Expr) at ./client.jl:467; &nbsp; | [18] top-level scope at ./none:3; &nbsp; | in expression starting at /storage7/buildkite-agent/.julia-2581/packages/PencilArrays/DTEhf/src/Pencils/Pencils.jl:7; &nbsp; | in expression starting at /storage7/buildkite-agent/.julia-2581/packages/PencilArrays/DTEhf/src/PencilArrays.jl:",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1662#issuecomment-843325731:4199,load,loading,4199,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1662#issuecomment-843325731,1,['load'],['loading']
Performance,"loat64,Float64}}},NamedTuple{(:u, :v, :w),Tuple{OffsetArray{Float64,3,CuDeviceArray{Float64,3,CUDA.AS.Global}},OffsetArray{Float64,3,CuDeviceArray{Float64,3,CUDA.AS.Global}},OffsetArray{Float64,3,CuDeviceArray{Float64,3,CUDA.AS.Global}}}},NamedTuple{(:T, :S),Tuple{OffsetArray{Float64,3,CuDeviceArray{Float64,3,CUDA.AS.Global}},OffsetArray{Float64,3,CuDeviceArray{Float64,3,CUDA.AS.Global}}}},Nothing,NamedTuple{(:u, :v, :w, :T, :S),Tuple{ParameterizedForcing{var""#Fu_func#83"",NamedTuple{(:τ,),Tuple{Int64}}},ParameterizedForcing{var""#Fv_func#84"",NamedTuple{(:τ,),Tuple{Int64}}},ParameterizedForcing{var""#Fw_func#85"",NamedTuple{(:τ,),Tuple{Int64}}},typeof(Oceananigans.Forcing.zeroforcing),typeof(Oceananigans.Forcing.zeroforcing)}},OffsetArray{Float64,3,CuDeviceArray{Float64,3,CUDA.AS.Global}},NamedTuple{(:time, :iteration),Tuple{Float64,Int64}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:maxthreads,),Tuple{Int64}}}) at /home/ancellin/.julia/packages/GPUCompiler/4e9CU/src/cache.jl:0; [23] cufunction(::typeof(Cassette.overdub), ::Type{Tuple{Cassette.Context{nametype(CUDACtx),KernelAbstractions.CompilerMetadata{KernelAbstractions.NDIteration.StaticSize{(16, 16, 16)},KernelAbstractions.NDIteration.DynamicCheck,Nothing,Nothing,KernelAbstractions.NDIteration.NDRange{3,KernelAbstractions.NDIteration.StaticSize{(1, 1, 16)},KernelAbstractions.NDIteration.StaticSize{(16, 16, 1)},Nothing,Nothing}},Nothing,KernelAbstractions.var""##PassType#253"",Nothing,Cassette.DisableHooks},typeof(Oceananigans.TimeSteppers.gpu_calculate_Gu!),OffsetArray{Float64,3,CuDeviceArray{Float64,3,CUDA.AS.Global}},RegularCartesianGrid{Float64,Periodic,Periodic,Bounded,OffsetArray{Float64,1,StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}}},CenteredSecondOrder,Nothing,Nothing,IsotropicDiffusivity{Float64,NamedTuple{(:T, :S),Tuple{Float64,Float64}}},NamedTuple{(:u, :v, :w),Tuple{OffsetArray{Float64,3,CuDeviceArray{Float64,3,CUDA.AS.Global}},Offset",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/882:48006,cache,cache,48006,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/882,1,['cache'],['cache']
Performance,"loat64,Float64}}},NamedTuple{(:u, :v, :w),Tuple{OffsetArray{Float64,3,CuDeviceArray{Float64,3,CUDA.AS.Global}},OffsetArray{Float64,3,CuDeviceArray{Float64,3,CUDA.AS.Global}},OffsetArray{Float64,3,CuDeviceArray{Float64,3,CUDA.AS.Global}}}},NamedTuple{(:T, :S),Tuple{OffsetArray{Float64,3,CuDeviceArray{Float64,3,CUDA.AS.Global}},OffsetArray{Float64,3,CuDeviceArray{Float64,3,CUDA.AS.Global}}}},Nothing,NamedTuple{(:u, :v, :w, :T, :S),Tuple{ParameterizedForcing{var""#Fu_func#83"",NamedTuple{(:τ,),Tuple{Int64}}},ParameterizedForcing{var""#Fv_func#84"",NamedTuple{(:τ,),Tuple{Int64}}},ParameterizedForcing{var""#Fw_func#85"",NamedTuple{(:τ,),Tuple{Int64}}},typeof(Oceananigans.Forcing.zeroforcing),typeof(Oceananigans.Forcing.zeroforcing)}},OffsetArray{Float64,3,CuDeviceArray{Float64,3,CUDA.AS.Global}},NamedTuple{(:time, :iteration),Tuple{Float64,Int64}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:maxthreads,),Tuple{Int64}}}) at /home/ancellin/.julia/packages/GPUCompiler/4e9CU/src/cache.jl:19; [17] + at ./int.jl:53 [inlined]; [18] hash_64_64 at ./hashing.jl:35 [inlined]; [19] hash_uint64 at ./hashing.jl:62 [inlined]; [20] hx at ./float.jl:568 [inlined]; [21] hash at ./float.jl:571 [inlined]; [22] cached_compilation(::typeof(CUDA._cufunction), ::GPUCompiler.FunctionSpec{typeof(Cassette.overdub),Tuple{Cassette.Context{nametype(CUDACtx),KernelAbstractions.CompilerMetadata{KernelAbstractions.NDIteration.StaticSize{(16, 16, 16)},KernelAbstractions.NDIteration.DynamicCheck,Nothing,Nothing,KernelAbstractions.NDIteration.NDRange{3,KernelAbstractions.NDIteration.StaticSize{(1, 1, 16)},KernelAbstractions.NDIteration.StaticSize{(16, 16, 1)},Nothing,Nothing}},Nothing,KernelAbstractions.var""##PassType#253"",Nothing,Cassette.DisableHooks},typeof(Oceananigans.TimeSteppers.gpu_calculate_Gu!),OffsetArray{Float64,3,CuDeviceArray{Float64,3,CUDA.AS.Global}},RegularCartesianGrid{Float64,Periodic,Periodic,Bounded,OffsetArray{Float64,1,StepRangeLen{Float64,Base.TwicePrec",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/882:45866,cache,cache,45866,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/882,1,['cache'],['cache']
Performance,"loat64}, Int64}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}, Nothing}, NamedTuple{(:time, :iteration, :stage), Tuple{Float64, Int64, Int64}}, NamedTuple{(:u, :v, :w), Tuple{OffsetArrays.OffsetArray{Float64, 3, CUDA.CuDeviceArray{Float64, 3, 1}}, OffsetArrays.OffsetArray{Float64, 3, CUDA.CuDeviceArray{Float64, 3, 1}}, OffsetArrays.OffsetArray{Float64, 3, CUDA.CuDeviceArray{Float64, 3, 1}}}}}}}}); @ GPUCompiler C:\Users\parfe\.julia\packages\GPUCompiler\1FdJy\src\driver.jl:74; [9] cufunction_compile(job::GPUCompiler.CompilerJob); @ CUDA C:\Users\parfe\.julia\packages\CUDA\Uurn4\src\compiler\execution.jl:324; [10] cached_compilation(cache::Dict{UInt64, Any}, job::GPUCompiler.CompilerJob, compiler::typeof(CUDA.cufunction_compile), linker::typeof(CUDA.cufunction_link)); @ GPUCompiler C:\Users\parfe\.julia\packages\GPUCompiler\1FdJy\src\cache.jl:90; [11] cufunction(f::typeof(Oceananigans.BoundaryConditions.gpu__fill_south_and_north_halo!), tt::Type{Tuple{KernelAbstractions.CompilerMetadata{KernelAbstractions.NDIteration.StaticSize{(1024, 1)}, KernelAbstractions.NDIteration.DynamicCheck, Nothing, Nothing, KernelAbstractions.NDIteration.NDRange{2, KernelAbstractions.NDIteration.StaticSize{(64, 1)}, KernelAbstractions.NDIteration.StaticSize{(16, 16)}, Nothing, Nothing}}, Tuple{OffsetArrays.OffsetArray{Float64, 3, CUDA.CuDeviceArray{Float64, 3, 1}}, OffsetArrays.OffsetArray{Float64, 3, CUDA.CuDeviceArray{Float64, 3, 1}}, OffsetArrays.OffsetArray{Float64, 3, CUDA.CuDeviceArray{Float64, 3, 1}}}, Tuple{BoundaryCondition{Oceananigans.BoundaryConditions.Value, Float64}, BoundaryCondition{Oceananigans.BoundaryConditions.Open, Nothing}, BoundaryCondition{Oceananigans.BoundaryConditions.Flux, Nothing}}, Tuple{BoundaryCondition{Oceananigans.BoundaryConditions.Value, Float64}, BoundaryCondition{Oceananigans.BoundaryC",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2530:10238,cache,cache,10238,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2530,1,['cache'],['cache']
Performance,looks like maybe some more race conditions on different hardware?,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1962#issuecomment-906714103:27,race condition,race conditions,27,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1962#issuecomment-906714103,1,['race condition'],['race conditions']
Performance,"ltiplicativeInverse{Int64}, Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64}}}, Oceananigans.AbstractOperations.ConditionalOperation{Center, Center, Center, OffsetArrays.OffsetArray{Float64, 3, CUDA.CuDeviceArray{Float64, 3, 1}}, typeof(identity), RectilinearGrid{Float64, Periodic, Periodic, Bounded, Float64, Float64, Float64, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, Nothing}, typeof(condition_greater_3), Int64, Float64}}}}}); @ GPUCompiler ~/.julia/packages/GPUCompiler/07qaN/src/driver.jl:76; [9] cufunction_compile(job::GPUCompiler.CompilerJob); @ CUDA ~/.julia/packages/CUDA/DfvRa/src/compiler/execution.jl:346; [10] cached_compilation(cache::Dict{UInt64, Any}, job::GPUCompiler.CompilerJob, compiler::typeof(CUDA.cufunction_compile), linker::typeof(CUDA.cufunction_link)); @ GPUCompiler ~/.julia/packages/GPUCompiler/07qaN/src/cache.jl:90; [11] cufunction(f::typeof(CUDA.partial_mapreduce_grid), tt::Type{Tuple{typeof(identity), typeof(Base.add_sum), Nothing, CartesianIndices{3, Tuple{Base.OneTo{Int64}, Base.OneTo{Int64}, Base.OneTo{Int64}}}, CartesianIndices{3, Tuple{Base.OneTo{Int64}, Base.OneTo{Int64}, Base.OneTo{Int64}}}, Val{true}, Base.ReshapedArray{Float64, 4, SubArray{Float64, 3, CUDA.CuDeviceArray{Float64, 3, 1}, Tuple{UnitRange{Int64}, UnitRange{Int64}, UnitRange{Int64}}, false}, Tuple{Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64}, Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64}}}, Oceananigans.AbstractOperations.ConditionalOperation{Center, Center, Center, OffsetArrays.OffsetArray{Float64, 3, CUDA.CuDeviceArray{Float64, 3, 1}}, typeof(identity), RectilinearGrid{Float64, Periodic, Perio",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2701#issuecomment-1242894568:9169,cache,cache,9169,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2701#issuecomment-1242894568,1,['cache'],['cache']
Performance,"lux, Nothing}}, Nothing, Oceananigans.Fields.FieldBoundaryBuffers{Nothing, Nothing, Nothing, Nothing}}, Clock{Float64}}}); @ Oceananigans.AbstractOperations ~/.julia/packages/Oceananigans/RnhUQ/src/AbstractOperations/computed_field.jl:37; [12] test_ke_dissipation_rate_terms(grid::RectilinearGrid{Float64, Periodic, Periodic, Bounded, Float64, Float64, Float64, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, CPU}; model_type::Type{NonhydrostaticModel}, closure::ScalarDiffusivity{Oceananigans.TurbulenceClosures.ExplicitTimeDiscretization, ThreeDimensionalFormulation, Float64, Float64}); @ Main ~/work/Oceanostics.jl/Oceanostics.jl/test/runtests.jl:218; [13] macro expansion; @ ~/work/Oceanostics.jl/Oceanostics.jl/test/runtests.jl:418 [inlined]; [14] macro expansion; @ /opt/hostedtoolcache/julia/1.9.2/x64/share/julia/stdlib/v1.9/Test/src/Test.jl:1498 [inlined]; [15] top-level scope; @ ~/work/Oceanostics.jl/Oceanostics.jl/test/runtests.jl:400; [16] include(fname::String); @ Base.MainInclude ./client.jl:478; [17] top-level scope; @ none:6; [18] eval; @ ./boot.jl:[370](https://github.com/tomchor/Oceanostics.jl/actions/runs/5955695197/job/16154870828?pr=151#step:6:373) [inlined]; [19] exec_options(opts::Base.JLOptions); @ Base ./client.jl:280; [20] _start(); @ Base ./client.jl:522; Test Summary: | Pass Error Total Time; Oceanostics | 58 1 59 2m45.4s; ERROR: LoadError: Some tests did not pass: 58 passed, 0 failed, 1 errored, 0 broken.; in expression starting at /home/runner/work/Oceanostics.jl/Oceanostics.jl/test/runtests.jl:399; ERROR: LoadError: Package Oceanostics errored during testing; Stacktrace:; ```. If so, can we register a bugfix version asap?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3227#issuecomment-1690550352:31154,Load,LoadError,31154,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3227#issuecomment-1690550352,2,['Load'],['LoadError']
Performance,"lways have them together. At some point we have to get the external value, and if it's in the condition slot in `BoundaryCondition` we can get it like this:; https://github.com/CliMA/Oceananigans.jl/blob/77ee4980a32d39d802fc6c7b2a8ef81b4c6c0c1a/src/BoundaryConditions/fill_halo_regions_value_gradient.jl#L9. which can be e.g.; https://github.com/CliMA/Oceananigans.jl/blob/77ee4980a32d39d802fc6c7b2a8ef81b4c6c0c1a/src/BoundaryConditions/continuous_boundary_function.jl#L124-L133; which is called because of there being a `ContinuousBoundaryFunction` in the `BoundaryCondition` type:; https://github.com/CliMA/Oceananigans.jl/blob/77ee4980a32d39d802fc6c7b2a8ef81b4c6c0c1a/src/BoundaryConditions/continuous_boundary_function.jl#L119; which was automatically set-up and then `regularize`-d. https://github.com/CliMA/Oceananigans.jl/blob/77ee4980a32d39d802fc6c7b2a8ef81b4c6c0c1a/src/BoundaryConditions/boundary_condition.jl#L53-L67. https://github.com/CliMA/Oceananigans.jl/blob/77ee4980a32d39d802fc6c7b2a8ef81b4c6c0c1a/src/BoundaryConditions/continuous_boundary_function.jl#L74-L75. But if we have this as part of the condition we get something like:; ```julia; BoundaryCondition{<:Open, <:OpenConditions{ES, MS}}; ```. then we'd have to rewrite loads of stuff to make it so the user can specify whatever they want for the external state. In the existing code the condition and classification are never directly accessed (except from inside `getbc`) and all of the other bits are based on the types:. https://github.com/CliMA/Oceananigans.jl/blob/77ee4980a32d39d802fc6c7b2a8ef81b4c6c0c1a/src/BoundaryConditions/fill_halo_regions_value_gradient.jl#L15. so we can just write methods like:. ```julia; @inline function _fill_west_halo!(j, k, grid, c, bc::BoundaryCondition{Open{<:SomeMatchingScheme}}, loc, args...); ```. I don't really understand how it makes the code any harder to understand if we then get the condition as above v.s. writing new things so we can do `external_state = some_function(bc)`.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3482#issuecomment-1988307344:1378,load,loads,1378,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3482#issuecomment-1988307344,1,['load'],['loads']
Performance,"ly it implements kernels for filling periodic boundary conditions rather than using `view` plus broadcasting, leading to significant performance improvement and reduction in memory allocations. The main problem was periodic directions I think, but there were also issues for normal velocity components. There may be more improvements to be had there. I think we can also do a lot less halo filling for flux boundary conditions, but full optimization would require a bit of work to the turbulence closures. For the default model configuration, we launch about half as many kernels for halo filling now. Before this PR:. ```julia; [ Info: Oceananigans will use 8 threads; [ Info: Benchmarking model with RegularRectilinearGrid{Float64, Periodic, Periodic, Bounded}(Nx=64, Ny=64, Nz=1)...; 226.285 ms (404628 allocations: 164.96 MiB); [ Info: Benchmarking model with RegularRectilinearGrid{Float64, Periodic, Periodic, Bounded}(Nx=64, Ny=1, Nz=64)...; 517.198 ms (431298 allocations: 347.27 MiB); [ Info: Benchmarking model with RegularRectilinearGrid{Float64, Periodic, Periodic, Bounded}(Nx=1, Ny=64, Nz=64)...; 432.185 ms (292065 allocations: 315.60 MiB); ```. After this PR:. ```julia; gregorywagner:benchmark/ (glw/performance✗) $ julia --project benchmark_two_dimensional_models.jl [18:25:59]; [ Info: Oceananigans will use 8 threads; [ Info: Benchmarking model with RegularRectilinearGrid{Float64, Periodic, Periodic, Bounded}(Nx=64, Ny=64, Nz=1)...; 183.673 ms (340830 allocations: 107.52 MiB); [ Info: Benchmarking model with RegularRectilinearGrid{Float64, Periodic, Periodic, Bounded}(Nx=64, Ny=1, Nz=64)...; 229.704 ms (359167 allocations: 108.74 MiB); [ Info: Benchmarking model with RegularRectilinearGrid{Float64, Periodic, Periodic, Bounded}(Nx=1, Ny=64, Nz=64)...; 175.995 ms (281950 allocations: 91.17 MiB); ```. The discrepencies are more extreme for larger models as noted on #1919. I think we should also do GPU benchmarks. @hennyg888 if you have anything to add please feel free...",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1923:1550,perform,performance,1550,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1923,1,['perform'],['performance']
Performance,"m(zspacings(grid, Center())) / maximum(abs, model.velocities.u),; stop_time=20); # Create regular output; simulation.output_writers[:fullfields] = NetCDFOutputWriter(model, (; model.velocities.u),; filename = ""fullfields.nc"",; schedule = TimeInterval(5),; overwrite_existing = true,). # Create interpolated u on coarse grid; coarse_grid = RectilinearGrid(size = (grid.Nx, grid.Ny, grid.Nz÷2), extent = (grid.Lx, grid.Ly, grid.Lz)); coarse_u = Field{Face, Center, Center}(coarse_grid). using Oceananigans.Fields: interpolate!; update_coarse_u(simulation) = interpolate!(coarse_u, simulation.model.velocities.u); simulation.callbacks[:update_interp] = Callback(update_coarse_u). # Create coarse output; simulation.output_writers[:coarsefields] = NetCDFOutputWriter(model, (; coarse_u,), coarse_grid;; filename=""coarsefields.nc"",; schedule=TimeInterval(5),; overwrite_existing=true,). run!(simulation); ```. Throws the following error:. ```; ERROR: LoadError: DimensionMismatch: new dimensions (1, 1, 8, 1) must be consistent with array size 4; Stacktrace:; [1] (::Base.var""#throw_dmrsa#328"")(dims::NTuple{4, Int64}, len::Int64); @ Base ./reshapedarray.jl:41; [2] reshape(a::Array{Float64, 3}, dims::NTuple{4, Int64}); @ Base ./reshapedarray.jl:45; [3] setindex_disk!(::NCDatasets.Variable{Float64, 4, NCDatasets.NCDataset{Nothing, Missing}}, ::Array{Float64, 3}, ::Function, ::Vararg{Any}); @ DiskArrays ~/.julia/packages/DiskArrays/bZBJE/src/diskarray.jl:56; [4] setindex!; @ ~/.julia/packages/DiskArrays/bZBJE/src/diskarray.jl:229 [inlined]; [5] setindex!(::CommonDataModel.CFVariable{…}, ::Array{…}, ::Colon, ::Colon, ::Colon, ::UnitRange{…}); @ CommonDataModel ~/.julia/packages/CommonDataModel/GGvMn/src/cfvariable.jl:477; [6] save_output!(ds::NCDatasets.NCDataset{…}, output::Field{…}, model::NonhydrostaticModel{…}, ow::NetCDFOutputWriter{…}, time_index::Int64, name::String); @ Oceananigans.OutputWriters ~/repos/Oceananigans.jl/src/OutputWriters/netcdf_output_writer.jl:482; [7] write_output!(",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3576#issuecomment-2092831084:1216,Load,LoadError,1216,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3576#issuecomment-2092831084,1,['Load'],['LoadError']
Performance,"magorinskyLilly</td>; <td style = ""text-align: right; "">1.81978</td>; <td style = ""text-align: right; "">1.06441</td>; <td style = ""text-align: right; "">1.07824</td>; </tr>; <tr>; <td style = ""text-align: right; "">CPU</td>; <td style = ""text-align: right; "">TwoDimensionalLeith</td>; <td style = ""text-align: right; "">2.52298</td>; <td style = ""text-align: right; "">1.06287</td>; <td style = ""text-align: right; "">1.07824</td>; </tr>; <tr>; <td style = ""text-align: right; "">CPU</td>; <td style = ""text-align: right; "">VerstappenAnisotropicMinimumDissipation</td>; <td style = ""text-align: right; "">1.821</td>; <td style = ""text-align: right; "">1.16951</td>; <td style = ""text-align: right; "">1.20525</td>; </tr>; </table>. <table>; <caption style = ""text-align: center; "">Turbulence closures relative performance (GPU)</caption>; <tr class = ""header headerLastRow"">; <th style = ""text-align: right; "">Architectures</th>; <th style = ""text-align: right; "">Closures</th>; <th style = ""text-align: right; "">slowdown</th>; <th style = ""text-align: right; "">memory</th>; <th style = ""text-align: right; "">allocs</th>; </tr>; <tr>; <td style = ""text-align: right; "">GPU</td>; <td style = ""text-align: right; "">AnisotropicBiharmonicDiffusivity</td>; <td style = ""text-align: right; "">1.61022</td>; <td style = ""text-align: right; "">1.0052</td>; <td style = ""text-align: right; "">1.00149</td>; </tr>; <tr>; <td style = ""text-align: right; "">GPU</td>; <td style = ""text-align: right; "">AnisotropicDiffusivity</td>; <td style = ""text-align: right; "">1.05231</td>; <td style = ""text-align: right; "">1.00498</td>; <td style = ""text-align: right; "">1.00044</td>; </tr>; <tr>; <td style = ""text-align: right; "">GPU</td>; <td style = ""text-align: right; "">IsotropicDiffusivity</td>; <td style = ""text-align: right; "">1.04851</td>; <td style = ""text-align: right; "">1.00122</td>; <td style = ""text-align: right; "">1.00009</td>; </tr>; <tr>; <td style = ""text-align: right; "">GPU</td>; <td style = ""text-align: right; ",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1169#issuecomment-725471594:18902,perform,performance,18902,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1169#issuecomment-725471594,1,['perform'],['performance']
Performance,"mark!(tracer_kernel_test, GPU();; use_benchmarktools = true); BenchmarkTools.Trial: 5 samples with 1 evaluation.; Range (min … max): 14.189 ms … 14.421 ms ┊ GC (min … max): 0.00% … 0.00%; Time (median): 14.261 ms ┊ GC (median): 0.00%; Time (mean ± σ): 14.269 ms ± 93.553 μs ┊ GC (mean ± σ): 0.00% ± 0.00%. ██ █ █ █; ██▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█ ▁; 14.2 ms Histogram: frequency by time 14.4 ms <. Memory estimate: 47.78 KiB, allocs estimate: 320. ```. The counterpart using the new branch `julia --project=""environments/one_sided_branch"" --check-bounds=no`; ```julia ; julia> using NESAPOceananigans; julia> set_problem_size!(500, 500, 50). julia> trial1 = run_model_benchmark!(momentum_kernel_test, GPU();; use_benchmarktools = true; BenchmarkTools.Trial: 5 samples with 1 evaluation.; Range (min … max): 16.463 ms … 18.503 ms ┊ GC (min … max): 0.00% … 0.00%; Time (median): 16.466 ms ┊ GC (median): 0.00%; Time (mean ± σ): 16.878 ms ± 908.449 μs ┊ GC (mean ± σ): 0.00% ± 0.00%. █; █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▆ ▁; 16.5 ms Histogram: frequency by time 18.5 ms <. Memory estimate: 250.06 KiB, allocs estimate: 676. julia> trial1 = run_model_benchmark!(tracer_kernel_test, arch;; use_benchmarktools = true); BenchmarkTools.Trial: 5 samples with 1 evaluation.; Range (min … max): 6.695 ms … 7.461 ms ┊ GC (min … max): 0.00% … 0.00%; Time (median): 6.789 ms ┊ GC (median): 0.00%; Time (mean ± σ): 6.908 ms ± 312.944 μs ┊ GC (mean ± σ): 0.00% ± 0.00%. █ ██ █ █; █▁▁▁▁▁██▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█ ▁; 6.69 ms Histogram: frequency by time 7.46 ms <. Memory estimate: 46.39 KiB, allocs estimate: 231. ```. P.S. some vestigial code not used is being removed as part of this PR because not beneficial (in terms of both accuracy and performance) that is; - the JS weno formulation (dominated by the Z-weno formulation); - Velocity Upwinding for the vector invariant weno formulation (dominated by the other two formulations)",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3658:3756,perform,performance,3756,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3658,1,['perform'],['performance']
Performance,"masc/.julia/packages/CUDA/fAEDi/src/compiler/exceptions.jl:34; [2] nonblocking_synchronize; @ /glade/work/tomasc/.julia/packages/CUDA/fAEDi/lib/cudadrv/context.jl:331 [inlined]; [3] device_synchronize(); @ CUDA /glade/work/tomasc/.julia/packages/CUDA/fAEDi/lib/cudadrv/context.jl:319; [4] CUDA.CuModule(data::Vector{UInt8}, options::Dict{CUDA.CUjit_option_enum, Any}); @ CUDA /glade/work/tomasc/.julia/packages/CUDA/fAEDi/lib/cudadrv/module.jl:41; [5] CuModule; @ /glade/work/tomasc/.julia/packages/CUDA/fAEDi/lib/cudadrv/module.jl:23 [inlined]; [6] cufunction_link(job::GPUCompiler.CompilerJob, compiled::NamedTuple{(:image, :entry, :external_gvars), Tuple{Vector{UInt8}, String, Vector{String}}}); @ CUDA /glade/work/tomasc/.julia/packages/CUDA/fAEDi/src/compiler/execution.jl:479; [7] cached_compilation(cache::Dict{UInt64, Any}, job::GPUCompiler.CompilerJob, compiler::typeof(CUDA.cufunction_compile), linker::typeof(CUDA.cufunction_link)); @ GPUCompiler /glade/work/tomasc/.julia/packages/GPUCompiler/XyxTy/src/cache.jl:95; [8] cufunction(f::typeof(CUDA.partial_mapreduce_grid), tt::Type{Tuple{typeof(identity), typeof(max), Nothing, CartesianIndices{3, Tuple{Base.OneTo{Int64}, Base.OneTo{Int64}, Base.OneTo{Int64}}}, CartesianIndices{3, Tuple{Base.OneTo{Int64}, Base.OneTo{Int64}, Base.OneTo{Int64}}}, Val{true}, CUDA.CuDeviceArray{Float64, 4, 1}, Oceananigans.AbstractOperations.ConditionalOperation{Face, Center, Center, OffsetArrays.OffsetArray{Float64, 3, CUDA.CuDeviceArray{Float64, 3, 1}}, typeof(identity), ImmersedBoundaryGrid{Float64, Periodic, Periodic, Bounded, RectilinearGrid{Float64, Periodic, Periodic, Bounded, Float64, Float64, Float64, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePreci",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2582#issuecomment-1142637871:5897,cache,cache,5897,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2582#issuecomment-1142637871,1,['cache'],['cache']
Performance,"mod.jl:90; &nbsp; | └ @ SuiteSparse.CHOLMOD /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/SuiteSparse/src/cholmod.jl:187; &nbsp; | /storage7/buildkite-agent/julia-1.5.4/bin/julia: error while loading shared libraries: libLLVM-9jl.so: ELF load command past end of file; &nbsp; | ERROR: LoadError: LoadError: IOError: write: broken pipe (EPIPE); &nbsp; | Stacktrace:; &nbsp; | [1] uv_write(::Base.PipeEndpoint, ::Ptr{UInt8}, ::UInt64) at ./stream.jl:951; &nbsp; | [2] unsafe_write(::Base.PipeEndpoint, ::Ptr{UInt8}, ::UInt64) at ./stream.jl:1005; &nbsp; | [3] write(::Base.PipeEndpoint, ::String) at ./strings/io.jl:183; &nbsp; | [4] create_expr_cache(::String, ::String, ::Array{Pair{Base.PkgId,UInt64},1}, ::Base.UUID) at ./loading.jl:1194; &nbsp; | [5] compilecache(::Base.PkgId, ::String) at ./loading.jl:1286; &nbsp; | [6] _require(::Base.PkgId) at ./loading.jl:1030; &nbsp; | [7] require(::Base.PkgId) at ./loading.jl:928; &nbsp; | [8] require(::Module, ::Symbol) at ./loading.jl:923; &nbsp; | [9] include(::Function, ::Module, ::String) at ./Base.jl:380; &nbsp; | [10] include at ./Base.jl:368 [inlined]; &nbsp; | [11] include(::String) at /storage7/buildkite-agent/.julia-2581/packages/PencilArrays/DTEhf/src/PencilArrays.jl:1; &nbsp; | [12] top-level scope at /storage7/buildkite-agent/.julia-2581/packages/PencilArrays/DTEhf/src/PencilArrays.jl:12; &nbsp; | [13] include(::Function, ::Module, ::String) at ./Base.jl:380; &nbsp; | [14] include(::Module, ::String) at ./Base.jl:368; &nbsp; | [15] top-level scope at none:2; &nbsp; | [16] eval at ./boot.jl:347 [inlined]; &nbsp; | [17] eval(::Expr) at ./client.jl:467; &nbsp; | [18] top-level scope at ./none:3; &nbsp; | in expression starting at /storage7/buildkite-agent/.julia-2581/packages/PencilArrays/DTEhf/src/Pencils/Pencils.jl:7; &nbsp; | in expression starting at /storage7/buildkite-agent/.julia-2581/packages/PencilArrays/DTEhf/src/PencilArrays.jl:12; &nbsp; | ERROR: LoadError: Failed to precompile PencilArra",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1662#issuecomment-843325731:4261,load,loading,4261,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1662#issuecomment-843325731,1,['load'],['loading']
Performance,"ms │ 13.06 KiB │ 290 │; └───────────────┴─────┴───────────────────────────────┴───────────┴───────────┴───────────┴───────────┴───────────┴────────┘; ```. ### CPU to GPU speedup. ```; Fourier-tridiagonal Poisson solver CPU -> GPU speedup; ┌─────┬───────────────────────────────┬─────────┬─────────┬─────────┐; │ Ns │ Topologies │ speedup │ memory │ allocs │; ├─────┼───────────────────────────────┼─────────┼─────────┼─────────┤; │ 256 │ (Bounded, Bounded, Bounded) │ 50.4045 │ 21.5194 │ 32.4444 │; │ 256 │ (Bounded, Periodic, Bounded) │ 51.2039 │ 15.8992 │ 23.2963 │; │ 256 │ (Periodic, Bounded, Bounded) │ 52.4472 │ 15.8992 │ 23.2963 │; │ 256 │ (Periodic, Periodic, Bounded) │ 99.4371 │ 6.48062 │ 10.7407 │; └─────┴───────────────────────────────┴─────────┴─────────┴─────────┘; ```. ### Relative performance on the CPU. ```; Fourier-tridiagonal Poisson solver relative performance (CPU); ┌───────────────┬─────┬───────────────────────────────┬──────────┬──────────┬────────┐; │ Architectures │ Ns │ Topologies │ slowdown │ memory │ allocs │; ├───────────────┼─────┼───────────────────────────────┼──────────┼──────────┼────────┤; │ CPU │ 256 │ (Bounded, Bounded, Bounded) │ 1.58185 │ 1.0 │ 1.0 │; │ CPU │ 256 │ (Bounded, Periodic, Bounded) │ 1.24529 │ 0.922481 │ 1.0 │; │ CPU │ 256 │ (Periodic, Bounded, Bounded) │ 1.27117 │ 0.922481 │ 1.0 │; │ CPU │ 256 │ (Periodic, Periodic, Bounded) │ 1.0 │ 1.0 │ 1.0 │; └───────────────┴─────┴───────────────────────────────┴──────────┴──────────┴────────┘; ```. ### Relative performance on the GPU. ```; Fourier-tridiagonal Poisson solver relative performance (GPU); ┌───────────────┬─────┬───────────────────────────────┬──────────┬─────────┬─────────┐; │ Architectures │ Ns │ Topologies │ slowdown │ memory │ allocs │; ├───────────────┼─────┼───────────────────────────────┼──────────┼─────────┼─────────┤; │ GPU │ 256 │ (Bounded, Bounded, Bounded) │ 3.12065 │ 3.32057 │ 3.02069 │; │ GPU │ 256 │ (Bounded, Periodic, Bounded) │ 2.41833 │ 2.26316 │ 2.16897 │;",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1403#issuecomment-786398050:8305,perform,performance,8305,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1403#issuecomment-786398050,1,['perform'],['performance']
Performance,"n as `err` and call `code_typed(err; interactive = true)` to introspect the erronous code with Cthulhu.jl; Stacktrace:; [1] check_ir(job::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget, CUDA.CUDACompilerParams}, args::LLVM.Module); @ GPUCompiler ~/.julia/packages/GPUCompiler/2CW9L/src/validation.jl:147; [2] macro expansion; @ ~/.julia/packages/GPUCompiler/2CW9L/src/driver.jl:382 [inlined]; [3] macro expansion; @ ~/.julia/packages/TimerOutputs/NRdsv/src/TimerOutput.jl:253 [inlined]; [4] macro expansion; @ ~/.julia/packages/GPUCompiler/2CW9L/src/driver.jl:381 [inlined]; [5] emit_llvm(job::GPUCompiler.CompilerJob; toplevel::Bool, libraries::Bool, optimize::Bool, cleanup::Bool, validate::Bool, only_entry::Bool); @ GPUCompiler ~/.julia/packages/GPUCompiler/2CW9L/src/utils.jl:108; [6] emit_llvm; @ ~/.julia/packages/GPUCompiler/2CW9L/src/utils.jl:106 [inlined]; [7] codegen(output::Symbol, job::GPUCompiler.CompilerJob; toplevel::Bool, libraries::Bool, optimize::Bool, cleanup::Bool, validate::Bool, strip::Bool, only_entry::Bool, parent_job::Nothing); @ GPUCompiler ~/.julia/packages/GPUCompiler/2CW9L/src/driver.jl:100; [8] codegen; @ ~/.julia/packages/GPUCompiler/2CW9L/src/driver.jl:82 [inlined]; [9] compile(target::Symbol, job::GPUCompiler.CompilerJob; kwargs::@Kwargs{}); @ GPUCompiler ~/.julia/packages/GPUCompiler/2CW9L/src/driver.jl:79; [10] compile; @ ~/.julia/packages/GPUCompiler/2CW9L/src/driver.jl:74 [inlined]; [11] #1145; @ ~/.julia/packages/CUDA/2kjXI/src/compiler/compilation.jl:250 [inlined]; [12] JuliaContext(f::CUDA.var""#1145#1148""{GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget, CUDA.CUDACompilerParams}}; kwargs::@Kwargs{}); @ GPUCompiler ~/.julia/packages/GPUCompiler/2CW9L/src/driver.jl:34; [13] JuliaContext(f::Function); @ GPUCompiler ~/.julia/packages/GPUCompiler/2CW9L/src/driver.jl:25; [14] compile(job::GPUCompiler.CompilerJob); @ CUDA ~/.julia/packages/CUDA/2kjXI/src/compiler/compilation.jl:249; [15] actual_compilation(cache::Dict{Any, CuFunction",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3646#issuecomment-2428001700:6527,optimiz,optimize,6527,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3646#issuecomment-2428001700,1,['optimiz'],['optimize']
Performance,"n at /home/fpoulin/.julia/packages/CUDA/0p5fn/lib/cudadrv/libcuda.jl:35 [inlined]; │ [11] macro expansion at /home/fpoulin/.julia/packages/CUDA/0p5fn/lib/cudadrv/error.jl:102 [inlined]; │ [12] cuDeviceGetCount(::Base.RefValue{Int32}) at /home/fpoulin/.julia/packages/CUDA/0p5fn/lib/utils/call.jl:93; │ [13] length at /home/fpoulin/.julia/packages/CUDA/0p5fn/lib/cudadrv/devices.jl:111 [inlined]; │ [14] iterate at /home/fpoulin/.julia/packages/CUDA/0p5fn/lib/cudadrv/devices.jl:106 [inlined] (repeats 2 times); │ [15] iterate at ./iterators.jl:139 [inlined]; │ [16] iterate at ./iterators.jl:138 [inlined]; │ [17] __init__() at /home/fpoulin/software/Oceananigans.jl/src/Oceananigans.jl:178; │ [18] _include_from_serialized(::String, ::Array{Any,1}) at ./loading.jl:697; │ [19] _require_search_from_serialized(::Base.PkgId, ::String) at ./loading.jl:782; │ [20] _require(::Base.PkgId) at ./loading.jl:1007; │ [21] require(::Base.PkgId) at ./loading.jl:928; │ [22] require(::Module, ::Symbol) at ./loading.jl:923; │ [23] eval(::Module, ::Any) at ./boot.jl:331; │ [24] eval_user_input(::Any, ::REPL.REPLBackend) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/REPL/src/REPL.jl:134; │ [25] repl_backend_loop(::REPL.REPLBackend) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/REPL/src/REPL.jl:195; │ [26] start_repl_backend(::REPL.REPLBackend, ::Any) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/REPL/src/REPL.jl:180; │ [27] run_repl(::REPL.AbstractREPL, ::Any; backend_on_current_task::Bool) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/REPL/src/REPL.jl:292; │ [28] run_repl(::REPL.AbstractREPL, ::Any) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/REPL/src/REPL.jl:288; │ [29] (::Base.var""#807#809""{Bool,Bool,Bool,Bool})(::Module) at ./client.jl:399; │ [30] #invokelatest#1 at ./essentials.jl:710 [inlined]; │ [31] invokelatest at ./essentials.jl:709 [inlined]; │ [32] ",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1189:2404,load,loading,2404,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1189,1,['load'],['loading']
Performance,"n interpolate in between and back to the sphere to get the other staggered grid coordinates. The file should provide this. > 5. We need to use the sine and cosine of the local angle between the grid orientation (e.g., u-velocity) and zonal west-east direction at the grid-cell centers to `set!` velocity fields. Right now we're limited to setting velocity fields from a streamfunction... Let's come up with an abstraction for vectors and use this for `HydrostaticFreeSurfaceModel`. > 6. Should we be filling the halos of the grid metrics? Seems like they should be filled like velocity halos but without any sign changes. Yes. > 7. Make sure that calling `fill_halo_regions!` on a horizontal velocity field only fills the halos with non-`CubedSphereExchange` boundary conditions. Seems easy, but setting boundary conditions will require an abstraction for vectors and coordinate system transformations. > 8. Add the cubed sphere passive tracer advection and surface gravity waves validation experiments to validation CI pipeline. Not urgent. > 9. Figure out a way to abstract and clean up `fill_horizontal_velocity_halos!` and `fill_*_halo!`... Let's come up with an abstraction for vectors and use this for `HydrostaticFreeSurfaceModel`. We want to write `fill_halo_regions!(::HorizontalVectorField, ...)`. > 10. Merge utils from `src/CubedSpheres/cubed_sphere_utils.jl` and `src/Distributed/distributed_utils.jl` into `Oceananigans.Grids`. No rush... > 11. Figure out if the tests in `test_cubed_sphere_halo_exchange.jl` can be abstracted and run for all six faces. It's tedious to have to type them out but could be clearer and easier to debug in its current form... I don't think less abstraction is more clear. > 12. Fix tests!. Tests must pass or we can't develop. > 13. I think cubed sphere performance can be improved, especially for halo filling. We might need some benchmarks and profiling to figure out where cubed sphere simulations are allocating too much memory. Not urgent for this PR.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1522#issuecomment-818046333:2881,perform,performance,2881,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1522#issuecomment-818046333,1,['perform'],['performance']
Performance,"nclude at ./Base.jl:368 [inlined]; [4] include(::String) at /Users/truedichotomy/.julia/packages/GPUArrays/PkHCM/src/GPUArrays.jl:1; [5] top-level scope at /Users/truedichotomy/.julia/packages/GPUArrays/PkHCM/src/GPUArrays.jl:25; [6] include(::Function, ::Module, ::String) at ./Base.jl:380; [7] include(::Module, ::String) at ./Base.jl:368; [8] top-level scope at none:2; [9] eval at ./boot.jl:331 [inlined]; [10] eval(::Expr) at ./client.jl:467; [11] top-level scope at ./none:3; in expression starting at /Users/truedichotomy/.julia/packages/GPUArrays/PkHCM/src/host/abstractarray.jl:24; in expression starting at /Users/truedichotomy/.julia/packages/GPUArrays/PkHCM/src/GPUArrays.jl:25; ERROR: LoadError: Failed to precompile GPUArrays [0c68f7d7-f131-5f86-a1c3-88cf8149b2d7] to /Users/truedichotomy/.julia/compiled/v1.5/GPUArrays/v5u0T_IyCmP.ji.; Stacktrace:; [1] error(::String) at ./error.jl:33; [2] compilecache(::Base.PkgId, ::String) at ./loading.jl:1290; [3] _require(::Base.PkgId) at ./loading.jl:1030; [4] require(::Base.PkgId) at ./loading.jl:928; [5] require(::Module, ::Symbol) at ./loading.jl:923; [6] include(::Function, ::Module, ::String) at ./Base.jl:380; [7] include(::Module, ::String) at ./Base.jl:368; [8] top-level scope at none:2; [9] eval at ./boot.jl:331 [inlined]; [10] eval(::Expr) at ./client.jl:467; [11] top-level scope at ./none:3; in expression starting at /Users/truedichotomy/.julia/packages/CUDA/7vLVC/src/CUDA.jl:5; ERROR: LoadError: Failed to precompile CUDA [052768ef-5323-5732-b1bb-66c8b64840ba] to /Users/truedichotomy/.julia/compiled/v1.5/CUDA/oWw5k_IyCmP.ji.; Stacktrace:; [1] error(::String) at ./error.jl:33; [2] compilecache(::Base.PkgId, ::String) at ./loading.jl:1290; [3] _require(::Base.PkgId) at ./loading.jl:1030; [4] require(::Base.PkgId) at ./loading.jl:928; [5] require(::Module, ::Symbol) at ./loading.jl:923; [6] include(::Function, ::Module, ::String) at ./Base.jl:380; [7] include(::Module, ::String) at ./Base.jl:368; [8] top-level scope a",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/854:1458,load,loading,1458,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/854,1,['load'],['loading']
Performance,"ndaries` is not yet there, but I am trying to use it to define the walls of a numerical rotating tank experiment. I am using `Oceananigans v0.73.1`. The model instantiation below. ```julia; model = NonhydrostaticModel(grid = grid,; advection = UpwindBiasedFifthOrder(),; tracers = (:T, :S),; coriolis = coriolis,; buoyancy = SeawaterBuoyancy(),; closure = ScalarDiffusivity(ν=1e-6,κ=1e-6),; boundary_conditions = (u=u_bcs,v=v_bcs)). set!(model,T=25,S=0); ```. which only returns the following error while using `GPU()`. ```julia; ERROR: a exception was thrown during kernel execution.; Run Julia on debug level 2 for device ; CUDA error: an illegal memory access was encountered (code 700, ERROR_ILLEGAL_ADDRESS). Stacktrace:; [1] throw_api_error(res::CUDA.cudaError_enum); @ CUDA ~/.julia/packages/CUDA/DL5Zo/lib/cudadrv/error.jl:105; [2] macro expansion; @ ~/.julia/packages/CUDA/DL5Zo/lib/cudadrv/error.jl:115 [inlined]; [3] cuCtxSynchronize(); @ CUDA ~/.julia/packages/CUDA/DL5Zo/lib/utils/call.jl:26; [4] device_synchronize; @ ~/.julia/packages/CUDA/DL5Zo/lib/cudadrv/context.jl:319 [inlined]; [5] CUDA.CuModule(data::Vector{UInt8}, options::Dict{CUDA.CUjit_option_enum, Any}); @ CUDA ~/.julia/packages/CUDA/DL5Zo/lib/cudadrv/module.jl:41; [6] CuModule; @ ~/.julia/packages/CUDA/DL5Zo/lib/cudadrv/module.jl:23 [inlined]; [7] cufunction_link(job::GPUCompiler.CompilerJob, compiled::NamedTuple{(:image, :entry, :external_gvars), Tuple{Vector{UInt8}, String, Vector{String}}}); @ CUDA ~/.julia/packages/CUDA/DL5Zo/src/compiler/execution.jl:442; [8] cached_compilation(cache::Dict{UInt64, Any}, job::GPUCompiler.CompilerJob, compiler::typeof(CUDA.cufunction_compile), linker::typeof(CUDA.cufunction_link)); @ GPUCompiler ~/.julia/packages/GPUCompiler/fG3xK/src/cache.jl:94; .; .; .; ```. The code runs when I use the `underlying_grid` instead or if I change to `CPU`.; The full code is available and can be run on Google Colab. https://github.com/iuryt/OceanGyreTank.jl/blob/main/OceanGyreTank.ipynb",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2367:1599,cache,cache,1599,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2367,2,['cache'],['cache']
Performance,"nded} on CPU with 3×3×3 halo; ├── Periodic x ∈ [0.0, 1.0) regularly spaced with Δx=1.0; ├── Periodic y ∈ [0.0, 1.0) regularly spaced with Δy=0.125; └── Bounded z ∈ [-1.0, 0.0] regularly spaced with Δz=0.125. julia> model = NonhydrostaticModel(; grid); warning: /Users/navid/.julia/packages/KernelAbstractions/WoCk1/src/extras/loopinfo.jl:28:0: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering; warning: /Users/navid/.julia/packages/KernelAbstractions/WoCk1/src/extras/loopinfo.jl:28:0: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering; warning: /Users/navid/.julia/packages/KernelAbstractions/WoCk1/src/extras/loopinfo.jl:28:0: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering; warning: /Users/navid/.julia/packages/KernelAbstractions/WoCk1/src/extras/loopinfo.jl:28:0: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering; warning: /Users/navid/.julia/packages/KernelAbstractions/WoCk1/src/extras/loopinfo.jl:28:0: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering; warning: /Users/navid/.julia/packages/KernelAbstractions/WoCk1/src/extras/loopinfo.jl:28:0: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering; warning: /Users/navid/.julia/packages/KernelAbstractions/WoCk1/src/extras/loopin",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3403#issuecomment-1872469814:1401,optimiz,optimizer,1401,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3403#issuecomment-1872469814,2,"['optimiz', 'perform']","['optimizer', 'perform']"
Performance,ne); MPI_Isend at /orcd/data/raffaele/001/glwagner/.julia/artifacts/e85c0a68e07fee0ee7b19c2abc210b1af2f4771a/lib/libmpi.so (unknown line); MPI_Isend at /orcd/data/raffaele/001/glwagner/.julia/packages/MPI/TKXAj/src/api/generated_api.jl:2151 [inlined]; Isend at /orcd/data/raffaele/001/glwagner/.julia/packages/MPI/TKXAj/src/pointtopoint.jl:66; Isend at /orcd/data/raffaele/001/glwagner/.julia/packages/MPI/TKXAj/src/pointtopoint.jl:70 [inlined]; Isend at /orcd/data/raffaele/001/glwagner/.julia/packages/MPI/TKXAj/src/pointtopoint.jl:70 [inlined]; send_south_halo at /orcd/data/raffaele/001/glwagner/Oceananigans.jl/src/DistributedComputations/halo_communication.jl:317; #fill_south_and_north_halo!#50 at /orcd/data/raffaele/001/glwagner/Oceananigans.jl/src/DistributedComputations/halo_communication.jl:263; fill_south_and_north_halo! at /orcd/data/raffaele/001/glwagner/Oceananigans.jl/src/DistributedComputations/halo_communication.jl:250; unknown function (ip: 0x2aaac8afa8b6); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; #fill_halo_event!#40 at /orcd/data/raffaele/001/glwagner/Oceananigans.jl/src/DistributedComputations/halo_communication.jl:208; fill_halo_event! at /orcd/data/raffaele/001/glwagner/Oceananigans.jl/src/DistributedComputations/halo_communication.jl:193; unknown function (ip: 0x2aaac8aefb2e); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; #fill_halo_regions!#38 at /orcd/data/raffaele/001/glwagner/Oceananigans.jl/src/DistributedComputations/halo_communication.jl:114; fill_halo_regions! at /orcd/data/raffaele/001/glwagner/Oceananigans.jl/src/DistributedComputations/halo_communication.jl:101 [inlined]; #fill_halo_regions!#37 at /orcd/data/raffaele/001/glwagner/O,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3878:2480,cache,cache,2480,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3878,1,['cache'],['cache']
Performance,"ned]; │ [17] _require_prelocked; │ @ ./loading.jl:1802 [inlined]; │ [18] run_extension_callbacks(extid::Base.ExtensionId); │ @ Base ./loading.jl:1295; │ [19] run_extension_callbacks(pkgid::Base.PkgId); │ @ Base ./loading.jl:1330; │ [20] run_package_callbacks(modkey::Base.PkgId); │ @ Base ./loading.jl:1164; │ [21] _tryrequire_from_serialized(modkey::Base.PkgId, path::String, ocachepath::String, sourcepath::String, depmods::Vector{Any}); │ @ Base ./loading.jl:1487; │ [22] _require_search_from_serialized(pkg::Base.PkgId, sourcepath::String, build_id::UInt128); │ @ Base ./loading.jl:1574; │ [23] _require(pkg::Base.PkgId, env::String); │ @ Base ./loading.jl:1938; │ [24] __require_prelocked(uuidkey::Base.PkgId, env::String); │ @ Base ./loading.jl:1812; │ [25] #invoke_in_world#3; │ @ ./essentials.jl:926 [inlined]; │ [26] invoke_in_world; │ @ ./essentials.jl:923 [inlined]; │ [27] _require_prelocked(uuidkey::Base.PkgId, env::String); │ @ Base ./loading.jl:1803; │ [28] macro expansion; │ @ ./loading.jl:1790 [inlined]; │ [29] macro expansion; │ @ ./lock.jl:267 [inlined]; │ [30] __require(into::Module, mod::Symbol); │ @ Base ./loading.jl:1753; │ [31] #invoke_in_world#3; │ @ ./essentials.jl:926 [inlined]; │ [32] invoke_in_world; │ @ ./essentials.jl:923 [inlined]; │ [33] require(into::Module, mod::Symbol); │ @ Base ./loading.jl:1746; │ [34] eval; │ @ ./boot.jl:385 [inlined]; │ [35] eval_user_input(ast::Any, backend::REPL.REPLBackend, mod::Module); │ @ REPL ~/julia-1.10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:150; │ [36] repl_backend_loop(backend::REPL.REPLBackend, get_module::Function); │ @ REPL ~/julia-1.10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:246; │ [37] start_repl_backend(backend::REPL.REPLBackend, consumer::Any; get_module::Function); │ @ REPL ~/julia-1.10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:231; │ [38] run_repl(repl::REPL.AbstractREPL, consumer::Any; backend_on_current_task::Bool, backend::Any); │ @ REPL ~/julia-1.10/usr/share/julia/stdlib/v1.10/REPL/",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3686#issuecomment-2272432528:3604,load,loading,3604,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3686#issuecomment-2272432528,1,['load'],['loading']
Performance,ng/julia-release-1-dot-10/src/toplevel.c:985; eval at ./boot.jl:385 [inlined]; eval_user_input at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:150; repl_backend_loop at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:246; #start_repl_backend#46 at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:231; start_repl_backend at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:228; _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; #run_repl#59 at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:389; run_repl at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:375; jfptr_run_repl_91805.1 at /home/alir/.julia/juliaup/julia-1.10.5+0.x64.linux.gnu/lib/julia/sys.so (unknown line); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; #1013 at ./client.jl:432; jfptr_YY.1013_82772.1 at /home/alir/.julia/juliaup/julia-1.10.5+0.x64.linux.gnu/lib/julia/sys.so (unknown line); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; jl_apply at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/julia.h:1982 [inlined]; jl_f__call_latest at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/builtins.c:812; #invokelatest#2 at ./essentials.jl:892 [inlined]; invokelat,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3852:7273,cache,cache,7273,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3852,1,['cache'],['cache']
Performance,ng/julia-release-1-dot-10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:389; run_repl at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:375; jfptr_run_repl_91805.1 at /home/alir/.julia/juliaup/julia-1.10.5+0.x64.linux.gnu/lib/julia/sys.so (unknown line); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; #1013 at ./client.jl:432; jfptr_YY.1013_82772.1 at /home/alir/.julia/juliaup/julia-1.10.5+0.x64.linux.gnu/lib/julia/sys.so (unknown line); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; jl_apply at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/julia.h:1982 [inlined]; jl_f__call_latest at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/builtins.c:812; #invokelatest#2 at ./essentials.jl:892 [inlined]; invokelatest at ./essentials.jl:889 [inlined]; run_main_repl at ./client.jl:416; exec_options at ./client.jl:333; _start at ./client.jl:552; jfptr__start_82798.1 at /home/alir/.julia/juliaup/julia-1.10.5+0.x64.linux.gnu/lib/julia/sys.so (unknown line); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; jl_apply at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/julia.h:1982 [inlined]; true_main at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/jlapi.c:582; jl_repl_entrypoint at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/jlapi.c:731; main at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/cli/loader_exe.c:58; unknown function (ip: 0x7c00f758ce07); __l,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3852:8166,cache,cache,8166,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3852,1,['cache'],['cache']
Performance,"ngeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, Nothing}, RectilinearGrid{Float64, Flat, Flat, Bounded, Float64, Float64, OffsetVector{Float64, CuDeviceVector{Float64, 1}}, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}, OffsetVector{Float64, CuDeviceVector{Float64, 1}}, Nothing}, SubArray{Float64, 1, OffsetVector{Float64, CuDeviceVector{Float64, 1}}, Tuple{UnitRange{Int64}}, true}}}}}); @ GPUCompiler /g/data/v45/nc3020/.julia/packages/GPUCompiler/jVY4I/src/driver.jl:76; [9] cufunction_compile(job::GPUCompiler.CompilerJob); @ CUDA /g/data/v45/nc3020/.julia/packages/CUDA/DfvRa/src/compiler/execution.jl:346; [10] cached_compilation(cache::Dict{UInt64, Any}, job::GPUCompiler.CompilerJob, compiler::typeof(CUDA.cufunction_compile), linker::typeof(CUDA.cufunction_link)); @ GPUCompiler /g/data/v45/nc3020/.julia/packages/GPUCompiler/jVY4I/src/cache.jl:90; [11] cufunction(f::typeof(Oceananigans.Fields.gpu__regrid!), tt::Type{Tuple{KernelAbstractions.CompilerMetadata{KernelAbstractions.NDIteration.StaticSize{(1, 1)}, KernelAbstractions.NDIteration.DynamicCheck, Nothing, Nothing, KernelAbstractions.NDIteration.NDRange{2, KernelAbstractions.NDIteration.StaticSize{(1, 1)}, KernelAbstractions.NDIteration.StaticSize{(1, 1)}, Nothing, Nothing}}, OffsetArray{Float64, 3, CuDeviceArray{Float64, 3, 1}}, OffsetArray{Float64, 3, CuDeviceArray{Float64, 3, 1}}, RectilinearGrid{Float64, Flat, Flat, Bounded, Float64, Float64, Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}, OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, Nothing}, RectilinearGrid{Float64, Flat, Flat, Bounded, Float64, Float64, OffsetVector{Float64, CuDeviceVe",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2701#issuecomment-1221177574:6853,cache,cache,6853,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2701#issuecomment-1221177574,1,['cache'],['cache']
Performance,"nicator); @load ""bathymetry.jld2"" bathymetry. @show size(bathymetry). grid = ImmersedBoundaryGrid(grid, GridFittedBottom(bathymetry)). @show grid; ```. Run this with. ```bash; mpiexec -n 2 julia --project mwe.jl; ```. from the Oceananigans repo. I get. ```julia; $ /Users/gregorywagner/.julia/bin/mpiexecjl -n 2 julia --project mwe.jl [17:11:03]; [ Info: MPI has not been initialized, so we are calling MPI.Init().; [ Info: MPI has not been initialized, so we are calling MPI.Init().; arch = arch = Distributed{CPU} across 2 = 2×1×1 ranks:; ├── local_rank: 1 of 0-1; ├── local_index: [2, 1, 1]; └── connectivity: east=0 west=0Distributed{CPU} across 2 = 2×1×1 ranks:; ├── local_rank: 0 of 0-1; ├── local_index: [1, 1, 1]; └── connectivity: east=1 west=1. size(grid) = size(grid) = (4, 2, 2); (4, 2, 2); size(bathymetry) = (8, 2, 2); size(bathymetry) = (8, 2, 2); ERROR: LoadError: ERROR: LoadError: ArgumentError: ERROR: DimensionMismatch: array could not be set to match destination field; Stacktrace:; [1] ArgumentError: ERROR: DimensionMismatch: array could not be set to match destination fieldset!(u::Field{Center, Center, Nothing, Nothing, RectilinearGrid{Float64, FullyConnected, Periodic, Bounded, Float64, Float64, Float64, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, Distributed{CPU, false, Partition{Int64, Nothing, Nothing}, Tuple{Int64, Int64, Int64}, Int64, Tuple{Int64, Int64, Int64}, Oceananigans.DistributedComputations.RankConnectivity{Int64, Int64, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing}, MPI.Comm, Vector{MPI.Request}, Base.RefValue{Int64}}}, Tuple{Colon, Colon, Colon}, OffsetArrays.OffsetArray{Float64, 3, Array{Float64, 3}}, Flo",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3816:1240,Load,LoadError,1240,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3816,2,['Load'],['LoadError']
Performance,"ns currently does not store intermediate terms in the computation of a PDE's right hand side (with notable exceptions hydrostatic pressure and eddy diffusivities). In other words, a single, sometimes large kernel that evaluates the right hand side at each grid point `i, j, k` is compiled for each PDE in a user's model, and there's no way to pull out intermediate steps in that calculation to use elsewhere. It's important to note when considering optimization strategies that our computations are probably memory-limited, rather than compute-limited. In other words, we think the process of transferring data from global memory to local thread memory is a bottleneck for our computations (we can really only know this through profiling a particular application, however, since all models are different...) Storing intermediate components of the tendency terms would probably create more memory accesses overall (since rather than immediately using intermediate results for subsequent calculations, we would have to send them to global memory, and then back, to complete the evaluation of a tendency) --- and thus could slow down tendency evaluations that are performed 1-3 times per time-step. For example, our best idea for speeding up tendency evaluations is to better manage memory movement using GPU shared memory (unfortunately, we haven't had the time to explore such optimization strategies...). I think there may be other ways to optimize diagnostics calculations, however. # Fusing `ComputedField` kernels. One possibility to speed up diagnostics is to ""fuse"" kernels for different `ComputedField` diagnostics. The kernel for a `ComputedField` is. https://github.com/CliMA/Oceananigans.jl/blob/9b52f3f911d26a66c75f1c3cb58fdd0a1cecb131/src/Fields/computed_field.jl#L112-L115. where `operand` is an `AbstractOperation`. But different `ComputedField`s may somehow depend on the same underlying data in memory. Thus if the kernels for differnet `ComputedField`s are fused into one, we overlap m",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1483#issuecomment-800567837:1196,perform,performed,1196,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1483#issuecomment-800567837,1,['perform'],['performed']
Performance,"ns v1.3.1; [d496a93d] SeawaterPolynomials v0.2.3; [09ab397b] StructArrays v0.6.11; [bc48ee85] Tullio v0.3.4; [ade2ca70] Dates; [b77e0a4c] InteractiveUtils; [37e2e46d] LinearAlgebra; [56ddb016] Logging; [44cfe95a] Pkg; [de0858da] Printf; [9a3f8284] Random; [2f01184e] SparseArrays; [10745b16] Statistics. (Oceananigans) pkg> precompile; Precompiling project...; ✗ Oceananigans; 0 dependencies successfully precompiled in 11 seconds (99 already precompiled). ERROR: The following 1 direct dependency failed to precompile:. Oceananigans [9e8cae18-63c1-5223-a75c-80ca9d6e9a09]. Failed to precompile Oceananigans [9e8cae18-63c1-5223-a75c-80ca9d6e9a09] to /Users/navid/.julia/compiled/v1.6/Oceananigans/jl_Z5b4Xf.; ERROR: LoadError: LoadError: LoadError: InitError: UndefVarError: libamgxsh not defined; Stacktrace:; [1] getproperty; @ ./Base.jl:26 [inlined]; [2] __init__(); @ AMGX ~/.julia/packages/AMGX/GFHHN/src/AMGX.jl:30; [3] _include_from_serialized(path::String, depmods::Vector{Any}); @ Base ./loading.jl:696; [4] _require_search_from_serialized(pkg::Base.PkgId, sourcepath::String); @ Base ./loading.jl:782; [5] _require(pkg::Base.PkgId); @ Base ./loading.jl:1020; [6] require(uuidkey::Base.PkgId); @ Base ./loading.jl:936; [7] require(into::Module, mod::Symbol); @ Base ./loading.jl:923; [8] include(mod::Module, _path::String); @ Base ./Base.jl:384; [9] include(x::String); @ Oceananigans.Solvers ~/Research/OC.jl/src/Solvers/Solvers.jl:1; [10] top-level scope; @ ~/Research/OC.jl/src/Solvers/Solvers.jl:48; [11] include(mod::Module, _path::String); @ Base ./Base.jl:384; [12] include(x::String); @ Oceananigans ~/Research/OC.jl/src/Oceananigans.jl:5; [13] top-level scope; @ ~/Research/OC.jl/src/Oceananigans.jl:195; [14] include; @ ./Base.jl:384 [inlined]; [15] include_package_for_output(pkg::Base.PkgId, input::String, depot_path::Vector{String}, dl_load_path::Vector{String}, load_path::Vector{String}, concrete_deps::Vector{Pair{Base.PkgId, UInt64}}, source::Nothing); @ Base ./loading.jl:",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2688#issuecomment-1217694987:2153,load,loading,2153,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2688#issuecomment-1217694987,1,['load'],['loading']
Performance,"ns.Fields: condition_operand; IF = AbstractField{<:Any, <:Any, <:Any, <:ImmersedBoundaryGrid}; using Oceananigans.ImmersedBoundaries: NotImmersed. using Oceananigans.Architectures: architecture, arch_array; @inline function condition_operand(func::Function, operand::IF, condition::AbstractArray, mask); condition = arch_array(architecture(operand.grid), NotImmersed(condition)); return ConditionalOperation(operand; func, condition, mask); end. using Oceananigans.Grids: ynode, znode; yᶜᶜᶜ = KernelFunctionOperation{Center, Center, Center}(ynode, grid, Center(), Center(), Center()); zᶜᶜᶜ = KernelFunctionOperation{Center, Center, Center}(znode, grid, Center(), Center(), Center()). c = Field((Center, Center, Center), grid); set!(c, (x, y, z) -> ifelse(z>-1/2, ifelse(y<3/4, 7, 13), -9)). b = compute!(Field(Average(c, condition=(yᶜᶜᶜ .>1/2))));; ```. As mentioned, this produces the correct result on the CPU (`(13+7)/2 = 10`) but fails on the GPU with the error:. ```; ERROR: LoadError: GPU compilation of MethodInstance for CUDA.partial_mapreduce_grid(::typeof(identity), ::typeof(Base.add_sum), ::Nothing, ::CartesianIndices{3, Tuple{Base.OneTo{Int64}, Base.OneTo{Int64}, Base.OneTo{Int64}}}, ::CartesianIndices{3, Tuple{Base.OneTo{Int64}, Base.OneTo{Int64}, Base.OneTo{Int64}}}, ::Val{true}, ::Base.ReshapedArray{Float64, 4, SubArray{Float64, 3, CUDA.CuDeviceArray{Float64, 3, 1}, Tuple{UnitRange{Int64}, UnitRange{Int64}, UnitRange{Int64}}, false}, Tuple{Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64}, Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64}}}, ::ConditionalOperation{Center, Center, Center, Oceananigans.AbstractOperations.GridMetricOperation{Center, Center, Center, ImmersedBoundaryGrid{Float64, Periodic, Periodic, Bounded, RectilinearGrid{Float64, Periodic, Periodic, Bounded, Float64, Float64, Float64, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.Offset",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3439#issuecomment-1905170213:2350,Load,LoadError,2350,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3439#issuecomment-1905170213,1,['Load'],['LoadError']
Performance,ns.jl/src/DistributedComputations/halo_communication.jl:90 [inlined]; fill_halo_regions! at /orcd/data/raffaele/001/glwagner/Oceananigans.jl/src/DistributedComputations/halo_communication.jl:87; unknown function (ip: 0x2aaac8ad0ee5); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; jl_apply at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/julia.h:1982 [inlined]; do_call at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:126; eval_value at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:223; eval_stmt_value at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:174 [inlined]; eval_body at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:617; jl_interpret_toplevel_thunk at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:775; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:934; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:877; ijl_toplevel_eval_in at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:985; eval at ./boot.jl:385 [inlined]; include_string at ./loading.jl:2076; _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; _include at ./loading.jl:2136; include at ./Base.jl:495; jfptr_include_46447.1 at /orcd/data/raffaele/001/glwagner/Software/julia-1.10.5/lib/julia/sys.so (unknown line); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-re,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3878:4474,cache,cache,4474,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3878,1,['cache'],['cache']
Performance,ns.jl/src/TimeSteppers/quasi_adams_bashforth_2.jl:124; time_step! at /home/alir/atdepth/Oceananigans.jl/src/TimeSteppers/quasi_adams_bashforth_2.jl:76; unknown function (ip: 0x7c00a0f12fbd); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; top-level scope at /home/alir/atdepth/Oceananigans.jl/particles_error.jl:37; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:925; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:877; ijl_toplevel_eval_in at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:985; eval at ./boot.jl:385 [inlined]; include_string at ./loading.jl:2076; _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; _include at ./loading.jl:2136; include at ./client.jl:489; unknown function (ip: 0x7c00f54ff855); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; jl_apply at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/julia.h:1982 [inlined]; do_call at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:126; eval_value at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:223; eval_stmt_value at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:174 [inlined]; eval_body at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:617; jl_interpret_toplevel_thunk at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:775; jl_tople,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3852:4806,cache,cache,4806,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3852,1,['cache'],['cache']
Performance,"nsion at /home/glwagner/.julia/packages/Oceananigans/cLFd3/src/Fields/computed_field.jl:86; [2] gpu__compute! at /home/glwagner/.julia/packages/KernelAbstractions/jAutM/src/macros.jl:80; [3] overdub at /home/glwagner/.julia/packages/Cassette/158rp/src/overdub.jl:0; Stacktrace:; [1] check_ir(::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDA.CUDACompilerParams}, ::LLVM.Module) at /home/glwagner/.julia/packages/GPUCompiler/uTpNx/src/validation.jl:123; [2] macro expansion at /home/glwagner/.julia/packages/GPUCompiler/uTpNx/src/driver.jl:239 [inlined]; [3] macro expansion at /home/glwagner/.julia/packages/TimerOutputs/ZmKD7/src/TimerOutput.jl:206 [inlined]; [4] codegen(::Symbol, ::GPUCompiler.CompilerJob; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, validate::Bool, only_entry::Bool) at /home/glwagner/.julia/packages/GPUCompiler/uTpNx/src/driver.jl:237; [5] compile(::Symbol, ::GPUCompiler.CompilerJob; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, validate::Bool, only_entry::Bool) at /home/glwagner/.julia/packages/GPUCompiler/uTpNx/src/driver.jl:39; [6] compile at /home/glwagner/.julia/packages/GPUCompiler/uTpNx/src/driver.jl:35 [inlined]; [7] cufunction_compile(::GPUCompiler.FunctionSpec; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:maxthreads,),Tuple{Int64}}}) at /home/glwagner/.julia/packages/CUDA/YeS8q/src/compiler/execution.jl:310; [8] check_cache(::Dict{UInt64,Any}, ::Any, ::Any, ::GPUCompiler.FunctionSpec{typeof(Cassette.overdub),Tuple{Cassette.Context{nametype(CUDACtx),KernelAbstractions.CompilerMetadata{KernelAbstractions.NDIteration.StaticSize{(1, 1, 1)},KernelAbstractions.NDIteration.DynamicCheck,Nothing,Nothing,KernelAbstractions.NDIteration.NDRange{3,KernelAbstractions.NDIteration.StaticSize{(1, 1, 1)},KernelAbstractions.NDIteration.StaticSize{(1, 1, 1)},Nothing,Nothing}},Nothing,KernelAbstractions.var""##PassType#253"",Nothing,Cassette.DisableHooks},typeof(Oceananigans.Fields.gp",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1241#issuecomment-738834053:11111,optimiz,optimize,11111,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1241#issuecomment-738834053,1,['optimiz'],['optimize']
Performance,"nt about it.; > ; > *The packages in JuliaGeo have been mostly focused on IO and has not had the bandwidth to think about how it might interface with packages for (climate/ocean/etc) models. That might be useful down the line! Yes we're both around MIT. I think we're still figuring how we want to do IO in the long-term but will definitely want some way to output NetCDF. > Hey, original author of both https://github.com/JuliaGeo/NetCDF.jl and https://github.com/meggart/ZarrNative.jl here. Regarding the state of NetCDF.jl , yes I would say I mostly stopped developing the package due to time constraints and currently shift my focus towards Zarr since this is what we are using in our current project.; > ; > My last attempt at improving the NetCDF solved many of the issues with the package JuliaGeo/NetCDF.jl#61 but was not merged because of conflicts with other bugfix PRs. However, might be source of inspiration if someone wants to do a rewrite.; > ; > Regarding write performance, I would be very interested to see examples where NetCDF.jl performs worse than e.g. python-netcdf4, since most of the time should be spent in the same NetCDF C library. I have been using the package extensively and did not experience it to be slower than comparable packages.; > ; > I you are worried about the robustness of NetCDF.jl, you should not even look at ZarrNative.jl, since it is still very young and rather a prototype.; > ; > I would be very happy to discuss the issues further, maybe in a call? Would also be interested to learn about your project which seems to be very cool. Thanks so much for working on NetCDF.jl! I didn't mean to sound ungrateful about NetCDF.jl's performance. We were just debating which package to use. With https://github.com/JuliaGeo/NetCDF.jl/issues/87 fixed, I think we'll be happy for a long time. The `compress=9` bug explains why the IO was slow. @glwagner has suggested that for a project of our scale we'd want to help and contribute to the packages we use. We de",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/145#issuecomment-476298847:1237,perform,performance,1237,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/145#issuecomment-476298847,1,['perform'],['performance']
Performance,"nt changes:. * I removed the `BackgroundField` v-velocity, which had no effect on the solution because the domain is `Flat` in y. The only effect of the background flow is through the bottom drag boundary condition.; * I reduced the domain aspect ratio to 400 x 100 because based on the visualization it seemed the domain didn't need to be so wide. This lets us increase the resolution and reduce the diffusivity, which is neat. It's a bit more turbulent now. Minor changes:. * Update style to be consistent with other examples / source code (commas, spaces, etc); * Reorganized the script to read like the other examples / tutorials (like a paper, parameters are introduced when they are used rather than at the top); * Reorganized a few other misc stuff for pedagogical reasons, like building buoyancy + coriolis together; * Note using a tuple for `ĝ` rather than `Array` means it can be used as a parameter on the GPU, so that's probably preferred. Arrays are needed only if we need to mutate elements or perform linear algebra.; * Change tilting angle to 3 degrees rather than 0.05 radians; * Don't import CUDA because the example wasn't GPU friendly anyways (if you like, we can make it GPU friendly but I don't think it should be ""partially"" GPU friendly since it just makes the code more complicated); * When I try to run the example multiple times I get `ERROR: LoadError: NetCDF error: Permission denied (NetCDF error code: 13)`. How can we avoid this error? I think it's important that users can easily change parameters and re-run without having to manually delete a file; this is key to productivity; * Plotting fewer contours makes the animation faster (the most expensive part of this example); * We don't need to form `Field`s any more for the output writers any more. Idle thoughts:. * This is a great inexpensive example. I do wonder if we should make it 3D with an LES closure?; * Can we tilt the visualization? It's disorienting to be looking at the flow at an angle of 3 degrees.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2333#issuecomment-1065127449:1563,Load,LoadError,1563,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2333#issuecomment-1065127449,1,['Load'],['LoadError']
Performance,"nt64}, Base.OneTo{Int64}}, typeof(identity), Tuple{Base.Broadcast.Extruded{SubArray{Float64, 3, CUDA.CuDeviceArray{Float64, 3, 1}, Tuple{UnitRange{Int64}, Base.Slice{Base.OneTo{Int64}}, Base.Slice{Base.OneTo{Int64}}}, false}, Tuple{Bool, Bool, Bool}, Tuple{Int64, Int64, Int64}}}}, Int64}}}, args::LLVM.Module); @ GPUCompiler ~/.julia/packages/GPUCompiler/e9hrk/src/validation.jl:111; [2] macro expansion; @ ~/.julia/packages/GPUCompiler/e9hrk/src/driver.jl:319 [inlined]; [3] macro expansion; @ ~/.julia/packages/TimerOutputs/ZQ0rt/src/TimerOutput.jl:236 [inlined]; [4] macro expansion; @ ~/.julia/packages/GPUCompiler/e9hrk/src/driver.jl:317 [inlined]; [5] emit_asm(job::GPUCompiler.CompilerJob, ir::LLVM.Module; strip::Bool, validate::Bool, format::LLVM.API.LLVMCodeGenFileType); @ GPUCompiler ~/.julia/packages/GPUCompiler/e9hrk/src/utils.jl:62; [6] cufunction_compile(job::GPUCompiler.CompilerJob); @ CUDA ~/.julia/packages/CUDA/lwSps/src/compiler/execution.jl:317; [7] cached_compilation(cache::Dict{UInt64, Any}, job::GPUCompiler.CompilerJob, compiler::typeof(CUDA.cufunction_compile), linker::typeof(CUDA.cufunction_link)); @ GPUCompiler ~/.julia/packages/GPUCompiler/e9hrk/src/cache.jl:89; [8] cufunction(f::GPUArrays.var""#broadcast_kernel#16"", tt::Type{Tuple{CUDA.CuKernelContext, SubArray{Float64, 3, CUDA.CuDeviceArray{Float64, 3, 1}, Tuple{UnitRange{Int64}, Base.Slice{Base.OneTo{Int64}}, Base.Slice{Base.OneTo{Int64}}}, false}, Base.Broadcast.Broadcasted{Nothing, Tuple{Base.OneTo{Int64}, Base.OneTo{Int64}, Base.OneTo{Int64}}, typeof(identity), Tuple{Base.Broadcast.Extruded{SubArray{Float64, 3, CUDA.CuDeviceArray{Float64, 3, 1}, Tuple{UnitRange{Int64}, Base.Slice{Base.OneTo{Int64}}, Base.Slice{Base.OneTo{Int64}}}, false}, Tuple{Bool, Bool, Bool}, Tuple{Int64, Int64, Int64}}}}, Int64}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{}, Union{}, Tuple{}, NamedTuple{(), Tuple{}}}); @ CUDA ~/.julia/packages/CUDA/lwSps/src/compiler/execution.jl:288; [9] cufunction; @ ~/.julia/pa",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1995:3364,cache,cache,3364,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1995,1,['cache'],['cache']
Performance,nterpreter.c:174 [inlined]; eval_body at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:617; jl_interpret_toplevel_thunk at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:775; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:934; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:877; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:877; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:877; ijl_toplevel_eval_in at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:985; eval at ./boot.jl:385 [inlined]; eval_user_input at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:150; repl_backend_loop at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:246; #start_repl_backend#46 at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:231; start_repl_backend at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:228; _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; #run_repl#59 at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:389; run_repl at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:375; jfptr_run_repl_91805.1 at /home/alir/.julia/juliaup/julia-1.10.5+0.x64.linux.gnu/lib/julia/sys.so (unknown line); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3852:6540,cache,cache,6540,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3852,1,['cache'],['cache']
Performance,"ntity is at (Face, Face, Cell). ```julia; using Oceananigans; using Oceananigans: Face, Cell; using Oceananigans.AbstractOperations. grid = RegularCartesianGrid((16, 16, 16), (16, 16, 16));. v = Field(Cell, Face, Cell, CPU(), grid);; u = Field(Face, Cell, Cell, CPU(), grid);. ζ = ∂x(v) - ∂y(u); BinaryOperation at (Face, Face, Cell); ├── grid: RegularCartesianGrid{Float64,StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}}; │ ├── size: (16, 16, 16); │ └── domain: x ∈ [0.0, 16.0], y ∈ [0.0, 16.0], z ∈ [0.0, -16.0]; └── tree: . - at (Face, Face, Cell) via Oceananigans.AbstractOperations.identity; ├── Oceananigans.TurbulenceClosures.∂x_faa at (Face, Face, Cell) via Oceananigans.AbstractOperations.identity; │   └── OffsetArrays.OffsetArray{Float64,3,Array{Float64,3}}; └── Oceananigans.TurbulenceClosures.∂y_afa at (Face, Face, Cell) via Oceananigans.AbstractOperations.identity;    └── OffsetArrays.OffsetArray{Float64,3,Array{Float64,3}}; ```; The locations after performing the operations seem to work out and I love the syntax!. ```julia; ζ_at_u = @at (Face, Cell, Cell) ∂x(v) - ∂y(u); BinaryOperation at (Face, Cell, Cell); ├── grid: RegularCartesianGrid{Float64,StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}}; │ ├── size: (16, 16, 16); │ └── domain: x ∈ [0.0, 16.0], y ∈ [0.0, 16.0], z ∈ [0.0, -16.0]; └── tree: . - at (Face, Cell, Cell) via Oceananigans.AbstractOperations.identity; ├── Oceananigans.TurbulenceClosures.∂x_faa at (Face, Cell, Cell) via Oceananigans.TurbulenceClosures.▶y_aca; │   └── OffsetArrays.OffsetArray{Float64,3,Array{Float64,3}}; └── Oceananigans.TurbulenceClosures.∂y_afa at (Face, Cell, Cell) via Oceananigans.TurbulenceClosures.▶y_aca;    └── OffsetArrays.OffsetArray{Float64,3,Array{Float64,3}}; ```; Now we set the values and check if they meet expectations. ```julia; set!(u, rand(size(grid)...)); set!(v, rand(size(grid)...)). ζ[8,8,8]; -0.2552259927949605. ζ_at_u[8,8,8]; 0.08552079812773183. ",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/463#issuecomment-543253912:1132,perform,performing,1132,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/463#issuecomment-543253912,1,['perform'],['performing']
Performance,"nuousForcing{Oceananigans.Grids.Face,Oceananigans.Grids.Cell,Oceananigans.Grids.Cell,Nothing,0,typeof(Oceananigans.Forcings.zeroforcing),Tuple{}},Oceananigans.Forcings.ContinuousForcing{Oceananigans.Grids.Cell,Oceananigans.Grids.Face,Oceananigans.Grids.Cell,Nothing,0,typeof(Oceananigans.Forcings.zeroforcing),Tuple{}},Oceananigans.Forcings.ContinuousForcing{Oceananigans.Grids.Cell,Oceananigans.Grids.Cell,Oceananigans.Grids.Face,Nothing,0,typeof(Oceananigans.Forcings.zeroforcing),Tuple{}},Oceananigans.Forcings.ContinuousForcing{Oceananigans.Grids.Cell,Oceananigans.Grids.Cell,Oceananigans.Grids.Cell,NamedTuple{(:K, :ℓ, :Δz),Tuple{Float64,Float64,Float64}},1,typeof(FT),Tuple{typeof(identity)}}}},OffsetArrays.OffsetArray{Float64,3,CUDA.CuDeviceArray{Float64,3,CUDA.AS.Global}},NamedTuple{(:time, :iteration, :stage),Tuple{Float64,Int64,Int64}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:maxthreads,),Tuple{Int64}}}) at /home/ptuckman/.julia/packages/GPUCompiler/4e9CU/src/cache.jl:0; [18] cufunction(::typeof(Cassette.overdub), ::Type{Tuple{Cassette.Context{nametype(CUDACtx),KernelAbstractions.CompilerMetadata{KernelAbstractions.NDIteration.StaticSize{(128, 128, 128)},KernelAbstractions.NDIteration.DynamicCheck,Nothing,Nothing,KernelAbstractions.NDIteration.NDRange{3,KernelAbstractions.NDIteration.StaticSize{(8, 8, 128)},KernelAbstractions.NDIteration.StaticSize{(16, 16, 1)},Nothing,Nothing}},Nothing,KernelAbstractions.var""##PassType#253"",Nothing,Cassette.DisableHooks},typeof(Oceananigans.TimeSteppers.gpu_calculate_Gu!),OffsetArrays.OffsetArray{Float64,3,CUDA.CuDeviceArray{Float64,3,CUDA.AS.Global}},RegularCartesianGrid{Float64,Periodic,Periodic,Bounded,OffsetArrays.OffsetArray{Float64,1,StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}}},Oceananigans.Advection.CenteredSecondOrder,NonTraditionalFPlane{Float64},Nothing,AnisotropicDiffusivity{Float64,Float64,Float64,NamedTuple{(:T,),Tuple{Float64}},NamedTuple{(:",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1010:15540,cache,cache,15540,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1010,1,['cache'],['cache']
Performance,"nuousForcing{Oceananigans.Grids.Face,Oceananigans.Grids.Cell,Oceananigans.Grids.Cell,Nothing,0,typeof(Oceananigans.Forcings.zeroforcing),Tuple{}},Oceananigans.Forcings.ContinuousForcing{Oceananigans.Grids.Cell,Oceananigans.Grids.Face,Oceananigans.Grids.Cell,Nothing,0,typeof(Oceananigans.Forcings.zeroforcing),Tuple{}},Oceananigans.Forcings.ContinuousForcing{Oceananigans.Grids.Cell,Oceananigans.Grids.Cell,Oceananigans.Grids.Face,Nothing,0,typeof(Oceananigans.Forcings.zeroforcing),Tuple{}},Oceananigans.Forcings.ContinuousForcing{Oceananigans.Grids.Cell,Oceananigans.Grids.Cell,Oceananigans.Grids.Cell,NamedTuple{(:K, :ℓ, :Δz),Tuple{Float64,Float64,Float64}},1,typeof(FT),Tuple{typeof(identity)}}}},OffsetArrays.OffsetArray{Float64,3,CUDA.CuDeviceArray{Float64,3,CUDA.AS.Global}},NamedTuple{(:time, :iteration, :stage),Tuple{Float64,Int64,Int64}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:maxthreads,),Tuple{Int64}}}) at /home/ptuckman/.julia/packages/GPUCompiler/4e9CU/src/cache.jl:19; [12] + at ./int.jl:53 [inlined]; [13] hash_64_64 at ./hashing.jl:35 [inlined]; [14] hash_uint64 at ./hashing.jl:62 [inlined]; [15] hx at ./float.jl:568 [inlined]; [16] hash at ./float.jl:571 [inlined]; [17] cached_compilation(::typeof(CUDA._cufunction), ::GPUCompiler.FunctionSpec{typeof(Cassette.overdub),Tuple{Cassette.Context{nametype(CUDACtx),KernelAbstractions.CompilerMetadata{KernelAbstractions.NDIteration.StaticSize{(128, 128, 128)},KernelAbstractions.NDIteration.DynamicCheck,Nothing,Nothing,KernelAbstractions.NDIteration.NDRange{3,KernelAbstractions.NDIteration.StaticSize{(8, 8, 128)},KernelAbstractions.NDIteration.StaticSize{(16, 16, 1)},Nothing,Nothing}},Nothing,KernelAbstractions.var""##PassType#253"",Nothing,Cassette.DisableHooks},typeof(Oceananigans.TimeSteppers.gpu_calculate_Gu!),OffsetArrays.OffsetArray{Float64,3,CUDA.CuDeviceArray{Float64,3,CUDA.AS.Global}},RegularCartesianGrid{Float64,Periodic,Periodic,Bounded,OffsetArrays.OffsetArray{Float64,1,",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1010:12779,cache,cache,12779,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1010,1,['cache'],['cache']
Performance,"n{Float64, Base.TwicePrecisi...; 763 0 @Oceananigans/src/Advection/weno_fifth_order.jl 211 left_biased_interpolate_xᶠᵃᵃ(::Int64, ::Int64, ::Int64, ::RegularRectilinearGrid{Float64, Periodic, Periodic, Bounded, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecisi...; 768 0 @Oceananigans/src/Advection/topologically_conditional_interpolation.jl 36 _right_biased_interpolate_xᶠᵃᵃ(::Int64, ::Int64, ::Int64, ::RegularRectilinearGrid{Float64, Periodic, Periodic, Bounded, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePreci...; 768 0 @Oceananigans/src/Advection/upwind_biased_advective_fluxes.jl 49 overdub; 781 0 @Oceananigans/src/Simulations/run.jl 127 run!(sim::Simulation{NonhydrostaticModel{Oceananigans.TimeSteppers.QuasiAdamsBashforth2TimeStepper{Float64, NamedTuple{(:u, :v, :w, :b), Tuple{Field{Face, Center, Center, CPU, OffsetArrays.Offs...; 781 0 @Base/boot.jl 360 eval; 781 0 @Base/loading.jl 1116 include_string(mapexpr::typeof(identity), mod::Module, code::String, filename::String); 781 0 @Base/loading.jl 1170 _include(mapexpr::Function, mod::Module, _path::String); 781 0 @Base/Base.jl 386 include(mod::Module, _path::String); 781 0 @Base/client.jl 285 exec_options(opts::Base.JLOptions); 781 0 @Base/client.jl 485 _start(); 796 796 @Cassette/src/context.jl ? overdub; 821 0 @Oceananigans/src/Advection/topologically_conditional_interpolation.jl 36 _right_biased_interpolate_yᵃᶠᵃ(::Int64, ::Int64, ::Int64, ::RegularRectilinearGrid{Float64, Periodic, Periodic, Bounded, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePreci...; 821 0 @Oceananigans/src/Advection/upwind_biased_advective_fluxes.jl 85 overdub; 860 860 @KernelAbstractions/src/compiler/contract.jl 18 sub_float_contract; 860 0 @KernelAbstractions/src/compiler.jl 46 overdub; 873 0 @Oceananigans/src/Advection/weno_fifth_order.jl 148 overdub; 879 0 @Oceananigans/src/Operators/difference_operators.jl 23 δyᵃᶜᵃ(::Int64, ::Int64, ::Int64, ::RegularRectilinearGrid{",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1912#issuecomment-892297846:31257,load,loading,31257,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1912#issuecomment-892297846,1,['load'],['loading']
Performance,"o @maleadt on the Julia slack's GPU channel and in regards to the shallow water model profiles:. > Don't focus on time spent in API calls to much. since GPU execution is asynchronous, you'll have to synchronize at some point, and that API call will then 'soak up' time until the stream has finished executing. and here that's literally the synchronize function, which is implemented using cuStreamQuery: https://github.com/JuliaGPU/CUDA.jl/blob/2b3ec03ff9774b65541fc88dd6b0f1f7aea5d9e0/lib/cudadrv/stream.jl#L115-L144. >use a timeline profiler (i.e. NSNight Systems) to profile your app, or nvpp if you really want to use the old profiler toolchain. plain nvprof results are too simple once your application hits some level of complexity. >now, it is possible that our CPU-side implementation of synchronize does too many API calls and could be optimized a little, but in the end the call serves to wait until the GPU has finished so it probably doesn't matter much. if it does, e.g. because you want to perform other useful work on another CPU task concurrently, you could try to profile that in isolation and file an issue. Essentially, Tim explains that `cuStreamQuery` takes up more time as the grid size increases because it's called in the synchronize function. The synchronize function as shown in the link above tends to be called more and soaks up more waiting time the bigger the problem hence why it scales positively to grid size. ; Taking a closer look at the shallow water gpu profiling results above, it seems that `cuStreamQuery` takes up a lot of time in the finer resolution runs because it is called many times and not because each call takes a lot of time. For example, in the 16k case, `cuSteamQuery` is called three order of magnitudes more times than `cuLaunchKernel` while both calls are measured in microseconds. ; I'm not sure if `cuStreamQuery` being called 400,000 times is an error with our code, an error with CUDA.jl, not an error at all, or an error with my profiling.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1912#issuecomment-899945654:1015,perform,perform,1015,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1912#issuecomment-899945654,2,"['concurren', 'perform']","['concurrently', 'perform']"
Performance,"o v0.3.4; [ade2ca70] Dates; [b77e0a4c] InteractiveUtils; [37e2e46d] LinearAlgebra; [56ddb016] Logging; [44cfe95a] Pkg; [de0858da] Printf; [9a3f8284] Random; [2f01184e] SparseArrays; [10745b16] Statistics. (Oceananigans) pkg> precompile; Precompiling project...; ✗ Oceananigans; 0 dependencies successfully precompiled in 11 seconds (99 already precompiled). ERROR: The following 1 direct dependency failed to precompile:. Oceananigans [9e8cae18-63c1-5223-a75c-80ca9d6e9a09]. Failed to precompile Oceananigans [9e8cae18-63c1-5223-a75c-80ca9d6e9a09] to /Users/navid/.julia/compiled/v1.6/Oceananigans/jl_Z5b4Xf.; ERROR: LoadError: LoadError: LoadError: InitError: UndefVarError: libamgxsh not defined; Stacktrace:; [1] getproperty; @ ./Base.jl:26 [inlined]; [2] __init__(); @ AMGX ~/.julia/packages/AMGX/GFHHN/src/AMGX.jl:30; [3] _include_from_serialized(path::String, depmods::Vector{Any}); @ Base ./loading.jl:696; [4] _require_search_from_serialized(pkg::Base.PkgId, sourcepath::String); @ Base ./loading.jl:782; [5] _require(pkg::Base.PkgId); @ Base ./loading.jl:1020; [6] require(uuidkey::Base.PkgId); @ Base ./loading.jl:936; [7] require(into::Module, mod::Symbol); @ Base ./loading.jl:923; [8] include(mod::Module, _path::String); @ Base ./Base.jl:384; [9] include(x::String); @ Oceananigans.Solvers ~/Research/OC.jl/src/Solvers/Solvers.jl:1; [10] top-level scope; @ ~/Research/OC.jl/src/Solvers/Solvers.jl:48; [11] include(mod::Module, _path::String); @ Base ./Base.jl:384; [12] include(x::String); @ Oceananigans ~/Research/OC.jl/src/Oceananigans.jl:5; [13] top-level scope; @ ~/Research/OC.jl/src/Oceananigans.jl:195; [14] include; @ ./Base.jl:384 [inlined]; [15] include_package_for_output(pkg::Base.PkgId, input::String, depot_path::Vector{String}, dl_load_path::Vector{String}, load_path::Vector{String}, concrete_deps::Vector{Pair{Base.PkgId, UInt64}}, source::Nothing); @ Base ./loading.jl:1235; [16] top-level scope; @ none:1; [17] eval; @ ./boot.jl:360 [inlined]; [18] eval(x::Expr); @ B",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2688#issuecomment-1217694987:2252,load,loading,2252,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2688#issuecomment-1217694987,1,['load'],['loading']
Performance,"oading.jl:1974; │ [5] mkpidlock(f::Base.var""#968#969""{Base.PkgId}, at::String, pid::Int32; kwopts::@Kwargs{stale_age::Int64, wait::Bool}); │ @ FileWatching.Pidfile ~/julia-1.10/usr/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:93; │ [6] #mkpidlock#6; │ @ ~/julia-1.10/usr/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:88 [inlined]; │ [7] trymkpidlock(::Function, ::Vararg{Any}; kwargs::@Kwargs{stale_age::Int64}); │ @ FileWatching.Pidfile ~/julia-1.10/usr/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:111; │ [8] #invokelatest#2; │ @ ./essentials.jl:894 [inlined]; │ [9] invokelatest; │ @ ./essentials.jl:889 [inlined]; │ [10] maybe_cachefile_lock(f::Base.var""#968#969""{Base.PkgId}, pkg::Base.PkgId, srcpath::String; stale_age::Int64); │ @ Base ./loading.jl:2983; │ [11] maybe_cachefile_lock; │ @ ./loading.jl:2980 [inlined]; │ [12] _require(pkg::Base.PkgId, env::Nothing); │ @ Base ./loading.jl:1970; │ [13] __require_prelocked(uuidkey::Base.PkgId, env::Nothing); │ @ Base ./loading.jl:1812; │ [14] #invoke_in_world#3; │ @ ./essentials.jl:926 [inlined]; │ [15] invoke_in_world; │ @ ./essentials.jl:923 [inlined]; │ [16] _require_prelocked; │ @ ./loading.jl:1803 [inlined]; │ [17] _require_prelocked; │ @ ./loading.jl:1802 [inlined]; │ [18] run_extension_callbacks(extid::Base.ExtensionId); │ @ Base ./loading.jl:1295; │ [19] run_extension_callbacks(pkgid::Base.PkgId); │ @ Base ./loading.jl:1330; │ [20] run_package_callbacks(modkey::Base.PkgId); │ @ Base ./loading.jl:1164; │ [21] _tryrequire_from_serialized(modkey::Base.PkgId, path::String, ocachepath::String, sourcepath::String, depmods::Vector{Any}); │ @ Base ./loading.jl:1487; │ [22] _require_search_from_serialized(pkg::Base.PkgId, sourcepath::String, build_id::UInt128); │ @ Base ./loading.jl:1574; │ [23] _require(pkg::Base.PkgId, env::String); │ @ Base ./loading.jl:1938; │ [24] __require_prelocked(uuidkey::Base.PkgId, env::String); │ @ Base ./loading.jl:1812; │ [25] #invoke_in_world#3; │ @ ./essentials.jl:926 [inl",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3686#issuecomment-2272432528:2415,load,loading,2415,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3686#issuecomment-2272432528,1,['load'],['loading']
Performance,"oat64,3,CUDA.AS.Global}}}}) resulted in invalid LLVM IR ; Reason: unsupported call to the Julia runtime (call to jl_f_tuple); Stacktrace:; [1] overdub at /home/alir/.julia/packages/Cassette/158rp/src/overdub.jl:586; [2] multiple call sites at unknown:0; Reason: unsupported call to the Julia runtime (call to jl_f_getfield); Stacktrace:; [1] overdub at /home/alir/.julia/packages/Cassette/158rp/src/overdub.jl:586; [2] multiple call sites at unknown:0; Stacktrace:; [1] check_ir(::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDA.CUDACompilerParams}, ::LLVM.Module) at /home/alir/.julia/packages/GPUCompiler/4e9CU/src/validation.jl:123; [2] macro expansion at /home/alir/.julia/packages/GPUCompiler/4e9CU/src/driver.jl:241 [inlined]; [3] macro expansion at /home/alir/.julia/packages/TimerOutputs/dVnaw/src/TimerOutput.jl:206 [inlined]; [4] codegen(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDA.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, validate::Bool, only_entry::Bool) at /home/alir/.julia/packages/GPUCompiler/4e9CU/src/driver.jl:239; [5] compile(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDA.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, validate::Bool, only_entry::Bool) at /home/alir/.julia/packages/GPUCompiler/4e9CU/src/driver.jl:39; [6] compile at /home/alir/.julia/packages/GPUCompiler/4e9CU/src/driver.jl:35 [inlined]; [7] _cufunction(::GPUCompiler.FunctionSpec{typeof(Cassette.overdub),Tuple{Cassette.Context{nametype(CUDACtx),KernelAbstractions.CompilerMetadata{KernelAbstractions.NDIteration.StaticSize{(16, 16, 16)},KernelAbstractions.NDIteration.DynamicCheck,Nothing,Nothing,KernelAbstractions.NDIteration.NDRange{3,KernelAbstractions.NDIteration.StaticSize{(1, 1, 16)},KernelAbstractions.NDIteration.StaticSize{(16, 16, 1)},Nothing,Nothing}},Nothing,KernelAbstractions.var""##PassType#253"",Nothing,Cassette.DisableHooks},typeof(Oc",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/828:5144,optimiz,optimize,5144,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/828,1,['optimiz'],['optimize']
Performance,"odic, Bounded) │ 51.2039 │ 15.8992 │ 23.2963 │; │ 256 │ (Periodic, Bounded, Bounded) │ 52.4472 │ 15.8992 │ 23.2963 │; │ 256 │ (Periodic, Periodic, Bounded) │ 99.4371 │ 6.48062 │ 10.7407 │; └─────┴───────────────────────────────┴─────────┴─────────┴─────────┘; ```. ### Relative performance on the CPU. ```; Fourier-tridiagonal Poisson solver relative performance (CPU); ┌───────────────┬─────┬───────────────────────────────┬──────────┬──────────┬────────┐; │ Architectures │ Ns │ Topologies │ slowdown │ memory │ allocs │; ├───────────────┼─────┼───────────────────────────────┼──────────┼──────────┼────────┤; │ CPU │ 256 │ (Bounded, Bounded, Bounded) │ 1.58185 │ 1.0 │ 1.0 │; │ CPU │ 256 │ (Bounded, Periodic, Bounded) │ 1.24529 │ 0.922481 │ 1.0 │; │ CPU │ 256 │ (Periodic, Bounded, Bounded) │ 1.27117 │ 0.922481 │ 1.0 │; │ CPU │ 256 │ (Periodic, Periodic, Bounded) │ 1.0 │ 1.0 │ 1.0 │; └───────────────┴─────┴───────────────────────────────┴──────────┴──────────┴────────┘; ```. ### Relative performance on the GPU. ```; Fourier-tridiagonal Poisson solver relative performance (GPU); ┌───────────────┬─────┬───────────────────────────────┬──────────┬─────────┬─────────┐; │ Architectures │ Ns │ Topologies │ slowdown │ memory │ allocs │; ├───────────────┼─────┼───────────────────────────────┼──────────┼─────────┼─────────┤; │ GPU │ 256 │ (Bounded, Bounded, Bounded) │ 3.12065 │ 3.32057 │ 3.02069 │; │ GPU │ 256 │ (Bounded, Periodic, Bounded) │ 2.41833 │ 2.26316 │ 2.16897 │; │ GPU │ 256 │ (Periodic, Bounded, Bounded) │ 2.41007 │ 2.26316 │ 2.16897 │; │ GPU │ 256 │ (Periodic, Periodic, Bounded) │ 1.0 │ 1.0 │ 1.0 │; └───────────────┴─────┴───────────────────────────────┴──────────┴─────────┴─────────┘; ```. ---. ## FFT-based Poisson solver. ### Raw benchmarks. ```; FFT-based Poisson solver benchmarks ; ┌───────────────┬─────┬───────────────────────────────┬────────────┬────────────┬────────────┬────────────┬───────────┬────────┐; │ Architectures │ Ns │ Topologies │ min │ median │ mean │ m",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1403#issuecomment-786398050:8950,perform,performance,8950,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1403#issuecomment-786398050,1,['perform'],['performance']
Performance,"odic,Bounded,OffsetArrays.OffsetArray{Float64,1,StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}}}},typeof(identity),typeof(identity),typeof(identity),RegularCartesianGrid{Float64,Periodic,Periodic,Bounded,OffsetArrays.OffsetArray{Float64,1,StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}}}}},Tuple{typeof(identity),typeof(identity),typeof(identity),typeof(Oceananigans.Operators.ℑxyᶜᶜᵃ)},RegularCartesianGrid{Float64,Periodic,Periodic,Bounded,OffsetArrays.OffsetArray{Float64,1,StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}}}},typeof(identity),typeof(identity),typeof(identity),RegularCartesianGrid{Float64,Periodic,Periodic,Bounded,OffsetArrays.OffsetArray{Float64,1,StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:maxthreads,),Tuple{Int64}}}) at /home/glwagner/.julia/packages/GPUCompiler/uTpNx/src/cache.jl:40; [9] gpu__compute! at ./array.jl:0 [inlined]; [10] cufunction(::typeof(Cassette.overdub), ::Type{Tuple{Cassette.Context{nametype(CUDACtx),KernelAbstractions.CompilerMetadata{KernelAbstractions.NDIteration.StaticSize{(1, 1, 1)},KernelAbstractions.NDIteration.DynamicCheck,Nothing,Nothing,KernelAbstractions.NDIteration.NDRange{3,KernelAbstractions.NDIteration.StaticSize{(1, 1, 1)},KernelAbstractions.NDIteration.StaticSize{(1, 1, 1)},Nothing,Nothing}},Nothing,KernelAbstractions.var""##PassType#253"",Nothing,Cassette.DisableHooks},typeof(Oceananigans.Fields.gpu__compute!),OffsetArrays.OffsetArray{Float64,3,CUDA.CuDeviceArray{Float64,3,1}},Oceananigans.AbstractOperations.BinaryOperation{Cell,Cell,Cell,typeof(*),Float64,Oceananigans.AbstractOperations.MultiaryOperation{Cell,Cell,Cell,4,typeof(+),Tuple{Oceananigans.AbstractOperations.BinaryOperation{Cell,Cell,Cell,typeof(^),Oceananigans.AbstractOperations.Derivative{Cell,Cell,Cell,typeof(Oceananigans.Operators.∂xᶜᵃᵃ),O",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1241#issuecomment-738834053:20486,cache,cache,20486,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1241#issuecomment-738834053,1,['cache'],['cache']
Performance,"oing wrong. I'm going to copy the errors below so others can see this more easily. The error in the docs complains about `PlotUtils` failing to precompile. That doesn't seem related to shallow water so I am confused. The CPU test seems to be with `MPI`, but I didn't know we had any `MPI` tests that used shallow water that were being run. Docs:; ```; ERROR: could not load library ""/storage7/buildkite-agent/julia-1.5.4/lib/julia/sys.so""; --; &nbsp; | /storage7/buildkite-agent/julia-1.5.4/lib/julia/sys.so: ELF load command past end of file; &nbsp; | ERROR: LoadError: Failed to precompile PlotUtils [995b91a9-d308-5afd-9ec6-746e21dbc043] to /storage7/buildkite-agent/.julia-2556/compiled/v1.5/PlotUtils/YveHG_R3lk8.ji.; &nbsp; | Stacktrace:; &nbsp; | [1] top-level scope at none:2; &nbsp; | [2] eval at ./boot.jl:347 [inlined]; &nbsp; | in expression starting at /storage7/buildkite-agent/.julia-2556/packages/Plots/SjqWU/src/Plots.jl:20; &nbsp; | ERROR: LoadError: Failed to precompile Plots [91a5bcdd-55d7-5caf-9e0b-520d859cae80] to /storage7/buildkite-agent/.julia-2556/compiled/v1.5/Plots/ld3vC_R3lk8.ji.; &nbsp; | in expression starting at /storage7/buildkite-agent/builds/tartarus-mit-edu-1/clima/oceananigans/docs/make.jl:6; &nbsp; | 🚨 Error: The command exited with status 1. ```. CPU test; ```; [8] test() at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Pkg/src/API.jl:72; --; &nbsp; | [9] top-level scope at none:1; &nbsp; | Union{},Union{},Tuple{},NamedTuple{(test(::Pkg.Types.Context, ::Array{Pkg.Types.PackageSpec,1}; ERROR: failed process: Process(`/storage7/buildkite-agent/.julia-2556/artifacts/2fcd463fb9498f362be9d1c4ef70a63c920b0e96/bin/mpiexec -np 4 /storage7/buildkite-agent/julia-1.5.4/bin/julia -O0 --color=yes -e 'using Pkg; Pkg.test()'`, ProcessExited(1)) [1]; &nbsp; | &nbsp;; &nbsp; | Stacktrace:; &nbsp; | [1] pipeline_error at ./process.jl:525 [inlined]; &nbsp; | [2] run(::Cmd; wait::Bool) at ./process.jl:440; &nbsp; | [3] run(::Cmd) at proc",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1662#issuecomment-842643141:1049,Load,LoadError,1049,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1662#issuecomment-842643141,1,['Load'],['LoadError']
Performance,"ol, precision::Type, boundary_layer_parameterization::Oceananigans.TurbulenceClosures.RiBasedVerticalDiffusivity{Oceananigans.TurbulenceClosures.VerticallyImplicitTimeDiscretization, Float64, Oceananigans.TurbulenceClosures.HyperbolicTangentRiDependentTapering}); @ OceanScalingTests ~/src/OceanScalingTests.jl/src/near_global_simulation.jl:64; [10] macro expansion; @ ~/src/OceanScalingTests.jl/src/OceanScalingTests.jl:54 [inlined]; [11] macro expansion; @ ~/.julia/packages/PrecompileTools/L8A3n/src/workloads.jl:78 [inlined]; [12] macro expansion; @ ~/src/OceanScalingTests.jl/src/OceanScalingTests.jl:53 [inlined]; [13] macro expansion; @ ~/.julia/packages/PrecompileTools/L8A3n/src/workloads.jl:140 [inlined]; [14] top-level scope; @ ~/src/OceanScalingTests.jl/src/OceanScalingTests.jl:32; [15] include; @ ./Base.jl:556 [inlined]; [16] include_package_for_output(pkg::Base.PkgId, input::String, depot_path::Vector{String}, dl_load_path::Vector{String}, load_path::Vector{String}, concrete_deps::Vector{Pair{Base.PkgId, UInt128}}, source::Nothing); @ Base ./loading.jl:2664; [17] top-level scope; @ stdin:4; in expression starting at /home/vchuravy/src/OceanScalingTests.jl/src/OceanScalingTests.jl:1; in expression starting at stdin:; ```. Caused by `@eval`. Note that `@eval` uses the current module and not the module the user is calling this function from.; This means we are trying to modify the Oceananigans after it has already been closed. This is imcompatible with precompilation since we are unable to track and restore this modification. My intuition is that you probably just want a dictionary for these kind of globals, maybe even within the model? ; Instead of using global variables. . https://github.com/CliMA/Oceananigans.jl/blob/00f028bb37f13692e24921588aeb8a9150f6dd55/src/Grids/latitude_longitude_grid.jl#L554-L555. The use-case is shown in https://github.com/simone-silvestri/OceanScalingTests.jl/pull/8 where one wants to use `PrecompileTools` to cache important functions.",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3555:6553,load,loading,6553,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3555,2,"['cache', 'load']","['cache', 'loading']"
Performance,"onAll,1}:; Oceananigans.AbstractOperations.BinaryOperation; Oceananigans.AbstractOperations.BinaryOperation; Oceananigans.AbstractOperations.BinaryOperation; ```. we find that it consists of a sum of three `BinaryOperations`. Finally, we see that each `BinaryOperation`,. ```; julia> names = [(a.op, typeof(a.a).name.wrapper, a.b) for a in kinetic_energy.a.args]; 3-element Array{Tuple{typeof(^),UnionAll,Int64},1}:; (^, OffsetArrays.OffsetArray, 2); (^, OffsetArrays.OffsetArray, 2); (^, OffsetArrays.OffsetArray, 2); ```. involves taking an `OffsetArray` (each of which holds the underlying data in `u, v, w`) to the power 2. Still, we index into it in the same way we index into other fields to obtain its data:. ```julia; julia> noise(x, y, z) = rand(). julia> [set!(ϕ, noise) for ϕ in (u, v, w)];. julia> kinetic_energy[8, 8, 8]; 0.789860912635921; ```. # Special considerations. There are a few special rules to how operations are handled:. * A `BinaryOperation` between two fields at the same location is always performed at their common location;. * a `BinaryOperation` between a field and a number always takes place at the location of the field. These special rules override the specification of operator location via `@at`. Thus, for example, in the operation. ```julia; uv = @at (Cell, Cell, Cell) u * v + v^2; ```. the product `u*v` is computed at the cell center, while `v^2` is computed at the `v`-point `Cell, Face, Cell`, and afterwards interpolated to cell centers. This functionality is achieved by endowing `BinaryOperation` with three interpolation operators: two interpolation operators applied to each field prior to interpolation, and an interpolation operator that is applied to the result. Special cases can then be handled by defining operators for cases in which a computation should be performed at the location of one or both of the fields. Furthermore, we do not provide `PolynaryOperation`s with default locations. In other words, a `PolynaryOperation` only arises when",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/463:3962,perform,performed,3962,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/463,1,['perform'],['performed']
Performance,"onditions{BoundaryCondition{Oceananigans.BoundaryConditions.Periodic, Nothing}, BoundaryCondition{Oceananigans.BoundaryConditions.Periodic, Nothing}, BoundaryCondition{Oceananigans.BoundaryConditions.Periodic, Nothing}, BoundaryCondition{Oceananigans.BoundaryConditions.Periodic, Nothing}, Nothing, Nothing, BoundaryCondition{Oceananigans.BoundaryConditions.Flux, Nothing}}, Nothing}, Field{Center, Center, Center, Nothing, RectilinearGrid{Float64, Periodic, Periodic, Bounded, Float64, Float64, OffsetArrays.OffsetVector{Float64, Vector{Float64}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVector{Float64, Vector{Float64}}, CPU}, Tuple{Colon, Colon, Colon}, OffsetArrays.OffsetArray{Float64, 3, Array{Float64, 3}}, Float64, FieldBoundaryConditions{BoundaryCondition{Oceananigans.BoundaryConditions.Periodic, Nothing}, BoundaryCondition{Oceananigans.BoundaryConditions.Periodic, Nothing}, BoundaryCondition{Oceananigans.BoundaryConditions.Periodic, Nothing}, BoundaryCondition{Oceananigans.BoundaryConditions.Periodic, Nothing}, BoundaryCondition{Oceananigans.BoundaryConditions.Flux, Nothing}, BoundaryCondition{Oceananigans.BoundaryConditions.Flux, Nothing}, BoundaryCondition{Oceananigans.BoundaryConditions.Flux, Nothing}}, Nothing}}}, Nothing}, Float64, Float64, OrderedCollections.OrderedDict{Symbol, Oceananigans.AbstractDiagnostic}, OrderedCollections.OrderedDict{Symbol, Oceananigans.AbstractOutputWriter}, OrderedCollections.OrderedDict{Symbol, Callback}}); @ Oceananigans.Simulations ~/.julia/packages/Oceananigans/Yz6ub/src/Simulations/run.jl:88; [10] top-level scope; @ In[3]:54; [11] eval; @ ./boot.jl:373 [inlined]; [12] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String); @ Base ./loading.jl:1196. ```; </details>",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2395#issuecomment-1084429618:118734,load,loading,118734,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2395#issuecomment-1084429618,1,['load'],['loading']
Performance,"onhydrostatic_tendency_kernel_functions.jl:92; [4] macro expansion; @ ~/.julia/packages/Oceananigans/xmqSH/src/Models/NonhydrostaticModels/compute_nonhydrostatic_tendencies.jl:148; [5] gpu_compute_Gu!; @ ~/.julia/packages/KernelAbstractions/491pi/src/macros.jl:95; [6] gpu_compute_Gu!; @ ./none:0; Hint: catch this exception as `err` and call `code_typed(err; interactive = true)` to introspect the erronous code with Cthulhu.jl; Stacktrace:; [1] check_ir(job::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget, CUDA.CUDACompilerParams}, args::LLVM.Module); @ GPUCompiler ~/.julia/packages/GPUCompiler/2CW9L/src/validation.jl:147; [2] macro expansion; @ ~/.julia/packages/GPUCompiler/2CW9L/src/driver.jl:382 [inlined]; [3] macro expansion; @ ~/.julia/packages/TimerOutputs/NRdsv/src/TimerOutput.jl:253 [inlined]; [4] macro expansion; @ ~/.julia/packages/GPUCompiler/2CW9L/src/driver.jl:381 [inlined]; [5] emit_llvm(job::GPUCompiler.CompilerJob; toplevel::Bool, libraries::Bool, optimize::Bool, cleanup::Bool, validate::Bool, only_entry::Bool); @ GPUCompiler ~/.julia/packages/GPUCompiler/2CW9L/src/utils.jl:108; [6] emit_llvm; @ ~/.julia/packages/GPUCompiler/2CW9L/src/utils.jl:106 [inlined]; [7] codegen(output::Symbol, job::GPUCompiler.CompilerJob; toplevel::Bool, libraries::Bool, optimize::Bool, cleanup::Bool, validate::Bool, strip::Bool, only_entry::Bool, parent_job::Nothing); @ GPUCompiler ~/.julia/packages/GPUCompiler/2CW9L/src/driver.jl:100; [8] codegen; @ ~/.julia/packages/GPUCompiler/2CW9L/src/driver.jl:82 [inlined]; [9] compile(target::Symbol, job::GPUCompiler.CompilerJob; kwargs::@Kwargs{}); @ GPUCompiler ~/.julia/packages/GPUCompiler/2CW9L/src/driver.jl:79; [10] compile; @ ~/.julia/packages/GPUCompiler/2CW9L/src/driver.jl:74 [inlined]; [11] #1145; @ ~/.julia/packages/CUDA/2kjXI/src/compiler/compilation.jl:250 [inlined]; [12] JuliaContext(f::CUDA.var""#1145#1148""{GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget, CUDA.CUDACompilerParams}}; kwargs::@Kwargs{}); @ GPUComp",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3646#issuecomment-2428001700:6221,optimiz,optimize,6221,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3646#issuecomment-2428001700,1,['optimiz'],['optimize']
Performance,"ons.NDIteration.NDRange{3,KernelAbstractions.NDIteration.StaticSize{(1, 1, 16)},KernelAbstractions.NDIteration.StaticSize{(16, 16, 1)},Nothing,Nothing}},Nothing,KernelAbstractions.var""##PassType#253"",Nothing,Cassette.DisableHooks},typeof(Oceananigans.Solvers.gpu_calculate_pressure_right_hand_side!),CUDA.CuDeviceArray{Complex{Float64},3,CUDA.AS.Global},Oceananigans.Solvers.HorizontallyPeriodic,GPU,RegularCartesianGrid{Float64,Periodic,Periodic,Bounded,OffsetArrays.OffsetArray{Float64,1,StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}}},Int64,NamedTuple{(:u, :v, :w),Tuple{OffsetArrays.OffsetArray{Float64,3,CUDA.CuDeviceArray{Float64,3,CUDA.AS.Global}},OffsetArrays.OffsetArray{Float64,3,CUDA.CuDeviceArray{Float64,3,CUDA.AS.Global}},OffsetArrays.OffsetArray{Float64,3,CUDA.CuDeviceArray{Float64,3,CUDA.AS.Global}}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:maxthreads,),Tuple{Int64}}}) at /home/alir/.julia/packages/GPUCompiler/4e9CU/src/cache.jl:0; [18] cufunction(::Function, ::Type{T} where T; name::String, kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:maxthreads,),Tuple{Int64}}}) at /home/alir/.julia/packages/CUDA/h38pe/src/compiler/execution.jl:296; [19] macro expansion at /home/alir/.julia/packages/CUDA/h38pe/src/compiler/execution.jl:108 [inlined]; [20] (::KernelAbstractions.Kernel{KernelAbstractions.CUDADevice,KernelAbstractions.NDIteration.StaticSize{(16, 16)},KernelAbstractions.NDIteration.StaticSize{(16, 16, 16)},typeof(Oceananigans.Solvers.gpu_calculate_pressure_right_hand_side!)})(::CUDA.CuArray{Complex{Float64},3,Nothing}, ::Vararg{Any,N} where N; ndrange::Nothing, dependencies::KernelAbstractions.CudaEvent, workgroupsize::Nothing, progress::Function) at /home/alir/.julia/packages/KernelAbstractions/yw9SF/src/backends/cuda.jl:211; [21] launch!(::GPU, ::RegularCartesianGrid{Float64,Periodic,Periodic,Bounded,OffsetArrays.OffsetArray{Float64,1,StepRangeLen{Float64,Base.Tw",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/828:11246,cache,cache,11246,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/828,1,['cache'],['cache']
Performance,"ons.NDIteration.NDRange{3,KernelAbstractions.NDIteration.StaticSize{(1, 1, 16)},KernelAbstractions.NDIteration.StaticSize{(16, 16, 1)},Nothing,Nothing}},Nothing,KernelAbstractions.var""##PassType#253"",Nothing,Cassette.DisableHooks},typeof(Oceananigans.Solvers.gpu_calculate_pressure_right_hand_side!),CUDA.CuDeviceArray{Complex{Float64},3,CUDA.AS.Global},Oceananigans.Solvers.HorizontallyPeriodic,GPU,RegularCartesianGrid{Float64,Periodic,Periodic,Bounded,OffsetArrays.OffsetArray{Float64,1,StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}}},Int64,NamedTuple{(:u, :v, :w),Tuple{OffsetArrays.OffsetArray{Float64,3,CUDA.CuDeviceArray{Float64,3,CUDA.AS.Global}},OffsetArrays.OffsetArray{Float64,3,CUDA.CuDeviceArray{Float64,3,CUDA.AS.Global}},OffsetArrays.OffsetArray{Float64,3,CUDA.CuDeviceArray{Float64,3,CUDA.AS.Global}}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:maxthreads,),Tuple{Int64}}}) at /home/alir/.julia/packages/GPUCompiler/4e9CU/src/cache.jl:19; [12] + at ./int.jl:86 [inlined]; [13] hash_64_64 at ./hashing.jl:35 [inlined]; [14] hash_uint64 at ./hashing.jl:62 [inlined]; [15] hx at ./float.jl:568 [inlined]; [16] hash at ./float.jl:571 [inlined]; [17] cached_compilation(::typeof(CUDA._cufunction), ::GPUCompiler.FunctionSpec{typeof(Cassette.overdub),Tuple{Cassette.Context{nametype(CUDACtx),KernelAbstractions.CompilerMetadata{KernelAbstractions.NDIteration.StaticSize{(16, 16, 16)},KernelAbstractions.NDIteration.DynamicCheck,Nothing,Nothing,KernelAbstractions.NDIteration.NDRange{3,KernelAbstractions.NDIteration.StaticSize{(1, 1, 16)},KernelAbstractions.NDIteration.StaticSize{(16, 16, 1)},Nothing,Nothing}},Nothing,KernelAbstractions.var""##PassType#253"",Nothing,Cassette.DisableHooks},typeof(Oceananigans.Solvers.gpu_calculate_pressure_right_hand_side!),CUDA.CuDeviceArray{Complex{Float64},3,CUDA.AS.Global},Oceananigans.Solvers.HorizontallyPeriodic,GPU,RegularCartesianGrid{Float64,Periodic,Periodic,Bounded,Off",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/828:9703,cache,cache,9703,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/828,1,['cache'],['cache']
Performance,"ons.NDIteration.NDRange{3,KernelAbstractions.NDIteration.StaticSize{(1, 1, 16)},KernelAbstractions.NDIteration.StaticSize{(16, 16, 1)},Nothing,Nothing}},Nothing,KernelAbstractions.var""##PassType#253"",Nothing,Cassette.DisableHooks},typeof(Oceananigans.Solvers.gpu_calculate_pressure_right_hand_side!),CUDA.CuDeviceArray{Complex{Float64},3,CUDA.AS.Global},Oceananigans.Solvers.HorizontallyPeriodic,GPU,RegularCartesianGrid{Float64,Periodic,Periodic,Bounded,OffsetArrays.OffsetArray{Float64,1,StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}}},Int64,NamedTuple{(:u, :v, :w),Tuple{OffsetArrays.OffsetArray{Float64,3,CUDA.CuDeviceArray{Float64,3,CUDA.AS.Global}},OffsetArrays.OffsetArray{Float64,3,CUDA.CuDeviceArray{Float64,3,CUDA.AS.Global}},OffsetArrays.OffsetArray{Float64,3,CUDA.CuDeviceArray{Float64,3,CUDA.AS.Global}}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:maxthreads,),Tuple{Int64}}}) at /home/alir/.julia/packages/GPUCompiler/GKp4B/src/cache.jl:0; [11] cufunction(::typeof(Cassette.overdub), ::Type{Tuple{Cassette.Context{nametype(CUDACtx),KernelAbstractions.CompilerMetadata{KernelAbstractions.NDIteration.StaticSize{(16, 16, 16)},KernelAbstractions.NDIteration.DynamicCheck,Nothing,Nothing,KernelAbstractions.NDIteration.NDRange{3,KernelAbstractions.NDIteration.StaticSize{(1, 1, 16)},KernelAbstractions.NDIteration.StaticSize{(16, 16, 1)},Nothing,Nothing}},Nothing,KernelAbstractions.var""##PassType#253"",Nothing,Cassette.DisableHooks},typeof(Oceananigans.Solvers.gpu_calculate_pressure_right_hand_side!),CUDA.CuDeviceArray{Complex{Float64},3,CUDA.AS.Global},Oceananigans.Solvers.HorizontallyPeriodic,GPU,RegularCartesianGrid{Float64,Periodic,Periodic,Bounded,OffsetArrays.OffsetArray{Float64,1,StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}}},Int64,NamedTuple{(:u, :v, :w),Tuple{OffsetArrays.OffsetArray{Float64,3,CUDA.CuDeviceArray{Float64,3,CUDA.AS.Global}},OffsetArrays.OffsetArray{",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/828#issuecomment-700320323:8575,cache,cache,8575,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/828#issuecomment-700320323,1,['cache'],['cache']
Performance,"ons.NDIteration.NDRange{3,KernelAbstractions.NDIteration.StaticSize{(1, 1, 16)},KernelAbstractions.NDIteration.StaticSize{(16, 16, 1)},Nothing,Nothing}},Nothing,KernelAbstractions.var""##PassType#253"",Nothing,Cassette.DisableHooks},typeof(Oceananigans.Solvers.gpu_calculate_pressure_right_hand_side!),CUDA.CuDeviceArray{Complex{Float64},3,CUDA.AS.Global},Oceananigans.Solvers.HorizontallyPeriodic,GPU,RegularCartesianGrid{Float64,Periodic,Periodic,Bounded,OffsetArrays.OffsetArray{Float64,1,StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}}},Int64,NamedTuple{(:u, :v, :w),Tuple{OffsetArrays.OffsetArray{Float64,3,CUDA.CuDeviceArray{Float64,3,CUDA.AS.Global}},OffsetArrays.OffsetArray{Float64,3,CUDA.CuDeviceArray{Float64,3,CUDA.AS.Global}},OffsetArrays.OffsetArray{Float64,3,CUDA.CuDeviceArray{Float64,3,CUDA.AS.Global}}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:maxthreads,),Tuple{Int64}}}) at /home/alir/.julia/packages/GPUCompiler/GKp4B/src/cache.jl:24; [9] gpu_calculate_pressure_right_hand_side! at ./none:0 [inlined]; [10] cached_compilation(::typeof(CUDA._cufunction), ::GPUCompiler.FunctionSpec{typeof(Cassette.overdub),Tuple{Cassette.Context{nametype(CUDACtx),KernelAbstractions.CompilerMetadata{KernelAbstractions.NDIteration.StaticSize{(16, 16, 16)},KernelAbstractions.NDIteration.DynamicCheck,Nothing,Nothing,KernelAbstractions.NDIteration.NDRange{3,KernelAbstractions.NDIteration.StaticSize{(1, 1, 16)},KernelAbstractions.NDIteration.StaticSize{(16, 16, 1)},Nothing,Nothing}},Nothing,KernelAbstractions.var""##PassType#253"",Nothing,Cassette.DisableHooks},typeof(Oceananigans.Solvers.gpu_calculate_pressure_right_hand_side!),CUDA.CuDeviceArray{Complex{Float64},3,CUDA.AS.Global},Oceananigans.Solvers.HorizontallyPeriodic,GPU,RegularCartesianGrid{Float64,Periodic,Periodic,Bounded,OffsetArrays.OffsetArray{Float64,1,StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}}},Int64,NamedTuple{(:u,",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/828#issuecomment-700320323:7167,cache,cache,7167,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/828#issuecomment-700320323,1,['cache'],['cache']
Performance,oop at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:246; #start_repl_backend#46 at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:231; start_repl_backend at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:228; _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; #run_repl#59 at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:389; run_repl at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:375; jfptr_run_repl_91805.1 at /home/alir/.julia/juliaup/julia-1.10.5+0.x64.linux.gnu/lib/julia/sys.so (unknown line); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; #1013 at ./client.jl:432; jfptr_YY.1013_82772.1 at /home/alir/.julia/juliaup/julia-1.10.5+0.x64.linux.gnu/lib/julia/sys.so (unknown line); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; jl_apply at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/julia.h:1982 [inlined]; jl_f__call_latest at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/builtins.c:812; #invokelatest#2 at ./essentials.jl:892 [inlined]; invokelatest at ./essentials.jl:889 [inlined]; run_main_repl at ./client.jl:416; exec_options at ./client.jl:333; _start at ./client.jl:552; jfptr__start_82798.1 at /home/alir/.julia/juliaup/julia-1.10.5+0.x64.linux.gnu/lib/julia/sys.s,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3852:7515,cache,cache,7515,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3852,1,['cache'],['cache']
Performance,"oordinate; z::Float64 # z-coordinate; T::Float64 # Temperature; end. particles = StructArray{CustomParticle}((x₀, y₀, z₀, T));. # Define tracked fields as a NamedTuple; tracked_fields = (T=particles.T,). # Initialize LagrangianParticles with the StructArray and tracked fields; lagrangian_particles = LagrangianParticles(particles; tracked_fields=tracked_fields). #include in the model setup; model = NonhydrostaticModel(grid = grid, ; particles=lagrangian_particles,; advection = WENO(grid=grid, order=5),; timestepper = :QuasiAdamsBashforth2, ; tracers = (:T, :S),; buoyancy = Buoyancy(model=SeawaterBuoyancy(equation_of_state=LinearEquationOfState(thermal_expansion = 3.87e-5,; haline_contraction = 7.86e-4)), gravity_unit_vector=(-sind(θ),0,-cosd(θ))),; coriolis = FPlane(0e-4),; closure = closure,; forcing = forcing,; boundary_conditions = boundary_conditions,; ). ```. I get the following error:. ```Julia. LoadError: MethodError: _fractional_indices(::Tuple{Float64, Float64, Float64}, ::RectilinearGrid{Float64, Bounded, Bounded, Bounded, Float64, Float64, Float64, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, CPU}, ::Nothing, ::Nothing, ::Nothing) is ambiguous. Candidates:; _fractional_indices(::Any, grid, ℓx, ::Nothing, ::Nothing); @ Oceananigans.Fields ~/.julia/packages/Oceananigans/17XSY/src/Fields/interpolate.jl:187; _fractional_indices(::Any, grid, ::Nothing, ℓy, ::Nothing); @ Oceananigans.Fields ~/.julia/packages/Oceananigans/17XSY/src/Fields/interpolate.jl:195; _fractional_indices(::Any, grid, ℓx, ℓy, ::Nothing); @ Oceananigans.Fields ~/.julia/packages/Oceananigans/17XSY/src/Fields/interpolate.jl:180; _fractional_indices(::Any, grid, ::Noth",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3609:1917,Load,LoadError,1917,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3609,1,['Load'],['LoadError']
Performance,"ope; @ ~/software/Oceananigans.jl/src/Models/ShallowWaterModels/ShallowWaterModels.jl:15; [4] include(mod::Module, _path::String); @ Base ./Base.jl:386; [5] include(x::String); @ Oceananigans.Models ~/software/Oceananigans.jl/src/Models/Models.jl:1; [6] top-level scope; @ ~/software/Oceananigans.jl/src/Models/Models.jl:20; [7] include(mod::Module, _path::String); @ Base ./Base.jl:386; [8] include(x::String); @ Oceananigans ~/software/Oceananigans.jl/src/Oceananigans.jl:1; [9] top-level scope; @ ~/software/Oceananigans.jl/src/Oceananigans.jl:179; [10] include; @ ./Base.jl:386 [inlined]; [11] include_package_for_output(pkg::Base.PkgId, input::String, depot_path::Vector{String}, dl_load_path::Vector{String}, load_path::Vector{String}, concrete_deps::Vector{Pair{Base.PkgId, UInt64}}, source::Nothing); @ Base ./loading.jl:1213; [12] top-level scope; @ none:1; [13] eval; @ ./boot.jl:360 [inlined]; [14] eval(x::Expr); @ Base.MainInclude ./client.jl:446; [15] top-level scope; @ none:1; in expression starting at /home/fpoulin/software/Oceananigans.jl/src/Models/ShallowWaterModels/shallow_water_model.jl:16; in expression starting at /home/fpoulin/software/Oceananigans.jl/src/Models/ShallowWaterModels/ShallowWaterModels.jl:1; in expression starting at /home/fpoulin/software/Oceananigans.jl/src/Models/Models.jl:1; in expression starting at /home/fpoulin/software/Oceananigans.jl/src/Oceananigans.jl:1; ERROR: Failed to precompile Oceananigans [9e8cae18-63c1-5223-a75c-80ca9d6e9a09] to /home/fpoulin/.julia/compiled/v1.6/Oceananigans/jl_cnHalv.; Stacktrace:; [1] error(s::String); @ Base ./error.jl:33; [2] compilecache(pkg::Base.PkgId, path::String, internal_stderr::Base.TTY, internal_stdout::Base.TTY); @ Base ./loading.jl:1360; [3] compilecache(pkg::Base.PkgId, path::String); @ Base ./loading.jl:1306; [4] _require(pkg::Base.PkgId); @ Base ./loading.jl:1021; [5] require(uuidkey::Base.PkgId); @ Base ./loading.jl:914; [6] require(into::Module, mod::Symbol); @ Base ./loading.jl:901; ```",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1663#issuecomment-843316815:2474,load,loading,2474,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1663#issuecomment-843316815,5,['load'],['loading']
Performance,"open at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Libdl/src/Libdl.jl:109 [inlined] (repeats 2 times); &nbsp; | │ [3] __init__() at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/SuiteSparse/src/cholmod.jl:90; &nbsp; | └ @ SuiteSparse.CHOLMOD /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/SuiteSparse/src/cholmod.jl:187; &nbsp; | /storage7/buildkite-agent/julia-1.5.4/bin/julia: error while loading shared libraries: libLLVM-9jl.so: ELF load command past end of file; &nbsp; | ERROR: LoadError: LoadError: IOError: write: broken pipe (EPIPE); &nbsp; | Stacktrace:; &nbsp; | [1] uv_write(::Base.PipeEndpoint, ::Ptr{UInt8}, ::UInt64) at ./stream.jl:951; &nbsp; | [2] unsafe_write(::Base.PipeEndpoint, ::Ptr{UInt8}, ::UInt64) at ./stream.jl:1005; &nbsp; | [3] write(::Base.PipeEndpoint, ::String) at ./strings/io.jl:183; &nbsp; | [4] create_expr_cache(::String, ::String, ::Array{Pair{Base.PkgId,UInt64},1}, ::Base.UUID) at ./loading.jl:1194; &nbsp; | [5] compilecache(::Base.PkgId, ::String) at ./loading.jl:1286; &nbsp; | [6] _require(::Base.PkgId) at ./loading.jl:1030; &nbsp; | [7] require(::Base.PkgId) at ./loading.jl:928; &nbsp; | [8] require(::Module, ::Symbol) at ./loading.jl:923; &nbsp; | [9] include(::Function, ::Module, ::String) at ./Base.jl:380; &nbsp; | [10] include at ./Base.jl:368 [inlined]; &nbsp; | [11] include(::String) at /storage7/buildkite-agent/.julia-2581/packages/PencilArrays/DTEhf/src/PencilArrays.jl:1; &nbsp; | [12] top-level scope at /storage7/buildkite-agent/.julia-2581/packages/PencilArrays/DTEhf/src/PencilArrays.jl:12; &nbsp; | [13] include(::Function, ::Module, ::String) at ./Base.jl:380; &nbsp; | [14] include(::Module, ::String) at ./Base.jl:368; &nbsp; | [15] top-level scope at none:2; &nbsp; | [16] eval at ./boot.jl:347 [inlined]; &nbsp; | [17] eval(::Expr) at ./client.jl:467; &nbsp; | [18] top-level scope at ./none:3; &nbsp; | in expression starting at /storage7/buildkite-agent/",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1662#issuecomment-843325731:4012,load,loading,4012,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1662#issuecomment-843325731,1,['load'],['loading']
Performance,"oplevel::Bool, optimize::Bool, cleanup::Bool, strip::Bool, validate::Bool, only_entry::Bool, parent_job::Nothing); @ GPUCompiler ~/.julia/packages/GPUCompiler/Cp7sE/src/driver.jl:129; [8] codegen; @ ~/.julia/packages/GPUCompiler/Cp7sE/src/driver.jl:110 [inlined]; [9] compile(target::Symbol, job::GPUCompiler.CompilerJob; libraries::Bool, toplevel::Bool, optimize::Bool, cleanup::Bool, strip::Bool, validate::Bool, only_entry::Bool); @ GPUCompiler ~/.julia/packages/GPUCompiler/Cp7sE/src/driver.jl:106; [10] compile; @ ~/.julia/packages/GPUCompiler/Cp7sE/src/driver.jl:98 [inlined]; [11] #45; @ ~/.julia/packages/Metal/lnkVP/src/compiler/compilation.jl:57 [inlined]; [12] JuliaContext(f::Metal.var""#45#46""{GPUCompiler.CompilerJob{GPUCompiler.MetalCompilerTarget, Metal.MetalCompilerParams}}); @ GPUCompiler ~/.julia/packages/GPUCompiler/Cp7sE/src/driver.jl:47; [13] compile(job::GPUCompiler.CompilerJob); @ Metal ~/.julia/packages/Metal/lnkVP/src/compiler/compilation.jl:56; [14] actual_compilation(cache::Dict{Any, Any}, src::Core.MethodInstance, world::UInt64, cfg::GPUCompiler.CompilerConfig{GPUCompiler.MetalCompilerTarget, Metal.MetalCompilerParams}, compiler::typeof(Metal.compile), linker::typeof(Metal.link)); @ GPUCompiler ~/.julia/packages/GPUCompiler/Cp7sE/src/execution.jl:125; [15] cached_compilation(cache::Dict{Any, Any}, src::Core.MethodInstance, cfg::GPUCompiler.CompilerConfig{GPUCompiler.MetalCompilerTarget, Metal.MetalCompilerParams}, compiler::Function, linker::Function); @ GPUCompiler ~/.julia/packages/GPUCompiler/Cp7sE/src/execution.jl:103; [16] macro expansion; @ ~/.julia/packages/Metal/lnkVP/src/compiler/execution.jl:162 [inlined]; [17] macro expansion; @ ./lock.jl:267 [inlined]; [18] mtlfunction(f::typeof(Oceananigans.Models.HydrostaticFreeSurfaceModels.gpu_compute_hydrostatic_free_surface_Gu!), tt::Type{Tuple{KernelAbstractions.CompilerMetadata{KernelAbstractions.NDIteration.StaticSize{(3, 3, 50)}, KernelAbstractions.NDIteration.DynamicCheck, Nothing, Nothing, K",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2618#issuecomment-1731573822:37624,cache,cache,37624,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2618#issuecomment-1731573822,1,['cache'],['cache']
Performance,"opy the lines directly into `REPL` to define the grid, model and do one time step, I don't get an error. Any ideas what might be going wrong in this benchmarking example?. ```; [2021/06/28 09:40:32.366] INFO Benchmarking 1/16: (CPU, Float32, 32)...; [2021/06/28 09:40:38.930] WARN Inflating model grid halo size to (3, 3, 3) and recreating grid. The model grid will be different from the input grid. To avoid this warning, pass halo=(3, 3, 3) when constructing the grid. -@-> /home/fpoulin/software/New_Oceananigans/Oceananigans.jl/src/Grids/automatic_halo_sizing.jl:41; [2021/06/28 09:42:28.384] INFO Benchmarking 2/16: (GPU, Float32, 32)...; [2021/06/28 09:42:32.299] WARN Inflating model grid halo size to (3, 3, 3) and recreating grid. The model grid will be different from the input grid. To avoid this warning, pass halo=(3, 3, 3) when constructing the grid. -@-> /home/fpoulin/software/New_Oceananigans/Oceananigans.jl/src/Grids/automatic_halo_sizing.jl:41; ERROR: LoadError: InvalidIRError: compiling kernel gpu_calculate_Gw!(Cassette.Context{nametype(CUDACtx), KernelAbstractions.CompilerMetadata{KernelAbstractions.NDIteration.StaticSize{(32, 32, 32)}, KernelAbstractions.NDIteration.DynamicCheck, Nothing, Nothing, KernelAbstractions.NDIteration.NDRange{3, KernelAbstractions.NDIteration.StaticSize{(2, 2, 32)}, KernelAbstractions.NDIteration.StaticSize{(16, 16, 1)}, Nothing, Nothing}}, Nothing, KernelAbstractions.var""##PassType#257"", Nothing, Cassette.DisableHooks}, typeof(Oceananigans.Models.IncompressibleModels.gpu_calculate_Gw!), OffsetArrays.OffsetArray{Float32, 3, CuDeviceArray{Float32, 3, 1}}, RegularRectilinearGrid{Float32, Periodic, Periodic, Bounded, OffsetArrays.OffsetVector{Float32, StepRangeLen{Float32, Float64, Float64}}}, WENO5, Nothing, Nothing, Nothing, Buoyancy{SeawaterBuoyancy{Float32, LinearEquationOfState{Float32}, Nothing, Nothing}, Oceananigans.BuoyancyModels.ZDirection}, NamedTuple{(:velocities, :tracers), Tuple{NamedTuple{(:u, :v, :w), Tuple{Oceananigan",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1780:1358,Load,LoadError,1358,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1780,1,['Load'],['LoadError']
Performance,"orted dynamic function invocation (call to getproperty); Stacktrace:; [1] call at /home/fpoulin/.julia/packages/Cassette/Wjztv/src/context.jl:456; [2] fallback at /home/fpoulin/.julia/packages/Cassette/Wjztv/src/context.jl:454; [3] overdub at /home/fpoulin/.julia/packages/Cassette/Wjztv/src/context.jl:279; [4] b̄ at /home/fpoulin/software/Oceananigans.jl/examples/inertially_unstable_jet.jl:39; [5] call_func at /home/fpoulin/software/Oceananigans.jl/src/Fields/function_field.jl:61; [6] getindex at /home/fpoulin/software/Oceananigans.jl/src/Fields/function_field.jl:63; [7] identity at /home/fpoulin/software/Oceananigans.jl/src/Operators/interpolation_utils.jl:6; [8] - at /home/fpoulin/software/Oceananigans.jl/src/AbstractOperations/binary_operations.jl:59; [9] identity at /home/fpoulin/software/Oceananigans.jl/src/Operators/interpolation_utils.jl:11; [10] getindex at /home/fpoulin/software/Oceananigans.jl/src/AbstractOperations/binary_operations.jl:34; [11] macro expansion at /home/fpoulin/software/Oceananigans.jl/src/Fields/computed_field.jl:114; [12] gpu__compute! at /home/fpoulin/.julia/packages/KernelAbstractions/mKsXc/src/macros.jl:80; [13] overdub at /home/fpoulin/.julia/packages/Cassette/Wjztv/src/overdub.jl:0; Stacktrace:; [1] check_ir(::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDA.CUDACompilerParams}, ::LLVM.Module) at /home/fpoulin/.julia/packages/GPUCompiler/uTpNx/src/validation.jl:123; [2] macro expansion at /home/fpoulin/.julia/packages/GPUCompiler/uTpNx/src/driver.jl:239 [inlined]; [3] macro expansion at /home/fpoulin/.julia/packages/TimerOutputs/4QAIk/src/TimerOutput.jl:206 [inlined]; [4] codegen(::Symbol, ::GPUCompiler.CompilerJob; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, validate::Bool, only_entry::Bool) at /home/fpoulin/.julia/packages/GPUCompiler/uTpNx/src/driver.jl:237; [5] compile(::Symbol, ::GPUCompiler.CompilerJob; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, validate:; ...; ```",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1554#issuecomment-815099807:5401,optimiz,optimize,5401,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1554#issuecomment-815099807,2,['optimiz'],['optimize']
Performance,"ostaticFreeSurfaceModel(;; grid,; particles; ). for n in 1:100; @info ""Iteration $n...""; time_step!(model, 0.1); end; ```. CPU segfault:. ```; [ Info: Iteration 1...; [ Info: Iteration 2... [503062] signal (11.1): Segmentation fault; in expression starting at /home/alir/atdepth/Oceananigans.jl/particles_error.jl:35; advect_particle at /home/alir/atdepth/Oceananigans.jl/src/Models/LagrangianParticleTracking/lagrangian_particle_advection.jl:0 [inlined]; macro expansion at /home/alir/atdepth/Oceananigans.jl/src/Models/LagrangianParticleTracking/lagrangian_particle_advection.jl:177 [inlined]; cpu__advect_particles! at /home/alir/.julia/packages/KernelAbstractions/491pi/src/macros.jl:291 [inlined]; cpu__advect_particles! at ./none:0; __thread_run at /home/alir/.julia/packages/KernelAbstractions/491pi/src/cpu.jl:144; unknown function (ip: 0x7c0090512182); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; __run at /home/alir/.julia/packages/KernelAbstractions/491pi/src/cpu.jl:111; unknown function (ip: 0x7c009050feb3); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; #_#16 at /home/alir/.julia/packages/KernelAbstractions/491pi/src/cpu.jl:46; _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; jl_apply at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/julia.h:1982 [inlined]; do_apply at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/builtins.c:768; Kernel at /home/alir/.julia/packages/KernelAbstractions/491pi/src/cpu.jl:39; _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3852:2184,cache,cache,2184,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3852,1,['cache'],['cache']
Performance,ot-10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:228; _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; #run_repl#59 at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:389; run_repl at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:375; jfptr_run_repl_91805.1 at /home/alir/.julia/juliaup/julia-1.10.5+0.x64.linux.gnu/lib/julia/sys.so (unknown line); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; #1013 at ./client.jl:432; jfptr_YY.1013_82772.1 at /home/alir/.julia/juliaup/julia-1.10.5+0.x64.linux.gnu/lib/julia/sys.so (unknown line); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; jl_apply at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/julia.h:1982 [inlined]; jl_f__call_latest at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/builtins.c:812; #invokelatest#2 at ./essentials.jl:892 [inlined]; invokelatest at ./essentials.jl:889 [inlined]; run_main_repl at ./client.jl:416; exec_options at ./client.jl:333; _start at ./client.jl:552; jfptr__start_82798.1 at /home/alir/.julia/juliaup/julia-1.10.5+0.x64.linux.gnu/lib/julia/sys.so (unknown line); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; jl_apply at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/julia.h:1982 [inlined]; true_main at /cach,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3852:7855,cache,cache,7855,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3852,1,['cache'],['cache']
Performance,"ou already did. I would usually try something like `]update Printf` which should remove orphaned packages from `Manifest.toml`. > We can. Such a test may end up running on the CPU via scalar operations though... ?. Ah ok, maybe not a great idea then as it would slow testing down. I guess the computations are tested on the GPU which is good enough. > Note that `Computation` allows the user to specify their own temporary array. `model.pressures.pHY′` is used as a default when `model` is passed to `Computation` in place of an array or field. Ah nice. I guess I was thinking in case `model.pressures.pHY′` disappears one day. > I think just a few will suffice for shallow and deep operations trees, perhaps choosing common use cases to ensure that using abstract operations rather than hard-coded kernels doesn't result in a big performance hit. It will be hard to interpret the results of a benchmark on a deep tree anyways, because we won't have an alternate implementation to compare against. Future performance optimization could use some kind of tree analysis utility + shared memory to accelerate kernels. . Hmmm, I was thinking it would be good to benchmark each operator at least once but I suppose if `sin` is fast then we can assume `cos` and `tanh` will also be fast. Shallow and deep trees makes sense. True we may not have an alternative implementation but we can compare the deep and shallow tree computations to get an idea. I find comparing the computation time to the time per iteration (~30 ms for 256^3) to be helpful. > Why extensive? I'm just not sure what to write: the rules for how things work are already all there in the docstrings. Maybe examples are what's needed?. Didn't mean to suggest that we need extensive documentation right away. Having examples of what's possible will be really useful, but we can build up a collection of good examples over time. PS: Think you missed half my comments as GitHub collapsed and hid them, but it was mostly minor comments anyways.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/463#issuecomment-545887965:1089,perform,performance,1089,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/463#issuecomment-545887965,2,"['optimiz', 'perform']","['optimization', 'performance']"
Performance,"p!. `Julia uses a semicolon to separate regular arguments from keyword arguments`. Ah yeah, I thought `Float64` should be passed as a keyword argument. The following works now:. ```; grid = VerticallyStretchedRectilinearGrid(Float64; architecture = GPU(),; size = (Nx,Ny,Nz), x=(0, Lx), y=(0, Ly), zF=zF,; halo = (3, 3, 3),; topology = (Periodic, Periodic, Bounded)); ```. `Hmmm, how small was the time step? `. I tried a few different values ranging from 1 sec to 60 sec. . I found that the `NaN` issue has to do with the `AnisotropicBiharmonicDiffusivity`. The following works:. ```; closure = AnisotropicDiffusivity(νh=kappaH_tmp, κh=kappaH_tmp, κz = kappaV, νz = kappaV); ```. but not this: . ```; closure = (AnisotropicDiffusivity(νh=0, κh=0, κz = kappaV, νz = kappaV),; AnisotropicBiharmonicDiffusivity(νh=kappaH, κh=kappaH)); ```. Also, `topology = (Periodic, Periodic, Bounded)` runs fine now, but `topology = (Periodic, Bounded, Bounded)` gives the following error:. ```; Loaded all modules; Starting the simulation...; ┌ Warning: You have used the default iteration_interval=1. This simulation will recalculate the time step every iteration which can be slow.; └ @ Oceananigans.Simulations ~/.julia/packages/Oceananigans/gCqmh/src/Simulations/simulation.jl:68; ERROR: LoadError: type VerticallyStretchedRectilinearGrid has no field Δz; Stacktrace:; [1] getproperty(::VerticallyStretchedRectilinearGrid{Float64,Periodic,Bounded,Bounded,OffsetArrays.OffsetArray{Float64,1,StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}},OffsetArrays.OffsetArray{Float64,1,CuArray{Float64,1}}}, ::Symbol) at ./Base.jl:33; [2] cell_advection_timescale(::CuArray{Float64,3}, ::CuArray{Float64,3}, ::CuArray{Float64,3}, ::VerticallyStretchedRectilinearGrid{Float64,Periodic,Bounded,Bounded,OffsetArrays.OffsetArray{Float64,1,StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}},OffsetArrays.OffsetArray{Float64,1,CuArray{Float64,1}}}) at /home/guptam/.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1429#issuecomment-792994007:999,Load,Loaded,999,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1429#issuecomment-792994007,1,['Load'],['Loaded']
Performance,"p; | Downloading artifact: HDF5; &nbsp; | ######################################################################## 100.0%; &nbsp; | Downloading artifact: LibSSH2; &nbsp; | ######################################################################## 100.0%; &nbsp; | Downloading artifact: CompilerSupportLibraries; &nbsp; | ######################################################################## 100.0%; &nbsp; | Downloading artifact: MPICH; &nbsp; | ######################################################################## 100.0%; &nbsp; | Downloading artifact: LibCURL; &nbsp; | ######################################################################## 100.0%; &nbsp; | Building MPI ─→ `/storage7/buildkite-agent/.julia-2581/packages/MPI/b7MVG/deps/build.log`; &nbsp; | [ Info: using default MPI jll; &nbsp; | Building FFTW → `/storage7/buildkite-agent/.julia-2581/packages/FFTW/G3lSO/deps/build.log`; &nbsp; | Precompiling project...; &nbsp; | WARNING: Error during initialization of module GMP:; &nbsp; | ErrorException(""could not load library ""libgmp""; &nbsp; | libgmp.so: ELF load command past end of file""); &nbsp; | WARNING: Error during initialization of module LinearAlgebra:; &nbsp; | ErrorException(""could not load library ""libopenblas64_""; &nbsp; | libopenblas64_.so: ELF load command past end of file""); &nbsp; | ┌ Error: Error during initialization of module CHOLMOD; &nbsp; | │ exception =; &nbsp; | │ could not load library ""libcholmod""; &nbsp; | │ libopenblas64_.so.0: ELF load command past end of file; &nbsp; | │ Stacktrace:; &nbsp; | │ [1] dlopen(::String, ::UInt32; throw_error::Bool) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Libdl/src/Libdl.jl:109; &nbsp; | │ [2] dlopen at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Libdl/src/Libdl.jl:109 [inlined] (repeats 2 times); &nbsp; | │ [3] __init__() at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/SuiteSparse/src/cholmod.jl:90; &nbsp; | └ @ SuiteSparse",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1662#issuecomment-843325731:2329,load,load,2329,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1662#issuecomment-843325731,1,['load'],['load']
Performance,"pe and type of A. (The first two arguments have the same meaning as for fft.) Returns an object P which; represents the linear operator computed by the FFT, and which contains all of the information needed to compute fft(A, dims) quickly. To apply P to an array A, use P * A; in general, the syntax for applying plans is much like that of matrices. (A plan can only be applied to arrays of the same size as the A for which; the plan was created.) You can also apply a plan with a preallocated output array Â by calling mul!(Â, plan, A). (For mul!, however, the input array A must be a complex floating-point; array like the output Â.) You can compute the inverse-transform plan by inv(P) and apply the inverse plan with P \ Â (the inverse plan is cached and reused for subsequent calls to inv or; \), and apply the inverse plan to a pre-allocated output array A with ldiv!(A, P, Â). The flags argument is a bitwise-or of FFTW planner flags, defaulting to FFTW.ESTIMATE. e.g. passing FFTW.MEASURE or FFTW.PATIENT will instead spend several seconds (or more) benchmarking; different possible FFT algorithms and picking the fastest one; see the FFTW manual for more information on planner flags. The optional timelimit argument specifies a rough upper bound on; the allowed planning time, in seconds. Passing FFTW.MEASURE or FFTW.PATIENT may cause the input array A to be overwritten with zeros during plan creation. plan_fft! is the same as plan_fft but creates a plan that operates in-place on its argument (which must be an array of complex floating-point numbers). plan_ifft and so on are similar; but produce plans that perform the equivalent of the inverse transforms ifft and so on. help?> plan_fft!; search: plan_fft! plan_ifft! plan_bfft! plan_fft plan_rfft plan_ifft plan_bfft plan_irfft plan_brfft. plan_fft!(A [, dims]; flags=FFTW.ESTIMATE, timelimit=Inf). Same as plan_fft, but operates in-place on A.; ```. But I just learned that you can apply the inverse plan with `P \ Â` which is cool!",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/119#issuecomment-471179127:1941,perform,perform,1941,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/119#issuecomment-471179127,1,['perform'],['perform']
Performance,ply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; __run at /home/alir/.julia/packages/KernelAbstractions/491pi/src/cpu.jl:111; unknown function (ip: 0x7c009050feb3); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; #_#16 at /home/alir/.julia/packages/KernelAbstractions/491pi/src/cpu.jl:46; _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; jl_apply at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/julia.h:1982 [inlined]; do_apply at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/builtins.c:768; Kernel at /home/alir/.julia/packages/KernelAbstractions/491pi/src/cpu.jl:39; _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; advect_lagrangian_particles! at /home/alir/atdepth/Oceananigans.jl/src/Models/LagrangianParticleTracking/lagrangian_particle_advection.jl:193; step_lagrangian_particles! at /home/alir/atdepth/Oceananigans.jl/src/Models/LagrangianParticleTracking/LagrangianParticleTracking.jl:143 [inlined]; step_lagrangian_particles! at /home/alir/atdepth/Oceananigans.jl/src/Models/HydrostaticFreeSurfaceModels/HydrostaticFreeSurfaceModels.jl:107 [inlined]; #time_step!#8 at /home/alir/atdepth/Oceananigans.jl/src/TimeSteppers/quasi_adams_bashforth_2.jl:124; time_step! at /home/alir/atdepth/Oceananigans.jl/src/TimeSteppers/quasi_adams_bashforth_2.jl:76; unknown function (ip: 0x7c00a0f12fbd); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julia,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3852:3145,cache,cache,3145,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3852,1,['cache'],['cache']
Performance,ply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; top-level scope at /home/alir/atdepth/Oceananigans.jl/particles_error.jl:37; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:925; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:877; ijl_toplevel_eval_in at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:985; eval at ./boot.jl:385 [inlined]; include_string at ./loading.jl:2076; _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; _include at ./loading.jl:2136; include at ./client.jl:489; unknown function (ip: 0x7c00f54ff855); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; jl_apply at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/julia.h:1982 [inlined]; do_call at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:126; eval_value at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:223; eval_stmt_value at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:174 [inlined]; eval_body at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:617; jl_interpret_toplevel_thunk at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:775; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:934; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:877; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3852:5105,cache,cache,5105,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3852,1,['cache'],['cache']
Performance,"ps://render.githubusercontent.com/render/math?math=c_{rj}"">) are based on the Lagrange interpolation of the integral of <img src=""https://render.githubusercontent.com/render/math?math=\overline{v}_{i}""> and, as such, are coordinate-dependent. The first thing to do, when wanting to implement the WENO scheme on a stretched grid, is then to make sure that the reconstruction is correctly done, and, as such, `WENO5(grid = grid)` takes care that (<img src=""https://render.githubusercontent.com/render/math?math=c_{rj}"">) are correctly calculated based on the underlying grid. Now, WENO schemes differ from ENO schemes since they do not just choose the ""smoothest stencil"" among the (in this case) three stencils, but weight them as such. <img src=""https://render.githubusercontent.com/render/math?math=v_{i %2B 1/2} = \sum_{r=0}^k w_{r} v_{i %2B 1/2,r} "">. Those weights have to satisfy <img src=""https://render.githubusercontent.com/render/math?math=\sum_{r=0}^k w_{r} = 1""> and are mostly a matter of choice (if the reconstruction is performed correctly for all stencils!). The general way to do this is to make them functions of a ""local smoothness indicator"" <img src=""https://render.githubusercontent.com/render/math?math=\beta_r""> which is calculated as such; <img src=""https://render.githubusercontent.com/render/math?math=\beta_{r} = \sum_{l=1}^k \int_{x_{i-1/2}}^{x_{i %2B 1/2}} \Delta x^{2l -1} \left( \frac{\partial^l p_r(x)}{\partial x^l} \right)^2 dx""> . These <img src=""https://render.githubusercontent.com/render/math?math=\beta_r""> simplify quite nicely for uniform grids. the `streched_smoothness=true` option calculates the coefficients of these smoothness indicators for a stretched grid, by assuming that (<img src=""https://render.githubusercontent.com/render/math?math=k = 2"">). Then the weights are calculated as such; <img src=""https://render.githubusercontent.com/render/math?math=w_{r} = \frac{\alpha_r}{\alpha_0 %2B \alpha_1 %2B \alpha_2}""> . where <img src=""https://render.git",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2060#issuecomment-975649397:2039,perform,performed,2039,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2060#issuecomment-975649397,1,['perform'],['performed']
Performance,"quire(uuidkey::Base.PkgId); @ Base ./loading.jl:1013; [9] require(into::Module, mod::Symbol); @ Base ./loading.jl:997; [10] top-level scope; @ ~/.julia/packages/MPI/08SPr/deps/deps.jl:8; [11] include(mod::Module, _path::String); @ Base ./Base.jl:418; [12] include(x::String); @ MPI ~/.julia/packages/MPI/08SPr/src/MPI.jl:1; [13] top-level scope; @ ~/.julia/packages/MPI/08SPr/src/MPI.jl:36; [14] include; @ ./Base.jl:418 [inlined]; [15] include_package_for_output(pkg::Base.PkgId, input::String, depot_path::Vector{String}, dl_load_path::Vector{String}, load_path::Vector{String}, concrete_deps::Vector{Pair{Base.PkgId, UInt64}}, source::String); @ Base ./loading.jl:1318; [16] top-level scope; @ none:1; [17] eval; @ ./boot.jl:373 [inlined]; [18] eval(x::Expr); @ Base.MainInclude ./client.jl:453; [19] top-level scope; @ none:1; during initialization of module MPICH_jll; in expression starting at /Users/sean/.julia/packages/MPI/08SPr/deps/deps.jl:1; ERROR: LoadError: Failed to precompile MPI [da04e1cc-30fd-572f-bb4f-1f8673147195] to /Users/sean/.julia/compiled/v1.7/MPI/jl_AfEwik.; Stacktrace:; [1] error(s::String); @ Base ./error.jl:33; [2] compilecache(pkg::Base.PkgId, path::String, internal_stderr::IO, internal_stdout::IO, ignore_loaded_modules::Bool); @ Base ./loading.jl:1466; [3] compilecache(pkg::Base.PkgId, path::String); @ Base ./loading.jl:1410; [4] _require(pkg::Base.PkgId); @ Base ./loading.jl:1120; [5] require(uuidkey::Base.PkgId); @ Base ./loading.jl:1013; [6] require(into::Module, mod::Symbol); @ Base ./loading.jl:997; [7] include(mod::Module, _path::String); @ Base ./Base.jl:418; [8] include(x::String); @ Oceananigans ~/.julia/packages/Oceananigans/jmNfq/src/Oceananigans.jl:5; [9] top-level scope; @ ~/.julia/packages/Oceananigans/jmNfq/src/Oceananigans.jl:190; [10] include; @ ./Base.jl:418 [inlined]; [11] include_package_for_output(pkg::Base.PkgId, input::String, depot_path::Vector{String}, dl_load_path::Vector{String}, load_path::Vector{String}, concrete_deps::",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2480:4286,Load,LoadError,4286,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2480,1,['Load'],['LoadError']
Performance,"r DNS with no slip, DNS with free-slip, and LES with a wall model). `Gradient` conditions on immersed boundaries may also be important in some cases, though I don't think it often arises in computational oceanography. We may want to define a type called `ImmersedBoundary` that combines the physical definition of the boundary location (a function of `x, y, z, t`) as well as any numerical aspects of the immersed boundary's implementation (eg the width of the regularization region, damping constants, order of interpolation for setting gradient / value boundary conditions, etc). . For interfacing with `model`, I think adding a field `immersed_boundaries` to `grid` could make sense. Then boundary conditions are applied in `6+n` places: along the 6 boundaries in `x, y, z`, and along the `n` immersed boundaries. The user may specify a single immersed boundary or a tuple of `n` immersed boundaries when constructing the `grid`. . The boundary conditions on all immersed boundaries can somehow be appended to the `field.boundary_conditions` tuple. This may require some adjustment of the boundary conditions implementation, since `FieldBoundaryConditions` will no longer be a `NamedTuple` with `propertynames` `(x, y, z)`, but will also include properties for each immersed boundaries (somehow). Perhaps the boundary conditions for immersed boundaries can be collected into a tuple `boundary_conditions.immersed_boundaries`, where each element of `boundary_conditions.immersed_boundaries` corresponds to the boundary in `grid.immersed_boundaries` and is of type `BoundaryCondition`. Finally, we can define functions that implement the immersed boundary continuous forcing for momentum and tracers within their respective tendency functions (rather than adding it as a user-defined forcing). We can implement the continuous forcing in a naive manner for small numbers of immersed boundaries, and consider optimizations in the future for large numbers of immersed boundaries should that need arise.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/694#issuecomment-597401365:2148,optimiz,optimizations,2148,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/694#issuecomment-597401365,1,['optimiz'],['optimizations']
Performance,"r/package_linux64/build/usr/share/julia/stdlib/v1.6/Statistics/src/Statistics.jl:131; [14] compute!; @ ~/Oceananigans.jl/src/Fields/averaged_field.jl:62 [inlined]; [15] compute!(avg::AveragedField{Nothing, Nothing, Center, Oceananigans.Fields.FieldStatus{Float64}, GPU, OffsetArrays.OffsetArray{Float64, 3, CuArray{Float64, 3}}, RegularRectilinearGrid{Float64, Periodic, Periodic, Bounded, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}}}}, Float64, 2, Field{Center, Center, Center, GPU, OffsetArrays.OffsetArray{Float64, 3, CuArray{Float64, 3}}, RegularRectilinearGrid{Float64, Periodic, Periodic, Bounded, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}}}}, Float64, NamedTuple{(:x, :y, :z), Tuple{CoordinateBoundaryConditions{BoundaryCondition{Oceananigans.BoundaryConditions.Periodic, Nothing}, BoundaryCondition{Oceananigans.BoundaryConditions.Periodic, Nothing}}, CoordinateBoundaryConditions{BoundaryCondition{Oceananigans.BoundaryConditions.Periodic, Nothing}, BoundaryCondition{Oceananigans.BoundaryConditions.Periodic, Nothing}}, CoordinateBoundaryConditions{BoundaryCondition{Oceananigans.BoundaryConditions.Flux, Nothing}, BoundaryCondition{Oceananigans.BoundaryConditions.Flux, Nothing}}}}}}); @ Oceananigans.Fields ~/Oceananigans.jl/src/Fields/averaged_field.jl:61; [16] top-level scope; @ REPL[35]:1; [17] top-level scope; @ ~/.julia/dev/CUDA/src/initialization.jl:50; ```. using. ```diff; diff --git a/src/mapreduce.jl b/src/mapreduce.jl; index f9489dcf..11c0aee0 100644; --- a/src/mapreduce.jl; +++ b/src/mapreduce.jl; @@ -209,6 +209,10 @@ function GPUArrays.mapreducedim!(f::F, op::OP, R::AnyCuArray{T},; shmem = reduce_shmem; blocks = reduce_blocks*other_blocks; ; + @show threads; + @show blocks; + @show reduce_blocks; +; # perform the actual reduction; if reduce_blocks == 1; # we can cover the dimensions to reduce using a single block; ```",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1684#issuecomment-845602157:10390,perform,perform,10390,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1684#issuecomment-845602157,1,['perform'],['perform']
Performance,"ransformation ordering; warning: /Users/navid/.julia/packages/KernelAbstractions/WoCk1/src/extras/loopinfo.jl:28:0: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering; warning: /Users/navid/.julia/packages/KernelAbstractions/WoCk1/src/extras/loopinfo.jl:28:0: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering; warning: /Users/navid/.julia/packages/KernelAbstractions/WoCk1/src/extras/loopinfo.jl:28:0: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering; warning: /Users/navid/.julia/packages/KernelAbstractions/WoCk1/src/extras/loopinfo.jl:28:0: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering; warning: /Users/navid/.julia/packages/KernelAbstractions/WoCk1/src/extras/loopinfo.jl:28:0: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering; warning: /Users/navid/.julia/packages/KernelAbstractions/WoCk1/src/extras/loopinfo.jl:28:0: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering; NonhydrostaticModel{CPU, RectilinearGrid}(time = 0 seconds, iteration = 0); ├── grid: 1×8×8 RectilinearGrid{Float64, Periodic, Periodic, Bounded} on CPU with 3×3×3 halo; ├── timestepper: QuasiAdamsBashforth2TimeStepper; ├── tracers: (); ├── closure: Nothing; ├── buoyancy: Nothing; └── coriolis: Nothing; ```",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3403#issuecomment-1872469814:3625,optimiz,optimizer,3625,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3403#issuecomment-1872469814,6,"['optimiz', 'perform']","['optimizer', 'perform']"
Performance,"red buoyancy frequency) should be calculated using the local thermal expansion coefficient and haline contraction coefficient (the analytical derivatives of the equation of state with respect to temperature and salinity at constant depth) --- This is (apparently) a more accurate approximation to the buoyancy gradient than calculating the buoyancy gradient with finite differences of buoyancy at different vertical levels. But this means that all equations of state should define functions that return these coefficients. In addition, we will supply a function to calculated the buoyancy frequency squared (I'm currently calling it `N2`). 3. For Boussinesq equations of state, we do not require the non-static components of pressure as input; instead we use the geopotential depth (corresponding to the use of hydrostatic pressure, `ρ₀ g depth`, to calculate compressive / thermobaric effects on density). At the moment we thus only require `-z` as an input --- though we may need to generalize the buoyancy implementation in the future if for some reason calculating the 'geopoential depth' requires more information (like free surface elevation). Luckily, this is a small change that just requires changing the function arguments to `buoyancy_perturbation`; etc. 4. The name `buoyancy_perturbation` is ok. It is actually potentially confusing, since ""buoyancy"" in ordinary usage is *already* associated explicitly with density anomalies --- rather than static, typically constant or z-dependent components of the density profile. So I think calling this function simply `buoyancy` might be better. On the other hand, we probably want to use `model.buoyancy` to refer to the model field that holds buoyancy-related parameters. Because of that we may want to stick with `buoyancy_perturbation`. Any ideas / alternative suggestions? An alternative is to perform a translation in the kernel functions; aka use `buoyancy_params` in the function signature of kernel(s) that call the function `buoyancy`.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/412#issuecomment-532995557:2152,perform,perform,2152,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/412#issuecomment-532995557,1,['perform'],['perform']
Performance,"rent as we will be adding some new pressure solvers soon, including a conjugate-gradient solver by @christophernhill. # Motivation. In PR #290 I implemented a pressure solver for the `(Periodic, Bounded, Bounded)` channel topology using the 2D fast cosine transform algorithm described by Makhoul (1982) as CUFFT does not provide cosine transforms for the GPU and does not support FFTs along non-batched dimensions (see https://github.com/JuliaGPU/CUDA.jl/issues/119). This has been a very unpopular decision for good reasons. The 2D DCT algorithm is quite slow (channels are ~2x slower than doubly periodic on GPUs) and is quite complicated. Due to my inexperience, I didn't realize that transposing the array to do the FFT was the way forward. The pressure solver module is also quite out of date, it hasn't been updated since topologies were introduced (#614) almost exactly a year ago. This PR refactors the pressure solver module to:; 1. Support all topologies on the CPU and GPU performing transposes and index permutations as needed by each transform.; 2. Use the fastest transforms as allowed by the topology. This means batching dimensions when possible.; 3. Consolidating all pressure solvers into a single solver for all topologies. This should simplify the code and make it easier to extend. Resolves #586; Resolves #593; Resolves #594; Resolves #1007. # To batch or not to batch for FFTW on CPUs?. TODO:; - [x] Benchmark 1D {FFT, IFFT}{x, y, z}.; - [x] Benchmark 3D {FFT, IFFT}; - [x] Benchmark 1D {DCT, IDCT}{x, y, z}.; - [x] Benchmark 3D {DCT, IDCT}; - [x] Try N = 16, 64, 256; - [x] Is it faster to do 3 1D transforms or 1 3D transform? Answer: 1 3D transform. To see whether we should just do 1D transforms for everything or whether batching is faster I ran some 1D and 3D FFT benchmarks. The results for triply-periodic are posted below. Based on the benchmarks, it seems that for 256^3 doing three 1D transforms is ~15% slower than doing one 3D transform. So it makes sense to batc",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1338:1108,perform,performing,1108,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1338,1,['perform'],['performing']
Performance,"require_prelocked(uuidkey::Base.PkgId, env::Nothing); │ @ Base ./loading.jl:1812; │ [14] #invoke_in_world#3; │ @ ./essentials.jl:926 [inlined]; │ [15] invoke_in_world; │ @ ./essentials.jl:923 [inlined]; │ [16] _require_prelocked; │ @ ./loading.jl:1803 [inlined]; │ [17] _require_prelocked; │ @ ./loading.jl:1802 [inlined]; │ [18] run_extension_callbacks(extid::Base.ExtensionId); │ @ Base ./loading.jl:1295; │ [19] run_extension_callbacks(pkgid::Base.PkgId); │ @ Base ./loading.jl:1330; │ [20] run_package_callbacks(modkey::Base.PkgId); │ @ Base ./loading.jl:1164; │ [21] _tryrequire_from_serialized(modkey::Base.PkgId, path::String, ocachepath::String, sourcepath::String, depmods::Vector{Any}); │ @ Base ./loading.jl:1487; │ [22] _require_search_from_serialized(pkg::Base.PkgId, sourcepath::String, build_id::UInt128); │ @ Base ./loading.jl:1574; │ [23] _require(pkg::Base.PkgId, env::String); │ @ Base ./loading.jl:1938; │ [24] __require_prelocked(uuidkey::Base.PkgId, env::String); │ @ Base ./loading.jl:1812; │ [25] #invoke_in_world#3; │ @ ./essentials.jl:926 [inlined]; │ [26] invoke_in_world; │ @ ./essentials.jl:923 [inlined]; │ [27] _require_prelocked(uuidkey::Base.PkgId, env::String); │ @ Base ./loading.jl:1803; │ [28] macro expansion; │ @ ./loading.jl:1790 [inlined]; │ [29] macro expansion; │ @ ./lock.jl:267 [inlined]; │ [30] __require(into::Module, mod::Symbol); │ @ Base ./loading.jl:1753; │ [31] #invoke_in_world#3; │ @ ./essentials.jl:926 [inlined]; │ [32] invoke_in_world; │ @ ./essentials.jl:923 [inlined]; │ [33] require(into::Module, mod::Symbol); │ @ Base ./loading.jl:1746; │ [34] eval; │ @ ./boot.jl:385 [inlined]; │ [35] eval_user_input(ast::Any, backend::REPL.REPLBackend, mod::Module); │ @ REPL ~/julia-1.10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:150; │ [36] repl_backend_loop(backend::REPL.REPLBackend, get_module::Function); │ @ REPL ~/julia-1.10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:246; │ [37] start_repl_backend(backend::REPL.REPLBackend, consumer::An",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3686#issuecomment-2272432528:3347,load,loading,3347,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3686#issuecomment-2272432528,1,['load'],['loading']
Performance,"rgs::@Kwargs{stale_age::Int64}); @ FileWatching.Pidfile /glade/u/apps/casper/23.10/spack/opt/spack/julia/1.10.2/gcc/7.5.0/apod/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:111; [8] #invokelatest#2; @ ./essentials.jl:894 [inlined]; [9] invokelatest; @ ./essentials.jl:889 [inlined]; [10] maybe_cachefile_lock(f::Base.var""#968#969""{Base.PkgId}, pkg::Base.PkgId, srcpath::String; stale_age::Int64); @ Base ./loading.jl:2983; [11] maybe_cachefile_lock; @ ./loading.jl:2980 [inlined]; [12] _require(pkg::Base.PkgId, env::String); @ Base ./loading.jl:1970; [13] __require_prelocked(uuidkey::Base.PkgId, env::String); @ Base ./loading.jl:1812; [14] #invoke_in_world#3; @ ./essentials.jl:926 [inlined]; [15] invoke_in_world; @ ./essentials.jl:923 [inlined]; [16] _require_prelocked(uuidkey::Base.PkgId, env::String); @ Base ./loading.jl:1803; [17] macro expansion; @ ./loading.jl:1790 [inlined]; [18] macro expansion; @ ./lock.jl:267 [inlined]; [19] __require(into::Module, mod::Symbol); @ Base ./loading.jl:1753; [20] #invoke_in_world#3; @ ./essentials.jl:926 [inlined]; [21] invoke_in_world; @ ./essentials.jl:923 [inlined]; [22] require(into::Module, mod::Symbol); @ Base ./loading.jl:1746; [23] include; @ ./Base.jl:495 [inlined]; [24] include_package_for_output(pkg::Base.PkgId, input::String, depot_path::Vector{String}, dl_load_path::Vector{String}, load_path::Vector{String}, concrete_deps::Vector{Pair{Base.PkgId, UInt128}}, source::String); @ Base ./loading.jl:2222; [25] top-level scope; @ stdin:3; in expression starting at /glade/u/home/knudsenl/.julia/packages/CUDA/Tl08O/src/CUDA.jl:1; in expression starting at stdin:3; ERROR: LoadError: Failed to precompile CUDA [052768ef-5323-5732-b1bb-66c8b64840ba] to ""/glade/u/home/knudsenl/.julia/compiled/v1.10/CUDA/jl_zRopeZ"".; Stacktrace:; [1] error(s::String); @ Base ./error.jl:35; [2] compilecache(pkg::Base.PkgId, path::String, internal_stderr::IO, internal_stdout::IO, keep_loaded_modules::Bool); @ Base ./loading.jl:2468; [3] compilecac",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2231635812:2287,load,loading,2287,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2231635812,1,['load'],['loading']
Performance,"rgs::@Kwargs{stale_age::Int64}); @ FileWatching.Pidfile /glade/u/apps/casper/23.10/spack/opt/spack/julia/1.10.2/gcc/7.5.0/apod/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:111; [8] #invokelatest#2; @ ./essentials.jl:894 [inlined]; [9] invokelatest; @ ./essentials.jl:889 [inlined]; [10] maybe_cachefile_lock(f::Base.var""#968#969""{Base.PkgId}, pkg::Base.PkgId, srcpath::String; stale_age::Int64); @ Base ./loading.jl:2983; [11] maybe_cachefile_lock; @ ./loading.jl:2980 [inlined]; [12] _require(pkg::Base.PkgId, env::String); @ Base ./loading.jl:1970; [13] __require_prelocked(uuidkey::Base.PkgId, env::String); @ Base ./loading.jl:1812; [14] #invoke_in_world#3; @ ./essentials.jl:926 [inlined]; [15] invoke_in_world; @ ./essentials.jl:923 [inlined]; [16] _require_prelocked(uuidkey::Base.PkgId, env::String); @ Base ./loading.jl:1803; [17] macro expansion; @ ./loading.jl:1790 [inlined]; [18] macro expansion; @ ./lock.jl:267 [inlined]; [19] __require(into::Module, mod::Symbol); @ Base ./loading.jl:1753; [20] #invoke_in_world#3; @ ./essentials.jl:926 [inlined]; [21] invoke_in_world; @ ./essentials.jl:923 [inlined]; [22] require(into::Module, mod::Symbol); @ Base ./loading.jl:1746; [23] include; @ ./Base.jl:495 [inlined]; [24] include_package_for_output(pkg::Base.PkgId, input::String, depot_path::Vector{String}, dl_load_path::Vector{String}, load_path::Vector{String}, concrete_deps::Vector{Pair{Base.PkgId, UInt128}}, source::String); @ Base ./loading.jl:2222; [25] top-level scope; @ stdin:3; in expression starting at /glade/u/home/knudsenl/.julia/packages/Oceananigans/M82LU/src/Oceananigans.jl:1; in expression starting at stdin:3; ERROR: LoadError: Failed to precompile Oceananigans [9e8cae18-63c1-5223-a75c-80ca9d6e9a09] to ""/glade/u/home/knudsenl/.julia/compiled/v1.10/Oceananigans/jl_k7YOZN"".; Stacktrace:; [1] error(s::String); @ Base ./error.jl:35; [2] compilecache(pkg::Base.PkgId, path::String, internal_stderr::IO, internal_stdout::IO, keep_loaded_modules::Bool); @ Base .",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2231635812:4868,load,loading,4868,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2231635812,1,['load'],['loading']
Performance,"right, remaking the performance benchmark is definitely on the todo list!",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3758#issuecomment-2326784686:20,perform,performance,20,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3758#issuecomment-2326784686,1,['perform'],['performance']
Performance,"rom_serialized(modkey::Base.PkgId, path::String, ocachepath::String, sourcepath::String, depmods::Vector{Any}); │ @ Base ./loading.jl:1487; │ [22] _require_search_from_serialized(pkg::Base.PkgId, sourcepath::String, build_id::UInt128); │ @ Base ./loading.jl:1574; │ [23] _require(pkg::Base.PkgId, env::String); │ @ Base ./loading.jl:1938; │ [24] __require_prelocked(uuidkey::Base.PkgId, env::String); │ @ Base ./loading.jl:1812; │ [25] #invoke_in_world#3; │ @ ./essentials.jl:926 [inlined]; │ [26] invoke_in_world; │ @ ./essentials.jl:923 [inlined]; │ [27] _require_prelocked(uuidkey::Base.PkgId, env::String); │ @ Base ./loading.jl:1803; │ [28] macro expansion; │ @ ./loading.jl:1790 [inlined]; │ [29] macro expansion; │ @ ./lock.jl:267 [inlined]; │ [30] __require(into::Module, mod::Symbol); │ @ Base ./loading.jl:1753; │ [31] #invoke_in_world#3; │ @ ./essentials.jl:926 [inlined]; │ [32] invoke_in_world; │ @ ./essentials.jl:923 [inlined]; │ [33] require(into::Module, mod::Symbol); │ @ Base ./loading.jl:1746; │ [34] eval; │ @ ./boot.jl:385 [inlined]; │ [35] eval_user_input(ast::Any, backend::REPL.REPLBackend, mod::Module); │ @ REPL ~/julia-1.10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:150; │ [36] repl_backend_loop(backend::REPL.REPLBackend, get_module::Function); │ @ REPL ~/julia-1.10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:246; │ [37] start_repl_backend(backend::REPL.REPLBackend, consumer::Any; get_module::Function); │ @ REPL ~/julia-1.10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:231; │ [38] run_repl(repl::REPL.AbstractREPL, consumer::Any; backend_on_current_task::Bool, backend::Any); │ @ REPL ~/julia-1.10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:389; │ [39] run_repl(repl::REPL.AbstractREPL, consumer::Any); │ @ REPL ~/julia-1.10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:375; │ [40] (::Base.var""#1013#1015""{Bool, Bool, Bool})(REPL::Module); │ @ Base ./client.jl:432; │ [41] #invokelatest#2; │ @ ./essentials.jl:892 [inlined]; │ [42] invokelatest; │ @ ./esse",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3686#issuecomment-2272432528:3932,load,loading,3932,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3686#issuecomment-2272432528,1,['load'],['loading']
Performance,"rop scaled by; delx, delz. This will be by far the most common model configuration. On Mon, May 27, 2019, 4:04 PM Ali Ramadhan <notifications@github.com> wrote:. > *@ali-ramadhan* commented on this pull request.; > ------------------------------; >; > In src/time_steppers.jl; > <https://github.com/climate-machine/Oceananigans.jl/pull/245#discussion_r287873945>; > :; >; > > @loop for j in (1:grid.Ny; (blockIdx().y - 1) * blockDim().y + threadIdx().y); >; > @loop for i in (1:grid.Nx; (blockIdx().x - 1) * blockDim().x + threadIdx().x); >; > - apply_z_top_bc!(top_bc, i, j, grid, ϕ, Gϕ, κ, t, iteration, u, v, w, T, S); >; > - apply_z_bottom_bc!(bottom_bc, i, j, grid, ϕ, Gϕ, κ, t, iteration, u, v, w, T, S); >; > +; >; > + κ_top = κ(i, j, 1, grid, closure, eos, g, u, v, w, T, S); >; > + κ_bottom = κ(i, j, grid.Nz, grid, closure, eos, g, u, v, w, T, S); >; > +; >; > + apply_z_top_bc!(top_bc, i, j, grid, ϕ, Gϕ, κ_top, t, iteration, u, v, w, T, S); >; > + apply_z_bottom_bc!(bottom_bc, i, j, grid, ϕ, Gϕ, κ_bottom, t, iteration, u, v, w, T, S); >; >; > It looks like if you want to impose a z boundary condition that does not; > depend on κ, you still have to calculate κ using the full closure which; > can be expensive if using an LES closure. Not sure how to get around this,; > probably some clever multiple dispatch?; >; > This is probably fine for now as constant Smagorinsky isn't integrated; > yet, and the performance hit probably isn't big enough to worry about right; > now.; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/climate-machine/Oceananigans.jl/pull/245?email_source=notifications&email_token=AKXUEQURKUBCRVSBQUC2LQDPXQ5FPA5CNFSM4HP6AWMKYY3PNVWWK3TUL52HS4DFWFIHK3DMKJSXC5LFON2FEZLWNFSXPKTDN5WW2ZLOORPWSZGOBZZG3JY#pullrequestreview-242380199>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AKXUEQXS3APE4PYPCYWURM3PXQ5FPANCNFSM4HP6AWMA>; > .; >",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/245#issuecomment-496297122:1481,perform,performance,1481,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/245#issuecomment-496297122,1,['perform'],['performance']
Performance,"rrays/PkHCM/src/GPUArrays.jl:1; [5] top-level scope at /Users/truedichotomy/.julia/packages/GPUArrays/PkHCM/src/GPUArrays.jl:25; [6] include(::Function, ::Module, ::String) at ./Base.jl:380; [7] include(::Module, ::String) at ./Base.jl:368; [8] top-level scope at none:2; [9] eval at ./boot.jl:331 [inlined]; [10] eval(::Expr) at ./client.jl:467; [11] top-level scope at ./none:3; in expression starting at /Users/truedichotomy/.julia/packages/GPUArrays/PkHCM/src/host/abstractarray.jl:24; in expression starting at /Users/truedichotomy/.julia/packages/GPUArrays/PkHCM/src/GPUArrays.jl:25; ERROR: LoadError: Failed to precompile GPUArrays [0c68f7d7-f131-5f86-a1c3-88cf8149b2d7] to /Users/truedichotomy/.julia/compiled/v1.5/GPUArrays/v5u0T_IyCmP.ji.; Stacktrace:; [1] error(::String) at ./error.jl:33; [2] compilecache(::Base.PkgId, ::String) at ./loading.jl:1290; [3] _require(::Base.PkgId) at ./loading.jl:1030; [4] require(::Base.PkgId) at ./loading.jl:928; [5] require(::Module, ::Symbol) at ./loading.jl:923; [6] include(::Function, ::Module, ::String) at ./Base.jl:380; [7] include(::Module, ::String) at ./Base.jl:368; [8] top-level scope at none:2; [9] eval at ./boot.jl:331 [inlined]; [10] eval(::Expr) at ./client.jl:467; [11] top-level scope at ./none:3; in expression starting at /Users/truedichotomy/.julia/packages/CUDA/7vLVC/src/CUDA.jl:5; ERROR: LoadError: Failed to precompile CUDA [052768ef-5323-5732-b1bb-66c8b64840ba] to /Users/truedichotomy/.julia/compiled/v1.5/CUDA/oWw5k_IyCmP.ji.; Stacktrace:; [1] error(::String) at ./error.jl:33; [2] compilecache(::Base.PkgId, ::String) at ./loading.jl:1290; [3] _require(::Base.PkgId) at ./loading.jl:1030; [4] require(::Base.PkgId) at ./loading.jl:928; [5] require(::Module, ::Symbol) at ./loading.jl:923; [6] include(::Function, ::Module, ::String) at ./Base.jl:380; [7] include(::Module, ::String) at ./Base.jl:368; [8] top-level scope at none:2; [9] eval at ./boot.jl:331 [inlined]; [10] eval(::Expr) at ./client.jl:467; [11] top-level s",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/854:1559,load,loading,1559,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/854,1,['load'],['loading']
Performance,"rtically-implicit time-stepping for diffusion terms (numerics). Ocean models typically use a time-stepping method that treats vertical diffusion terms implicitly. We haven't worried about this because we are focused on LES for the most part, or problems with very little diffusion. But ultimately we will need this, especially when we get around to implementing boundary layer closures. We have a tridiagonal solver that works on the GPU, so in some respects the hard work is already done for this problem. 3. Closures for LES and ocean modeling (Dynamic Smagorinsky, Deardorff, k-epsilon, Gent-McWilliams, convective adjustment (?) etc --- physics). We have a need to implement new turbulence closures new and old alike. Gent-McWilliams is probably easy since we already have a Leith closure implemented which calculates the tensor needed to rotate the diffusivity into an isopycnal coordinate. The others are a bit more challenging. 4. Extensive profiling and benchmarking on the GPU to identify bottlenecks / places for improvement in the algorithm (numerics). We might be able to make the code a lot faster (but we aren't sure). 5. A solver for hydrostatic problems with a free surface that uses a split-explicit time-integration method. This is notoriously tricky, but we've made a lot of progress on this for the purposes of the ClimateMachine and it might be interesting to translate what we've learned into Oceananigans. There's also some minor package stuff that I want to work on, such as refactoring the output writer (#963), improving logging / feedback while oceananigans builds a model or runs a simulation (#1013), getting arrays with named axes to work (#457), and figuring out what's going wrong with models that have `Flat` dimensions (#1024). A more fun project is to come up with cool plotting recipes in 1D, 2D, and 3D (!) We'd like to show people how to use Makie (for example) to create really cool visualizations / animations of turbulence. The examples do some animation, but",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1153#issuecomment-724262281:2153,bottleneck,bottlenecks,2153,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1153#issuecomment-724262281,1,['bottleneck'],['bottlenecks']
Performance,"runcation error. The error I is consistent with single precision accuracy. ; (The slopes in the legend are taken from the plot shown below and are not the slopes of the curves from Oceananigans.) . ![convergence_rates_oceananigans](https://user-images.githubusercontent.com/8239041/103024246-79841700-451d-11eb-8321-454171fa2381.png). This plot is similar but computed using my own time-stepping code but it does use `advective_tracer_flux_x`. In developing this code I was able to ensure that everything is double precision and they give the correct slopes. ![convergence_rates](https://user-images.githubusercontent.com/8239041/103024241-7721bd00-451d-11eb-8717-6437c7c2c577.png). **Good news:** The advection schemes in Oceananigans can produce the correct slopes, as predicted by theory. **Bad news:** Some part of Oceananigans (maybe times-stepping?) must use single precision accuracy, and that truncates the error of the method as a whole. Question: where is the bottleneck that reduces the global spatial accuracy from double to single precision?. This third figure shows the result for increased spatial resolution and we observe that the higher order methods saturate near `1e-16`, as you would expect from double precision. ![convergence_rates](https://user-images.githubusercontent.com/8239041/103028959-c7514d00-4526-11eb-94c6-81fb3d429882.png). In case you are interested, these are the calculations of the rates of convergence for the two sets of calculations. ```; Method = Center2ⁿᵈ, Rate of Convergence = 1.99, Expected = 2; Method = CenteredSecondOrder(), Rate of Convergence = 1.99, Expected = 2; Method = Upwind3ʳᵈ, Rate of Convergence = 2.99, Expected = 3; Method = UpwindBiasedThirdOrder(), Rate of Convergence = 3.20, Expected = 3; Method = Center4ᵗʰ, Rate of Convergence = 3.98, Expected = 4; Method = CenteredFourthOrder(), Rate of Convergence = 2.45, Expected = 4; Method = Upwind5ᵗʰ, Rate of Convergence = 4.97, Expected = 5; Method = UpwindBiasedFifthOrder(), Rate of Conv",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1276#issuecomment-750409595:1239,bottleneck,bottleneck,1239,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1276#issuecomment-750409595,1,['bottleneck'],['bottleneck']
Performance,"r{String}, load_path::Vector{String}, concrete_deps::Vector{Pair{Base.PkgId, UInt64}}, source::String); @ Base ./loading.jl:1213; [12] top-level scope; @ none:1; [13] eval; @ ./boot.jl:360 [inlined]; [14] eval(x::Expr); @ Base.MainInclude ./client.jl:446; [15] top-level scope; @ none:1; in expression starting at /home/tomas/.julia/packages/CUDA/3VnCC/src/device/intrinsics/math.jl:5; in expression starting at /home/tomas/.julia/packages/CUDA/3VnCC/src/device/intrinsics.jl:22; in expression starting at /home/tomas/.julia/packages/CUDA/3VnCC/src/CUDA.jl:1; ERROR: LoadError: Failed to precompile CUDA [052768ef-5323-5732-b1bb-66c8b64840ba] to /home/tomas/.julia/compiled/v1.6/CUDA/jl_q4lPlx.; Stacktrace:; [1] error(s::String); @ Base ./error.jl:33; [2] compilecache(pkg::Base.PkgId, path::String, internal_stderr::Base.TTY, internal_stdout::Base.TTY); @ Base ./loading.jl:1360; [3] compilecache(pkg::Base.PkgId, path::String); @ Base ./loading.jl:1306; [4] _require(pkg::Base.PkgId); @ Base ./loading.jl:1021; [5] require(uuidkey::Base.PkgId); @ Base ./loading.jl:914; [6] require(into::Module, mod::Symbol); @ Base ./loading.jl:901; [7] include; @ ./Base.jl:386 [inlined]; [8] include_package_for_output(pkg::Base.PkgId, input::String, depot_path::Vector{String}, dl_load_path::Vector{String}, load_path::Vector{String}, concrete_deps::Vector{Pair{Base.PkgId, UInt64}}, source::String); @ Base ./loading.jl:1213; [9] top-level scope; @ none:1; [10] eval; @ ./boot.jl:360 [inlined]; [11] eval(x::Expr); @ Base.MainInclude ./client.jl:446; [12] top-level scope; @ none:1; in expression starting at /home/tomas/repos/Oceananigans.jl/src/Oceananigans.jl:1; ERROR: LoadError: Failed to precompile Oceananigans [9e8cae18-63c1-5223-a75c-80ca9d6e9a09] to /home/tomas/.julia/compiled/v1.6/Oceananigans/jl_psrPk0.; Stacktrace:; [1] error(s::String); @ Base ./error.jl:33; [2] compilecache(pkg::Base.PkgId, path::String, internal_stderr::Base.TTY, internal_stdout::Base.TTY); @ Base ./loading.jl:1360; [3] c",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1707#issuecomment-849206371:2234,load,loading,2234,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1707#issuecomment-849206371,1,['load'],['loading']
Performance,"s (like the models implemented in OceanBioME) to ""opt-in"" to a substepping scheme. In this design, there's no need to change the existing ""slow"" kernels I don't think. However, we could consider changing the name of them. For example, all that's needed is for the _implementation_ to know when to return a slow source term vs fast source term. > this is where I had to make some more changes to the existing code (and probably the messiest part of what I did), because the code often expects the tendencies to live at timestepper.G / $G^-$, so I made functions that dispatched on the timestepper and usually returned that, but for the new timestepepr returned timestepper.physics.G etc. I'm not sure that new tendencies are needed for substepping. The substepping scheme for CATKE manages to avoid allocating any additional tendencies by preserving the ""slow"" source term:. https://github.com/CliMA/Oceananigans.jl/blob/6c40d7e225c2127051b2703b9c62a8b18260e3a5/src/TurbulenceClosures/turbulence_closure_implementations/TKEBasedVerticalDiffusivities/time_step_catke_equation.jl#L167-L171. The only change that is needed within Oceananigans (in principle) --- as far as I can tell --- is to skip the tracer update for certain tracers (like we do for CATKE and TKEDissipation):. https://github.com/CliMA/Oceananigans.jl/blob/6c40d7e225c2127051b2703b9c62a8b18260e3a5/src/Models/HydrostaticFreeSurfaceModels/hydrostatic_free_surface_ab2_step.jl#L78-L85. Then the implementer of the BGC model has to perform the substepping inside `update_biogeochemical_state!`:. https://github.com/CliMA/Oceananigans.jl/blob/6c40d7e225c2127051b2703b9c62a8b18260e3a5/src/Models/HydrostaticFreeSurfaceModels/update_hydrostatic_free_surface_model_state.jl#L49. Possibly we can go further and define an interface that does the substepping automatically though. I think that an effort like this would be good not to go so far, and first test ideas in a ""minimal"" implementation that simply uses `update_biogeochemical_state!`.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3888#issuecomment-2455670851:2037,perform,perform,2037,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3888#issuecomment-2455670851,1,['perform'],['perform']
Performance,"s than `mean`) and benchmarks show that it is ~20x faster than what we were doing before (copying to CPU and calculating there) which is great but it's ~5x slower than optimal performance. As it does not allocate memory, we can now calculate vertical profiles even when running large models that fill up memory. Although I should mention that an intermediate array with a size of at least `1*Ny*Nz` is required for the parallel reduction step (so I'm using `poisson_solvers.storage` because it's a vanilla CuArray that can be overwritten). ```julia; N, H = 512, 1; T = N + 2H. a = rand(T, T, T) |> CuArray; h = zeros(N) |> CuArray; ```. What we were doing before:; ```julia; @benchmark CuArrays.@sync mean(Array(view(a, H:N+H, H:N+H, H:N+H)), dims=[1, 2]). BenchmarkTools.Trial: ; memory estimate: 1.01 GiB; allocs estimate: 250; --------------; minimum time: 684.013 ms (2.29% GC); median time: 712.570 ms (6.28% GC); mean time: 732.480 ms (8.79% GC); maximum time: 807.437 ms (16.95% GC); --------------; samples: 7; evals/sample: 1; ```. What this PR does:; ```julia; Nx, Ny, Nz = 512, 512, 512; C = rand(Nx, Ny, Nz) |> CuArray; Rx = zeros(Float64, 1, Ny, Nz) |> CuArray; Rxy = zeros(Float64, 1, 1, Nz) |> CuArray. @benchmark CuArrays.@sync @cuda threads=Nx blocks=(Ny, Nz) shmem=2*Nx*sizeof(eltype(C)) gpu_accumulate_xy!(Rxy, Rx, C, +). BenchmarkTools.Trial: ; memory estimate: 2.88 KiB; allocs estimate: 64; --------------; minimum time: 39.129 ms (0.00% GC); median time: 39.245 ms (0.00% GC); mean time: 39.248 ms (0.00% GC); maximum time: 39.374 ms (0.00% GC); --------------; samples: 128; evals/sample: 1; ```. Probably optimal performance:; ```julia; @benchmark CuArrays.@sync mean(a, dims=[1, 2]). BenchmarkTools.Trial: ; memory estimate: 8.56 KiB; allocs estimate: 220; --------------; minimum time: 7.426 ms (0.00% GC); median time: 7.526 ms (0.00% GC); mean time: 7.527 ms (0.00% GC); maximum time: 8.817 ms (0.00% GC); --------------; samples: 663; evals/sample: 1; ```. Resolves #186",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/352:2476,perform,performance,2476,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/352,1,['perform'],['performance']
Performance,"s │ 899.881 ms │ 900.082 ms │ 837.97 KiB │ 1154 │ 6 │; │ CPU │ (128, 128, 128) │ (Periodic, Periodic, Bounded) │ 882.563 ms │ 883.710 ms │ 883.524 ms │ 884.131 ms │ 703.45 KiB │ 1081 │ 6 │; │ CPU │ (128, 128, 128) │ (Bounded, Periodic, Periodic) │ 912.389 ms │ 916.994 ms │ 915.908 ms │ 917.212 ms │ 703.45 KiB │ 1081 │ 6 │; │ CPU │ (128, 128, 128) │ (Periodic, Periodic, Periodic) │ 916.691 ms │ 917.084 ms │ 917.270 ms │ 918.438 ms │ 575.12 KiB │ 1010 │ 6 │; │ CPU │ (128, 128, 128) │ (Bounded, Bounded, Periodic) │ 918.221 ms │ 920.232 ms │ 920.871 ms │ 925.207 ms │ 772.19 KiB │ 1131 │ 6 │; └───────────────┴─────────────────┴────────────────────────────────┴────────────┴────────────┴────────────┴────────────┴────────────┴────────┴─────────┘; [2022/05/03 00:36:02.528] INFO Writing Topologies_benchmarks.html...; Topologies relative performance (CPU); ┌───────────────┬─────────────────┬────────────────────────────────┬──────────┬─────────┬─────────┐; │ Architectures │ Ns │ Topologies │ slowdown │ memory │ allocs │; ├───────────────┼─────────────────┼────────────────────────────────┼──────────┼─────────┼─────────┤; │ CPU │ (128, 128, 128) │ (Bounded, Bounded, Bounded) │ 0.981253 │ 1.45702 │ 1.14257 │; │ CPU │ (128, 128, 128) │ (Bounded, Bounded, Periodic) │ 1.00343 │ 1.34264 │ 1.1198 │; │ CPU │ (128, 128, 128) │ (Bounded, Periodic, Bounded) │ 0.976245 │ 1.34264 │ 1.1198 │; │ CPU │ (128, 128, 128) │ (Bounded, Periodic, Periodic) │ 0.999901 │ 1.22313 │ 1.0703 │; │ CPU │ (128, 128, 128) │ (Periodic, Bounded, Bounded) │ 0.972442 │ 1.34134 │ 1.10396 │; │ CPU │ (128, 128, 128) │ (Periodic, Bounded, Periodic) │ 0.980426 │ 1.22313 │ 1.0703 │; │ CPU │ (128, 128, 128) │ (Periodic, Periodic, Bounded) │ 0.963608 │ 1.22313 │ 1.0703 │; │ CPU │ (128, 128, 128) │ (Periodic, Periodic, Periodic) │ 1.0 │ 1.0 │ 1.0 │; └───────────────┴─────────────────┴────────────────────────────────┴──────────┴─────────┴─────────┘; ```. ## 2D model (256x256x1). ```; Topologies benchmarks; ┌───────────────┬──",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2477#issuecomment-1115745728:7992,perform,performance,7992,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2477#issuecomment-1115745728,1,['perform'],['performance']
Performance,"s │ 910.295 ms │ 914.968 ms │ 837.97 KiB │ 1154 │ 6 │; │ CPU │ (128, 128, 128) │ (Periodic, Periodic, Bounded) │ 886.053 ms │ 892.564 ms │ 894.281 ms │ 904.917 ms │ 703.45 KiB │ 1081 │ 6 │; │ CPU │ (128, 128, 128) │ (Bounded, Periodic, Periodic) │ 910.500 ms │ 924.085 ms │ 923.571 ms │ 931.683 ms │ 703.45 KiB │ 1081 │ 6 │; │ CPU │ (128, 128, 128) │ (Periodic, Periodic, Periodic) │ 914.391 ms │ 916.636 ms │ 916.407 ms │ 917.164 ms │ 575.12 KiB │ 1010 │ 6 │; │ CPU │ (128, 128, 128) │ (Bounded, Bounded, Periodic) │ 923.307 ms │ 925.553 ms │ 926.892 ms │ 932.185 ms │ 772.19 KiB │ 1131 │ 6 │; └───────────────┴─────────────────┴────────────────────────────────┴────────────┴────────────┴────────────┴────────────┴────────────┴────────┴─────────┘; [2022/05/03 01:05:45.458] INFO Writing Topologies_benchmarks.html...; Topologies relative performance (CPU); ┌───────────────┬─────────────────┬────────────────────────────────┬──────────┬─────────┬─────────┐; │ Architectures │ Ns │ Topologies │ slowdown │ memory │ allocs │; ├───────────────┼─────────────────┼────────────────────────────────┼──────────┼─────────┼─────────┤; │ CPU │ (128, 128, 128) │ (Bounded, Bounded, Bounded) │ 0.993457 │ 1.45702 │ 1.14257 │; │ CPU │ (128, 128, 128) │ (Bounded, Bounded, Periodic) │ 1.00973 │ 1.34264 │ 1.1198 │; │ CPU │ (128, 128, 128) │ (Bounded, Periodic, Bounded) │ 0.994272 │ 1.34264 │ 1.1198 │; │ CPU │ (128, 128, 128) │ (Bounded, Periodic, Periodic) │ 1.00813 │ 1.22313 │ 1.0703 │; │ CPU │ (128, 128, 128) │ (Periodic, Bounded, Bounded) │ 0.98639 │ 1.34134 │ 1.10396 │; │ CPU │ (128, 128, 128) │ (Periodic, Bounded, Periodic) │ 0.98391 │ 1.22313 │ 1.0703 │; │ CPU │ (128, 128, 128) │ (Periodic, Periodic, Bounded) │ 0.973739 │ 1.22313 │ 1.0703 │; │ CPU │ (128, 128, 128) │ (Periodic, Periodic, Periodic) │ 1.0 │ 1.0 │ 1.0 │; └───────────────┴─────────────────┴────────────────────────────────┴──────────┴─────────┴─────────┘; ```. ## 2D model (256x256x1). ```; Topologies benchmarks; ┌───────────────┬─────",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2477#issuecomment-1115745728:2326,perform,performance,2326,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2477#issuecomment-1115745728,1,['perform'],['performance']
Performance,se-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; _include at ./loading.jl:2136; include at ./client.jl:489; unknown function (ip: 0x7c00f54ff855); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; jl_apply at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/julia.h:1982 [inlined]; do_call at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:126; eval_value at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:223; eval_stmt_value at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:174 [inlined]; eval_body at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:617; jl_interpret_toplevel_thunk at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:775; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:934; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:877; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:877; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:877; ijl_toplevel_eval_in at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:985; eval at ./boot.jl:385 [inlined]; eval_user_input at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:150; repl_backend_loop at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:246; #start_repl_backend#46 at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/usr/sh,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3852:5728,cache,cache,5728,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3852,1,['cache'],['cache']
Performance,"se.TwicePrecision{Float64}, Int64}}, OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, CPU}, Vector{Int64}, Vector{Periodic}, Int64, Nothing, Nothing}, Oceananigans.Solvers.DiscreteTransform{FFTW.r2rFFTWPlan{ComplexF64, Vector{Int32}, true, 3, Vector{Int64}}, Oceananigans.Solvers.Backward, RectilinearGrid{Float64, Periodic, Periodic, Bounded, Float64, Float64, Float64, OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, CPU}, Int64, Bounded, Float64, Nothing, Nothing}}}}}, Nothing, NamedTuple{(:velocities, :tracers), Tuple{NamedTuple{(:u, :v, :w), Tuple{Oceananigans.Fields.ZeroField{Int64, 3}, Oceananigans.Fields.ZeroField{Int64, 3}, Oceananigans.Fields.ZeroField{Int64, 3}}}, NamedTuple{(), Tuple{}}}}, Nothing, Nothing, Nothing, NamedTuple{(), Tuple{}}}, Δt::Int64; callbacks::Vector{Any}, euler::Bool); @ Oceananigans.TimeSteppers ~/Documents/Projects/Oceananigans.jl/src/TimeSteppers/quasi_adams_bashforth_2.jl:91; </details>. In this example, you could achieve it by just using a normal array forcing, but there are more complicated cases where I would find this useful. I would like to do this for boundary conditions too but removing the `Forcing` from here:https://github.com/CliMA/Oceananigans.jl/blob/29a99a0c235b2f6bf0cec525f2249125ad254ccc/src/BoundaryConditions/boundary_condition.jl#L52. and changing ; https://github.com/CliMA/Oceananigans.jl/blob/29a99a0c235b2f6bf0cec525f2249125ad254ccc/src/BoundaryConditions/boundary_condition.jl#L20. to dispatch on numbers or arrays does not work and loads of the tests fail.",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3237:53085,load,loads,53085,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3237,1,['load'],['loads']
Performance,"se.jl:386 [inlined]; [11] include_package_for_output(pkg::Base.PkgId, input::String, depot_path::Vector{String}, dl_load_path::Vector{String}, load_path::Vector{String}, concrete_deps::Vector{Pair{Base.PkgId, UInt64}}, source::String); @ Base ./loading.jl:1213; [12] top-level scope; @ none:1; [13] eval; @ ./boot.jl:360 [inlined]; [14] eval(x::Expr); @ Base.MainInclude ./client.jl:446; [15] top-level scope; @ none:1; in expression starting at /home/tomas/.julia/packages/CUDA/3VnCC/src/device/intrinsics/math.jl:5; in expression starting at /home/tomas/.julia/packages/CUDA/3VnCC/src/device/intrinsics.jl:22; in expression starting at /home/tomas/.julia/packages/CUDA/3VnCC/src/CUDA.jl:1; ERROR: LoadError: Failed to precompile CUDA [052768ef-5323-5732-b1bb-66c8b64840ba] to /home/tomas/.julia/compiled/v1.6/CUDA/jl_q4lPlx.; Stacktrace:; [1] error(s::String); @ Base ./error.jl:33; [2] compilecache(pkg::Base.PkgId, path::String, internal_stderr::Base.TTY, internal_stdout::Base.TTY); @ Base ./loading.jl:1360; [3] compilecache(pkg::Base.PkgId, path::String); @ Base ./loading.jl:1306; [4] _require(pkg::Base.PkgId); @ Base ./loading.jl:1021; [5] require(uuidkey::Base.PkgId); @ Base ./loading.jl:914; [6] require(into::Module, mod::Symbol); @ Base ./loading.jl:901; [7] include; @ ./Base.jl:386 [inlined]; [8] include_package_for_output(pkg::Base.PkgId, input::String, depot_path::Vector{String}, dl_load_path::Vector{String}, load_path::Vector{String}, concrete_deps::Vector{Pair{Base.PkgId, UInt64}}, source::String); @ Base ./loading.jl:1213; [9] top-level scope; @ none:1; [10] eval; @ ./boot.jl:360 [inlined]; [11] eval(x::Expr); @ Base.MainInclude ./client.jl:446; [12] top-level scope; @ none:1; in expression starting at /home/tomas/repos/Oceananigans.jl/src/Oceananigans.jl:1; ERROR: LoadError: Failed to precompile Oceananigans [9e8cae18-63c1-5223-a75c-80ca9d6e9a09] to /home/tomas/.julia/compiled/v1.6/Oceananigans/jl_psrPk0.; Stacktrace:; [1] error(s::String); @ Base ./error.jl:33; [2",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1707#issuecomment-849206371:2102,load,loading,2102,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1707#issuecomment-849206371,1,['load'],['loading']
Performance,"se.var""#968#969""{Base.PkgId})(); @ Base ./loading.jl:1974; [5] mkpidlock(f::Base.var""#968#969""{Base.PkgId}, at::String, pid::Int32; kwopts::@Kwargs{stale_age::Int64, wait::Bool}); @ FileWatching.Pidfile /glade/u/apps/casper/23.10/spack/opt/spack/julia/1.10.2/gcc/7.5.0/apod/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:93; [6] #mkpidlock#6; @ /glade/u/apps/casper/23.10/spack/opt/spack/julia/1.10.2/gcc/7.5.0/apod/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:88 [inlined]; [7] trymkpidlock(::Function, ::Vararg{Any}; kwargs::@Kwargs{stale_age::Int64}); @ FileWatching.Pidfile /glade/u/apps/casper/23.10/spack/opt/spack/julia/1.10.2/gcc/7.5.0/apod/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:111; [8] #invokelatest#2; @ ./essentials.jl:894 [inlined]; [9] invokelatest; @ ./essentials.jl:889 [inlined]; [10] maybe_cachefile_lock(f::Base.var""#968#969""{Base.PkgId}, pkg::Base.PkgId, srcpath::String; stale_age::Int64); @ Base ./loading.jl:2983; [11] maybe_cachefile_lock; @ ./loading.jl:2980 [inlined]; [12] _require(pkg::Base.PkgId, env::String); @ Base ./loading.jl:1970; [13] __require_prelocked(uuidkey::Base.PkgId, env::String); @ Base ./loading.jl:1812; [14] #invoke_in_world#3; @ ./essentials.jl:926 [inlined]; [15] invoke_in_world; @ ./essentials.jl:923 [inlined]; [16] _require_prelocked(uuidkey::Base.PkgId, env::String); @ Base ./loading.jl:1803; [17] macro expansion; @ ./loading.jl:1790 [inlined]; [18] macro expansion; @ ./lock.jl:267 [inlined]; [19] __require(into::Module, mod::Symbol); @ Base ./loading.jl:1753; [20] #invoke_in_world#3; @ ./essentials.jl:926 [inlined]; [21] invoke_in_world; @ ./essentials.jl:923 [inlined]; [22] require(into::Module, mod::Symbol); @ Base ./loading.jl:1746; [23] include; @ ./Base.jl:495 [inlined]; [24] include_package_for_output(pkg::Base.PkgId, input::String, depot_path::Vector{String}, dl_load_path::Vector{String}, load_path::Vector{String}, concrete_deps::Vector{Pair{Base.PkgId, UInt128}}, source::String); @ Base ./loadi",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2231635812:4332,load,loading,4332,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2231635812,1,['load'],['loading']
Performance,"sing Oceananigans. julia> IncompressibleModel(architecture=GPU(), grid=RegularRectilinearGrid(size=(32, 32, 32), extent=(1, 1, 1))); ```. ```julia; ERROR: cfunction: closures are not supported on this platform; Stacktrace:; [1] compile_method_instance(job::GPUCompiler.CompilerJob, method_instance::Core.MethodInstance); @ GPUCompiler ~/.julia/packages/GPUCompiler/eJOtJ/src/jlgen.jl:325; [2] macro expansion; @ ~/.julia/packages/TimerOutputs/PZq45/src/TimerOutput.jl:226 [inlined]; [3] irgen(job::GPUCompiler.CompilerJob, method_instance::Core.MethodInstance); @ GPUCompiler ~/.julia/packages/GPUCompiler/eJOtJ/src/irgen.jl:4; [4] macro expansion; @ ~/.julia/packages/GPUCompiler/eJOtJ/src/driver.jl:142 [inlined]; [5] macro expansion; @ ~/.julia/packages/TimerOutputs/PZq45/src/TimerOutput.jl:226 [inlined]; [6] macro expansion; @ ~/.julia/packages/GPUCompiler/eJOtJ/src/driver.jl:141 [inlined]; [7] emit_llvm(job::GPUCompiler.CompilerJob, method_instance::Any, world::UInt64; libraries::Bool, deferred_codegen::Bool, optimize::Bool, only_entry::Bool); @ GPUCompiler ~/.julia/packages/GPUCompiler/eJOtJ/src/utils.jl:62; [8] emit_llvm(job::GPUCompiler.CompilerJob, method_instance::Any, world::UInt64); @ GPUCompiler ~/.julia/packages/GPUCompiler/eJOtJ/src/utils.jl:60; [9] cufunction_compile(job::GPUCompiler.CompilerJob); @ CUDA ~/.julia/packages/CUDA/3VnCC/src/compiler/execution.jl:300; [10] check_cache; @ ~/.julia/packages/GPUCompiler/eJOtJ/src/cache.jl:47 [inlined]; [11] cached_compilation; @ ~/.julia/packages/GPUArrays/Z5nPF/src/host/broadcast.jl:57 [inlined]; [12] cached_compilation(cache::Dict{UInt64, Any}, job::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget, CUDA.CUDACompilerParams, GPUCompiler.FunctionSpec{GPUArrays.var""#broadcast_kernel#16"", Tuple{CUDA.CuKernelContext, CUDA.CuDeviceArray{Float64, 3, 1}, Base.Broadcast.Broadcasted{Nothing, Tuple{Base.OneTo{Int64}, Base.OneTo{Int64}, Base.OneTo{Int64}}, typeof(identity), Tuple{Int64}}, Int64}}}, compiler::typeof(CUDA.cufu",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1706:1100,optimiz,optimize,1100,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1706,1,['optimiz'],['optimize']
Performance,so (unknown line); MPI_Isend at /orcd/data/raffaele/001/glwagner/.julia/packages/MPI/TKXAj/src/api/generated_api.jl:2151 [inlined]; Isend at /orcd/data/raffaele/001/glwagner/.julia/packages/MPI/TKXAj/src/pointtopoint.jl:66; Isend at /orcd/data/raffaele/001/glwagner/.julia/packages/MPI/TKXAj/src/pointtopoint.jl:70 [inlined]; Isend at /orcd/data/raffaele/001/glwagner/.julia/packages/MPI/TKXAj/src/pointtopoint.jl:70 [inlined]; send_south_halo at /orcd/data/raffaele/001/glwagner/Oceananigans.jl/src/DistributedComputations/halo_communication.jl:317; #fill_south_and_north_halo!#50 at /orcd/data/raffaele/001/glwagner/Oceananigans.jl/src/DistributedComputations/halo_communication.jl:263; fill_south_and_north_halo! at /orcd/data/raffaele/001/glwagner/Oceananigans.jl/src/DistributedComputations/halo_communication.jl:250; unknown function (ip: 0x2aaac8afa8b6); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; #fill_halo_event!#40 at /orcd/data/raffaele/001/glwagner/Oceananigans.jl/src/DistributedComputations/halo_communication.jl:208; fill_halo_event! at /orcd/data/raffaele/001/glwagner/Oceananigans.jl/src/DistributedComputations/halo_communication.jl:193; unknown function (ip: 0x2aaac8aefb2e); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; #fill_halo_regions!#38 at /orcd/data/raffaele/001/glwagner/Oceananigans.jl/src/DistributedComputations/halo_communication.jl:114; fill_halo_regions! at /orcd/data/raffaele/001/glwagner/Oceananigans.jl/src/DistributedComputations/halo_communication.jl:101 [inlined]; #fill_halo_regions!#37 at /orcd/data/raffaele/001/glwagner/Oceananigans.jl/src/DistributedComputations/halo_communication.jl:90 [inlined]; fill_halo_regions! at /orcd/data/raffaele,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3878:2589,cache,cache,2589,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3878,1,['cache'],['cache']
Performance,"solves to calling a function, whereas calling `getindex` on a `Field` / `Array` fetches data from memory. So they are different. Naively I would expect that evaluating a complicated advection term involving `BackgroundField` would be cheaper than one that involved two concrete `Array`. But sometimes unexpected things happen (the compiler might decide to optimize the code differently...). Can you try running the code on the GPU? I think it would be enlightening to see if there's a slowdown in that case. We could potentially implement an interface whereby `Field`s can be used as `BackgroundField` rather than functions. That might give us some insight, because then the ""additional"" advection terms associated with `BackgroundField` would truly be identical to the ""original"" advection term. > because presumably the advection of the background state by the background state is zero. I would say that the background self-interaction terms are _neglected_ rather than presumed to be zero. Linear terms associated with the background fields are also neglected. The idea being that if there is a valid way to decompose a flow into background and perturbation components, then the equation that governs the background component is completely neglected (this includes both the nonlinear terms and any linear terms involving the background flow). _Side note:_ some nonlinear terms are additionally neglected in the case of nonlinear viscosity. Ideally I think we would include these because the above argument doesn't justify neglecting them, but the implementation seemed too complicated the last time I thought about it so I put it off. I think it's a solvable problem but requires a bit of thought to integrate into the current algorithm seamlessly. > Mu understanding is that it is required for GPU runs, but it also helps optimize CPU runs. This is correct --- if you're missing a `const` on a variable that's used in a kernel, that kernel will fail to compile on the GPU (it won't just be slow).",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1564#issuecomment-816760623:2485,optimiz,optimize,2485,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1564#issuecomment-816760623,1,['optimiz'],['optimize']
Performance,"src/compiler.jl:6657; [7] EnzymeCreatePrimalAndGradient(logic::Enzyme.Logic, todiff::LLVM.Function, retType::Enzyme.API.CDIFFE_TYPE, constant_args::Vector{…}, TA::Enzyme.TypeAnalysis, returnValue::Bool, dretUsed::Bool, mode::Enzyme.API.CDerivativeMode, width::Int64, additionalArg::Ptr{…}, forceAnonymousTape::Bool, typeInfo::Enzyme.FnTypeInfo, uncacheable_args::Vector{…}, augmented::Ptr{…}, atomicAdd::Bool); @ Enzyme.API ~/Projects/Enzymantics/Enzyme.jl/src/api.jl:141; [8] enzyme!(job::GPUCompiler.CompilerJob{…}, mod::LLVM.Module, primalf::LLVM.Function, TT::Type, mode::Enzyme.API.CDerivativeMode, width::Int64, parallel::Bool, actualRetType::Type, wrap::Bool, modifiedBetween::Tuple{…}, returnPrimal::Bool, jlrules::Vector{…}, expectedTapeType::Type, loweredArgs::Set{…}, boxedArgs::Set{…}); @ Enzyme.Compiler ~/Projects/Enzymantics/Enzyme.jl/src/compiler.jl:7715; [9] codegen(output::Symbol, job::GPUCompiler.CompilerJob{…}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, toplevel::Bool, strip::Bool, validate::Bool, only_entry::Bool, parent_job::Nothing); @ Enzyme.Compiler ~/Projects/Enzymantics/Enzyme.jl/src/compiler.jl:9278; [10] codegen; @ Enzyme.Compiler ~/Projects/Enzymantics/Enzyme.jl/src/compiler.jl:8886 [inlined]; [11] _thunk(job::GPUCompiler.CompilerJob{Enzyme.Compiler.EnzymeTarget, Enzyme.Compiler.EnzymeCompilerParams}, postopt::Bool) (repeats 2 times); @ Enzyme.Compiler ~/Projects/Enzymantics/Enzyme.jl/src/compiler.jl:9830; [12] cached_compilation; @ Enzyme.Compiler ~/Projects/Enzymantics/Enzyme.jl/src/compiler.jl:9864 [inlined]; [13] (::Enzyme.Compiler.var""#474#475""{DataType, DataType, DataType, Enzyme.API.CDerivativeMode, Tuple{Bool, Bool, Bool}, Int64, Bool, Bool, UInt64, DataType})(ctx::LLVM.Context); @ Enzyme.Compiler ~/Projects/Enzymantics/Enzyme.jl/src/compiler.jl:9921; [14] JuliaContext(f::Enzyme.Compiler.var""#474#475""{DataType, DataType, DataType, Enzyme.API.CDerivativeMode, Tuple{Bool, Bool, Bool}, Int64, Bool, Bool, UInt64, DataType}); @ GPUC",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3360#issuecomment-1791250259:2313,optimiz,optimize,2313,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3360#issuecomment-1791250259,1,['optimiz'],['optimize']
Performance,"ssentials.jl:926 [inlined]; [21] invoke_in_world; @ ./essentials.jl:923 [inlined]; [22] require(into::Module, mod::Symbol); @ Base ./loading.jl:1746; [23] include; @ ./Base.jl:495 [inlined]; [24] include_package_for_output(pkg::Base.PkgId, input::String, depot_path::Vector{String}, dl_load_path::Vector{String}, load_path::Vector{String}, concrete_deps::Vector{Pair{Base.PkgId, UInt128}}, source::String); @ Base ./loading.jl:2222; [25] top-level scope; @ stdin:3; in expression starting at /glade/u/home/knudsenl/.julia/packages/Oceananigans/M82LU/src/Oceananigans.jl:1; in expression starting at stdin:3; ERROR: LoadError: Failed to precompile Oceananigans [9e8cae18-63c1-5223-a75c-80ca9d6e9a09] to ""/glade/u/home/knudsenl/.julia/compiled/v1.10/Oceananigans/jl_k7YOZN"".; Stacktrace:; [1] error(s::String); @ Base ./error.jl:35; [2] compilecache(pkg::Base.PkgId, path::String, internal_stderr::IO, internal_stdout::IO, keep_loaded_modules::Bool); @ Base ./loading.jl:2468; [3] compilecache; @ ./loading.jl:2340 [inlined]; [4] (::Base.var""#968#969""{Base.PkgId})(); @ Base ./loading.jl:1974; [5] mkpidlock(f::Base.var""#968#969""{Base.PkgId}, at::String, pid::Int32; kwopts::@Kwargs{stale_age::Int64, wait::Bool}); @ FileWatching.Pidfile /glade/u/apps/casper/23.10/spack/opt/spack/julia/1.10.2/gcc/7.5.0/apod/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:93; [6] #mkpidlock#6; @ /glade/u/apps/casper/23.10/spack/opt/spack/julia/1.10.2/gcc/7.5.0/apod/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:88 [inlined]; [7] trymkpidlock(::Function, ::Vararg{Any}; kwargs::@Kwargs{stale_age::Int64}); @ FileWatching.Pidfile /glade/u/apps/casper/23.10/spack/opt/spack/julia/1.10.2/gcc/7.5.0/apod/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:111; [8] #invokelatest#2; @ ./essentials.jl:894 [inlined]; [9] invokelatest; @ ./essentials.jl:889 [inlined]; [10] maybe_cachefile_lock(f::Base.var""#968#969""{Base.PkgId}, pkg::Base.PkgId, srcpath::String; stale_age::Int64); @ Base ./loading.jl:2983; [1",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2231635812:5912,load,loading,5912,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2231635812,1,['load'],['loading']
Performance,"stdout::IO, keep_loaded_modules::Bool); │ @ Base ./loading.jl:2468; │ [3] compilecache; │ @ ./loading.jl:2340 [inlined]; │ [4] (::Base.var""#968#969""{Base.PkgId})(); │ @ Base ./loading.jl:1974; │ [5] mkpidlock(f::Base.var""#968#969""{Base.PkgId}, at::String, pid::Int32; kwopts::@Kwargs{stale_age::Int64, wait::Bool}); │ @ FileWatching.Pidfile ~/julia-1.10/usr/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:93; │ [6] #mkpidlock#6; │ @ ~/julia-1.10/usr/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:88 [inlined]; │ [7] trymkpidlock(::Function, ::Vararg{Any}; kwargs::@Kwargs{stale_age::Int64}); │ @ FileWatching.Pidfile ~/julia-1.10/usr/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:111; │ [8] #invokelatest#2; │ @ ./essentials.jl:894 [inlined]; │ [9] invokelatest; │ @ ./essentials.jl:889 [inlined]; │ [10] maybe_cachefile_lock(f::Base.var""#968#969""{Base.PkgId}, pkg::Base.PkgId, srcpath::String; stale_age::Int64); │ @ Base ./loading.jl:2983; │ [11] maybe_cachefile_lock; │ @ ./loading.jl:2980 [inlined]; │ [12] _require(pkg::Base.PkgId, env::Nothing); │ @ Base ./loading.jl:1970; │ [13] __require_prelocked(uuidkey::Base.PkgId, env::Nothing); │ @ Base ./loading.jl:1812; │ [14] #invoke_in_world#3; │ @ ./essentials.jl:926 [inlined]; │ [15] invoke_in_world; │ @ ./essentials.jl:923 [inlined]; │ [16] _require_prelocked; │ @ ./loading.jl:1803 [inlined]; │ [17] _require_prelocked; │ @ ./loading.jl:1802 [inlined]; │ [18] run_extension_callbacks(extid::Base.ExtensionId); │ @ Base ./loading.jl:1295; │ [19] run_extension_callbacks(pkgid::Base.PkgId); │ @ Base ./loading.jl:1330; │ [20] run_package_callbacks(modkey::Base.PkgId); │ @ Base ./loading.jl:1164; │ [21] _tryrequire_from_serialized(modkey::Base.PkgId, path::String, ocachepath::String, sourcepath::String, depmods::Vector{Any}); │ @ Base ./loading.jl:1487; │ [22] _require_search_from_serialized(pkg::Base.PkgId, sourcepath::String, build_id::UInt128); │ @ Base ./loading.jl:1574; │ [23] _require(pkg::Base.PkgId, env::Strin",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3686#issuecomment-2272432528:2238,load,loading,2238,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3686#issuecomment-2272432528,1,['load'],['loading']
Performance,"stol=1.49e-08, solver iterations: 0, residual: (mean=0.00e+00, abs(max)=0.00e+00); [ Info: ... simulation initialization complete (109.482 ms); [ Info: Executing initial time step...; [ Info: ... initial time step complete (4.840 seconds).; iteration: 1, time: 0.1000, U_max=(1.19e-02, 5.23e-03, 5.19e-03); reltol=1.49e-08, abstol=1.49e-08, solver iterations: 4096, residual: (mean=-1.78e-03, abs(max)=1.06e-02); iteration: 2, time: 0.2000, U_max=(1.21e-02, 4.93e-03, 5.20e-03); reltol=1.49e-08, abstol=1.49e-08, solver iterations: 4096, residual: (mean=-9.30e-04, abs(max)=5.56e-03); iteration: 3, time: 0.3000, U_max=(1.25e-02, 5.02e-03, 5.32e-03); reltol=1.49e-08, abstol=1.49e-08, solver iterations: 4096, residual: (mean=-4.85e-04, abs(max)=2.74e-03); iteration: 4, time: 0.4000, U_max=(6.23e+24, 4.49e+24, 6.19e+24); reltol=1.49e-08, abstol=1.49e-08, solver iterations: 4096, residual: (mean=-2.39e+23, abs(max)=1.51e+24); iteration: 5, time: 0.5000, U_max=(5.48e+179, 4.19e+179, 9.91e+179); reltol=1.49e-08, abstol=1.49e-08, solver iterations: 0, residual: (mean=-1.70e+174, abs(max)=1.13e+181); ERROR: LoadError: time = 0.6, iteration = 6: NaN found in field u. Aborting simulation.; ```. I tried reducing the time step from 0.1 to 0.01 but now it always blows up at iteration 1. Increasing the time step kept it blowing up after ~5 iterations. ---. > You can test this by omitting the preconditioner. Running with `preconditioner = nothing` causes it to always blow up on iteration 1 no matter the time step (after 4096 iterations). ```; [ Info: Initializing simulation...; iteration: 0, time: 0.0000, U_max=(0.00e+00, 0.00e+00, 0.00e+00); reltol=1.49e-08, abstol=1.49e-08, solver iterations: 0, residual: (mean=0.00e+00, abs(max)=0.00e+00); [ Info: ... simulation initialization complete (99.042 ms); [ Info: Executing initial time step...; [ Info: ... initial time step complete (1.896 seconds).; ERROR: LoadError: time = 0.1, iteration = 1: NaN found in field u. Aborting simulation.; ```",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3831#issuecomment-2412573112:3516,Load,LoadError,3516,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3831#issuecomment-2412573112,2,['Load'],['LoadError']
Performance,"strip::Bool, validate::Bool, only_entry::Bool); @ GPUCompiler ~/.julia/packages/GPUCompiler/U36Ed/src/driver.jl:106; [14] compile; @ ~/.julia/packages/GPUCompiler/U36Ed/src/driver.jl:98 [inlined]; [15] #40; @ ~/.julia/packages/AMDGPU/FdIJi/src/compiler/codegen.jl:140 [inlined]; [16] JuliaContext(f::AMDGPU.Compiler.var""#40#41""{GPUCompiler.CompilerJob{GPUCompiler.GCNCompilerTarget, AMDGPU.Compiler.HIPCompilerParams}}); @ GPUCompiler ~/.julia/packages/GPUCompiler/U36Ed/src/driver.jl:47; [17] hipcompile(job::GPUCompiler.CompilerJob); @ AMDGPU.Compiler ~/.julia/packages/AMDGPU/FdIJi/src/compiler/codegen.jl:139; [18] actual_compilation(cache::Dict{Any, AMDGPU.HIP.HIPFunction}, src::Core.MethodInstance, world::UInt64, cfg::GPUCompiler.CompilerConfig{GPUCompiler.GCNCompilerTarget, AMDGPU.Compiler.HIPCompilerParams}, compiler::typeof(AMDGPU.Compiler.hipcompile), linker::typeof(AMDGPU.Compiler.hiplink)); @ GPUCompiler ~/.julia/packages/GPUCompiler/U36Ed/src/execution.jl:125; [19] cached_compilation(cache::Dict{Any, AMDGPU.HIP.HIPFunction}, src::Core.MethodInstance, cfg::GPUCompiler.CompilerConfig{GPUCompiler.GCNCompilerTarget, AMDGPU.Compiler.HIPCompilerParams}, compiler::Function, linker::Function); @ GPUCompiler ~/.julia/packages/GPUCompiler/U36Ed/src/execution.jl:103; [20] macro expansion; @ ~/.julia/packages/AMDGPU/FdIJi/src/compiler/codegen.jl:107 [inlined]; [21] macro expansion; @ ./lock.jl:267 [inlined]; [22] hipfunction(f::GPUArrays.var""#6#7"", tt::Type{Tuple{AMDGPU.ROCKernelContext, AMDGPU.Device.ROCDeviceArray{Float64, 3, 1}, Float64}}; kwargs::@Kwargs{name::Nothing}); @ AMDGPU.Compiler ~/.julia/packages/AMDGPU/FdIJi/src/compiler/codegen.jl:101; [23] hipfunction; @ ~/.julia/packages/AMDGPU/FdIJi/src/compiler/codegen.jl:100 [inlined]; [24] macro expansion; @ ~/.julia/packages/AMDGPU/FdIJi/src/highlevel.jl:157 [inlined]; [25] #gpu_call#49; @ ~/.julia/packages/AMDGPU/FdIJi/src/gpuarrays.jl:8 [inlined]; [26] gpu_call; @ ~/.julia/packages/AMDGPU/FdIJi/src/gpuarrays.jl:5 [",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3468#issuecomment-1935971273:3204,cache,cache,3204,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3468#issuecomment-1935971273,2,['cache'],['cache']
Performance,"t /Users/truedichotomy/.julia/packages/GPUArrays/PkHCM/src/host/abstractarray.jl:24; in expression starting at /Users/truedichotomy/.julia/packages/GPUArrays/PkHCM/src/GPUArrays.jl:25; ERROR: LoadError: Failed to precompile GPUArrays [0c68f7d7-f131-5f86-a1c3-88cf8149b2d7] to /Users/truedichotomy/.julia/compiled/v1.5/GPUArrays/v5u0T_IyCmP.ji.; Stacktrace:; [1] error(::String) at ./error.jl:33; [2] compilecache(::Base.PkgId, ::String) at ./loading.jl:1290; [3] _require(::Base.PkgId) at ./loading.jl:1030; [4] require(::Base.PkgId) at ./loading.jl:928; [5] require(::Module, ::Symbol) at ./loading.jl:923; [6] include(::Function, ::Module, ::String) at ./Base.jl:380; [7] include(::Module, ::String) at ./Base.jl:368; [8] top-level scope at none:2; [9] eval at ./boot.jl:331 [inlined]; [10] eval(::Expr) at ./client.jl:467; [11] top-level scope at ./none:3; in expression starting at /Users/truedichotomy/.julia/packages/CUDA/7vLVC/src/CUDA.jl:5; ERROR: LoadError: Failed to precompile CUDA [052768ef-5323-5732-b1bb-66c8b64840ba] to /Users/truedichotomy/.julia/compiled/v1.5/CUDA/oWw5k_IyCmP.ji.; Stacktrace:; [1] error(::String) at ./error.jl:33; [2] compilecache(::Base.PkgId, ::String) at ./loading.jl:1290; [3] _require(::Base.PkgId) at ./loading.jl:1030; [4] require(::Base.PkgId) at ./loading.jl:928; [5] require(::Module, ::Symbol) at ./loading.jl:923; [6] include(::Function, ::Module, ::String) at ./Base.jl:380; [7] include(::Module, ::String) at ./Base.jl:368; [8] top-level scope at none:2; [9] eval at ./boot.jl:331 [inlined]; [10] eval(::Expr) at ./client.jl:467; [11] top-level scope at ./none:3; in expression starting at /Users/truedichotomy/.julia/packages/Oceananigans/LW3v4/src/Oceananigans.jl:70; ERROR: Failed to precompile Oceananigans [9e8cae18-63c1-5223-a75c-80ca9d6e9a09] to /Users/truedichotomy/.julia/compiled/v1.5/Oceananigans/hU93i_IyCmP.ji.; Stacktrace:; [1] error(::String) at ./error.jl:33; [2] compilecache(::Base.PkgId, ::String) at ./loading.jl:1290; [3] _requir",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/854:1923,Load,LoadError,1923,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/854,1,['Load'],['LoadError']
Performance,t /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/julia.h:1982 [inlined]; do_call at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:126; eval_value at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:223; eval_stmt_value at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:174 [inlined]; eval_body at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:617; jl_interpret_toplevel_thunk at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:775; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:934; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:877; ijl_toplevel_eval_in at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:985; eval at ./boot.jl:385 [inlined]; include_string at ./loading.jl:2076; _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; _include at ./loading.jl:2136; include at ./Base.jl:495; jfptr_include_46447.1 at /orcd/data/raffaele/001/glwagner/Software/julia-1.10.5/lib/julia/sys.so (unknown line); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; exec_options at ./client.jl:318; _start at ./client.jl:552; jfptr__start_82798.1 at /orcd/data/raffaele/001/glwagner/Software/julia-1.10.5/lib/julia/sys.so (unknown line); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; jl_apply at /cache/build/builder-amdci4-4/,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3878:4935,load,loading,4935,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3878,1,['load'],['loading']
Performance,t /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:985; eval at ./boot.jl:385 [inlined]; include_string at ./loading.jl:2076; _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; _include at ./loading.jl:2136; include at ./client.jl:489; unknown function (ip: 0x7c00f54ff855); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; jl_apply at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/julia.h:1982 [inlined]; do_call at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:126; eval_value at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:223; eval_stmt_value at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:174 [inlined]; eval_body at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:617; jl_interpret_toplevel_thunk at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:775; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:934; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:877; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:877; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:877; ijl_toplevel_eval_in at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:985; eval at ./boot.jl:385 [inlined]; eval_user_input at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:150; repl_backe,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3852:5502,cache,cache,5502,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3852,1,['cache'],['cache']
Performance,t /home/alir/.julia/packages/KernelAbstractions/491pi/src/cpu.jl:111; unknown function (ip: 0x7c009050feb3); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; #_#16 at /home/alir/.julia/packages/KernelAbstractions/491pi/src/cpu.jl:46; _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; jl_apply at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/julia.h:1982 [inlined]; do_apply at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/builtins.c:768; Kernel at /home/alir/.julia/packages/KernelAbstractions/491pi/src/cpu.jl:39; _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; advect_lagrangian_particles! at /home/alir/atdepth/Oceananigans.jl/src/Models/LagrangianParticleTracking/lagrangian_particle_advection.jl:193; step_lagrangian_particles! at /home/alir/atdepth/Oceananigans.jl/src/Models/LagrangianParticleTracking/LagrangianParticleTracking.jl:143 [inlined]; step_lagrangian_particles! at /home/alir/atdepth/Oceananigans.jl/src/Models/HydrostaticFreeSurfaceModels/HydrostaticFreeSurfaceModels.jl:107 [inlined]; #time_step!#8 at /home/alir/atdepth/Oceananigans.jl/src/TimeSteppers/quasi_adams_bashforth_2.jl:124; time_step! at /home/alir/atdepth/Oceananigans.jl/src/TimeSteppers/quasi_adams_bashforth_2.jl:76; unknown function (ip: 0x7c00a0f12fbd); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; top-level scope at /home/alir/atdepth/Oceananigans.jl/par,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3852:3254,cache,cache,3254,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3852,1,['cache'],['cache']
Performance,t/test_benchmarks.jl:49; [4] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113; [5] top-level scope at /home/ancellin/.julia/packages/Oceananigans/LW3v4/test/test_benchmarks.jl:13; [6] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.4/Test/src/Test.jl:1113; [7] top-level scope at /home/ancellin/.julia/packages/Oceananigans/LW3v4/test/test_benchmarks.jl:10; BenchmarkTools.Trial: ; memory estimate: 857.91 KiB; allocs estimate: 10652; --------------; minimum time: 5.317 ms (0.00% GC); median time: 5.676 ms (0.00% GC); mean time: 5.798 ms (1.93% GC); maximum time: 122.454 ms (78.68% GC); --------------; samples: 862; evals/sample: 1; BenchmarkTools.Trial: ; memory estimate: 859.72 KiB; allocs estimate: 10768; --------------; minimum time: 12.002 ms (0.00% GC); median time: 12.150 ms (0.00% GC); mean time: 12.156 ms (0.00% GC); maximum time: 14.535 ms (0.00% GC); --------------; samples: 412; evals/sample: 1; Test Summary: | Pass Fail Error Broken Total; Oceananigans | 1579 4 3 2 1588; Grids | 106 106; Operators | 146 146; Boundary conditions | 142 142; Fields | 198 198; Halo regions | 40 40; Solvers | 22 1 23; Pressure solvers | 152 152; Coriolis | 44 44; Buoyancy | 70 70; Surface waves | 1 1; Models | 12 12; Simulations | 26 26; Time stepping | 33 1 34; Time stepping with boundary conditions | 122 122; Forcing | 12 2 14; Forcing function initialization | 5 5; Forcing function time stepping [GPU] | 7 2 9; Turbulence closures | 46 46; Dynamics | 18 18; Diagnostics | 22 22; Output writers | 80 1 81; NetCDF [GPU] | 1 1; JLD2 [GPU] | 9 9; Checkpointer [GPU] | 71 71; Abstract operations | 258 258; Regression | 20 20; Examples | 6 6; Verification | 1 1; Performance benchmarks | 2 4 6; Performance benchmark scripts | 4 4; Static ocean benchmark | 1 1; Channel benchmark | 1 1; Turbulence closures benchmark | 1 1; Tracers benchmark | 1 1; Selected performance benchmarks | 2 2; ```,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/882:119003,Perform,Performance,119003,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/882,3,"['Perform', 'perform']","['Performance', 'performance']"
Performance,"tPass where T<:Union{Nothing, Cassette.Tag{N, X, E} where E where X where N<:Cassette.AbstractContextName} where M where N<:Cassette.AbstractContextName, Any...) in module Cassette at /home/alir_mit_edu/.julia/packages/Cassette/xggAf/src/overdub.jl:508 overwritten in module GPUifyLoops at /home/alir_mit_edu/.julia/packages/Cassette/xggAf/src/overdub.jl:508.; WARNING: Method definition recurse(Cassette.Context{N, M, T, P, B, H} where H<:Union{Cassette.DisableHooks, Nothing} where B<:Union{Nothing, Base.IdDict{Module, Base.Dict{Symbol, Cassette.BindingMeta}}} where P<:Cassette.AbstractPass where T<:Union{Nothing, Cassette.Tag{N, X, E} where E where X where N<:Cassette.AbstractContextName} where M where N<:Cassette.AbstractContextName, Any...) in module Cassette at /home/alir_mit_edu/.julia/packages/Cassette/xggAf/src/overdub.jl:521 overwritten in module GPUifyLoops at /home/alir_mit_edu/.julia/packages/Cassette/xggAf/src/overdub.jl:521.; CUDA-enabled GPU(s) detected:; CuDevice(0): Tesla V100-SXM2-16GB; ERROR: LoadError: TypeError: in setfield!, expected BoundaryCondition{Default,Nothing}, got BoundaryCondition{Flux,Float64}; Stacktrace:; [1] setbc!(::Oceananigans.CoordinateBoundaryConditions{BoundaryCondition{Default,Nothing},BoundaryCondition{Default,Nothing}}, ::Val{:left}, ::BoundaryCondition{Flux,Float64}) at /home/alir_mit_edu/Oceananigans.jl/src/boundary_conditions.jl:85; [2] setproperty!(::Oceananigans.CoordinateBoundaryConditions{BoundaryCondition{Default,Nothing},BoundaryCondition{Default,Nothing}}, ::Symbol, ::BoundaryCondition{Flux,Float64}) at /home/alir_mit_edu/Oceananigans.jl/src/boundary_conditions.jl:84; [3] top-level scope at none:0; [4] include at ./boot.jl:326 [inlined]; [5] include_relative(::Module, ::String) at ./loading.jl:1038; [6] include(::Module, ::String) at ./sysimg.jl:29; [7] exec_options(::Base.JLOptions) at ./client.jl:267; [8] _start() at ./client.jl:436; in expression starting at /home/alir_mit_edu/Oceananigans.jl/newscript.jl:53; ```",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/272:1476,Load,LoadError,1476,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/272,2,"['Load', 'load']","['LoadError', 'loading']"
Performance,t_lagrangian_particles! at /home/alir/atdepth/Oceananigans.jl/src/Models/LagrangianParticleTracking/lagrangian_particle_advection.jl:193; step_lagrangian_particles! at /home/alir/atdepth/Oceananigans.jl/src/Models/LagrangianParticleTracking/LagrangianParticleTracking.jl:143 [inlined]; step_lagrangian_particles! at /home/alir/atdepth/Oceananigans.jl/src/Models/HydrostaticFreeSurfaceModels/HydrostaticFreeSurfaceModels.jl:107 [inlined]; #time_step!#8 at /home/alir/atdepth/Oceananigans.jl/src/TimeSteppers/quasi_adams_bashforth_2.jl:124; time_step! at /home/alir/atdepth/Oceananigans.jl/src/TimeSteppers/quasi_adams_bashforth_2.jl:76; unknown function (ip: 0x7c00a0f12fbd); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; top-level scope at /home/alir/atdepth/Oceananigans.jl/particles_error.jl:37; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:925; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:877; ijl_toplevel_eval_in at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:985; eval at ./boot.jl:385 [inlined]; include_string at ./loading.jl:2076; _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; _include at ./loading.jl:2136; include at ./client.jl:489; unknown function (ip: 0x7c00f54ff855); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; jl_apply at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/julia.h:1982 [inlined]; do_call at /cache/build/builder-amdci4-4/julialang/,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3852:4315,cache,cache,4315,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3852,1,['cache'],['cache']
Performance,"taticArraysStatisticsExt; ✓ UnsafeAtomicsLLVM; ✓ Colors; ✓ GPUArraysCore; ✓ NVTX. ✓ GPUArrays; ✓ KernelAbstractions; ✓ PrettyTables; ✓ GPUCompiler; ✓ DataFrames; ✗ CUDA; 61 dependencies successfully precompiled in 190 seconds. 5 already precompiled. The following 1 direct dependency failed to precompile:. CUDA [052768ef-5323-5732-b1bb-66c8b64840ba]. Failed to precompile CUDA [052768ef-5323-5732-b1bb-66c8b64840ba] to ""/glade/u/home/knudsenl/.julia/compiled/v1.10/CUDA/jl_UQIv2i"".; [45592] signal (11.1): Segmentation fault; in expression starting at /glade/u/home/knudsenl/.julia/packages/CUDA/Tl08O/src/CUDA.jl:25; Allocations: 2907 (Pool: 2898; Big: 9); GC: ERROR: LoadError: Failed to precompile CUDA [052768ef-5323-5732-b1bb-66c8b64840ba] to ""/glade/u/home/knudsenl/.julia/compiled/v1.10/CUDA/jl_CUC33l"".; Stacktrace:; [1] error(s::String); @ Base ./error.jl:35; [2] compilecache(pkg::Base.PkgId, path::String, internal_stderr::IO, internal_stdout::IO, keep_loaded_modules::Bool); @ Base ./loading.jl:2468; [3] compilecache; @ ./loading.jl:2340 [inlined]; [4] (::Base.var""#968#969""{Base.PkgId})(); @ Base ./loading.jl:1974; [5] mkpidlock(f::Base.var""#968#969""{Base.PkgId}, at::String, pid::Int32; kwopts::@Kwargs{stale_age::Int64, wait::Bool}); @ FileWatching.Pidfile /glade/u/apps/casper/23.10/spack/opt/spack/julia/1.10.2/gcc/7.5.0/apod/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:93; [6] #mkpidlock#6; @ /glade/u/apps/casper/23.10/spack/opt/spack/julia/1.10.2/gcc/7.5.0/apod/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:88 [inlined]; [7] trymkpidlock(::Function, ::Vararg{Any}; kwargs::@Kwargs{stale_age::Int64}); @ FileWatching.Pidfile /glade/u/apps/casper/23.10/spack/opt/spack/julia/1.10.2/gcc/7.5.0/apod/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:111; [8] #invokelatest#2; @ ./essentials.jl:894 [inlined]; [9] invokelatest; @ ./essentials.jl:889 [inlined]; [10] maybe_cachefile_lock(f::Base.var""#968#969""{Base.PkgId}, pkg::Base.PkgId, srcpath::String; stale_ag",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2245919472:5202,load,loading,5202,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2245919472,1,['load'],['loading']
Performance,"tdlib/v1.10/FileWatching/src/pidfile.jl:111; │ [8] #invokelatest#2; │ @ ./essentials.jl:894 [inlined]; │ [9] invokelatest; │ @ ./essentials.jl:889 [inlined]; │ [10] maybe_cachefile_lock(f::Base.var""#968#969""{Base.PkgId}, pkg::Base.PkgId, srcpath::String; stale_age::Int64); │ @ Base ./loading.jl:2983; │ [11] maybe_cachefile_lock; │ @ ./loading.jl:2980 [inlined]; │ [12] _require(pkg::Base.PkgId, env::Nothing); │ @ Base ./loading.jl:1970; │ [13] __require_prelocked(uuidkey::Base.PkgId, env::Nothing); │ @ Base ./loading.jl:1812; │ [14] #invoke_in_world#3; │ @ ./essentials.jl:926 [inlined]; │ [15] invoke_in_world; │ @ ./essentials.jl:923 [inlined]; │ [16] _require_prelocked; │ @ ./loading.jl:1803 [inlined]; │ [17] _require_prelocked; │ @ ./loading.jl:1802 [inlined]; │ [18] run_extension_callbacks(extid::Base.ExtensionId); │ @ Base ./loading.jl:1295; │ [19] run_extension_callbacks(pkgid::Base.PkgId); │ @ Base ./loading.jl:1330; │ [20] run_package_callbacks(modkey::Base.PkgId); │ @ Base ./loading.jl:1164; │ [21] _tryrequire_from_serialized(modkey::Base.PkgId, path::String, ocachepath::String, sourcepath::String, depmods::Vector{Any}); │ @ Base ./loading.jl:1487; │ [22] _require_search_from_serialized(pkg::Base.PkgId, sourcepath::String, build_id::UInt128); │ @ Base ./loading.jl:1574; │ [23] _require(pkg::Base.PkgId, env::String); │ @ Base ./loading.jl:1938; │ [24] __require_prelocked(uuidkey::Base.PkgId, env::String); │ @ Base ./loading.jl:1812; │ [25] #invoke_in_world#3; │ @ ./essentials.jl:926 [inlined]; │ [26] invoke_in_world; │ @ ./essentials.jl:923 [inlined]; │ [27] _require_prelocked(uuidkey::Base.PkgId, env::String); │ @ Base ./loading.jl:1803; │ [28] macro expansion; │ @ ./loading.jl:1790 [inlined]; │ [29] macro expansion; │ @ ./lock.jl:267 [inlined]; │ [30] __require(into::Module, mod::Symbol); │ @ Base ./loading.jl:1753; │ [31] #invoke_in_world#3; │ @ ./essentials.jl:926 [inlined]; │ [32] invoke_in_world; │ @ ./essentials.jl:923 [inlined]; │ [33] require(into::Mod",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3686#issuecomment-2272432528:2898,load,loading,2898,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3686#issuecomment-2272432528,1,['load'],['loading']
Performance,"te:. > Some pretty promising Lagrangian particle tracking benchmarks!; >; > Couple of takeaways (all assuming a model with 128^3 grid points and QAB2; > time stepping):; >; > 1. *Low overhead*: You can advect up to ~100,000 particles on the CPU; > and up to ~10,000,000 particles on a (Titan V) GPU before the model slows; > down by more than 30%.; > 2. *Great on GPUs*: Seems that the GPU is great for advecting millions; > of particles. You can advect ~100,000,000 particles and your model only; > slows down by a factor of 4x. In this scenario, the GPU is ~620x faster; > than a single CPU core.; > 3. Calculated using (t_100000000 - t_0) / 100000000, advecting a; > single particle on the CPU takes ~110 ns while on the GPU it only takes; > ~0.127 ns. This seems a little too good to be true but I'll double check; > this.; >; > I'll start refactoring this PR using @glwagner; > <https://github.com/glwagner>'s and @zhenwu0728; > <https://github.com/zhenwu0728>'s feedback, but I think it would be; > really great if we can keep this performance.; > Benchmarks; >; > Oceananigans v0.44.1; > Julia Version 1.5.2; > Commit 539f3ce943 (2020-09-23 23:17 UTC); > Platform Info:; > OS: Linux (x86_64-pc-linux-gnu); > CPU: Intel(R) Xeon(R) Silver 4214 CPU @ 2.20GHz; > WORD_SIZE: 64; > LIBM: libopenlibm; > LLVM: libLLVM-9.0.1 (ORCJIT, cascadelake); > GPU: TITAN V; >; > Lagrangian particle tracking benchmarks; > ┌───────────────┬─────────────┬────────────┬────────────┬────────────┬────────────┬────────────┬────────┐; > │ Architectures │ N_particles │ min │ median │ mean │ max │ memory │ allocs │; > ├───────────────┼─────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────┤; > │ CPU │ 0 │ 361.749 ms │ 364.041 ms │ 364.293 ms │ 368.854 ms │ 293.44 KiB │ 1876 │; > │ CPU │ 1 │ 375.030 ms │ 376.591 ms │ 377.959 ms │ 385.248 ms │ 297.16 KiB │ 1906 │; > │ CPU │ 10 │ 377.251 ms │ 380.792 ms │ 387.560 ms │ 443.325 ms │ 297.16 KiB │ 1906 │; > │ CPU │ 100 │ 378.867 ms │ 3",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1091#issuecomment-732535982:1375,perform,performance,1375,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1091#issuecomment-732535982,1,['perform'],['performance']
Performance,"ted by signal SIGSEGV (Address boundary error); ```. GPU illegal memory access:. ```; [ Info: Skipping precompilation since __precompile__(false). Importing Oceananigans [9e8cae18-63c1-5223-a75c-80ca9d6e9a09].; [ Info: Iteration 1...; [ Info: Iteration 2...; [ Info: Iteration 3...; [ Info: Iteration 4...; [ Info: Iteration 5...; [ Info: Iteration 6...; [ Info: Iteration 7...; [ Info: Iteration 8...; [ Info: Iteration 9...; [ Info: Iteration 10...; [ Info: Iteration 11...; [ Info: Iteration 12...; [ Info: Iteration 13...; [ Info: Iteration 14...; [ Info: Iteration 15...; [ Info: Iteration 16...; [ Info: Iteration 17...; [ Info: Iteration 18...; [ Info: Iteration 19...; [ Info: Iteration 20...; [ Info: Iteration 21...; [ Info: Iteration 22...; [ Info: Iteration 23...; [ Info: Iteration 24...; [ Info: Iteration 25...; [ Info: Iteration 26...; [ Info: Iteration 27...; [ Info: Iteration 28...; [ Info: Iteration 29...; ERROR: LoadError: CUDA error: an illegal memory access was encountered (code 700, ERROR_ILLEGAL_ADDRESS); Stacktrace:; [1] throw_api_error(res::CUDA.cudaError_enum); @ CUDA ~/.julia/packages/CUDA/2kjXI/lib/cudadrv/libcuda.jl:30; [2] check; @ ~/.julia/packages/CUDA/2kjXI/lib/cudadrv/libcuda.jl:37 [inlined]; [3] cuStreamGetCaptureInfo; @ ~/.julia/packages/CUDA/2kjXI/lib/utils/call.jl:34 [inlined]; [4] capture_status(stream::CUDA.CuStream); @ CUDA ~/.julia/packages/CUDA/2kjXI/lib/cudadrv/graph.jl:174; [5] is_capturing; @ ~/.julia/packages/CUDA/2kjXI/lib/cudadrv/graph.jl:179 [inlined]; [6] convert(::Type{CUDA.CuPtr{Float64}}, managed::CUDA.Managed{CUDA.DeviceMemory}); @ CUDA ~/.julia/packages/CUDA/2kjXI/src/memory.jl:539; [7] unsafe_convert; @ ~/.julia/packages/CUDA/2kjXI/src/array.jl:434 [inlined]; [8] #pointer#1123; @ ~/.julia/packages/CUDA/2kjXI/src/array.jl:392 [inlined]; [9] pointer (repeats 2 times); @ ~/.julia/packages/CUDA/2kjXI/src/array.jl:384 [inlined]; [10] unsafe_convert(::Type{CUDA.CuDeviceArray{Float64, 3, 1}}, a::CUDA.CuArray{Float64, 3, CUDA.Dev",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3852:10302,Load,LoadError,10302,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3852,1,['Load'],['LoadError']
Performance,"th `Field`s and `Number`s in `AbstractOperations`, we also define a `FunctionField` type that acts like a field, but invokes a function to compute values at its location under the hood. This type probably needs more tests. We should also investigate whether this type can be used to specify things like background fields. A `FunctionField` can either be time-dependent and a function of `x, y, z, t` (achieved by allowing it to possess a reference to `clock`), or a static function of `x, y, z`. * This PR changes the result of a horizontal average to `HorizontalAverage.result` to provide a common terminology with `Computation`, as well as future reductions along other dimensions (previously the result of a horizontal average was called `profile`). * This PR adapts `Field` to work on the GPU (after the hard work of adapting all the `AbstractOperation`s to work on the GPU, this seemed trivial). That this works now should be tested. If it does indeed work and there is no loss of performance, we can eliminate a lot of cruft from our `time_step!`, and also use fields directly in `AbstractOperation`s (right now we extract the underlying `OffsetArray` instead). # In summary. Miraculously, tests pass on the GPU. However, this framework is quite general and powerful, so we need to . - [x] think carefully about the tests we need (and don't need). . And certainly before merging we also need. - [x] docstrings... . Also, it'd be nice to. - [x] use `AbstractOperation` to define useful output in an example. Some discussion may be warranted about what's exported from `AbstractOperations`; we almost always want to have `Face` and `Cell`, for example. Ultimately, I think it'd be nice to write `using Oceananigans.Fields, Oceananigans.AbstractOperators` to get what's needed for this purpose. But that's yet another future PR. I hope this sparks some discussion about the future of the `Field` abstraction as well. I think it has the potential to be quite powerful. Resolves #454 ; Resolves #428",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/463:7544,perform,performance,7544,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/463,1,['perform'],['performance']
Performance,th/Oceananigans.jl/src/Models/HydrostaticFreeSurfaceModels/HydrostaticFreeSurfaceModels.jl:107 [inlined]; #time_step!#8 at /home/alir/atdepth/Oceananigans.jl/src/TimeSteppers/quasi_adams_bashforth_2.jl:124; time_step! at /home/alir/atdepth/Oceananigans.jl/src/TimeSteppers/quasi_adams_bashforth_2.jl:76; unknown function (ip: 0x7c00a0f12fbd); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; top-level scope at /home/alir/atdepth/Oceananigans.jl/particles_error.jl:37; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:925; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:877; ijl_toplevel_eval_in at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:985; eval at ./boot.jl:385 [inlined]; include_string at ./loading.jl:2076; _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; _include at ./loading.jl:2136; include at ./client.jl:489; unknown function (ip: 0x7c00f54ff855); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; jl_apply at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/julia.h:1982 [inlined]; do_call at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:126; eval_value at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:223; eval_stmt_value at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:174 [inlined]; eval_body at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3852:4665,load,loading,4665,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3852,1,['load'],['loading']
Performance,"the ENO coefficients for reconstruction in the stretched directions (""stretched"" setting) (I have tested ""on the fly"" coefficient calculation for stretched directions and it is way too expensive! therefore, useless to keep as a possibility); - `WENO5(grid = grid, stretched_smoothness=true)` will compute coefficients for the smoothness indicators `β₀, β₁` and `β₂` to account for stretched grid; - `WENO5(zweno = true)` will implement a Z-WENO formulation for the weno weight calculation; - No support is given for `WENO5S(grid = grid)` for curvilinear grids for the moment (defaults to uniform setting). Comments:; Despite the fact that all methods have the same execution speed, `stretched_smoothness` requires more memory (and slightly more computation time) and is not much impactful. As such, most of the time it is better to use just the `WENO5(grid = grid)` keyword argument as it does not decrease accuracy but decreases memory utilization (and speed up slightly). (I haven't tried all types of grids, so maybe it is good to check before performing a large simulation on a weird stretched grid); On the other hand, a Z-WENO formulation is always beneficial (also in case of a uniform mesh) with no major decrease in performance. The same can be said for the stretched `WENO5(grid=grid)` formulation in case of stretched grids. `validation/advection/validate_weno_scheme.jl` compares all these methods in terms of time and accuracy on a simple 1D and 2D tracer advection simulations. Below some animations showing the performance of the three methods for a ""center coarsened"" grid type (`grid_str2`); `U` => `WENO5()` ; `S` => `WENO5(grid = grid)` ; `β` => `WENO5(grid = grid, stretched_smoothness=true)` ; `Z` => `WENO5(grid = grid, stretched_smoothness=true, zweno=true)` . https://user-images.githubusercontent.com/7112768/142819747-4e4083cf-d725-4f1a-bac7-4fb10800ecc7.mp4. https://user-images.githubusercontent.com/7112768/142819755-fec13fb6-684f-4f1a-b7ec-78d9768c4ff7.mp4. Closes #1704",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2060:1442,perform,performing,1442,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2060,3,['perform'],"['performance', 'performing']"
Performance,"the Redi tensor and GM for TwoDimensionalLeith so GM-Redi might; > not need too much extra work:; > https://github.com/CliMA/Oceananigans.jl/blob/master/src/TurbulenceClosures/turbulence_closure_implementations/leith_enstrophy_diffusivity.jl; > Questions/proposal; >; > 1. Do we make the small-angle approximation? I would say yes.; >; > In Oceananigans.jl right now each component of the tensor diffusivity is; > calculated on-the-fly so the unapproximated tensor could end up being 2-4x; > more expensive to compute. We cannot compute the full tensor in one go; > (reusing the isopycnal slopes) and we may not want to since it takes up a; > lot of memory (9 extra field?). But maybe there's a good reason to not make; > the small-angle approximation? We could eventually support both; > (unapproximated and small-angle approximation).; >; > See; > https://mitgcm.readthedocs.io/en/latest/phys_pkgs/gmredi.html#redi-scheme-isopycnal-diffusion; > for a comparison of the two.; >; > 1. Is GM-Redi the main parameterization of interest? Would people use; > the fully symmetric or the fully anti-symmetric diffusivity tensor by; > themselves?; >; > If not then maybe we can save some time by just implementing one new; > closure, GMRedi or GentMcWilliamsRedi. If we take the isopycnal; > diffusivity to be the same for both the symmetric and anti-symmetric tensor; > then we the resulting tensor is quite cheap to compute (only 3 components); > if performance is important:; >; > From; > https://mitgcm.readthedocs.io/en/latest/phys_pkgs/gmredi.html#griffies-skew-flux; > [image: image]; > <https://user-images.githubusercontent.com/20099589/111556957-1abbeb80-8762-11eb-9fb3-870d447efa5f.png>; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/CliMA/Oceananigans.jl/issues/1492>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AKXUEQT2T2NK46W7G7ZS5ULTEFFH5ANCNFSM4ZLUCVJQ>; > .; >",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1492#issuecomment-801541587:2311,perform,performance,2311,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1492#issuecomment-801541587,1,['perform'],['performance']
Performance,"these or fix a possible flaw. I am using the latest versions of Julia and Oceananigans as at 5th March 2024. model = NonhydrostaticModel(; grid,; timestepper = :RungeKutta3,; advection = UpwindBiasedFifthOrder(),; closure = ScalarDiffusivity(ν=1e-5)); warning: C:\Users\kab\.julia\packages\KernelAbstractions\Zcyra\src\extras\loopinfo.jl:28:0: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering; warning: C:\Users\kab\.julia\packages\KernelAbstractions\Zcyra\src\extras\loopinfo.jl:28:0: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering; warning: C:\Users\kab\.julia\packages\KernelAbstractions\Zcyra\src\extras\loopinfo.jl:28:0: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering; warning: C:\Users\kab\.julia\packages\KernelAbstractions\Zcyra\src\extras\loopinfo.jl:28:0: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering; warning: C:\Users\kab\.julia\packages\KernelAbstractions\Zcyra\src\extras\loopinfo.jl:28:0: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering; warning: C:\Users\kab\.julia\packages\KernelAbstractions\Zcyra\src\extras\loopinfo.jl:28:0: loop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering; warning: C:\Users\kab\.julia\packages\KernelAbstractions\Zcyra\src\extras\loopin",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3494:1081,optimiz,optimizer,1081,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3494,2,"['optimiz', 'perform']","['optimizer', 'perform']"
Performance,"this line:. https://github.com/climate-machine/Oceananigans.jl/blob/8dacd119c2638d7034f8cb64a9ca56d721b9a7d7/src/models.jl#L57. throws . ```julia; ERROR: LoadError: ArgumentError: Cannot create a GPU model. No CUDA-enabled GPU was detected!; ```. when `HAVE_CUDA` is false. However, `HAVE_CUDA` is not really false when a device is 'not detected' (as far as I can tell), but rather throws an error when one of `CUDAdrv, CUDAnative, CuArrays` fails to load when `Oceananigans.jl` is built. A better error message might simply say that . ""One of `CUDAdrv, CUDAnative, CuArrays` threw an error at the time that `Oceananigans.jl` was built. If cuda is currently available, try running Pkg.build()."" . If that is, after all, the cause of the error. The issue is that its possible to have a device but not cuda, or to have a device but also some other issue with cuda libraries or julia packages that causes this error, or that cuda was not found *at some point in the past* when Oceananigans was built. We aren't actually detecting any device upon model instantiation, nor are we rechecking (however that would be done...) as far as I can tell.",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/295:154,Load,LoadError,154,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/295,2,"['Load', 'load']","['LoadError', 'load']"
Performance,"this to work seamlessly when a `JLD2OutputWriter` is involved we might need to modify the constructor for `JLD2OutputWriter`. Currently the constructor runs an arbitrary `init(file, model)` function, and saves certain properties identified in the constructor signature:. https://github.com/CliMA/Oceananigans.jl/blob/9cfb568ec3a8e6f7efe8c7c41300ea9673c6b103/src/OutputWriters/jld2_output_writer.jl#L157-L160. This will fail if the output file already exists; thus it's not possible to pickup from a checkpoint with the same script used to initialize the model (without destroying prior output). A simple change could just be to allow `init` and `save_properties!` to fail with `try/catch` (or to avoid running those functions if a file already exists). There are also some shenanigans that'd have to be done for output files split into multiple `part`s. (We could, potentially, require `pickup=true` in the `JLD2OutputWriter` constructor, rather than changing its default behavior to accommodate auto-checkpoint-pickup). Another caveat is that this method of restoring checkpointed data will not work for large CPU models that consume almost all of the CPU memory (such that a single field cannot be loaded from file after `model` has been instantiated). These cases are relatively rare right now, since such large models would typically run very slowly on a typical single node. The basic idea is:. ```julia; # Model and simulation setup. simulation.output_writers[:checkpointer] = Checkpointer(...). run!(simulation, pickup=true); ```. We could also allow `pickup` to be an iteration number, eg. ```julia; # Model and simulation setup. simulation.output_writers[:checkpointer] = Checkpointer(...). run!(simulation, pickup=10138); ```. It may also be possible to enable this functionality with an environment variable; eg. ```bash; PICKUP=true julia --project run_cool_simulation.jl; ```. Note that this design works even if `model.clock.iteration==0`, since the initial checkpoint can be picked up.",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1068:2189,load,loaded,2189,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1068,1,['load'],['loaded']
Performance,"thub.com/CliMA/Oceananigans.jl/blob/29a99a0c235b2f6bf0cec525f2249125ad254ccc/src/Models/HydrostaticFreeSurfaceModels/split_explicit_free_surface.jl#L305-L307. gets triggered. A solution was to add `substeps = nothing`. This is a bit counter-intuitive though. Is there a different way we should have done it? If not, perhaps a good idea is to add a note in the `SplitExplicitFreeSurface` dostring.... Moving on, we got another error down the road. Running this:. ```Julia; using Oceananigans. grid = RectilinearGrid(size = (10, 10), x = (-100, 100), z = (-100, 0), topology = (Periodic, Flat, Bounded)); 								 ; free_surface = SplitExplicitFreeSurface(; grid, cfl = 0.7, substeps = nothing). model = HydrostaticFreeSurfaceModel(; grid, free_surface,; buoyancy = BuoyancyTracer(),; tracers = :b,; momentum_advection = WENO(),; tracer_advection = WENO()). simulation = Simulation(model, Δt=10, stop_time=3600). run!(simulation); ```. spits out . ```julia; ERROR: LoadError: UndefVarError: `settings` not defined; Stacktrace:; [1] calculate_substeps; @ ~/.julia/packages/Oceananigans/pbNSE/src/Models/HydrostaticFreeSurfaceModels/split_explicit_free_surface_kernels.jl:324 [inlined]; [2] iterate_split_explicit!(free_surface::SplitExplicitFreeSurface{Field{Center, Center, Face, Nothing, RectilinearGrid{Float64, Periodic, Flat, Bounded, Float64, Float64, Float64, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, CPU}, Tuple{Colon, Colon, UnitRange{Int64}}, OffsetArrays.OffsetArray{Float64, 3, Array{Float64, 3}}, Float64, FieldBoundaryConditions{BoundaryCondition{Oceananigans.BoundaryConditions.Periodic, Nothing}, BoundaryCondition{Oceananigans.BoundaryConditions.Periodic, Nothing}, Nothing, Nothing, N",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3238:3312,Load,LoadError,3312,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3238,1,['Load'],['LoadError']
Performance,"tion, ::Module, ::String) at ./Base.jl:380; [3] include at ./Base.jl:368 [inlined]; [4] include(::String) at /Users/truedichotomy/.julia/packages/GPUArrays/PkHCM/src/GPUArrays.jl:1; [5] top-level scope at /Users/truedichotomy/.julia/packages/GPUArrays/PkHCM/src/GPUArrays.jl:25; [6] include(::Function, ::Module, ::String) at ./Base.jl:380; [7] include(::Module, ::String) at ./Base.jl:368; [8] top-level scope at none:2; [9] eval at ./boot.jl:331 [inlined]; [10] eval(::Expr) at ./client.jl:467; [11] top-level scope at ./none:3; in expression starting at /Users/truedichotomy/.julia/packages/GPUArrays/PkHCM/src/host/abstractarray.jl:24; in expression starting at /Users/truedichotomy/.julia/packages/GPUArrays/PkHCM/src/GPUArrays.jl:25; ERROR: LoadError: Failed to precompile GPUArrays [0c68f7d7-f131-5f86-a1c3-88cf8149b2d7] to /Users/truedichotomy/.julia/compiled/v1.5/GPUArrays/v5u0T_IyCmP.ji.; Stacktrace:; [1] error(::String) at ./error.jl:33; [2] compilecache(::Base.PkgId, ::String) at ./loading.jl:1290; [3] _require(::Base.PkgId) at ./loading.jl:1030; [4] require(::Base.PkgId) at ./loading.jl:928; [5] require(::Module, ::Symbol) at ./loading.jl:923; [6] include(::Function, ::Module, ::String) at ./Base.jl:380; [7] include(::Module, ::String) at ./Base.jl:368; [8] top-level scope at none:2; [9] eval at ./boot.jl:331 [inlined]; [10] eval(::Expr) at ./client.jl:467; [11] top-level scope at ./none:3; in expression starting at /Users/truedichotomy/.julia/packages/CUDA/7vLVC/src/CUDA.jl:5; ERROR: LoadError: Failed to precompile CUDA [052768ef-5323-5732-b1bb-66c8b64840ba] to /Users/truedichotomy/.julia/compiled/v1.5/CUDA/oWw5k_IyCmP.ji.; Stacktrace:; [1] error(::String) at ./error.jl:33; [2] compilecache(::Base.PkgId, ::String) at ./loading.jl:1290; [3] _require(::Base.PkgId) at ./loading.jl:1030; [4] require(::Base.PkgId) at ./loading.jl:928; [5] require(::Module, ::Symbol) at ./loading.jl:923; [6] include(::Function, ::Module, ::String) at ./Base.jl:380; [7] include(::Module, ",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/854:1409,load,loading,1409,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/854,1,['load'],['loading']
Performance,tions/491pi/src/cpu.jl:144; unknown function (ip: 0x7c0090512182); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; __run at /home/alir/.julia/packages/KernelAbstractions/491pi/src/cpu.jl:111; unknown function (ip: 0x7c009050feb3); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; #_#16 at /home/alir/.julia/packages/KernelAbstractions/491pi/src/cpu.jl:46; _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; jl_apply at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/julia.h:1982 [inlined]; do_apply at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/builtins.c:768; Kernel at /home/alir/.julia/packages/KernelAbstractions/491pi/src/cpu.jl:39; _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; advect_lagrangian_particles! at /home/alir/atdepth/Oceananigans.jl/src/Models/LagrangianParticleTracking/lagrangian_particle_advection.jl:193; step_lagrangian_particles! at /home/alir/atdepth/Oceananigans.jl/src/Models/LagrangianParticleTracking/LagrangianParticleTracking.jl:143 [inlined]; step_lagrangian_particles! at /home/alir/atdepth/Oceananigans.jl/src/Models/HydrostaticFreeSurfaceModels/HydrostaticFreeSurfaceModels.jl:107 [inlined]; #time_step!#8 at /home/alir/atdepth/Oceananigans.jl/src/TimeSteppers/quasi_adams_bashforth_2.jl:124; time_step! at /home/alir/atdepth/Oceananigans.jl/src/TimeSteppers/quasi_adams_bashforth_2.jl:76; unknown function (ip:,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3852:2971,cache,cache,2971,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3852,1,['cache'],['cache']
Performance,"tly focused on IO and has not had the bandwidth to think about how it might interface with packages for (climate/ocean/etc) models. That might be useful down the line! Yes we're both around MIT. I think we're still figuring how we want to do IO in the long-term but will definitely want some way to output NetCDF. > Hey, original author of both https://github.com/JuliaGeo/NetCDF.jl and https://github.com/meggart/ZarrNative.jl here. Regarding the state of NetCDF.jl , yes I would say I mostly stopped developing the package due to time constraints and currently shift my focus towards Zarr since this is what we are using in our current project.; > ; > My last attempt at improving the NetCDF solved many of the issues with the package JuliaGeo/NetCDF.jl#61 but was not merged because of conflicts with other bugfix PRs. However, might be source of inspiration if someone wants to do a rewrite.; > ; > Regarding write performance, I would be very interested to see examples where NetCDF.jl performs worse than e.g. python-netcdf4, since most of the time should be spent in the same NetCDF C library. I have been using the package extensively and did not experience it to be slower than comparable packages.; > ; > I you are worried about the robustness of NetCDF.jl, you should not even look at ZarrNative.jl, since it is still very young and rather a prototype.; > ; > I would be very happy to discuss the issues further, maybe in a call? Would also be interested to learn about your project which seems to be very cool. Thanks so much for working on NetCDF.jl! I didn't mean to sound ungrateful about NetCDF.jl's performance. We were just debating which package to use. With https://github.com/JuliaGeo/NetCDF.jl/issues/87 fixed, I think we'll be happy for a long time. The `compress=9` bug explains why the IO was slow. @glwagner has suggested that for a project of our scale we'd want to help and contribute to the packages we use. We definitely want to stick with NetCDF as it's the _de facto_ s",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/145#issuecomment-476298847:1309,perform,performs,1309,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/145#issuecomment-476298847,1,['perform'],['performs']
Performance,"to discuss on what exactly needs to be implemented. My main reference is https://mitgcm.readthedocs.io/en/latest/phys_pkgs/gmredi.html. It seems that there are three different new closures that could potentially be added:; 1. Redi (symmetric) tensor diffusivity.; 2. Gent-McWilliams (anti-symmetric) tensor diffusivity.; 3. GM-Redi combining the two. Note: @glwagner has already implemented parts of the Redi tensor and GM for `TwoDimensionalLeith` so GM-Redi might not need too much extra work: https://github.com/CliMA/Oceananigans.jl/blob/master/src/TurbulenceClosures/turbulence_closure_implementations/leith_enstrophy_diffusivity.jl. # Questions/proposal. 1. Do we make the small-angle approximation? I would say yes. In Oceananigans.jl right now each component of the tensor diffusivity is calculated on-the-fly so the unapproximated tensor could end up being 2-4x more expensive to compute. We cannot compute the full tensor in one go (reusing the isopycnal slopes) and we may not want to since it takes up a lot of memory (9 extra field?). But maybe there's a good reason to not make the small-angle approximation? We could eventually support both (unapproximated and small-angle approximation). See https://mitgcm.readthedocs.io/en/latest/phys_pkgs/gmredi.html#redi-scheme-isopycnal-diffusion for a comparison of the two. 2. Is GM-Redi the main parameterization of interest? Would people use the fully symmetric or the fully anti-symmetric diffusivity tensor by themselves?. If not then maybe we can save some time by just implementing one new closure, `GMRedi` or `GentMcWilliamsRedi`. If we take the isopycnal diffusivity to be the same for both the symmetric and anti-symmetric tensor then we the resulting tensor is quite cheap to compute (only 3 components) if performance is important:. From https://mitgcm.readthedocs.io/en/latest/phys_pkgs/gmredi.html#griffies-skew-flux; ![image](https://user-images.githubusercontent.com/20099589/111556957-1abbeb80-8762-11eb-9fb3-870d447efa5f.png)",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1492:1982,perform,performance,1982,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1492,1,['perform'],['performance']
Performance,"to two parts, so that we can build a tridiagonal solver even when all three directions are regular. That could be useful for testing, for example.; > ; > I have added this capability by passing the `stretched_direction` kwarg.; > ; > > Also note that in terms of operation count the tridiagonal solve is cheaper than FFT...; > ; > I think, all things considered, the mixed FFT / tridiagonal solve will have basically the same computational cost as the pure FFT solve only for a stretched x direction. The additional transposes required for a y or stretched z direction will completely dominate the cost of the actual operations.; > ; > As an example, this is a slab decomposition with a fairly big grid (512 x 256^2) split on 2 GPUs on Tartarus ![311333172-43dba752-a91f-4b33-8ade-5a6ec57c982b](https://private-user-images.githubusercontent.com/33547697/356260457-1a376c1e-9912-4e00-8dca-91deb46338f8.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjMyMjUwMzMsIm5iZiI6MTcyMzIyNDczMywicGF0aCI6Ii8zMzU0NzY5Ny8zNTYyNjA0NTctMWEzNzZjMWUtOTkxMi00ZTAwLThkY2EtOTFkZWI0NjMzOGY4LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA4MDklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwODA5VDE3MzIxM1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPThhMGNhZDRiOTllYzFjOWFlNGZkMzAwYzI2NDVjY2Y3MzhmYzY5YmU0YzJjOTBlMzI2YzdmNTg4ODNlMGRmMTgmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.6MqQHl0lgCvD7e15b8kOkTduQBQlvSRwfyHAPQuxAps) The AlltoAllv is the dominant cost, while the FFT (in between the two transposes) is quite irrelevant. In the near future, I'll perform scaling tests on Perlmutter, which has a much better network, so it might be that (even if I think it's unlikely) the cost is not all communication after all. Just to clarify --- the mixed tridiagonal + FFT solver also needs eigenvalues, doesn't it?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3689#issuecomment-2278419522:3112,perform,perform,3112,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3689#issuecomment-2278419522,1,['perform'],['perform']
Performance,"tractPass where T<:Union{Nothing, Cassette.Tag{N, X, E} where E where X where N<:Cassette.AbstractContextName} where M where N<:Cassette.AbstractContextName, Any...) in module Cassette at /home/travis/.julia/packages/Cassette/1rVkq/src/overdub.jl:500 overwritten in module GPUifyLoops at /home/travis/.julia/packages/Cassette/1rVkq/src/overdub.jl:500.; WARNING: Method definition recurse(Cassette.Context{N, M, T, P, B, H} where H<:Union{Cassette.DisableHooks, Nothing} where B<:Union{Nothing, Base.IdDict{Module, Base.Dict{Symbol, Cassette.BindingMeta}}} where P<:Cassette.AbstractPass where T<:Union{Nothing, Cassette.Tag{N, X, E} where E where X where N<:Cassette.AbstractContextName} where M where N<:Cassette.AbstractContextName, Any...) in module Cassette at /home/travis/.julia/packages/Cassette/1rVkq/src/overdub.jl:512 overwritten in module GPUifyLoops at /home/travis/.julia/packages/Cassette/1rVkq/src/overdub.jl:512.; ERROR: LoadError: LoadError: UndefVarError: CUBLAS not defined; Stacktrace:; [1] top-level scope at none:0 (repeats 2 times); [2] include at ./boot.jl:326 [inlined]; [3] include_relative(::Module, ::String) at ./loading.jl:1038; [4] include at ./sysimg.jl:29 [inlined]; [5] include(::String) at /home/travis/.julia/packages/CuArrays/qZCAt/src/CuArrays.jl:3; [6] top-level scope at none:0; [7] include at ./boot.jl:326 [inlined]; [8] include_relative(::Module, ::String) at ./loading.jl:1038; [9] include(::Module, ::String) at ./sysimg.jl:29; [10] top-level scope at none:2; [11] eval at ./boot.jl:328 [inlined]; [12] eval(::Expr) at ./client.jl:404; [13] top-level scope at ./none:3; in expression starting at /home/travis/.julia/packages/CuArrays/qZCAt/src/deprecated.jl:5; in expression starting at /home/travis/.julia/packages/CuArrays/qZCAt/src/CuArrays.jl:54; ERROR: LoadError: LoadError: LoadError: LoadError: UndefVarError: @setup not defined; Stacktrace:; [1] top-level scope; [2] #macroexpand#35 at ./expr.jl:107 [inlined]; [3] macroexpand at ./expr.jl:106 [in",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/153#issuecomment-477579168:2611,Load,LoadError,2611,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/153#issuecomment-477579168,2,['Load'],['LoadError']
Performance,tributedComputations/halo_communication.jl:208; fill_halo_event! at /orcd/data/raffaele/001/glwagner/Oceananigans.jl/src/DistributedComputations/halo_communication.jl:193; unknown function (ip: 0x2aaac8aefb2e); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; #fill_halo_regions!#38 at /orcd/data/raffaele/001/glwagner/Oceananigans.jl/src/DistributedComputations/halo_communication.jl:114; fill_halo_regions! at /orcd/data/raffaele/001/glwagner/Oceananigans.jl/src/DistributedComputations/halo_communication.jl:101 [inlined]; #fill_halo_regions!#37 at /orcd/data/raffaele/001/glwagner/Oceananigans.jl/src/DistributedComputations/halo_communication.jl:90 [inlined]; fill_halo_regions! at /orcd/data/raffaele/001/glwagner/Oceananigans.jl/src/DistributedComputations/halo_communication.jl:87; unknown function (ip: 0x2aaac8ad0ee5); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; jl_apply at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/julia.h:1982 [inlined]; do_call at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:126; eval_value at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:223; eval_stmt_value at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:174 [inlined]; eval_body at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:617; jl_interpret_toplevel_thunk at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:775; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:934; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3878:3742,cache,cache,3742,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3878,1,['cache'],['cache']
Performance,"tructuring:. * Break-out ""quick"" unit-tests to be run by github actions under a single configuration (ex: linux, julia 1.6) and be able to run them in parallel. We have a ""Team"" account donated by Github so we can have tons of concurrent GitHub actions so this is beneficial if you can take advantage (might be limited by compilation costs). These tests will be run for every PR push and fast fail on error.; * `bors try` trigger a more expensive CI job to be submitted to the cluster, allowing for GPU / MPI tests. The logic here is that if the cpu tests are not working then the GPU tests almost certainly won't so you can get away with executing them less often. We have a daemon running on the cluster that synchronizes the jobs from buildkite with the local slurm controller, so every step in the buildkite config is submitted as a separate slurm job and canceling buildkite jobs kills them with slum. What is nice about that setup is you can tailor the resources used for each buildkite step just as you would slurm (ex. ""gres:1"" for 1 gpu). You can run jobs on multiple ranks, multiple GPU's, different resource limits, timeouts, etc. basically anything you can pass through to as a cli argument to a slurm batch job is supported. Also it's running on a cluster so obviously your job parallelism is very good.; * `bors r+` trigger merging the PR into `main` branch. This serializes the commits to `main` (and roll-up concurrent PR's to be submitted) so that all merge commits will pass the tests. This is an opportunity to also maybe run more expensive tests (it's easy in buildkite to conditionally run steps if running on `staging` branch) because you'll probably only run the staging CI step one or at most a few times at the very end,. the general strategy is to tier the tests so that they get progressively more expensive and to maximize ci-parallelism to reduce the overall time. @glwagner can control who on the project can submit bors jobs by editing the bors access control settings.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1962#issuecomment-906541778:1570,concurren,concurrent,1570,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1962#issuecomment-906541778,1,['concurren'],['concurrent']
Performance,"try to the *Journal of Open Source Software*. High-level documentation can be found at: https://www.overleaf.com/project/5d73051a46be42000164d1ef. ## Documentation contents; 1. Documentation home page (+ package description); 2. Continuous equations; 3. Numerical methods; 	1. Overview of time-stepping algorithm; 	2. Finite volume method; 	3. Staggered grid; 	4. Fractional step method; 5. Time stepping; 6. Spatial operators; 7. Poisson solvers; 8. Boundary conditions; 9. Turbulence closures; 10. Large eddy simulation; 4. Model setup (description of all the options); 5. Examples; 	1. Simple diffusion; 2. Two-dimensional turbulence; 3. Ocean wind mixing and convection; 4. Ocean convection with plankton; 5. Internal wave; 6. Thermal rising bubble?; 7. Lid-driven cavity?; 8. Eddying channel?; 6. Verification experiments; 1. Taylor-Green vortex; 2. Lid-driven cavity?; 3. Stratified Couette flow; 4. Free convection (Split into ocean and Kato & Phillips?); 7. Gallery (movies!); 8. Performance benchmarks; 9. Documentation of public (+ private?) user interface; 10. References section (if we want to store them all on one page). ## References in the docs; Unfortunately there is no support for bibtex citations/references in Documenter.jl and we have a lot of them =/ This has been brought up by @simonbyrne and @charleskawczynski: https://github.com/climate-machine/CLIMA/issues/152. For now I've just copy pasted stuff from the LaTeX document and left in the `\citet` and `\citep` commands. But we have a few options:; 1. Painstakingly format all the citations and references by hand using Markdown footnotes.; 2. Cite judiciously so we don't have to manually format as many references.; 3. Work on a general solution that integrates with Documenter. I'm leaning towards option 2. EDIT: @johncmarshall54 argues in favor of option 1, which I'm now leaning towards as well. ## Examples. Thanks @glwagner for writing all the examples, they generate beautiful tutorials that we can directly embed ",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/474:1362,Perform,Performance,1362,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/474,1,['Perform'],['Performance']
Performance,"ts...; [2023/02/25 15:42:20.355] INFO Testing shallow water Bickley jet simulation regression [CPU, ConservativeFormulation]; [2023/02/25 15:42:21.128] INFO Initializing simulation...; [2023/02/25 15:42:21.129] INFO ... simulation initialization complete (685.283 μs); [2023/02/25 15:42:21.129] INFO Executing initial time step...; [2023/02/25 15:42:23.148] INFO ... initial time step complete (2.019 seconds).; [2023/02/25 15:42:23.672] INFO Simulation is stopping after running for 2.521 seconds.; [2023/02/25 15:42:23.672] INFO Model iteration 20 equals or exceeds stop iteration 20.; [2023/02/25 15:42:23.814] INFO Δu: min=-1.673332e-04, max=+1.094476e-04, mean=+2.507827e-07, absmean=+5.538768e-06, std=+1.574960e-05 (16384/16384 matching grid points); [2023/02/25 15:42:23.814] INFO Δv: min=-2.878156e-04, max=+1.230686e-04, mean=+1.406671e-07, absmean=+2.141572e-05, std=+4.550342e-05 (256/16512 matching grid points); [2023/02/25 15:42:23.814] INFO Δh: min=-1.126421e-03, max=+1.182485e-03, mean=+9.255018e-09, absmean=+7.901569e-05, std=+2.067916e-04 (16384/16384 matching grid points); Shallow Water Bickley jet simulation [CPU, ConservativeFormulation]: Test Failed at /home/fpoulin/Software/Oceananigans.jl/test/regression_tests/shallow_water_bickley_jet_regression.jl:95; Expression: all(test_fields.v .≈ truth_fields.v); Stacktrace:; [1] macro expansion; @ ~/Software/julia-1.8.2/share/julia/stdlib/v1.8/Test/src/Test.jl:464 [inlined]; [2] run_shallow_water_regression(arch::CPU, formulation::ConservativeFormulation; regenerate_data::Bool); @ Main ~/Software/Oceananigans.jl/test/regression_tests/shallow_water_bickley_jet_regression.jl:95; Test Summary: | Fail Total Time; Shallow Water Regression | 1 1 3.5s; Shallow Water Bickley jet simulation [CPU, ConservativeFormulation] | 1 1 3.5s; ERROR: LoadError: Some tests did not pass: 0 passed, 1 failed, 0 errored, 0 broken.; in expression starting at /home/fpoulin/Software/Oceananigans.jl/test/test_shallow_water_regression.jl:6; ```",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2922#issuecomment-1445209446:3574,Load,LoadError,3574,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2922#issuecomment-1445209446,1,['Load'],['LoadError']
Performance,"ty is not part of the source code, so this doesn't actually verify new aspects of the code (at best, the plots serve to verify the implementation in the script itself).; - Implementing IB functionality and quantitative verification is probably beyond the scope of this PR right now.; - As an example, we can justifiably provide this code with caveats that it's is merely a proof-of-concept, without advertising general IB functionality, and invite users to refine and improve on our example (maybe someone will want to implement better IB functionality in Oceananigans after seeing the example). 2. The method underlying this script's IB implementation is fairly well described in Lai and Peskin (2000): . https://www.sciencedirect.com/science/article/pii/S0021999100964830. See Lai and Peskin (2000)'s equation 7, and note that the implementation in this script uses a ""true"" discrete delta function, rather than a ""regularized"" delta function. As a result, the IB forcing can be implemented point wise, rather than requiring small sums around neighboring elements. Of course, this means that the boundary is not smooth; instead we make a stair-step approximation to the cylinder boundary. . 3. As an aside --- for a general IB implementation, we may have to either evaluate integral stencils, or perform ""precomputation"". Precomputation of arbitrary terms is not supported with Oceananigans's current design, but I think we should eventually support this (we need this, for example, to include averaged terms or general integrated terms on the RHS of our equations, among other things). I think if the method is documented and well-explained (and the user is provided with caveats that this is a proof-of-concept, without formal verification of the method's order of accuracy, etc), this would be a nice example that might pique some user's interests and spur them to contribute, or think creatively about what they can do with Oceananigans and the ability to prescribe arbitrary RHS via functions.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/693#issuecomment-599491196:1602,perform,perform,1602,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/693#issuecomment-599491196,1,['perform'],['perform']
Performance,"tyle = ""text-align: right; "">4.30545</td>; <td style = ""text-align: right; "">3.30755</td>; </tr>; <tr>; <td style = ""text-align: right; "">GPU</td>; <td style = ""text-align: right; "">128</td>; <td style = ""text-align: right; "">(2, 3)</td>; <td style = ""text-align: right; "">1.68306</td>; <td style = ""text-align: right; "">2.25033</td>; <td style = ""text-align: right; "">1.96409</td>; </tr>; <tr>; <td style = ""text-align: right; "">GPU</td>; <td style = ""text-align: right; "">128</td>; <td style = ""text-align: right; "">(2, 5)</td>; <td style = ""text-align: right; "">1.98803</td>; <td style = ""text-align: right; "">2.79892</td>; <td style = ""text-align: right; "">2.35453</td>; </tr>; </table>. ## Equation of state benchmarks. <table>; <caption style = ""text-align: center; "">Equation of state relative performance (CPU)</caption>; <tr class = ""header headerLastRow"">; <th style = ""text-align: right; "">Architectures</th>; <th style = ""text-align: right; "">EquationsOfState</th>; <th style = ""text-align: right; "">slowdown</th>; <th style = ""text-align: right; "">memory</th>; <th style = ""text-align: right; "">allocs</th>; </tr>; <tr>; <td style = ""text-align: right; "">CPU</td>; <td style = ""text-align: right; "">LinearEquationOfState</td>; <td style = ""text-align: right; "">1.0</td>; <td style = ""text-align: right; "">1.0</td>; <td style = ""text-align: right; "">1.0</td>; </tr>; <tr>; <td style = ""text-align: right; "">CPU</td>; <td style = ""text-align: right; "">RoquetEquationOfState</td>; <td style = ""text-align: right; "">1.14378</td>; <td style = ""text-align: right; "">1.00266</td>; <td style = ""text-align: right; "">1.0</td>; </tr>; <tr>; <td style = ""text-align: right; "">CPU</td>; <td style = ""text-align: right; "">TEOS10EquationOfState</td>; <td style = ""text-align: right; "">1.32274</td>; <td style = ""text-align: right; "">1.0</td>; <td style = ""text-align: right; "">1.0</td>; </tr>; </table>. <table>; <caption style = ""text-align: center; "">Equation of state relative performance (GPU)</ca",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1169#issuecomment-725471594:9077,perform,performance,9077,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1169#issuecomment-725471594,1,['perform'],['performance']
Performance,"ub.com/CliMA/OceananigansArtifacts.jl. The easiest way to use them is to load them as a data dependency with DataDeps.jl: https://github.com/CliMA/Oceananigans.jl/blob/master/test/data_dependencies.jl. This works well for the CS32 and CS96 grids, but for much larger grids like the CS510 the grid file is ~200 MiB uncompressed (~118 MiB compressed with JLD2's `compress=true`) which is bigger than GitHub's 100 MiB file size limit. 200 MiB for a grid file is also a bit cumbersome. Right now I'm thinking of hosting CS510 on the engaging cluster, although git lfs for OceananigansArtifacts.jl may be an option. It would be nice to be able to generate conformal cubed sphere grid files to make it easier for users to use cubed sphere grids, and also for the added flexibility of not being limited to three common resolutions (CS32, CS96, and CS510). It would also be good to keep the ability to load a cubed sphere grid from file since we may want to do this for other grids besides the cubed sphere in the future (lat-lon-cap or LLC grids?), and it would be useful to test that the grids we generate are indeed correct by comparing with the grid files. I'm opening this issue just to document what we know about conformal cubed sphere grid generation. It's not a particularly urgent issue. # Computing grid metrics. We already have some code that generates conformal cubed sphere grids with the coordinates, but they are missing the grid metrics (grid spacings and areas). @christophernhill has pointed out these MITgcm MATLAB scripts that may just be what we need to compute the grid metrics. http://wwwcvs.mitgcm.org/viewvc/MITgcm/MITgcm_contrib/high_res_cube/matlab-grid-generator/README?revision=1.1.1.1&view=markup; http://wwwcvs.mitgcm.org/viewvc/MITgcm/MITgcm_contrib/high_res_cube/matlab-grid-generator/calc_fvgrid.m?revision=1.1.1.1&view=markup; http://wwwcvs.mitgcm.org/viewvc/MITgcm/MITgcm_contrib/high_res_cube/matlab-grid-generator/gengrid_fn.m?revision=1.1.1.1&view=markup. He also ment",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1586:1065,load,load,1065,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1586,1,['load'],['load']
Performance,"ulia/packages/MPI/08SPr/src/MPI.jl:36; [14] include; @ ./Base.jl:418 [inlined]; [15] include_package_for_output(pkg::Base.PkgId, input::String, depot_path::Vector{String}, dl_load_path::Vector{String}, load_path::Vector{String}, concrete_deps::Vector{Pair{Base.PkgId, UInt64}}, source::String); @ Base ./loading.jl:1318; [16] top-level scope; @ none:1; [17] eval; @ ./boot.jl:373 [inlined]; [18] eval(x::Expr); @ Base.MainInclude ./client.jl:453; [19] top-level scope; @ none:1; during initialization of module MPICH_jll; in expression starting at /Users/sean/.julia/packages/MPI/08SPr/deps/deps.jl:1; ERROR: LoadError: Failed to precompile MPI [da04e1cc-30fd-572f-bb4f-1f8673147195] to /Users/sean/.julia/compiled/v1.7/MPI/jl_AfEwik.; Stacktrace:; [1] error(s::String); @ Base ./error.jl:33; [2] compilecache(pkg::Base.PkgId, path::String, internal_stderr::IO, internal_stdout::IO, ignore_loaded_modules::Bool); @ Base ./loading.jl:1466; [3] compilecache(pkg::Base.PkgId, path::String); @ Base ./loading.jl:1410; [4] _require(pkg::Base.PkgId); @ Base ./loading.jl:1120; [5] require(uuidkey::Base.PkgId); @ Base ./loading.jl:1013; [6] require(into::Module, mod::Symbol); @ Base ./loading.jl:997; [7] include(mod::Module, _path::String); @ Base ./Base.jl:418; [8] include(x::String); @ Oceananigans ~/.julia/packages/Oceananigans/jmNfq/src/Oceananigans.jl:5; [9] top-level scope; @ ~/.julia/packages/Oceananigans/jmNfq/src/Oceananigans.jl:190; [10] include; @ ./Base.jl:418 [inlined]; [11] include_package_for_output(pkg::Base.PkgId, input::String, depot_path::Vector{String}, dl_load_path::Vector{String}, load_path::Vector{String}, concrete_deps::Vector{Pair{Base.PkgId, UInt64}}, source::Nothing); @ Base ./loading.jl:1318; [12] top-level scope; @ none:1; [13] eval; @ ./boot.jl:373 [inlined]; [14] eval(x::Expr); @ Base.MainInclude ./client.jl:453; [15] top-level scope; @ none:1; in expression starting at /Users/sean/.julia/packages/Oceananigans/jmNfq/src/Distributed/Distributed.jl:1; in express",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2480:4674,load,loading,4674,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2480,1,['load'],['loading']
Performance,"und + deviation. If that's not the case then the boundary conditions are set up correctly. I believe the next thing to try is the same set up but without `BackgroundField` and imposing the Neumann boundary conditions associated with the buoyancy frequency. If that behaves correctly, then the problem would appear to be in `BackgroundField`, as you suspect. . **Update:** ; I made the following changes,. - Removed the background field from model,; - Added `N^2 * (z + D)` into the initial conditiosn of the buoyancy,; - Imposed `buoyancy_gradient_bc` at the top and bottom of the domain. Unfortunately, the same problems occur at the boundary. . This does not seem to be imposing the correct boundary conditions on a tracer field at the top and bottom. **Periodic condition:**; I did try setting the vertical direction to periodic and unfortunately that gave an error, copied below. . Is this a seperate problem or do people think it's related?. ```; ERROR: LoadError: ArgumentError: batching dims must be sequential; Stacktrace:; [1] create_plan(::CUDA.CUFFT.cufftType_t, ::Tuple{Int64,Int64,Int64}, ::Array{Int64,1}) at /home/fpoulin/.julia/packages/CUDA/wTQsK/lib/cufft/fft.jl:140; [2] plan_fft! at /home/fpoulin/.julia/packages/CUDA/wTQsK/lib/cufft/fft.jl:256 [inlined]; [3] plan_forward_transform at /home/fpoulin/software/Oceananigans.jl/src/Solvers/plan_transforms.jl:42 [inlined]; [4] plan_transforms(::GPU, ::RegularRectilinearGrid{Float64,Flat,Bounded,Periodic,OffsetArrays.OffsetArray{Float64,1,StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}}}, ::CUDA.CuArray{Complex{Float64},3}, ::UInt32) at /home/fpoulin/software/Oceananigans.jl/src/Solvers/plan_transforms.jl:106; [5] Oceananigans.Solvers.FFTBasedPoissonSolver(::GPU, ::RegularRectilinearGrid{Float64,Flat,Bounded,Periodic,OffsetArrays.OffsetArray{Float64,1,StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}}}, ::UInt32) at /home/fpoulin/software/Oceananigans.jl/src/Solv",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1554#issuecomment-815670594:1092,Load,LoadError,1092,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1554#issuecomment-815670594,1,['Load'],['LoadError']
Performance,"up Ratio"", title=""Incompressible Model Benchmarks: CPU/GPU""); benchmark/benchmark_incompressible_model.jl:benchmarks_pretty_table(df, title=""Incompressible model benchmarks""); benchmark/benchmark_incompressible_model.jl: benchmarks_pretty_table(df_Δ, title=""Incompressible model CPU to GPU speedup""); benchmark/strong_scaling_incompressible_model.jl: @info ""Benchmarking distributed incompressible model strong scaling with $(typeof(decomposition)) decomposition [N=($Nx, $Ny, $Nz), ranks=($Rx, $Ry, $Rz)]...""; benchmark/strong_scaling_incompressible_model.jl:benchmarks_pretty_table(df, title=""Incompressible model strong scaling benchmark""); benchmark/strong_scaling_incompressible_model.jl:benchmarks_pretty_table(df_Δ, title=""Incompressible model strong scaling speedup""); benchmark/README.md:Running the `benchmark_regression.jl` script will run the incompressible model tests on the current branch and on the master branch for comparison. This is useful to test whether the current branch slows down the code or introduces any performance regression.; benchmark/benchmark_vertically_stretched_incompressible_model.jl:benchmarks_pretty_table(df, title=""Vertically-stretched incompressible model benchmarks""); benchmark/benchmark_vertically_stretched_incompressible_model.jl: benchmarks_pretty_table(df_Δ, title=""Vertically-stretched incompressible model CPU to GPU speedup""); benchmark/strong_scaling_incompressible_model_single.jl:@info ""Setting up distributed incompressible model with N=($Nx, $Ny, $Nz) grid points and ranks=($Rx, $Ry, $Rz) ($decomposition decomposition) on rank $local_rank...""; benchmark/strong_scaling_incompressible_model_single.jl:@info ""Warming up distributed incompressible model on rank $local_rank...""; benchmark/strong_scaling_incompressible_model_single.jl:@info ""Benchmarking distributed incompressible model on rank $local_rank...""; docs/src/physics/incompressible_model.md:# Incompressible model; docs/src/numerical_implementation/time_stepping.md:where, e.g., ",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1870#issuecomment-882146326:1344,perform,performance,1344,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1870#issuecomment-882146326,1,['perform'],['performance']
Performance,"uple{(:T, :S), Tuple{Oceananigans.Fields.ZeroField{Int64, 3}, Oceananigans.Fields.ZeroField{Int64, 3}}}}}, LagrangianParticles{StructVector{CustomParticle, NamedTuple{(:x, :y, :z, :T), NTuple{4, Vector{Float64}}}, Int64}, Float64, NamedTuple{(:T,), Tuple{Vector{Float64}}}, typeof(Oceananigans.Models.LagrangianParticleTracking.no_dynamics), Nothing}, Nothing, Nothing, NamedTuple{(), Tuple{}}}, Δt::Float64); @ Oceananigans.Models.LagrangianParticleTracking ~/.julia/packages/Oceananigans/17XSY/src/Models/LagrangianParticleTracking/update_lagrangian_particle_properties.jl:31; [10] step_lagrangian_particles!; @ ~/.julia/packages/Oceananigans/17XSY/src/Models/LagrangianParticleTracking/LagrangianParticleTracking.jl:137 [inlined]; [11] step_lagrangian_particles!. ```. (2) I also ran (x,y,z) position tracking only while immersed boundary was active. There was a method error relating to cpu__advect_particle >>advect_lagrangian_particles:. ```Julia. ERROR: LoadError: MethodError: no method matching cpu__advect_particles!(::KernelAbstractions.CompilerMetadata{KernelAbstractions.NDIteration.StaticSize{(2000,)}, KernelAbstractions.NDIteration.DynamicCheck, CartesianIndex{1}, Nothing, KernelAbstractions.NDIteration.NDRange{1, KernelAbstractions.NDIteration.StaticSize{(8,)}, KernelAbstractions.NDIteration.StaticSize{(256,)}, Nothing, Nothing}}, ::StructVector{Oceananigans.Models.LagrangianParticleTracking.Particle, NamedTuple{(:x, :y, :z), Tuple{Vector{Float64}, Vector{Float64}, Vector{Float64}}}, Int64}, ::Float64, ::ImmersedBoundaryGrid{Float64, Bounded, Bounded, Bounded, RectilinearGrid{Float64, Bounded, Bounded, Bounded, Float64, Float64, Float64, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3609:31803,Load,LoadError,31803,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3609,1,['Load'],['LoadError']
Performance,"ure out a clean way of integrating `fill_horizontal_velocity_halos!` with the existing time-stepping. Perhaps we just need a `maybe_fill_horizontal_velocity_halos!` function that only does something on a connected cubed sphere. 4. We need values for the grid coordinates `λᶜᶠᵃ`, `λᶠᶜᵃ`, `φᶜᶠᵃ`, and `φᶠᶜᵃ` so that we can use `set!` for velocities on the cubed sphere. They are not provided by the file but we should be able to convert the available coordinates back to the cubed sphere face in (ξ, η) coordinates where -1 ≤ ξ, η ≤ 1 which forms a regular grid then interpolate in between and back to the sphere to get the other staggered grid coordinates. 5. We need to use the sine and cosine of the local angle between the grid orientation (e.g., u-velocity) and zonal west-east direction at the grid-cell centers to `set!` velocity fields. Right now we're limited to setting velocity fields from a streamfunction... 6. Should we be filling the halos of the grid metrics? Seems like they should be filled like velocity halos but without any sign changes. 7. Make sure that calling `fill_halo_regions!` on a horizontal velocity field only fills the halos with non-`CubedSphereExchange` boundary conditions. 8. Add the cubed sphere passive tracer advection and surface gravity waves validation experiments to validation CI pipeline. 9. Figure out a way to abstract and clean up `fill_horizontal_velocity_halos!` and `fill_*_halo!`... 10. Merge utils from `src/CubedSpheres/cubed_sphere_utils.jl` and `src/Distributed/distributed_utils.jl` into `Oceananigans.Grids`. 11. Figure out if the tests in `test_cubed_sphere_halo_exchange.jl` can be abstracted and run for all six faces. It's tedious to have to type them out but could be clearer and easier to debug in its current form... 12. Fix tests!. 13. I think cubed sphere performance can be improved, especially for halo filling. We might need some benchmarks and profiling to figure out where cubed sphere simulations are allocating too much memory.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1522#issuecomment-816631979:2202,perform,performance,2202,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1522#issuecomment-816631979,1,['perform'],['performance']
Performance,"ures, such as sinking/rising velocities, or reaction systems for biological/chemical tracer systems. When I have talked to various people about this, there was a concern that this system would be 'inelegant' or 'complex'. However I believe an equation abstraction system provides the opposite: with an abstraction system, equations are 'written down' in some logical place (like a file `equations.jl` in the `src` directory where they can be easily read and modified, rather than buried inside a time-stepping loop. Correspondly, our time-stepping code becomes shorter and more concise. Using multiple dispatch correctly, we avoid the `infinite if-statement` problem. This abstraction may also make the code more modular such that we move closer to supporting multiple time-steppers. Below I provide one example of an implementation that would solve some of the problems I listed. However, *this is not the only solution*, and I think we should expend some intellectual effort and have a discussion about what the best solution might be, so that we design something that is nice, easy to extend, performant, and powerful. ## A list of kernel equations in a named tuple. The simplest solution for this abstraction is probably just to add new fields to `Model` (`model.equations.velocities` and `model.equations.tracers`) that are named tuples of kernel equations. An example of how this might work while demonstrating hierarchical multiple dispatch is:. ```julia; forcing(i, j, k, grid, F::Function, u, v, w, T, S) = F(grid, u, v, w, T, S, i, j, k); forcing(i, j, k, grid, F::AbstractArray, u, v, w, T, S) = F[i, j, k]. u_eqn(i, j, k, grid, etc...) = (-u∇u(grid, u, v, w, i, j, k); + fv(grid, v, fCor, i, j, k); - δx_c2f(grid, pHY′, i, j, k) / (Δx * ρ₀); + ∂ⱼ_2ν_Σ₁ⱼ(i, j, k, grid, closure, eos, grav, u, v, w, T, S); + forcing(i, j, k, grid, F, u, v, w, T, S); ). # Note omission of pressure term here; u_eqn(i, j, k, grid, pHY::Nothing, etc...) = (-u∇u(grid, u, v, w, i, j, k); + fv(grid, v, fCor, i,",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/259:2080,perform,performant,2080,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/259,1,['perform'],['performant']
Performance,"ut::String, depot_path::Vector{String}, dl_load_path::Vector{String}, load_path::Vector{String}, concrete_deps::Vector{Pair{Base.PkgId, UInt128}}, source::Nothing); @ Base ./loading.jl:2222; [4] top-level scope; @ stdin:3; in expression starting at /Users/navid/.julia/packages/TaylorSeries/2qRvJ/ext/TaylorSeriesIAExt.jl:1; in expression starting at stdin:3; ┌ Error: Error during loading of extension TaylorSeriesIAExt of TaylorSeries, use `Base.retry_load_extensions()` to retry.; │ exception =; │ 1-element ExceptionStack:; │ Failed to precompile TaylorSeriesIAExt [ed7ef945-33a4-511e-97fe-2b89c7a130ca] to ""/Users/navid/.julia/compiled/v1.10/TaylorSeriesIAExt/jl_TNauRw"".; │ Stacktrace:; │ [1] error(s::String); │ @ Base ./error.jl:35; │ [2] compilecache(pkg::Base.PkgId, path::String, internal_stderr::IO, internal_stdout::IO, keep_loaded_modules::Bool); │ @ Base ./loading.jl:2468; │ [3] compilecache; │ @ ./loading.jl:2340 [inlined]; │ [4] (::Base.var""#968#969""{Base.PkgId})(); │ @ Base ./loading.jl:1974; │ [5] mkpidlock(f::Base.var""#968#969""{Base.PkgId}, at::String, pid::Int32; kwopts::@Kwargs{stale_age::Int64, wait::Bool}); │ @ FileWatching.Pidfile ~/julia-1.10/usr/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:93; │ [6] #mkpidlock#6; │ @ ~/julia-1.10/usr/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:88 [inlined]; │ [7] trymkpidlock(::Function, ::Vararg{Any}; kwargs::@Kwargs{stale_age::Int64}); │ @ FileWatching.Pidfile ~/julia-1.10/usr/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:111; │ [8] #invokelatest#2; │ @ ./essentials.jl:894 [inlined]; │ [9] invokelatest; │ @ ./essentials.jl:889 [inlined]; │ [10] maybe_cachefile_lock(f::Base.var""#968#969""{Base.PkgId}, pkg::Base.PkgId, srcpath::String; stale_age::Int64); │ @ Base ./loading.jl:2983; │ [11] maybe_cachefile_lock; │ @ ./loading.jl:2980 [inlined]; │ [12] _require(pkg::Base.PkgId, env::Nothing); │ @ Base ./loading.jl:1970; │ [13] __require_prelocked(uuidkey::Base.PkgId, env::Nothing); │ @ Base ./loadin",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3686#issuecomment-2272432528:1417,load,loading,1417,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3686#issuecomment-2272432528,1,['load'],['loading']
Performance,"utedComputations.Distributed{Oceananigans.Architectures.GPU, false, Oceananigans.DistributedComputations.Partition{Int64, Int64, Int64}, Tuple{Int64, Int64, Int64}, Int64, Tuple{Int64, Int64, Int64}, Oceananigans.DistributedComputations.RankConnectivity{Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing}, MPI.Comm, Vector{MPI.Request}, Base.RefValue{Int64}}, precision::Type, N::Tuple{Int64, Int64, Int64}, latitude::Tuple{Int64, Int64}, z_faces::Vector{Float64}, resolution::Int64, ::Val{false}, ::Val{:DoubleDrake}); @ OceanScalingTests ~/src/OceanScalingTests.jl/src/grid_load_balance.jl:16; [9] scaling_test_simulation(resolution::Int64, ranks::Tuple{Int64, Int64, Int64}, Δt::Tuple{Int64, Float64}, stop_time::Float64; child_arch::Oceananigans.Architectures.GPU, experiment::Symbol, Depth::Int64, latitude::Tuple{Int64, Int64}, restart::String, z_faces_function::typeof(OceanScalingTests.exponential_z_faces), Nz::Int64, profile::Bool, with_fluxes::Bool, with_restoring::Bool, loadbalance::Bool, precision::Type, boundary_layer_parameterization::Oceananigans.TurbulenceClosures.RiBasedVerticalDiffusivity{Oceananigans.TurbulenceClosures.VerticallyImplicitTimeDiscretization, Float64, Oceananigans.TurbulenceClosures.HyperbolicTangentRiDependentTapering}); @ OceanScalingTests ~/src/OceanScalingTests.jl/src/near_global_simulation.jl:64; [10] macro expansion; @ ~/src/OceanScalingTests.jl/src/OceanScalingTests.jl:54 [inlined]; [11] macro expansion; @ ~/.julia/packages/PrecompileTools/L8A3n/src/workloads.jl:78 [inlined]; [12] macro expansion; @ ~/src/OceanScalingTests.jl/src/OceanScalingTests.jl:53 [inlined]; [13] macro expansion; @ ~/.julia/packages/PrecompileTools/L8A3n/src/workloads.jl:140 [inlined]; [14] top-level scope; @ ~/src/OceanScalingTests.jl/src/OceanScalingTests.jl:32; [15] include; @ ./Base.jl:556 [inlined]; [16] include_package_for_output(pkg::Base.PkgId, input::String, depot_path::Vector{String}, dl_load_path::Vector{String}, load_path::Vector{Strin",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3555:5475,load,loadbalance,5475,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3555,1,['load'],['loadbalance']
Performance,"utputs/PZq45/src/TimerOutput.jl:226 [inlined]; [3] irgen(job::GPUCompiler.CompilerJob, method_instance::Core.MethodInstance); @ GPUCompiler ~/.julia/packages/GPUCompiler/eJOtJ/src/irgen.jl:4; [4] macro expansion; @ ~/.julia/packages/GPUCompiler/eJOtJ/src/driver.jl:142 [inlined]; [5] macro expansion; @ ~/.julia/packages/TimerOutputs/PZq45/src/TimerOutput.jl:226 [inlined]; [6] macro expansion; @ ~/.julia/packages/GPUCompiler/eJOtJ/src/driver.jl:141 [inlined]; [7] emit_llvm(job::GPUCompiler.CompilerJob, method_instance::Any, world::UInt64; libraries::Bool, deferred_codegen::Bool, optimize::Bool, only_entry::Bool); @ GPUCompiler ~/.julia/packages/GPUCompiler/eJOtJ/src/utils.jl:62; [8] emit_llvm(job::GPUCompiler.CompilerJob, method_instance::Any, world::UInt64); @ GPUCompiler ~/.julia/packages/GPUCompiler/eJOtJ/src/utils.jl:60; [9] cufunction_compile(job::GPUCompiler.CompilerJob); @ CUDA ~/.julia/packages/CUDA/3VnCC/src/compiler/execution.jl:300; [10] check_cache; @ ~/.julia/packages/GPUCompiler/eJOtJ/src/cache.jl:47 [inlined]; [11] cached_compilation; @ ~/.julia/packages/GPUArrays/Z5nPF/src/host/broadcast.jl:57 [inlined]; [12] cached_compilation(cache::Dict{UInt64, Any}, job::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget, CUDA.CUDACompilerParams, GPUCompiler.FunctionSpec{GPUArrays.var""#broadcast_kernel#16"", Tuple{CUDA.CuKernelContext, CUDA.CuDeviceArray{Float64, 3, 1}, Base.Broadcast.Broadcasted{Nothing, Tuple{Base.OneTo{Int64}, Base.OneTo{Int64}, Base.OneTo{Int64}}, typeof(identity), Tuple{Int64}}, Int64}}}, compiler::typeof(CUDA.cufunction_compile), linker::typeof(CUDA.cufunction_link)); @ GPUCompiler ~/.julia/packages/GPUCompiler/eJOtJ/src/cache.jl:0; [13] cufunction(f::GPUArrays.var""#broadcast_kernel#16"", tt::Type{Tuple{CUDA.CuKernelContext, CUDA.CuDeviceArray{Float64, 3, 1}, Base.Broadcast.Broadcasted{Nothing, Tuple{Base.OneTo{Int64}, Base.OneTo{Int64}, Base.OneTo{Int64}}, typeof(identity), Tuple{Int64}}, Int64}}; name::Nothing, kwargs::Base.Iterators.Pairs{",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1706:1532,cache,cache,1532,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1706,1,['cache'],['cache']
Performance,"var""#968#969""{Base.PkgId}, at::String, pid::Int32; kwopts::@Kwargs{stale_age::Int64, wait::Bool}); @ FileWatching.Pidfile /glade/u/apps/casper/23.10/spack/opt/spack/julia/1.10.2/gcc/7.5.0/apod/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:93; [6] #mkpidlock#6; @ /glade/u/apps/casper/23.10/spack/opt/spack/julia/1.10.2/gcc/7.5.0/apod/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:88 [inlined]; [7] trymkpidlock(::Function, ::Vararg{Any}; kwargs::@Kwargs{stale_age::Int64}); @ FileWatching.Pidfile /glade/u/apps/casper/23.10/spack/opt/spack/julia/1.10.2/gcc/7.5.0/apod/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:111; [8] #invokelatest#2; @ ./essentials.jl:894 [inlined]; [9] invokelatest; @ ./essentials.jl:889 [inlined]; [10] maybe_cachefile_lock(f::Base.var""#968#969""{Base.PkgId}, pkg::Base.PkgId, srcpath::String; stale_age::Int64); @ Base ./loading.jl:2983; [11] maybe_cachefile_lock; @ ./loading.jl:2980 [inlined]; [12] _require(pkg::Base.PkgId, env::String); @ Base ./loading.jl:1970; [13] __require_prelocked(uuidkey::Base.PkgId, env::String); @ Base ./loading.jl:1812; [14] #invoke_in_world#3; @ ./essentials.jl:926 [inlined]; [15] invoke_in_world; @ ./essentials.jl:923 [inlined]; [16] _require_prelocked(uuidkey::Base.PkgId, env::String); @ Base ./loading.jl:1803; [17] macro expansion; @ ./loading.jl:1790 [inlined]; [18] macro expansion; @ ./lock.jl:267 [inlined]; [19] __require(into::Module, mod::Symbol); @ Base ./loading.jl:1753; [20] #invoke_in_world#3; @ ./essentials.jl:926 [inlined]; [21] invoke_in_world; @ ./essentials.jl:923 [inlined]; [22] require(into::Module, mod::Symbol); @ Base ./loading.jl:1746; [23] include; @ ./Base.jl:495 [inlined]; [24] include_package_for_output(pkg::Base.PkgId, input::String, depot_path::Vector{String}, dl_load_path::Vector{String}, load_path::Vector{String}, concrete_deps::Vector{Pair{Base.PkgId, UInt128}}, source::String); @ Base ./loading.jl:2222; [25] top-level scope; @ stdin:3; in expression starting at /glade/u/h",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2231635812:4413,load,loading,4413,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2231635812,1,['load'],['loading']
Performance,"wn. So there's definitely something going on here for GPUs; > * I have struggled with this for quite some time until I found the culprit, so if we can't immediately find and fix the cause I'd suggest leaving a caution statement on the simulation tips page. But I agree it has to be less strongly worded...; > ; > I'll try to provide a MWE that reproduces the behavior, but I'm currently having trouble getting my hands on some GPU, so I'm not sure how fast I can do that. Ok, no rush!. Trig functions aren't generically slower on GPUs than CPUs. On CPUs I think our code is fairly non-optimal right now, so various sources of overhead (eg non-optimal threading) can ""hide"" slow operations on the CPU. On the GPU we are more efficient, so overall speed might depend more sensitively on user code when it's injected. (I'd also argue that the beginning of this section is a bit misleading in how it claims we ""try to optimize"" internal source code. In fact, we have performed almost no performance optimization, and this is an important topic for future work.). I found this reference for the cost of various floating point operations on the CPU:. https://latkin.org/blog/2014/11/09/a-simple-benchmark-of-various-math-operations/. We could reproduce this chart on a GPU with CUDA.jl if we want to provide some useful information to users. I think if we're talking about a _constant_ (the current case), then precomputation hardly harms code complexity (both examples are equally readable to me). Precomputing an _array_ is another story (for example, a forcing function or boundary condition that depends on `sin(x)`). This lesson is definitely not restricted to trigonometric functions or the GPU. The basic principle here is that _there is a trade-off_ between precomputing a potentially expensive operation, and performing it on-the-fly. For constants, precomputation is harmless. For arrays, on-the-fly computation has significant benefits, both for code readability and also possibly for performance",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2029#issuecomment-952107151:1281,perform,performed,1281,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2029#issuecomment-952107151,3,"['optimiz', 'perform']","['optimization', 'performance', 'performed']"
Performance,"writer should be separated from output writer ""initialization"". Currently both JLD2 and NetCDF output writers open their files and save a bunch of data when they are constructed. For example, . https://github.com/CliMA/Oceananigans.jl/blob/7b029a75b8e17d3ca818db27eba41fd5d3a0397e/src/OutputWriters/netcdf_output_writer.jl#L300-L328. and. https://github.com/CliMA/Oceananigans.jl/blob/7b029a75b8e17d3ca818db27eba41fd5d3a0397e/src/OutputWriters/jld2_output_writer.jl#L170-L181. This initialization can make ""pickup"" of a simulation annoying. For example, the JLD2 writer will _remove_ an existing file if `force=true`:. https://github.com/CliMA/Oceananigans.jl/blob/7b029a75b8e17d3ca818db27eba41fd5d3a0397e/src/OutputWriters/jld2_output_writer.jl#L172. This is desirable when experimenting with a script and setting up (and thus we usually set `force=true` in examples as a matter of user-friendliness). But if you're picking up a simulation you certainly don't want to set `force=true`, since this will overwrite existing data. Another example is the warning implemented in #1162 , which I suppose one must resign themselves to always receiving if they are picking up a simulation?. Debacles such as these might be avoided if we performed initialization in `run!` rather than when the output writers are created --- because then we can implement special behavior in the case that we are picking up a simulation. More or less I think the point here is that we don't _know_ if we want to create / destroy a file until `run!` is called with appropriate arguments. If `run!` is never called, there will not be output (so maybe we don't want to create a file). If `run!` is called and we are not picking up and `force` or `overwrite_existing=true` then we should do that. But if `run!` is called and `pickup=true` we can likely assume that existing files should not be deleted. Output writing only occurs in `run!`, so it might make sense to let `run!` also manage the creation / deletion of output files.",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1163:1295,perform,performed,1295,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1163,1,['perform'],['performed']
Performance,"y.jl:737; [2] checkbounds; @ ./abstractarray.jl:702 [inlined]; [3] view; @ ./subarray.jl:184 [inlined]; [4] offset_windowed_data(data::OffsetArrays.OffsetArray{…}, Loc::Tuple{…}, grid::RectilinearGrid{…}, indices::Tuple{…}); @ Oceananigans.Fields ~/github/Oceananigans.jl/src/Fields/field.jl:248; [5] view(f::Field{Center, Center, Face, Nothing, RectilinearGrid{…}, Tuple{…}, OffsetArrays.OffsetArray{…}, Float64, FieldBoundaryConditions{…}, Nothing, Oceananigans.Fields.FieldBoundaryBuffers{…}}, i::UnitRange{Int64}, j::UnitRange{Int64}, k::Int64); @ Oceananigans.Fields ~/github/Oceananigans.jl/src/Fields/field.jl:316; [6] #Field#15; @ ~/github/Oceananigans.jl/src/Fields/field.jl:182 [inlined]; [7] top-level scope; @ REPL[39]:1; Some type information was truncated. Use `show(err)` to see complete types.; ``` . > @josuemtzmo can you show the whole stack trace of your error so we can see where the indexing issue comes in?; Yes, here it is:; ```; ERROR: LoadError: BoundsError: attempt to access 11×11×1 Array{Float64, 3} at index [4:8, 4:8, 4:8]; Stacktrace:; [1] throw_boundserror(A::Array{Float64, 3}, I::Tuple{UnitRange{Int64}, UnitRange{Int64}, UnitRange{Int64}}); @ Base ./abstractarray.jl:737; [2] checkbounds; @ ./abstractarray.jl:702 [inlined]; [3] view; @ ./subarray.jl:184 [inlined]; [4] offset_windowed_data(data::OffsetArrays.OffsetArray{…}, Loc::Tuple{…}, grid::RectilinearGrid{…}, indices::Tuple{…}); @ Oceananigans.Fields ~/github/Oceananigans.jl/src/Fields/field.jl:248; [5] view(f::Field{Center, Center, Face, Nothing, RectilinearGrid{…}, Tuple{…}, OffsetArrays.OffsetArray{…}, Float64, FieldBoundaryConditions{…}, Nothing, Oceananigans.Fields.FieldBoundaryBuffers{…}}, i::UnitRange{Int64}, j::UnitRange{Int64}, k::UnitRange{Int64}); @ Oceananigans.Fields ~/github/Oceananigans.jl/src/Fields/field.jl:316; [6] Field; @ ~/github/Oceananigans.jl/src/Fields/field.jl:182 [inlined]; [7] construct_output(user_output::Field{…}, grid::RectilinearGrid{…}, user_indices::Tuple{…}, wit",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3260#issuecomment-2014615658:3760,Load,LoadError,3760,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3260#issuecomment-2014615658,1,['Load'],['LoadError']
Performance,"yCondition{Oceananigans.BoundaryConditions.Flux, Nothing}}, BoundaryCondition{Oceananigans.BoundaryConditions.Value, Float64}, BoundaryCondition{Oceananigans.BoundaryConditions.Value, Float64}, Oceananigans.BoundaryConditions.DefaultBoundaryCondition{BoundaryCondition{Oceananigans.BoundaryConditions.Flux, Nothing}}, Oceananigans.BoundaryConditions.DefaultBoundaryCondition{BoundaryCondition{Oceananigans.BoundaryConditions.Flux, Nothing}}, Oceananigans.BoundaryConditions.DefaultBoundaryCondition{BoundaryCondition{Oceananigans.BoundaryConditions.Flux, Nothing}}}, FieldBoundaryConditions{BoundaryCondition{Oceananigans.BoundaryConditions.Value, Float64}, BoundaryCondition{Oceananigans.BoundaryConditions.Value, Float64}, Oceananigans.BoundaryConditions.DefaultBoundaryCondition{BoundaryCondition{Oceananigans.BoundaryConditions.Flux, Nothing}}, Oceananigans.BoundaryConditions.DefaultBoundaryCondition{BoundaryCondition{Oceananigans.BoundaryConditions.Flux, Nothing}}, Oceananigans.BoundaryConditions.DefaultBoundaryCondition{BoundaryCondition{Oceananigans.BoundaryConditions.Flux, Nothing}}, Oceananigans.BoundaryConditions.DefaultBoundaryCondition{BoundaryCondition{Oceananigans.BoundaryConditions.Flux, Nothing}}, Oceananigans.BoundaryConditions.DefaultBoundaryCondition{BoundaryCondition{Oceananigans.BoundaryConditions.Flux, Nothing}}}}}, tracers::Nothing, timestepper::Symbol, background_fields::NamedTuple{(), Tuple{}}, particles::Nothing, velocities::Nothing, pressures::Nothing, diffusivity_fields::Nothing, pressure_solver::Nothing, immersed_boundary::Nothing, auxiliary_fields::NamedTuple{(), Tuple{}}); @ Oceananigans.Models.NonhydrostaticModels C:\Users\parfe\.julia\packages\Oceananigans\B958I\src\Models\NonhydrostaticModels\nonhydrostatic_model.jl:204; [28] top-level scope; @ In[23]:1; [29] eval; @ .\boot.jl:373 [inlined]; [30] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String); @ Base .\loading.jl:1196; ```; What is wrong here?</div>",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2530:55878,load,loading,55878,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2530,1,['load'],['loading']
Performance,"yStretchedRectilinearGrid{Float64, Periodic, Periodic, Bounded, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}}}, OffsetArrays.OffsetVector{Float64, Vector{Float64}}, CPU}, Vector{Int64}, Vector{Periodic}, Int64, Nothing, Nothing}, Oceananigans.Solvers.DiscreteTransform{Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing}}}}}, Nothing, NamedTuple{(:velocities, :tracers), Tuple{NamedTuple{(:u, :v, :w), Tuple{Oceananigans.Fields.ZeroField, Oceananigans.Fields.ZeroField, Oceananigans.Fields.ZeroField}}, NamedTuple{(:T, :S), Tuple{Oceananigans.Fields.ZeroField, Oceananigans.Fields.ZeroField}}}}, Nothing, Nothing, NamedTuple{(), Tuple{}}}, String}, ::String, ::Vararg{String, N} where N; kws::Base.Iterators.Pairs{Union{}, Union{}, Tuple{}, NamedTuple{(), Tuple{}}}); @ JLD2 /glade/work/tomasc/.julia_bkp/packages/JLD2/DcnTD/src/loadsave.jl:4; [4] jldopen; @ /glade/work/tomasc/.julia_bkp/packages/JLD2/DcnTD/src/loadsave.jl:2 [inlined]; [5] set!; @ /glade/work/tomasc/.julia_bkp/packages/Oceananigans/52CTk/src/OutputWriters/checkpointer.jl:193 [inlined]; [6] run!(sim::Simulation{NonhydrostaticModel{Oceananigans.TimeSteppers.QuasiAdamsBashforth2TimeStepper{Float64, NamedTuple{(:u, :v, :w, :T, :S), Tuple{Field{Face, Center, Center, CPU, OffsetArrays.OffsetArray{Float64, 3, Array{Float64, 3}}, VerticallyStretchedRectilinearGrid{Float64, Periodic, Periodic, Bounded, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}}}, OffsetArrays.OffsetVector{Float64, Vector{Float64}}, CPU}, Float64, FieldBoundaryConditions{BoundaryCondition{Oceananigans.BoundaryConditions.Periodic, Nothing}, BoundaryCondition{Oceananigans.BoundaryConditions.Periodic, Nothing}, BoundaryCondition{Oceananigans.BoundaryConditions.Periodic, Nothing}, BoundaryCondition{Oceananigans.BoundaryConditions.Periodic, Nothing}, BoundaryCondition{Oceananigans.Boundary",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2018:28412,load,loadsave,28412,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2018,1,['load'],['loadsave']
Performance,y_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; #fill_halo_regions!#38 at /orcd/data/raffaele/001/glwagner/Oceananigans.jl/src/DistributedComputations/halo_communication.jl:114; fill_halo_regions! at /orcd/data/raffaele/001/glwagner/Oceananigans.jl/src/DistributedComputations/halo_communication.jl:101 [inlined]; #fill_halo_regions!#37 at /orcd/data/raffaele/001/glwagner/Oceananigans.jl/src/DistributedComputations/halo_communication.jl:90 [inlined]; fill_halo_regions! at /orcd/data/raffaele/001/glwagner/Oceananigans.jl/src/DistributedComputations/halo_communication.jl:87; unknown function (ip: 0x2aaac8ad0ee5); _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_generic at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:3077; jl_apply at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/julia.h:1982 [inlined]; do_call at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:126; eval_value at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:223; eval_stmt_value at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:174 [inlined]; eval_body at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:617; jl_interpret_toplevel_thunk at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/interpreter.c:775; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:934; jl_toplevel_eval_flex at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:877; ijl_toplevel_eval_in at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/toplevel.c:985; eval at ./boot.jl:385 [inlined]; include_string at ./loading.jl:2076; _jl_invoke at /cache/build/builder-amdci4-4/julialang/julia-release-1-dot-10/src/gf.c:2895 [inlined]; ijl_apply_gen,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3878:4043,cache,cache,4043,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3878,1,['cache'],['cache']
Performance,"ymbol} which is not isbits.; .1 is of type Symbol which is not isbits. Passing non-isbits types is only allowed if they they are unused by the kernel. Stacktrace:; [1] check_invocation(::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDA.CUDACompilerParams}, ::LLVM.Function) at /home/ptuckman/.julia/packages/GPUCompiler/4e9CU/src/validation.jl:75; [2] macro expansion at /home/ptuckman/.julia/packages/GPUCompiler/4e9CU/src/driver.jl:240 [inlined]; [3] macro expansion at /home/ptuckman/.julia/packages/TimerOutputs/dVnaw/src/TimerOutput.jl:206 [inlined]; [4] codegen(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDA.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, validate::Bool, only_entry::Bool) at /home/ptuckman/.julia/packages/GPUCompiler/4e9CU/src/driver.jl:239; [5] compile(::Symbol, ::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDA.CUDACompilerParams}; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, validate::Bool, only_entry::Bool) at /home/ptuckman/.julia/packages/GPUCompiler/4e9CU/src/driver.jl:39; [6] compile at /home/ptuckman/.julia/packages/GPUCompiler/4e9CU/src/driver.jl:35 [inlined]; [7] _cufunction(::GPUCompiler.FunctionSpec{typeof(Cassette.overdub),Tuple{Cassette.Context{nametype(CUDACtx),KernelAbstractions.CompilerMetadata{KernelAbstractions.NDIteration.StaticSize{(128, 128, 128)},KernelAbstractions.NDIteration.DynamicCheck,Nothing,Nothing,KernelAbstractions.NDIteration.NDRange{3,KernelAbstractions.NDIteration.StaticSize{(8, 8, 128)},KernelAbstractions.NDIteration.StaticSize{(16, 16, 1)},Nothing,Nothing}},Nothing,KernelAbstractions.var""##PassType#253"",Nothing,Cassette.DisableHooks},typeof(Oceananigans.TimeSteppers.gpu_calculate_Gu!),OffsetArrays.OffsetArray{Float64,3,CUDA.CuDeviceArray{Float64,3,CUDA.AS.Global}},RegularCartesianGrid{Float64,Periodic,Periodic,Bounded,OffsetArrays.OffsetArray{Float64,1,StepRangeLen{Float64,Base.TwicePrecision{Float64},B",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1010:4833,optimiz,optimize,4833,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1010,1,['optimiz'],['optimize']
Performance,"you mean like this?. ```; ERROR: LoadError: AssertionError: false; Stacktrace:; [1] calling_conv_fixup(builder::LLVM.IRBuilder, val::LLVM.AddrSpaceCastInst, tape::LLVM.PointerType, prev::LLVM.UndefValue, lidxs::Vector{UInt32}, ridxs::Vector{UInt32}); @ Enzyme.Compiler ~/Projects/Enzymantics/Enzyme.jl/src/compiler/utils.jl:271; [2] calling_conv_fixup (repeats 2 times); @ Enzyme.Compiler ~/Projects/Enzymantics/Enzyme.jl/src/compiler/utils.jl:183 [inlined]; [3] calling_conv_fixup(builder::LLVM.IRBuilder, val::LLVM.AddrSpaceCastInst, tape::LLVM.PointerType); @ Enzyme.Compiler ~/Projects/Enzymantics/Enzyme.jl/src/compiler/utils.jl:183; [4] enzyme_custom_common_rev(forward::Bool, B::LLVM.IRBuilder, orig::LLVM.CallInst, gutils::Enzyme.Compiler.GradientUtils, normalR::Ptr{Nothing}, shadowR::Ptr{Nothing}, tape::LLVM.ExtractValueInst); @ Enzyme.Compiler ~/Projects/Enzymantics/Enzyme.jl/src/compiler.jl:4610; [5] enzyme_custom_rev(B::LLVM.IRBuilder, orig::LLVM.CallInst, gutils::Enzyme.Compiler.GradientUtils, tape::LLVM.ExtractValueInst); @ Enzyme.Compiler ~/Projects/Enzymantics/Enzyme.jl/src/compiler.jl:4770; [6] (::Enzyme.Compiler.var""#201#202"")(B::Ptr{LLVM.API.LLVMOpaqueBuilder}, OrigCI::Ptr{LLVM.API.LLVMOpaqueValue}, gutils::Ptr{Nothing}, tape::Ptr{LLVM.API.LLVMOpaqueValue}); @ Enzyme.Compiler ~/Projects/Enzymantics/Enzyme.jl/src/compiler.jl:6657; [7] EnzymeCreatePrimalAndGradient(logic::Enzyme.Logic, todiff::LLVM.Function, retType::Enzyme.API.CDIFFE_TYPE, constant_args::Vector{…}, TA::Enzyme.TypeAnalysis, returnValue::Bool, dretUsed::Bool, mode::Enzyme.API.CDerivativeMode, width::Int64, additionalArg::Ptr{…}, forceAnonymousTape::Bool, typeInfo::Enzyme.FnTypeInfo, uncacheable_args::Vector{…}, augmented::Ptr{…}, atomicAdd::Bool); @ Enzyme.API ~/Projects/Enzymantics/Enzyme.jl/src/api.jl:141; [8] enzyme!(job::GPUCompiler.CompilerJob{…}, mod::LLVM.Module, primalf::LLVM.Function, TT::Type, mode::Enzyme.API.CDerivativeMode, width::Int64, parallel::Bool, actualRetType::Type, wrap::B",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3360#issuecomment-1791250259:33,Load,LoadError,33,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3360#issuecomment-1791250259,1,['Load'],['LoadError']
Performance,"| ######################################################################## 100.0%; &nbsp; | Downloading artifact: CompilerSupportLibraries; &nbsp; | ######################################################################## 100.0%; &nbsp; | Downloading artifact: MPICH; &nbsp; | ######################################################################## 100.0%; &nbsp; | Downloading artifact: LibCURL; &nbsp; | ######################################################################## 100.0%; &nbsp; | Building MPI ─→ `/storage7/buildkite-agent/.julia-2581/packages/MPI/b7MVG/deps/build.log`; &nbsp; | [ Info: using default MPI jll; &nbsp; | Building FFTW → `/storage7/buildkite-agent/.julia-2581/packages/FFTW/G3lSO/deps/build.log`; &nbsp; | Precompiling project...; &nbsp; | WARNING: Error during initialization of module GMP:; &nbsp; | ErrorException(""could not load library ""libgmp""; &nbsp; | libgmp.so: ELF load command past end of file""); &nbsp; | WARNING: Error during initialization of module LinearAlgebra:; &nbsp; | ErrorException(""could not load library ""libopenblas64_""; &nbsp; | libopenblas64_.so: ELF load command past end of file""); &nbsp; | ┌ Error: Error during initialization of module CHOLMOD; &nbsp; | │ exception =; &nbsp; | │ could not load library ""libcholmod""; &nbsp; | │ libopenblas64_.so.0: ELF load command past end of file; &nbsp; | │ Stacktrace:; &nbsp; | │ [1] dlopen(::String, ::UInt32; throw_error::Bool) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Libdl/src/Libdl.jl:109; &nbsp; | │ [2] dlopen at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Libdl/src/Libdl.jl:109 [inlined] (repeats 2 times); &nbsp; | │ [3] __init__() at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/SuiteSparse/src/cholmod.jl:90; &nbsp; | └ @ SuiteSparse.CHOLMOD /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/SuiteSparse/src/cholmod.jl:187; &nbsp; | /storage7/buildkite-agent/julia-1.5.4/bin/julia: er",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1662#issuecomment-843325731:2376,load,load,2376,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1662#issuecomment-843325731,2,['load'],['load']
Performance,"| [8] read;   | @ ./io.jl:434 [inlined];   | [9] _include(mapexpr::Function, mod::Module, _path::String);   | @ Base ./loading.jl:1166;   | [10] include(mod::Module, _path::String);   | @ Base ./Base.jl:386;   | [11] include(x::String);   | @ Oceananigans.Advection ~/builds/tartarus-7/clima/oceananigans/src/Advection/Advection.jl:1;   | [12] top-level scope;   | @ ~/builds/tartarus-7/clima/oceananigans/src/Advection/Advection.jl:43;   | [13] include(mod::Module, _path::String);   | @ Base ./Base.jl:386;   | [14] include(x::String);   | @ Oceananigans ~/builds/tartarus-7/clima/oceananigans/src/Oceananigans.jl:1;   | [15] top-level scope;   | @ ~/builds/tartarus-7/clima/oceananigans/src/Oceananigans.jl:173;   | [16] include;   | @ ./Base.jl:386 [inlined];   | [17] include_package_for_output(pkg::Base.PkgId, input::String, depot_path::Vector{String}, dl_load_path::Vector{String}, load_path::Vector{String}, concrete_deps::Vector{Pair{Base.PkgId, UInt64}}, source::Nothing);   | @ Base ./loading.jl:1235;   | [18] top-level scope;   | @ none:1;   | [19] eval;   | @ ./boot.jl:360 [inlined];   | [20] eval(x::Expr);   | @ Base.MainInclude ./client.jl:446;   | [21] top-level scope;   | @ none:1;   | in expression starting at /var/lib/buildkite-agent/builds/tartarus-7/clima/oceananigans/src/Advection/Advection.jl:1;   | in expression starting at /var/lib/buildkite-agent/builds/tartarus-7/clima/oceananigans/src/Oceananigans.jl:1;   | Stacktrace:;   | [1] pkgerror(msg::String);   | @ Pkg.Types /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Pkg/src/Types.jl:55;   | [2] precompile(ctx::Pkg.Types.Context; internal_call::Bool, strict::Bool, kwargs::Base.Iterators.Pairs{Union{}, Union{}, Tuple{}, NamedTuple{(), Tuple{}}});   | @ Pkg.API /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Pkg/src/API.jl:1265;   | [3] precompile;   | @ /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Pkg/src/API.jl:921 [inlined];   | [4] #prec",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1957#issuecomment-904165134:2273,load,loading,2273,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1957#issuecomment-904165134,1,['load'],['loading']
Performance,"}, Tuple{UnitRange{Int64}}, true}, first::Int64, last::Int64); @ Base ./reduce.jl:638; [7] _mapreduce(f::typeof(identity), op::typeof(min), #unused#::IndexLinear, A::SubArray{Float64, 1, OffsetArrays.OffsetVector{Float64, CUDA.CuArray{Float64, 1, CUDA.Mem.DeviceBuffer}}, Tuple{UnitRange{Int64}}, true}); @ Base ./reduce.jl:442; [8] _mapreduce_dim; @ ./reducedim.jl:365 [inlined]; [9] #mapreduce#765; @ ./reducedim.jl:357 [inlined]; [10] mapreduce; @ ./reducedim.jl:357 [inlined]; [11] #_minimum#787; @ ./reducedim.jl:999 [inlined]; [12] _minimum; @ ./reducedim.jl:999 [inlined]; [13] #_minimum#786; @ ./reducedim.jl:998 [inlined]; [14] _minimum; @ ./reducedim.jl:998 [inlined]; [15] #minimum#784; @ ./reducedim.jl:994 [inlined]; [16] minimum; @ ./reducedim.jl:994 [inlined]; [17] min_Δx; @ /nfs/cnhlab001/ssilvest/julia_pkg/packages/Oceananigans/KTw3g/src/Grids/latitude_longitude_grid.jl:653 [inlined]; ```. `min_Δx` triggers scalar indexing. ```; ERROR: LoadError: InvalidIRError: compiling kernel #gpu_compute_ri_based_diffusivities!(KernelAbstractions.CompilerMetadata{KernelAbstractions.NDIteration.StaticSize{(1024, 2240, 69)}, KernelAbstractions.NDIteration.DynamicCheck, Nothing, Nothing, KernelAbstractions.NDIteration.NDRange{3, KernelAbstractions.NDIteration.StaticSize{(64, 140, 69)}, KernelAbstractions.NDIteration.StaticSize{(16, 16, 1)}, Nothing, Nothing}}, NamedTuple{(:\u03ba\u1d9c, :\u03ba\u1d58), Tuple{OffsetArrays.OffsetArray{Float64, 3, CUDA.CuDeviceArray{Float64, 3, 1}}, OffsetArrays.OffsetArray{Float64, 3, CUDA.CuDeviceArray{Float64, 3, 1}}}}, ImmersedBoundaryGrid{Float64, Periodic, Bounded, Bounded, LatitudeLongitudeGrid{Float64, Periodic, Bounded, Bounded, OffsetArrays.OffsetVector{Float64, CUDA.CuDeviceVector{Float64, 1}}, Float64, Float64, Float64, OffsetArrays.OffsetVector{Float64, CUDA.CuDeviceVector{Float64, 1}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}, OffsetArrays.OffsetVec",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3038:3491,Load,LoadError,3491,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3038,1,['Load'],['LoadError']
Performance,"}, args::LLVM.Module); @ GPUCompiler /g/data/v45/nc3020/.julia/packages/GPUCompiler/2WWTr/src/validation.jl:111; [2] macro expansion; @ /g/data/v45/nc3020/.julia/packages/GPUCompiler/2WWTr/src/driver.jl:319 [inlined]; [3] macro expansion; @ /g/data/v45/nc3020/.julia/packages/TimerOutputs/PZq45/src/TimerOutput.jl:226 [inlined]; [4] macro expansion; @ /g/data/v45/nc3020/.julia/packages/GPUCompiler/2WWTr/src/driver.jl:317 [inlined]; [5] emit_asm(job::GPUCompiler.CompilerJob, ir::LLVM.Module; strip::Bool, validate::Bool, format::LLVM.API.LLVMCodeGenFileType); @ GPUCompiler /g/data/v45/nc3020/.julia/packages/GPUCompiler/2WWTr/src/utils.jl:62; [6] cufunction_compile(job::GPUCompiler.CompilerJob); @ CUDA /g/data/v45/nc3020/.julia/packages/CUDA/mVgLI/src/compiler/execution.jl:313; [7] cached_compilation(cache::Dict{UInt64, Any}, job::GPUCompiler.CompilerJob, compiler::typeof(CUDA.cufunction_compile), linker::typeof(CUDA.cufunction_link)); @ GPUCompiler /g/data/v45/nc3020/.julia/packages/GPUCompiler/2WWTr/src/cache.jl:89; [8] cufunction(f::typeof(Cassette.overdub), tt::Type{Tuple{Cassette.Context{nametype(CUDACtx), KernelAbstractions.CompilerMetadata{KernelAbstractions.NDIteration.StaticSize{(32, 32, 32)}, KernelAbstractions.NDIteration.DynamicCheck, Nothing, Nothing, KernelAbstractions.NDIteration.NDRange{3, KernelAbstractions.NDIteration.StaticSize{(2, 2, 32)}, KernelAbstractions.NDIteration.StaticSize{(16, 16, 1)}, Nothing, Nothing}}, Nothing, KernelAbstractions.var""##PassType#257"", Nothing, Cassette.DisableHooks}, typeof(Oceananigans.Models.IncompressibleModels.gpu_calculate_Gw!), OffsetArrays.OffsetArray{Float32, 3, CUDA.CuDeviceArray{Float32, 3, 1}}, RegularRectilinearGrid{Float32, Periodic, Periodic, Bounded, OffsetArrays.OffsetVector{Float32, StepRangeLen{Float32, Float64, Float64}}}, WENO5, Nothing, Nothing, Nothing, Buoyancy{SeawaterBuoyancy{Float32, LinearEquationOfState{Float32}, Nothing, Nothing}, Oceananigans.BuoyancyModels.ZDirection}, NamedTuple{(:velocities, ",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1780#issuecomment-870162360:17445,cache,cache,17445,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1780#issuecomment-870162360,1,['cache'],['cache']
Performance,"}}, CPU}, MinimalBiogeochemistryType::Type{MinimalDiscreteBiogeochemistry}, ModelType::Type{HydrostaticFreeSurfaceModel}); @ Main ~/Research/OC5.jl/test/test_biogeochemistry.jl:109; [22] macro expansion; @ ~/Research/OC5.jl/test/test_biogeochemistry.jl:139 [inlined]; [23] macro expansion; @ ~/julia-1.9/usr/share/julia/stdlib/v1.9/Test/src/Test.jl:1498 [inlined]; [24] top-level scope; @ ~/Research/OC5.jl/test/test_biogeochemistry.jl:127; [25] include(fname::String); @ Base.MainInclude ./client.jl:478; [26] top-level scope; @ REPL[2]:1; [27] eval; @ ./boot.jl:370 [inlined]; [28] eval_user_input(ast::Any, backend::REPL.REPLBackend, mod::Module); @ REPL ~/julia-1.9/usr/share/julia/stdlib/v1.9/REPL/src/REPL.jl:153; [29] repl_backend_loop(backend::REPL.REPLBackend, get_module::Function); @ REPL ~/julia-1.9/usr/share/julia/stdlib/v1.9/REPL/src/REPL.jl:249; [30] start_repl_backend(backend::REPL.REPLBackend, consumer::Any; get_module::Function); @ REPL ~/julia-1.9/usr/share/julia/stdlib/v1.9/REPL/src/REPL.jl:234; [31] run_repl(repl::REPL.AbstractREPL, consumer::Any; backend_on_current_task::Bool, backend::Any); @ REPL ~/julia-1.9/usr/share/julia/stdlib/v1.9/REPL/src/REPL.jl:379; [32] run_repl(repl::REPL.AbstractREPL, consumer::Any); @ REPL ~/julia-1.9/usr/share/julia/stdlib/v1.9/REPL/src/REPL.jl:365; [33] (::Base.var""#1017#1019""{Bool, Bool, Bool})(REPL::Module); @ Base ./client.jl:421; [34] #invokelatest#2; @ ./essentials.jl:816 [inlined]; [35] invokelatest; @ ./essentials.jl:813 [inlined]; [36] run_main_repl(interactive::Bool, quiet::Bool, banner::Bool, history_file::Bool, color_set::Bool); @ Base ./client.jl:405; [37] exec_options(opts::Base.JLOptions); @ Base ./client.jl:322; [38] _start(); @ Base ./client.jl:522; Test Summary: | Pass Error Total Time; Biogeochemistry | 9 1 10 1m32.9s; ERROR: LoadError: Some tests did not pass: 9 passed, 0 failed, 1 errored, 0 broken.; in expression start; ```. so it seems like those lines are called by the matrix implicit solver somehow?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3179#issuecomment-1630460635:23261,Load,LoadError,23261,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3179#issuecomment-1630460635,1,['Load'],['LoadError']
Performance,"}}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}}}, GPU}, Float64, OffsetArrays.OffsetArray{Float64, 3, CUDA.CuArray{Float64, 3}}, FieldBoundaryConditions{BoundaryCondition{Oceananigans.BoundaryConditions.Periodic, Nothing}, BoundaryCondition{Oceananigans.BoundaryConditions.Periodic, Nothing}, BoundaryCondition{Oceananigans.BoundaryConditions.Flux, Nothing}, BoundaryCondition{Oceananigans.BoundaryConditions.Flux, Nothing}, BoundaryCondition{Oceananigans.BoundaryConditions.Flux, Nothing}, BoundaryCondition{Oceananigans.BoundaryConditions.Flux, Nothing}, BoundaryCondition{Oceananigans.BoundaryConditions.Flux, Nothing}}, Nothing}}}, Nothing}, NamedTuple{(), Tuple{}}}, String}, ::String, ::Vararg{String, N} where N; kws::Base.Iterators.Pairs{Union{}, Union{}, Tuple{}, NamedTuple{(), Tuple{}}}); @ JLD2 /g/data/v45/nc3020/.julia/packages/JLD2/5iijr/src/loadsave.jl:4; [8] jldopen; @ /g/data/v45/nc3020/.julia/packages/JLD2/5iijr/src/loadsave.jl:2 [inlined]; [9] set!; @ /g/data/v45/nc3020/.julia/packages/Oceananigans/Bks9B/src/OutputWriters/checkpointer.jl:199 [inlined]; [10] run!(sim::Simulation{HydrostaticFreeSurfaceModel{Oceananigans.TimeSteppers.QuasiAdamsBashforth2TimeStepper{Float64, NamedTuple{(:u, :v, :η, :b, :e, :c), Tuple{Field{Face, Center, Center, Nothing, RectilinearGrid{Float64, Periodic, Bounded, Bounded, Float64, Float64, Float64, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}}}, GPU}, Float64, OffsetArrays.OffsetArray{Float64, 3, CUDA.CuArray{Float64, 3}}, FieldBoundaryConditions{BoundaryCondition{Oceananigans.BoundaryConditions.Periodic, Nothing}, BoundaryCondition{Oceananigans.Boundary",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2223:80391,load,loadsave,80391,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2223,1,['load'],['loadsave']
Performance,"~/.julia/packages/MPICH_jll/dhUyI/src/wrappers/aarch64-apple-darwin-libgfortran5.jl:32; [5] _include_from_serialized(path::String, depmods::Vector{Any}); @ Base ./loading.jl:768; [6] _require_search_from_serialized(pkg::Base.PkgId, sourcepath::String); @ Base ./loading.jl:854; [7] _require(pkg::Base.PkgId); @ Base ./loading.jl:1097; [8] require(uuidkey::Base.PkgId); @ Base ./loading.jl:1013; [9] require(into::Module, mod::Symbol); @ Base ./loading.jl:997; [10] top-level scope; @ ~/.julia/packages/MPI/08SPr/deps/deps.jl:8; [11] include(mod::Module, _path::String); @ Base ./Base.jl:418; [12] include(x::String); @ MPI ~/.julia/packages/MPI/08SPr/src/MPI.jl:1; [13] top-level scope; @ ~/.julia/packages/MPI/08SPr/src/MPI.jl:36; [14] include; @ ./Base.jl:418 [inlined]; [15] include_package_for_output(pkg::Base.PkgId, input::String, depot_path::Vector{String}, dl_load_path::Vector{String}, load_path::Vector{String}, concrete_deps::Vector{Pair{Base.PkgId, UInt64}}, source::String); @ Base ./loading.jl:1318; [16] top-level scope; @ none:1; [17] eval; @ ./boot.jl:373 [inlined]; [18] eval(x::Expr); @ Base.MainInclude ./client.jl:453; [19] top-level scope; @ none:1; during initialization of module MPICH_jll; in expression starting at /Users/sean/.julia/packages/MPI/08SPr/deps/deps.jl:1; ERROR: LoadError: Failed to precompile MPI [da04e1cc-30fd-572f-bb4f-1f8673147195] to /Users/sean/.julia/compiled/v1.7/MPI/jl_AfEwik.; Stacktrace:; [1] error(s::String); @ Base ./error.jl:33; [2] compilecache(pkg::Base.PkgId, path::String, internal_stderr::IO, internal_stdout::IO, ignore_loaded_modules::Bool); @ Base ./loading.jl:1466; [3] compilecache(pkg::Base.PkgId, path::String); @ Base ./loading.jl:1410; [4] _require(pkg::Base.PkgId); @ Base ./loading.jl:1120; [5] require(uuidkey::Base.PkgId); @ Base ./loading.jl:1013; [6] require(into::Module, mod::Symbol); @ Base ./loading.jl:997; [7] include(mod::Module, _path::String); @ Base ./Base.jl:418; [8] include(x::String); @ Oceananigans ~/.julia/p",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2480:3981,load,loading,3981,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2480,1,['load'],['loading']
Performance,~~~; (Oceananigans) pkg> status; Project Oceananigans v0.40.0; Status `/gpfs7kw/linkhome/rech/genbes01/use25mx/Oceananigans.jl/Project.toml`; [79e6a3ab] Adapt v2.0.2; [052768ef] CUDA v1.3.3; [a8cc5b0e] Crayons v4.0.4; [7a1cc6ca] FFTW v1.2.4; [033835bb] JLD2 v0.1.14; [63c18a36] KernelAbstractions v0.4.0; [85f8d34a] NCDatasets v0.10.4; [6fe1bfb0] OffsetArrays v1.1.3; [bac558e1] OrderedCollections v1.3.0; [1bc83da4] SafeTestsets v0.0.1; [d496a93d] SeawaterPolynomials v0.2.0; [90137ffa] StaticArrays v0.12.4; [ade2ca70] Dates; [b77e0a4c] InteractiveUtils; [37e2e46d] LinearAlgebra; [56ddb016] Logging; [44cfe95a] Pkg; [de0858da] Printf; [9a3f8284] Random; [10745b16] Statistics. shell> module list; Currently Loaded Modulefiles:; 1) cuda/10.2 2) julia/1.5.2 ; ~~~,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1035#issuecomment-707688295:710,Load,Loaded,710,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1035#issuecomment-707688295,1,['Load'],['Loaded']
Performance,"─── v1.3.0; Installed Reexport ──────────────────── v1.2.2; Installed JuliaNVTXCallbacks_jll ────── v0.2.1+0; Installed BFloat16s ─────────────────── v0.5.0; Installed MacroTools ────────────────── v0.5.13; Installed DataStructures ────────────── v0.18.20; Installed Colors ────────────────────── v0.12.11; Installed KernelAbstractions ────────── v0.9.22; Installed RandomNumbers ─────────────── v1.5.3; Installed Missings ──────────────────── v1.2.0; Installed Compat ────────────────────── v4.15.0; Installed StringManipulation ────────── v0.3.4; Installed SortingAlgorithms ─────────── v1.2.1; Installed UnsafeAtomics ─────────────── v0.2.1; Installed Atomix ────────────────────── v0.1.0; Installed LLVM ──────────────────────── v8.0.0; Installed CUDA ──────────────────────── v5.4.3; Updating `/glade/derecho/scratch/knudsenl/BottomBoundaryLayer/Project.toml`; [052768ef] + CUDA v5.4.3; Updating `/glade/derecho/scratch/knudsenl/BottomBoundaryLayer/Manifest.toml`; ERROR: LoadError: failed process: Process(`/glade/u/apps/casper/23.10/spack/opt/spack/julia/1.10.2/gcc/7.5.0/apod/bin/julia -C native -J/glade/u/apps/casper/23.10/spack/opt/spack/julia/1.10.2/gcc/7.5.0/apod/lib/julia/sys.so -g1 -O0 --color=no --history-file=no --startup-file=no --project=/glade/derecho/scratch/knudsenl/BottomBoundaryLayer/Project.toml --eval 'append!(empty!(Base.DEPOT_PATH), [""/glade/u/home/knudsenl/.julia"", ""/glade/u/apps/casper/23.10/spack/opt/spack/julia/1.10.2/gcc/7.5.0/apod/local/share/julia"", ""/glade/u/apps/casper/23.10/spack/opt/spack/julia/1.10.2/gcc/7.5.0/apod/share/julia""]); append!(empty!(Base.DL_LOAD_PATH), String[]). cd(""/glade/u/home/knudsenl/.julia/packages/CUDA_Runtime_jll/YgJCI/.pkg""); include(""/glade/u/home/knudsenl/.julia/packages/CUDA_Runtime_jll/YgJCI/.pkg/select_artifacts.jl""); ' -t1 --startup-file=no x86_64-linux-gnu-libgfortran5-cxx11-libstdcxx30-julia_version+1.10.2`, ProcessSignaled(11)) [0]. Stacktrace:; [1] pipeline_error; @ ./process.jl:565 [inlined]; [2] read(cmd::Cmd)",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2246012900:3274,Load,LoadError,3274,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2246012900,1,['Load'],['LoadError']
Performance,"─────────────────────; Static ocean benchmarks Time Allocations ; ────────────────────── ───────────────────────; Tot / % measured: 85.2s / 0.56% 8.46GiB / 0.43% . Section ncalls time %tot avg alloc %tot avg; ──────────────────────────────────────────────────────────────────────────────────────; 256×256×256 (GPU, Float64) 10 277ms 57.6% 27.7ms 9.16MiB 24.6% 938KiB; 32× 32× 32 (GPU, Float64) 10 82.2ms 17.1% 8.22ms 9.71MiB 26.1% 0.97MiB; 128×128×128 (GPU, Float64) 10 65.9ms 13.7% 6.59ms 9.16MiB 24.6% 938KiB; 64× 64× 64 (GPU, Float64) 10 55.5ms 11.6% 5.55ms 9.16MiB 24.6% 938KiB; ──────────────────────────────────────────────────────────────────────────────────────; ```. ## `arbitrary-tracers-inner-loops`. ```julia; Oceananigans package status:; Status `~/.julia/environments/v1.1/Project.toml`; [9e8cae18] Oceananigans v0.11.0 #arbitrary-tracers-inner-loops (https://github.com/climate-machine/Oceananigans.jl.git). ┌ Warning: Performing scalar operations on GPU arrays: This is very slow, consider disallowing these operations with `allowscalar(false)`; └ @ GPUArrays ~/.julia/packages/GPUArrays/fLiQ1/src/indexing.jl:16; Running static ocean benchmark: 32× 32× 32 (GPU, Float64)...; Running static ocean benchmark: 64× 64× 64 (GPU, Float64)...; Running static ocean benchmark: 128×128×128 (GPU, Float64)...; Running static ocean benchmark: 256×256×256 (GPU, Float64)...; ──────────────────────────────────────────────────────────────────────────────────────; Static ocean benchmarks Time Allocations ; ────────────────────── ───────────────────────; Tot / % measured: 86.4s / 0.56% 8.40GiB / 0.37% . Section ncalls time %tot avg alloc %tot avg; ──────────────────────────────────────────────────────────────────────────────────────; 256×256×256 (GPU, Float64) 10 288ms 59.9% 28.8ms 7.71MiB 24.5% 790KiB; 32× 32× 32 (GPU, Float64) 10 78.3ms 16.3% 7.83ms 8.39MiB 26.6% 859KiB; 128×128×128 (GPU, Float64) 10 62.0ms 12.9% 6.20ms 7.71MiB 24.5% 790KiB; 64× 64× 64 (GPU, Float64) 10 52.6ms 10.9% 5.2",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/452#issuecomment-542262852:3724,Perform,Performing,3724,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/452#issuecomment-542262852,1,['Perform'],['Performing']
Performance,"──────────────────────; Static ocean benchmarks Time Allocations ; ────────────────────── ───────────────────────; Tot / % measured: 82.9s / 0.58% 8.26GiB / 0.37% . Section ncalls time %tot avg alloc %tot avg; ──────────────────────────────────────────────────────────────────────────────────────; 256×256×256 (GPU, Float64) 10 287ms 59.8% 28.7ms 7.73MiB 24.5% 791KiB; 32× 32× 32 (GPU, Float64) 10 75.9ms 15.8% 7.59ms 8.28MiB 26.3% 848KiB; 128×128×128 (GPU, Float64) 10 66.7ms 13.9% 6.67ms 7.79MiB 24.7% 798KiB; 64× 64× 64 (GPU, Float64) 10 50.6ms 10.5% 5.06ms 7.73MiB 24.5% 791KiB; ──────────────────────────────────────────────────────────────────────────────────────; ```. ## `arbitrary-tracers-outer-loops`. ```julia; Oceananigans package status:; Status `~/.julia/environments/v1.1/Project.toml`; [9e8cae18] Oceananigans v0.11.0 #arbitrary-tracers-outer-loops (https://github.com/climate-machine/Oceananigans.jl.git). ┌ Warning: Performing scalar operations on GPU arrays: This is very slow, consider disallowing these operations with `allowscalar(false)`; └ @ GPUArrays ~/.julia/packages/GPUArrays/fLiQ1/src/indexing.jl:16; Running static ocean benchmark: 32× 32× 32 (GPU, Float64)...; Running static ocean benchmark: 64× 64× 64 (GPU, Float64)...; Running static ocean benchmark: 128×128×128 (GPU, Float64)...; Running static ocean benchmark: 256×256×256 (GPU, Float64)...; ──────────────────────────────────────────────────────────────────────────────────────; Static ocean benchmarks Time Allocations ; ────────────────────── ───────────────────────; Tot / % measured: 85.2s / 0.56% 8.46GiB / 0.43% . Section ncalls time %tot avg alloc %tot avg; ──────────────────────────────────────────────────────────────────────────────────────; 256×256×256 (GPU, Float64) 10 277ms 57.6% 27.7ms 9.16MiB 24.6% 938KiB; 32× 32× 32 (GPU, Float64) 10 82.2ms 17.1% 8.22ms 9.71MiB 26.1% 0.97MiB; 128×128×128 (GPU, Float64) 10 65.9ms 13.7% 6.59ms 9.16MiB 24.6% 938KiB; 64× 64× 64 (GPU, Float64) 10 55.5ms 11.6% 5.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/452#issuecomment-542262852:2279,Perform,Performing,2279,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/452#issuecomment-542262852,1,['Perform'],['Performing']
Performance,"──────────────────────────────┴─────────┴─────────┴─────────┘; ```. ### Relative performance on the CPU. ```; Fourier-tridiagonal Poisson solver relative performance (CPU); ┌───────────────┬─────┬───────────────────────────────┬──────────┬──────────┬────────┐; │ Architectures │ Ns │ Topologies │ slowdown │ memory │ allocs │; ├───────────────┼─────┼───────────────────────────────┼──────────┼──────────┼────────┤; │ CPU │ 256 │ (Bounded, Bounded, Bounded) │ 1.58185 │ 1.0 │ 1.0 │; │ CPU │ 256 │ (Bounded, Periodic, Bounded) │ 1.24529 │ 0.922481 │ 1.0 │; │ CPU │ 256 │ (Periodic, Bounded, Bounded) │ 1.27117 │ 0.922481 │ 1.0 │; │ CPU │ 256 │ (Periodic, Periodic, Bounded) │ 1.0 │ 1.0 │ 1.0 │; └───────────────┴─────┴───────────────────────────────┴──────────┴──────────┴────────┘; ```. ### Relative performance on the GPU. ```; Fourier-tridiagonal Poisson solver relative performance (GPU); ┌───────────────┬─────┬───────────────────────────────┬──────────┬─────────┬─────────┐; │ Architectures │ Ns │ Topologies │ slowdown │ memory │ allocs │; ├───────────────┼─────┼───────────────────────────────┼──────────┼─────────┼─────────┤; │ GPU │ 256 │ (Bounded, Bounded, Bounded) │ 3.12065 │ 3.32057 │ 3.02069 │; │ GPU │ 256 │ (Bounded, Periodic, Bounded) │ 2.41833 │ 2.26316 │ 2.16897 │; │ GPU │ 256 │ (Periodic, Bounded, Bounded) │ 2.41007 │ 2.26316 │ 2.16897 │; │ GPU │ 256 │ (Periodic, Periodic, Bounded) │ 1.0 │ 1.0 │ 1.0 │; └───────────────┴─────┴───────────────────────────────┴──────────┴─────────┴─────────┘; ```. ---. ## FFT-based Poisson solver. ### Raw benchmarks. ```; FFT-based Poisson solver benchmarks ; ┌───────────────┬─────┬───────────────────────────────┬────────────┬────────────┬────────────┬────────────┬───────────┬────────┐; │ Architectures │ Ns │ Topologies │ min │ median │ mean │ max │ memory │ allocs │; ├───────────────┼─────┼───────────────────────────────┼────────────┼────────────┼────────────┼────────────┼───────────┼────────┤; │ CPU │ 256 │ (Bounded, Bounded, Bounded) │",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1403#issuecomment-786398050:9023,perform,performance,9023,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1403#issuecomment-786398050,1,['perform'],['performance']
Performance,"───────────────────────────┬─────────┬─────────┬────────┐; │ Ns │ Topologies │ speedup │ memory │ allocs │; ├─────┼────────────────────────────────┼─────────┼─────────┼────────┤; │ 192 │ (Bounded, Bounded, Bounded) │ 33.4393 │ 448.0 │ 226.0 │; │ 192 │ (Bounded, Bounded, Periodic) │ 32.0602 │ 368.0 │ 325.5 │; │ 192 │ (Bounded, Periodic, Bounded) │ 34.7631 │ 366.8 │ 322.5 │; │ 192 │ (Bounded, Periodic, Periodic) │ 17.1932 │ 178.2 │ 147.0 │; │ 192 │ (Periodic, Bounded, Bounded) │ 36.7915 │ 368.0 │ 325.5 │; │ 192 │ (Periodic, Bounded, Periodic) │ 35.3884 │ 196.0 │ 193.0 │; │ 192 │ (Periodic, Periodic, Bounded) │ 49.4769 │ 178.2 │ 147.0 │; │ 192 │ (Periodic, Periodic, Periodic) │ 58.4816 │ 5.83333 │ 7.75 │; └─────┴────────────────────────────────┴─────────┴─────────┴────────┘; ```. ## CPU slowdown (vs. triply-periodic). ```; FFT-based Poisson solver relative performance (CPU); ┌───────────────┬─────┬────────────────────────────────┬──────────┬──────────┬────────┐; │ Architectures │ Ns │ Topologies │ slowdown │ memory │ allocs │; ├───────────────┼─────┼────────────────────────────────┼──────────┼──────────┼────────┤; │ CPU │ 192 │ (Bounded, Bounded, Bounded) │ 2.76522 │ 1.0 │ 1.0 │; │ CPU │ 192 │ (Bounded, Bounded, Periodic) │ 2.14077 │ 0.833333 │ 0.5 │; │ CPU │ 192 │ (Bounded, Periodic, Bounded) │ 2.32425 │ 0.833333 │ 0.5 │; │ CPU │ 192 │ (Bounded, Periodic, Periodic) │ 1.64349 │ 0.833333 │ 0.5 │; │ CPU │ 192 │ (Periodic, Bounded, Bounded) │ 2.44462 │ 0.833333 │ 0.5 │; │ CPU │ 192 │ (Periodic, Bounded, Periodic) │ 1.79278 │ 0.833333 │ 0.5 │; │ CPU │ 192 │ (Periodic, Periodic, Bounded) │ 1.72073 │ 0.833333 │ 0.5 │; │ CPU │ 192 │ (Periodic, Periodic, Periodic) │ 1.0 │ 1.0 │ 1.0 │; └───────────────┴─────┴────────────────────────────────┴──────────┴──────────┴────────┘; ```. ## GPU slowdown (vs. triply-periodic). ```; FFT-based Poisson solver relative performance (GPU); ┌───────────────┬─────┬────────────────────────────────┬──────────┬─────────┬─────────┐; │ Architectures │ ",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1338#issuecomment-773394296:3722,perform,performance,3722,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1338#issuecomment-773394296,1,['perform'],['performance']
Performance,"────────────┬────────────┬──────────┬────────┬─────────┐; │ size │ ranks │ min │ median │ mean │ max │ memory │ allocs │ samples │; ├─────────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼──────────┼────────┼─────────┤; │ (256, 256, 256) │ (1, 1, 1) │ 4.412 s │ 4.486 s │ 4.486 s │ 4.560 s │ 1.89 MiB │ 2623 │ 2 │; │ (256, 256, 256) │ (1, 2, 1) │ 2.275 s │ 2.303 s │ 2.297 s │ 2.312 s │ 1.74 MiB │ 2979 │ 6 │; │ (256, 256, 256) │ (1, 4, 1) │ 1.095 s │ 1.149 s │ 1.152 s │ 1.280 s │ 1.74 MiB │ 3019 │ 20 │; │ (256, 256, 256) │ (1, 8, 1) │ 583.413 ms │ 667.475 ms │ 663.315 ms │ 924.523 ms │ 1.74 MiB │ 3099 │ 64 │; │ (256, 256, 256) │ (1, 16, 1) │ 341.718 ms │ 383.898 ms │ 405.727 ms │ 771.915 ms │ 1.76 MiB │ 3259 │ 160 │; └─────────────────┴────────────┴────────────┴────────────┴────────────┴────────────┴──────────┴────────┴─────────┘. Incompressible model strong scaling speedup; ┌─────────────────┬────────────┬─────────┬────────────┬──────────┬─────────┐; │ size │ ranks │ speedup │ efficiency │ memory │ allocs │; ├─────────────────┼────────────┼─────────┼────────────┼──────────┼─────────┤; │ (256, 256, 256) │ (1, 1, 1) │ 1.0 │ 1.0 │ 1.0 │ 1.0 │; │ (256, 256, 256) │ (1, 2, 1) │ 1.94757 │ 0.973786 │ 0.918534 │ 1.13572 │; │ (256, 256, 256) │ (1, 4, 1) │ 3.90488 │ 0.976221 │ 0.91981 │ 1.15097 │; │ (256, 256, 256) │ (1, 8, 1) │ 6.72045 │ 0.840057 │ 0.922588 │ 1.18147 │; │ (256, 256, 256) │ (1, 16, 1) │ 11.6847 │ 0.730294 │ 0.928952 │ 1.24247 │; └─────────────────┴────────────┴─────────┴────────────┴──────────┴─────────┘; ```; ![ss_incompressible_times](https://user-images.githubusercontent.com/45054739/122680369-c5f29b80-d1bc-11eb-96eb-79174872e345.png); ![ss_incompressible_efficiency](https://user-images.githubusercontent.com/45054739/122680383-d0ad3080-d1bc-11eb-9939-5af092b042da.png). The overall trend looks like that efficiency plateaus off at around 75% when using 32 or more cores. We'll be trying to benchmark the GPUs' scaling performance next.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1722#issuecomment-864573659:4985,perform,performance,4985,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1722#issuecomment-864573659,1,['perform'],['performance']
Performance,"───┬─────────┬─────────┬─────────┐; │ Ns │ Topologies │ speedup │ memory │ allocs │; ├─────┼────────────────────────────────┼─────────┼─────────┼─────────┤; │ 192 │ (Bounded, Bounded, Bounded) │ 107.967 │ 2.24983 │ 3.89024 │; │ 192 │ (Bounded, Bounded, Periodic) │ 122.789 │ 2.55222 │ 3.89223 │; │ 192 │ (Bounded, Periodic, Bounded) │ 103.133 │ 2.56202 │ 3.83395 │; │ 192 │ (Bounded, Periodic, Periodic) │ 84.934 │ 2.95066 │ 3.86268 │; │ 192 │ (Periodic, Bounded, Bounded) │ 102.06 │ 2.56254 │ 3.83673 │; │ 192 │ (Periodic, Bounded, Periodic) │ 108.457 │ 2.95943 │ 3.91362 │; │ 192 │ (Periodic, Periodic, Bounded) │ 161.616 │ 2.96456 │ 3.81949 │; │ 192 │ (Periodic, Periodic, Periodic) │ 178.682 │ 3.4072 │ 3.38448 │; └─────┴────────────────────────────────┴─────────┴─────────┴─────────┘; ```. ## CPU slowdown (vs. triply-periodic). ```; Topologies relative performance (CPU); ┌───────────────┬─────┬────────────────────────────────┬──────────┬─────────┬─────────┐; │ Architectures │ Ns │ Topologies │ slowdown │ memory │ allocs │; ├───────────────┼─────┼────────────────────────────────┼──────────┼─────────┼─────────┤; │ CPU │ 192 │ (Bounded, Bounded, Bounded) │ 1.34309 │ 1.46266 │ 1.48014 │; │ CPU │ 192 │ (Bounded, Bounded, Periodic) │ 1.25281 │ 1.30927 │ 1.30084 │; │ CPU │ 192 │ (Bounded, Periodic, Bounded) │ 1.05249 │ 1.30927 │ 1.30084 │; │ CPU │ 192 │ (Bounded, Periodic, Periodic) │ 1.07645 │ 1.14247 │ 1.08664 │; │ CPU │ 192 │ (Periodic, Bounded, Bounded) │ 1.0409 │ 1.30927 │ 1.30084 │; │ CPU │ 192 │ (Periodic, Bounded, Periodic) │ 0.938853 │ 1.14247 │ 1.08664 │; │ CPU │ 192 │ (Periodic, Periodic, Bounded) │ 1.17749 │ 1.14247 │ 1.08664 │; │ CPU │ 192 │ (Periodic, Periodic, Periodic) │ 1.0 │ 1.0 │ 1.0 │; └───────────────┴─────┴────────────────────────────────┴──────────┴─────────┴─────────┘; ```. ## GPU slowdown (vs. triply-periodic). ```; Topologies relative performance (GPU); ┌───────────────┬─────┬────────────────────────────────┬──────────┬──────────┬─────────┐; │ Architectu",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1338#issuecomment-773394296:8955,perform,performance,8955,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1338#issuecomment-773394296,1,['perform'],['performance']
Performance,"│ 684.541 μs │ 688.275 μs │ 811.874 μs │ 1.463 ms │ 0 bytes │ 0 │; │ CPU │ 256 │ 3 │ 93.902 ms │ 94.489 ms │ 94.604 ms │ 95.639 ms │ 0 bytes │ 0 │; └───────────────┴─────┴───────────┴────────────┴────────────┴────────────┴────────────┴───────────┴────────┘. 3D FFT --> 3 × 1D FFTs slowdown:; CPU, 16: 1.0807x; CPU, 64: 1.0053x; CPU, 256: 1.1567x; ```. # To batch or not to batch for CUFFT on GPUs?. We should investigate this separately for CUFFT since FFT along dimension 2 requires a transpose. TODO:; - [x] Figure out how to do a FFT_y on the GPU!; - [x] Implement and benchmark doing it the distributed way.; - [x] Benchmark 1 3D FFT with 3 1D FFTs.; - [x] Benchmark 1 3D DCT with 3 1D DCTs. Same benchmarks for the GPU are posted below. Batching is much faster (by a factor of 2-3) so we should batch when possible. Note that FFTs along non-batched dimensions (dimension 2 in this case) are much slower since it involves two transpose operations. Batching will not be possible for some topologies in which cases so we'll take a performance hit. But if the pressure solver is still 10~15% then a 2x hit on the pressure solver is not that large. The hit will mostly affect topologies we don't currently support anyways. ```; FFT benchmarks; ┌───────────────┬─────┬───────────┬────────────┬────────────┬────────────┬────────────┬───────────┬────────┐; │ Architectures │ Ns │ dims │ min │ median │ mean │ max │ memory │ allocs │; ├───────────────┼─────┼───────────┼────────────┼────────────┼────────────┼────────────┼───────────┼────────┤; │ GPU │ 16 │ (1, 2, 3) │ 25.478 μs │ 32.459 μs │ 122.062 μs │ 703.376 μs │ 224 bytes │ 13 │; │ GPU │ 64 │ (1, 2, 3) │ 67.226 μs │ 71.497 μs │ 146.042 μs │ 647.734 μs │ 224 bytes │ 13 │; │ GPU │ 256 │ (1, 2, 3) │ 2.982 ms │ 3.041 ms │ 3.036 ms │ 3.116 ms │ 224 bytes │ 13 │; │ GPU │ 16 │ 1 │ 14.755 μs │ 30.020 μs │ 107.932 μs │ 677.045 μs │ 96 bytes │ 5 │; │ GPU │ 64 │ 1 │ 26.521 μs │ 41.294 μs │ 114.587 μs │ 674.834 μs │ 96 bytes │ 5 │; │ GPU │ 256 │ 1 │ 9",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1338:4585,perform,performance,4585,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1338,1,['perform'],['performance']
Performance,"│ 7.35221 │; │ (2, 0) │ 111.678 │ 1.66267 │ 6.91569 │; │ (2, 3) │ 118.737 │ 1.55043 │ 4.57587 │; │ (2, 5) │ 133.803 │ 1.5155 │ 4.87734 │; │ (2, 10) │ 137.615 │ 1.44535 │ 4.0466 │; └─────────┴─────────┴─────────┴─────────┘. Arbitrary tracers relative performance (CPU); ┌───────────────┬─────────┬──────────┬─────────┬─────────┐; │ Architectures │ tracers │ slowdown │ memory │ allocs │; ├───────────────┼─────────┼──────────┼─────────┼─────────┤; │ CPU │ (0, 0) │ 1.0 │ 1.0 │ 1.0 │; │ CPU │ (0, 1) │ 1.09293 │ 1.39873 │ 1.17271 │; │ CPU │ (0, 2) │ 1.15948 │ 1.99019 │ 1.38345 │; │ CPU │ (1, 0) │ 1.06409 │ 1.39873 │ 1.17271 │; │ CPU │ (2, 0) │ 1.17887 │ 1.99054 │ 1.38949 │; │ CPU │ (2, 3) │ 1.55493 │ 4.04677 │ 2.37198 │; │ CPU │ (2, 5) │ 1.97115 │ 5.84537 │ 2.96377 │; │ CPU │ (2, 10) │ 2.6031 │ 11.7179 │ 4.63889 │; └───────────────┴─────────┴──────────┴─────────┴─────────┘. Arbitrary tracers relative performance (GPU); ┌───────────────┬─────────┬──────────┬─────────┬─────────┐; │ Architectures │ tracers │ slowdown │ memory │ allocs │; ├───────────────┼─────────┼──────────┼─────────┼─────────┤; │ GPU │ (0, 0) │ 1.0 │ 1.0 │ 1.0 │; │ GPU │ (0, 1) │ 1.0941 │ 1.39053 │ 1.16013 │; │ GPU │ (0, 2) │ 1.19399 │ 1.85081 │ 1.29592 │; │ GPU │ (1, 0) │ 1.08489 │ 1.39037 │ 1.15883 │; │ GPU │ (2, 0) │ 1.19157 │ 1.85109 │ 1.29153 │; │ GPU │ (2, 3) │ 1.47824 │ 3.50924 │ 1.45881 │; │ GPU │ (2, 5) │ 1.66293 │ 4.95474 │ 1.94286 │; │ GPU │ (2, 10) │ 2.13524 │ 9.47276 │ 2.52301 │; └───────────────┴─────────┴──────────┴─────────┴─────────┘; ```; Some errors were encountered running the turbulence closure benchmark script with grid size 256 x 256 x 128.; There was an issue with the Nothing closure which was avoided by removing that type of closure from the closure array.; ```. Oceananigans v0.58.1; Julia Version 1.6.0; Commit f9720dc2eb (2021-03-24 12:55 UTC); Platform Info:; OS: Linux (x86_64-pc-linux-gnu); CPU: Intel(R) Xeon(R) Silver 4216 CPU @ 2.10GHz; WORD_SIZE: 64; LIBM: libopenlibm; LLVM: li",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1722:10893,perform,performance,10893,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1722,1,['perform'],['performance']
Performance,┤; │ 0 │ 82.2319 │ 3.01565 │ 6.08156 │; │ 1 │ 83.1848 │ 3.01946 │ 6.06663 │; │ 10 │ 83.6845 │ 3.01704 │ 6.0425 │; │ 100 │ 84.1322 │ 3.01704 │ 6.0425 │; │ 1000 │ 84.6106 │ 3.01946 │ 6.06663 │; │ 10000 │ 84.072 │ 3.01704 │ 6.0425 │; │ 100000 │ 86.1581 │ 3.01704 │ 6.0425 │; │ 1000000 │ 106.432 │ 3.01946 │ 6.06663 │; │ 10000000 │ 254.889 │ 3.01935 │ 6.06558 │; │ 100000000 │ 618.565 │ 3.01714 │ 6.04355 │; └─────────────┴─────────┴─────────┴─────────┘; ```. ```; Lagrangian particle tracking relative performance (CPU); ┌───────────────┬─────────────┬──────────┬─────────┬─────────┐; │ Architectures │ N_particles │ slowdown │ memory │ allocs │; ├───────────────┼─────────────┼──────────┼─────────┼─────────┤; │ CPU │ 0 │ 1.0 │ 1.0 │ 1.0 │; │ CPU │ 1 │ 1.03447 │ 1.01267 │ 1.01599 │; │ CPU │ 10 │ 1.04601 │ 1.01267 │ 1.01599 │; │ CPU │ 100 │ 1.04712 │ 1.01267 │ 1.01599 │; │ CPU │ 1000 │ 1.05514 │ 1.01267 │ 1.01599 │; │ CPU │ 10000 │ 1.05397 │ 1.01267 │ 1.01599 │; │ CPU │ 100000 │ 1.07213 │ 1.01267 │ 1.01599 │; │ CPU │ 1000000 │ 1.34006 │ 1.01267 │ 1.01599 │; │ CPU │ 10000000 │ 4.09045 │ 1.01267 │ 1.01599 │; │ CPU │ 100000000 │ 31.6534 │ 1.01267 │ 1.01599 │; └───────────────┴─────────────┴──────────┴─────────┴─────────┘; ```. ```; Lagrangian particle tracking relative performance (GPU); ┌───────────────┬─────────────┬──────────┬─────────┬─────────┐; │ Architectures │ N_particles │ slowdown │ memory │ allocs │; ├───────────────┼─────────────┼──────────┼─────────┼─────────┤; │ GPU │ 0 │ 1.0 │ 1.0 │ 1.0 │; │ GPU │ 1 │ 1.02262 │ 1.01395 │ 1.0135 │; │ GPU │ 10 │ 1.02786 │ 1.01314 │ 1.00947 │; │ GPU │ 100 │ 1.02347 │ 1.01314 │ 1.00947 │; │ GPU │ 1000 │ 1.02548 │ 1.01395 │ 1.0135 │; │ GPU │ 10000 │ 1.0309 │ 1.01314 │ 1.00947 │; │ GPU │ 100000 │ 1.02327 │ 1.01314 │ 1.00947 │; │ GPU │ 1000000 │ 1.03536 │ 1.01395 │ 1.0135 │; │ GPU │ 10000000 │ 1.31966 │ 1.01391 │ 1.01332 │; │ GPU │ 100000000 │ 4.20799 │ 1.01317 │ 1.00964 │; └───────────────┴─────────────┴──────────┴─────────┴─────────┘; ```,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1091#issuecomment-732529975:4831,perform,performance,4831,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1091#issuecomment-732529975,1,['perform'],['performance']
Performance,"✓ PrettyTables; ✓ GPUCompiler; ✓ DataFrames; ✗ CUDA; 61 dependencies successfully precompiled in 190 seconds. 5 already precompiled. The following 1 direct dependency failed to precompile:. CUDA [052768ef-5323-5732-b1bb-66c8b64840ba]. Failed to precompile CUDA [052768ef-5323-5732-b1bb-66c8b64840ba] to ""/glade/u/home/knudsenl/.julia/compiled/v1.10/CUDA/jl_UQIv2i"".; [45592] signal (11.1): Segmentation fault; in expression starting at /glade/u/home/knudsenl/.julia/packages/CUDA/Tl08O/src/CUDA.jl:25; Allocations: 2907 (Pool: 2898; Big: 9); GC: ERROR: LoadError: Failed to precompile CUDA [052768ef-5323-5732-b1bb-66c8b64840ba] to ""/glade/u/home/knudsenl/.julia/compiled/v1.10/CUDA/jl_CUC33l"".; Stacktrace:; [1] error(s::String); @ Base ./error.jl:35; [2] compilecache(pkg::Base.PkgId, path::String, internal_stderr::IO, internal_stdout::IO, keep_loaded_modules::Bool); @ Base ./loading.jl:2468; [3] compilecache; @ ./loading.jl:2340 [inlined]; [4] (::Base.var""#968#969""{Base.PkgId})(); @ Base ./loading.jl:1974; [5] mkpidlock(f::Base.var""#968#969""{Base.PkgId}, at::String, pid::Int32; kwopts::@Kwargs{stale_age::Int64, wait::Bool}); @ FileWatching.Pidfile /glade/u/apps/casper/23.10/spack/opt/spack/julia/1.10.2/gcc/7.5.0/apod/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:93; [6] #mkpidlock#6; @ /glade/u/apps/casper/23.10/spack/opt/spack/julia/1.10.2/gcc/7.5.0/apod/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:88 [inlined]; [7] trymkpidlock(::Function, ::Vararg{Any}; kwargs::@Kwargs{stale_age::Int64}); @ FileWatching.Pidfile /glade/u/apps/casper/23.10/spack/opt/spack/julia/1.10.2/gcc/7.5.0/apod/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:111; [8] #invokelatest#2; @ ./essentials.jl:894 [inlined]; [9] invokelatest; @ ./essentials.jl:889 [inlined]; [10] maybe_cachefile_lock(f::Base.var""#968#969""{Base.PkgId}, pkg::Base.PkgId, srcpath::String; stale_age::Int64); @ Base ./loading.jl:2983; [11] maybe_cachefile_lock; @ ./loading.jl:2980 [inlined]; [12] _require(pkg::Bas",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2245919472:5319,load,loading,5319,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2245919472,1,['load'],['loading']
Safety," ; > > ; > > But for one layer, WENO vector invariant plus WENO for mass flux may be sufficient, and we don't need any other dissipation (which is nice, less work for us).; > ; > I agree. If we try using whatever closure here that is used in the hydrostatic model, I would think that it should yield a similar result. But that's maybe better saved for a future PR?. Let me clarify. We've never run the hydrostatic model with a single layer on the sphere, but this is a good idea. Most of our runs have been with 50 vertical levels. In the 3D configuration with 50 vertical levels, we smooth results with 1) WENO5 vector invariant advection and 2) biharmonic ""divergence damping"" of the divergence component of the flow. If we are going to compare the shallow water on a sphere to they hydrostatic model, we will have to produce additional runs with just 1 vertical level. In other words, there are no results to compare with right now. When we perform additional runs, we may choose to include divergence damping, or another kind of closure, if we want to. The results presented in #2317 also use a single layer hydrostatic model, and these did not require any explicit dissipation. Therefore, there is reason to believe that divergence damping is only required for 3D simulations. An additional point is that in the shallow water equations we can introduce a WENO reconstruction of the mass flux. This reconstruction may provide divergence damping (whether or not we need it!). In summary, if we would like to perform a comparison, we can choose to either 1) produce a comparison between the shallow water model and one layer hydrostatic model with no explicit closure on the sphere or 2) run new 1 layer hydrostatic simulations with some closure (of our choosing) and then _also_ implement that closure in the shallow water model. My opinion is that with these nice WENO5 numerics there may not be a need for using explicit dissipation right now, so we might as well do the easy thing and avoid it.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2522#issuecomment-1119959226:2104,avoid,avoid,2104,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2522#issuecomment-1119959226,1,['avoid'],['avoid']
Safety," It's only an opinion piece, so obviously not the objective truth, but I quite agree with it. This passage stuck with me:. > Sometimes the quest for better simulations subordinates even simple physics. About 20 years ago I pointed out that most models of that era neglected to turn dissipated kinetic energy back into heat. For most atmospheric phenomena, this is indeed a small term in the thermodynamic energy budget (though technically required to close any net energy budget), but in strong windstorms like hurricanes, it becomes important. Moreover, no substantial computational benefit accrues from neglecting it. A few weeks later, a researcher came to me to report that he had added this term to his model and found that it made simulated hurricanes too intense, so he took it out again. [...] For this researcher, getting the “right answer” was the goal, even if it is obtained for the wrong reasons. Still today, the conversion of dissipated kinetic energy back into heat remains an optional switch (whose default position is “off”) in a state‐of‐the‐art hurricane prediction model. I'm definitely not accusing anyone of doing this here! But I just wanted to explain a bit where I'm coming from. That said, I agree with @glwagner that maybe we should focus on the more pressing issues at hand:. - 1 - Should Oceananigans keep adding the molecular viscosity as a background to the eddy viscosity? My vote is yes since it's more inline with the physics, which are additive, and we don't gain significant computational efficiency from neglecting it. But that's just my vote. (I also like calling it `ν_total`, as it is unambiguous.); - 2 - What should we do about the numerical implementation docs being out of sync with both the physics docs and the code? I'm happy to change the docs myself, but I think we haven't converged on how to do that yet. I'm okay with nuking one of the docs sections. I don't see why two separate sections are needed and the [physics / turbulence closures](https://",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1277#issuecomment-775361802:1733,predict,prediction,1733,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1277#issuecomment-775361802,1,['predict'],['prediction']
Safety," Random.seed!(rng, 1414); Ξ(z) = randn(rng) * z / model.grid.Lz * (1 + z / model.grid.Lz) ; Tᵢ(x, y, z) = 20 + dTdz * z + dTdz * model.grid.Lz * 1e-6 * Ξ(z); uᵢ(x, y, z) = sqrt(abs(Qᵘ)) * 1e-3 * Ξ(z); set!(model, u=uᵢ, w=uᵢ, T=Tᵢ, S=35); ```. You can try replacing these lines with something like. ```julia; Random.seed!(1414). T = model.tracers.T; u, v, w = model.velocities. x, y, z = nodes(T, reshape=true); Lz = model.grid.Lz. shape = @. z / Lz * (1 + z / Lz); ΞT = randn(size(T)...) *. shape; Ξu = randn(size(u)...) *. shape; Ξw = randn(size(w)...) *. shape. Tᵢ = @. 20 + dTdz * z + dTdz * Lz * 1e-6 * ΞT; uᵢ = @. sqrt(abs(Qᵘ)) * 1e-3 * Ξu; wᵢ = @. sqrt(abs(Qᵘ)) * 1e-3 * Ξw. set!(model, u=uᵢ, w=wᵢ, T=Tᵢ, S=35); ```. I'd be curious to know if this works. Here are a few more tips and best practices for raising issues here:. * Please reduce your code to minimum working examples. The script that was linked contains a lot of extraneous code (including comments copy/pasted from an example) that is irrelevant to the issue we are discussing. Reducing your code to a minimum example is kind to the community and will help people answer your questions faster. * If you can, avoid linking to code and instead paste your code directly into the issue. * Use [github's markdown formatting](https://docs.github.com/en/get-started/writing-on-github/getting-started-with-writing-and-formatting-on-github/basic-writing-and-formatting-syntax) (such as triple backticks, ""```"") to format code and julia prompt instructions. * Finally, I strongly recommend using environments to manage the Oceananigans version (though I don't think this is intrinsic to the issue, it will help us rule out a lot of possible issues if we can focus on one Oceananigans version). We have written some tips in our wiki: https://github.com/CliMA/Oceananigans.jl/wiki/Productive-Oceananigans-workflows-and-Julia-environments which includes a link to the official Julia documentation. * Sounds like LESbrary.jl needs to be updated!",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2766#issuecomment-1267909092:1853,avoid,avoid,1853,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2766#issuecomment-1267909092,1,['avoid'],['avoid']
Safety," a similar option could be given to FPlane, avoiding confusion. Ah, the issue is that abstract types cannot have properties --- they can only be used to organize type parameters and dispatch.; The logic and motivation of your suggestion is sound though (avoiding code duplication through good design); we just have to come up with a different solution. A similar solution could perhaps design a more hierarchical interface to `IncompressibleModel.buoyancy`. For example, we might write. ```julia; struct Buoyancy{G, B}; gravitational_direction :: G; model :: B; end; ```. The existing subtypes of `AbstractBuoyancy` are used for `Buoyancy.model`. The user API could be. ```julia; buoyancy = Buoyancy(gravitational_direction=(0.1, 0, 0.9), model=BuoyancyTracer()); ```. or. ```julia; buoyancy = Buoyancy(gravitational_direction=(0.1, 0, 0.9),; model=SeawaterBuoyancy(gravitational_acceleration=9.81, equation_of_state=LinearEquationOfState(α=2e-4, β=8e-5))); ```. This is more verbose but could avoid some of the issues that @tomchor sees. As for `Plane` and `BetaPlane`, I agree that those models are really predicated on a thin aspect ratio assumption that has to do with gravitational accelerations, and therefore ""know"" about the direction of gravity. We could either add a property `gravitational_direction` or, perhaps, add some wrapper / helper functions for constructing coriolis forces in tilted domains (eg `coriolis = TiltedCoriolisForces(FPlane(f=1e-4), vertical_direction=(0.1, 0.0, 0.9))` which returns `GeneralCoriolis` with 3D Coriolis forces --- or something). I think we will start to run into these sorts of issues more generically, where we need to specify ""global"" parameters that impact multiple model components at the same time. This has cropped up in `HydrostaticFreeSurfaceModel` where `gravitational_acceleration` can be specified separately in the buoyancy model and the free surface model. Dealing with global physical parameters is the purpose of the [`CLIMAParameters`](",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1242#issuecomment-782962047:1285,avoid,avoid,1285,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1242#issuecomment-782962047,1,['avoid'],['avoid']
Safety," have met some strange things in a simple channel flow case, the velocity profile is larger than the log-low profile, and the momentum flux of the second and third points are obviously lower than bottom boundary condition (or other point near the bottom boundary). The code is written below, what causes this difference? ![u_profile (2)](https://user-images.githubusercontent.com/117068415/255586911-14e2c865-ec3b-4065-a13a-213a99257d31.png) ![uw_flux (1)](https://user-images.githubusercontent.com/117068415/255630762-1cd16836-6f59-465b-8d02-0577052b52ba.png); > ; > ```julia; > const H=15 #/m; > grid = RectilinearGrid(GPU(),size=(64,64,64), extent=(π*H, π*H, H)); > const u★=0.01 #friction velocity; > Fx(x,y,z,t)=u★^2/H #forcing; > ; > const z₀ = H*1e-4 # m (roughness length); > const κ = 0.4 # von Karman constant; > const z₁ = -1*znodes(Center,grid)[grid.Nz] # Closest grid center to the bottom; > const cᴰᵇ = (κ / log(z₁ / z₀))^2 # Drag coefficient; > ; > @inline drag_u(x, y, t, u, v, p) = - p.cᴰᵇ * √(u^2 + v^2) * (u); > @inline drag_v(x, y, t, u, v, p) = - p.cᴰᵇ * √(u^2 + v^2) * (v); > ; > drag_bc_u = FluxBoundaryCondition(drag_u, field_dependencies=(:u, :v), parameters=(; cᴰᵇ)); > drag_bc_v = FluxBoundaryCondition(drag_v, field_dependencies=(:u, :v), parameters=(; cᴰᵇ)); > ; > u_bcs = FieldBoundaryConditions(top=FluxBoundaryCondition(0.0),bottom = drag_bc_u); > v_bcs = FieldBoundaryConditions(bottom = drag_bc_v); > ; > model = NonhydrostaticModel(; grid, coriolis,; > advection = WENO(),; > timestepper = :RungeKutta3,; > tracers =(:T,:S),; > buoyancy = SeawaterBuoyancy(),; > closure = AnisotropicMinimumDissipation(),; > boundary_conditions = (u=u_bcs,v=v_bcs,T=T_bcs,S=S_bcs),; > forcing=(u=Fx,)); > ```. I also have the same problem here. I am trying to simulate the neutral turbulent boundary layer here. However, I found that the velocity shear at the first grid points is much larger than that predicted by the Monin-Obukhov similarity theory. Any ideas? @glwagner @tomchor",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3195#issuecomment-1649204834:1932,predict,predicted,1932,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3195#issuecomment-1649204834,1,['predict'],['predicted']
Safety," implement the following setup with a `VerticallyStretchedRectilinearGrid` and `AnisotropicBiharmonicDiffusivity`:. ```; ## Initializing grid; # Horizontal grid; Nx = 64; Ny = 64; Δy = 250.0; Δx = 250.0; Lx = Δx*Nx; Ly = Δy*Ny; # Vertical grid; Lz = 160; zF = collect(0:2.5:160); zF = -zF[end:-1:1]; ; Nz = length(zF) - 1; # Setup grid; grid = VerticallyStretchedRectilinearGrid(Float64; architecture = GPU(),size = (Nx,Ny,Nz), x=(0, Lx), y=(0, Ly), zF=zF, halo = (3, 3, 3), topology = (Periodic, Bounded, Bounded)). ## Turbulence closure; kappaH = 1e5 # m4/s; kappaV = 5e-5 # m2/s. ## Setting up model; model = IncompressibleModel(; architecture = GPU(),; grid = grid,; closure = (AnisotropicDiffusivity(νh=0, κh=0, κz = kappaV, νz = kappaV),; AnisotropicBiharmonicDiffusivity(νh=kappaH, κh=kappaH)); ). ## Running; simulation = Simulation(model, Δt=2, stop_iteration=1) ; run!(simulation); ```. I get the following error:. ```; ERROR: LoadError: time = 2.0, iteration = 1: NaN found in u. Aborting simulation.; Stacktrace:; [1] error(::String) at ./error.jl:33; [2] run_diagnostic!(::NaNChecker{IterationInterval,NamedTuple{(:u,),Tuple{Field{Face,Center,Center,OffsetArrays.OffsetArray{Float64,3,CuArray{Float64,3}},VerticallyStretchedRectilinearGrid{Float64,Periodic,Bounded,Bounded,OffsetArrays.OffsetArray{Float64,1,StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}},OffsetArrays.OffsetArray{Float64,1,CuArray{Float64,1}}},NamedTuple{(:x, :y, :z),Tuple{CoordinateBoundaryConditions{BoundaryCondition{Oceananigans.BoundaryConditions.Periodic,Nothing},BoundaryCondition{Oceananigans.BoundaryConditions.Periodic,Nothing}},CoordinateBoundaryConditions{BoundaryCondition{Flux,Nothing},BoundaryCondition{Flux,Nothing}},CoordinateBoundaryConditions{BoundaryCondition{Flux,Nothing},BoundaryCondition{Flux,Nothing}}}}}}}}, ::IncompressibleModel{Oceananigans.TimeSteppers.RungeKutta3TimeStepper{Float64,NamedTuple{(:u, :v, :w, :T, :S),Tuple{Field{Face,Center,Center,OffsetArra",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1574:1009,Abort,Aborting,1009,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1574,1,['Abort'],['Aborting']
Safety," into arrays (the latter is especially important in GPU code) for unused tracers or hydrostatic pressure fields. 3. Equations are constructed / specified clearly and concisely (both in source code and user scripts). 4. Users can specify arbitrary types of forcing, including numbers, arrays, or functions (solving #110). 5. We can support arbitrary tracers with various features, such as sinking/rising velocities, or reaction systems for biological/chemical tracer systems. When I have talked to various people about this, there was a concern that this system would be 'inelegant' or 'complex'. However I believe an equation abstraction system provides the opposite: with an abstraction system, equations are 'written down' in some logical place (like a file `equations.jl` in the `src` directory where they can be easily read and modified, rather than buried inside a time-stepping loop. Correspondly, our time-stepping code becomes shorter and more concise. Using multiple dispatch correctly, we avoid the `infinite if-statement` problem. This abstraction may also make the code more modular such that we move closer to supporting multiple time-steppers. Below I provide one example of an implementation that would solve some of the problems I listed. However, *this is not the only solution*, and I think we should expend some intellectual effort and have a discussion about what the best solution might be, so that we design something that is nice, easy to extend, performant, and powerful. ## A list of kernel equations in a named tuple. The simplest solution for this abstraction is probably just to add new fields to `Model` (`model.equations.velocities` and `model.equations.tracers`) that are named tuples of kernel equations. An example of how this might work while demonstrating hierarchical multiple dispatch is:. ```julia; forcing(i, j, k, grid, F::Function, u, v, w, T, S) = F(grid, u, v, w, T, S, i, j, k); forcing(i, j, k, grid, F::AbstractArray, u, v, w, T, S) = F[i, j, k]. u_eqn(i,",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/259:1609,avoid,avoid,1609,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/259,1,['avoid'],['avoid']
Safety," you will see two plots of the rates of convergence rusing schemes that exist in Oceananigans. . This plot uses the solution obtained from Oceananigans directly. The lower order methods produce the correct slopes but the high order methods flatten out because of truncation error. The error I is consistent with single precision accuracy. ; (The slopes in the legend are taken from the plot shown below and are not the slopes of the curves from Oceananigans.) . ![convergence_rates_oceananigans](https://user-images.githubusercontent.com/8239041/103024246-79841700-451d-11eb-8321-454171fa2381.png). This plot is similar but computed using my own time-stepping code but it does use `advective_tracer_flux_x`. In developing this code I was able to ensure that everything is double precision and they give the correct slopes. ![convergence_rates](https://user-images.githubusercontent.com/8239041/103024241-7721bd00-451d-11eb-8717-6437c7c2c577.png). **Good news:** The advection schemes in Oceananigans can produce the correct slopes, as predicted by theory. **Bad news:** Some part of Oceananigans (maybe times-stepping?) must use single precision accuracy, and that truncates the error of the method as a whole. Question: where is the bottleneck that reduces the global spatial accuracy from double to single precision?. This third figure shows the result for increased spatial resolution and we observe that the higher order methods saturate near `1e-16`, as you would expect from double precision. ![convergence_rates](https://user-images.githubusercontent.com/8239041/103028959-c7514d00-4526-11eb-94c6-81fb3d429882.png). In case you are interested, these are the calculations of the rates of convergence for the two sets of calculations. ```; Method = Center2ⁿᵈ, Rate of Convergence = 1.99, Expected = 2; Method = CenteredSecondOrder(), Rate of Convergence = 1.99, Expected = 2; Method = Upwind3ʳᵈ, Rate of Convergence = 2.99, Expected = 3; Method = UpwindBiasedThirdOrder(), Rate of Convergence = 3",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1276#issuecomment-750409595:1040,predict,predicted,1040,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1276#issuecomment-750409595,1,['predict'],['predicted']
Safety," Δt: 8.888 s, umax = (9.2e-02, 8.9e-02, 2.3e-02) ms⁻¹, wall time: 46.877 s; [ Info: i: 1700, t: 3.695 hr, Δt: 8.730 s, umax = (8.6e-02, 9.0e-02, 2.5e-02) ms⁻¹, wall time: 49.291 s; [ Info: i: 1800, t: 3.943 hr, Δt: 8.922 s, umax = (9.2e-02, 9.4e-02, 2.5e-02) ms⁻¹, wall time: 51.564 s; [ Info: i: 1900, t: 4.180 hr, Δt: 8.548 s, umax = (8.7e-02, 9.4e-02, 2.5e-02) ms⁻¹, wall time: 53.794 s; [ Info: Simulation is stopping. Model time 4.180 hr has hit or exceeded simulation stop time 4.000 hr.; WARNING: using Plots.grid in module Main conflicts with an existing identifier.; [ Info: Making an animation from the saved data...; [ Info: Drawing frame 1 from iteration 0 ; Illegal inttoptr; %33 = ptrtoint double addrspace(13)* %32 to i64; Illegal inttoptr; %52 = inttoptr i64 %51 to i8 addrspace(13)*. signal (6): Aborted; in expression starting at /home/raphael/Documents/Code/Oceananigans.jl/examples/langmuir_turbulence.jl:271; gsignal at /usr/bin/../lib/x86_64-linux-gnu/libc.so.6 (unknown line); abort at /usr/bin/../lib/x86_64-linux-gnu/libc.so.6 (unknown line); unknown function (ip: 0x7f83cc060d04); _ZN4llvm13FPPassManager13runOnFunctionERNS_8FunctionE at /usr/bin/../lib/x86_64-linux-gnu/libLLVM-8.so.1 (unknown line); _ZN4llvm13FPPassManager11runOnModuleERNS_6ModuleE at /usr/bin/../lib/x86_64-linux-gnu/libLLVM-8.so.1 (unknown line); _ZN4llvm6legacy15PassManagerImpl3runERNS_6ModuleE at /usr/bin/../lib/x86_64-linux-gnu/libLLVM-8.so.1 (unknown line); unknown function (ip: 0x7f83cc14aac1); unknown function (ip: 0x7f83cc14d2d8); unknown function (ip: 0x7f83cc14d8cd); unknown function (ip: 0x7f83cc088b4a); unknown function (ip: 0x7f83cc0ba082); unknown function (ip: 0x7f83cc0df84b); jl_apply_generic at /usr/bin/../lib/x86_64-linux-gnu/libjulia.so.1 (unknown line); gr_display at /home/raphael/.julia/packages/Plots/mXrnb/src/backends/gr.jl:1616; unknown function (ip: 0x7f838be529a8); gr_display at /home/raphael/.julia/packages/Plots/mXrnb/src/backends/gr.jl:674; _show at /home/raphael",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/787:2728,abort,abort,2728,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/787,1,['abort'],['abort']
Safety,""")}, ::Any) at /Users/navid/julia/usr/share/julia/stdlib/v1.5/REPL/src/REPL.jl:214; [5] display(::REPL.REPLDisplay, ::Any) at /Users/navid/julia/usr/share/julia/stdlib/v1.5/REPL/src/REPL.jl:218; [6] display(::Any) at ./multimedia.jl:328; [7] #invokelatest#1 at ./essentials.jl:710 [inlined]; [8] invokelatest at ./essentials.jl:709 [inlined]; [9] print_response(::IO, ::Any, ::Bool, ::Bool, ::Any) at /Users/navid/julia/usr/share/julia/stdlib/v1.5/REPL/src/REPL.jl:238; [10] print_response(::REPL.AbstractREPL, ::Any, ::Bool, ::Bool) at /Users/navid/julia/usr/share/julia/stdlib/v1.5/REPL/src/REPL.jl:223; [11] (::REPL.var""#do_respond#54""{Bool,Bool,REPL.var""#64#73""{REPL.LineEditREPL,REPL.REPLHistoryProvider},REPL.LineEditREPL,REPL.LineEdit.Prompt})(::Any, ::Any, ::Any) at /Users/navid/julia/usr/share/julia/stdlib/v1.5/REPL/src/REPL.jl:822; [12] #invokelatest#1 at ./essentials.jl:710 [inlined]; [13] invokelatest at ./essentials.jl:709 [inlined]; [14] run_interface(::REPL.Terminals.TextTerminal, ::REPL.LineEdit.ModalInterface, ::REPL.LineEdit.MIState) at /Users/navid/julia/usr/share/julia/stdlib/v1.5/REPL/src/LineEdit.jl:2355; [15] run_frontend(::REPL.LineEditREPL, ::REPL.REPLBackendRef) at /Users/navid/julia/usr/share/julia/stdlib/v1.5/REPL/src/REPL.jl:1144; [16] (::REPL.var""#38#42""{REPL.LineEditREPL,REPL.REPLBackendRef})() at ./task.jl:356. (Oceananigans) pkg> st; Project Oceananigans v0.40.0; Status `~/Research/Oceananigans.jl/Project.toml`; [79e6a3ab] Adapt v2.0.2; [052768ef] CUDA v1.3.3; [a8cc5b0e] Crayons v4.0.4; [7a1cc6ca] FFTW v1.2.4; [033835bb] JLD2 v0.1.14; [63c18a36] KernelAbstractions v0.4.0; [85f8d34a] NCDatasets v0.10.4; [6fe1bfb0] OffsetArrays v1.1.3; [bac558e1] OrderedCollections v1.3.0; [1bc83da4] SafeTestsets v0.0.1; [d496a93d] SeawaterPolynomials v0.2.0; [90137ffa] StaticArrays v0.12.4; [ade2ca70] Dates; [b77e0a4c] InteractiveUtils; [37e2e46d] LinearAlgebra; [56ddb016] Logging; [44cfe95a] Pkg; [de0858da] Printf; [9a3f8284] Random; [10745b16] Statistics; ```",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1026:5948,Safe,SafeTestsets,5948,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1026,1,['Safe'],['SafeTestsets']
Safety,"#1514 is failing so at this point you are using Julia v1.6 ""at your own risk"" :)",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1547#issuecomment-813710211:72,risk,risk,72,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1547#issuecomment-813710211,1,['risk'],['risk']
Safety,"#2740 Introduced memory allocation in output construction of `ComputedFields` because the field was recomputed with the new indices to satisfy sliced boundary conditions constraints. This PR changes the contents of `view(f::Field, indices...)` in order to automatically satisfy sliced boundary condition constraints and avoid unnecessary memory allocation.",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2796:320,avoid,avoid,320,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2796,1,['avoid'],['avoid']
Safety,"**IMPORTANT**: @glwagner @suyashbire1 we should delete all old repos and clone fresh. From the BFG website:. ""At this point, you're ready for everyone to ditch their old copies of the repo and do fresh clones of the nice, new pristine data. It's best to delete all old clones, as they'll have dirty history that you don't want to risk pushing back into your newly cleaned repo. "". ---. I used BFG Repo Cleaner to delete all files larger than 1 MB in git history. This deleted two files: `deep_convection_golden_master_model_checkpoint_10.jld` and `ocean_wind_mixing_and_convection.jld2`. I have a backup of the old ""dirty"" repository in case we need it for any reason. Before:; ```; $ du -hs Oceananigans.jl.git; 620M Oceananigans.jl.git; ```. After:; ```; $ du -hs Oceananigans.jl.git; 14M Oceananigans.jl.git; ```. BFG log:; ```; (base) [aramadhan@login-1 ~]$ java -jar bfg-1.13.0.jar --strip-blobs-bigger-than 1M Oceananigans.jl.git . Using repo : /home/gridsan/aramadhan/Oceananigans.jl.git. Scanning packfile for large blobs: 17491; Scanning packfile for large blobs completed in 209 ms.; Found 17 blob ids for large blobs - biggest=54015055 smallest=2524792; Total size (unpacked)=56539847; Found 155 objects to protect; Found 19 tag-pointing refs : refs/tags/v0.10.0, refs/tags/v0.10.1, refs/tags/v0.11.0, ...; Found 273 commit-pointing refs : HEAD, refs/heads/ar/lid-driven-cavity, refs/heads/ar/more-solvers, ... Protected commits; -----------------. These are your protected commits, and so their contents will NOT be altered:. * commit 91e5626e (protected by 'HEAD'). Cleaning; --------. Found 3270 commits; Cleaning commits: 100% (3270/3270); Cleaning commits completed in 35,468 ms. Updating 255 Refs; -----------------. Ref Before After ; ------------------------------------------------------------------------------; refs/heads/ar/lid-driven-cavity | 8eae1762 | 0401753a; refs/heads/ar/more-solvers | 5446ae47 | 4cf1d809; refs/heads/ar/vertically-stretched-grid | 695eb278 | 50fbc9d0; ",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/509#issuecomment-549156736:330,risk,risk,330,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/509#issuecomment-549156736,1,['risk'],['risk']
Safety,"**TL;DR: We can compress 18GB of Oceananigans simulation checkpoints into 350MB with bitrounding and lossless compression.**. #### Problem; Output is currently uncompressed in Float64 which contains; - redundancies: zeros for immersed boundaries, halos; similar/identical exponent bits; - false information: tailing mantissa bits with no mutual information to neighbouring grid points. #### Proposed solution; Bitrounding to remove false information (replaced with zero bits -> redundancies) then lossless compression to remove redundancies. I've looked into the bitwise real information content for a single checkpoint in Simone's OMIP simulations and I got this with the orange line denothing the 99.9% of real information . ![image](https://github.com/CliMA/Oceananigans.jl/assets/25530332/9a211dce-08dc-41aa-adb2-1d4c2c7d99d1). So ; - u, v have 0-2 mantissa bits of information (=keepbits) with more information in the surface layer (k=60); - w has 0 keepbits (exponent bits though!); - tempreture T (in ˚C) has 7 keepbits (that's 3-4 digits) relatively independent of depth; - salinity S has 12 at the surface which however increases to 16 in the deep ocean; - sea surface height $\eta$ is at 6 keepbits; - tendencies are generally lower but maybe then shouldn't be stored anyway (use single Euler forward instead). The checkpoint file Simone provided had; - 18GB total file size, single time step; - including 7 halo points in all directions; - 400MB are grid; - u,v,w,T,S,$\eta$ variables and 2x tendencies (AB2) for all but w, all in Float64. #### Compression options. The 18GB can be compressed into. - Only lossless: 6.9GB (2.6x), removes redundancies from halo and immersed boundaries; - Only Float32: 9GB (2x), removes only some false information in tailing bits; - Float32 then lossless: 3.25GB (5.5x); - Bitrounded then lossless: 1GB (18x); - Bitrounded, zero tendencies, then lossless: 350MB (51x), with lossy compression saving the tendencies becomes eventually pointless as restarting",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3599:202,redund,redundancies,202,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3599,3,['redund'],['redundancies']
Safety,", true finite volume operators, and the new FFT+tridiagonal Poisson solvers needed to solve for the pressure on a stretched grid. Seems like a good idea to split it up into steps with one pull request per step:; 1. Reverse the `k` index. Currently PR #462.; 2. Revise the `RegularCartesianGrid` struct so we're happy with it. Currently PR #464.; 3. Add finite volume operators as a separate piece of code. Technically they won't be tested in this PR and could have mistakes. Currently PR #283; 4. Nuke the old operators and start using the same set of finite volume operators for both `Oceananigans.Operators` and `closure_operators.jl`. This will test that the finite volume operators reduce down to the operators that currently work, but doesn't test them on a stretched grid.; 5. Implement a `VerticallyStretchedCartesianGrid`. Might have to iterate bit to figure out what we need, e.g. I think we'll want `ΔzC` to include the distance between the first cell center and the halo cell center, etc.; 6. Implement CPU and GPU pressure solvers for vertically stretched grids with tests. There will be two: one for horizontally periodic domains and another for channel models. I've figured most of this stuff out in Jupyter notebooks.; 7. Ensure that models with vertically stretched grids pass basic tests: e.g. incompressibility, tracer conservation, etc. This will test the finite volume operators.; 8. Run a model with a `VerticallyStretchedCartesianGrid` but with uniform grid spacing and make sure it produces the same numbers as a model with `RegularCartesianGrid`. This is a sanity check.; 9. Run additional tests for vertically stretched grids: e.g. vertical diffusion, internal wave, etc. This will also test boundary conditions with stretched grids.; 10. Rerun the stratified Couette flow verification experiment but with a stretching factor matching Vreugdenhil & Taylor (2018). This will test the AMD closure on stretched grids. Let me know if anyone has any thoughts. cc @jm-c @rafferrari",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/471:1875,sanity check,sanity check,1875,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/471,1,['sanity check'],['sanity check']
Safety,", Δt=25, stop_time=1e4,). using Statistics: std; using Printf; progress_message(sim) = @printf(""Iteration: %04d, time: %s, iteration×Δt: %s, std(pNHS) = %.2e\n"",; iteration(sim), sim.model.clock.time, iteration(sim) * sim.Δt, std(model.pressures.pNHS)); add_callback!(simulation, progress_message, IterationInterval(1)). simulation.output_writers[:snaps] = NetCDFOutputWriter(model, (; model.pressures.pNHS,),; filename = ""test_pressure.nc"",; schedule = TimeInterval(100),; overwrite_existing = true,); run!(simulation); ```. On main this produces stuff like:. ```; Iteration: 0001, time: 25.0, iteration×Δt: 25.0, std(pNHS) = 6.02e-03; Iteration: 0002, time: 50.0, iteration×Δt: 50.0, std(pNHS) = 6.02e-03; Iteration: 0003, time: 75.0, iteration×Δt: 75.0, std(pNHS) = 6.02e-03; Iteration: 0004, time: 99.99999999999999, iteration×Δt: 100.0, std(pNHS) = 6.02e-03; Iteration: 0005, time: 100.0, iteration×Δt: 125.0, std(pNHS) = 2.72e+10; ```. The last two lines are of note where we went from `time: 99.99999999999999` to `time: 100.0`, implying a very tiny time-step, which results in a weird pressure field, as quantified by the last output of the last line: `std(pNHS) = 2.72e+10`. Note that because of this, `time` and `iteration×Δt` don't match up anymore in the last line. Namely `time: 100.0, iteration×Δt: 125.0`. This ""misstep"" happens many times throughout the run on `main`. On this branch this doesn't happen anymore, and even after many time-steps things remain aligned (albeit with a very small round-off error):. ```; Iteration: 0396, time: 9900.0, iteration×Δt: 9900.0, std(pNHS) = 5.99e-03; Iteration: 0397, time: 9925.000000000002, iteration×Δt: 9925.0, std(pNHS) = 5.99e-03; Iteration: 0398, time: 9950.000000000004, iteration×Δt: 9950.0, std(pNHS) = 5.99e-03; Iteration: 0399, time: 9975.000000000005, iteration×Δt: 9975.0, std(pNHS) = 5.99e-03; ```. Ideally the way to really fix this would be to figure out a way to avoid round-off errors, but I haven't been able to do that yet.",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3606:2721,avoid,avoid,2721,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3606,1,['avoid'],['avoid']
Safety,",Int64,Periodic,Int64,Nothing,Nothing},Oceananigans.Solvers.DiscreteTransform{AbstractFFTs.ScaledPlan{Complex{Float64},CUDA.CUFFT.cCuFFTPlan{Complex{Float64},1,true,3},Float64},Oceananigans.Solvers.Backward,GPU,VerticallyStretchedRectilinearGrid{Float64,Periodic,Bounded,Bounded,OffsetArrays.OffsetArray{Float64,1,StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}},OffsetArrays.OffsetArray{Float64,1,CuArray{Float64,1}}},Int64,Bounded,Int64,NamedTuple{(:forward, :backward),Tuple{CuArray{Complex{Float64},3},CuArray{Complex{Float64},3}}},Tuple{Int64,Int64,Int64}}}}}},Tuple{Nothing,Nothing},NamedTuple{(:velocities, :tracers),Tuple{NamedTuple{(:u, :v, :w),Tuple{Oceananigans.Fields.ZeroField,Oceananigans.Fields.ZeroField,Oceananigans.Fields.ZeroField}},NamedTuple{(:T, :S),Tuple{Oceananigans.Fields.ZeroField,Oceananigans.Fields.ZeroField}}}},Nothing,Nothing},Int64,Array{Any,1},Int64,Float64,Float64,Float64,OrderedCollections.OrderedDict{Symbol,Oceananigans.AbstractDiagnostic},OrderedCollections.OrderedDict{Symbol,Oceananigans.AbstractOutputWriter},typeof(Oceananigans.Simulations.default_progress),Int64,Nothing}) at /home/guptam/.julia/packages/Oceananigans/wJDxT/src/Simulations/run.jl:127; [8] top-level scope at /central/home/guptam/ocean_floes/test_stratif/test_stretched.jl:80; [9] include(::Function, ::Module, ::String) at ./Base.jl:380; [10] include(::Module, ::String) at ./Base.jl:368; [11] exec_options(::Base.JLOptions) at ./client.jl:296; [12] _start() at ./client.jl:506; in expression starting at /central/home/guptam/ocean_floes/test_stratif/test_stretched.jl:76; ```. I believe this is caused by the `AnisotropicBiharmonicDiffusivity` component of the turbulence closure, since using a simple `AnisotropicDiffusivity` works fine. Also note that the above setup works on a `RegularRectilinearGrid`. Any ideas? . I am working on the `ali/unclog-docs` branch to avoid the error reported here #1571, using Julia v1.5.4 and Oceananigans v0.54.0 + GPU.",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1574:93768,avoid,avoid,93768,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1574,1,['avoid'],['avoid']
Safety,". But now I realize that because of the status of the immersed Poisson solver, the velocity along the boundary is divergent, strongly so. So, `div(u')` is large along the boundary. And when we divide by `Δt` we get something huge. The magnitude of `div(u')` also somehow seems to depend on the time step (as does the magnitude of the spurious circulation). The correct solution to this case remains at rest of course. (An aside is that this problem _could_ be avoided by separately computing the hydrostatic pressure, and then using a special horizontal gradient operators that avoid computing a hydrostatic pressure gradient across an immersed boundary. However, this would only be correct for no-flux boundary conditions on buoyancy on side walls). Anyways, apparently because of this issue with the immersed pressure solver, it seems that `div(u')` is large (because `div(u)` is large) even when `Δt = O(1e-14)`... - As a result of all of this I am confused about whether this MWE is actually reliable for debugging the issue. I guess we should expect to see problems simply when `Δt = O(eps)` because this is when `div(u') / Δt` cannot be reliably computed, I think. This leads to a fairly simple criteria for the time step that's compatible with the pressure correction. But as noted in this PR, this is not enough to fix issues with the immersed boundary MWE... but whether or not that is because of problems with the setup itself, I'm not sure... - All of that said, taking @tomchor suggestion to be more careful in updating the clock for RK3 actually does solve the MWE here. Obviously, this is again addressing the (in principle not entirely solvable) issue of error accumulation in `clock.time`, rather than addressing the other issue with very small time-steps producing an ill-posed pressure correction. I think we should fix RK3 separately, basically because we cannot completely avoid accumulating error in `clock.time`, every little thing we do to make it more accurate is a good idea.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3606#issuecomment-2153020136:2644,avoid,avoid,2644,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3606#issuecomment-2153020136,1,['avoid'],['avoid']
Safety,.2; [21216c6a] + Preferences v1.4.0; [49802e3a] + ProgressBars v1.5.0; [94ee1d12] + Quaternions v0.7.4; [74087812] + Random123 v1.6.1; [e6cf234a] + RandomNumbers v1.5.3; [c1ae055f] + RealDot v0.1.0; [3cdcf5f2] + RecipesBase v1.3.4; [189a3867] + Reexport v1.2.2; [ae029012] + Requires v1.3.0; [6038ab10] + Rotations v1.5.1; [6c6a2e73] + Scratch v1.2.0; [d496a93d] + SeawaterPolynomials v0.3.2; [66db9d55] + SnoopPrecompile v1.0.3; [276daf66] + SpecialFunctions v2.3.0; [aedffcd0] + Static v0.8.7; [0d7ed370] + StaticArrayInterface v1.4.0; [90137ffa] + StaticArrays v1.6.1; [1e83bf80] + StaticArraysCore v1.4.1; [15972242] + StaticPermutations v0.3.0; [5e0ebb24] + Strided v2.0.1; [4db3bf67] + StridedViews v0.1.2; [09ab397b] + StructArrays v0.6.15; [856f2bd8] + StructTypes v1.10.0; [3783bdb8] + TableTraits v1.0.1; [bd369af6] + Tables v1.10.1; [6aa5eb33] + TaylorSeries v0.15.2; [a759f4b9] + TimerOutputs v0.5.23; [3bb67fe8] + TranscodingStreams v0.9.13; [9d95972d] + TupleTools v1.3.0; [013be700] + UnsafeAtomics v0.2.1; [d80eeb9a] + UnsafeAtomicsLLVM v0.1.3; [81def892] + VersionParsing v1.3.0; [6e34b625] + Bzip2_jll v1.0.8+0; [4ee394cb] + CUDA_Driver_jll v0.5.0+1; [76a88914] + CUDA_Runtime_jll v0.6.0+0; [f5851436] + FFTW_jll v3.3.10+0; [0234f1f7] + HDF5_jll v1.14.0+0; [1d5cc7b8] + IntelOpenMP_jll v2023.1.0+0; [dad2f222] + LLVMExtra_jll v0.0.23+0; [1d63c593] + LLVMOpenMP_jll v15.0.4+0; [94ce4f54] + Libiconv_jll v1.16.1+2; [856f044c] + MKL_jll v2023.1.0+0; [7cb0a576] + MPICH_jll v4.1.2+0; [f1f71cc9] + MPItrampoline_jll v5.3.1+0; [9237b28f] + MicrosoftMPI_jll v10.1.3+4; [7243133f] + NetCDF_jll v400.902.208+0; [fe0851c0] + OpenMPI_jll v4.1.5+0; [458c3c95] + OpenSSL_jll v3.0.9+0; [efe28fd5] + OpenSpecFun_jll v0.5.5+0; [02c8fc9c] + XML2_jll v2.10.3+0; [3161d3a3] + Zstd_jll v1.5.5+0; [477f73a3] + libaec_jll v1.0.6+1; [0dad84c5] + ArgTools v1.1.1; [56f22d72] + Artifacts; [2a0f44e3] + Base64; [ade2ca70] + Dates; [8ba89e20] + Distributed; [f43a241f] + Downloads v1.6.0; [7b1f6079] + FileWatc,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3184#issuecomment-1636792243:3648,Unsafe,UnsafeAtomics,3648,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3184#issuecomment-1636792243,1,['Unsafe'],['UnsafeAtomics']
Safety,.3.4; Installed CEnum ─────────────────────── v0.5.0; Installed Random123 ─────────────────── v1.7.0; Installed JuliaNVTXCallbacks_jll ────── v0.2.1+0; Installed InvertedIndices ───────────── v1.3.0; Installed BFloat16s ─────────────────── v0.5.0; Installed Reexport ──────────────────── v1.2.2; Installed CUDA_Runtime_jll ──────────── v0.14.1+0; Installed GPUArrays ─────────────────── v10.2.3; Installed RandomNumbers ─────────────── v1.5.3; Installed DataFrames ────────────────── v1.6.1; Installed DataStructures ────────────── v0.18.20; Installed Compat ────────────────────── v4.15.0; Installed Requires ──────────────────── v1.3.0; Installed ExprTools ─────────────────── v0.1.10; Installed MacroTools ────────────────── v0.5.13; Installed Colors ────────────────────── v0.12.11; Installed KernelAbstractions ────────── v0.9.22; Installed CUDA ──────────────────────── v5.4.3; Installed Missings ──────────────────── v1.2.0; Installed StringManipulation ────────── v0.3.4; Installed UnsafeAtomics ─────────────── v0.2.1; Installed SortingAlgorithms ─────────── v1.2.1; Installed Atomix ────────────────────── v0.1.0; Installed LLVM ──────────────────────── v8.0.0; Precompiling project...; ✓ LLVMLoopInfo; ✓ DataValueInterfaces; ✓ Reexport; ✓ IteratorInterfaceExtensions; ✓ LaTeXStrings; ✓ InvertedIndices; ✓ ExprTools; ✓ DataAPI; ✓ Requires; ✓ CompilerSupportLibraries_jll; ✓ OrderedCollections; ✓ Compat; ✓ AbstractFFTs; ✓ InlineStrings; ✓ Scratch; ✓ CEnum; ✓ StaticArraysCore; ✓ TableTraits; ✓ Missings; ✓ PooledArrays; ✓ BFloat16s; ✓ Preferences. ✓ Adapt; ✓ Statistics; ✓ SentinelArrays; ✓ Compat → CompatLinearAlgebraExt; ✓ Crayons; ✓ UnsafeAtomics; ✓ CUDA_Runtime_Discovery; ✓ PrecompileTools; ✓ JLLWrappers; ✓ Tables; ✓ RandomNumbers; ✓ Atomix; ✓ TimerOutputs; ✓ AbstractFFTs → AbstractFFTsTestExt; ✓ NVTX_jll; ✓ JuliaNVTXCallbacks_jll; ✓ MacroTools; ✓ LLVMExtra_jll; ✓ CUDA_Driver_jll; ✓ Random123; ✓ DataStructures; ✓ StringManipulation; ✓ FixedPointNumbers; ✓ SortingAlgorithms; ✗ CU,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2245919472:3068,Unsafe,UnsafeAtomics,3068,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2245919472,1,['Unsafe'],['UnsafeAtomics']
Safety,".3.4; Installed NVTX_jll ──────────────────── v3.1.0+2; Installed LLVMExtra_jll ─────────────── v0.0.30+0; Installed LaTeXStrings ──────────────── v1.3.1; Installed OrderedCollections ────────── v1.6.3; Installed NVTX ──────────────────────── v0.3.4; Installed UnsafeAtomicsLLVM ─────────── v0.1.5; Installed CEnum ─────────────────────── v0.5.0; Installed InvertedIndices ───────────── v1.3.0; Installed Reexport ──────────────────── v1.2.2; Installed JuliaNVTXCallbacks_jll ────── v0.2.1+0; Installed BFloat16s ─────────────────── v0.5.0; Installed MacroTools ────────────────── v0.5.13; Installed DataStructures ────────────── v0.18.20; Installed Colors ────────────────────── v0.12.11; Installed KernelAbstractions ────────── v0.9.22; Installed RandomNumbers ─────────────── v1.5.3; Installed Missings ──────────────────── v1.2.0; Installed Compat ────────────────────── v4.15.0; Installed StringManipulation ────────── v0.3.4; Installed SortingAlgorithms ─────────── v1.2.1; Installed UnsafeAtomics ─────────────── v0.2.1; Installed Atomix ────────────────────── v0.1.0; Installed LLVM ──────────────────────── v8.0.0; Installed CUDA ──────────────────────── v5.4.3; Updating `/glade/derecho/scratch/knudsenl/BottomBoundaryLayer/Project.toml`; [052768ef] + CUDA v5.4.3; Updating `/glade/derecho/scratch/knudsenl/BottomBoundaryLayer/Manifest.toml`; ERROR: LoadError: failed process: Process(`/glade/u/apps/casper/23.10/spack/opt/spack/julia/1.10.2/gcc/7.5.0/apod/bin/julia -C native -J/glade/u/apps/casper/23.10/spack/opt/spack/julia/1.10.2/gcc/7.5.0/apod/lib/julia/sys.so -g1 -O0 --color=no --history-file=no --startup-file=no --project=/glade/derecho/scratch/knudsenl/BottomBoundaryLayer/Project.toml --eval 'append!(empty!(Base.DEPOT_PATH), [""/glade/u/home/knudsenl/.julia"", ""/glade/u/apps/casper/23.10/spack/opt/spack/julia/1.10.2/gcc/7.5.0/apod/local/share/julia"", ""/glade/u/apps/casper/23.10/spack/opt/spack/julia/1.10.2/gcc/7.5.0/apod/share/julia""]); append!(empty!(Base.DL_LOAD_PATH), Str",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2246012900:2904,Unsafe,UnsafeAtomics,2904,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2246012900,1,['Unsafe'],['UnsafeAtomics']
Safety,0.14.3; [5da4648a] NVTX v0.3.4; [77ba4419] NaNMath v1.0.2; [d8793406] ObjectFile v0.4.1; [9e8cae18] Oceananigans v0.90.11 `~/Research/OC11.jl`; [6fe1bfb0] OffsetArrays v1.13.0; [4d8831e6] OpenSSL v1.4.2; [bac558e1] OrderedCollections v1.6.3; [65ce6f38] PackageExtensionCompat v1.0.2; [69de0a69] Parsers v2.8.1; [0e08944d] PencilArrays v0.19.3; [4a48f351] PencilFFTs v0.15.1; [b98c9c47] Pipe v1.3.0; [eebad327] PkgVersion v0.3.3; [ccf2f8ad] PlotThemes v3.1.0; [995b91a9] PlotUtils v1.4.1; [91a5bcdd] Plots v1.40.2; [2dfb63ee] PooledArrays v1.4.3; ⌃ [aea7be01] PrecompileTools v1.2.0; [21216c6a] Preferences v1.4.3; [08abe8d2] PrettyTables v2.3.1; [49802e3a] ProgressBars v1.5.1; [94ee1d12] Quaternions v0.7.6; [74087812] Random123 v1.7.0; [e6cf234a] RandomNumbers v1.5.3; [c1ae055f] RealDot v0.1.0; [3cdcf5f2] RecipesBase v1.3.4; [01d81517] RecipesPipeline v0.6.12; [189a3867] Reexport v1.2.2; [05181044] RelocatableFolders v1.0.1; [ae029012] Requires v1.3.0; [6038ab10] Rotations v1.7.0; [1bc83da4] SafeTestsets v0.1.0; [6c6a2e73] Scratch v1.2.1; [d496a93d] SeawaterPolynomials v0.3.4; [91c51154] SentinelArrays v1.4.1; [992d4aef] Showoff v1.0.3; [777ac1f9] SimpleBufferStream v1.1.0; [a2af1166] SortingAlgorithms v1.2.1; [276daf66] SpecialFunctions v2.3.1; [aedffcd0] Static v0.8.10; [0d7ed370] StaticArrayInterface v1.5.0; [90137ffa] StaticArrays v1.9.3; [1e83bf80] StaticArraysCore v1.4.2; [15972242] StaticPermutations v0.3.0; [82ae8749] StatsAPI v1.7.0; [2913bbd2] StatsBase v0.34.2; [5e0ebb24] Strided v2.0.4; [4db3bf67] StridedViews v0.2.2; [892a3eda] StringManipulation v0.3.4; [09ab397b] StructArrays v0.6.18; [53d494c1] StructIO v0.3.0; [856f2bd8] StructTypes v1.10.0; [dc5dba14] TZJData v1.1.0+2023d; [3783bdb8] TableTraits v1.0.1; [bd369af6] Tables v1.11.1; ⌅ [6aa5eb33] TaylorSeries v0.16.0; [62fd8b95] TensorCore v0.1.1; [f269a46b] TimeZones v1.13.0; [a759f4b9] TimerOutputs v0.5.23; [bdfc003b] TimesDates v0.3.1; [3bb67fe8] TranscodingStreams v0.10.4; [9d95972d] TupleTools v1.5.0; [5c,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3511:5442,Safe,SafeTestsets,5442,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3511,1,['Safe'],['SafeTestsets']
Safety,"00) ᵒC; └ └── max|η|: 3.09e-06 m; [ Info: Simulation is stopping. Model iteration 3 has hit or exceeded simulation stop iteration 3.; ┌ Info: Iteration: 3, time: 3 seconds, wall time: 843.428 ms; │ ├── max(u): (2.48e-08, 1.37e-04, 3.09e-06) m s⁻¹; │ ├── extrema(T): (0.31, 30.00) ᵒC; └ └── max|η|: 6.17e-06 m; ```. When `IsopycnalSkewSymmetricDiffusivity` is included, however, we find. ```julia; julia> include(""near_global_one_degree.jl""); underlying_grid = LatitudeLongitudeGrid(arch; size = (Nx, Ny, Nz), halo = (4, 4, 4), latitude, z, longitude = (-180, 180), precompute_metrics = true) = 360×150×48 LatitudeLongitudeGrid{Float64, Periodic, Bounded, Bounded} on CPU with 4×4×4 halo and with precomputed metrics; ├── longitude: Periodic λ ∈ [-180.0, 180.0) regularly spaced with Δλ=1.0; ├── latitude: Bounded φ ∈ [-75.0, 75.0] regularly spaced with Δφ=1.0; └── z: Bounded z ∈ [-5244.5, 0.0] variably spaced with min(Δz)=10.0, max(Δz)=410.5; ┌ Warning: WENO on a curvilinear stretched coordinate is not validated, use at your own risk!!; └ @ Oceananigans.Advection ~/Projects/dev/Oceananigans.jl/src/Advection/weno_fifth_order.jl:160; [ Info: Initializing simulation...; ┌ Info: Iteration: 0, time: 0 seconds, wall time: 259.679 ms; │ ├── max(u): (0.00e+00, 0.00e+00, 0.00e+00) m s⁻¹; │ ├── extrema(T): (0.31, 30.00) ᵒC; └ └── max|η|: 0.00e+00 m; [ Info: ... simulation initialization complete (319.597 ms); [ Info: Executing initial time step...; [ Info: ... initial time step complete (1.116 seconds).; ┌ Info: Iteration: 1, time: 1 second, wall time: 1.121 seconds; │ ├── max(u): (1.94e-10, 4.57e-05, 1.03e-06) m s⁻¹; │ ├── extrema(T): (0.31, 30.00) ᵒC; └ └── max|η|: 1.03e-06 m; ┌ Info: Iteration: 2, time: 2 seconds, wall time: 998.405 ms; │ ├── max(u): (9.46e-09, 9.13e-05, 2.06e-06) m s⁻¹; │ ├── extrema(T): (-16387479425.34, 18491107419.68) ᵒC; └ └── max|η|: 3.09e-06 m; [ Info: Simulation is stopping. Model iteration 3 has hit or exceeded simulation stop iteration 3.; ┌ Info: Iteration:",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2463#issuecomment-1107298518:2611,risk,risk,2611,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2463#issuecomment-1107298518,1,['risk'],['risk']
Safety,"1.32e-16, 5.40e-02, 6.44e-04) m s⁻¹; │ ├── extrema(T): (0.31, 30.00) ᵒC; └ └── max|η|: 5.93e-01 m; ┌ Info: Iteration: 2, time: 40 minutes, wall time: 945.169 ms; │ ├── max(u): (1.39e-02, 1.06e-01, 1.05e-03) m s⁻¹; │ ├── extrema(T): (0.05, 30.00) ᵒC; └ └── max|η|: 1.33e+00 m; [ Info: Simulation is stopping. Model iteration 3 has hit or exceeded simulation stop iteration 3.; ┌ Info: Iteration: 3, time: 1 hour, wall time: 945.031 ms; │ ├── max(u): (3.55e-02, 1.52e-01, 1.37e-03) m s⁻¹; │ ├── extrema(T): (0.15, 30.00) ᵒC; └ └── max|η|: 2.08e+00 m; ```. But when they're put back, it still blows up (even with coefficients 1e-9):. ```julia; julia> include(""idealized_one_degree_simulation.jl""); grid = ImmersedBoundaryGrid on:; architecture: CPU(); grid: 360×150×48 LatitudeLongitudeGrid{Float64, Periodic, Bounded, Bounded} on CPU with 4×4×4 halo and with precomputed metrics; with immersed: GridFittedBottom{OffsetArrays.OffsetMatrix{Float64, Matrix{Float64}}}; ┌ Warning: WENO on a curvilinear stretched coordinate is not validated, use at your own risk!!; └ @ Oceananigans.Advection ~/Projects/dev/Oceananigans.jl/src/Advection/weno_fifth_order.jl:160; [ Info: Initializing simulation...; ┌ Info: Iteration: 0, time: 0 seconds, wall time: 256.442 ms; │ ├── max(u): (0.00e+00, 0.00e+00, 0.00e+00) m s⁻¹; │ ├── extrema(T): (0.31, 30.00) ᵒC; └ └── max|η|: 0.00e+00 m; [ Info: ... simulation initialization complete (317.156 ms); [ Info: Executing initial time step...; [ Info: ... initial time step complete (1.252 seconds).; ┌ Info: Iteration: 1, time: 20 minutes, wall time: 1.258 seconds; │ ├── max(u): (7.36e-02, 5.18e-02, 7.93e-04) m s⁻¹; │ ├── extrema(T): (0.31, 30.00) ᵒC; └ └── max|η|: 3.73e-01 m; ┌ Info: Iteration: 2, time: 40 minutes, wall time: 1.003 seconds; │ ├── max(u): (2.83e-01, 1.13e-01, 1.65e-03) m s⁻¹; │ ├── extrema(T): (-6.76, 50.82) ᵒC; └ └── max|η|: 8.05e-01 m; [ Info: Simulation is stopping. Model iteration 3 has hit or exceeded simulation stop iteration 3.; ┌ Info: Iter",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2463#issuecomment-1107311807:2232,risk,risk,2232,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2463#issuecomment-1107311807,1,['risk'],['risk']
Safety,23 v1.7.0; [e6cf234a] RandomNumbers v1.5.3; [c1ae055f] RealDot v0.1.0; [3cdcf5f2] RecipesBase v1.3.4; [189a3867] Reexport v1.2.2; [ae029012] Requires v1.3.0; [6038ab10] Rotations v1.7.1; [6c6a2e73] Scratch v1.2.1; [d496a93d] SeawaterPolynomials v0.3.4; [91c51154] SentinelArrays v1.4.5; [a2af1166] SortingAlgorithms v1.2.1; [276daf66] SpecialFunctions v2.4.0; ⌅ [aedffcd0] Static v0.8.9; [0d7ed370] StaticArrayInterface v1.5.1; [90137ffa] StaticArrays v1.9.7; [1e83bf80] StaticArraysCore v1.4.3; [15972242] StaticPermutations v0.3.0; [82ae8749] StatsAPI v1.7.0; [5e0ebb24] Strided v2.1.0; [4db3bf67] StridedViews v0.3.1; [892a3eda] StringManipulation v0.3.4; [09ab397b] StructArrays v0.6.18; [856f2bd8] StructTypes v1.10.0; [3783bdb8] TableTraits v1.0.1; [bd369af6] Tables v1.12.0; ⌅ [6aa5eb33] TaylorSeries v0.16.0; [a759f4b9] TimerOutputs v0.5.24; [3bb67fe8] TranscodingStreams v0.11.0; [24ddb15e] TransmuteDims v0.1.16; [9d95972d] TupleTools v1.5.0; [013be700] UnsafeAtomics v0.2.1; [d80eeb9a] UnsafeAtomicsLLVM v0.1.5; [81def892] VersionParsing v1.3.0; [0b7ba130] Blosc_jll v1.21.5+0; [6e34b625] Bzip2_jll v1.0.8+1; [4ee394cb] CUDA_Driver_jll v0.9.1+1; [76a88914] CUDA_Runtime_jll v0.14.1+0; [f5851436] FFTW_jll v3.3.10+0; [0951126a] GnuTLS_jll v3.8.4+0; ⌃ [0234f1f7] HDF5_jll v1.14.2+1; [e33a78d0] Hwloc_jll v2.11.0+0; [1d5cc7b8] IntelOpenMP_jll v2024.2.0+0; [9c1d0b0a] JuliaNVTXCallbacks_jll v0.2.1+0; [dad2f222] LLVMExtra_jll v0.0.30+0; [1d63c593] LLVMOpenMP_jll v15.0.7+0; [94ce4f54] Libiconv_jll v1.17.0+0; [5ced341a] Lz4_jll v1.9.4+0; [856f044c] MKL_jll v2024.2.0+0; [7cb0a576] MPICH_jll v4.2.1+1; [f1f71cc9] MPItrampoline_jll v5.4.0+0; [9237b28f] MicrosoftMPI_jll v10.1.4+2; [e98f9f5b] NVTX_jll v3.1.0+2; ⌃ [7243133f] NetCDF_jll v400.902.209+0; ⌅ [4c82536e] Nettle_jll v3.7.2+0; [fe0851c0] OpenMPI_jll v5.0.3+0; [458c3c95] OpenSSL_jll v3.0.14+0; [efe28fd5] OpenSpecFun_jll v0.5.5+0; [c2071276] P11Kit_jll v0.24.1+0; [02c8fc9c] XML2_jll v2.13.1+0; [ffd25f8a] XZ_jll v5.4.6+0; [3161d3a3] Zstd,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2233740720:19247,Unsafe,UnsafeAtomicsLLVM,19247,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2233740720,1,['Unsafe'],['UnsafeAtomicsLLVM']
Safety,"4_S4_S4_S4_S4_S4_S4_S11_S11_S4_S4_S4_S4_S4_S4_S4_S4_S4_S3_IS4_Li3ES5_IS4_Li3ELi1EEEEE12_DIC_forcingvS19_IS11_S11_S11_S11_S11_S11_S11_S11_S11_ES19_IS34_S30_S31_S32_S33_S34_S30_S31_S32_EES27_IS28_S28_S28_S18_I291__p____g_z___K_z___k_r0___k_b0_____rp_____bp___e_r___e_b___r_pig___K_par_______K_no____K_nh____v_dd_min___v_dd_max___V_d___V_dd_________p___a_z___m_z_____z___m_p_____d_____dd_________n_____p_____z_____d_____dd___Rd_phy___Rd_dom___Rd_chl_____caco3___Rd_oxy___Rd_nit___f_z___f_d_____dom___PAR_S19_IS4_S4_S4_S4_S4_S4_S4_S4_S4_S4_S11_S11_S4_S4_S4_S4_S4_S4_S11_S4_S4_S4_S4_S4_S4_S4_S4_S4_S4_S4_S11_S11_S4_S4_S4_S4_S4_S4_S4_S4_S4_S3_IS4_Li3ES5_IS4_Li3ELi1EEEEE12_ALK_forcingvS19_IS11_S11_S11_S11_S11_S11_S11_S11_S11_ES19_IS33_S34_S30_S31_S32_S33_S34_S30_S31_EES27_IS28_S28_S28_S18_I291__p____g_z___K_z___k_r0___k_b0_____rp_____bp___e_r___e_b___r_pig___K_par_______K_no____K_nh____v_dd_min___v_dd_max___V_d___V_dd_________p___a_z___m_z_____z___m_p_____d_____dd_________n_____p_____z_____d_____dd___Rd_phy___Rd_dom___Rd_chl_____caco3___Rd_oxy___Rd_nit___f_z___f_d_____dom___PAR_S19_IS4_S4_S4_S4_S4_S4_S4_S4_S4_S4_S11_S11_S4_S4_S4_S4_S4_S4_S11_S4_S4_S4_S4_S4_S4_S4_S4_S4_S4_S4_S11_S11_S4_S4_S4_S4_S4_S4_S4_S4_S4_S3_IS4_Li3ES5_IS4_Li3ELi1EEEEE12_OXY_forcingvS19_IS11_S11_S11_S11_S11_S11_S11_S11_ES19_IS32_S33_S34_S30_S31_S32_S33_S34_EEEES3_IS4_Li3ES5_IS4_Li3ELi1EEES18_I27__time___iteration___stage_S19_IS4_S11_S11_EE' uses too much parameter space (0x19a8 bytes, 0x1100 max).; ptxas fatal : Ptx assembly aborted due to errors; If you think this is a bug, please file an issue and attach /tmp/jl_4JwMaF.ptx; in expression starting at /nfs/st01/hpc-atmos-jrt51/js2430/OceanBioME.jl/examples/subpolar.jl:223; (stacktrace); (user); > Base; + error ./error.jl:33; CUDA; + cufunction_compile ~/.julia/packages/CUDA/DfvRa/src/c; + [inlined]; GPUCompiler; + JuliaContext ~/.julia/packages/GPUCompiler/N98un/src/; v CUDA. ```. (Apologies this error message is a bit mangled because I use `InteractiveErrors`)",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2700:19167,abort,aborted,19167,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2700,1,['abort'],['aborted']
Safety,"6d7b675f75dd867b9f153685""; ---; > project_hash = ""bfbc7775b0a550569ac26abdec5f544ef80e881c""; 23c23; < git-tree-sha1 = ""76289dc51920fdc6e0013c872ba9551d54961c24""; ---; > git-tree-sha1 = ""02f731463748db57cc2ebfbd9fbc9ce8280d3433""; 25c25; < version = ""3.6.2""; ---; > version = ""3.7.1""; 37c37; < git-tree-sha1 = ""f83ec24f76d4c8f525099b2ac475fc098138ec31""; ---; > git-tree-sha1 = ""16267cf279190ca7c1b30d020758ced95db89cd0""; 39c39; < version = ""7.4.11""; ---; > version = ""7.5.1""; 93,94c93,94; < deps = [""AbstractFFTs"", ""Adapt"", ""BFloat16s"", ""CEnum"", ""CUDA_Driver_jll"", ""CUDA_Runtime_Discovery"", ""CUDA_Runtime_jll"", ""Crayons"", ""DataFrames"", ""ExprTools"", ""GPUArrays"", ""GPUCompiler"", ""KernelAbstractions"", ""LLVM"", ""LazyArtifacts"", ""Libdl"", ""LinearAlgebra"", ""Logging"", ""NVTX"", ""Preferences"", ""PrettyTables"", ""Printf"", ""Random"", ""Random123"", ""RandomNumbers"", ""Reexport"", ""Requires"", ""SparseArrays"", ""Statistics"", ""UnsafeAtomicsLLVM""]; < git-tree-sha1 = ""f062a48c26ae027f70c44f48f244862aec47bf99""; ---; > deps = [""AbstractFFTs"", ""Adapt"", ""BFloat16s"", ""CEnum"", ""CUDA_Driver_jll"", ""CUDA_Runtime_Discovery"", ""CUDA_Runtime_jll"", ""Crayons"", ""DataFrames"", ""ExprTools"", ""GPUArrays"", ""GPUCompiler"", ""KernelAbstractions"", ""LLVM"", ""LLVMLoopInfo"", ""LazyArtifacts"", ""Libdl"", ""LinearAlgebra"", ""Logging"", ""NVTX"", ""Preferences"", ""PrettyTables"", ""Printf"", ""Random"", ""Random123"", ""RandomNumbers"", ""Reexport"", ""Requires"", ""SparseArrays"", ""Statistics"", ""UnsafeAtomicsLLVM""]; > git-tree-sha1 = ""64461b0e9df3069248979113ce8ab6d11bd371cf""; 96,97c96; < version = ""5.0.0""; < weakdeps = [""SpecialFunctions""]; ---; > version = ""5.1.0""; 99a99; > ChainRulesCoreExt = ""ChainRulesCore""; 101a102,105; > [deps.CUDA.weakdeps]; > ChainRulesCore = ""d360d2e6-b24c-11e9-a2a3-2a2ae2dbcce4""; > SpecialFunctions = ""276daf66-3868-5448-9aa4-cd146d93841b""; >; 104c108; < git-tree-sha1 = ""35a37bb72b35964f2895c12c687ae263b4ac170c""; ---; > git-tree-sha1 = ""1e42ef1bdb45487ff28de16182c0df4920181dc3""; 106c110; < version = ""0.6.0+3""; ---; > version = ""0.7.0+0""",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3381#issuecomment-1807091361:3425,Unsafe,UnsafeAtomicsLLVM,3425,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3381#issuecomment-1807091361,2,['Unsafe'],['UnsafeAtomicsLLVM']
Safety,"7 │ 2.37198 │; │ CPU │ (2, 5) │ 1.97115 │ 5.84537 │ 2.96377 │; │ CPU │ (2, 10) │ 2.6031 │ 11.7179 │ 4.63889 │; └───────────────┴─────────┴──────────┴─────────┴─────────┘. Arbitrary tracers relative performance (GPU); ┌───────────────┬─────────┬──────────┬─────────┬─────────┐; │ Architectures │ tracers │ slowdown │ memory │ allocs │; ├───────────────┼─────────┼──────────┼─────────┼─────────┤; │ GPU │ (0, 0) │ 1.0 │ 1.0 │ 1.0 │; │ GPU │ (0, 1) │ 1.0941 │ 1.39053 │ 1.16013 │; │ GPU │ (0, 2) │ 1.19399 │ 1.85081 │ 1.29592 │; │ GPU │ (1, 0) │ 1.08489 │ 1.39037 │ 1.15883 │; │ GPU │ (2, 0) │ 1.19157 │ 1.85109 │ 1.29153 │; │ GPU │ (2, 3) │ 1.47824 │ 3.50924 │ 1.45881 │; │ GPU │ (2, 5) │ 1.66293 │ 4.95474 │ 1.94286 │; │ GPU │ (2, 10) │ 2.13524 │ 9.47276 │ 2.52301 │; └───────────────┴─────────┴──────────┴─────────┴─────────┘; ```; Some errors were encountered running the turbulence closure benchmark script with grid size 256 x 256 x 128.; There was an issue with the Nothing closure which was avoided by removing that type of closure from the closure array.; ```. Oceananigans v0.58.1; Julia Version 1.6.0; Commit f9720dc2eb (2021-03-24 12:55 UTC); Platform Info:; OS: Linux (x86_64-pc-linux-gnu); CPU: Intel(R) Xeon(R) Silver 4216 CPU @ 2.10GHz; WORD_SIZE: 64; LIBM: libopenlibm; LLVM: libLLVM-11.0.1 (ORCJIT, cascadelake); Environment:; EBVERSIONJULIA = 1.6.0; JULIA_DEPOT_PATH = :; EBROOTJULIA = /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/julia/1.6.0; EBDEVELJULIA = /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/julia/1.6.0/easybuild/avx2-Core-julia-1.6.0-easybuild-devel; JULIA_LOAD_PATH = :; GPU: Tesla V100-SXM2-32GB. Turbulence closure benchmarks; ┌───────────────┬──────────────────────────────────┬───────────┬───────────┬───────────┬───────────┬──────────┬────────┬─────────┐; │ Architectures │ Closures │ min │ median │ mean │ max │ memory │ allocs │ samples │; ├───────────────┼──────────────────────────────────┼───────────┼───────────┼──────────",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1722:11691,avoid,avoided,11691,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1722,1,['avoid'],['avoided']
Safety,"7208cd3bc5d29631a26bc0cff78902""; uuid = ""21216c6a-2e73-6563-6e65-726566657250""; version = ""1.2.1"". [[Printf]]; deps = [""Unicode""]; uuid = ""de0858da-6303-5e67-8744-51eddeeeb8d7"". [[REPL]]; deps = [""InteractiveUtils"", ""Markdown"", ""Sockets""]; uuid = ""3fa0cd96-eef1-5676-8a61-b3b8758bbffb"". [[Random]]; deps = [""Serialization""]; uuid = ""9a3f8284-a2c9-5f02-9a11-845980a1fd5c"". [[Reexport]]; git-tree-sha1 = ""57d8440b0c7d98fc4f889e478e80f268d534c9d5""; uuid = ""189a3867-3050-52da-a836-e630ba90ab69""; version = ""1.0.0"". [[Requires]]; deps = [""UUIDs""]; git-tree-sha1 = ""4036a3bd08ac7e968e27c203d45f5fff15020621""; uuid = ""ae029012-a4dd-5104-9daa-d747884805df""; version = ""1.1.3"". [[Rotations]]; deps = [""LinearAlgebra"", ""StaticArrays"", ""Statistics""]; git-tree-sha1 = ""2ed8d8a16d703f900168822d83699b8c3c1a5cd8""; uuid = ""6038ab10-8711-5258-84ad-4b1120ba62dc""; version = ""1.0.2"". [[SHA]]; uuid = ""ea8e919c-243c-51af-8825-aaa63cd721ce"". [[SafeTestsets]]; deps = [""Test""]; git-tree-sha1 = ""36ebc5622c82eb9324005cc75e7e2cc51181d181""; uuid = ""1bc83da4-3b8d-516f-aca4-4fe02f6d838f""; version = ""0.0.1"". [[Scratch]]; deps = [""Dates""]; git-tree-sha1 = ""ad4b278adb62d185bbcb6864dc24959ab0627bf6""; uuid = ""6c6a2e73-6563-6170-7368-637461726353""; version = ""1.0.3"". [[SeawaterPolynomials]]; deps = [""Test""]; git-tree-sha1 = ""6db1b6004791962cb12d425cd12691506ad7d2b6""; uuid = ""d496a93d-167e-4197-9f49-d3af4ff8fe40""; version = ""0.2.0"". [[Serialization]]; uuid = ""9e88b42a-f829-5b0c-bbe9-9e923198166b"". [[SharedArrays]]; deps = [""Distributed"", ""Mmap"", ""Random"", ""Serialization""]; uuid = ""1a1011a3-84de-559e-8e89-a11a2f7dc383"". [[Sockets]]; uuid = ""6462fe0b-24de-5631-8697-dd941f90decc"". [[SparseArrays]]; deps = [""LinearAlgebra"", ""Random""]; uuid = ""2f01184e-e22b-5df5-ae63-d93ebab69eaf"". [[SpecialFunctions]]; deps = [""ChainRulesCore"", ""OpenSpecFun_jll""]; git-tree-sha1 = ""5919936c0e92cff40e57d0ddf0ceb667d42e5902""; uuid = ""276daf66-3868-5448-9aa4-cd146d93841b""; version = ""1.3.0"". [[Static]]; deps = [""IfElse""]; git-tree-sha1 =",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1613#issuecomment-827235838:14304,Safe,SafeTestsets,14304,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1613#issuecomment-827235838,1,['Safe'],['SafeTestsets']
Safety,"8_S31_S8_IS9_Li3ES10_IS9_Li3ELi1EEES9_S32_S33_S11_IS9_S12_S12_S13_S9_S9_S8_IS9_Li1ES10_IS9_Li1ELi1EEES8_IS9_Li1ES14_IS9_S15_IS9_ES15_IS9_ES16_EES8_IS9_Li1ES14_IS9_S15_IS9_ES15_IS9_ES16_EES8_IS9_Li1ES10_IS9_Li1ELi1EEEvES9_ES30_IS28_S28_S28_S31_S8_IS9_Li3ES10_IS9_Li3ELi1EEES9_S34_S35_S11_IS9_S12_S12_S13_S9_S9_S8_IS9_Li1ES10_IS9_Li1ELi1EEES8_IS9_Li1ES14_IS9_S15_IS9_ES15_IS9_ES16_EES8_IS9_Li1ES14_IS9_S15_IS9_ES15_IS9_ES16_EES8_IS9_Li1ES10_IS9_Li1ELi1EEEvES9_EEEEES23_I46__u___v___w___b____1____2____3____4____5____6_S18_I15DiscreteForcingIS23_I13______u______S18_IS16_S16_S9_EE9_sponge_uES37_IS23_I13______u______S18_IS16_S16_S9_EE9_sponge_vES37_IS23_I13______u______S18_IS16_S16_S9_EE9_sponge_wES37_IS23_I13______u______S18_IS16_S16_S9_EE9_sponge_bE12_zeroforcingS42_S42_S42_S42_S42_EES8_IS9_Li3ES10_IS9_Li3ELi1EEES23_I27__time___iteration___stage_S18_IS9_S16_S16_EE' uses too much parameter space (0x1a10 bytes, 0x1100 max).; ptxas fatal : Ptx assembly aborted due to errors; If you think this is a bug, please file an issue and attach /glade/scratch/tomasc/jl_XSJ4P4z47a.ptx; Stacktrace:; [1] error(s::String); @ Base ./error.jl:35; [2] cufunction_compile(job::GPUCompiler.CompilerJob, ctx::LLVM.Context); @ CUDA /glade/work/tomasc/.julia/packages/CUDA/Ey3w2/src/compiler/execution.jl:428; [3] #224; @ /glade/work/tomasc/.julia/packages/CUDA/Ey3w2/src/compiler/execution.jl:347 [inlined]; [4] JuliaContext(f::CUDA.var""#224#225""{GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget, CUDA.CUDACompilerParams, GPUCompiler.FunctionSpec{typeof(Cassette.overdub), Tuple{Cassette.Context{nametype(CUDACtx), Nothing, Nothing, KernelAbstractions.var""##PassType#312"", Nothing, Cassette.DisableHooks}, typeof(Oceananigans.Models.NonhydrostaticModels.gpu_calculate_Gu!), KernelAbstractions.CompilerMetadata{KernelAbstractions.NDIteration.StaticSize{(8, 8, 6)}, KernelAbstractions.NDIteration.DynamicCheck, Nothing, Nothing, KernelAbstractions.NDIteration.NDRange{3, KernelAbstractions.NDIteration.StaticSize{",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2869#issuecomment-1401133112:5042,abort,aborted,5042,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2869#issuecomment-1401133112,1,['abort'],['aborted']
Safety,"9_Li3ES10_IS9_Li3ELi1EEE10_identity1S19_IS9_S20_S20_S21_S8_IS9_Li1ES22_IS9_S23_IS9_ES23_IS9_EEEEES24_S18_S25_S26_S19_IS9_S20_S20_S21_S8_IS9_Li1ES22_IS9_S23_IS9_ES23_IS9_EEEEES15_IS12_S12_S12_S13_S16_IS12_S12_S12_6__z___S8_IS9_Li3ES10_IS9_Li3ELi1EEES27_S19_IS9_S20_S20_S21_S8_IS9_Li1ES22_IS9_S23_IS9_ES23_IS9_EEEEES24_S29_S18_S25_S19_IS9_S20_S20_S21_S8_IS9_Li1ES22_IS9_S23_IS9_ES23_IS9_EEEEES15_I4FaceS12_S31_S13_S16_IS31_S12_S31_S17_S8_IS9_Li3ES10_IS9_Li3ELi1EEES26_S19_IS9_S20_S20_S21_S8_IS9_Li1ES22_IS9_S23_IS9_ES23_IS9_EEEEES24_S27_S29_S18_S19_IS9_S20_S20_S21_S8_IS9_Li1ES22_IS9_S23_IS9_ES23_IS9_EEEEES15_IS12_S31_S31_S13_S16_IS12_S31_S31_S28_S8_IS9_Li3ES10_IS9_Li3ELi1EEES25_S19_IS9_S20_S20_S21_S8_IS9_Li1ES22_IS9_S23_IS9_ES23_IS9_EEEEES24_S26_S27_S29_S19_IS9_S20_S20_S21_S8_IS9_Li1ES22_IS9_S23_IS9_ES23_IS9_EEEEEES14_IS18_S25_S26_7__xz___7__yz___ES19_IS9_S20_S20_S21_S8_IS9_Li1ES22_IS9_S23_IS9_ES23_IS9_EEEEE' uses too much parameter space (0x1408 bytes, 0x1100 max).; ptxas fatal : Ptx assembly aborted due to errors; ```. We haven't discussed the problem with `AveragedField` on this thread. I think we need a new issue for that (can't remember if there already is one). It actually looks like some, but not all of the issue there have been solved. A minimal example is:. ```julia; julia> U = AveragedField(u, dims=(1, 2)); AveragedField over dims=(1, 2) located at (⋅, ⋅, Center) of Field located at (Face, Center, Center); ├── data: OffsetArrays.OffsetArray{Float64,3,CUDA.CuArray{Float64,3}}, size: (1, 1, 3); ├── grid: RegularRectilinearGrid{Float64, Periodic, Periodic, Bounded}(Nx=1, Ny=1, Nz=1); ├── dims: (1, 2); ├── operand: Field located at (Face, Center, Center); └── status: time=0.0. julia> compute!(ComputedField(u - U)). julia> compute!(ComputedField((u - U)^2)). julia> V = AveragedField(v, dims=(1, 2)); AveragedField over dims=(1, 2) located at (⋅, ⋅, Center) of Field located at (Center, Face, Center); ├── data: OffsetArrays.OffsetArray{Float64,3,CUDA.CuArray{Float64,3}}, ",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1241#issuecomment-821872338:4135,abort,aborted,4135,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1241#issuecomment-821872338,1,['abort'],['aborted']
Safety,"; Installed Missings ──────────────────── v1.2.0; Installed StringManipulation ────────── v0.3.4; Installed UnsafeAtomics ─────────────── v0.2.1; Installed SortingAlgorithms ─────────── v1.2.1; Installed Atomix ────────────────────── v0.1.0; Installed LLVM ──────────────────────── v8.0.0; Precompiling project...; ✓ LLVMLoopInfo; ✓ DataValueInterfaces; ✓ Reexport; ✓ IteratorInterfaceExtensions; ✓ LaTeXStrings; ✓ InvertedIndices; ✓ ExprTools; ✓ DataAPI; ✓ Requires; ✓ CompilerSupportLibraries_jll; ✓ OrderedCollections; ✓ Compat; ✓ AbstractFFTs; ✓ InlineStrings; ✓ Scratch; ✓ CEnum; ✓ StaticArraysCore; ✓ TableTraits; ✓ Missings; ✓ PooledArrays; ✓ BFloat16s; ✓ Preferences. ✓ Adapt; ✓ Statistics; ✓ SentinelArrays; ✓ Compat → CompatLinearAlgebraExt; ✓ Crayons; ✓ UnsafeAtomics; ✓ CUDA_Runtime_Discovery; ✓ PrecompileTools; ✓ JLLWrappers; ✓ Tables; ✓ RandomNumbers; ✓ Atomix; ✓ TimerOutputs; ✓ AbstractFFTs → AbstractFFTsTestExt; ✓ NVTX_jll; ✓ JuliaNVTXCallbacks_jll; ✓ MacroTools; ✓ LLVMExtra_jll; ✓ CUDA_Driver_jll; ✓ Random123; ✓ DataStructures; ✓ StringManipulation; ✓ FixedPointNumbers; ✓ SortingAlgorithms; ✗ CUDA_Runtime_jll; ✓ ColorTypes; ✓ LLVM; ✓ LLVM → BFloat16sExt; ✓ StaticArrays; ✓ Adapt → AdaptStaticArraysExt; ✓ StaticArrays → StaticArraysStatisticsExt; ✓ UnsafeAtomicsLLVM; ✓ Colors; ✓ GPUArraysCore; ✓ NVTX. ✓ GPUArrays; ✓ KernelAbstractions; ✓ PrettyTables; ✓ GPUCompiler; ✓ DataFrames; ✗ CUDA; 61 dependencies successfully precompiled in 190 seconds. 5 already precompiled. The following 1 direct dependency failed to precompile:. CUDA [052768ef-5323-5732-b1bb-66c8b64840ba]. Failed to precompile CUDA [052768ef-5323-5732-b1bb-66c8b64840ba] to ""/glade/u/home/knudsenl/.julia/compiled/v1.10/CUDA/jl_UQIv2i"".; [45592] signal (11.1): Segmentation fault; in expression starting at /glade/u/home/knudsenl/.julia/packages/CUDA/Tl08O/src/CUDA.jl:25; Allocations: 2907 (Pool: 2898; Big: 9); GC: ERROR: LoadError: Failed to precompile CUDA [052768ef-5323-5732-b1bb-66c8b64840ba] to ""/glade",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2245919472:3725,Unsafe,UnsafeAtomics,3725,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2245919472,2,['Unsafe'],"['UnsafeAtomics', 'UnsafeAtomicsLLVM']"
Safety,"<:Union{Nothing, Cassette.Tag{N, X, E} where E where X where N<:Cassette.AbstractContextName} where M where N<:Cassette.AbstractContextName, Any...) in module Cassette at /data5/glwagner/.julia/packages/Cassette/xggAf/src/overdub.jl:521 overwritten in module GPUifyLoops at /data5/glwagner/.julia/packages/Cassette/xggAf/src/overdub.jl:521.; WARNING: Method definition overdub(Cassette.Context{N, M, T, P, B, H} where H<:Union{Cassette.DisableHooks, Nothing} where B<:Union{Nothing, Base.IdDict{Module, Base.Dict{Symbol, Cassette.BindingMeta}}} where P<:Cassette.AbstractPass where T<:Union{Nothing, Cassette.Tag{N, X, E} where E where X where N<:Cassette.AbstractContextName} where M where N<:Cassette.AbstractContextName, Any...) in module Cassette at /data5/glwagner/.julia/packages/Cassette/xggAf/src/overdub.jl:508 overwritten in module GPUifyLoops at /data5/glwagner/.julia/packages/Cassette/xggAf/src/overdub.jl:508. CUDA-enabled GPU(s) detected:; CuDevice(0): Quadro P6000; [ Info: Building the CUDAnative run-time library for your sm_61 device, this might take a while...; ERROR: LoadError: InvalidIRError: compiling #12(RegularCartesianGrid{Float64,StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}}, PlanetaryConstants{Float64}, LinearEquationOfState{Float64}, Oceananigans.TurbulenceClosures.ConstantAnisotropicDiffusivity{Float64}, CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global}, CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global}, CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global}, CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global}, CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global}, CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global}, CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global}, CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global}, CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global}, CUDAnative.CuDeviceArray{Float64,3,CUDAnative.AS.Global}, CUDAnative.CuDeviceArray{Float64,3,CUDAnative.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/248#issuecomment-496489468:1544,detect,detected,1544,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/248#issuecomment-496489468,1,['detect'],['detected']
Safety,"> * Should we look for other places in the code that do that? Seems like a pretty easy ""mistake"" to make. I agree we should probably always exponentiate with `Int32` since I can't think of a reason to exponentiate with a number bigger than 2^32 for a fluid dynamics calculation... Perhaps we should help users too by converting `Int64` to `Int32` in diagnostics calculations. We can do this via `AbstractOperations`. We should be careful not to add too many surprising under-the-hood transformations, but this one seems fairly safe, since exponentiation by numbers larger than 2^32 seems very unlikely. We may find that `CUDA.jl` has a better solution in the pipeline though.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1770#issuecomment-869782247:527,safe,safe,527,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1770#issuecomment-869782247,1,['safe'],['safe']
Safety,"> 1. Modify the CG method to make it numerically stably for positive semi-definite matrices. Right, although I am not sure that this is done correctly in this PR. There may be more work to do... > My method belongs to the second category. I create a new matrix. > $$\tilde{A} = A + cv_0v_0^\dagger$$. > where $c$ is a positive real number and $v_0$ is the basis of the null space of $A$. I was a bit confused about the motivation for defining a new matrix, but with some help from @amontoison I feel I understand this better. The basic idea applies to any semi-definite system; the idea is to ""shift"" an operator by the identity:. $$ \tilde A = A + \epsilon I $$. Where $\epsilon$ is a constant. The key to this method is the smallness of $\epsilon$. Note that we are solving a different system: if the original system is $A x = b$, then the new system is. $$ \left (A + \epsilon I \right ) y = b $$. We do not automatically have $y=x$ or even $y \approx x$. However, we may have $y \approx x$ when $\epsilon$ is smaller than the tolerance of the underlying CG algorithm, for example. In addition to making $\epsilon$ small, another strategy to recover $x$ from $y$ is to solve a new system:. $$ A z = b - A y $$. which, we can show, allows us to recover $x$ via $x = y + z$. For the purpose of implementing shifted operators, some methods in Krylov.jl support provision of a ""shift"" as an argument to the solver, eg the argument `λ` here: https://jso.dev/Krylov.jl/dev/solvers/sid/#Krylov.symmlq. It'd be easy to support a shift of the Poisson operator, the caveat being that the problem being solved is no longer exact. We could additionally support finding the exact solution through a second CG iteration via the method described above, but this would be a bit more work.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3802#issuecomment-2415277183:1145,recover,recover,1145,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3802#issuecomment-2415277183,2,['recover'],['recover']
Safety,"> 1. Should I have to open an Interactive session by typing julia --project?. This should work. I just wanted to give you commands that would work at the terminal. > Purpose of creating a separate environment? what I understood is that a particular environment will restrict to a particular version. Does it even I mistakenly update it inside that environment?. The purpose of creating a new environment is to carefully control the package versions that are used when you compile and run Oceananigans code. This is good practice regardless of whether bugs like this exist, because you can ensure that you always run the same code with the same release version of Oceananigans. You should update packages in your environment _carefully and deliberately_ --- because updating packages can (and certainly will, given you are working for enough time) break your scripts. Even better, you make it easier for others to run your code, because they can swiftly install and build the environment needed to run your code with a few commands (after you give them your `Project.toml` and `Manifest.toml`). I'm not sure what went wrong for you! There are only a few possibilities though: 1) the Manifest was added to the wrong folder (?); 2) the Manifest was ""accidentally"" updated somehow, either by calling `Pkg.update()` or by adding a package to the environment that required packages to update. I'm not sure. Oceananigans 0.56.0 does not require any packages in the Manifest to be updated, so we can add it safely without updating anything. PS this is an unfortunately tricky solution. A better solution would be isolate the package that's causing the problem and explicitly add it to Project.toml and pin its version to one we know is safe. But since we don't know what the problem package is yet, we can't do that.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1613#issuecomment-827293556:1499,safe,safely,1499,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1613#issuecomment-827293556,2,['safe'],"['safe', 'safely']"
Safety,> :/. Only if that will make your life simpler though. :) Just a suggestion so you avoid waiting for ever to see if the test actually fail.,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1784#issuecomment-870915738:83,avoid,avoid,83,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1784#issuecomment-870915738,1,['avoid'],['avoid']
Safety,"> > :/; > ; > Only if that will make your life simpler though. :) Just a suggestion so you avoid waiting for ever to see if the test actually fail. I'm drinking coffee, but some might describe that as ""waiting""",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1784#issuecomment-870928976:91,avoid,avoid,91,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1784#issuecomment-870928976,1,['avoid'],['avoid']
Safety,> > @iuryt do you want to help set up a validation case for this feature?; > ; > Yep. I will be working on the NP model with a P sinking velocity this week. Any idea for an experiment that changes MLD with time? Maybe making it 1D for simplicity? But I would like to test for 3D to make sure I am programming it correctly. Maybe buoyancy=nothing and diffuse a tanh-like temp profile? But would be better to have a shallowing mld instead. I think a 2D example with turbulence + reactions + sinking would be neat. Probably best to keep it simple and avoid an MLD calculation. Now I'm thinking maybe just a sinking tracer with a source near the surface would be good.. 🤔,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2389#issuecomment-1081819313:548,avoid,avoid,548,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2389#issuecomment-1081819313,1,['avoid'],['avoid']
Safety,"> > @tomchor @whitleyv what's the ultimate vision for this PR; > ; > I think we just want to add a more ""precise"" drag option to IB by using the `conditional_flux` functions, so that a drag can be added at the exact locations that need it for the grid fitted version, while also avoiding the user having to call the correct conditional for each location. (For context, when adding drag to IB I have been just adding drag as a forcing over a cell-width above the boundary. For sure! A forcing function could serve as a prototype for the new tendency kernel function that I'm proposing, right? That was my original motivation for proposing the forcing function --- because we would be able to re-use that code (eg all the right conditionals, etc) when we went to implement immersed boundary conditions in the source code. The new tendency kernel function that @simone-silvestri and I are suggesting would be identical to such a forcing function. > I think, one of our concerns was the interpolating of field dependencies within a functional form of a boundary condition, and how they would interact with the IB, could it be generalized to any input. That's true we have to take care with interpolation. I think the first question is what user API we'd like to support; then we can generate source code to match.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2275#issuecomment-1075661345:279,avoid,avoiding,279,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2275#issuecomment-1075661345,1,['avoid'],['avoiding']
Safety,"> > Can we use types rather than symbols? `ThreeDimensional()`, `Horizontal()`, etc.; > ; > They are converted into types in the constructor `return ScalarDiffusivity{TD, eval(direction)}(FT(ν), κ)` (and are effectively types in the type signature. I personally prefer it this way because it allows to build closures in the API avoiding to have to import additional types...; > ; > But, I don't feel so strongly about it... I can also change it. I think the most important thing is a uniform API that's easy to remember. With a few exceptions (that we will hopefully address eventually) we use types throughout (since for many cases we have to since the types can store data). I think if we have an API that's half types, half symbols it will get hard to remember when we use what... I see your point though re: polluting the namespace if there are very many possibilities. Precedent for a ""symbol based specification"" is in Julia's `Colors.jl`. `Makie.jl` also uses a fair amount of symbols.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2247#issuecomment-1039821816:328,avoid,avoiding,328,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2247#issuecomment-1039821816,1,['avoid'],['avoiding']
Safety,"> > Does it have to do with output? Do you know what the time-step is when this happens?I'm wondering if it has to do with using a very small time-step, leading to a round-off error issue. That would also explain why it doesn't affect dynamics, because huge pressure gradient integrated over a machine epsilon duration may not have an impact.; > ; > Are you referring to the fact that sometimes `model` has to use a very tiny time-step to bridge the gap between the current time and the output time? If so, that's an interesting possibility that I hadn't considered. Although in the example above I'm fixing the time-step at `25`, while the output time interval is `200`, so I wouldn't expect any issues there. Can you check just to be sure? Because having an output interval that's some multiple of the time step is exactly when we expect to see miniscule time-steps due to round off error. The pressure source term is the divergence of the predictor velocity divided by time-step. As the time-step vanishes, the divergence of the predictor velocity also vanishes (because the flow has not evolved from its previous, non-divergent solution). We get a situation tending to 0/0. I think there's a few things we could do to solve this. First of all if we take a very small time-step, I think we can actually just re-set the model time rather than taking a time-step. Second I am wondering if we want to implement a time type that has finite resolution (ie there is a smallest time increment one can take). For example, datetimes have a smallest unit (micro or nanoseconds). A non-dimensional or dimensional-agnostic time type could also be designed analogously (eg every time is the multiple of an integer by the fundamental unit). This would eliminate round off error but it's a bit of work and also we have to put some thought into how best to accomplish it. There might also be a simpler solution by adjusting how we increment time. I'm not sure. > Also wouldn't that also affect simulations with `bu",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3593#issuecomment-2103013025:942,predict,predictor,942,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3593#issuecomment-2103013025,1,['predict'],['predictor']
Safety,"> > First, if `c` is the concentration of a tracer it should be non-negative. If you are picking it to be a `sine` I might suggest having `1 + sin` just to avoid negative values. That being said, I don't think it's going to have any impact on the results but might be worth trying.; > ; > Fair point; using `1 + sin(x)` as the initial condition would measure leakage since the total ""mass"" is then Lx * Ly. I propose adding another tracer though specifically for that purpose since it's easy. I ran a tracer with initial value of 1 everywhere to look at the concentration leakage. I took the percentage of the difference in the area integrated concentration of the IBM and nonIBM scaled by the initial concentration, ie. `100 x abs(IBM - nonIBM)/ initial`. The results are below. It looks like for a 256 x 256 grid after 200s only about 3 x 10^-5 % of the initial concentration has leaked out, which is pretty good and seems to scale well with refinement!; ![volint_Concentration_leakage](https://user-images.githubusercontent.com/67593861/123316291-b051de80-d4fa-11eb-86e7-4ee969af5703.png)",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1733#issuecomment-867903313:156,avoid,avoid,156,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1733#issuecomment-867903313,1,['avoid'],['avoid']
Safety,"> > I can create a separate issue to address this, but perhaps more importantly, I don't see the interior velocity `V∞` being added anywhere there except in the BCs. Usually we add it as a background field to avoid inertial oscillations, and indeed it used to be done like that in [older versions of the docs](https://clima.github.io/OceananigansDocumentation/v0.73.8/generated/tilted_bottom_boundary_layer/), but at some point this was changed and I don't really understand why (nor can I pinpoint where).; > > I think it's okay to run the example without adding it as a background field, but then we're solving the problem in a reference frame that's moving with the interior flow and that should be made explicit, and I don't see that explanation there. Personally, I'm in favor of including `V∞` as a background field because it's simpler.; > > CC @hdrake @glwagner who were in #3581; > ; > What's the problem with adding it back?. No problem at all, and in my opinion that's what we should do. I'm just checking with the community first since it might have been removed for some reason I'm not aware of. Are you okay if I add it back?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3813#issuecomment-2388736091:209,avoid,avoid,209,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3813#issuecomment-2388736091,1,['avoid'],['avoid']
Safety,"> > I might suggest removing the *z^3 to avoid this potential problem.; > ; > Good idea, I agree that doesn't make sense!; > ; > Pretty good point about user input. We also might want to introduce special abstractions for two-dimensional fields so we don't have to write `uh[i, j, 1]` all the time (which we will unfortunately have to do right now). Since this is a solvable problem and not a big deal I suggest we wait until we have working code and then we can raise a bunch of issues then. Since you agree I will go in and modify the test file to remove this from the two lines.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1188#issuecomment-730564345:41,avoid,avoid,41,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1188#issuecomment-730564345,1,['avoid'],['avoid']
Safety,"> > I think having a default but non-trivial CFL would be surprising and therefore undesirable, even if it's convenient for some users.; > > I was hoping we might be able to deprecate `diffusive_cfl` because it's a big maintenance burden, but let's keep it if its useful.; > ; > I wouldn't setting the `diffusive_cfl` is surprising. After all the diffusive cfl is as much of a necessary condition for convergence as the advective cfl, it just is a limiting-factor less often for oceanic simulations. But I'm okay with keeping the current behavior as well. I certainly don't disagree that it's convenient; the question is more whether the benefit justifies the cost. Users also want a code that's easy to maintain and extend because then they get more features! For constant diffusivities, the CFL constraint can be pre-calculated. For some common closures the calculation is simple, but this is not _generically_ the case for all turbulence closures. It's really only for variable diffusivities that we _need_ a feature for on-the-fly calculation. We could also use something like `diffusive_cfl=nothing` which completely avoids the calculation. Then we can justify supporting a diffusive CFL calculation for selected turbulence closures. That might be a good route (discussion for another issue).",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2205#issuecomment-1025985247:1122,avoid,avoids,1122,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2205#issuecomment-1025985247,1,['avoid'],['avoids']
Safety,"> > I think that's a fine strategy. We can add a kwarg to `NonhydrostaticModel` called `hydrostatic_pressure_anomaly`. We can set it to `CenterField(grid)` to preserve existing behavior, or set it to `nothing` to avoid the separation. And we should probably make `nothing` default so that triply periodic problems can be done out of the box. Then we don't have to re-do the regression tests either because we preserve existing behavior...; > > I think that's also a less invasive change than this PR because we don't have to change `pressures` to `pressure` everywhere, hmm.; > ; > Agreed.; > ; > > Since you've done most of the legwork I think you have prerogative to open a new PR if you like (and I can help once you do).; > ; > Thanks, but I unfortunately won't have time to dedicate to this for at least a few weeks. So please feel free to start the PR!. Great. I think the PR is nearly finished over at #3574. Give it a look over when you have time.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3080#issuecomment-2088592499:213,avoid,avoid,213,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3080#issuecomment-2088592499,1,['avoid'],['avoid']
Safety,"> > I think to preserve the work in this PR, we should add a `Float32` test which will fail if a spurious promotion undermines performance; > ; > Agreed. I'll revisit this PR later to see if I can find where the conversion happens. The test I added only checks to see if we can take a time step. But I should be able to also add a test to ensure no spurious promotion occurred. Ah, that will work as a test if we remove the `convert`. The `convert` is a good sanity check to find where the problem is, but its not a solution since it merely allows the code to run without error --- it doesn't actually allow us to realize the benefits of using `Float32`. Arguably with this it is actually worse to use `Float32`, since the numerics are degraded bbut the perfrmance benefit is not fully realized",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3876#issuecomment-2445330720:459,sanity check,sanity check,459,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3876#issuecomment-2445330720,1,['sanity check'],['sanity check']
Safety,"> > I would think that passing a function instead of collect(0:Nz) would probably be cleaner but both can work.; > ; > Yeah I definitely think passing a function is more intuitive and efficient. As long as that's still an option, I'm happy :); > ; > > VerticallyStretchedRectilinearGrid(FT, size=(1, 1, Nz), x=(0, 1), y=(0, 1), z=(0, Nz), z_stretch=collect(0:Nz)); > > What's nice about this is that you still specify the domain boundaries, as other grids do, and you specify the stretching function separately. This makes it easier to do a stretch in any direction, and #1532 does just that.; > ; > Sorry, but I'm a but confused. Aren't `x`, `y` and `z` supposed to be dimensional? Meaning they are the physical bounds of the domain? In your example `z=(0, Nz)`, which does not correspond to the physical boundaries at all. Or did you mean to write something like; > ; > ```julia; > VerticallyStretchedRectilinearGrid(FT, size=(1, 1, Nz), x=(0, 1), y=(0, 1), z=(0, Lz), z_stretch=collect(0:Δz:Lz)); > ```; > ; > If so then it makes sense to me, but that's a bit redundant, no? (Sorry for the confusion!). First, yes, it will certainly be an option. Second, unfortunately I took this example from one of the tests, hence the strange choice of parameters. Sorry about that. . But yes, x,y,z are dimensional and are the physical bounds of the domain. What you propose is what I should have written down before. That would be how to specify the stretched grid using an array. Otherwise, you could pass a function.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1544#issuecomment-813447822:1063,redund,redundant,1063,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1544#issuecomment-813447822,1,['redund'],['redundant']
Safety,"> > I'm wondering if we should provide a separate page on ""Using GPUs""? While the simulation tips for CPUs are really performance optimizations that are optional, the GPU simulation tips are mostly required to run without errors.; > ; > That's a good point. Although I think we could avoid creating another page and put that information in the [""Using GPUs""](https://clima.github.io/OceananigansDocumentation/stable/using_gpus/) page, so that things are more condensed. 🤦 there's already Using GPUs of course, silly me...",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1543#issuecomment-818368200:284,avoid,avoid,284,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1543#issuecomment-818368200,1,['avoid'],['avoid']
Safety,"> > Now what is remaining to do is a global run on a lat lon grid of a `ShallowWater` ocean!; > ; > We can do that? That would be amazing to see, and so much faster than the hydrostatic model. I'm not sure about _so much_. The only thing avoided is the computation of vertical velocity, right? The differences are not cost per time step (likely very similar), but physics and numerics: the `ShallowWaterModel` has a fully nonlinear, explicit free surface, while the hydrostatic model has an implicit, linear free surface. So the shallow water model is more accurate for non-negligible surface slopes, but much more costly because gravity wave time-scales must be resolved.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2522#issuecomment-1119176866:238,avoid,avoided,238,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2522#issuecomment-1119176866,1,['avoid'],['avoided']
Safety,"> > Second, this is a plot of the solution for the 1D case that starts off with no motion and a Gaussian field for height. This is a nice example of geostrophic adjustment, and can certainly be compared to papers that do such things.; > > ![one_dimensional_shallow_water_x](https://user-images.githubusercontent.com/8239041/101361665-45e29500-386d-11eb-82d8-f440f3b07d80.gif); > > Third, I have done a 2D case and everything looks qualitatively the same, but it is much more diffusive. This is a plot of a slide through the centre that should reproduce the same thing as above, but the amplitudes are much weaker. This is only 64x64 so it's not very high resolution but I should probably change the advection scheme to make it more accurate.; > > ![one_dimensional_shallow_water_2D_x](https://user-images.githubusercontent.com/8239041/101361578-251a3f80-386d-11eb-8e59-8c02e271cfcc.gif); > ; > A sanity check would be to check whether geostrophic balance is satisfied in the center of the domain, e.g., something like the last figure from [this example](https://fourierflows.github.io/FourierFlowsDocumentation/dev/generated/OneDShallowWaterGeostrophicAdjustment/#Geostrophic-balance). Thanks @navidcy for the suggestion. After takling with @glwagner I remembered that radiating waves in 2D decay like 1/r, compared to 1D. That's the reason for the large difference in amplitude. I might try and put together an example showing this in detail. As for geostrophic balance, I will work on computing that usin the center differencing operators since they are pretty straightforward to apply.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1258#issuecomment-740619873:896,sanity check,sanity check,896,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1258#issuecomment-740619873,1,['sanity check'],['sanity check']
Safety,"> > This looks good! I'll approve it now but I'll leave the following suggestions if you think they're helpful:; > > ; > > * Add brief docstrings with a quick example; > > * Change the names from `u` and `v` in the functions to something more general in order to avoid confusion. Maybe `a`, `b` or some greek letters. (I'm assuming that this works for any `Field`s, no?); > ; > I agree. Sorry that seems ""rushed"" but we need this feature for some project. But I will put these as an issue!. That's alright. Thanks for creating https://github.com/CliMA/Oceananigans.jl/issues/1991",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1987#issuecomment-923311897:263,avoid,avoid,263,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1987#issuecomment-923311897,1,['avoid'],['avoid']
Safety,"> @Sbozzolo might be able to help because I believe they do something special to avoid round off issues in `ClimaAtmos`. With regards to floating point instabilities due to arithmetic with time and time intervals, ultimately, we will be solving this issue (and others) by moving away from a floating point time in favor of an integer one (e.g., milliseconds from a start date). As a fun fact, if you are using Float32 and keep track of your time in seconds, `t + dt` is going to have numerical error after approximately one year of simulated time (which is not that much).",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3606#issuecomment-2153112248:81,avoid,avoid,81,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3606#issuecomment-2153112248,1,['avoid'],['avoid']
Safety,"> @glwagner ,; > ; > So then, based on the code snippet given by @johnryantaylor and our discussion, this validation could be adapted to something like:; > ; > ```julia; > lamb = 1 # decay scale for slip velocity in meters; > @inline w(x,y,z,t) = (tanh(max(-z/lamb,0))*tanh(max((z+H)/lamb,0))); > sinking = SlipVelocity(WENO5(), w=w); > ```; > ; > The way the code is, does it accept functions for velocity inputs? Is that something I could do to help you?. You can do this a few ways:. ```julia; # Vertical velocity function; const lamb = # something; const H = # something; @inline w_func(x, y, z) = tanh(max(-z / lamb, 0.0)) * tanh(max((z + H) / lamb, 0.0)). # Field (allocates memory and precalculates w_func); w = ZFaceField(grid); set!(w, w_func); sinking = AdvectiveForcing(w = w). # Function field (avoids allocating memory but calls `w_func` every iteration); w = FunctionField{Center, Center, Face}(w_func, grid); sinking = AdvectiveForcing(w = w); ```. We could also add the feature to `AdvectiveForcing` that, if an input is a function, we assume it's a function of `x, y, z, t` and deal with it appropriately (by wrapping in a `FunctionField` and adding reference to the model `clock`).",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2389#issuecomment-1083548436:807,avoid,avoids,807,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2389#issuecomment-1083548436,1,['avoid'],['avoids']
Safety,"> @glwagner : I agree that the major savings would be the lack of vertical grid points, but having a full free-surface will likely force a smaller time step because of CFL. There would be two ways to reduce this constraint (in the future).; > ; > 1. Rigid lid (solve for the surface pressure); > 2. Implicit free-surface (treat the free-surface implicitly in the time-stepping); > ; > Both of these would borrow from the hydrostatic model, but the ingrediants are there, and would make it a lot faster. I would vote for implementing a rigid lid / vertically-`Flat` mode for the hydrostatic model instead, in order to keep the shallow water model as simple as possible (generalizing to multiple layers as an alternative direction would be nice I think). Have you benchmarked this? It is true that vertically `Flat` avoids a few vertical operations in the vertical advection term but since the kernel sizes are the same I'd be surprised if the performance differences are huge in terms of cost per time step.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2522#issuecomment-1119195736:814,avoid,avoids,814,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2522#issuecomment-1119195736,1,['avoid'],['avoids']
Safety,"> @glwagner, how can we avoid `collect`? E.g., at; > ; > https://github.com/CliMA/Oceananigans.jl/blob/d913858aa771f096e8ce48da132749361c8f1647/validation/elliptic_solvers/doubly_bounded_poisson.jl#L130; > ; > I tried `arch_array` but seems like we need to write some more methods for it?; > ; > ```julia; > julia> r_array = arch_array(arch, reshape(interior(r), Nx * Ny * Nz)); > ERROR: MethodError: no method matching arch_array(::CPU, ::Base.ReshapedArray{Float64, 1, SubArray{Float64, 3, Array{Float64, 3}, Tuple{UnitRange{Int64}, UnitRange{Int64}, UnitRange{Int64}}, false}, Tuple{Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64}, Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64}}}); > Closest candidates are:; > arch_array(::CPU, ::Array) at /Users/navid/Research/OC.jl/src/Architectures.jl:102; > arch_array(::CPU, ::CUDA.CuArray) at /Users/navid/Research/OC.jl/src/Architectures.jl:103; > arch_array(::Any, ::AbstractRange) at /Users/navid/Research/OC.jl/src/Architectures.jl:107; > ...; > Stacktrace:; > [1] top-level scope; > @ REPL[6]:1; > [2] top-level scope; > @ ~/.julia/packages/CUDA/fAEDi/src/initialization.jl:52; > ; > julia> r_array = arch_array(arch, interior(r)); > ERROR: MethodError: no method matching arch_array(::CPU, ::SubArray{Float64, 3, Array{Float64, 3}, Tuple{UnitRange{Int64}, UnitRange{Int64}, UnitRange{Int64}}, false}); > Closest candidates are:; > arch_array(::CPU, ::Array) at /Users/navid/Research/OC.jl/src/Architectures.jl:102; > arch_array(::CPU, ::CUDA.CuArray) at /Users/navid/Research/OC.jl/src/Architectures.jl:103; > arch_array(::Any, ::AbstractRange) at /Users/navid/Research/OC.jl/src/Architectures.jl:107; > ...; > Stacktrace:; > [1] top-level scope; > @ REPL[7]:1; > [2] top-level scope; > @ ~/.julia/packages/CUDA/fAEDi/src/initialization.jl:52; > ```. Why do you need to convert to architecture? Is it enough to use. ```julia; r_reshaped = reshape(interior(r), Nx * Ny * Nz); ```. ?. If you can't use that then I think y",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2396#issuecomment-1163085492:24,avoid,avoid,24,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2396#issuecomment-1163085492,1,['avoid'],['avoid']
Safety,> @hennyg888 I think we need line info (not just file) to precisely interpret the profiling results?. If you scroll right in my big block of text you can see a column that shows the line number and function name in the file specified in the file column that's visible without scrolling. Please see the full file attached below. Might be easier to view or reformat than the embedded code block above.; [nonhydrostatic_profile_flat.txt](https://github.com/CliMA/Oceananigans.jl/files/6931920/nonhydrostatic_profile_flat.txt). I tried to avoid flame graphs and go for something as close to percentages as I could so I went with the default output. I'll add in StatProfilerHTML.jl outputs as well since the flame graphs and html files do look very neat. In the very last row there's a total snapshots count of 24177. Dividing the counts shown in the left-most column by this number should give the percentage time spent on this line or in any functions executed by this line.,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1912#issuecomment-892703448:535,avoid,avoid,535,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1912#issuecomment-892703448,1,['avoid'],['avoid']
Safety,"> @navidcy Hmmm, digging into Documenter.jl's Buildkite config it looks like push previews are only provided if the build belongs to a PR?; > ; > https://github.com/JuliaDocs/Documenter.jl/blob/4be8243d31a6ffdbc64d779e3f8c2fb7c61de075/src/deployconfig.jl#L719-L725; > ; > Maybe there was no push preview here since @tomchor pushed this branch at which Buildkite ran, before the PR existed, so Buildkite did not publish a preview. Maybe if a second commit was made on this PR, it would be detected by Buildkite as a PR and therefore a preview would be published?. Perhaps if we update Documenter.jl in the docs to the latest version we can alleviate that. At the moment DocumenterCitations v0.2.0 is precluding Documenter from v0.26 and later; see https://github.com/ali-ramadhan/DocumenterCitations.jl/pull/34#issuecomment-782308024.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1383#issuecomment-782308873:488,detect,detected,488,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1383#issuecomment-782308873,1,['detect'],['detected']
Safety,"> @tomchor @whitleyv what's the ultimate vision for this PR. I think we just want to add a more ""precise"" drag option to IB by using the `conditional_flux` functions, so that a drag can be added at the exact locations that need it for the grid fitted version, while also avoiding the user having to call the correct conditional for each location. (For context, when adding drag to IB I have been just adding drag as a forcing over a cell-width above the boundary. So I'd prefer something a little more exact.) Hard coding this option in is mostly a test to see how it would go while using the available framework, before creating a full method that users would be able to implement nicely. . Implementing immersed boundary conditions as a separate term would work, I believe! A `value` immersed option wouldn't involve as much effort (as we've previously worked out for no slip), besides getting the direction correct. I think, one of our concerns was the interpolating of field dependencies within a functional form of a boundary condition, and how they would interact with the IB, could it be generalized to any input.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2275#issuecomment-1075635953:271,avoid,avoiding,271,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2275#issuecomment-1075635953,1,['avoid'],['avoiding']
Safety,"> @tomchor do you want help with this? I'd be happy to help move this along (looks like we need to merge with master). I'm also wondering if we can simplify the setup. For example, I don't think `BackgroundField` is essential --- we can just run an initial value problem and obtain very similar physics and visualizations. I also think we can get rid of the sponge layer at the top of the domain. I think these simplifications might make the example a little easier to interpret and focus attention on the novel parts of the example (mostly, the tilting of gravity + Coriolis). Thanks for the offer! I'm actually making progress very slowly on this branch. I just haven't pushed it to avoid running CI (I have the feeling that the CI servers have been kinda overwhelmed lately?). But some help would definitely be nice :)",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1498#issuecomment-974301910:685,avoid,avoid,685,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1498#issuecomment-974301910,1,['avoid'],['avoid']
Safety,"> Added some dynamics tests for Coriolis with this last commit. It two a 0-D case for half an inertial period with a rotation about the `z` axis and `x` axis and then compares both to make sure they produce the same result (but rotated).; > ; > There's one part that tests if the total velocity magnitude is approximately unchanged (magnitude=1), which relies on an implicit arbitrary tolerance which might be bad. I'd curious about your feedback on that one.; > ; > > if the name is changed to include Cartesian then we can merge this and discuss further in an issue.; > ; > Per the comment above I'm going to change the name to `ConstantCartesianCoriolis` and (provided the tests all pass and you're okay with my new test addition) I'll proceed to merge this into master and open an issue to further discuss the issues that emerged here. For physics tests we can't avoid introducing an arbitrary tolerance. So its ok. That's one reason why physics tests in CI are a bit problematic and we also need validation tests analyzed by humans.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1892#issuecomment-888549957:867,avoid,avoid,867,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1892#issuecomment-888549957,1,['avoid'],['avoid']
Safety,"> Agreed. Although I think that's for another PR, right?. For sure but if that case actually requires another abstraction that would mean we can safely recommend not using `ImmersedBoundaryCondition`, then we shouldn't put too much effort into it.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3142#issuecomment-1599998597:145,safe,safely,145,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3142#issuecomment-1599998597,1,['safe'],['safely']
Safety,"> Ah interesting. Does this work because broadcasting over GPU subarray views is a little rough around the edges?; > ; > Also, is it worth adding the MWE from [#1767 (comment)](https://github.com/CliMA/Oceananigans.jl/issues/1767#issuecomment-868793917) as a test?. Certainly _something_ is rough... I think it's ""broadcasting over ReducedField"" that's the issue here. Prior to this PR, `mean!` used a broadcast over `R::AbstractReducedField` to compute the normalization. For some reason this has data synchronicity issues on the GPU (I'm at a loss to explain why). This PR just changes that broadcast operation to compute over all members of `R`. The halo regions of `R` (presumably) aren't touched during `sum!`, so doing some extra normalization in the halos doesn't really matter, I guess... (if we want halos to be right we should probably fill them after executing `mean!` in `compute!`, or something). But the _reason_ why this change fixes the issue isn't at all obvious to me. Maybe there's a bad interaction between `KernelAbstractions` (which gets used for the broadcast) and `sum!`? Not sure. Maybe another solution would wrap `sum!` in `CUDA.@sync` (eg the solution here could _implicitly_ synchronize in order to perform the broadcast correctly; otherwise CUDA wouldn't work generally... ?!?). Might be worth testing that, though I don't know which solution we should prefer (if any). In fact, it seems better to avoid using custom broadcasting machinery if possible (which is the current solution)...",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1769#issuecomment-868839671:1428,avoid,avoid,1428,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1769#issuecomment-868839671,1,['avoid'],['avoid']
Safety,"> Ah, I see. Sounds like it wouldn't be trivial to add that support.; > ; > I guess a workaround to avoid partially-averaged results when picking up would be to set the `Checkpointer` to only write checkpoints when the `TimeAveraged` results are also written. I'm not sure what that would do to other (more frequent) outputs though, since it'd potentially try to write some time steps twice (and not in monotonic ordering)... There are two things. One is to fix the flow of information... that's probably pretty easy because we can either 1) make `Checkpointer` a callback or 2) change `write_output!` to have the syntax `write_output(writer, simulation)` here:. https://github.com/CliMA/Oceananigans.jl/blob/643b484e81e0aeb038b3038266912ad051bce9b8/src/Simulations/run.jl#L147. then with a fallback `write_output!(writer, sim) = write_output!(writer, sim.model)`, very little has to change... The other task is to figure out how to save down the ""state"" of the time-averaging apparatus so that it can be restored correctly. That's maybe the harder part but of course unavoidable to make checkpointing work with it.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3485#issuecomment-1969272305:100,avoid,avoid,100,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3485#issuecomment-1969272305,1,['avoid'],['avoid']
Safety,"> Any place where the loop limits are not types, it's wrong. It only works if the limits are known via types (so they are known at compile time rather than runtime). Typically this would require uusing `Val{N}` or `Val{H}` but even then it can fail sometimes. I'm not sure I understand what you mean here. Can you give an example that's OK and one that's not?; Also, @unroll comes from `KernelAbstractions.Extras.LoopInfo.@unroll`, right? The docstring is not really helping me on this:. ```Julia; help?> KernelAbstractions.Extras.LoopInfo.@unroll; @unroll expr. Takes a for loop as expr and informs the LLVM unroller to fully unroll it, if it is safe to do so and the loop count is known. ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────. @unroll N expr. Takes a for loop as expr and informs the LLVM unroller to unroll it N times, if it is safe to do so.; ```. In particular, I don't know what ""if it is safe to do so"" refers to.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3374#issuecomment-1874465464:647,safe,safe,647,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3374#issuecomment-1874465464,3,['safe'],['safe']
Safety,"> Apparently running on GPUs has a few caveats even after you guys did most of the hard work in Oceananigans. @tomchor the reason your code did not compile is because this function. ```julia; function bottom_mask_func(x, y, z); sponge_one = -grid.Lz/2; sponge_zero = sponge_one + grid.Lz/10; return heaviside(-(z-sponge_zero)) * (z - sponge_zero)^2 / (sponge_one - sponge_zero)^2; end; ```. references `grid.Lz` as a global variable. Global variables can only be used inside functions on the GPU if they are declared `const` as @ali-ramadhan has done. One strategy to avoid having to use `const` is to build callable objects like `GaussianMask` that store their parameters. This is why a parameterized `GaussianMask` callable object is provided for users to use as `mask`. It might be nice to add a `SigmoidMask` masking function as well that uses `tanh` and has a `width`, `center`, and `direction`.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1203#issuecomment-734412987:568,avoid,avoid,568,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1203#issuecomment-734412987,1,['avoid'],['avoid']
Safety,"> As soon as I put `fig[0, :] = Label(fig, title, textsize=24)` then the plot size gets all small and weird... Any ideas?. I would avoid using index `0` and also using `:` if you can. What happens when you put the label in `fig[1, 1:2]` and the contours in `fig[1, 2]`?. I also think it's probably better to use heatmap for all plots. While contour does look better, heatmap is a lot faster and less finnicky, and we get 95% percent of the educational value of the visualization with a heatmap. I almost never use contour except at the very final stages of visualization for paper plots, so...",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2553#issuecomment-1126952448:131,avoid,avoid,131,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2553#issuecomment-1126952448,1,['avoid'],['avoid']
Safety,"> Billy mentioned it in some other comments but while we do splat args for some of the function calls, the function definitions use Varargs instead. This should avoid the catastrophic slowdown we saw with splatting earlier, but I agree that it should be tested. Do you have any good CPU performance tests set up @glwagner ?. Any simple test will do, for example one of the examples",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3480#issuecomment-2150416681:161,avoid,avoid,161,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3480#issuecomment-2150416681,1,['avoid'],['avoid']
Safety,"> But I do have one question: how are we supposed to treat open boundaries for quantities located at cell centers? I don't see any specific new code dealing with that. I see you used `GradientBoundaryCondition(0)` in your validation codes but, for example, for buoyancy that's not gonna cut it and will lead to reflections. The majority of the new code here deals with the fact that you can't update the boundary condition on wall normal velocities after the pressure correction step and maintain the interior domain to be physical and divergence-free, so we have a new function `_fill_X_open_halo!`, but there is nothing stopping you from also defining new methods for `_fill_west_halo!` which still acts before both the predictor and correct step and is what you would need to write for centre field OBCs. The reason I used `GradientBoundaryCondition(0)` in the validation case is that the literature suggests that it doesn't make much of a difference for the non-wall normal velocity component what boundary condition you use (e.g. https://doi.org/10.1002/fld.1650181006 ""(iii)&plat + Vacp/dn = 0, where V is ‘user-specified’, but cannot be zero, and should be positive if fluid is leaving the domain. (The average normal velocity through the boundary is a reasonable candidate.) This OBC may be gaining in popularity over acp/an = 0 for reasons which are not entirely clear to us.""), but I guess the reflection might be reduced by something else.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3482#issuecomment-2263595729:722,predict,predictor,722,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3482#issuecomment-2263595729,1,['predict'],['predictor']
Safety,"> By the way, i didn't use the scheme described in #1704 because it didn't give great results when implemented. I have implemented the ENO coefficients as derived before the assumption of uniform grid to the stencil interpolation (tomorrow I'll go back and check the report which describes them) . The smoothness functions are kept the same. That's interesting. Does that mean you were not able to reproduce the results reported in ""[A simple algorithm to improve the performance of the WENO scheme on non-uniform grids](https://link.springer.com/article/10.1007/s10409-017-0715-2)""?. > > By the way, it would be quite easy to modify the type to include the order of the WENO scheme which we could change at will. What do you think? Would it be usefull to do that?; > ; > If it's easy, then I definitely vote that we do that! A lower order, faster WENO scheme may be a good compromise between computational cost and accuracy. If that's true, we could use it as the default scheme for models. I think a higher order WENO scheme is more important than lower-order scheme. However, a low-order WENO method is proposed [here](https://link.springer.com/article/10.1007/s10915-020-01164-6). I think the 3rd order scheme may be quite diffusive and runs the risk of limiting to 1st order (?!). #995 attempts to implement nth order WENO but I think failed to achieve good CPU performance / compilation on the GPU. But @simone-silvestri perhaps you're able to achieve this?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2060#issuecomment-969290093:1250,risk,risk,1250,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2060#issuecomment-969290093,1,['risk'],['risk']
Safety,"> Can we use types rather than symbols? `ThreeDimensional()`, `Horizontal()`, etc. They are converted into types in the constructor `return ScalarDiffusivity{TD, eval(direction)}(FT(ν), κ)` (and are effectively types in the type signature. I personally prefer it this way because it allows to build closures in the API avoiding to have to import additional types... But, I don't feel so strongly about it... I can also change it",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2247#issuecomment-1039818469:319,avoid,avoiding,319,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2247#issuecomment-1039818469,1,['avoid'],['avoiding']
Safety,"> Could you give a brief summary how the model equations differ from implicit to explicit free-surface?. It might be best to save that for the PR that implements the implicit free surface. Suffice to say that the implicit free surface uses a fractional step algorithm that first advances the velocity field to a predictor state U*, and then takes a free surface time-step using a backwards Euler method that utilizes U*. After that, the predictor velocity field is updated using the new barotropic pressure associated with η^{n+1}. The algorithm is described in the MITgcm docs:. https://mitgcm.readthedocs.io/en/latest/algorithm/algorithm.html#pressure-method-with-implicit-linear-free-surface",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1352#issuecomment-778466439:312,predict,predictor,312,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1352#issuecomment-778466439,2,['predict'],['predictor']
Safety,"> Do I understand that to mean that you want to test the performance of the code before and after the PR? If there are tests that I can run to do this with both versions, I would be happy to try that on my desktop, only CPU, but that probably wouldn't be as nice as trying it on a better computer. We would like to test the performance of the version of Oceananigans on this PR versus Oceananigans#master on the CPU and GPU and for a variety of problem sizes. Maintaining good performance is a top priority of ours. Generally speaking we would like to avoid performance regressions --- even small ones (which accumulated over many PRs could become significant).",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1210#issuecomment-734413997:552,avoid,avoid,552,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1210#issuecomment-734413997,1,['avoid'],['avoid']
Safety,"> Do you want that all for this PR or can we merge this part after I've implemented a simple matching scheme as a demonstration and then work on the rest elsewhere?. That's up to you. Smaller PRs can be easier because you will have less risk of merge conflicts. However you should make sure that the code in any individual PR is motivated and tested (ie if you implement a new type then it'd be best to have a use case for it, plus a test).",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3482#issuecomment-2038114155:237,risk,risk,237,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3482#issuecomment-2038114155,1,['risk'],['risk']
Safety,"> Edit: Possibly could just remove a lot of the information from the fields using the `adapt_structure` method from [PR #1057](https://github.com/CliMA/Oceananigans.jl/pull/1057)?. We already do ""unwrap"" data from fields on the GPU:. https://github.com/CliMA/Oceananigans.jl/blob/ea6826fd2ffaed3f0df330cc952667ec878deb6a/src/Fields/field.jl#L368. > I realised it would be much harder to reduce the parameter size of the tracer tendency function if the tracers depend on lots of other tracers. Okay, this makes sense. Solving the problem for `calculate_Gu` just bumps the error down to `calculate_tracer_tendency`, and the issue isn't tractable in that case, since it's clearly important to support tracer forcing functions that depend on other tracers (eg for biogeochemical models). Can you provide a minimum script that reproduces the error? I do think many tracers is an important use case so solving this could warrant reducing the kinds of forcing functions we support --- if that's necessary. I think there also may be solutions that don't change what we support while still solving this problem; eg we provide some features that extend model capabilities specifically for the case of large numbers of tracers. For example, we could avoid passing the tracers explicitly into the kernels. Instead, we can attach references _only_ to the relevant / used fields directly in `Forcing` (we'd have to change the user API to `DiscreteForcing` to support this, but the changes need not be major). This way one might be able to support systems of reacting tracers, as well as many ""additional"" passive tracers that are not involved in a forcing function. Or, we can explore https://github.com/JuliaGPU/CUDA.jl/issues/267.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2700#issuecomment-1223989878:1239,avoid,avoid,1239,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2700#issuecomment-1223989878,1,['avoid'],['avoid']
Safety,"> Exactly, in practice a transpose is communication + permutation, but with `unified_memory` we avoid the communication part (I mean it is there but it's handled by CUDA). So maybe we need three unified memory arrays for `solver.storage`, each permuted with respect to one another?. I was thinking it'd be nice to avoid coding it ourselves by using cufftxt, but now that we're talking about it doesn't seem too difficult.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2523#issuecomment-1119921486:96,avoid,avoid,96,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2523#issuecomment-1119921486,2,['avoid'],['avoid']
Safety,"> Have you tried using `wc = Field(w*c, indices=(:, 1, :))` and also `wc_avg = Average(w*c, dims=2)`? This would allow you to avoid using the `indices` kwarg. I haven't tried that solution specifically, but I suspect it would work. I think the issue with this bug isn't that there aren't workarounds (for example, one could just separate slices and averages into two different files, which is what I'm currently doing) it's just that it fails silently and subtly, so it could catch users off guard.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2931#issuecomment-1438716451:126,avoid,avoid,126,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2931#issuecomment-1438716451,1,['avoid'],['avoid']
Safety,"> Hmm. What is; > ; > ```julia; > view(u[region_W], 1, Nc+1-Hc:Nc, k); > ```; > ; > is this a `Field`? Next question, what is; > ; > ```julia; > reverse(view(u[region_W], 1, Nc+1-Hc:Nc, k)); > ```; > ; > If the former is a `Field` I don't think we've defined `reverse` on `Field`. But you could do that... @glwagner, yes, `view(u[region_W], 1, Nc+1-Hc:Nc, k)` is a `WindowedField`, on which `reverse` is not defined yet. But `reverse(view(u[region_W], 1, Nc+1-Hc:Nc, k).data)` is a valid operation. However, simply typing ; ```julia; u[region][1, Nc+1:Nc+Hc, k] .= reverse(view(u[region_W], 1, Nc+1-Hc:Nc, k).data) * plmn; u[region][Nc+1, 1-Hc:0, k] .= reverse(view(u[region_E], 2:Hc+1, 1, k)); ```; might throw the above-mentioned error due to a dimension mismatch or fill the LHS with junk or incorrect values. So, a safe solution is ; ```julia; view(u[region], 1, Nc+1:Nc+Hc, k).data .= reshape(reverse(view(u[region_W], 1, Nc+1-Hc:Nc, k).data) * plmn, 1:1, Nc+1:Nc+Hc, k:k); view(u[region], Nc+1, 1-Hc:0, k).data .= reshape(reverse(view(u[region_E], 2:Hc+1, 1, k).data), Nc+1:Nc+1, 1-Hc:0, k:k); ```; @glwagner, @navidcy, if you guys are fine with this fix, I will close the issue.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3361#issuecomment-2033319769:819,safe,safe,819,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3361#issuecomment-2033319769,1,['safe'],['safe']
Safety,"> Hmmm, I am not sure how you solved this in the serial grid, but I guess in the same way. By removing that assumption and calculating the coordinate accordingly?. Since we removed the assumption, do we also eliminate the need for this code which avoids the assumption?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3467#issuecomment-1946400260:247,avoid,avoids,247,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3467#issuecomment-1946400260,1,['avoid'],['avoids']
Safety,"> Hmmm, doesn't rectilinear imply orthogonal?; > ; > `RegularRectilinearOrthogonalGrid` might be a little redundant?. No. Rectilinear just means that the axis are straight lines... But the axis could intercept in angles other than 90 degrees.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1386#issuecomment-782295927:106,redund,redundant,106,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1386#issuecomment-782295927,1,['redund'],['redundant']
Safety,"> How about this?; > ; > ```julia; > @warn ""Inflating model grid halo size to ($Hx, $Hy, $Hz) and recreating grid. "" *; > ""The model grid will be different from the input grid. To avoid this warning, "" *; > ""pass halo=($Hx, $Hy, $Hz) when constructing the grid.""; > ```. Sounds great! I have added it in my branch. I will close this issue and we cna modify this when the branch makes its way as a PR. Soon I hope!",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1501#issuecomment-805046294:180,avoid,avoid,180,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1501#issuecomment-805046294,1,['avoid'],['avoid']
Safety,"> How did you determine all of these requirements? Are these strict requirements or guesses?. The CUDA packages and GPUifyLoops are strict requirements because we rely on recent features. For the other packages I just used the version we currently have specified in `Manifest.toml` as we haven't had problems with the current environment (and tests pass). I assumed that future versions will be compatible to avoid having to constantly update upper bounds, but if a new version of a package introduces problems we can add upper bounds. I added `^1` for packages that may release a v1.0.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/492#issuecomment-544990104:409,avoid,avoid,409,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/492#issuecomment-544990104,1,['avoid'],['avoid']
Safety,"> However, I found that the velocity shear at the first grid points is much larger than that predicted by the Monin-Obukhov similarity theory. I don't have much experience with solid wall boundary layers. A quick search returns this review:. https://journals.aps.org/prfluids/pdf/10.1103/PhysRevFluids.2.104601?casa_token=VBO0hrNqp-cAAAAA%3A3XEKZfLTdhiVluwRl8sCQCkOK44GoGUX-546uFtqQjSWAUIDQpKyyQsA4lQd65Oz6Kw5ClWias_CVQ0. suggesting that correct flux predictions in wall-modeled LES is unsolved. For example:. <img width=""616"" alt=""image"" src=""https://github.com/CliMA/Oceananigans.jl/assets/15271942/a6ffe67e-2009-441a-a440-80afe0dc4a70"">. shows a mismatch between DNS and wall-modeled LES for a few standard codes. Based on the literature, a failure to reproduce the log-law is expected?. It might help if you provide more background on what exactly you're trying to achieve, and why you believe the current approach will achieve that objective. For example, are we attempting to reproduce a known result?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3195#issuecomment-1650818364:93,predict,predicted,93,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3195#issuecomment-1650818364,2,['predict'],"['predicted', 'predictions']"
Safety,"> I actually think having an error=true option to NaNChecker would be useful for other purposes than avoiding a nan-filled checkpoint. Okay. > I personally still think that's the best solution here. I disagree, because it only solves the problem for a narrow range of checkpointer schedules. If you checkpoint based on `TimeInterval` and check for NaNs based on `IterationInterval`, there is no guarantee that you won't write NaNs to a checkpoint file whether or not the NaNChecker throws an error. I think a better solution is to explicitly avoid writing a checkpoint file (or any other task) when NaNs are detected. > it's also pretty simple. There's hardly a difference in the simplicity of either implementation. > Unless there's a reason why that's a bad idea that I'm missing... I don't think there is an intrinsic problem with throwing an error, I just think that it's often a brittle hack that can damage productivity (errors thrown by the NaNChecker generate a long stacktrace of irrelevant information --- because the emergence of a NaN is not an error in the usual sense). So I think it's useful to think of more user-friendly solutions to various issues / tasks we face when setting up simulations.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2086#issuecomment-982934538:101,avoid,avoiding,101,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2086#issuecomment-982934538,3,"['avoid', 'detect']","['avoid', 'avoiding', 'detected']"
Safety,"> I also think it'd be fun to try to automatically detect the appropriate device. Something like; > ; > ```julia; > function GPU(); > has_cuda_device = has_cuda(); > has_amd_device = AMDGPU.has_rocm_gpu(); > has_cuda_device && has_amd_device && # this user is blessed with CUDA _and_ ROCm!!; > throw(ArgumentError(""Both a CUDA _and_ an AMD GPU are detected! One of them must be selected for the GPU architecture"")); > ; > has_cuda_device && return CUDAGPU(); > has_amd_device && return ROCMGPU(); > ; > throw(ArgumentError(""Cannot build GPU architecture because neither a CUDA GPU nor an AMD GPU were detected!"")); > end; > ```. I think it's a nice thing to have, but I'm not sure where this will be called. Will it be Oceananigans.jl's top module file?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2494#issuecomment-1112510197:51,detect,detect,51,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2494#issuecomment-1112510197,3,['detect'],"['detect', 'detected']"
Safety,"> I believe for your implementation, since the tracer values are not corrected for pressure, any leakage will be due to the advection of tracer by momentum that _is_ impacted by the pressure correction. This advection should be less with a smaller spacial step if I remember correctly. Well, I agree that less leakage implies that less tracer is advected across the boundary. But why is less tracer advected across the boundary?. It occurs to me that we are not converging to a particular solution as we refine the grid in this case, because this problem has no viscosity. Perhaps we should do a convergence test for a case with finite viscosity. We do introduce grid-scale gradients in the predictor velocity field via masking, so it does seem possible to me that the leakage / pressure gradient error might scale with resolution. It'd be nice to have a solid mathematical explanation for this behavior. Another question might be whether the leakage / error depends on the extent of the masked region (holding resolution constant). I'm not sure whether its worth looking into all this stuff though --- it might be better simply to work on implementing an accurate pressure solver, since I don't think it would be too difficult.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1733#issuecomment-867965142:691,predict,predictor,691,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1733#issuecomment-867965142,1,['predict'],['predictor']
Safety,"> I can create a separate issue to address this, but perhaps more importantly, I don't see the interior velocity `V∞` being added anywhere there except in the BCs. Usually we add it as a background field to avoid inertial oscillations, and indeed it used to be done like that in [older versions of the docs](https://clima.github.io/OceananigansDocumentation/v0.73.8/generated/tilted_bottom_boundary_layer/), but at some point this was changed and I don't really understand why (nor can I pinpoint where).; > ; > I think it's okay to run the example without adding it as a background field, but then we're solving the problem in a reference frame that's moving with the interior flow and that should be made explicit, and I don't see that explanation there. Personally, I'm in favor of including `V∞` as a background field because it's simpler.; > ; > CC @hdrake @glwagner who were in #3581. What's the problem with adding it back?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3813#issuecomment-2388706841:207,avoid,avoid,207,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3813#issuecomment-2388706841,1,['avoid'],['avoid']
Safety,"> I did a few tests with some criteria for timestep-skipping with a couple of my own simulations in addition to the MWE included here. In summary:; > ; > 1. Criterion `sim.Δt / 1e10`: successfully gets rids of the problem in both the MWE and in my simulations; > 2. Criterion `10 * eps(sim.Δt) * sim.Δt`: doesn't get rid of the problem in any simulation; > 3. `100 * eps(sim.Δt) * sim.Δt`: fixes the problem in the MWE but not in my simulations, although it does decrease its frequency of occurrence a good amount.; > 4. `1000 * eps(sim.Δt) * sim.Δt`: fixes everything in all simulations I've tried.; > ; > So only options 1 and 4 fully fix the problem (at least in the simulations I've tried so far). For me both those options rely on pretty arbitrary numbers though, so I'm not very happy with neither. From the point of view seeing the timestep-skipping as an approximation (un+1≈un), then maybe criterion 1 makes more sense, although I'm not sure how it'd behave for Float32 simulations.; > ; > I see three possible ways to go about it right now:; > ; > 1. Do what this PR is doing, and manually set the criterion to either option 1 or 4 above. If it turns out that some simulations still have issues, we revisit.; > 2. We add `min_Δt` as a property of `NonhydrostaticModel` (or maybe `Simulation`?). I think the minimum `Δt` for which time skipping will be necessary will vary significantly between simulations, so this solution deals with that by leaving the decision up to the user if they are interested in the pressure output.; > 3. We try something that actually prevents these round-off errors instead of dealing with them. @glwagner suggested an `Integer`-based model clock, but there might be other options. Note that `eps(sim.Δt)` is similar to `sim.Δt * eps(typeof(Δt))`. So `Δt / 1e10` is pretty similar to `100000 * eps(sim.Δt)`. The only point of using `eps` is to avoid hard coding `Float64`.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3606#issuecomment-2136281567:1883,avoid,avoid,1883,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3606#issuecomment-2136281567,1,['avoid'],['avoid']
Safety,"> I feel we need a more general solution to avoid boilerplate when new grids are added. If we really can't crack this problem then we can merge this as an experimental feature. But I feel we really will want many more grid types that accept stretching, curvilinearity, cubed sphere, etc. I agree that we can do better and am keen to try things out. I need to think about this a little more before I can suggest an alternative.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1504#issuecomment-805152260:44,avoid,avoid,44,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1504#issuecomment-805152260,1,['avoid'],['avoid']
Safety,"> I found that using --check-bounds=yes or running the code on a CPU cannot reproduce the error in the original script posted by me. Interesting --- does that mean that this is not an out-of-bounds issue?. > I expect this model should abort itself when NANs appear instead of crashing due to a memory illegal access error. Are the NaNs appearing in the particle coordinates, or in the model velocity fields? The _default_ `NaNChecker` only checks the first entry in the model's prognostic field:. https://github.com/CliMA/Oceananigans.jl/blob/00006c17c2ede7f819e15ae14aabb14ab62a0136/src/Simulations/simulation.jl#L71. https://github.com/CliMA/Oceananigans.jl/blob/00006c17c2ede7f819e15ae14aabb14ab62a0136/src/Models/Models.jl#L163-L169. Note also that by default, the `NaNChecker` is only actuated every 100 iterations. You are free to use a different NaNChecker, however. The balance between the computational cost of checking NaNs / frequency of NaN checking and the cost of a time-step is use-case-specific. Can you clarify why you expect the model to abort itself when NaNs appear? Are you suggesting that we can improve the default NaN checker?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3320#issuecomment-1771382783:235,abort,abort,235,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3320#issuecomment-1771382783,2,['abort'],['abort']
Safety,"> I hear it's easy to get burned by adaptive time-stepping, especially if taking time steps close to what's allowed by CFL. Adaptive time-stepping is useful for avoiding ""getting burned"", since you can ensure that your time-step is always stable by choosing a conservative CFL number. > Or we can always take time steps that are e.g. 0.2*max_dt to ensure we're significantly below CFL. I don't understand this comment. The `TimeStepWizard` has a parameter `cfl`, which sets `dt` to achieve a certain CFL number. If you set `cfl=0.1`, the time-step is chosen so that CFL = 0.1. The parameter `max_dt` is a user-defined parameter that limits `dt` to some maximum number.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/297#issuecomment-505552350:161,avoid,avoiding,161,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/297#issuecomment-505552350,1,['avoid'],['avoiding']
Safety,"> I hope this description of the workflow is clearer. It's clear!. > I thought the optimal implementation is to append to the previous existing file when using checkpoints. Of course I agree with this. I think our basic principle should be that there is no difference in workflow, either for running or analysis, between scripts for checkpointing or not. Also, I think that `overwrite_existing` should not delete your files if you are picking up from a checkpoint (regardless of what it is set to). Do you agree?. > If I don't change the name each time I pickup, with the overwrite_existing=false it crashes, and with overwrite_existing=true it rewrites all the files. If we have a system that does not overwrite files when picking up from a checkpoint, do you think that it will be necessary to have `overwrite_existing=false` by default? In this case, the only time we risk overwriting files is when we _re-run_ a simulation from the beginning. Assume for the following discussion that the ""overwriting issue"" is solved for checkpointing:. In my experience, I typically set up a script with `overwrite_existing=true` from the very beginning. This is because when we first write a script, we are prototyping. I have never then gone back and changed `overwrite_existing=false` because of some concern about overwriting data. It's inconvenient, in fact, to set `overwrite_existing=false`. My thought is that it makes more sense to ask people to intentionally request `overwrite_existing=false` in the _rare_ case that this is desired. The key insight is that we spend the vast majority of time prototyping. I would even argue from an economic standpoint that the productivity saved from this setting outweights any rare instances of lost data, if they actually would ever occur due to changing the default. Also, even for this purpose I don't really want a ""non-overwriting guarantee"" as a feature. I would prefer something like ""unique_filenames = true"" or something, which I could then ""turn on"" if I",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3818#issuecomment-2392162201:871,risk,risk,871,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3818#issuecomment-2392162201,1,['risk'],['risk']
Safety,"> I might suggest removing the *z^3 to avoid this potential problem. Good idea, I agree that doesn't make sense!. Pretty good point about user input. We also might want to introduce special abstractions for two-dimensional fields so we don't have to write `uh[i, j, 1]` all the time (which we will unfortunately have to do right now). Since this is a solvable problem and not a big deal I suggest we wait until we have working code and then we can raise a bunch of issues then.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1188#issuecomment-730562385:39,avoid,avoid,39,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1188#issuecomment-730562385,1,['avoid'],['avoid']
Safety,"> I propose; > ; > ```julia; > xspacing(X, Y, Z, grid); > yspacing(X, Y, Z, grid); > zspacing(X, Y, Z, grid); > ```; > ; > where `X, Y, Z` are all _instantiated_ locations eg; > ; > ```julia; > x = xspacing(Center(), Center(), Center(), grid); > ```; > ; > I propose we avoid ""one location"" versions that only work for rectilinear / lat-lon grids. I like this API. Although, since this is a user-facing function, I'd vote for us to have one-location methods defined only for grids that support it. Although I don't feel strongly about it.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2058#issuecomment-1328277603:270,avoid,avoid,270,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2058#issuecomment-1328277603,1,['avoid'],['avoid']
Safety,"> I see @glwagner's point. I'm a bit skeptical in that part of the PR (which claims that trig functions should be avoided). Perhaps we can elaborate a bit more on that or point the reader to some other source? I'm a bit on the fence. On the other hand, if I was trying to code up something and was ending up having my code 100 slower because I was doing `cos(pi/4)` instead of `sqrt(2)/2`, then I'd be very grateful if somebody pointed it out in a ""Tips"" section!. 100x slower is definitely a scripting bug. We are missing `@inline` in the examples?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2029#issuecomment-951425555:114,avoid,avoided,114,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2029#issuecomment-951425555,1,['avoid'],['avoided']
Safety,"> I think I get the idea, but I still can't imagine what could be wrong with the vertically stretched grids themselves. They seems pretty straightforward. Could you clarify what specific metrics you're talking about that are different?; > ; > > We might be able to convert the internal wave setup dynamics test to a vertically bounded domain and put it on a stretched grid to test these issues.; > ; > This seems like a good idea. I don't know which ones are problematic --- once we find those, the problem is solved. The answer might be obvious; it looks like we only redefine `Flat` metrics for `RegularRectilinearGrid`:. https://github.com/CliMA/Oceananigans.jl/blob/326f22aff244ad1f9d0778b9d06184f348db211b/src/Operators/spacings_and_areas_and_volumes.jl#L57-L62. We may just have to use `AbstractGrid` rather than `RegularRectilinearGrid`. I'm not sure why the above functions are specific to `RegularRectilinearGrid` in the first place. @francispoulin you helped with this, right? Do you know?. We're missing many of the horizontal metrics too so I think we should add those, otherwise `Flat` won't work with curvilinear grids, either. EDIT: definitions for flat _area_ metrics are probably not very useful / represent edge cases that need special consideration. So we might just focus on flat _linear_ metrics for now (dx, dy at various locations). Right now additional horizontal linear metrics are irrelevant because there are no rectilinear grids that are stretched in horizontal directions. However, we will need these once we have general stretched rectilinear grids and it might be good to avoid confusion like what's happening in the present issue... I'm putting together a test for rectilinear grids that catches this. I suggest we fix the problem after we have the test. If anyone has more information about `Flat` please chime in.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1849#issuecomment-881923453:1603,avoid,avoid,1603,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1849#issuecomment-881923453,1,['avoid'],['avoid']
Safety,> I think for accumulation of a scalar you should use `val = Ref(0.0)` then `val[] += dt * d_val`. Thanks! Yeah it makes sense to avoid globals. I went with this suggestion,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3025#issuecomment-1485437711:130,avoid,avoid,130,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3025#issuecomment-1485437711,1,['avoid'],['avoid']
Safety,"> I think it should be T₁(kᵖ, z) - T₂(kᵖ, z) in the above code?. Thanks @qingli411 ! Fixed:. ![image](https://user-images.githubusercontent.com/15271942/156587002-2f14ad93-f287-4900-bb49-b89101076c4d.png). I talked to Nick Pizzo and spent some time reading [Lenain and Melville 2017](https://airsea.ucsd.edu/wp-content/uploads/sites/10/2019/06/2017_Lenain_Melville-Journal_of_Physical_Oceanography_vol_47-2.pdf) and [Lenain and Pizzo 2020](https://journals.ametsoc.org/view/journals/phoc/50/12/JPO-D-20-0116.1.xml) and I'm left with a few questions. First and foremost is: how is it possible to formulate a stationary relationship between Stokes drift and transport as in McWilliams and Restrepo? In the experiments we run, we impose a constant momentum flux / constant wind on top of an _initially quiescent_ boundary layer. Thus it seems the most appropriate model for the peak wavenumber would be one that accounts for the _fetch dependence_; eg, the peak wavenumber decreases in time as the wind continues to blow, consistent with the concept of a constant wind that starts blowing impulsively over an ocean at rest. Two other details in those papers are:. * The relationships are formulated in terms of friction velocity rather than atmospheric wind speed, so we avoid introducing bulk formula which I think is an advantage.; * [Lenain and Pizzo 2020](https://journals.ametsoc.org/view/journals/phoc/50/12/JPO-D-20-0116.1.xml) claim that their correction to the surface wave spectrum changes the surface Stokes drift (and I suppose Stokes shear) by 15-20%. I'm not sure how much this would imprint on our results but it's something to think about.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2290#issuecomment-1058116153:1268,avoid,avoid,1268,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2290#issuecomment-1058116153,1,['avoid'],['avoid']
Safety,"> I think it would be more effort if we take into account the need to rebenchmark a lot of cases. I also think it adds some risk... Yeah true, they do claim it is more peformant so maybe something to consider in the future.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3288#issuecomment-1735298477:124,risk,risk,124,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3288#issuecomment-1735298477,1,['risk'],['risk']
Safety,"> I think passing the tendencies automatically is going to require some materialization step when the model is setup to pass into the boundary conditions but I know we're trying to avoid doing that so any other suggestions would be useful. I've started testing this by initialising the timestepper first but it is a bit clumsy. Well, we already do materialize boundary conditions, so possibly this isn't such a big deal. Another possibility is to pass the tendencies through to `fill_halo_regions!`, but that has wider implications that we'd have to think about (for example should the tendencies also be passed on to discrete-form boundary condition functions?)",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3482#issuecomment-1965127320:181,avoid,avoid,181,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3482#issuecomment-1965127320,1,['avoid'],['avoid']
Safety,"> I think that's a fine strategy. We can add a kwarg to `NonhydrostaticModel` called `hydrostatic_pressure_anomaly`. We can set it to `CenterField(grid)` to preserve existing behavior, or set it to `nothing` to avoid the separation. And we should probably make `nothing` default so that triply periodic problems can be done out of the box. Then we don't have to re-do the regression tests either because we preserve existing behavior...; > ; > I think that's also a less invasive change than this PR because we don't have to change `pressures` to `pressure` everywhere, hmm. Agreed. > Since you've done most of the legwork I think you have prerogative to open a new PR if you like (and I can help once you do). Thanks, but I unfortunately won't have time to dedicate to this for at least a few weeks. So please feel free to start the PR!",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3080#issuecomment-2088574534:211,avoid,avoid,211,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3080#issuecomment-2088574534,1,['avoid'],['avoid']
Safety,"> I think the timestamp coming first would be good. I also think having the LogLevel string be uppercase would also help readability. True, that would be nice! I'll merge this PR to avoid dragging it out but we should discuss how to improve the logger in an issue or future PR!",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/585#issuecomment-568207954:182,avoid,avoid,182,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/585#issuecomment-568207954,1,['avoid'],['avoid']
Safety,"> I think we should find a general solution to adding citations in docs, rather than attempting to avoid it. Can we define a macro that transforms a citation into a footnote? Then we can include this macro at the top of a documentation file that needs citations. I think that raising an issue with Documenter is a good idea for this. In general we should be comfortable with driving improvements to packages that we depend on rather than hacking inelegant solutions (a last resort). I think @charleskawczynski is also interested in this problem, we can try discussing on Slack. It might not be too hard if we can reuse some existing pieces:. There was an attempt back in 2017 to add a `bibtex` option to `makedocs` that allowed citations with `[<label>](@ref)`: https://github.com/JuliaDocs/Documenter.jl/issues/379#issuecomment-292157714. But it had to use the Python package `pybtex` to parse bibtex files. But since then there's been a native Julia version: https://github.com/JuliaTeX/BibTeX.jl. So maybe if we can get that `bibtex` option to work with BibTeX.jl then we can pretty easily have citations in Documenter?. I'll open an issue on Documenter.jl to see if it's possible.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/474#issuecomment-543170619:99,avoid,avoid,99,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/474#issuecomment-543170619,1,['avoid'],['avoid']
Safety,> I think we should fix the problem once. Otherwise we'll end up with unnecessary code somewhere that has to be deleted. @glwagner Can you please be clearer? Does that mean adding `min_Δt` to `Simulation` is an acceptable solution? Or should we try to avoid these round-off errors to even happen in the first place?,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3606#issuecomment-2150576412:252,avoid,avoid,252,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3606#issuecomment-2150576412,1,['avoid'],['avoid']
Safety,"> I would avoid using index `0` and also using `:` if you can. What happens when you put the label in `fig[1, 1:2]` and the contours in `fig[1, 2]`?. The 0 indexing was not the issue. `tellwidth=false` did the trick!!!. ```julia; fig[0, :] = Label(fig, title, textsize=24, tellwidth=false); ```",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2553#issuecomment-1126969454:10,avoid,avoid,10,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2553#issuecomment-1126969454,1,['avoid'],['avoid']
Safety,"> I would think that passing a function instead of collect(0:Nz) would probably be cleaner but both can work. Yeah I definitely think passing a function is more intuitive and efficient. As long as that's still an option, I'm happy :). > VerticallyStretchedRectilinearGrid(FT, size=(1, 1, Nz), x=(0, 1), y=(0, 1), z=(0, Nz), z_stretch=collect(0:Nz)); What's nice about this is that you still specify the domain boundaries, as other grids do, and you specify the stretching function separately. This makes it easier to do a stretch in any direction, and #1532 does just that. Sorry, but I'm a but confused. Aren't `x`, `y` and `z` supposed to be dimensional? Meaning they are the physical bounds of the domain? In your example `z=(0, Nz)`, which does not correspond to the physical boundaries at all. Or did you mean to write something like. ```julia; VerticallyStretchedRectilinearGrid(FT, size=(1, 1, Nz), x=(0, 1), y=(0, 1), z=(0, Lz), z_stretch=collect(0:Δz:Lz)); ```. If so then it makes sense to me, but that's a bit redundant, no? (Sorry for the confusion!)",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1544#issuecomment-813445443:1021,redund,redundant,1021,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1544#issuecomment-813445443,1,['redund'],['redundant']
Safety,"> I'm not sure I follow then. Are you saying that you're not sure all the versions in this range are bug-free? That being the case it's best to pin it to a version we know is safe?; > […](#); > On Tue, Oct 5, 2021, 17:05 Gregory L. Wagner ***@***.***> wrote: ***@***.**** commented on this pull request. ------------------------------ In Project.toml <[#1997 (comment)](https://github.com/CliMA/Oceananigans.jl/pull/1997#discussion_r722703022)> : > @@ -33,7 +33,7 @@ Tullio = ""bc48ee85-29a4-5162-ae0b-a64e1601d4bc"" [compat] Adapt = ""^3"" -CUDA = ""3"" +CUDA = ""3.0.0 - 3.3.6"" Note that Manifest.toml pins a specific version. Right, that's why I figured making it specific was the safest option. — You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub <[#1997 (comment)](https://github.com/CliMA/Oceananigans.jl/pull/1997#discussion_r722703022)>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/ADEX5KRJ3DYWCNI4IRO6RNTUFNSC3ANCNFSM5FKUICWQ> . Triage notifications on the go with GitHub Mobile for iOS <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675> or Android <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>. Oh, I just can't remember how far back we're able to go. But if you've gone to the effort to test a whole range of versions then certainly we might as well use a range, since it has the benefits you mentioned! My thought was just that it'd be quick and simple to pin to the version in the Manifest.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1997#issuecomment-935031502:175,safe,safe,175,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1997#issuecomment-935031502,2,['safe'],"['safe', 'safest']"
Safety,"> I'm not sure if `cuStreamQuery` being called 400,000 times is an error with our code, an error with CUDA.jl, not an error at all, or an error with my profiling. I didn't know this was a KA.jl-based GPU workload when commenting on Slack. The dependency/event model of KernelAbstractions.jl also uses stream queries (i.e. `cuStreamQuery`) when selecting a new stream. Maybe that's the source of these calls. It'd be good to figure out where they come from: if it's from CUDA.jl, and thus presumably because of calling the `synchronize` function, (1) why are you synchronizing that much [1], and if it's for good reasons (2) does it hurt performance and should we tweak our `synchronize` implementation to perform fewer stream queries?. [1]: some synchronization happens implicitly, e.g. when copying memory to or from the CPU (https://github.com/JuliaGPU/CUDA.jl/blob/6758fcab7ae0d72659a1ca0d56ad2c86d3b451f1/src/array.jl#L385-L399). One way to avoid some of those synchronizations, is by using pinned memory, but that's up to the application.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1912#issuecomment-900059869:945,avoid,avoid,945,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1912#issuecomment-900059869,1,['avoid'],['avoid']
Safety,"> I'm still confused about how this changes what was previously done. Did we previously create arrays on the GPU, copy checkpoint from the CPU to ""temporary"" GPU arrays, and then copy from temporary GPU arrays to previously-instantiated model data?. Yeah this is what we were previously doing in `restore_fields!`:; https://github.com/climate-machine/Oceananigans.jl/blob/8352e56f5839b23d3441f6f8bd0f297f3e0b508f/src/OutputWriters/checkpointer.jl#L102. which is kinda stupid stupid because you create a temporary `CuArray` then copy elements over (we already allocated memory for the fields in the `model`). We could have done it with a `copyto!(::CuArray, ::Array)` which would avoid unnecessary allocations except for one temporary array. Now we construct the fields with the restored data and pass it to the model constructor so there are no temporary arrays and zero unnecessary allocations.; https://github.com/climate-machine/Oceananigans.jl/blob/4ed366019c3f6a9b3ba9cf19691fef721204ea3c/src/OutputWriters/checkpointer.jl#L110-L114. > is it possible to load data from disk directly to the GPU?. Hmmm, not sure but doesn't seem impossible. At the lowest level it'll have to do some host to device copies though I think. @leios or @vchuravy would know.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/628#issuecomment-589160127:679,avoid,avoid,679,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/628#issuecomment-589160127,1,['avoid'],['avoid']
Safety,"> I'm wondering if we should provide a separate page on ""Using GPUs""? While the simulation tips for CPUs are really performance optimizations that are optional, the GPU simulation tips are mostly required to run without errors. That's a good point. Although I think we could avoid creating another page and put that information in the [""Using GPUs""](https://clima.github.io/OceananigansDocumentation/stable/using_gpus/) page, so that things are more condensed.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1543#issuecomment-818364275:275,avoid,avoid,275,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1543#issuecomment-818364275,1,['avoid'],['avoid']
Safety,"> I've been thinking on how to divide the function, but it will require the duplication of the lines:; > ; > ```; > filename = first(split(basename(filepath),""."")) * ""_part""; > filelist = filter(startswith(filename), readdir(folder)); > existing_files = length(filelist) > 0; > ```; > ; > within the code, how important is it to avoid duplication vs clarity?. Clarity matters most. But usually there are not strong trade-offs: duplication improves clarify only if the duplicated code is short and easy to understand. How is this:. ```julia; filename = first(split(basename(filepath),""."")) * ""_part""; ```. different from `current_filename`. For clarity I think it's it's best to use more lines for complicated operations rather than concatenating many operations into a single line. That line does a lot of things at once: basename, split with ""."", first, and then finally concatenation with _part. That's quite a few things for one line.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3818#issuecomment-2418455151:329,avoid,avoid,329,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3818#issuecomment-2418455151,1,['avoid'],['avoid']
Safety,"> In the HDF5 outputs for dedalus, we have a ""tasks"" group that contains the specified output tasks, to avoid any name collisions like you mentioned. I suppose `tasks` is just another name for `timeseries`. We could also call this special group `output` (which, like `tasks` for dedalus, is a bit more semantic than `timeseries`).",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/963#issuecomment-702098448:104,avoid,avoid,104,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/963#issuecomment-702098448,1,['avoid'],['avoid']
Safety,"> Indeed, it seems like; > ; > ```; > @inline tank(x, y) = ifelse(radius(x, y) < L, -H, Float64(0.0)); > ```; > ; > clears the problem. Interesting... Do you guys have any idea why that could influence the `fill_halo_bottom`?. If there's a function that can return two possible types (which is inferred to a type `Union` I think) then this might propagate downstream in the compiled code (ie two possible scenarios must be maintained; one when the returned value is `Float64`, and another with different intrinsics for when the returned value is `Int`). I think in principle this can greatly complicate the kernel function for filling the boundary condition. Broadly speaking, for functions that must be inlined into complicated kernels, it's safest to use functions that can only return one type. But I don't have a good answer for why we get the specific error `an illegal memory access was encountered`. This could be a bug in `CUDA.jl`. However, we are using an old version of `CUDA.jl` so there's not much point in raising an issue. The behavior very well may be different in the latest version... Someone with deeper knowledge of the Julia compiler / compilation system might have a better answer too...",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2367#issuecomment-1084049440:743,safe,safest,743,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2367#issuecomment-1084049440,1,['safe'],['safest']
Safety,"> Instead of saying that trig functions should be avoided on gpus, which seems very strong, I suggest pointing out that there have been some examples where trig functions have performed much slower, and include this example.; > ; > This clearly problem needs further exploration but I don't want people to be scared to use sin and cos because sometimes they just make a lot of sense. My two cents worth. I think there may be a bug in the setup that produces 100x slowdown. (@tomchor if you supply the whole script, we can investigate.) We've run many successful problems with trig functions. Some recent work by @simone-silvestri suggests we can get 2x speed up for _some_ problems by precomputing grid metrics for the `RegularLatitudeLongitudeGrid` rather than computing them on the fly:. https://github.com/CliMA/Oceananigans.jl/blob/da9c53ddd9e28d123b40726cfac2fad835284879/src/Operators/spacings_and_areas_and_volumes.jl#L178-L179. But I don't think we've definitely shown that we _always_ will get 2x speed up. Thus we are going to retain the option to compute metrics on the fly in #2025 so we can continue to investigate it. 2x is a long way from 100x though. If used in a boundary conditions, its basically irrelevant whether one uses a trig function or not. Even a forcing function is only evaluated once per grid point compared to 15x (or more?) for a `BackgroundField` velocity component with high-order advection. Some of these thoughts might be distilled into useful advice in this section of the docs. But we should definitely focus on _approaches_ to performance optimization rather than advice for specific scenarios.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2029#issuecomment-951555050:50,avoid,avoided,50,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2029#issuecomment-951555050,1,['avoid'],['avoided']
Safety,> Is it also safer to infer `FT` from `grid`?. There is no safety difference but it is preferred to use `eltype(grid)`. This also conforms to YASGuide.,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3419#issuecomment-1883486297:13,safe,safer,13,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3419#issuecomment-1883486297,2,['safe'],"['safer', 'safety']"
Safety,"> Is it possible to calculate the components `fx, fy, fz` on the fly? Also, can we infer `rotation_axis` from user-specified `fx, fy, fz`? If so, we can have one type for a constant rotation rate (storing just the parameters `rotation_axis` and `rotation_rate`), and use it for both Cartesian / rectilinear and spherical geometries. EDIT: this might be a bad idea if it adds functions like sin / cos to kernels. If we do need to pre-calculate fx, fy, fz, I think we should somehow indicate that this type is specifically Cartesian or rectilinear to avoid confusion with its spherical-geometry counterpart... unfortuntately, because its annoying.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1892#issuecomment-887762725:549,avoid,avoid,549,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1892#issuecomment-887762725,1,['avoid'],['avoid']
Safety,"> Is that a reason why you define the closure for the coarse_model?. There reason is that at the moment `NetCDFWriter` needs it to get some info on the `grid`. But as @glwagner and I pointed out, it's probably pretty easy to change `NetCDFWriter` to avoid that. I might try a PR soon that makes the simplest change possible and see it tests pass.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3460#issuecomment-2091121584:250,avoid,avoid,250,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3460#issuecomment-2091121584,1,['avoid'],['avoid']
Safety,"> It is also of course possible to simply write a function that calculates the eddy viscosity, and call this function repeatedly to avoid temporaries altogether. Don't know if it's always possible but if we can avoid temporary variables by aggressively inlining all calculations, that would be cool. Every temporary field saved means being able to run a larger model on GPUs. We will always need a couple of temporary arrays so if we can't inline we can just try to share and reuse the temporary arrays as much as possible.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/73#issuecomment-468751396:132,avoid,avoid,132,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/73#issuecomment-468751396,2,['avoid'],['avoid']
Safety,"> It might be a problem of the forcings which are divided by `h` (I am not sure that is correct, but I forsee a problem when `h → 0`). I agree that this case is a problem and we need to avoid vanishing layer depth. I noticed that the bathymetry had a maximum that was 100, not 0. Since we are setting `h = - bathymetry`, i think we have negative heights from the very beginning. This is part of the problem maybe?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2522#issuecomment-1129192752:186,avoid,avoid,186,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2522#issuecomment-1129192752,1,['avoid'],['avoid']
Safety,"> It occurs to me that we are not converging to a particular solution as we refine the grid in this case, because this problem has no viscosity. Perhaps we should do a convergence test for a case with finite viscosity. This is likely to help since the velocity is small in the boundary layer, and so will reduce the magnitude of the local correction in a step. > We do introduce grid-scale gradients in the predictor velocity field via masking, so it does seem possible to me that the leakage / pressure gradient error might scale with resolution. It'd be nice to have a solid mathematical explanation for this behavior. I would be interested in such an argument. Other than zooming in on the slowest part of the BL, I'm not sure if/why increasing resolution would reduce the total flux over a fixed interval of time.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1733#issuecomment-868299100:407,predict,predictor,407,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1733#issuecomment-868299100,1,['predict'],['predictor']
Safety,"> Just so I understand this filtering thing. It doesn't remove the regex-filtered lines from the output; it just doesn't test those lines against the ""prediction"", right?. Yeap!",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2083#issuecomment-982048875:151,predict,prediction,151,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2083#issuecomment-982048875,1,['predict'],['prediction']
Safety,"> Just to avoid confusion:; > length (in second) of minute, hour and day are well defined, as well as the; > ""second"" (but it's more complicated) and are independent of which planet you are.; > They are not ""planetary constants"".; > On the other hand, the rotation period (which, BTW, on earth, is not 1 day; > but ~86164.s, so a bit less) is a planetary constant. Good points. I've been using `seconds_per_day = 86400` to count the number of days and impose time-varying forcings, when I suppose I meant to use the rotation period. `rotation_period` makes more sense as a planetary constant.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/315#issuecomment-515485462:10,avoid,avoid,10,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/315#issuecomment-515485462,1,['avoid'],['avoid']
Safety,"> May I ask why it was decided to not call it Gent-McWilliams? I agree that in general it's good to avoid people's names in things, but in this case (imo) it makes it so much more clear what closure this is since that's what people always call it. As far as I can tell when people say they use ""Gent-McWilliams"" they are referring to a constant skew diffusivity (typically 0.3 m^2 / s). But in this closure, the diffusivity can be an arbitrary function or field. If anything it would have to be `GentMcWilliamsRedi` since we include the symmetric component as well as the skew component. Still though, I think it's important to emphasize that this closure is more general than Gent-McWilliams; not least because the only point in implementing this scheme is to develop a new, better, different parameterization.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1994#issuecomment-924962650:100,avoid,avoid,100,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1994#issuecomment-924962650,1,['avoid'],['avoid']
Safety,"> Nice! Do you have an image that illustrates the problems at low resolution, ie a plot that reproduces the Adcroft result?. Yes. Here they are. I improved the visualization on the latest commit and pushed it right now. The types of immersed boundary and plot are included in the filenames. The discrete version of the topography (i.e. the Gaussian bump for this test case) is detectable only for the heatmaps. https://github.com/CliMA/Oceananigans.jl/assets/12926768/f94a6b92-cb5e-4197-9e5b-bd8ea1bad42a. https://github.com/CliMA/Oceananigans.jl/assets/12926768/c2d37eb9-931e-4efa-aa99-c113e38bf393. https://github.com/CliMA/Oceananigans.jl/assets/12926768/38771b36-e030-4877-8c9f-61944cd9aefc. https://github.com/CliMA/Oceananigans.jl/assets/12926768/baf6c24b-3fc1-4f4e-aec7-17826bd4da61",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3123#issuecomment-1593757333:377,detect,detectable,377,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3123#issuecomment-1593757333,1,['detect'],['detectable']
Safety,"> No we should fix this for sure. I'm asking because you would save a lot of memory if you avoid writing `Field(w*c)`. Ah, I see. That's a good point, I'll investigate that. I'm wrapping things in `Field()` calls because I use the `data` option in all of them to reuse scratch space and save memory. But indeed when averages are involved that might not be the best way to go...",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2931#issuecomment-1438745870:91,avoid,avoid,91,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2931#issuecomment-1438745870,1,['avoid'],['avoid']
Safety,"> Ok here's an idea: replace `Scan` with `Accumulation`. I think a sum can be thought of as ""accumulating"" without any leaps. We then would have _reducing_ accumulations (intermediate accumulations are not stored), and _cumulative_ accumulations (where the result is not reduced). Pardon my being pedantic, but ""cumulative accumulations"" sounds pretty redundant. We might not have enough words in the English language to describe this functionality precisely :laughing:",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3590#issuecomment-2104572604:352,redund,redundant,352,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3590#issuecomment-2104572604,1,['redund'],['redundant']
Safety,"> One thing to note is that the current implementation appears to be very slow. While the simulation with the `SmagorinskyLilly` closure runs on my laptop in 10 seconds, it takes 4 minutes for the simulation with the `ScaleInvariantSmagorinsky`. I know the dynamic model will be slower given the extra computations, but such a difference seems large to me, so I'm hoping something can be changed here to improve performance:. Avoided recomputation of the strain rate at `ccc` and sped things up a bit more. Now it runs in 2.9 minutes. A lot more to go...",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3642#issuecomment-2241696460:426,Avoid,Avoided,426,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3642#issuecomment-2241696460,1,['Avoid'],['Avoided']
Safety,"> Only concern might be that by using Logging.global_logger, Oceananigans now hijacks the global logger and everything starts using it for logging. But maybe this isn't a big deal and we can revert behaviour at any time. I'm not passionate either way, but is it easy to simply avoid importing the macros associated with the global logger and defining them for a custom logger instead?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/855#issuecomment-674115587:277,avoid,avoid,277,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/855#issuecomment-674115587,1,['avoid'],['avoid']
Safety,> PS @christophernhill: Buildkite tests did not run on this PR since we disable Buildkite on PRs from forks (to avoid random PRs executing potentially malicious code on Tartarus and Sverdrup).; > ; > So we usually open and merge branches from the repo itself. OK - I'll put it in a branch sometime today,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1333#issuecomment-772577271:112,avoid,avoid,112,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1333#issuecomment-772577271,1,['avoid'],['avoid']
Safety,"> Pardon my being pedantic, but ""cumulative accumulations"" sounds pretty redundant. I think ""cumulative sum"" is redundant too. If I say I have an ""accumulation of beans"", I'm talking about the whole pile of beans. It doesn't seem to me that terminology exists, to answer your first question about whether ""scan"" is commonly used. There isn't any term that is commonly used.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3590#issuecomment-2104624096:73,redund,redundant,73,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3590#issuecomment-2104624096,2,['redund'],['redundant']
Safety,"> Perhaps the feature being asked for here is a checkpointer that avoids writing NaNs --- is that correct?. I actually think having an `error=true` option to `NaNChecker` would be useful for other purposes than avoiding a nan-filled checkpoint, so I personally still think that's the best solution here, since it's also pretty simple. If the script has plots or other analyses after `run!()`, a user might not want to waste resources running all that stuff if a `NaN` is detected. (Or they might! And hence the option :) ). Unless there's a reason why that's a bad idea that I'm missing...",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2086#issuecomment-982910455:66,avoid,avoids,66,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2086#issuecomment-982910455,3,"['avoid', 'detect']","['avoiding', 'avoids', 'detected']"
Safety,"> Point taken, but I think there are still Oceananigans-relevant applications where Value / Gradient boundary conditions on non-immersed boundaries are useful enough to be a ""core"" feature (e.g. imposing observed SST patterns rather than observed air-sea heat fluxes), but there is always the workaround of strongly restoring boundary-adjacent sponge regions. I think this is what many ocean modelers do to implement such boundary conditions anyway. True! I'm not aware that has ever been done, but since it's not difficult to support (notwithstanding @simone-silvestri's concerns about parallel performance) it's interesting to allow it --- agree. For future readers I want to point out that SST restoring (and similar models) are typically be implemented as a `FluxBoundaryCondition` using a piston velocity model, rather than using `ValueBoundaryCondition` (which implies a flux mediated by some diffusivity / viscosity, possibly derived from a parameterization). (`FluxBoundaryCondition` is mathematically identical to restoring in the surface grid point, though it would be a slightly different model to distribute the restoring over some depth). It could be an interesting project to explore using some parameterization-derived diffusivity together with `ValueBoundaryCondition` to predict surface fluxes, though, I'm not sure what the implications would be.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3224#issuecomment-1690278452:1288,predict,predict,1288,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3224#issuecomment-1690278452,1,['predict'],['predict']
Safety,"> Right yeah RegularRectilinearGrid called it grid.zC but VerticallyStretchedRectilinearGrid calls it grid.zᵃᵃᶠ to be more consistent with the curvilinear grids being added, but the output writers might not know this yet. Can we refactor the tests to use `all_z_nodes`? This could avoid issues with property names.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1429#issuecomment-791630388:281,avoid,avoid,281,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1429#issuecomment-791630388,1,['avoid'],['avoid']
Safety,"> Second, are there any boundary conditions imposed on the tracer at the immersed boundary? I remember there was a discussion but I don't remember the conclusion. Sorry. Boundary conditions are zero _second-order_ / diffusive flux on tracer. However, there can be non-zero advective flux as noted. > First, if `c` is the concentration of a tracer it should be non-negative. If you are picking it to be a `sine` I might suggest having `1 + sin` just to avoid negative values. That being said, I don't think it's going to have any impact on the results but might be worth trying. Fair point; using `1 + sin(x)` as the initial condition would measure leakage since the total ""mass"" is then Lx * Ly. I propose adding another tracer though specifically for that purpose since it's easy.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1733#issuecomment-866298163:452,avoid,avoid,452,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1733#issuecomment-866298163,1,['avoid'],['avoid']
Safety,"> Second, this is a plot of the solution for the 1D case that starts off with no motion and a Gaussian field for height. This is a nice example of geostrophic adjustment, and can certainly be compared to papers that do such things.; > ; > ![one_dimensional_shallow_water_x](https://user-images.githubusercontent.com/8239041/101361665-45e29500-386d-11eb-82d8-f440f3b07d80.gif); > ; > Third, I have done a 2D case and everything looks qualitatively the same, but it is much more diffusive. This is a plot of a slide through the centre that should reproduce the same thing as above, but the amplitudes are much weaker. This is only 64x64 so it's not very high resolution but I should probably change the advection scheme to make it more accurate.; > ; > ![one_dimensional_shallow_water_2D_x](https://user-images.githubusercontent.com/8239041/101361578-251a3f80-386d-11eb-8e59-8c02e271cfcc.gif). A sanity check would be to check whether geostrophic balance is satisfied in the center of the domain, e.g., something like the last figure from [this example](https://fourierflows.github.io/FourierFlowsDocumentation/dev/generated/OneDShallowWaterGeostrophicAdjustment/#Geostrophic-balance).",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1258#issuecomment-740212374:894,sanity check,sanity check,894,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1258#issuecomment-740212374,1,['sanity check'],['sanity check']
Safety,> Seems risky... let's see how the tests do... OK... indeed... too ambitious. I'm closing.,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1618#issuecomment-826248213:8,risk,risky,8,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1618#issuecomment-826248213,1,['risk'],['risky']
Safety,"> Should be safe to bump as long as you don't use `unsafe_wait`. I like to wait safely, thanks.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/924#issuecomment-689904623:12,safe,safe,12,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/924#issuecomment-689904623,2,['safe'],"['safe', 'safely']"
Safety,"> Since we need the performance provided by KA 0.7, and we need to use KA 0.8+ on GPU, does that mean that we should invest in developing our own CPU infrastructure (replicating what KA 0.7 offered) to achieve that performance?; > ; > Another possibility is that we re-write much of the code base to avoid the performance pitfalls we are currently facing in order to get back to the level of performance we have with current code + KA 0.7. I believe the issue is basically an interaction between some of the abstractions / indirection we have developed and the compiler, so possibly rolling back that abstraction / indirection will bring us back to where we were previously. To follow up with @vchuravy, it seems that rewriting just _some_ of the code was sufficient, so we are (probably) in the clear! The lesson learned is that we cannot slurp / splat `@kernel` function arguments, because it prevents the kernel code from being inlined.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2996#issuecomment-1482198741:300,avoid,avoid,300,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2996#issuecomment-1482198741,1,['avoid'],['avoid']
Safety,"> So if support for `pressure = nothing` (or `pressure_solver = nothing`) is added, would `BackgroundField` effectively work as a prescribed unchanging velocity?. Yes, something like this is an alternative design. But do we think that setting `pressure=nothing` might be too roundabout of a way to avoid launching kernels that calculate the velocity tendencies and to avoid allocating memory for the velocity fields?. The design I'm suggesting would have users write `velocities = PrescribedVelocities(...)` in the constructor for `IncompressibleModel` as the interface; so there's just a single keyword arg to set. We don't want `advection=nothing` since this turns off all advection, including advection by `BackgroundField`s. So out of the box @ali-ramadhan's suggestion doesn't work, but its possible that something _like_ it might be designable.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/958#issuecomment-768491247:298,avoid,avoid,298,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/958#issuecomment-768491247,2,['avoid'],['avoid']
Safety,"> So what do you guys think if this tilting is implement at the AbstractBuoyancy level? If I understand correctly all buoyancy types inherit that, right? That way we'd have a keyword for the tilt (maybe tilt, gravitational_direction or gravity_projection) that would default to (0, 0, 1) and a similar option could be given to FPlane, avoiding confusion. Ah, the issue is that abstract types cannot have properties --- they can only be used to organize type parameters and dispatch.; The logic and motivation of your suggestion is sound though (avoiding code duplication through good design); we just have to come up with a different solution. A similar solution could perhaps design a more hierarchical interface to `IncompressibleModel.buoyancy`. For example, we might write. ```julia; struct Buoyancy{G, B}; gravitational_direction :: G; model :: B; end; ```. The existing subtypes of `AbstractBuoyancy` are used for `Buoyancy.model`. The user API could be. ```julia; buoyancy = Buoyancy(gravitational_direction=(0.1, 0, 0.9), model=BuoyancyTracer()); ```. or. ```julia; buoyancy = Buoyancy(gravitational_direction=(0.1, 0, 0.9),; model=SeawaterBuoyancy(gravitational_acceleration=9.81, equation_of_state=LinearEquationOfState(α=2e-4, β=8e-5))); ```. This is more verbose but could avoid some of the issues that @tomchor sees. As for `Plane` and `BetaPlane`, I agree that those models are really predicated on a thin aspect ratio assumption that has to do with gravitational accelerations, and therefore ""know"" about the direction of gravity. We could either add a property `gravitational_direction` or, perhaps, add some wrapper / helper functions for constructing coriolis forces in tilted domains (eg `coriolis = TiltedCoriolisForces(FPlane(f=1e-4), vertical_direction=(0.1, 0.0, 0.9))` which returns `GeneralCoriolis` with 3D Coriolis forces --- or something). I think we will start to run into these sorts of issues more generically, where we need to specify ""global"" parameters that impact mu",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1242#issuecomment-782962047:335,avoid,avoiding,335,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1242#issuecomment-782962047,2,['avoid'],['avoiding']
Safety,"> Somehow, though, we can't have every module redefining `R_Earth` and using it as default, right?. I think you're right that if we are going to have a default value for this, then it should only be defined once. But if we are going to go ahead with a ""master module"" approach, then we need to implement tests. What we should avoid is people trying to change constants in the master module as a way of setting parameters. This is going to take time and effort to resolve. We should discuss in an issue, not a PR, I think.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3045#issuecomment-1492631291:326,avoid,avoid,326,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3045#issuecomment-1492631291,1,['avoid'],['avoid']
Safety,"> Thank you @navidcy! ; > ; > ; > ; > Mind if I ask how? I feel like I might have been given permission to do this last year, but just never tried it. If I don't have permission, no problem, but if I do I can learn to do this to avoid bothering others with this. . I don't have permission to give you permission. :(",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2607#issuecomment-1158827707:229,avoid,avoid,229,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2607#issuecomment-1158827707,1,['avoid'],['avoid']
Safety,"> Thank you for your help! I have installed the updated version of Julia and when running it appears to be Segfaulting when trying to add CUDA. I will open an issue with them if I cannot figure out how to solve it. Thank you again. @logan can you link the CUDA issue here please?. Also, if you haven't tried, erase everything on your `$JULIA_DEPOT_PATH` (really to be safe you should make a backup of everything there first) and then run the same script you've been running with `using Pkg; Pkg.instantiate()` on the first line. This should ""re-install"" all the packages from scratch. For context [`$JULIA_DEPOT_PATH`](https://docs.julialang.org/en/v1/manual/environment-variables/#JULIA_DEPOT_PATH) by default is `~/.julia`, I think, and that's where julia stores the package files it downloads. The hypothesis here is that at some point some package download/compilation went wrong and you have some broken code there.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2245639622:368,safe,safe,368,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3655#issuecomment-2245639622,1,['safe'],['safe']
Safety,"> That probably shouldn't have changed, can you file an issue on CUDA.jl/GPUArrays.jl? I'll have a look next week. The only change to `@allowscalar` that comes to mind is task/thread-safety, which does come at a certain performance cost (it now does a TLS lookup instead of a simple pointer check, but the cost of that should be negligible compared to the subsequent memory transfer). Seems I can't reproduce the supposed error so, sorry, my bad... Something else must have been the issue. 😔",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1514#issuecomment-817385393:183,safe,safety,183,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1514#issuecomment-817385393,1,['safe'],['safety']
Safety,"> The paper that I cited used periodic boundary conditions for the deviations in the horizontal. Not physically meaningful but it is one way to avoid the walls. Agreed. But I am arguing that there is a problem with the boundary conditions in the vertical in your script (and the tilted bottom boundary layer script in the docs). Rearranging the boundary conditions in WT2020, . <img width=""313"" alt=""Screenshot 2024-04-29 at 12 09 29 PM"" src=""https://github.com/CliMA/Oceananigans.jl/assets/12971166/216d7f9f-6f5c-4ab7-8a73-15a0f2318aeb"">. we get $\frac{\partial b}{\partial z} = - N^{2}_{\infty} \cos {\theta}$ at $z=0$.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3568#issuecomment-2083473885:144,avoid,avoid,144,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3568#issuecomment-2083473885,1,['avoid'],['avoid']
Safety,"> Third, what exactly is computed in the last plot fo the integrated stress tensor? Also, have you computed these for the nonIBM case to see what the real wall does?. Sorry probably could have been clearer on this. This is just du/dy, calculated with a centered difference between the closest two fluid cells to the boundary, then integrated along the boundary line (ie. du/dy summed and multiplied by Lx). The plot above is the error between the IBM and nonIBM case. This plot here is the values themselves (dashed is nonIBM, solid is IBM); ![Bickley_dudy](https://user-images.githubusercontent.com/67593861/122993772-b4a0bf00-d375-11eb-9b60-2243077ce838.png); ; > > First, if `c` is the concentration of a tracer it should be non-negative. If you are picking it to be a `sine` I might suggest having `1 + sin` just to avoid negative values. That being said, I don't think it's going to have any impact on the results but might be worth trying.; > ; > Fair point; using `1 + sin(x)` as the initial condition would measure leakage since the total ""mass"" is then Lx * Ly. I propose adding another tracer though specifically for that purpose since it's easy. You are correct. I think the added tracer idea would be perfect. My mind was stuck on the cylinder concentration one I've done before, and didn't think about the difference here!",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1733#issuecomment-866308816:820,avoid,avoid,820,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1733#issuecomment-866308816,1,['avoid'],['avoid']
Safety,"> This looks good! I'll approve it now but I'll leave the following suggestions if you think they're helpful:; > ; > * Add brief docstrings with a quick example; > * Change the names from `u` and `v` in the functions to something more general in order to avoid confusion. Maybe `a`, `b` or some greek letters. (I'm assuming that this works for any `Field`s, no?). I agree. Sorry that seems ""rushed"" but we need this feature for some project. But I will put these as an issue!",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1987#issuecomment-923295323:255,avoid,avoid,255,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1987#issuecomment-923295323,1,['avoid'],['avoid']
Safety,"> This was mostly dealt with #3488 but still the implementation in #3488 requires 1 region per panel. Maybe we close this issue (to avoid confusion), and later, create a new one to enable multiple regions per panel and an associated PR (to close it)?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3199#issuecomment-2044094083:132,avoid,avoid,132,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3199#issuecomment-2044094083,1,['avoid'],['avoid']
Safety,"> Thought: we provide the sugary syntax `.top` and `.bottom` for boundary conditions in `z`. Should we also provide east, west, south, and north, and avoid using `.left` and `.right` in the code for full clarity?. Sounds like a good idea. I think `setbc!` and `getbc` aren't fully tested so also worth adding some simple tests there. I'll open an issue to document this.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/631#issuecomment-589873540:150,avoid,avoid,150,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/631#issuecomment-589873540,1,['avoid'],['avoid']
Safety,"> Thought: we provide the sugary syntax `.top` and `.bottom` for boundary conditions in `z`. Should we also provide east, west, south, and north, and avoid using `.left` and `.right` in the code for full clarity?; > ; > _Originally posted by @glwagner in https://github.com/climate-machine/Oceananigans.jl/pull/631#pullrequestreview-362963583_",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/637:150,avoid,avoid,150,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/637,1,['avoid'],['avoid']
Safety,"> To start working on this I think we need an MWE. Mabye that's easy, just a simulation with constant time-step and output on TimeInterval which should, in theory, work perfectly. Wouldn't the MWE I posted above work? Or do you mean a MWE that generates round-off errors at _predictable_ times? (I cannot predict when the errors will occur in the MWE above.). > Hmm and there is one more point. Round-off error is the reason we get tiny time-steps, and we should fix that. However, that would still leave open the underlying problem, which is that the pressure correction fails for machine epsilon time-steps. So I'm wondering if in fact we should fix both issues. I agree with this point, but I feel like I'm also missing something here. Let's say we change the pressure correction so that it works for machine epsilon. Won't the pressure gradient force still depend on `Δt`? That is, won't it be larger for small `Δt`s and vice-versa? If so, this implies the pressure gradient doesn't converge with `Δt` and if so, how do we close a budget where the pressure term is important? (Maybe that's a question for another place also... I don't want to derail the discussion from the specific issue at hand.)",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3593#issuecomment-2104553427:305,predict,predict,305,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3593#issuecomment-2104553427,1,['predict'],['predict']
Safety,> Under ColPrac people should merge their own PRs to avoid surprises but I'll take your comment as permission to merge and tag a new release :P. I have been doing that. I'm just not sure about the tagging thing so I thought it was best if you did it.,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1367#issuecomment-780208984:53,avoid,avoid,53,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1367#issuecomment-780208984,1,['avoid'],['avoid']
Safety,"> Update: I used the formulas in the paper to approximate the gradient of pressure. In a test case that is pressure dominated, I found that the accuracy imporoved by more than what the theory predicted (> 16), but that is very encouraging that we are doing something right. Note that ""4th-order"" refers to the rate of convergence of the scheme as the grid spacing is reduced. For a particular resolution (and assuming that you are in the ""asymptotic regime of convergence""), the improvement gained from higher-order scheme involves both the rate of convergence / slope of the error estimate as well as a constant (the intercept) that's specific to the problem. As a result, you usually don't have a theoretical prediction for improvement at fixed resolution, I don't think (though I suppose you might be able to generate an estimate if you are differentiating a function with easily knowable properties). Empirically we usually find that higher-order schemes reduce the constant by quite a bit! Which is good --- since we often run marginally resolved problems, improving the constant (rather than the slope/rate of convergence) turns out to be the most important benefit of a high-order scheme. The plot is super busy, but the point is hopefully illustrated: higher-order advection schemes not only converge _more quickly_ to the exact solution as resolution is increased (eg, the slopes of the lines are steeper) but _also_ have reduced error at a _fixed_ resolution (eg, the error is less for WENO5 than CenteredSecondOrder at the fixed resolution 2^7):. ![image](https://user-images.githubusercontent.com/15271942/101667154-fba01600-3a1c-11eb-824e-f3ec82367229.png)",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1266#issuecomment-741942088:192,predict,predicted,192,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1266#issuecomment-741942088,2,['predict'],"['predicted', 'prediction']"
Safety,"> We could procede in a couple of ways from here:. Let me provide a few other options:. * Compute the 2D boolean masks for reduced operations in `x`, `y`, and `z` when forming `ImmersedBoundaryGrid`. Then, use those masks for conditional differencing of reduced fields, rather than calling `immersed_inactive_node`. * For `GridFittedBottom` and `PartialCellBottom`, as a stop gap, define these methods:. ```julia; @inline conditional_δx_f(ℓy, ℓz, i, j, k, ibg::GFBIBG, δx, r::ZIRF, args...) = ifelse(immersed_inactive_node(i, j, ibg.Nz, ibg, c, ℓy, ℓz) |; immersed_inactive_node(i-1, j, ibg.Nz, ibg, c, ℓy, ℓz),; zero(ibg),; δx(i, j, k, ibg.underlying_grid, r, args...)). ```. That use a condition based on whether `k == Nz` is immersed. This will fix fields that are reduced in `z` without increasing memory storage or doing a computation in the immersed boundary grid constructor. And that's the most common case anyways. * When building a `ReducedField`, compute the mask that has to be applied to abstract operations. Then, extend the `_derivative` constructor for the combination of a `ReducedField` argument + `ImmersedBoundaryGrid` using conditional operation. This has the advantage of avoiding the mask computation in `ImmersedBoundaryGrid` (since its only needed to do operations on reduced fields). The disadvantage is that different reduced fields have to redo the computation. Also, this only fixes abstract operations and does not fix the internal operators. We also have to update the conditional operators to throw away the immersed boundary grid for reduced fields, or throw away the immersed boundary grid inside the abstract operation. * A variant on the above approach is to compute the mask when forming `Derivative`. But then a new mask is computed for every operation. There's probably a lot of other options. Keep the brainstorming coming.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3588#issuecomment-2099195353:1194,avoid,avoiding,1194,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3588#issuecomment-2099195353,1,['avoid'],['avoiding']
Safety,"> We have to keep masking for velocities in the hydrostatic model.; > ; > In the nonhydrostatic model, the equivalent masking procedure acts on the predictor velocity field prior to solving for pressure:. Yes, I kept that in the nonhydrostatic model. Although I think I removed it on the hydrostatic one so I'll put it back in.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2616#issuecomment-1224091327:148,predict,predictor,148,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2616#issuecomment-1224091327,1,['predict'],['predictor']
Safety,"> We should use the same method on the CPU and GPU that permutes the dimensions of the array. . The problem is that we can do something more efficient on the CPU with FFTW's cosine transforms (I haven't benchmarked the permutating DCT algorithm on the CPU, although it's probably safe to assume that FFTW is faster). We may be able to switch to a a wholly more efficient pressure solver that solves tridiagonal systems in the vertical with PR #306 so then we can get rid of the permutation algorithm for doubly periodic configurations and have fewer Poisson solvers.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/475#issuecomment-545510373:280,safe,safe,280,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/475#issuecomment-545510373,1,['safe'],['safe']
Safety,"> What is being plotted in the comparisons? It looks like 3D fields are being outputted, but the time series plots show some reduction of the 3D data. In the 1D comparisons I'm plotting the volume average over the whole domain. Sorry if that wasn't clear. > I think I might have missed something --- in the very first example, was TKE computed using a ComputedField or KernelComputedField? Are the later results in this post consistent with the first posted results?. My example script changed a bit throughout the day. In the beginning I was only using KernelComutedFields since I thought that was the source of the problem. Later I started computing the diagnostics with both KernelComputedFields and ComputedFields for comparison. Which is when I found that ComputedFields were also output incorrectly... So basically the code that I linked [in my previous comment](https://github.com/CliMA/Oceananigans.jl/issues/1517#issuecomment-809655170), which is the most up-to-date, should be the one we should continue to use to sort this out I think. > Is there any way that any of this has to do with time-step alignment?. It's possible, but I'm not sure how to test that for now... > Lastly, why is the window slightly different from the TimeInterval? What happens when the time-interval and averaging window are the same (which appears to be our default?). I think I reported it in an issue a while ago, but basically if I set the `window` to be exactly the same as `interval` I get warnings on Oceananigans telling me that I'm calculating the average before the window is complete or someting like that (this should be easy to fix I think...). So I always set the `window` to be slightly smaller than `interval` to avoid those messages. I tried setting the `window` exactly the same as `interval` and the results were the same (plus I got a lot of those warnings...).",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1517#issuecomment-809905267:1715,avoid,avoid,1715,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1517#issuecomment-809905267,1,['avoid'],['avoid']
Safety,"> What's the motivation for using a macro rather than multiple dispatch?. The main motivation being that we don't have to write extra functions that dispatch on the forcing, thus simplifying the time stepping code. As you point out we don't want to write _5! = 120_ new functions. The `update_source_terms!` function is already 52 lines long so I'd rather avoid having to dispatch on this function. > An argument against macros is that they make the code more obscure. It's harder to figure out what is happening because you have to find the definition of the macro. I think this is context-dependent. The purpose of a macro with a name like `@insert_forcing_u` or `@insert_forcing_term` is pretty clear. If we used dispatch then you'd still have to scroll through multiple function definitions. > Come to think of it, the user can also just define a forcing function that indexes into some constant array. Why is this not a good solution?. I think this would work pretty well. I couldn't figure out how to pass in the array to be indexed so that it can fit in the `Fu(grid, velocities, tracers, i, j, k)` signature and be available for the user to fill. Where in the model should we store the forcing array in this case?. Hmmm, actually we could make the function accept the forcing struct, e.g.; ```julia; Fu(grid, velocities, tracers, forcing, i, j, k); ```; but then we'd have to have arrays in the forcing struct, e.g.; ```julia; struct Forcing{Tu,Tv,Tw,TT,TS,TA<:AbstractArray}; u::Tu; v::Tv; w::Tw; T::TT; S::TS; u_arr::TA; v_arr::TA; w_arr::TA; T_arr::TA; S_arr::TA; end; ```; and then the forcing function is just; ```julia; Fu(grid, velocities, tracers, forcing, i, j, k) = forcing.u_arr[i, j, k]; ```. Either we have 5 array types so fields with an actual forcing function get `nothing` for the array or we have 1 array type `TA` and set the arrays for forcings with a function to something like `Array{Float64}(undef, 0)`. Might be a little too ugly but I think yeah we should be able to a",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/110#issuecomment-470548318:356,avoid,avoid,356,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/110#issuecomment-470548318,1,['avoid'],['avoid']
Safety,"> When I set the halo to be `halo=(3,3)` I get the following error message since it wants `halo=(3,3,0)`. This seems slightly less than idea. Should we make it so that either are acceptable?; > ; > ```; > [2021/05/18 12:08:59.339] WARN Inflating model grid halo size to (3, 3, 0) and recreating grid. The model grid will be different from the input grid. To avoid this warning, pass halo=(3, 3, 0) when constructing the grid. -@-> /home/fpoulin/software/Oceananigans.jl/src/Grid; > ```. This is a warning, not an error right?. Yes, I think it'd be good to change the warning (since it's wrong). I would save this for another PR.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1662#issuecomment-843321244:358,avoid,avoid,358,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1662#issuecomment-843321244,1,['avoid'],['avoid']
Safety,"> When `data` returns a view, all of the redundant `@views` should be removed. Yup, that'll be nice!",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/250#issuecomment-496893274:41,redund,redundant,41,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/250#issuecomment-496893274,1,['redund'],['redundant']
Safety,"> Where in the model should we store the forcing array in this case?. The user would define an array in a script, declare it `const` to the compiler, and then write a function that indexes into it as a global:. ```julia; # define a; forcing(..., i, j, k) = a[i, j, k]; ```. We can also include a constructor for `Forcing` that allows the user to pass some function that defines a constant array, and set up the same functionality internally. > The main motivation being that we don't have to write extra functions that dispatch on the forcing, thus simplifying the time stepping code. As you point out we don't want to write 5! = 120 new functions. The update_source_terms! function is already 52 lines long so I'd rather avoid having to dispatch on this function. I think the problem is that our functions are trying to do too much at once. We need smaller functions that perform more atomic operations so we can dispatch on atomic operations. I don't think we need to re-invent multiple dispatch with macros. We just need to refactor the code so we can use multiple dispatch effectively.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/110#issuecomment-470556414:722,avoid,avoid,722,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/110#issuecomment-470556414,1,['avoid'],['avoid']
Safety,"> Work with FieldTimeSeries as if it were a 4D array (with operations ignoring the halos). To clarify, are you referring to getting broadcasting and reduction to work?. If so I would clarify this comment: we need to extend our existing broadcasting and reduction machinery for 3D fields to 4D field time series. The problem is not merely to avoid operations on halos, but also to correctly interpolate between locations on a staggered grid and correctly apply derivatives on arbitrary grids. The built-in broadcasting machinery launches kernels via `launch!` so such functionality avoids operations on halos like all other `launch!` kernels.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1641#issuecomment-835433735:341,avoid,avoid,341,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1641#issuecomment-835433735,2,['avoid'],"['avoid', 'avoids']"
Safety,> Yeah we could just do it for Metal. I was just thinking it might be just as much effort as using it for all. I think it would be more effort if we take into account the need to rebenchmark a lot of cases. I also think it adds some risk...,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3288#issuecomment-1735296417:233,risk,risk,233,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3288#issuecomment-1735296417,1,['risk'],['risk']
Safety,"> You are passing too many parameters in the `compute!` kernel, unfortunately. GPUs have a limit on the size of the parameters you can pass (4352 bytes, and you are using 4592 bytes). Ah I see. So basically this is just a GPU limitation and there's nothing to be done in the Oceananigans/user side (apart ofc from splitting the computations)?. Also, is there a way to know at compile time whether the max parameter size is being reached? That way I can define things without ""nesting"" `Field` calls when possible (which saves memory), and use `Field` calls only when necessary to avoid this error. Thanks!",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3140#issuecomment-1581740522:580,avoid,avoid,580,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3140#issuecomment-1581740522,1,['avoid'],['avoid']
Safety,"> You can try incorporating the background buoyancy field into the forcing functions (formulated using the discrete form). You may also try inserting the parameters as globals rather than using the kwarg `parameters` (not sure if that will help). I'd also suggest testing whether the Smagorinsky closure affects the results of the simulation; if you can avoid using that you might be able to compile more complexity. @glwagner thanks for all these tips. I've tried them all (including using the discrete form) and the only thing that allows me to achieve the number of tracers I need is using `closure=nothing`. However I don't think that's an option for me since I will probably need the physical (KE) dissipation at some point in the research, which doesn't exist without a closure. ; Also I think `closure=nothing` prevents me from using flux boundary conditions, no?. What would you recommend as the next step?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2869#issuecomment-1402856222:354,avoid,avoid,354,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2869#issuecomment-1402856222,1,['avoid'],['avoid']
Safety,"> `clobber=false` is probably safer. That's what I was thinking too but apparently with NCDatasets.jl the other modes are append (`""a""`) and read-only (`""r""`) so if you're creating files for the first time (the most common use case) then clobber must be used. Otherwise you get NetCDF errors when it tries to open a non-existent file. > Named tuples are trivial to support; but maybe users don't care. Personally, I much prefer them in the JLD2OutputWriter. I actually wasn't able to get named tuples to work with JLD2 but I opened an issue about that: #562. I don't see the big advantage of a `NamedTuple` over a `Dict` in this case so I'm going to keep it simple and stick to named tuples here for now. The original point of this PR was that the documentation was wrong. `NetCDFOutputWriter` needs to be refactored a little anyways, and I'd like to add some features to it as well so I'll add support for named tuples then.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/556#issuecomment-564362931:30,safe,safer,30,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/556#issuecomment-564362931,1,['safe'],['safer']
Safety,"> grid_base = RectilinearGrid(size=(4, 4, 4),; x = (0, 1), y=(0, 1),; z = (0, 1),; halo=(3,3,3),; ); 4×4×4 RectilinearGrid{Float64, Periodic, Periodic, Bounded} on CPU with 3×3×3 halo; ├── Periodic x ∈ [0.0, 1.0) regularly spaced with Δx=0.25; ├── Periodic y ∈ [0.0, 1.0) regularly spaced with Δy=0.25; └── Bounded z ∈ [0.0, 1.0] regularly spaced with Δz=0.25. julia> grid = ImmersedBoundaryGrid(grid_base, GridFittedBottom(bathymetry)); 4×4×4 ImmersedBoundaryGrid{Float64, Periodic, Periodic, Bounded} on CPU with 3×3×3 halo:; ├── immersed_boundary: GridFittedBottom{OffsetArrays.OffsetMatrix{Float64, Matrix{Float64}}}; ├── underlying_grid: 4×4×4 RectilinearGrid{Float64, Periodic, Periodic, Bounded} on CPU with 3×3×3 halo; ├── Periodic x ∈ [0.0, 1.0) regularly spaced with Δx=0.25; ├── Periodic y ∈ [0.0, 1.0) regularly spaced with Δy=0.25; └── Bounded z ∈ [0.0, 1.0] regularly spaced with Δz=0.25. julia> model = NonhydrostaticModel(grid = grid,; advection = WENO5(grid=grid),; ); ┌ Warning: WENO on a curvilinear stretched coordinate is not validated, use at your own risk!!; └ @ Oceananigans.Advection ~/repos/Oceananigans.jl/src/Advection/weno_fifth_order.jl:197; ERROR: MethodError: no method matching return_metrics(::ImmersedBoundaryGrid{Float64, Periodic, Periodic, Bounded, RectilinearGrid{Float64, Periodic, Periodic, Bounded, Float64, Float64, Float64, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}}}, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}}}, CPU}, GridFittedBottom{OffsetArrays.OffsetMatrix{Float64, Matrix{Float64}}}, CPU}); Closest candidates are:; return_metrics(::LatitudeLongitudeGrid) at /home/tomas/repos/Oceananigans.jl/src/Advection/weno_fifth_order.jl:215; return_metrics(::RectilinearGrid) at /home/tomas/re",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2549:1154,risk,risk,1154,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2549,1,['risk'],['risk']
Safety,> hey you need to use; > ; > ```; > DocTestSetup = quote; > function foo(x); > return x^2; > end; > end; > ```. I wanted to avoid activating it globally,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3707#issuecomment-2287869743:124,avoid,avoid,124,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3707#issuecomment-2287869743,1,['avoid'],['avoid']
Safety,"> is it easy to simply avoid importing the macros associated with the global logger and defining them for a custom logger instead?. You mean like defining and using `@custom_info`, `@custom_warn` for `OceananigansLogger`? That might not be great in case users want to use a different logger. I think we want to use `@info`, `@warn`, etc. and users can always go back to the base/default logger whenever they want.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/855#issuecomment-674213130:23,avoid,avoid,23,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/855#issuecomment-674213130,1,['avoid'],['avoid']
Safety,"> the constructors of a CuSparseMatrix are different from the ones for a SparseMatrix. Can the constructors for CuSparse be derived from Sparse? If so, we just build the sparse on CPU, and then implement `arch_array`. > It means that we will first have to create the full matrix (of size Nx * Ny x Nx * Ny!!) Or is this computationally/memory-wise restrictive?. I think you should avoid allocating the whole matrix and build the sparse representation on the fly. The sparse representation is like a graph --- we add nodes only when they exist. Once we've built the graph, we can convert it to a computationally efficient format for time-stepping on CPU or GPU. > Also it is kind of a brute force which would be quite computationally inefficient because it scales badly. I propose we get this up and running in 2D and see whether this is a feasible calculation to perform during model construction. There is the question of preconditioners / changing time-step. Do the sparse matrix implementations we work with have efficient implementations for operations like. ```julia; A += b * I; ```. where `I isa UniformScaling` and `b` is a scalar?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2396#issuecomment-1109680238:381,avoid,avoid,381,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2396#issuecomment-1109680238,1,['avoid'],['avoid']
Safety,> to avoid waiting on another round of tests to pass 🙃. Exactly! Takes for ever...!,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3876#issuecomment-2448498814:5,avoid,avoid,5,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3876#issuecomment-2448498814,1,['avoid'],['avoid']
Safety,"> yep broadcasting works. My thought was that `plan` can be hacked to store unified memory. I still have to look at the data structure to see how to do it.; > ; > cufftxt basically works in the same way (local FFT direction distributed among the workers then transpose and nonlocal FFT). I am not sure they use unified memory but they for sure use transposes https://on-demand.gputechconf.com/gtc/2014/presentations/S4788-rapid-multi-gpu-programming-cuda-libraries.pdf. Mmm ok. Is this proposal a way to avoid cuffxt basically? I think what you outlined is somehow roughly how PencilFTTs work:. 1. FFT along local direction (dim=1); 2. Simultaneously communicate and permute data to (2, 1, 3) (or is it (2, 3, 1)?); 3. FFT along local direction (dim=2); 4. Simultaneously communicate and permute data to (3, 2, 1); 5. FFT along dim=3. At the end, the data has permutation (3, 2, 1). The backwards transform then reverses this process. `solver.storage` is actually a tuple of 3 preallocated arrays to support this algorithm. For the tridiagonal solver I think we want to use the same algorithm, except that we skip step 1 (ie the first step is to communicate and permute data with no transform). Once the two other transforms are complete we have data in the configuration (x, y, z) where z is local, and we can do a tridiagonal solve in eigenfunction space. Then we transform back and obtain data back in the (z, y, x) permutation, with z local, and copy into the pressure. We have to extend the tridiagonal solver to accomodate this kind of permutation for distributed CPU, so if we have an algorithm like the one above we can then also use it for MultiRegionGrid solves on the GPU.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2523#issuecomment-1119916674:504,avoid,avoid,504,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2523#issuecomment-1119916674,1,['avoid'],['avoid']
Safety,">The core of the method is an eigenfunction expansion of the discrete Poisson operator... I think?; >; >Perhaps we can call this a `EigenPoissonSolver`, or even `FFTPoissonSolver` --- through we should use caution to avoid misleading people into thinking that our code is spectral. This algorithm solves the Poisson equation when it has been *discretized on a staggered grid*. We should not discuss spectral methods or Fourier transforms, and the word 'spectral' should not appear anywhere in the code. I think this is very misleading. It should be a high priority to change this language.; >; >_Originally posted by @glwagner in https://github.com/climate-machine/Oceananigans.jl/issues/102#issuecomment-468947308_. I think `EigenPoissonSolver` is a better name. Just creating a separate issue as this should be a pretty easy item to resolve, and is separate from updating the documentation.",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/106:217,avoid,avoid,217,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/106,1,['avoid'],['avoid']
Safety,@ali-ramadhan @jm-c I realized that I may have specified the flux boundary condition wrong --- I think that I am double counting the flux coming in/out of the cell from the interior (this part is calculated during the main time-stepping loop. I guess the sanity check is that it shouldn't do anything if `flux` is zero. I'll add you as collaborators on my repo in case you want to change anything (I probably won't have time till Monday). Also looks like Travis is failing; not sure why that is. Feel free to criticize my design. This is just a first pass as I see it.,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/118#issuecomment-471143226:255,sanity check,sanity check,255,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/118#issuecomment-471143226,1,['sanity check'],['sanity check']
Safety,"@ali-ramadhan I did not see this in PR #147 (too many lines of diff for time_steppers.jl, so i missed this).; But regarding your question about ""epsilon"", this is just an other name for parameter ""χ"" (chi).; In fact, I would prefer to have ""0.5 + χ"" i beeing called the AB-2 parameter; this way, if set to zero then ; we recover a simple first order forward time stepping.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/160#issuecomment-479312286:321,recover,recover,321,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/160#issuecomment-479312286,1,['recover'],['recover']
Safety,"@ali-ramadhan could the NVidia cufftw interface help ( https://docs.nvidia.com/cuda/cufft/index.html#fftw-supported-interface ) as a step. My reading is that this does things on GPU, so could avoid copy, but may not be as super optimal as native cuFFT. Maybe you already tried that - the advertising on the box seems to imply you just switch a few headers, link and existing FFTW works, but maybe there is small print?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/56#issuecomment-464772997:192,avoid,avoid,192,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/56#issuecomment-464772997,1,['avoid'],['avoid']
Safety,@ali-ramadhan do we need to call `CUDA.versioninfo()`? Can we avoid this for now?,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1799#issuecomment-872252367:62,avoid,avoid,62,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1799#issuecomment-872252367,1,['avoid'],['avoid']
Safety,"@ali-ramadhan, the docs log says:; ```; ┌ Info:;   | │ Buildkite config:;   | │ Commit branch: ""tomchor-patch-1"";   | │ Pull request: ""false"";   | │ Commit tag: """";   | │ Detected build type: devbranch;   | │ - ✘ ENV[""BUILDKITE_BRANCH""] matches devbranch=""master"";   | │ - ✔ ENV[""DOCUMENTER_KEY""] exists;   | └ Deploying to folder ""dev"": ✘; ```; Seems like buildkite will only reply from master? I don't see where that setting in buildkite is though...",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1383#issuecomment-782282187:171,Detect,Detected,171,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1383#issuecomment-782282187,1,['Detect'],['Detected']
Safety,"@christophernhill is it possible to come up with a benchmark that does not use `ContinuousBoundaryFunction`, thereby avoiding the bug in #1928 ?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1912#issuecomment-894674873:117,avoid,avoiding,117,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1912#issuecomment-894674873,1,['avoid'],['avoiding']
Safety,"@francispoulin @glwagner @ali-ramadhan @christophernhill ; I took a look at all the benchmarking scripts in our benchmarks folder and realized that many of them are very similar and can be unified. For example, the single script for shallow water model's strong and weak scaling differ only by one substring. ; The latest commit I pushed to this branch unifies all of the launcher and single scripts for shallow water model into three scripts. Now, at the top of the launcher script `distributed_shallow_water_model.jl`, there are two boolean variables that the user can toggle for strong vs weak scaling and mpi vs threaded parallel execution. Everything including output graphs, HTML tables, and info messages also change accordingly based on the two booleans. There are some other features that Francis and I have discussed but would like your approval first before adding them in.; Other possible additions to the script can include a for loop which wraps around the whole launcher script which loops through the strong/weak scaling and mpi/threaded parallelism options to allow for running 4 benchmarks at once. Another possible addition is to have what model is benchmarked also be an option. Granted, I could just copy and paste the shallow water model scripts and replace all instances of `shallow water` with `nonhydrostatic` or `hydrostatic` and tune some options a little bit, but then this would again cause avoidable clutter. Having what model is benchmarked as an easily changeable option can be achieved through a model setup function that dispatches what model is initialized based on a custom model type object that's passed to it. Everything else would be cosmetic formatting of outputs and info messages.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1881#issuecomment-899928441:1420,avoid,avoidable,1420,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1881#issuecomment-899928441,1,['avoid'],['avoidable']
Safety,"@francispoulin Ah the warning about Δt = 0 is probably specific to the `IncompressibleModel` but is printed in the time stepper so it would show up for other models as well. I think we agreed in https://github.com/CliMA/Oceananigans.jl/pull/1255 to avoid calling `time_step!(model, 0)` since `time_step!` is not a user-facing function. Taking this thought further: I guess the responsibility for not taking zero time steps lies with the `Simulation` and with the user?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1138#issuecomment-779864028:249,avoid,avoid,249,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1138#issuecomment-779864028,1,['avoid'],['avoid']
Safety,"@francispoulin I do not know but I think it would be good to find out. `VerticallyImplicitTimeDiscretization`, despite being verbose, is actually too terse --- what we are really implementing is a _backwards_ vertically implicit time discretization. Our implementation in `QuasiAdamsBashforth2` is a first-order backwards approximation. Note that `QuasiAdamsBashforth2` is also first-order for explicit stepping, so this implementation should not change the accuracy of the scheme, I don't think. In `RungeKutta3` we implement a scheme that is backwards ""within stages"". This corresponds to some particular choice of Butcher tableau and I am not sure if it is formally 3rd order still. We might be able to develop a convergence test to an analytical solution to investigate this problem. It needs to be relatively high spatial resolution so that time-stepping errors are larger than spatial discretization errors. I propose we implement some validation tests and convergence tests in a future PR though to avoid bogging this PR down more...",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1639#issuecomment-838806675:1006,avoid,avoid,1006,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1639#issuecomment-838806675,1,['avoid'],['avoid']
Safety,"@francispoulin I think you can give a minimum Δt to `TimeStepWizard` that avoids that. It's zero by default, but you can set something like `min_Δt=0.05` (or something else unreasonably small) to achieve what you want.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1138#issuecomment-779862916:74,avoid,avoids,74,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1138#issuecomment-779862916,1,['avoid'],['avoids']
Safety,"@francispoulin you are correct that a source charge is added to the Poisson RHS to facilitate a solve using eigenfunctions for the homogeneous Neumann problem. The way the source charge is added is very subtle: the source is added by setting the predictor velocity boundary values (typically this means zeroing them out, when the boundary is impermeable):. https://github.com/CliMA/Oceananigans.jl/blob/4011a3431144c3a58671e5d827ab39da8fe6e948/src/Models/IncompressibleModels/pressure_correction.jl#L12. This algorithm is very subtle because one can implement it by simply _not updating the predictor velocities_ on boundaries. Changing the wall-normal values of the predictor velocity field modifies the predictor velocity divergence that contributes to the RHS of the pressure Poisson equation. The reason this is an effective source term is because the predictor velocities _do not_ satisfy the same boundary conditions as the physical velocity field (in particular, the predictor velocities have non-zero wall normal components when there is a pressure gradient on the boundary). Thus zeroing out the wall-normal predictor velocities changes the Poisson equation RHS. With some head scratching, it turns out that the modification is precisely what is needed to describe pressure gradients on the boundary. We need to write this up somewhere, not least because there's no clear reference for this algorithm in the literature. We also would like to generalize the algorithm to work for time-varying wall-normal velocities (a separate issue but worth noting here that the current algorithm does not work for this case).",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1170#issuecomment-735421171:246,predict,predictor,246,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1170#issuecomment-735421171,7,['predict'],['predictor']
Safety,"@francispoulin you are correct that a source charge is added to the Poisson RHS to facilitate a solve using eigenfunctions for the homogeneous Neumann problem. The way the source charge is added is very subtle: the source is added by setting the predictor velocity boundary values (typically this means zeroing them out, when the boundary is impermeable):. https://github.com/CliMA/Oceananigans.jl/blob/4011a3431144c3a58671e5d827ab39da8fe6e948/src/Models/IncompressibleModels/pressure_correction.jl#L12. This algorithm is very subtle because one can implement it by simply _not updating the predictor velocities_ on boundaries. Changing the wall-normal values of the predictor velocity field modifies the predictor velocity divergence that contributes to the RHS of the pressure Poisson equation. The reason this is an effective source term is because the predictor velocities _do not_ satisfy the same boundary conditions as the physical velocity field (in particular, the predictor velocities have non-zero wall normal components when there is a pressure gradient on the boundary). Thus zeroing out the wall-normal predictor velocities changes the Poisson equation RHS. With some head scratching, it turns out that the modification is precisely what is needed to describe pressure gradients on the boundary. We need to write this up somewhere, not least because there's no clear reference for this algorithm in the literature. We also would like to generalize the algorithm to work for time-varying wall-normal velocities (a separate issue but worth noting here that the current algorithm does not work for this case). _Originally posted by @glwagner in https://github.com/CliMA/Oceananigans.jl/issues/1170#issuecomment-735421171_",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1232:246,predict,predictor,246,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1232,7,['predict'],['predictor']
Safety,"@francispoulin you shouldn't have to worry about this when developing `ShallowWaterModel`. This purely concerns `AbstractOperations`, output, and diagnostics, which doesn't affect the time-stepping / physics kernels. You still care about resolving this of course, because you're using `Fields` and therefore can use `AbstractOperations` for specifying output... I think you're right that right now you can hack your way to success by avoiding nested Binary's (when possible). The above example is one case...",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1241#issuecomment-738864975:434,avoid,avoiding,434,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1241#issuecomment-738864975,1,['avoid'],['avoiding']
Safety,@glwagner ; All Krylov solvers have an in-place version where the first argument is a workspace that contains all storage needed by the solver.; You can recover the solution with `solver.x` or just `solution(solver)`. I wrote a section about in-place methods in the documentation:; https://jso.dev/Krylov.jl/dev/inplace/. The in-place version of CG is detailed [here](https://jso.dev/Krylov.jl/dev/solvers/spd/#Krylov.cg!). The cost in terms of storage of each solver is also documented:; https://jso.dev/Krylov.jl/dev/storage/,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3803#issuecomment-2387566461:153,recover,recover,153,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3803#issuecomment-2387566461,1,['recover'],['recover']
Safety,"@glwagner @ali-ramadhan and @jm-c sounds good. . Some thoughts. 1. where would increased vertical diffusion for static instability show up? Is that a new closure or some attribute of the VerticalDiffusion closure? . 2. in many cases a closure could be some blend of several approaches and may want to allow some experimentation. e.g. a Redi like isopycnal coefficient, a biharmonic coefficient, some slope dependent bolus term etc... Do we need to think about that at this stage - maybe have a way several closures can be applied. 3. in a case like GM (for example), its more a 3d closure in practice. It has pieces in the isopycnal direction and pieces in the diapycnal direction. The terms tend to be evaluated together, so that numerically the discrete formulation has the right properties (conservation, no problematic null spaces, avoiding large v small term truncations, consistent empirical thresholds to deal with small number/zero divisions, consistent handling of domain and immersed boundary rules etc...) .",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/587#issuecomment-782877943:836,avoid,avoiding,836,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/587#issuecomment-782877943,1,['avoid'],['avoiding']
Safety,"@glwagner As you mentioned, I attempted to follow your suggestion by specifying the ```CUDA.@allowscalar```. However, I still encountered the same error on a different line. Code:; ```; function rescale!(model, energy; target_kinetic_energy = 1e-6); compute!(energy); rescale_factor = CUDA.@allowscalar √(target_kinetic_energy / energy[1, 1, 1]) ; #rescale_factor = √(target_kinetic_energy / energy[1, 1, 1]). for f in merge(model.velocities, model.tracers); f .*= rescale_factor; end. return nothing; end; ```; Error:; ```; Scalar indexing is disallowed.; Invocation of getindex resulted in scalar indexing of a GPU array.; This is typically caused by calling an iterating implementation of a method.; Such implementations *do not* execute on the GPU, but very slowly on the CPU,; and therefore should be avoided. If you want to allow scalar iteration, use `allowscalar` or `@allowscalar`; to enable scalar iteration globally or for the operations in question. Stacktrace:; [1] error(s::String); @ Base .\error.jl:35; [2] errorscalar(op::String); @ GPUArraysCore C:\Users\ADMIN\.julia\packages\GPUArraysCore\GMsgk\src\GPUArraysCore.jl:155; [3] _assertscalar(op::String, behavior::GPUArraysCore.ScalarIndexing); @ GPUArraysCore C:\Users\ADMIN\.julia\packages\GPUArraysCore\GMsgk\src\GPUArraysCore.jl:128; [4] assertscalar(op::String); @ GPUArraysCore C:\Users\ADMIN\.julia\packages\GPUArraysCore\GMsgk\src\GPUArraysCore.jl:116; [5] getindex(A::CuArray{Float64, 3, CUDA.Mem.DeviceBuffer}, I::Int64); @ GPUArrays C:\Users\ADMIN\.julia\packages\GPUArrays\Hd5Sk\src\host\indexing.jl:48; [6] scalar_getindex; @ C:\Users\ADMIN\.julia\packages\GPUArrays\Hd5Sk\src\host\indexing.jl:34 [inlined]; [7] _getindex; @ C:\Users\ADMIN\.julia\packages\GPUArrays\Hd5Sk\src\host\indexing.jl:17 [inlined]; [8] getindex; @ C:\Users\ADMIN\.julia\packages\GPUArrays\Hd5Sk\src\host\indexing.jl:15 [inlined]; [9] getindex; @ .\subarray.jl:290 [inlined]; [10] macro expansion; @ .\multidimensional.jl:917 [inlined]; [11] macro",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3522#issuecomment-2022273164:806,avoid,avoided,806,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3522#issuecomment-2022273164,1,['avoid'],['avoided']
Safety,"@glwagner I am attempting to run the Kelvin-Helmholtz instability example on a GPU, but the model fails and throws errors. Can someone help me to sort out this error? Please find the attached error below: . ```julia; using Random, Statistics. mean_perturbation_kinetic_energy = Field(Average(1/2 * (u^2 + w^2))); noise(x, z) = randn(); set!(model, u=noise, w=noise, b=noise); rescale!(simulation.model, mean_perturbation_kinetic_energy, target_kinetic_energy=1e-6); growth_rates, power_method_data = estimate_growth_rate(simulation, mean_perturbation_kinetic_energy, perturbation_vorticity, b). @info ""Power iterations converged! Estimated growth rate: $(growth_rates[end])""; ```. ```; Error: Scalar indexing is disallowed.; Invocation of getindex resulted in scalar indexing of a GPU array.; This is typically caused by calling an iterating implementation of a method.; Such implementations *do not* execute on the GPU, but very slowly on the CPU,; and therefore should be avoided. If you want to allow scalar iteration, use `allowscalar` or `@allowscalar`; to enable scalar iteration globally or for the operations in question.; ```",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3522:974,avoid,avoided,974,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3522,1,['avoid'],['avoided']
Safety,"@glwagner I am trying to run the Kelvin-Helmholtz instability example on a GPU; however, the model has failed, throwing some errors. Can someone help me to sort this error. Please find the attached error below: . using Random, Statistics. mean_perturbation_kinetic_energy = Field(Average(1/2 * (u^2 + w^2))); noise(x, z) = randn(); set!(model, u=noise, w=noise, b=noise); rescale!(simulation.model, mean_perturbation_kinetic_energy, target_kinetic_energy=1e-6); growth_rates, power_method_data = estimate_growth_rate(simulation, mean_perturbation_kinetic_energy, perturbation_vorticity, b). @info ""Power iterations converged! Estimated growth rate: $(growth_rates[end])"". Error: Scalar indexing is disallowed.; Invocation of getindex resulted in scalar indexing of a GPU array.; This is typically caused by calling an iterating implementation of a method.; Such implementations *do not* execute on the GPU, but very slowly on the CPU,; and therefore should be avoided. If you want to allow scalar iteration, use `allowscalar` or `@allowscalar`; to enable scalar iteration globally or for the operations in question.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1863#issuecomment-2016739183:960,avoid,avoided,960,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1863#issuecomment-2016739183,1,['avoid'],['avoided']
Safety,"@glwagner I have opened a PR with the branch sb/cut-cells-experiment, on which I have been working on implementing the cut cells. To summarize the progress:. - The two-dimensional [tracer advection test case](https://github.com/CliMA/Oceananigans.jl/blob/main/validation/immersed_boundaries/tracer_advection_over_bump.jl) over a Gaussian bump is run successfully using the low resolution from the [Adcroft](https://journals.ametsoc.org/view/journals/mwre/125/9/1520-0493_1997_125_2293_rotbsc_2.0.co_2.xml) paper and a relatively higher resolution. Both partial cells and full cells are used. Time evolution of the passive tracer concentration is visualized. - CutCellBottom is implemented in the ImmersedBoundary module similar to PartialCellBottom, and the tracer advection test case is repeated with cut cells. No noticeable difference in the results is detected so far (with the simulation using partial cells). So, tasks 1 (a) and 2 (b) have been completed so far. Tasks 2(b) and 2(c) are being worked on right now, after completion of which tasks 1 (b) and 3 will be initiated.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3123#issuecomment-1592059402:856,detect,detected,856,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3123#issuecomment-1592059402,1,['detect'],['detected']
Safety,"@glwagner I've been thinking about the WENO5 algorithm based on our discussion in https://github.com/CliMA/Oceananigans.jl/discussions/2054, and I just wanted to ask a quick question to make sure I understand the problem in this issue. If I understand correctly, the WENO5 scheme calculates the advection with different advection schemes (all of which are correct regardless of the grid) and then does a weighted sum of these solutions according so some criterion. So the issue with WENO5 and StretchedGrids is not that the advection calculated is ""wrong"", it's just that the weighting of the solutions is done in a non-optimal manner, and therefore introduces errors that could be avoided, right?. Basically I'm asking the question: is it the case the shouldn't use WENO5 with stretched grids because the answer is wrong, period? Or we can use it, it's just not gonna be as accurate as it potentially could be?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1704#issuecomment-967303636:682,avoid,avoided,682,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1704#issuecomment-967303636,1,['avoid'],['avoided']
Safety,@glwagner The goal of shifting the system is to recover a positive definite matrix by adding \lambda to all eigenvalue of the problem.; The proposition of @Yixiao-Zhang is to do that by adding v_0 * v_0^T. `v_0 * v_0^T` is ONLY semi-positive definite so not all eigenvalues will be modified. But it should lead to a positive definite A + c * v_0 * v_0^T (rank-one updating) by only updating one eigenvalue (the 0 eigenvalue). I just don't understand why we will find the same solution because CG should return one solution among an infinity of solution (before the shift).,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3802#issuecomment-2415314678:48,recover,recover,48,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3802#issuecomment-2415314678,1,['recover'],['recover']
Safety,"@glwagner and @ali-ramadhan I took a quick look. ; I think you should go ahead and merge, but quick comments for future reference - . 1. there are a bunch of changes unrelated to the PR in the PR. They are probably there by accident, but the Git gods really don't like that. When done well a PR can be a very helpful record for show what needs to be changed to achieve ""X"". When the PR is polluted with random other stuff that valuable use is lost. Most git projects more diligent about avoiding polluted PRs, which can be useful. . 2. I think ultimately we want halos to just be a thing that does a slightly more general form of what is in ::PBC mode. Everything else is not in halo rules. This is where we ended up in MITgcm and is similar to what is in MPIStateArray in the DG work (I am fairly sure!). A distinction to maybe think about is a set of halo() functions that just do the stuff that does not appear in any of the equations. Anything in equations turns out to be stuff that people may want to tinker with in surprisingly interesting ways. For example someone ultimately might want to do a numerical experiment that has flux BC on some set of the one bit of a boundary, something else on another bit etc.... (people do do things like this). For better or worse having bc's better separated from halos may ultimately prove the right thing. For now I wouldn't worry about it though! . 3. Related to 2. I suspect that trying to express complicated things like hybrid bc's will eventually break reducing bc to a type. Types are good for simple things, but Type::ItsComplicated is often also needed eventually.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/371#issuecomment-528944237:487,avoid,avoiding,487,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/371#issuecomment-528944237,1,['avoid'],['avoiding']
Safety,"@glwagner this almost became stale but I think it's ready to review and possibly merge. It doesn't re-work the code like you suggest [here](https://github.com/CliMA/Oceananigans.jl/pull/2752/#issuecomment-1315578507), but it does make the functions more easily understandable on first pass by being more verbose. . It also changes the behavior of `viscosity()` and `diffusivity()`, which no longer sum over all individual closures by default when the closure is a tuple, avoiding user error (as you suggested [here](https://github.com/CliMA/Oceananigans.jl/pull/2752/#issuecomment-1273620776)). Finally it also changes the function `calc_κᶜᶜᶜ` ro `calc_nonlinear_κᶜᶜᶜ` in order to differentiate from `κᶜᶜᶜ`. Although I'm agnostic about the name and could change it to anything else. This PR used to also remove a fallback that was a bit problematic because it silently returned zero diffusivity values for closures like Smagorinsky, but it seems some other PR got around to that first before merging this one :)",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2752#issuecomment-1398498518:471,avoid,avoiding,471,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2752#issuecomment-1398498518,1,['avoid'],['avoiding']
Safety,"@glwagner, how can we avoid `collect`? E.g., at. https://github.com/CliMA/Oceananigans.jl/blob/d913858aa771f096e8ce48da132749361c8f1647/validation/elliptic_solvers/doubly_bounded_poisson.jl#L130. I tried `arch_array` but seems like we need to write some more methods for it?. ```julia; julia> r_array = arch_array(arch, reshape(interior(r), Nx * Ny * Nz)); ERROR: MethodError: no method matching arch_array(::CPU, ::Base.ReshapedArray{Float64, 1, SubArray{Float64, 3, Array{Float64, 3}, Tuple{UnitRange{Int64}, UnitRange{Int64}, UnitRange{Int64}}, false}, Tuple{Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64}, Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64}}}); Closest candidates are:; arch_array(::CPU, ::Array) at /Users/navid/Research/OC.jl/src/Architectures.jl:102; arch_array(::CPU, ::CUDA.CuArray) at /Users/navid/Research/OC.jl/src/Architectures.jl:103; arch_array(::Any, ::AbstractRange) at /Users/navid/Research/OC.jl/src/Architectures.jl:107; ...; Stacktrace:; [1] top-level scope; @ REPL[6]:1; [2] top-level scope; @ ~/.julia/packages/CUDA/fAEDi/src/initialization.jl:52. julia> r_array = arch_array(arch, interior(r)); ERROR: MethodError: no method matching arch_array(::CPU, ::SubArray{Float64, 3, Array{Float64, 3}, Tuple{UnitRange{Int64}, UnitRange{Int64}, UnitRange{Int64}}, false}); Closest candidates are:; arch_array(::CPU, ::Array) at /Users/navid/Research/OC.jl/src/Architectures.jl:102; arch_array(::CPU, ::CUDA.CuArray) at /Users/navid/Research/OC.jl/src/Architectures.jl:103; arch_array(::Any, ::AbstractRange) at /Users/navid/Research/OC.jl/src/Architectures.jl:107; ...; Stacktrace:; [1] top-level scope; @ REPL[7]:1; [2] top-level scope; @ ~/.julia/packages/CUDA/fAEDi/src/initialization.jl:52; ```",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2396#issuecomment-1162602827:22,avoid,avoid,22,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2396#issuecomment-1162602827,1,['avoid'],['avoid']
Safety,"@glwagner, there isn't much code in it. So, it is safe to close this PR and delete the associated branch. I will proceed with that. @navid, after some digging, I discovered that my default git pull command was set to git rebase instead of git merge following git fetch. This altered the commit history to linearize it, causing confusion in both my previous tracer advection branch and the one associated with this PR. I have now reset the default setting. My apologies for the oversight.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3222#issuecomment-1724393955:50,safe,safe,50,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3222#issuecomment-1724393955,1,['safe'],['safe']
Safety,"@ilyascfd welcome and thanks for opening this issue, it's very helpful!. The problem here is that `Value` is not exported by default when we write `using Oceananigans` anymore. As a result, these lines:. https://github.com/CliMA/Oceananigans.jl/blob/ff19b7e0d328557dc198eb23349db5eed0680c65/validation/stratified_couette_flow/stratified_couette_flow.jl#L103-L110. do not work. . The recommended syntax nowadays is, for example,. ```; bc = ValueBoundaryCondition(0); ```. rather than `BoundaryCondition(Value, 0)`. This change was made to avoid name conflicts with some common names in the Julia ecosystem (like `Flux`). But, it looks like the validation script was not updated when this change was made to our exported names. I opened a PR to fix it in #1982. You can fix the script yourself as well by replacing `BoundaryCondition(Value, ` with `ValueBoundaryCondition(`. PS @ilyascfd here are a few tips for writing issues that will help us solve your problem as fast as possible; * Include links to the lines that are failing so we don't have to look for them ourselves. Github is really handy for this!; * When including code in an issue, please format it with triple backticks (```). Note: we run this script during CI, so at first I was surprised that we hadn't caught this yet. But when we run the tests, we do indeed import `Oceananigans.BoundaryConditions.Value` via. https://github.com/CliMA/Oceananigans.jl/blob/ff19b7e0d328557dc198eb23349db5eed0680c65/test/runtests.jl#L20. and. https://github.com/CliMA/Oceananigans.jl/blob/ff19b7e0d328557dc198eb23349db5eed0680c65/src/BoundaryConditions/BoundaryConditions.jl#L3-L4",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1981#issuecomment-918194547:538,avoid,avoid,538,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1981#issuecomment-918194547,1,['avoid'],['avoid']
Safety,@jagoosw I would say avoid the flat `LatitudeLongitudeGrid` in the testing (use periodic directions instead) in this PR,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3179#issuecomment-1630792224:21,avoid,avoid,21,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3179#issuecomment-1630792224,1,['avoid'],['avoid']
Safety,"@maleadt, I tried all combinations I could think of but I couldn't get `Codecov.submit()` to work using the `SECRET_CODECOV_TOKEN`. It was only when I added the actual token as a kwarg that worked, e.g.,; https://github.com/CliMA/Oceananigans.jl/blob/ed1fe321199fd08c28052cd23a8c1b9d68368345/.buildkite/pipeline.yml#L540. Any ideas how to avoid using the token per se?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2329#issuecomment-1063818500:339,avoid,avoid,339,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2329#issuecomment-1063818500,1,['avoid'],['avoid']
Safety,"@matinraayai I think it will also help us get those tests passing quicker, because keeping `GPU` will allow us to avoid refactoring a lot of existing code.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2493#issuecomment-1112369281:114,avoid,avoid,114,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2493#issuecomment-1112369281,1,['avoid'],['avoid']
Safety,"@mukund-gupta discovered a bug (see #1104) that prevents the use of function diffusivities with the `TimeStepWizard`. We don't want the wizard to be so all-powerful that it samples a diffusivity function over the whole grid just to compute it's maximum value. So, this PR does the simple thing and avoids limiting the time-step by the diffusivity when it's a function. A better solution would require users to explicitly ask their time-step to be limited by diffusivities, and for that step to fail when the diffusivity is a function (eg #1087). It could make sense to add a test for all possible combinations of diffusivities and wizards, but it might be better to wait for a more comprehensive PR that addresses #1087... Resolves #1104",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1105:298,avoid,avoids,298,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1105,1,['avoid'],['avoids']
Safety,"@navidcy Hmmm, digging into Documenter.jl's Buildkite config it looks like push previews are only provided if the build belongs to a PR?. https://github.com/JuliaDocs/Documenter.jl/blob/4be8243d31a6ffdbc64d779e3f8c2fb7c61de075/src/deployconfig.jl#L719-L725 . Maybe there was no push preview here since @tomchor pushed this branch at which Buildkite ran, before the PR existed, so Buildkite did not publish a preview. Maybe if a second commit was made on this PR, it would be detected by Buildkite as a PR and therefore a preview would be published?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1383#issuecomment-782289428:475,detect,detected,475,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1383#issuecomment-782289428,1,['detect'],['detected']
Safety,"@navidcy Thanks for the feedback. I added a docstring for `aligned_time_step` that explains its purpose:. ```; aligned_time_step(sim). Returns a time step Δt that is aligned with the output writer schedules and stop time of the simulation `sim`.; The purpose of aligning the time step is to ensure simulations do not time step beyond the `sim.stop_time` and; to ensure that output is written at the exact time specified by the output writer schedules.; ```. ---. > 1. Why does `aligned_time_step(sim)` return a negative time-step? Is this a bug? ; > 2. Why is this fix temporary? What would a more permanent fix look like?; > ; > It seems that aligning the time-step is causing a host of issues. Is this the real problem? Should we stop aligning time-steps until we are sure that we can do it safely?. @glwagner Sorry I thought I explained the issue in #1280 but looks like I didn't. The problem has to do with checkpointing and pickup when output writers are using `TimeInterval` schedules. Output writer schedules are not checkpointed so when you pickup a simulations that includes output writers with `TimeInterval` schedules, the `model.clock` is restored but the `schedule.previous_actuation_time` is 0 for all `TimeInterval` schedules. So `aligned_time_step` thinks it needs to take a negative time step as it needs to write output at t = `schedule.time_interval`. The permanent fix is to checkpoint and pickup schedules (properly fixing #1280) which would prevent `aligned_time_step` from returning negative time steps. I see this as more of a checkpointing issue than a time step alignment issue. In general we are not extensively testing checkpointing.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1303#issuecomment-766101050:793,safe,safely,793,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1303#issuecomment-766101050,1,['safe'],['safely']
Safety,"@peterahrens thanks for that tip!. @vchuravy, by closure-specific temporary variables (or fields), I am referring to domain-size quantities that can be re-used in computing the contribution of an LES closure to the 'source terms' for momentum and tracers. In other words, [this line, which computes the source term for `u`](https://github.com/ali-ramadhan/Oceananigans.jl/blob/a544300cabe6f8d872b7f9658284cad34c8619ad/src/time_steppers.jl#L223) could become something like. ```julia; kernel_temporaries!(subgrid_closure, ..., i, j, k); Gu[i, j, k] = ... + subgrid_closure.u(subgrid_closure_temporaries, ..., i, j, k); ```. ... I think. In physical terms, the intermediate variable is an 'eddy viscosity' that acts on all momentum terms. But I guess when I say the computation 'will benefit' from variables to store intermediate results, what I really mean is that I'm expecting the calculation of this temporary variable to be fairly involved (potentially >20 derivatives of velocity and tracer fields in x, y, z, plus scalar operations to combine the fields, and a 'clipping' function to make sure the viscosity is not negative --- see [the formula for the eddy viscosity predictor](https://dedales.readthedocs.io/en/latest/closures/anisotropic_minimum_dissipation.html#the-eddy-viscosity-predictor)). It is also of course possible to simply write a function that calculates the eddy viscosity, and call this function repeatedly to avoid temporaries altogether.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/73#issuecomment-468747674:1173,predict,predictor,1173,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/73#issuecomment-468747674,3,"['avoid', 'predict']","['avoid', 'predictor']"
Safety,"@simonbyrne do you have any advice for how to avoid these annoying round-off errors? Basically I'm wondering if theres a robust way to get. ```; ├── Periodic x ∈ [0.0, 6.28319) regularly spaced with Δx=0.0490874; ```. here we are given the length of the domain (2pi), but then we have to construct a range of nodes which extends _left_ of 0, so that it includes our halo region. The problem is when we do that, the first face (which is then something like `range[4]` if we have 3 halo points) has round-off error (or at least that's how I'm interpreting it... maybe there's no way around it).",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2499#issuecomment-1113423017:46,avoid,avoid,46,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2499#issuecomment-1113423017,1,['avoid'],['avoid']
Safety,"@simone-silvestri I refactored the tuple halo filling (partly to shorten the code but also to filter more stuff from the tuples, and to avoid filling halos for the same fields twice --- sometimes a field can appear twice in a named tuple...) But it looks like my changes broke some stuff, so I might need some help fixing that up.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2426#issuecomment-1094117367:136,avoid,avoid,136,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2426#issuecomment-1094117367,1,['avoid'],['avoid']
Safety,@tomchor I think it should automatically detect dimensions for `WindowedSpatialAverage` now but the solution is not pretty since `NetCDFOutputWriter` plays nicest with fields. Could probably be made clearer though.,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1397#issuecomment-783762674:41,detect,detect,41,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1397#issuecomment-783762674,1,['detect'],['detect']
Safety,"@tomchor does this result occur with just one OutputWriter, or do you have more than one OutputWriter being used at the same time?. Averaging collection is finalized here:. https://github.com/CliMA/Oceananigans.jl/blob/81db22f4a26396142e8cd5b5a4c50c75790c1d50/src/OutputWriters/windowed_time_average.jl#L179-L187. when `run_diagnostic!(wta::WindowedTimeAverage, model)` is called. But the criterion that triggers output is. https://github.com/CliMA/Oceananigans.jl/blob/81db22f4a26396142e8cd5b5a4c50c75790c1d50/src/Utils/schedules.jl#L38-L52. I have to say I don't understand why we special case `time == schedule.previous_actuation_time + schedule.interval`, since `rem(time, schedule.interval)` should be 0 and the algorithm should still work. Or am I missing something? It does seem to assume that `0` is the initial time (we could avoid that maybe by saving the initial `model.clock.time`...)",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1274#issuecomment-747092535:835,avoid,avoid,835,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1274#issuecomment-747092535,1,['avoid'],['avoid']
Safety,"@views allocates because it creates a new view every time it runs. Not sure if that’s the issue. To avoid this we have to preallocate the view, somehow.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/685#issuecomment-595615905:100,avoid,avoid,100,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/685#issuecomment-595615905,1,['avoid'],['avoid']
Safety,A NaN checker diagnostic that aborts the simulation when NaN values are detected. We could have some shell script that look for this and sends an email or something. Julia could do this too but then we'd add a big dependency for a small diagnostic. Resolves #38,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/170:30,abort,aborts,30,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/170,2,"['abort', 'detect']","['aborts', 'detected']"
Safety,"A consideration when picking up from a checkpoint and using `NetCDFOutputWriter` is that `mode=""a""` (append) needs to be used instead of `mode=""c""` (create or clobber) when creating the `NetCDFOutputWriter`. This functionality works and is tested, but currently needs to be set manually by the user. Not sure of the best way of making this easy for users without accidentally overwriting their data. I can think of three solutions:; 1. Not specifying a `mode` causes `mode=""c""` if the file does not exist and `mode=""a""` if the file does exist. I like this solution the most as it works well with and without a checkpointer (and users don't have to do anything to get reasonable default behavior).; 2. Add a `force` kwarg to `NetCDFOutputWriter` that is `false` by default. The `NetCDFOutputWriter` will error if you try to overwrite an existing file, allowing the user to go back and set `mode=""a""` without any data loss. A `pickup` kwarg could perform a similar function if it's `false` by default.; 3. Setting the `PICKUP` environment variable causes `mode=""a""` to be the default if the file already exists. But I think we should avoid using global environment variables to modify internal behavior.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1068#issuecomment-711035671:1132,avoid,avoid,1132,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1068#issuecomment-711035671,1,['avoid'],['avoid']
Safety,"A few comments:. 1. Let's call this `materialize_biogeochemistry`. We would like to change the names of the functions to ""materialize"" as well.; 2. This should be avoided unless strictly necessary. Generally, this step introduces complexity and fragility to model setup. One source of insidious bugs is when the user-build `biogeochemistry` is different from `model.biogeochemistry`. Can you provide an example of when this is necessary to motivate this feature, given its serious downsides?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3262#issuecomment-1714600321:163,avoid,avoided,163,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3262#issuecomment-1714600321,1,['avoid'],['avoided']
Safety,"A few thoughts while I'm debugging these test failures:. * I think at least some of the failures are occurring because we are still using `time_step!(model, dt)` for tests. Now we have to call `update_state!(model)` before a hand-written loop when `model.clock.iteration != 0`. * checking `model.clock.iteration == 0` does not catch some cases. In the regression tests, we manually restore the model state to a configuration (essentially manual checkpointing). However, the halos are not correct (after this PR is merged they will be correct in saved data. However we still could have incorrect halos if models are ""hack checkpointed"" with NetCDFOutputWriter or JLD2OutputWriter that don't include halos). For these cases, we need to call `update_state!` before running a simulation. It seems we probably want to call `update_state` as part of the initialization of `run!` whether or not `iteration == 0` to avoid this gotcha... * Calling `update_state!(model)` every time that `run!` is called can lead to excess computation. But I think the excess is very small even in cases where `run!` is ""misused"" to advance the simulation only 10-20 time-steps (because `update_state!` is already a minor part of the total cost of time-stepping --- I think).",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1083#issuecomment-712988897:908,avoid,avoid,908,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1083#issuecomment-712988897,1,['avoid'],['avoid']
Safety,"A long time ago, I did some scaling tests of the pure FFT algorithm. These were the results. <img width=""537"" alt=""Screenshot 2024-08-10 at 2 57 20 PM"" src=""https://github.com/user-attachments/assets/a60f2f50-13f3-4fc2-858f-8d5303ccd918"">. I will probably redo the scaling test later on after the summer. ; In general, always use slab partitioning if you can because you avoid one transposition. ; This might lead to larger halo to domain ratio, but the fill halo, contrarily to the transpose, is hidden so it should still be better to have a slab partitioning then a pencil one.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3689#issuecomment-2282246515:371,avoid,avoid,371,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3689#issuecomment-2282246515,1,['avoid'],['avoid']
Safety,A related issue but really outside our scope is that someone that has CUDA but doesn't have things configured correctly can have difficulty running Oceananigans --- even if they only ever planned on running CPU models. This issue was avoided when `has_cuda()` was based on whether `using CuArrays` failed or not.,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/840#issuecomment-670516629:234,avoid,avoided,234,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/840#issuecomment-670516629,1,['avoid'],['avoided']
Safety,Aborted (core dumped) on tutorial,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1281:0,Abort,Aborted,0,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1281,1,['Abort'],['Aborted']
Safety,Aborting a simulation with NaNs without stacktraces,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1734:0,Abort,Aborting,0,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1734,1,['Abort'],['Aborting']
Safety,Actually I think I did it wrong in #524. I'm supposed to delete leftover files after `makedocs` but before `deploydocs`. So I'll modify this PR to do that. I also feel that Documenter shouldn't even be adding jld2 files. Asking on Julia Slack to see if this is expected behavior and how to avoid it. EDIT: There is no `.gitignore` file on `gh-pages` so we just have to add one there with `*.jld2`.,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/558#issuecomment-563265164:290,avoid,avoid,290,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/558#issuecomment-563265164,1,['avoid'],['avoid']
Safety,"Actually `Array` and `CuArray` aren't `isbits`. But from what I understand, I think CUDAnative uses Adapt.jl to convert a `CuArray` to something like a `CuDeviceArray` which is `isbits` when passing arguments to CUDA kernels, which is what we would have to do to convert our `Field` structs to be `isbits`. But yeah, definitely agree about avoiding abstract and unspecified types. Comes with learning Julia I guess.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/19#issuecomment-477678970:340,avoid,avoiding,340,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/19#issuecomment-477678970,1,['avoid'],['avoiding']
Safety,"Actually, I think those docs are misleading. The straightforward way to arrive at the implicit step equation is to first discretize in space, and then after that derive the fractional step method. The key steps are:. 1. Discretize the continuity equation, and then sum in the vertical to obtain the finite-volume discretized free surface equation in terms of the vertically integrated transports; 2. Introduce the predictor velocities in the discretized momentum equation, and then sum the discretized momentum equation in the vertical.; 3. Combine the discretized momentum equation with the free surface equation by taking the divergence of the vertically-summed momentum equation. Steps 2 introduces the vertical sum of the discretized barotropic pressure gradient into the implicit step equation, which is how we see that these two contributions in fact come from the same place.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2475#issuecomment-1109690145:414,predict,predictor,414,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2475#issuecomment-1109690145,1,['predict'],['predictor']
Safety,"After discussion in #3177 I noticed that the warning only warns for Julia v1.7 and earlier... But it should warn for Julia v1.8 as well. (Although, if we decide to add a compat entry for Julia v1.9 as a response to #3184 then the warning is rendered redundant.)",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3182:250,redund,redundant,250,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3182,1,['redund'],['redundant']
Safety,"Agree this is redundant and if we aren't checking in the grid constructor, we should.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2595#issuecomment-1144794360:14,redund,redundant,14,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2595#issuecomment-1144794360,1,['redund'],['redundant']
Safety,"Agreed. Useful and related comment. > I was thinking of doing some prototyping and benchmarking in a sandbox by building off the example in my PR [vchuravy/GPUifyLoops.jl#18](https://github.com/vchuravy/GPUifyLoops.jl/pull/18).; > ; > The PR contains an example that can be extended to rely on a `Grid` struct, multiple `FaceField`s and ` CellField`. So I'll prototype grids and fields that are `isbitstype` (you already helped by doing this for a grid in [#59 (comment)](https://github.com/climate-machine/Oceananigans.jl/issues/59#issuecomment-467660181)) and test to see if they work on the GPU with GPUifyLoops.jl. If they do work and performance isn't degraded then I'll rewrite the operators to use grid and field structs.; > ; > You probably know how to do this better than me, but might be good if I rewrite the operators as they's still undocumented and do some _slightly convoluted_ stuff to avoid having to store intermediate calculations.; > ; > Right now I'm focusing on system tests and benchmarks but once @christophernhill @jm-c and I get closer to implementing the variable _Δz_ grid #47 I will work on this.; >; >_Originally posted by @ali-ramadhan in https://github.com/climate-machine/Oceananigans.jl/issues/115#issuecomment-470782067_",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/59#issuecomment-470297910:902,avoid,avoid,902,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/59#issuecomment-470297910,1,['avoid'],['avoid']
Safety,"Ah I see, sorry for misunderstanding. I guess I saw three separate issues:; 1. Having to always index `f.data` instead of `f` (#13).; 2. Propagating `@inbounds` (#58) and using `@inbounds` safely (#164).; 3. GPU-compatible `Field` abstractions (#163). Perhaps all three could be addressed in a single PR (but they could be addressed separately).",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/164#issuecomment-479858415:189,safe,safely,189,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/164#issuecomment-479858415,1,['safe'],['safely']
Safety,Ah I think Buildkite tests did not run on this PR since we disable Buildkite on PRs from forks (to avoid random PRs executing potentially malicious code on our machines). So we usually open and merge branches from the repo itself. You should be able to do this since you're a repo collaborator.,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1346#issuecomment-775428627:99,avoid,avoid,99,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1346#issuecomment-775428627,1,['avoid'],['avoid']
Safety,"Ah I think you forgot to squash and merge and instead created a merge commit so the image file entered git history, e.g. it's now in the `master` branch: https://github.com/CliMA/Oceananigans.jl/commit/0d10840b92e67c60c18102d5183affff993f896a. It's not a big deal since this particular file only increases the repo size by 64 KiB but in general I think we should avoid merging binary files like images into the repo since the increase in repo size is cumulative.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1650#issuecomment-839882879:363,avoid,avoid,363,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1650#issuecomment-839882879,1,['avoid'],['avoid']
Safety,Ah is it worth doing something with #3740 then so the model doesn't error with `free_surface = nothing` or would that just be misleading?. I can also update the PR to get rid of `rigid_lid.jl` to avoid confusing users into thinking a rigid lid mode exists.,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3735#issuecomment-2313395254:196,avoid,avoid,196,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3735#issuecomment-2313395254,1,['avoid'],['avoid']
Safety,"Ah ok so I think this is just an issue with running `makedocs` multiple times after each other without cleaning up what `makedocs` has done before. So it's complaining that when it ran the `jldoctest` a second time it got this message:. ```; more_fields.nc already exists but no NetCDFOutputWriter mode was explicitly specified. Will default to mode = ""a"" to append to existing file. You might experience errors when writing output if the existing file belonged to a different simulation!; ```. so the output is different. I guess we have 3 possible automated fixes (manual fix being to remove the files manually):; 1. We can clean up and remove these files as part of `make.jl`.; 2. Using `NetCDFOutputWriter` in `jldoctest` we should use the `mode = ""c""` kwarg to avoid the info/warning message.; 3. `jldoctest` blocks should remove files at the end of the block (and there should be a way to hide this from the docs but not 100% sure right now).",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1455#issuecomment-796853994:766,avoid,avoid,766,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1455#issuecomment-796853994,1,['avoid'],['avoid']
Safety,Ah once the PR is merged you just do it via a comment (we comment on the latest commit): https://github.com/CliMA/Oceananigans.jl/commit/47caa85138047acb4584cf97092c62c3e59b3543#comments. You're a repo collaborator so you should be able to tag new releases. Under ColPrac people should merge their own PRs to avoid surprises but I'll take your comment as permission to merge and tag a new release :P,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1367#issuecomment-780201946:309,avoid,avoid,309,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1367#issuecomment-780201946,1,['avoid'],['avoid']
Safety,"Ah sorry about that @raphaelouillon! Working from master can be kinda risky and we still need to update the docs... Not sure why `GaussianMask{:x}(grid.Lx, grid.Lx/10)` would error. Probably deserves its own issue and tests. Can you try `GaussianMask{:x}(center=grid.Lx, width=grid.Lx/10)` instead? Seems to work for me on Julia 1.4.2. ```julia; julia> using Oceananigans, Oceananigans.Forcing; julia> grid= RegularCartesianGrid(size=(64, 64, 64), extent=(1, 1, 1)); julia> u_forcing = Relaxation(; rate=1/60, mask=GaussianMask{:x}(center=grid.Lx, width=grid.Lx/10), target=0.1); SimpleForcing{Oceananigans.Grids.Cell,Oceananigans.Grids.Cell,Oceananigans.Grids.Cell,true,Nothing,:tracer,:tracers,Oceananigans.Forcing.RelaxingFunction{Float64,GaussianMask{:x,Float64},Float64}}(Oceananigans.Forcing.RelaxingFunction{Float64,GaussianMask{:x,Float64},Float64}(0.016666666666666666, GaussianMask{:x,Float64}(1.0, 0.1), 0.1), nothing, true); ```",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/789#issuecomment-657651146:70,risk,risky,70,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/789#issuecomment-657651146,1,['risk'],['risky']
Safety,"Ah sorry about that! Happy to chat over Slack but don't wanna butcher @glwagner's suggestion. I guess having the `X, Y, Z` is important for `NetCDFOutputWriter` since it uses that information to know which dimensions to assign to each variable. They could be specified manually via the `dimensions` kwarg (there should be an example of this in the docstring) but ideally the dimensions should be detected automatically for an Oceananigans field. Maybe you just need to use `reduced_location` to determine the `X, Y, Z` for the `SpatialWindowAverage`? https://github.com/CliMA/Oceananigans.jl/blob/0ff8cd8e7d4565c8493ec7b81e531599277ab645/src/Fields/reduced_field.jl#L137. Maybe something like this could work?. ```julia; function SpatialWindowAverage(field; dims, field_slicer=FieldSlicer()); X, Y, Z = reduced_location(location(field), dims=dims); return SpatialWindowAverage{X, Y, Z}(field.data, field.grid, field, field_slicer, dims); end; ```",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1392#issuecomment-783553934:396,detect,detected,396,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1392#issuecomment-783553934,1,['detect'],['detected']
Safety,"Ah yeah I agree that cleaning up the `Operators` module is a big and largely thankless task. Something that should be done soon though. But for `ShallowWaterModel` I don't think anything in `Operators` needs to be touched. What needs to change is where `ShallowWaterModel` uses those grid spacing operators. In other words, on a regular grid one has two options for computing grid spacing. One can either compute grid spacing by calling the function `Δx`, or by calling the function `Δxᶜᶜᵃ`. These produce the _same_ result. In order to completely clean up `Operators`, we need to change all of the ; _model code_ so that nobody uses the operator `Δx`. Once nobody uses `Δx`, that is the point at which we can safely delete it from `Operators`. If you can find where in `ShallowWaterModel` the operator `Δx` is used, and replace this with something like `Δxᶜᶜᵃ` (or the appropriate spacing operator for the correct location, thus ensuring that `ShallowWaterModel` will work on curvilinear grids as well), we have made good progress.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1663#issuecomment-848011218:710,safe,safely,710,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1663#issuecomment-848011218,1,['safe'],['safely']
Safety,"Ah, I see. Sounds like it wouldn't be trivial to add that support. I guess a workaround to avoid partially-averaged results when picking up would be to set the `Checkpointer` to only write checkpoints when the `TimeAveraged` results are also written. I'm not sure what that would do to other (more frequent) outputs though, since it'd potentially try to write some time steps twice (and not in monotonic ordering)...",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3485#issuecomment-1969259005:91,avoid,avoid,91,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3485#issuecomment-1969259005,1,['avoid'],['avoid']
Safety,Ah. Maybe we can avoid instantiation then.,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/698#issuecomment-599734737:17,avoid,avoid,17,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/698#issuecomment-599734737,1,['avoid'],['avoid']
Safety,"Ahh hmm... I think you can get more diagnostics to work this way. The reason complex diagnostics fail, as discussed on #1241, is because of the recursive calls to `identity` (for example). We have to do this with `AbstractOperations` because that framework is written so that users don't have to ""know"" where their computations occur (or end up) --- at least not necessarily. In the above code, the user (you) is manually providing location information by selecting `∂zᵃᵃᶜ`; therefore we don't have to call an interpolation function (such as `identity`). This can be seen in the code in the crucial `getindex` function defined for all `AbstractOperations`, such as:. https://github.com/CliMA/Oceananigans.jl/blob/f6d65f8bd0f97c4e97d9e81b126c78c3e0d463c8/src/AbstractOperations/derivatives.jl#L25. I believe in this case that the function `d.▶` from the above is `identity`, which could fail because it's also called in the calculation of the binary operation `w * v`?. I think we could avoid some calls to `identity` in `AbstractOperations` if we restrict their generality. But I'm not sure that's the right philosophy. Maybe we just want to fix the compilation issues instead and preserve full generality in `AbstractOperations`.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1401#issuecomment-786817960:986,avoid,avoid,986,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1401#issuecomment-786817960,1,['avoid'],['avoid']
Safety,"Ahh, very nice ideas. A convenience constructor for a collection of writers is a much better idea than the ""global variable""-y solution I proposed regarding `Simulation`; we just have to invent a file naming convention. We actually don't need separate `Averaged*` constructors if we extend the philosophy of time-averaging and add a kwarg like `spatially_averaged_dims` (maybe there's a better name, but we have to avoid confusion with time-averaging) that transforms fields into `AveragedField`s, eg. ```julia; simulations.output_writers[:horizontal_averages] =; NetCDFOutputWriter(model, output_fields, spatially_averaged_dims=(1, 2), ...). simulations.output_writers[:volume_averages] =; NetCDFOutputWriter(model, output_fields, spatially_averaged_dims=(1, 2, 3), ...); ```. the default `nothing` doesn't transform anything.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1171#issuecomment-726748981:415,avoid,avoid,415,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1171#issuecomment-726748981,1,['avoid'],['avoid']
Safety,AllSchedule for combining scheduling criteria and avoiding checkpointing with NaNs,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2088:50,avoid,avoiding,50,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2088,1,['avoid'],['avoiding']
Safety,"Also, I noticed there was an error at the very beginning, copied below. ```; TEST_GROUP=shallow_water julia --project -e 'using Pkg; Pkg.test()'; Testing Oceananigans; ┌ Error: Pkg.Resolve.ResolverError(""Unsatisfiable requirements detected for package JLLWrappers [692b3bcd]:\n JLLWrappers [692b3bcd] log:\n ├─possible versions are: [1.0.0-1.0.2, 1.1.0-1.1.4, 1.2.0] or uninstalled\n ├─restricted to versions 1.1.3 by an explicit requirement, leaving only versions 1.1.3\n └─restricted by compatibility requirements with IntelOpenMP_jll [1d5cc7b8] to versions: 1.2.0 — no versions left\n └─IntelOpenMP_jll [1d5cc7b8] log:\n ├─possible versions are: 2018.0.3 or uninstalled\n └─restricted to versions 2018.0.3 by an explicit requirement, leaving only versions 2018.0.3"", nothing); ```",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1326#issuecomment-770853975:231,detect,detected,231,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1326#issuecomment-770853975,1,['detect'],['detected']
Safety,"Also, I tried the following that mostly worked:; ```; using Oceananigans.Grids; using Oceananigans.Models; using Oceananigans.ImmersedBoundaries: ImmersedBoundaryGrid, GridFittedBoundary. grid = RegularRectilinearGrid(size=(256, 256), x=(-10, 10), y=(0, 5), topology=(Periodic, Bounded, Flat)). # Gaussian bump of width ""1""; bump(x, y, z) = y < exp(-x^2). grid_with_bump = ImmersedBoundaryGrid(grid, GridFittedBoundary(bump)); model = ShallowWaterModel(grid=grid_with_bump, gravitational_acceleration=1); ```. and it worked until the last line, which gave an error I copied below. I presume we should fix this before we think about doing any merging. Any suggestions @glwagner ?. ```; Warning: Inflating model grid halo size to (3, 3, 0) and recreating grid. The model grid will be different from the input grid. To avoid this warning, pass halo=(3, 3, 0) when constructing the grid.; └ @ Oceananigans.Grids ~/software/Oceananigans.jl/src/Grids/automatic_halo_sizing.jl:41; ERROR: MethodError: no method matching with_halo(::Tuple{Int64, Int64, Int64}, ::ImmersedBoundaryGrid{Float64, Periodic, Bounded, Flat, RegularRectilinearGrid{Float64, Periodic, Bounded, Flat, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}}}}, GridFittedBoundary{typeof(bump)}}); Closest candidates are:; with_halo(::Any, ::RegularRectilinearGrid) at /home/fpoulin/software/Oceananigans.jl/src/Grids/regular_rectilinear_grid.jl:218; with_halo(::Any, ::VerticallyStretchedRectilinearGrid) at /home/fpoulin/software/Oceananigans.jl/src/Grids/vertically_stretched_rectilinear_grid.jl:242; Stacktrace:; [1] ShallowWaterModel(; grid::ImmersedBoundaryGrid{Float64, Periodic, Bounded, Flat, RegularRectilinearGrid{Float64, Periodic, Bounded, Flat, OffsetArrays.OffsetVector{Float64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}}}}, GridFittedBoundary{typeof(bump)}}, gravitational_acceleration::Int64, architecture::Oceananigans.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1663#issuecomment-844218915:816,avoid,avoid,816,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1663#issuecomment-844218915,1,['avoid'],['avoid']
Safety,Also:; * Print available `CuDevice`s at startup; * Throw `ArgumentError` when attempting to construct a `Model{GPU}` when no CUDA-enabled GPU is detected. Resolves #82,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/148:145,detect,detected,145,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/148,1,['detect'],['detected']
Safety,"An easier method might be to use a streamfunction (a field at Face, Face, Center) and use built in operators and broadcasting to calculate the velocity field and buoyancy in terms of this steam function. You avoid bringing in another package with this method and I think the result is closer to the discrete finite volume balance condition.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1826#issuecomment-874985362:208,avoid,avoid,208,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1826#issuecomment-874985362,1,['avoid'],['avoid']
Safety,Another idea: check that the grid architecture and model architecture match to avoid issues like the test failure in #1467.,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1453#issuecomment-799734876:79,avoid,avoid,79,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1453#issuecomment-799734876,1,['avoid'],['avoid']
Safety,"Apparently the new syntax does help avoid `@allowscalar` instances, and things do compile locally for me, but the errors on buildkite are still [there](https://buildkite.com/clima/oceananigans/builds/9727#01860b25-0d9a-419e-bd01-3ec11bcb8c6b/38-603):. ```; Computations with Averaged Fields [GPU, RectilinearGrid]: Test Failed at /net/ocean/home/data44/data5/glwagner/.buildkite-agent/builds/sverdrup-13/clima/oceananigans/test/test_computed_field.jl:583; --;   | Expression: all(interior(tke_yz) .== 9 / 2); ```. Any ideas on what might be the cause of the differences between builkite and my local server? If someone could also run one of the failing tests on a GPU locally and see if they get the same errors that buildkite is throwing, that would be helpful.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2865#issuecomment-1414131063:36,avoid,avoid,36,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2865#issuecomment-1414131063,1,['avoid'],['avoid']
Safety,"As @navidcy hints I think the difficulties here are using `Plots.jl` on a headless system. To address this issue @navidcy is suggesting that you execute / write `ENV[""GKSwstype""] = ""100""` at the top of the tutorial. Alternatively, you can avoid remote plotting and attempt to run the `@animate` block locally, after downloading the data. I do think it is convenient to plot remotely however, so it'd be nice if you can make that work for you. Searching ""headless plotting julia Plots"" might turn up more information (I didn't find anything immediately, but I've seen useful threads before and it'd be nice to paste them here).",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1281#issuecomment-752068938:239,avoid,avoid,239,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1281#issuecomment-752068938,1,['avoid'],['avoid']
Safety,"As discussed in https://github.com/CliMA/Oceananigans.jl/discussions/3177#discussioncomment-6448740 I see two solutions: either drop support for v1.8 or prior with a v1.9 compat entry, or add something like; ```Julia; if VERSION < v""1.8""; ...; else; ...; end; ```. Adding the compat requirement is safer since tests only run on v1.9. . On the other hand, in this case, allowing backwards compatibility might be just a few lines like above.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3184#issuecomment-1636223845:298,safe,safer,298,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3184#issuecomment-1636223845,1,['safe'],['safer']
Safety,"As discussed via zoom with @kburns, passing a key to computations to be stored and used to determine whether a computation needs to be performed is a simple method that may work for us. A simple option for a key is the current model time, which works for all the time stepping methods we employ and has the additional advantage of interpretability. One complication is that we allow users to specify memory space for `ComputedField`s and `AveragedField`s. As a result, two `ComputedField`s that share memory space may have incorrect `data` if the memory is overwritten. This is, in fact, a problem even in the current code and not dependent on the optimizations discussed in this issue. Since we think it is important to give users the option of avoiding unnecessary memory allocation by managing the allocation of scratch space for computations, we cannot prevent incorrect output resulting from overwriting of scratch space during operations with embedded averaged fields and computed fields. We simply have to document this potential ""gotcha"". We can make avoiding repeated operations a bit safer by requiring users to enable it when a `ComputedField` or `AveragedField` is constructed by a keyword argument, something like `recompute_safely`: . ```julia; U = AveragedField(model.velocities.u, dims=(1, 2), data=scratch, recompute_safely=false); ```. When `recompute_safely` is disabled, the model time at computation will be cached. The cache can either be inside `AveragedField`, or in a global cache. (A global cache has the advantage of being on the CPU; a local cache has the advantage of being local).",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/955#issuecomment-694601458:746,avoid,avoiding,746,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/955#issuecomment-694601458,3,"['avoid', 'safe']","['avoiding', 'safer']"
Safety,At some point we should implement the following tests suggested by @edoddridge in https://github.com/climate-machine/Oceananigans.jl/issues/81. - [ ] Spin down of a flow-field under the influence of friction; - [ ] Thermal wind balance: specify a density structure and compare model velocity fields with analytical solution; - [ ] Rayleigh–Bénard convection (I think analytic solutions exist at low Rayleigh number?); - [ ] Onset of baroclinic instability: compare growth rates with analytical predictions,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/157:494,predict,predictions,494,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/157,1,['predict'],['predictions']
Safety,"At the moment, the construction of a matrix from a linear operation for the multigrid solver on the GPU involves scalar operations:. https://github.com/CliMA/Oceananigans.jl/blob/60470f4d716026f161dfd303a30d9d14fbfb97a1/src/Solvers/multigrid_solver.jl#L246-L279. This matrix construction is **only** called once upon initialization of the solver. Furthermore, this particular method that uses scalar operations is **not** used by the implicit free-surface solvers since in that case we provide the `MultigridSolver` constructor with the matrix itself, see:. https://github.com/CliMA/Oceananigans.jl/blob/60470f4d716026f161dfd303a30d9d14fbfb97a1/src/Models/HydrostaticFreeSurfaceModels/mg_implicit_free_surface_solver.jl#L77-L104. For general matrix construction that works for any linear operation, it would be nice to avoid using scalar operations on the GPU _or_ construct the matrix on the CPU and convert it to the GPU afterwards. cc @elise-palethorpe",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2737:819,avoid,avoid,819,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2737,1,['avoid'],['avoid']
Safety,Avoid computing hydrostatic pressure when z is Flat,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2093:0,Avoid,Avoid,0,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2093,1,['Avoid'],['Avoid']
Safety,Avoid data dep race condition in CI,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1804:0,Avoid,Avoid,0,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1804,1,['Avoid'],['Avoid']
Safety,Avoid executing the lines to install dependencies within examples,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1316:0,Avoid,Avoid,0,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1316,1,['Avoid'],['Avoid']
Safety,Avoid need for `data` in user-interaction with fields through dispatch on `Colon` indices,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/458:0,Avoid,Avoid,0,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/458,1,['Avoid'],['Avoid']
Safety,Avoid redefining `R_Earth` in the `Coriolis` module,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3046:0,Avoid,Avoid,0,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3046,1,['Avoid'],['Avoid']
Safety,Avoid type instability in HydrostaticSphericalCoriolis,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2926:0,Avoid,Avoid,0,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2926,1,['Avoid'],['Avoid']
Safety,Avoid updating hydrostatic pressure for Flat z dimensions,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2092:0,Avoid,Avoid,0,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2092,1,['Avoid'],['Avoid']
Safety,Avoiding repeated computations in the evaluation of `AbstractOperations`,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/955:0,Avoid,Avoiding,0,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/955,1,['Avoid'],['Avoiding']
Safety,Avoiding unnecessary recomputation of fields in output evaluation,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/968:0,Avoid,Avoiding,0,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/968,1,['Avoid'],['Avoiding']
Safety,"Awesome! Seems like a good idea to check for this as otherwise the resulting segfault gives no indication about what to debug. Some ideas to improve this PR:; 1. Right now you only iterate over the values of the named tuple `boundary_conditions`. If you also iterate over the keys, then when the warning is printed you can also say exactly which boundary condition is the problem which might help the user.; 2. Might also be good to check the opposite case: when `arch == CPU()` but one of the boundary conditions is a `CuArray`.; 3. Would be good to have a test for this functionality. `@test_throws` can check that the Model constructor throws the expected error when running on the GPU with an `Array` and vice versa. We might also want to implement a similar function to check that all float types in a model are consistent, i.e. to avoid running with `Float32` fields and a `Float64` equation of state.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/421#issuecomment-532788530:837,avoid,avoid,837,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/421#issuecomment-532788530,1,['avoid'],['avoid']
Safety,"Based on [this finding](https://github.com/CliMA/Oceananigans.jl/pull/2029#issuecomment-952119253) it's not just the trig functions that are causing problems, so I agree that the statement about avoiding them shouldn't be there. I removed that and hopefully we can still merge this PR since it's a small improvement. I do think we should look into what's causing the slowdown with the trig function for me specifically though, so I created an issue for that: https://github.com/CliMA/Oceananigans.jl/issues/2034",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2029#issuecomment-956540913:195,avoid,avoiding,195,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2029#issuecomment-956540913,1,['avoid'],['avoiding']
Safety,Better safe than sorry :),MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1305#issuecomment-798968747:7,safe,safe,7,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1305#issuecomment-798968747,1,['safe'],['safe']
Safety,"Billy mentioned it in some other comments but while we do splat args for some of the function calls, the function definitions use Varargs instead. This should avoid the catastrophic slowdown we saw with splatting earlier, but I agree that it should be tested. Do you have any good CPU performance tests set up @glwagner ?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3480#issuecomment-2150096206:159,avoid,avoid,159,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3480#issuecomment-2150096206,1,['avoid'],['avoid']
Safety,"Btw, when I am doing small exploratory runs I only need to ""pay compilation time"" once per model. Constructing another model in the same REPL session doesn't have any extra compilation costs unless I change something in the source code (Oceananigans source code; node my scripts). I think my gut feeling would be against implementing such a feature, mostly because of the _test and maintain_ part you mentioned. Also because I can see sort-of-easy ways around it. Perhaps I'm missing something? E.g. I don't really quite understand what do you mean by ""safe to pick-up"" and ""due to the way pick-ups work"" in. > It makes for more complex code, especially when figuring out when it's safe to pick-up a simulation or not, due to the way pick-ups work.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3154#issuecomment-1605301393:553,safe,safe,553,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3154#issuecomment-1605301393,2,['safe'],['safe']
Safety,"But this is confusing notation... can we update to use the triple notation that we use throughout the code?. The confusion is that MITgcm uses the notation `ΔzF` to denote the _difference of_ `zF`. This is _opposite_ the meaning of our superscripts, which indicate the _location_. In our notation `Δz^aaf` is the _difference of_ `z^aac`. . I suggest we just avoid these capital letters and use only superscripts...",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2150#issuecomment-1012204391:358,avoid,avoid,358,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2150#issuecomment-1012204391,1,['avoid'],['avoid']
Safety,"CUDA error: a PTX JIT compilation failed (code 218, ERROR_INVALID_PTX); ptxas application ptx input, line 803; error : Entry function '_Z19julia_gpu__compute_7ContextI14__CUDACtx_Name16CompilerMetadataI10StaticSizeI9_1__1__1_E12DynamicCheckvv7NDRangeILi3ES2_I9_1__1__1_ES2_I9_1__1__1_EvvEEv14__PassType_253v12DisableHooksE14_gpu__compute_11OffsetArrayI7Float64Li3E13CuDeviceArrayIS9_Li3ELi1EEE17MultiaryOperationI4CellS12_S12_Li5E2__5TupleI15BinaryOperationIS12_S12_S12_S13_10DerivativeIS12_S12_S12_6__x___S8_IS9_Li3ES10_IS9_Li3ELi1EEE9_identity20RegularCartesianGridIS9_8PeriodicS20_7BoundedS8_IS9_Li1E12StepRangeLenIS9_14TwicePrecisionIS9_ES23_IS9_EEEEE5Int64S18_S18_S18_S19_IS9_S20_S20_S21_S8_IS9_Li1ES22_IS9_S23_IS9_ES23_IS9_EEEEES15_IS12_S12_S12_S13_S16_IS12_S12_S12_6__y___S8_IS9_Li3ES10_IS9_Li3ELi1EEES18_S19_IS9_S20_S20_S21_S8_IS9_Li1ES22_IS9_S23_IS9_ES23_IS9_EEEEES24_S18_S18_S18_S19_IS9_S20_S20_S21_S8_IS9_Li1ES22_IS9_S23_IS9_ES23_IS9_EEEEES15_IS12_S12_S12_S13_S16_IS12_S12_S12_6__z___S8_IS9_Li3ES10_IS9_Li3ELi1EEES18_S19_IS9_S20_S20_S21_S8_IS9_Li1ES22_IS9_S23_IS9_ES23_IS9_EEEEES24_S18_S18_S18_S19_IS9_S20_S20_S21_S8_IS9_Li1ES22_IS9_S23_IS9_ES23_IS9_EEEEES15_I4FaceS12_S27_S13_S16_IS27_S12_S27_S17_S8_IS9_Li3ES10_IS9_Li3ELi1EEES18_S19_IS9_S20_S20_S21_S8_IS9_Li1ES22_IS9_S23_IS9_ES23_IS9_EEEEES24_S18_S18_S18_S19_IS9_S20_S20_S21_S8_IS9_Li1ES22_IS9_S23_IS9_ES23_IS9_EEEEES15_IS12_S27_S27_S13_S16_IS12_S27_S27_S25_S8_IS9_Li3ES10_IS9_Li3ELi1EEES18_S19_IS9_S20_S20_S21_S8_IS9_Li1ES22_IS9_S23_IS9_ES23_IS9_EEEEES24_S18_S18_S18_S19_IS9_S20_S20_S21_S8_IS9_Li1ES22_IS9_S23_IS9_ES23_IS9_EEEEEES14_IS18_S18_S18_7__xz___7__yz___ES19_IS9_S20_S20_S21_S8_IS9_Li1ES22_IS9_S23_IS9_ES23_IS9_EEEEE' uses too much parameter space (0x1408 bytes, 0x1100 max).; ptxas fatal : Ptx assembly aborted due to errors; ```. Solution: probably some of the suggestions on #746 are useful; we might need to submit a PR to CUDA.jl for this. I think this might be the toughest of all (more argument for also resolving #1246)",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1241#issuecomment-738870593:2640,abort,aborted,2640,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1241#issuecomment-738870593,1,['abort'],['aborted']
Safety,"Calculating trigonometric functions is ""slow"" (compared to simple arithmetic operations) on _all_ hardware because they require various series expansions and iterative calculations. Division is also ""slow""; a single division operation requires something like 20 operations. An issue with any psuedo-anecdotal advice like this is that it may not be valid on all hardware. Just how ""slow"" is `sin` on various hardware? Is it slow on all Nvidia GPUs, or just some? Is it slow on AMD GPUs? What does ""slow"" mean, exactly?. Philosophically, I disagree that trigonometric functions should be avoided. Speed is not always a primary concern for computations. Other concerns that may be equally or more important are reproducibility, programmer productivity, accuracy, etc. I think that users will probably better served by a nice comprehensive reference to floating point calculations on various hardware on CPUs and GPUs...",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2029#issuecomment-951424095:586,avoid,avoided,586,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2029#issuecomment-951424095,1,['avoid'],['avoided']
Safety,Can someone explain the purpose of `MPI.Init()`? Is it a bad idea to call this inside `MultiArch` constructor rather than requiring users to write it out? Can we detect if it's been called in the constructor and then call _by default_ if it hasn't been called yet?,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2349#issuecomment-1066202040:162,detect,detect,162,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2349#issuecomment-1066202040,1,['detect'],['detect']
Safety,Can we also avoid using index 0 even if it works this time?,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2553#issuecomment-1126983885:12,avoid,avoid,12,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2553#issuecomment-1126983885,1,['avoid'],['avoid']
Safety,"Cassette.AbstractPass where T<:Union{Nothing, Cassette.Tag{N, X, E} where E where X where N<:Cassette.AbstractContextName} where M where N<:Cassette.AbstractContextName, Any...) in module Cassette at /home/alir_mit_edu/.julia/packages/Cassette/xggAf/src/overdub.jl:508 overwritten in module GPUifyLoops at /home/alir_mit_edu/.julia/packages/Cassette/xggAf/src/overdub.jl:508.; WARNING: Method definition recurse(Cassette.Context{N, M, T, P, B, H} where H<:Union{Cassette.DisableHooks, Nothing} where B<:Union{Nothing, Base.IdDict{Module, Base.Dict{Symbol, Cassette.BindingMeta}}} where P<:Cassette.AbstractPass where T<:Union{Nothing, Cassette.Tag{N, X, E} where E where X where N<:Cassette.AbstractContextName} where M where N<:Cassette.AbstractContextName, Any...) in module Cassette at /home/alir_mit_edu/.julia/packages/Cassette/xggAf/src/overdub.jl:521 overwritten in module GPUifyLoops at /home/alir_mit_edu/.julia/packages/Cassette/xggAf/src/overdub.jl:521.; CUDA-enabled GPU(s) detected:; CuDevice(0): Tesla V100-SXM2-16GB; [000.00%] Time: 0.0 / 604800.0... average wall clock time per iteration: 283.890 ms; [000.05%] Time: 300.0 / 604800.0... average wall clock time per iteration: 39.199 ms; [000.10%] Time: 600.0 / 604800.0... average wall clock time per iteration: 39.270 ms; [000.15%] Time: 900.0 / 604800.0... average wall clock time per iteration: 39.172 ms; [000.20%] Time: 1200.0 / 604800.0... average wall clock time per iteration: 39.167 ms; [000.25%] Time: 1500.0 / 604800.0... average wall clock time per iteration: 39.245 ms; [000.30%] Time: 1800.0 / 604800.0... average wall clock time per iteration: 39.119 ms; [000.35%] Time: 2100.0 / 604800.0... average wall clock time per iteration: 39.181 ms; [000.40%] Time: 2400.0 / 604800.0... average wall clock time per iteration: 39.246 ms; [000.45%] Time: 2700.0 / 604800.0... average wall clock time per iteration: 45.696 ms; [000.50%] Time: 3000.0 / 604800.0... average wall clock time per iteration: 40.170 ms; [000.55%] Time: 3",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/273:1392,detect,detected,1392,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/273,1,['detect'],['detected']
Safety,Change `TimeInterval` to avoid roundoff error issue plus some cleanup,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3616:25,avoid,avoid,25,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3616,1,['avoid'],['avoid']
Safety,Clean up redundant errors in `FieldTimeSeries`,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3869:9,redund,redundant,9,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3869,1,['redund'],['redundant']
Safety,"Clearly I was very mixed up in terms of my directions but I think I have improved things and you can see my code below. - At the moment, the sponge is only to the west. Since I am imposing outflow conditions to the east I feel like I should have a second sponge there. Or, said differently, have a sponge that is zero in the interior but turns on both to the west and east. Agreed?; - Some good news is that I do seem to be imposing boundary conditions. Bad news, there is some asymmetry in that the inflow condition does no seem to be imposed at `x=-10` along the southern wall. I can't think if a physical reason why that would be the case. Any ideas?; - Is there an easy way to plot the grid? It occurs to me that if I had done this at the beginning I would have avoided a bit of trouble that I created for myself. ```; julia> model.solution.uh[:,:,1]; 14×14 OffsetArray(::Matrix{Float64}, -2:11, -2:11) with eltype Float64 with indices -2:11×-2:11:; 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0; 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0; 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0; 0.0 0.0 0.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0; 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0; 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0; 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0; 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0; 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0; 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0; 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0; 0.0 0.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0; 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0; 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0. ```. ```; using Oceananigans; using Oceananigans.Grids; using Oceananigans.Models; using Oceananigans.ImmersedBoundaries: ImmersedBoundaryGrid, GridFittedBoundary. using Oceananigans.BoundaryConditions: NormalFlow. grid = RegularRectilinearGrid(size",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1730#issuecomment-852958373:766,avoid,avoided,766,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1730#issuecomment-852958373,1,['avoid'],['avoided']
Safety,"Closes #1995 and closes #1996. (I'm not sure when the bug was introduced, so I capped the version up to 3.3.6. Feel free to change if there's a newer version that's also safe.). Also we need to remember to remove this after the bug gets put into a tagged release. (Can we exclude only version 3.4.2 using compat?)",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1997:170,safe,safe,170,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1997,1,['safe'],['safe']
Safety,Closing this since its essentially redundant with #330.,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/287#issuecomment-525536829:35,redund,redundant,35,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/287#issuecomment-525536829,1,['redund'],['redundant']
Safety,"CompatHelper: add new compat entry for ""SafeTestsets"" at version ""0.0.1""",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/898:40,Safe,SafeTestsets,40,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/898,1,['Safe'],['SafeTestsets']
Safety,"Consider:. * The Adams-Bashforth time stepper is not well-suited for adaptive time-stepping, because it requires a initialization step. Thus it behooves scientists to update their time-step only infrequently. * However, during model spinup / transition to turbulence, a sharp change in flow regime may occur. This sharp change necessitates updating the time-step frequently to avoid blow up (an alternative strategy is to take uniform small time-steps during this time, which is safer but will take longer). * In addition to the need to update a time-step frequently near transition to turbulence, it is *also* wise to take conservatively small time-steps . Because of these considerations I am not sure it makes sense to add `spinup_cfl`, which only addresses the last, minor point. The main feature that is needed is a change in the frequency at which the time-step is updated (we currently do this manually by writing two loops). However, a change in update frequency is only really needed because Adams-Bashforth is ill-suited for adaptive time stepping (otherwise it'd be fine to update frequently even in the main loop). So it's a hack-on-a-hack. What do you think?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/356#issuecomment-520824946:377,avoid,avoid,377,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/356#issuecomment-520824946,2,"['avoid', 'safe']","['avoid', 'safer']"
Safety,Construct grid parameters with `BigFloat` to avoid round-off error,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2499:45,avoid,avoid,45,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2499,1,['avoid'],['avoid']
Safety,"Currently halo regions are filled _prior_ to performing a time-step. This means that _after_ the time-step, they are incorrect. We therefore cannot output fields with correct halo regions, since data is outputted after a time-step is taken. But it gets worse. If the average of a field is taken, we zero out the halo regions. Zeroing out the halo regions corrupts near-boundary data for all subsequent computations with the fields. Currently, abstract operations cannot be trusted in boundary-adjacent cells. To remedy this we need to fill halo regions on fields prior to performing computations. One way we might do this is to write a `compute!` method for fields:. ```julia; compute!(field::Field) = fill_halo_regions!(field); ```. We can also define a `conditional_compute!` method for `Field`s and add a `status` property, so that halo regions are not filled ""redundantly"". For this to work, we also need to invalidate `field.status` when halo regions are zeroed out by `compute!(averaged_field::AveragedField)`, (for example by setting `field.status.time = NaN`). This won't work currently, of course, due to #971 . So this issue cannot be resolved until #971 is resolved.",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1063:864,redund,redundantly,864,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1063,1,['redund'],['redundantly']
Safety,"Currently the `JLD2OutputWriter` labels output in a timeseries by `model.clock.iteration`. It might be easier to handle output if data is labeled by ""save point"", so it goes 0, 1, 2... This is easier to work with at the REPL, since you always know, for example, that `file[""timeseries/u/1""]` exists (rather than having to manually inspect the file to figure out the iterations that data ended up being saved at). We can add a field `iteration` so that the model iteration can be recovered, if that's needed.",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/963:479,recover,recovered,479,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/963,1,['recover'],['recovered']
Safety,"Currently the `Oceananigans` module exports. ```julia; Flux, Value, Gradient, NormalFlow,; FluxBoundaryCondition, ValueBoundaryCondition, GradientBoundaryCondition,; ```. so there's some redundancy (perhaps we should export one set to reduce namespace pollution) but a minor practical issue is that the `Flux` type conflicts with the popular Flux.jl package (I guess I'm the only one using them together right now but there may be more in the future?). There's also some inconsistency in exporting `NormalFlow` but not `NormalFlowBoundaryCondition`. In deciding on what to export for the user interface (see #1132) I'm wondering what do people think about only exporting the long-name version, e.g. `FluxBoundaryCondition` instead of `Flux`. I think this will have a few benefits:; 1. Lower probability for conflicts. `Flux` is one example, but `Value` and `Gradient` are pretty generic terms so I wouldn't be surprised if they conflict with exports from other packages future users may want to work with.; 2. Scripts might read more intuitively, e.g. because you say ""a flux boundary condition"" and not ""a boundary condition of type flux"".; 3. If we decide to export the complete set of boundary conditions we could do it by exporting `PeriodicBoundaryCondition` without having to worry about conflicting with the `Periodic` topology. X-Ref: #1132",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1140:187,redund,redundancy,187,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1140,1,['redund'],['redundancy']
Safety,"Currently, we use the constructor `output_writer.array_type` to convert array data prior to outputting:. https://github.com/CliMA/Oceananigans.jl/blob/03a6f855f839504d94cb8cee3c2665b17afbc6d5/src/OutputWriters/fetch_output.jl#L17. But we should use `convert` instead, because this avoids allocating memory when no type conversion is needed:. ```julia; julia> a = rand(1, 1, 1); 1×1×1 Array{Float64,3}:; [:, :, 1] =; 0.7727202498256802. julia> b = convert(Array{Float64}, a); 1×1×1 Array{Float64,3}:; [:, :, 1] =; 0.7727202498256802. julia> b === a; true; ```. so `b` is just a reference to `a`, but. ```julia; julia> c = Array{Float64}(a); 1×1×1 Array{Float64,3}:; [:, :, 1] =; 0.7727202498256802. julia> c === a; false; ```. `c` is not a reference to `a`. This matters very little since we basically always need to allocate memory to convert from Float64 to Float32. A related minor optimization would be to avoid converting views to the same type as the parent array, since we could just output those directly. I think we have to use dispatch on one of the type parameters of `SubArray` for that. But maybe simpler code is worth not implementing a minor optimization for an edge case.",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1182:281,avoid,avoids,281,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1182,2,['avoid'],"['avoid', 'avoids']"
Safety,"Dedalus adds all the ""grid-crossing frequencies"" (e.g. `u/dx`, `v/dy`, `w/dz`) at each point, takes the maximum of this quantity across all grid points, and takes its reciprocal to determine the CFL time. We chose this because its pretty simple and properly handles doppler boosting of waves on top of mean flows. Apparently according to wikipedia this is also the right thing for diagonal velocities... but I'm not sure that's something you can show rigorously for spectral methods. Maybe so. In any event, we find that the stability boundaries in terms of the CFL safety factor (scaling the `dt` selected as described) vary a lot based on the timestepper and problem, anyways.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1201#issuecomment-735250942:566,safe,safety,566,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1201#issuecomment-735250942,1,['safe'],['safety']
Safety,Defines many identity's to avoid recursion when compiling AbstractOperations,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1595:27,avoid,avoid,27,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1595,1,['avoid'],['avoid']
Safety,Did you try avoiding broadcasting ? That's the first thing to do. Then we can take a closer look at the initial condition function and see if there's more to change.,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2479#issuecomment-1115990673:12,avoid,avoiding,12,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2479#issuecomment-1115990673,1,['avoid'],['avoiding']
Safety,Do not install/load CUDA packages if no GPU is detected.,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/178:47,detect,detected,47,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/178,1,['detect'],['detected']
Safety,Drop SafeTestset from package dependency,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2601:5,Safe,SafeTestset,5,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2601,1,['Safe'],['SafeTestset']
Safety,"During the coefficients down to 1e-9 (!!) allows 3 stable time steps _when continents / bathymetry are removed_:. ```julia; julia> include(""idealized_one_degree_simulation.jl""); grid = 360×150×48 LatitudeLongitudeGrid{Float64, Periodic, Bounded, Bounded} on CPU with 4×4×4 halo and with precomputed metrics; ├── longitude: Periodic λ ∈ [-180.0, 180.0) regularly spaced with Δλ=1.0; ├── latitude: Bounded φ ∈ [-75.0, 75.0] regularly spaced with Δφ=1.0; └── z: Bounded z ∈ [-5244.5, 0.0] variably spaced with min(Δz)=10.0, max(Δz)=410.5; ┌ Warning: WENO on a curvilinear stretched coordinate is not validated, use at your own risk!!; └ @ Oceananigans.Advection ~/Projects/dev/Oceananigans.jl/src/Advection/weno_fifth_order.jl:160; [ Info: Initializing simulation...; ┌ Info: Iteration: 0, time: 0 seconds, wall time: 226.942 ms; │ ├── max(u): (0.00e+00, 0.00e+00, 0.00e+00) m s⁻¹; │ ├── extrema(T): (0.31, 30.00) ᵒC; └ └── max|η|: 0.00e+00 m; [ Info: ... simulation initialization complete (288.084 ms); [ Info: Executing initial time step...; [ Info: ... initial time step complete (1.446 seconds).; ┌ Info: Iteration: 1, time: 20 minutes, wall time: 1.452 seconds; │ ├── max(u): (1.32e-16, 5.40e-02, 6.44e-04) m s⁻¹; │ ├── extrema(T): (0.31, 30.00) ᵒC; └ └── max|η|: 5.93e-01 m; ┌ Info: Iteration: 2, time: 40 minutes, wall time: 945.169 ms; │ ├── max(u): (1.39e-02, 1.06e-01, 1.05e-03) m s⁻¹; │ ├── extrema(T): (0.05, 30.00) ᵒC; └ └── max|η|: 1.33e+00 m; [ Info: Simulation is stopping. Model iteration 3 has hit or exceeded simulation stop iteration 3.; ┌ Info: Iteration: 3, time: 1 hour, wall time: 945.031 ms; │ ├── max(u): (3.55e-02, 1.52e-01, 1.37e-03) m s⁻¹; │ ├── extrema(T): (0.15, 30.00) ᵒC; └ └── max|η|: 2.08e+00 m; ```. But when they're put back, it still blows up (even with coefficients 1e-9):. ```julia; julia> include(""idealized_one_degree_simulation.jl""); grid = ImmersedBoundaryGrid on:; architecture: CPU(); grid: 360×150×48 LatitudeLongitudeGrid{Float64, Periodic, Bounded, Bound",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2463#issuecomment-1107311807:624,risk,risk,624,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2463#issuecomment-1107311807,1,['risk'],['risk']
Safety,EDIT: Sorry I see now that it's to avoid additional allocation :) https://github.com/CliMA/Oceananigans.jl/blob/24710f4b9d3733d09f8af0eb459eb9984908175a/src/Models/HydrostaticFreeSurfaceModels/implicit_free_surface.jl#L108. I'm trying to implement a multigrid equivalent of the PCG implicit free surface solver but I don't understand why we need this field https://github.com/CliMA/Oceananigans.jl/blob/aea5494a763eb3949b234b5302ddb0a2e9764678/src/Models/HydrostaticFreeSurfaceModels/pcg_implicit_free_surface_solver.jl#L25. I thought the right hand side would change each iteration and is passed to the `solve!` function?,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2396#issuecomment-1168091279:35,avoid,avoid,35,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2396#issuecomment-1168091279,1,['avoid'],['avoid']
Safety,ERROR: Unsatisfiable requirements detected for package CuArrays,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/796:34,detect,detected,34,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/796,1,['detect'],['detected']
Safety,"ES23_IS7_S24_IS7_ES24_IS7_ES4_EEvES25_IS22_IS7_Li2ES9_IS7_Li2ELi1EEES26_EvvES7_5Az___E9identity1S30_S18_IS7_S19_S19_S20_S21_IS7_S19_S19_S20_S7_S7_S7_S22_IS7_Li1ES23_IS7_S24_IS7_ES24_IS7_ES4_EES22_IS7_Li1ES23_IS7_S24_IS7_ES24_IS7_ES4_EES22_IS7_Li1ES23_IS7_S24_IS7_ES24_IS7_ES4_EEvES25_IS22_IS7_Li2ES9_IS7_Li2ELi1EEES26_EvvES7_E5FieldIvvS14_vvvS22_IS7_Li3ES9_IS7_Li3ELi1EEES7_vvvES31_S32_S18_IS7_S19_S19_S20_S21_IS7_S19_S19_S20_S7_S7_S7_S22_IS7_Li1ES23_IS7_S24_IS7_ES24_IS7_ES4_EES22_IS7_Li1ES23_IS7_S24_IS7_ES24_IS7_ES4_EES22_IS7_Li1ES23_IS7_S24_IS7_ES24_IS7_ES4_EEvES25_IS22_IS7_Li2ES9_IS7_Li2ELi1EEES26_EvvES7_ES_S18_IS7_S19_S19_S20_S21_IS7_S19_S19_S20_S7_S7_S7_S22_IS7_Li1ES23_IS7_S24_IS7_ES24_IS7_ES4_EES22_IS7_Li1ES23_IS7_S24_IS7_ES24_IS7_ES4_EES22_IS7_Li1ES23_IS7_S24_IS7_ES24_IS7_ES4_EEvES25_IS22_IS7_Li2ES9_IS7_Li2ELi1EEES26_EvvE11NotImmersedI8truefuncES4_S7_E' uses too much parameter space (0x11f0 bytes, 0x1100 max).; ptxas fatal : Ptx assembly aborted due to errors; If you think this is a bug, please file an issue and attach /glade/scratch/tomasc/jl_uD0VONe1Cz.ptx; Stacktrace:; [1] error(s::String); @ Base ./error.jl:35; [2] compile(job::GPUCompiler.CompilerJob, ctx::LLVM.Context); @ CUDA /glade/work/tomasc/.julia/packages/CUDA/pCcGc/src/compiler/compilation.jl:208; [3] #1032; @ /glade/work/tomasc/.julia/packages/CUDA/pCcGc/src/compiler/compilation.jl:120 [inlined]; [4] JuliaContext(f::CUDA.var""#1032#1033""{GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget, CUDA.CUDACompilerParams}}); @ GPUCompiler /glade/work/tomasc/.julia/packages/GPUCompiler/NVLGB/src/driver.jl:37; [5] compile; @ /glade/work/tomasc/.julia/packages/CUDA/pCcGc/src/compiler/compilation.jl:119 [inlined]; [6] actual_compilation(cache::Dict{Any, Any}, src::Core.MethodInstance, world::UInt64, cfg::GPUCompiler.CompilerConfig{GPUCompiler.PTXCompilerTarget, CUDA.CUDACompilerParams}, compiler::typeof(CUDA.compile), linker::typeof(CUDA.link)); @ GPUCompiler /glade/work/tomasc/.julia/packages/GPUCompiler/NVLG",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3140:4318,abort,aborted,4318,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3140,1,['abort'],['aborted']
Safety,"Eventually, we will need to represent the effects of surface waves in `Oceananigans.jl`. The best way to do that is to introduce a term that models the 'wave-averaged' effects of surface waves on near-surface motions. I propose that we use the Lagrangian-mean interpretation for our velocity field in adding this term, rather than an Eulerian-mean interpretation. The Lagrangian-mean interpretation has the advantage of . 1. Avoiding ""accidental"" initialization of large near-inertial oscillations due to initial conditions that are out of equilibrium with the surface wave field (eg the initial conditions in [McWilliams et al 1997](https://www.cambridge.org/core/journals/journal-of-fluid-mechanics/article/langmuir-turbulence-in-the-ocean/638FD0E368140E5972144348DB930A38));. 2. Requiring the least modification of physics, since we have to only introduce two terms (or just one for steady surface wave fields) to the momentum equations (as opposed to two terms in the momentum equations and one term in the tracer equations in the Eulerian-mean formulation);. 3. Arguably using the Lagrangian-mean interpretation means we can avoid modifying our subgrid turbulence closure, since surface wave terms do not affect the turbulent kinetic energy balance;. 4. The pressure field retains its original interpretation. The surface wave field interacts with interior dynamics through its Stokes drift field, which is generally prescribed to boundary layer LES. To implement the surface wave term in the Lagrangian-mean formulation, we require gradients of the Stokes drift velocity associated with the surface wave field. To start the best way to implement this, at least to start, is through functions. This might look something like. ```julia; u_stokes(x, y, z, t) = U * exp(2*k*z). model = Model(stokes_drift=StokesDrift(u=u_stokes), ... ); ```. In this pattern, we have to take gradients of the function `u_stokes` either numerically or analytically (?) . Alternatively, we can ask users to supply the ",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/443:425,Avoid,Avoiding,425,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/443,1,['Avoid'],['Avoiding']
Safety,"Exactly, in practice a transpose is communication + permutation, but with `unified_memory` we avoid the communication part (I mean it is there but it's handled by CUDA)",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2523#issuecomment-1119919377:94,avoid,avoid,94,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2523#issuecomment-1119919377,1,['avoid'],['avoid']
Safety,"Excellent!. To summarize:. 1. A `materialize_biogeochemistry` feature may still be needed sometime in the future; 2. Generally we think it's best to avoid this step if possible, for a few reasons:; - code that relies on this step may end up unnecessarily complicated or convoluted; - materializing objects leads to a distinction between user input and model properties that can produce hard-to-find bugs; - it can be complicated to maintain a `materialize_X` feature.; 3. For the specific problem considered here, we can probably find a solution that avoids `materialize_biogeochemistry`.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3262#issuecomment-1719382840:149,avoid,avoid,149,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3262#issuecomment-1719382840,2,['avoid'],"['avoid', 'avoids']"
Safety,"Extrinsic is what we decide. When we use the cubed sphere we want to use geographic coordinates. But vector fields on the c-grid have their own coordinate system dictated by the structure of the grid. Except for `LatitudeLongitudeGrid`, the geographic coordinate system does not coincide with the ""other"" coordinate system (I called it intrinsic above, but if this is confusion we need another name) that's associated with the grid. What language would you like to use to describe (1) the coordinate system that we would like to impose on the grid for the user interface and 2) the coordinate system that the raw vector fields use (I'm trying to avoid the language I proposed to ask this question).",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3625#issuecomment-2187115309:646,avoid,avoid,646,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3625#issuecomment-2187115309,1,['avoid'],['avoid']
Safety,Finally all type issues have been resolved using three different fixes; - Ensure that `apply_regionally!` and `construct_regionally` are always inlined; - Avoid any Generator in the `getregion` function but explicitly pass arguments to it by using metaprogramming to input the correct expression; - Use alternating inner and outer `getregion` to force compilation through recursive use; - For `Tuple`s force compilation by recursive getregion based on tuple's size,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2253#issuecomment-1116006280:155,Avoid,Avoid,155,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2253#issuecomment-1116006280,1,['Avoid'],['Avoid']
Safety,"Float64,3,CUDA.CuDeviceArray{Float64,3,CUDA.AS.Global}},OffsetArrays.OffsetArray{Float64,3,CUDA.CuDeviceArray{Float64,3,CUDA.AS.Global}},OffsetArrays.OffsetArray{Float64,3,CUDA.CuDeviceArray{Float64,3,CUDA.AS.Global}}}}) resulted in invalid LLVM IR ; Reason: unsupported call to the Julia runtime (call to jl_f_tuple); ```. Might have something to do with the new tuple/named tuple syntax but I thought Julia 1.4 -> 1.5 wasn't supposed to introduce any breaking changes? https://julialang.org/blog/2020/08/julia-1.5-highlights/#implicit_keyword_argument_values. # Julia 1.4.2. ```; _; _ _ _(_)_ | Documentation: https://docs.julialang.org; (_) | (_) (_) |; _ _ _| |_ __ _ | Type ""?"" for help, ""]?"" for Pkg help.; | | | | | | |/ _` | |; | | |_| | | | (_| | | Version 1.4.2 (2020-05-23); _/ |\__'_|_|_|\__'_| | Official https://julialang.org/ release; |__/ |. julia> using Oceananigans; grid = RegularCartesianGrid(size=(16, 16, 16), extent=(1, 1, 1)); model = IncompressibleModel(architecture=GPU(), grid=grid); time_step!(model, 1); [ Info: Precompiling Oceananigans [9e8cae18-63c1-5223-a75c-80ca9d6e9a09]; CUDA-enabled GPU(s) detected:; CUDA.CuDevice(0); CUDA.CuDevice(1); CUDA.CuDevice(2); CUDA.CuDevice(3); ┌ Warning: Performing scalar operations on GPU arrays: This is very slow, consider disallowing these operations with `allowscalar(false)`; └ @ GPUArrays ~/.julia/packages/GPUArrays/4W5rW/src/host/indexing.jl:43; ```. # Julia 1.5.0. ```; _; _ _ _(_)_ | Documentation: https://docs.julialang.org; (_) | (_) (_) |; _ _ _| |_ __ _ | Type ""?"" for help, ""]?"" for Pkg help.; | | | | | | |/ _` | |; | | |_| | | | (_| | | Version 1.5.0 (2020-08-01); _/ |\__'_|_|_|\__'_| | Official https://julialang.org/ release; |__/ |. julia> using Oceananigans; grid = RegularCartesianGrid(size=(16, 16, 16), extent=(1, 1, 1)); model = IncompressibleModel(architecture=GPU(), grid=grid); time_step!(model, 1); ┌ Warning: Performing scalar operations on GPU arrays: This is very slow, consider disallowing these o",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/828:2054,detect,detected,2054,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/828,1,['detect'],['detected']
Safety,"Following up on #1662, copied below. We should make it so that for `ShallowWaterModel`, if we set halo to (3,3), or whatever is appropriate, there is no warning. ```; [2021/05/18 12:18:29.494] WARN Inflating model grid halo size to (3, 3, 0) and recreating grid. The model grid will be different from the input grid. To avoid this warning, pass halo=(3, 3, 0) when constructing the grid. -@-> /home/fpoulin/software/Oceananigans.jl/src/Grids/automatic_halo_sizing.jl:41; ```",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1666:320,avoid,avoid,320,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1666,1,['avoid'],['avoid']
Safety,"For those following this thread the [SGRID PR](https://github.com/xgcm/xgcm/pull/559#pullrequestreview-1383786186) mentioned above has now been merged to add SGRID functionalities into [xgcm](https://github.com/xgcm/xgcm). Please do try out if useful.; There are some docs for it [here](https://xgcm.readthedocs.io/en/latest/grids.html) describing to to have it automatically detect and extract an SGRID grid, and I'm happy for feedback/questions.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1334#issuecomment-1507314745:376,detect,detect,376,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1334#issuecomment-1507314745,1,['detect'],['detect']
Safety,Found a trick to configure Travis CI to build pull requests but only pushes to master (thus reducing redundancy as default Travis CI behavior is to build both pushes and merge commits on PRs). See: https://stackoverflow.com/a/31882307,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/872#issuecomment-682117407:101,redund,redundancy,101,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/872#issuecomment-682117407,1,['redund'],['redundancy']
Safety,"From the talks we've had here and in [this discussion](https://github.com/CliMA/Oceananigans.jl/discussions/1482), as well me looking things I have some things we could include as tips. Please let me know if I'm missing something. ## General simulation tips; - In general defining variables (that are used in the calculations) as constants makes things faster as it helps the compiler optimize things; - It's probably worth inlining small functions that get called often to reduce function call overhead (at the very least it's worth playing with this). ## GPU simulation tips:; - Any global variable that needs to be accessed by the GPU needs to be a constant or the simulation will crash; - Complex `ComputedField`s may not work, so the user can either nest `ComputedField`s (simple, but costly; probably good for development) or use `KernelComputedField`s (complex but efficient; probably what you wanna use for production-ready code); - GPU runs are generally memory-limited, so it's good to both keep track of and try to reduce the size of your runs. Useful tips in this regard are; - Try to use higher-order schemes as you need fewer grid points to achieve the same resolution; - Use `nvidia-smi` to monitor the memory usage of the GPU; - Manually define scratch space to be reused in diagnostics, to avoid creating one scratch space for each separate diagnostic you have.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1478#issuecomment-802225575:1307,avoid,avoid,1307,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1478#issuecomment-802225575,1,['avoid'],['avoid']
Safety,GPU tests look like they failed but they actually passed on v1.1 (but crapped out on v1.2). . I could rerun the v1.2 job but this PR seems safe to merge.,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/514#issuecomment-548182491:139,safe,safe,139,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/514#issuecomment-548182491,1,['safe'],['safe']
Safety,Good to keep packages updated to avoid abrupt changes in dependencies.,MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/404:33,avoid,avoid,33,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/404,1,['avoid'],['avoid']
Safety,"Greg, Doesn't Ali have a version of immersed boundary layers going?. On Mon, Oct 12, 2020, 7:50 PM Gregory L. Wagner <notifications@github.com>; wrote:. > One extra consideration is that we also allow users to specify diffusive; > fluxes across boundaries. This is especially important for geophysical; > problems at LES scales and larger, where its appropriate to employ a ""wall; > model"" to predict momentum and tracer fluxes at boundaries where there's an; > unresolved or partially-resolved turbulent boundary layer (rather than; > prescribing a particular value or gradient). That said, I think if we are; > able to specify the gradients of a field across a boundary it will likely; > be straightforward to extend that implementation to specifying fluxes.; >; > The paper; >; > ""Moving from momentum transfer to heat transfer – A comparative study of; > an advanced Graetz-Nusselt problem using immersed boundary methods""; > <https://www.sciencedirect.com/science/article/pii/S0009250918306250> by; > Lu et al. (2019); >; > may also be relevant. Their conclusion is a bit confusing. They state; >; > In all simulations, excellent agreement are reached between CFM and DFM; > results, with the deviation being below 10%.; >; > which suggests that *accuracy* may not be an important factor in deciding; > which method to use. But the next sentence is; >; > Considering the nature of capturing the discontinuity at the fluid-solid; > interface, DFM might offer a more accurate result, which however requires; > more follow-up simulations to give a solid investigation.; >; > which is difficult to interpret. I suppose all they can say is that their; > results are similar to one another, but they cannot say which one is more; > accurate (and perhaps it doesn't matter which method is more accurate in; > their case, if both methods return similar results).; >; > That said, I think time-step considerations are really important, and seem; > like a good reason to choose DFM over CFM.; >; > Balaras ",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1036#issuecomment-707404766:393,predict,predict,393,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1036#issuecomment-707404766,1,['predict'],['predict']
Safety,"Had an earlier clone, was on master branch, everything was in order as ""git status"" ; ; nothing to commit, working tree clean. Then I updated my clone the usual way, just typing ""git pull"", but got plenty of warnings about; conflicts and so on. After that, ""git status"" reports:. On branch master; Your branch and 'origin/master' have diverged,; and have 1106 and 2212 different commits each, respectively.; (use ""git pull"" to merge the remote branch into yours); You have unmerged paths.; (fix conflicts and run ""git commit""); (use ""git merge --abort"" to abort the merge). so it looks like I am far from a clean update. My question: how to proceed when trying to update my Oceananigans local clone and; why the standard way does not work for this repos ?",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/598:546,abort,abort,546,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/598,2,['abort'],['abort']
Safety,"Haha it did take a while but with a satisfying ending!. And thanks for the suggestion! I didn't realize that `4.1.1` and `^4.1.1` would be the same here. But since it's okay with you, I'll merge to avoid waiting on another round of tests to pass :upside_down_face:",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3876#issuecomment-2448306882:198,avoid,avoid,198,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3876#issuecomment-2448306882,1,['avoid'],['avoid']
Safety,"Have you tried using `wc = Field(w*c, indices=(:, 1, :))` and also `wc_avg = Average(w*c, dims=2)`? This would allow you to avoid using the `indices` kwarg.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2931#issuecomment-1438706195:124,avoid,avoid,124,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2931#issuecomment-1438706195,1,['avoid'],['avoid']
Safety,"Here's some benchmarks against the current implementation. Contrary to what I just claimed, there is some speed up for `Bounded` domains, especially with 2D problems (where avoiding halos is advantageous) -- but nothing too significant. Note these are with `WENO5` advection, which should pessimize `Periodic` models as there are larger halos to fill. I came to the prior conclusions by running the two-dimensional turbulence example. I'll try to make sure the ""speed up"" is generic and also benchmark GPU and the hydrostatic model. # `main`. ## Modest 3D model (128x128x128). ```; Topologies benchmarks; ┌───────────────┬─────────────────┬────────────────────────────────┬────────────┬────────────┬────────────┬────────────┬────────────┬────────┬─────────┐; │ Architectures │ Ns │ Topologies │ min │ median │ mean │ max │ memory │ allocs │ samples │; ├───────────────┼─────────────────┼────────────────────────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────┼─────────┤; │ CPU │ (128, 128, 128) │ (Periodic, Bounded, Periodic) │ 900.894 ms │ 901.887 ms │ 902.516 ms │ 905.129 ms │ 703.45 KiB │ 1081 │ 6 │; │ CPU │ (128, 128, 128) │ (Bounded, Periodic, Bounded) │ 895.527 ms │ 911.386 ms │ 910.441 ms │ 921.211 ms │ 772.19 KiB │ 1131 │ 6 │; │ CPU │ (128, 128, 128) │ (Periodic, Bounded, Bounded) │ 900.389 ms │ 904.161 ms │ 906.879 ms │ 916.145 ms │ 771.44 KiB │ 1115 │ 6 │; │ CPU │ (128, 128, 128) │ (Bounded, Bounded, Bounded) │ 903.676 ms │ 910.638 ms │ 910.295 ms │ 914.968 ms │ 837.97 KiB │ 1154 │ 6 │; │ CPU │ (128, 128, 128) │ (Periodic, Periodic, Bounded) │ 886.053 ms │ 892.564 ms │ 894.281 ms │ 904.917 ms │ 703.45 KiB │ 1081 │ 6 │; │ CPU │ (128, 128, 128) │ (Bounded, Periodic, Periodic) │ 910.500 ms │ 924.085 ms │ 923.571 ms │ 931.683 ms │ 703.45 KiB │ 1081 │ 6 │; │ CPU │ (128, 128, 128) │ (Periodic, Periodic, Periodic) │ 914.391 ms │ 916.636 ms │ 916.407 ms │ 917.164 ms │ 575.12 KiB │ 1010 │ 6 │; │ CPU │ (128, 128, 128) │ (Bounded, Bounded, Periodic",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2477#issuecomment-1115745728:173,avoid,avoiding,173,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2477#issuecomment-1115745728,1,['avoid'],['avoiding']
Safety,"Here's the docstring before this PR. ```julia; help?> Oceananigans.Fields.FunctionField; FunctionField{LX, LY, LZ}(func, grid; clock=nothing, parameters=nothing) where {LX, LY, LZ}. Returns a FunctionField on grid and at location LX, LY, LZ. If clock is not specified, then func must be a function with signature func(x, y, z). If clock is specified, func must be a; function with signature func(x, y, z, t), where t is internally determined from clock.time. A FunctionField will return the result of func(x, y, z [, t]) at LX, LY, LZ on grid when indexed at i, j, k. ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────. FunctionField{LX, LY, LZ}(func::FunctionField, grid; clock) where {LX, LY, LZ}. Adds clock to an existing FunctionField and relocates it to (LX, LY, LZ) on grid. ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────. FunctionField(L::Tuple, func, grid). Returns a stationary FunctionField on grid and at location L = (LX, LY, LZ), where func is callable with signature func(x, y, z).; ```. Three docstrings come up. But we only want users to use one of them. Moreover the second one is actually redundant with the first and provides no new information. The last one is supposed to be for internal use only. But as a user who is new to julia (perhaps), you may see only the last docstring. This is what happened over on the discussion linked at the top.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3678#issuecomment-2269968189:1270,redund,redundant,1270,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3678#issuecomment-2269968189,1,['redund'],['redundant']
Safety,"Hi @tomchor There are a few examples of SGRID datasets listed on the original [xgcm sgrid issue](https://github.com/xgcm/xgcm/issues/109). I can also dig these out on my computer and share via email if you need - let me know. When you say documentation, is that the general [SGRID documentation](https://sgrid.github.io/sgrid/), or the [specific xgcm documentation relating to SGRID](https://xgcm.readthedocs.io/en/latest/grids.html#detecting-axes-from-dataset-attributes)?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1334#issuecomment-1516623832:433,detect,detecting-axes-from-dataset-attributes,433,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1334#issuecomment-1516623832,1,['detect'],['detecting-axes-from-dataset-attributes']
Safety,"Hi all, I recently switched from using v0.30.0 to using the latest version of the master branch, and I am now unable to add the CuArrays package, as it seems to clash with some of the packages used by Oceananigans. Has anyone else experienced this? I was able to reproduce this issue on two computers. When reverting back to v0.30.0, I have no such problem and can add CuArrays without packages clashing. Thanks for letting me know if you get the same issue!. Full error message suggest that Adapt, GPUArrays, CUDA and NNlib are clashing with CuArrays but I am a beginner at Julia so not sure I interpret the error correctly. I tried removing Oceananigans and all dependencies, installing CuArrays first and then adding Oceananigans, but I get the same problem doing things in that order. ```; ERROR: Unsatisfiable requirements detected for package CuArrays [3a865a2d]:; CuArrays [3a865a2d] log:; ├─possible versions are: [0.2.1, 0.3.0, 0.4.0, 0.5.0, 0.6.0-0.6.2, 0.7.0-0.7.3, 0.8.0-0.8.1, 0.9.0-0.9.1, 1.0.0-1.0.2, 1.1.0, 1.2.0-1.2.1, 1.3.0, 1.4.0-1.4.7, 1.5.0, 1.6.0, 1.7.0-1.7.3, 2.0.0-2.0.1, 2.1.0, 2.2.0-2.2.2] or uninstalled; ├─restricted to versions * by an explicit requirement, leaving only versions [0.2.1, 0.3.0, 0.4.0, 0.5.0, 0.6.0-0.6.2, 0.7.0-0.7.3, 0.8.0-0.8.1, 0.9.0-0.9.1, 1.0.0-1.0.2, 1.1.0, 1.2.0-1.2.1, 1.3.0, 1.4.0-1.4.7, 1.5.0, 1.6.0, 1.7.0-1.7.3, 2.0.0-2.0.1, 2.1.0, 2.2.0-2.2.2]; ├─restricted by compatibility requirements with Adapt [79e6a3ab] to versions: [0.2.1, 0.3.0] or uninstalled, leaving only versions: [0.2.1, 0.3.0]; │ └─Adapt [79e6a3ab] log:; │ ├─possible versions are: [0.3.0-0.3.1, 0.4.0-0.4.2, 1.0.0-1.0.1, 1.1.0, 2.0.0-2.0.2] or uninstalled; │ └─restricted to versions 2 by Oceananigans [9e8cae18], leaving only versions 2.0.0-2.0.2; │ └─Oceananigans [9e8cae18] log:; │ ├─possible versions are: 0.30.0 or uninstalled; │ └─Oceananigans [9e8cae18] is fixed to version 0.30.0; ├─restricted by compatibility requirements with GPUArrays [0c68f7d7] to versions: 0.2.1",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/796:828,detect,detected,828,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/796,1,['detect'],['detected']
Safety,"Hi both, Thanks for the quick replies on this. I agree that it seems like it is likely coming from the background gradient and lack of periodicity when applied to the hydrostatic presssure. As far as I can tell there aren't any problems for passive scalars which supports that interpretation. As an aside, I think that there are also boundary artifacts in the internal_wave.jl example (although you need to run the simulation longer and you can't see them with the default contouring of the plots). . A solution that should work is to implicitly cancel the hydrostatic pressure gradient associated with the background field with the buoyancy term. This is what we do in Diablo (although we don't decompose the pressure into hydrostatic and nonhydrostatic components). Implementing this is simple since you just don't include the background gradient when calculating the hydrostatic pressure. The only trouble that I see is figuring out when to do this. In other contexts you want to keep this term. E.g. in the geostrophic adjustment problem, the hydrostatic pressure gradient is needed to drive the flow. Maybe it would be safe to exclude the background buoyancy field in the calculation of the hydrostatic pressure anytime when the topology is periodic in the vertical direction?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3364#issuecomment-1783557267:1124,safe,safe,1124,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3364#issuecomment-1783557267,1,['safe'],['safe']
Safety,"Hi folks. I just saw this issue. We are very glad that you're working to support interoperability btw oceananigans and xgcm! 🎉 We'd love to help however we can. Ideally you would not have to really do much here other than use CF conventions in your netCDF output and things would ""just work."" That's the beauty of standards. Unfortunately, CF conventions don't quite provide the right vocabulary to describe the curvilinear geometry of staggered grid models compactly (see https://github.com/cf-convention/discuss/issues/5). In the meantime, every modeling center seems to have their own preference for how to encode this (e.g. [comodo conventions](https://web.archive.org/web/20160417032300/http://pycomodo.forge.imag.fr/norm.html) [now offline] used by ROMS and NEMO, [S-grid](https://github.com/sgrid/sgrid), [mosaics](https://extranet.gfdl.noaa.gov/~vb/talks/grids-short.pdf) from GFDL). . With xgcm, we decided to use the Comodo conventions (rather than invent yet another new convention). In retrospect, this was maybe the wrong choice, since the pycomodo project seems to have totally disappeared. 🤦 However, if you put the [right metadata](https://xgcm.readthedocs.io/en/latest/grids.html#detecting-axes-from-dataset-attributes) in your attributes, xgcm should be able to figure out your grid. Whatever you do, please try your best to squeeze your data into existing standard file formats and metadata conventions. ; Don't invent something new. MITgcm did this with the mds data format and it has been endless headaches for our community. I don't know what JLD2 is, but it sounds like you could be going down that route... If you have any questions, please ask! I'll try to respond.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1334#issuecomment-777066134:1197,detect,detecting-axes-from-dataset-attributes,1197,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1334#issuecomment-777066134,1,['detect'],['detecting-axes-from-dataset-attributes']
Safety,"Hi, PencilArrays / PencilFFTs author here. I'd be happy to help solving this issue, and I'd also welcome any ideas that would help improve the PencilArrays interface and docs. I am not familiar with how you define your domain partition. However, from the point of view of PencilFFTs, there is one restriction, which is that the first dimension (`x`) must *not* be decomposed in physical space. This is because FFTs are first performed along this dimension (along which data is contiguous, as usual for Julia arrays). Also, to avoid potential issues, your eigenvalues should be `PencilArray` wrappers. For now, PencilArrays allows broadcasting together `PencilArray`s and regular `Array`s, but I'm thinking this is not a good idea since the behaviour is non-intuitive and can lead to precisely this kind of issue. So I'm thinking about changing the current behaviour in the future -- and improving documentation on this. Finally, when you write:. ```julia; xc = b = solver.storage[2]; ```. you probably mean `xc = solver.storage[3]` (or, equivalently, `last(solver.storage)`, as in [this example](https://jipolanco.github.io/PencilFFTs.jl/dev/generated/in-place/#Applying-plans)). `storage[2]` holds an intermediate state that has no ""physical"" meaning, since it has been overwritten by the end of a transform. I should clarify all this in the in-place transforms example... Feel free to ping me if things are unclear in the docs or if I can provide any other information.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2347#issuecomment-1102319088:526,avoid,avoid,526,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2347#issuecomment-1102319088,1,['avoid'],['avoid']
Safety,"Hi,. I have been confused about what the `max_change` and `min_change` means.; I understand from the documentation that this is to avoid the time step to change too quickly, but how is it used?. Is it in time units? e.g., taking the default `max_change` of 1.1, that means that for dated simulation it will change the time step at max in 1.1 seconds? . Is it a multiplication? e.g., `new_time_step = 1.1*time_step` ?. I think we can try to be more clear about that in the docs.",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3244:131,avoid,avoid,131,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3244,1,['avoid'],['avoid']
Safety,Hm... I also noticed they were defined here:; https://github.com/CliMA/Oceananigans.jl/blob/a3faff771f3dec60be12cc7fab8ebabeffc1657e/test/test_time_stepping.jl#L202; so I thought the code I removed was redundant. But perhaps some tests were using that... I'll look into it.,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2016#issuecomment-946342621:202,redund,redundant,202,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2016#issuecomment-946342621,1,['redund'],['redundant']
Safety,"Hmm... I don’t think this is a typo. The functions *_gradient for “west”, “south”, and “bottom” are *identical*. Therefore we should use the alias “left” for all three to avoid copy-pasting identical functions. The same applies to the three “right” boundaries: “east”, “north”, and “top”. If we want to use matching names in other functions then we should use ‘const’ aliases. I think copy-pasting function definitions is bad practice that decreases maintainability and can make bugs harder to catch and fix.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/690#issuecomment-596539100:171,avoid,avoid,171,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/690#issuecomment-596539100,1,['avoid'],['avoid']
Safety,"Hmm... we'd have to convert this to an array, and then convert back for this to work automatically. A simple fix to start could just be to avoid saving boundary conditions when they contain references to an array. Is it possible to use `has_reference(CuArray, boundary_conditions)`?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/420#issuecomment-532780341:139,avoid,avoid,139,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/420#issuecomment-532780341,1,['avoid'],['avoid']
Safety,"Hmmm looks like it was `TEST_GROUP=integration` that went from taking ~20 minutes to ~32 minutes, enough to cause timeouts. We can live with this for now I suppose, can look into it in the future.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/860#issuecomment-681986752:114,timeout,timeouts,114,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/860#issuecomment-681986752,1,['timeout'],['timeouts']
Safety,"Hmmm that might be it. I tried to construct an analytic solution that obeys all the boundary conditions: using the source term **f(x,y,z) = 4(x²+y²-1-π²/H²) cos(2πz/Lz) exp(-(x²+y²))** the analytic solution is **ϕ(x,y,z) = cos(2πz/H) exp(-(x²+y²))** which does satisfy ∂ϕ/∂z = 0 at z=0 and z=H, and is ""numerically periodic"" as the exponential should decay to < machine epsilon at the boundaries in a large enough domain, but maybe I did something wrong in the process. I'll try testing with a different analytic solution. I wanted to avoid a function that is just sines and cosines as that would give a spectral solution that is accurate to within machine precision, which doesn't help with testing convergence properties.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/7#issuecomment-439692344:535,avoid,avoid,535,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/7#issuecomment-439692344,1,['avoid'],['avoid']
Safety,"Hmmm, doesn't rectilinear imply orthogonal?. `RegularRectilinearOrthogonalGrid` might be a little redundant?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1386#issuecomment-782295162:98,redund,redundant,98,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1386#issuecomment-782295162,1,['redund'],['redundant']
Safety,"Hmmm, right so if a boundary condition uses the `discrete_form=true` then it might require access to `state(model)` to fill halos, and if it `depends_on` other fields, then it needs access to them. I agree that we don't want to make invasive changes where fields depend on other fields and fields have extra properties that link to other big objects... It would make the code less modular I think. I think the current field abstraction is pretty lean so it might be nice to avoid bloating it. Would it make sense to add these dependencies in the boundary condition or `BoundaryFunction`? Then filling halos just calls the boundary condition which has immediate access to the state/fields it needs?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/971#issuecomment-698403877:474,avoid,avoid,474,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/971#issuecomment-698403877,1,['avoid'],['avoid']
Safety,"Hmmm, to avoid weaving together diagnostics and output writers maybe we can add a `Simulation` property called `align_diagnostics` or `align_output` (`true` by default) that ensures that diagnostics and output writers are called at exact intervals, rather than when the model clock time exceeds an interval. Unless you have tons of diagnostics with different interval frequencies it shouldn't kick in too often. Plus, you probably want proper output. Then we can define a function like; ```julia; time_to_next_action(time, diag) = diag.previous + diag.interval - time; ...; if simulation.align_diagnostics; for diag in simulation.diagnostics; Δτ = time_to_next_action(simulation.model.clock.time, diag); Δτ < Δt && (Δt = Δτ); end; end; ```; for diagnostics and output writers. This way no diagnostic or output writer will miss its target.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/501#issuecomment-586292471:9,avoid,avoid,9,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/501#issuecomment-586292471,1,['avoid'],['avoid']
Safety,"How about `PrimitiveSolutionLinearHeight`? It might be redundant to refer to the surface as ""free"" in the context of the shallow water equations.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/1339#issuecomment-772789508:55,redund,redundant,55,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/1339#issuecomment-772789508,1,['redund'],['redundant']
Safety,"How about this?. ```julia; @warn ""Inflating model grid halo size to ($Hx, $Hy, $Hz) and recreating grid. "" *; ""The model grid will be different from the input grid. To avoid this warning, "" *; ""pass halo=($Hx, $Hy, $Hz) when constructing the grid.""; ```",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1501#issuecomment-804961658:168,avoid,avoid,168,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1501#issuecomment-804961658,1,['avoid'],['avoid']
Safety,"I actually appreciate the verbose-ness of the grid show functions (actually most of Oceananigans show functions in general). So I can only assume that more beginner users appreciate that as well, since it makes extremely clear what the grid characteristics are. I also don't think there's a very important downside to being verbose here, so I'd vote to keep the same style. Although I agree that we could get rid of some redundant information. My suggestion would be something like:. ```julia; julia> grid = RectilinearGrid(size=3, z=[0.0, 0.2, 0.5, 1.0], topology=(Flat, Flat, Bounded)); RectilinearGrid{Float64, Flat, Flat, Bounded} on the CPU(); domain: x ∈ [1.0, 1.0], y ∈ [1.0, 1.0], z ∈ [0.0, 1.0]; topology: (Flat, Flat, Bounded); size (Nx, Ny, Nz): (1, 1, 3); halo (Hx, Hy, Hz): (0, 0, 1); z spacing: Stretched, with min=0.2, max=0.5; ```. In other words, I'd remove the flat dimensions (since that's unnecessary), and clean and align the `z` spacing statement. I'd also keep the ` topology: (Flat, Flat, Bounded)` because the line `RectilinearGrid{Float64, Flat, Flat, Bounded}` might not be very intuitive to everyone. If a shorter show is necessary in some places maybe we could create something like `short_show`? (Or maybe define `show` differently based on the display?: https://schurkus.com/2018/01/05/julia-print-show-display-dump-what-to-override-what-to-use/)",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2064#issuecomment-971810297:421,redund,redundant,421,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2064#issuecomment-971810297,1,['redund'],['redundant']
Safety,I added a `.gitignore` to the `gh-pages` branch so we should be good now. This PR now deletes leftover JLD2 files before deploying (just to be safe and also to unclutter the CI server in case we need to cache files between builds).,MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/558#issuecomment-563309498:143,safe,safe,143,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/558#issuecomment-563309498,1,['safe'],['safe']
Safety,"I agree that `z_faces` is clearer! I just wanted to be sure we were talking about the same changes. I believe that specifying the grid with `z_faces` does indeed require knowledge of the number of grid points, because the user specification depends on the vertical _index_. The case of a uniform grid illustrates this point: `z_faces = k -> (k - 1) * Lz / (Nz + 1)` for `z = (0, Lz)`. Note that the keyword argument `z` is redundant, since it can be inferred from `z_faces` (by evaluating `z_faces(1)` and `z_faces(Nz+1)`. Thus I am not sure I agree that we should require the `z` keyword argument when `z_faces` is supplied. I think it'd be nice to have both a ""discrete"" interface that uses the `z_faces` or `z_cell_interfaces` keyword argument, as well as a ""continuous"" interface that accepts a ""mapping"" or ""stretching"" function as I described above. With a ""continuous"" interface, the interval kwarg `z` is required because it cannot be inferred from `z_stretching` or `z_map` (whichever we prefer).",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/1551#issuecomment-815159490:423,redund,redundant,423,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/1551#issuecomment-815159490,1,['redund'],['redundant']
Safety,"I agree that it'd be good to have a different name for them to avoid confusion. I think `thermodynamics` is not very intuitive, but I do agree with the name `active_tracers`. We would need to change the name `SeawaterBuoyancy` though, like you mentioned.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2022#issuecomment-949813544:63,avoid,avoid,63,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2022#issuecomment-949813544,1,['avoid'],['avoid']
Safety,"I agree that your example does assign the first halo points correct. . We are using the main branch now and indeed we are trying to use higher order schemes near the boundary, a bad idea, and we will try and avoid that. . I looked over #2603 and can you point out where, say 3rd order upwinding switches to 1st order near the boundaries? That should help us to modify our forcing function appropriately. A more general piont is, if we never want to use the extra two halo points, why do we define them? Wouldn't it be more cost efficientive to only define them in the periodic case, and have smaller halos in the bounded case?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/2650#issuecomment-1182107240:208,avoid,avoid,208,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/2650#issuecomment-1182107240,1,['avoid'],['avoid']
Safety,"I also don't feel we can motivate this feature unless we understand the specific thing we are going for and ensure that it can't be achieved otherwise. It has wider implications too, because if we can't move away from the `materialize_x` pattern, then we can commit to supporting it. We've discussed at various times trying to eliminate the `materialize_x` step, and ways to avoid having it. A similar example pertains to grids and halos. We've decided that it's best to throw an error when the grid halo is not adequate for the chosen advection scheme. Previously, we had the notion that we might ""automatically expand"" the grid within the model constructor if the halo was not correct. So, let's discuss.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3262#issuecomment-1717609010:375,avoid,avoid,375,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3262#issuecomment-1717609010,1,['avoid'],['avoid']
Safety,"I also reported this on https://github.com/CliMA/Oceananigans.jl/issues/3056, so maybe we should close this to avoid multiple issues of the same problem. I don't have a PR specifically trying to fix that, but it is possible that https://github.com/CliMA/Oceananigans.jl/pull/3606 fixes, or least ameliorates, the issue. @mncrowe can you test your simulation on that branch and check if you see the same behavior?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3614#issuecomment-2150439493:111,avoid,avoid,111,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3614#issuecomment-2150439493,1,['avoid'],['avoid']
Safety,"I also think it'd be fun to try to automatically detect the appropriate device. Something like. ```julia; function GPU(); has_cuda_device = has_cuda(); has_amd_device = AMDGPU.has_rocm_gpu(); has_cuda_device && has_amd_device && # this user is blessed with CUDA _and_ ROCm!!; throw(ArgumentError(""Both a CUDA _and_ an AMD GPU are detected! One of them must be selected for the GPU architecture"")). has_cuda_device && return CUDAGPU(); has_amd_device && return ROCMGPU(). throw(ArgumentError(""Cannot build GPU architecture because neither a CUDA GPU nor an AMD GPU were detected!"")); end; ```",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2494#issuecomment-1112418189:49,detect,detect,49,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2494#issuecomment-1112418189,3,['detect'],"['detect', 'detected']"
Safety,"I am fine either way. Documentation is the important part in my opinion. In general, I think defaults are important for useability. However, if using a default isn't helpful for useability, then I agree it should be avoided.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/459#issuecomment-581892251:216,avoid,avoided,216,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/459#issuecomment-581892251,1,['avoid'],['avoided']
Safety,"I am in favor of this change but I think Jeanmichel might not be. J-M?. On Tue, Aug 27, 2019, 8:27 PM Gregory L. Wagner <notifications@github.com>; wrote:. > Idea:; >; > - continue to use calculate_boundary_source_terms to add fluxes; > specified via Flux boundary conditions (and also via potential new; > future boundary condition types), *but*; > - use halo filling + interior source term calculation, rather than; > calculate_boundary_source_terms, to enforce Value and Gradient; > boundary conditions; >; > Advantages:; >; > - this change eliminates the need for diffusivities to be known by; > calculate_boundary_source_terms --- diffusivities are only needed if; > enforcing Flux boundary conditions via halos, or to enforce Value/; > Gradient boundary conditions via calculate_boundary_source_terms; both; > of these situations are avoided with this change;; > - nonlinear diffusivities that depend on gradients at the boundary can; > be calculated correctly for Value and Gradient boundary conditions; > - gradient information is now included in output for Value and Gradient; > boundary conditions and can be used in post-processing; > - the calculate_boundary_source_terms function remains a part of the; > algorithm for Flux boundary conditions, or more complicated boundary; > conditions (like those associated with irregular bathymetry).; >; > Previously, we were operating under the assumption that we should either; > fill halos for all inhomogeneous boundary condition types, or use; > calculate_boundary_source_terms. For some reason, we did not consider; > that our method of enforcing inhomogeneous boundary conditions could (and; > perhaps should) depend on the *type* of the boundary condition.; >; > Thoughts?; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/climate-machine/Oceananigans.jl/issues/371?email_source=notifications&email_token=AKXUEQQKF52Q6OSJGERBYOLQGXA6XA5CNFS",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/371#issuecomment-525534795:840,avoid,avoided,840,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/371#issuecomment-525534795,1,['avoid'],['avoided']
Safety,"I am not very familiar with this use of scan, and despite ""cumulative sum"" sounding redundant, it express to me better the idea that I am not simply summing all values of a series. I think that this is the reason why `cumsum` is so common.; I agree that we might not find a perfect word for that... haha; However, I was thinking that `CumulativeScan` also works. Scan gives this idea of an action moving through the series and Cumulative suggests that we are summing terms as we move through.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3590#issuecomment-2105535912:84,redund,redundant,84,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3590#issuecomment-2105535912,1,['redund'],['redundant']
Safety,"I attempted to reproduce the issue using the 1D diffusion example in the same environment, but I was unable to do so. After picking up the checkpoint, the output saving interval looked normal (not saving every iteration). The simple example is demonstrated as follows: [here](https://github.com/liuchihl/internal-tide-mixing/blob/3D-realtopo-delta-glw-background-flux-div/oneD_diffusion_checkpoint_test.jl). . Our initial guess is that it might be related to #3056. However, after conducting some tests, such as avoiding setting intervals to transcendental numbers, the output saving interval after picking up the checkpoint is still 1 iteration for a while (which is not the desired behavior). I noticed that when I use `IterationInterval` instead of `TimeInterval`, the problem is resolved.",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/3657#issuecomment-2244122542:512,avoid,avoiding,512,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/3657#issuecomment-2244122542,1,['avoid'],['avoiding']
Safety,"I can approve this but I am concerned about the benchmarking being contained in another package. What's the reason for this? For example, we already have a `benchmark` directory in the code, which not only has some custom source code in `benchmark/src` but many benchmarking scripts. It seems that rather than address the issue with this code (which has a lot of stale stuff) we are trying to skirt / avoid the problem by creating another repo. Ultimately though this just leads to lower quality code across the board since its not clear whether we are supposed to continue to maintain `benchmark` or move to `NESAPOceananigans`. Eventually _somebody_ will have to address this, right? Who will do that?",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3658#issuecomment-2246632050:401,avoid,avoid,401,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3658#issuecomment-2246632050,1,['avoid'],['avoid']
Safety,"I can create a separate issue to address this, but perhaps more importantly, I don't see the interior velocity `V∞` being added anywhere there except in the BCs. Usually we add it as a background field to avoid inertial oscillations, and indeed it used to be done like that in [older versions of the docs](https://clima.github.io/OceananigansDocumentation/v0.73.8/generated/tilted_bottom_boundary_layer/), but at some point this was changed and I don't really understand why (nor can I pinpoint where). I think it's okay to run the example without adding it as a background field, but then we're solving the problem in a reference frame that's moving with the interior flow and that should be made explicit, and I don't see that explanation there. Personally, I'm in favor of including `V∞` as a background field because it's simpler. CC @hdrake @glwagner who were in https://github.com/CliMA/Oceananigans.jl/pull/3581",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/3813#issuecomment-2388670415:205,avoid,avoid,205,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/3813#issuecomment-2388670415,1,['avoid'],['avoid']
Safety,"I can help with some of this. You can avoid code replication by writing a 'translator' function for the GPU case --- you can use the ordinary `Grid` constructor to build a `Grid` object on the CPU, and then in a line or two 'translate' that object into an object with CuArrays. See, for example, the [strategy employed by `CuGeophysicalFlows.TwoDTurb`](https://github.com/FourierFlows/CuGeophysicalFlows.jl/blob/a5eb6a84cedbe2c120d40a9c8335812e2cc28205/src/twodturb.jl#L52).",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/45#issuecomment-462809816:38,avoid,avoid,38,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/45#issuecomment-462809816,1,['avoid'],['avoid']
Safety,"I cannot get the checkpointing test running in PR #140 as JLD is not able to serialize the model to disk with forcing functions. We can go back to forcing arrays but we I think that's a bad idea as we should avoid increasing GPU memory usage. I believe that [JLD2.jl](https://github.com/JuliaIO/JLD2.jl) might be able to serialize functions to disk but it's not actively maintained anymore and their README says ""If your tolerance for data loss is low, JLD may be a better choice at this time."". If we can fix this and figure out how to serialize functions to disk, then we may also be able to serialize the FFTW and CuFFT plans to disk (although we might still want to reconstruct them as in case the model is restored on a different computer with a different architecture). Stacktrace:; ```julia; Deserializing model from disk: test_model_checkpoint_5.jld; error parsing type string Oceananigans.Forcing{Oceananigans.#zero_func,Oceananigans.#zero_func,Oceananigans.#zero_func,Oceananigans.#zero_func,Oceananigans.#zero_func}; Checkpointing: Error During Test at D:\Home\Git\Oceananigans.jl\test\runtests.jl:246; Got exception outside of a @test; syntax: incomplete: premature end of input; Stacktrace:; [1] eval at .\boot.jl:328 [inlined]; [2] eval at C:\Users\Ali\.julia\packages\JLD\1BoSz\src\JLD.jl:3 [inlined]; [3] _julia_type(::String) at C:\Users\Ali\.julia\packages\JLD\1BoSz\src\JLD.jl:983; [4] julia_type(::String) at C:\Users\Ali\.julia\packages\JLD\1BoSz\src\JLD.jl:30; [5] jldatatype(::JLD.JldFile, ::HDF5.HDF5Datatype) at C:\Users\Ali\.julia\packages\JLD\1BoSz\src\jld_types.jl:701; [6] read(::JLD.JldDataset) at C:\Users\Ali\.julia\packages\JLD\1BoSz\src\JLD.jl:370; [7] read_ref(::JLD.JldFile, ::HDF5.HDF5ReferenceObj) at C:\Users\Ali\.julia\packages\JLD\1BoSz\src\JLD.jl:502; [8] jlconvert(::Type{Model}, ::JLD.JldFile, ::Ptr{UInt8}) at C:\Users\Ali\.julia\packages\JLD\1BoSz\src\jld_types.jl:387; [9] read_scalar(::JLD.JldDataset, ::HDF5.HDF5Datatype, ::Type) at C:\Users\Ali\.julia",MatchSource.ISSUE,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/141:208,avoid,avoid,208,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/141,1,['avoid'],['avoid']
Safety,"I checked and this seems to work:. ```julia; julia> using Oceananigans. julia> grid = RectilinearGrid(size=(4, 4, 4), extent=(1, 1, 1));. julia> model = NonhydrostaticModel(grid=grid); NonhydrostaticModel{CPU, Float64}(time = 0 seconds, iteration = 0) ; ├── grid: RectilinearGrid{Float64, Periodic, Periodic, Bounded}(Nx=4, Ny=4, Nz=4); ├── tracers: (); ├── closure: Nothing; ├── buoyancy: Nothing; └── coriolis: Nothing. julia> noise(x, y, z) = 100+randn(); noise (generic function with 1 method). julia> set!(model, u=noise). julia> simulation = Simulation(model, Δt=100, stop_iteration=200); Simulation{typename(NonhydrostaticModel){typename(CPU), Float64}}; ├── Model clock: time = 0 seconds, iteration = 0; ├── Next time step: 1.667 minutes; ├── Elapsed wall time: 0 seconds; ├── Stop time: Inf years; ├── Stop iteration : 200.0; ├── Wall time limit: Inf; ├── Callbacks: typename(OrderedCollections.OrderedDict) with 4 entries:; │ ├── stop_time_exceeded => typename(Callback); │ ├── stop_iteration_exceeded => typename(Callback); │ ├── wall_time_limit_exceeded => typename(Callback); │ └── nan_checker => typename(Callback); ├── Output writers: typename(OrderedCollections.OrderedDict) with no entries; └── Diagnostics: typename(OrderedCollections.OrderedDict) with no entries. julia> simulation.callbacks[:nan_checker].func.erroring = true; true. julia> run!(simulation); [ Info: Initializing simulation...; [ Info: ... simulation initialization complete (454.946 μs); [ Info: Executing initial time step...; [ Info: ... initial time step complete (3.370 ms).; ERROR: time = 10000.0, iteration = 100: NaN found in field u. Aborting simulation.; Stacktrace:; [1] error(s::String); @ Base ./error.jl:33; [2] (::Oceananigans.Simulations.NaNChecker{NamedTuple{(:u,), Tuple{Field{Face, Center, Center, CPU, OffsetArrays.OffsetArray{Float64, 3, Array{Float64, 3}}, RectilinearGrid{Float64, Period....; ....; ```",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/pull/2087#issuecomment-986101637:1629,Abort,Aborting,1629,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/pull/2087#issuecomment-986101637,1,['Abort'],['Aborting']
Safety,"I definitely think we should make restoring from a checkpoint as seamless as possible within scripts. Ideally you would just have to change one line to restore from checkpoint and continue time stepping. A common use case is running a long simulation on a cluster with strict time limits (e.g. 12 hour job time limits). Basically you just keep submitting jobs with the same script in which case being able to automatically ""restart from the most recent checkpoint"" might be a useful feature. This could be done via something like; ```julia; restore_from_checkpoint(find_most_recent_checkpoint_file(""."")); ```. A couple of thoughts from discussion with @suyashbire1 yesterday:. 1. To avoid have to enclose sections of code that set initial conditions with `if model.clock.iteration == 0`, we can introduce `set_initial_condition!(model, ...)` alongside `set!(model, ...)` that only sets fields if `model.clock.iteration == 0`. 2. A `Simulation` type could make implementing something like this easier. Although I think adding another struct or additional similar terminology (`Model` vs. `Simulation`) isn't the way to go. I think we can implement all the ideas we want via a function like; ```julia; run_model!(model, time_step=TimeStepWizard(...), end_time=10day, progress_message=print_terse_message, ...); ```",MatchSource.ISSUE_COMMENT,CliMA,Oceananigans.jl,v0.93.2,https://github.com/CliMA/Oceananigans.jl/issues/602#issuecomment-579919139:683,avoid,avoid,683,https://clima.github.io/OceananigansDocumentation/stable,https://github.com/CliMA/Oceananigans.jl/issues/602#issuecomment-579919139,1,['avoid'],['avoid']
