quality_attribute,sentence,source,author,repo,version,id,keyword,matched_word,match_idx,wiki,url,total_similar,target_keywords,target_matched_words
Modifiability,"@lbergelson. Our simple s3 nio library is not currently open source, but we would be willing to make it so after testing it properly. We found that to get the s3 nio library work with GATK, a few minor changes needed to be made in the GATK source. This is especially true for the spark tools because, on AWS EMR Spark clusters, s3 uris can be treated exactly as if they are HDFS uris. Therefore, it was not quite as simple for us to just add the s3 nio library to the classpath and have everything work as expected. For that reason we put the project on hold until GATK is closer to release. Thanks,; David. ________________________________; From: Louis Bergelson <notifications@github.com>; Sent: Monday, July 31, 2017 11:57 AM; To: broadinstitute/gatk; Cc: David Brown; Mention; Subject: Re: [broadinstitute/gatk] update com.google.guava version (#3102). @david-wb<https://github.com/david-wb> Is your s3 plugin available as an open source plugin that others could use? We had another question about s3 support in gatk and I thought you might have some insight about it. —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub<https://github.com/broadinstitute/gatk/issues/3102#issuecomment-319146368>, or mute the thread<https://github.com/notifications/unsubscribe-auth/ABxO-d5XTtUyeAI0GzCFLP5eVGYiyQJEks5sThWegaJpZM4N31U->.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3102#issuecomment-319185834:907,plugin,plugin,907,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3102#issuecomment-319185834,2,['plugin'],['plugin']
Modifiability,"@ldgauthier 's error sounds like what I saw before when trying to run the joint genotyping pipeline. When I spoke about it with @ruchim she said that based on some experiments she did and conversations with the production team she thought it was a symptom VM's running under PAPI; Slack excerpt:. ```; rmunshi [9:59 AM]; Last night I reached out to the Pipelines API folks; I ran a script that outputs the external IP address of the VM every 5 mins; and I see that after about ~17 hours, the request fails with. `curl: (6) Could not resolve host: metadata.google.internal` (edited); ```. So we decided that there's an effective limit of about 17 hours for VMs managed by PAPI at which point either the network configuration or metadata process server on the VMs changes, causing these failures. I worked around the issue by increasing my scatter interval count such that no tasks took longer than the ~17 hours that seems to be the critical point.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-412938932:710,config,configuration,710,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-412938932,1,['config'],['configuration']
Modifiability,"@ldgauthier ; My thinking on not doing the larger tests was that the native code hasn't changed for this support, so performance and functionality shouldn't see anything unexpected. Additionally, we technically do ""incremental import"" whenever the import is batched currently. We're just extending that same paradigm to extend beyond the case where the initial GenomicsDBImport command is used. Of course, all of this is not to say I don't want to do the larger tests...just wondering if we could capture that in a separate issue? @droazen mentioned that there's a tentative plan for a new GATK release this week and we would like to have this feature in there, if you agree. We'll work in parallel on the performance testing you requested. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5970#issuecomment-518327043:288,extend,extending,288,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5970#issuecomment-518327043,2,['extend'],"['extend', 'extending']"
Modifiability,@ldgauthier @davidbenjamin please take a look. . Don't allow the number of lines changed intimidate you... (most are in test resource files). The first commit contains the actual main code changes. . The second and third commits update the test resources (where most of the changed lines come from) and test code. . The very last commit changes the default radius to 2... I was planning to set it to 0 since it is more parsimonious (less complex configuration) but it may well affect sensitivity and certainly changes the PL/QUAL values so I guess set the value two the current 2 (for PLs) is a safer and more conservative approach until we evaluate what is the optimal value for this parameter. . Perhaps @davidbenjamin would like to have a different default for Mutec. This is last minute change and may break some of the integration test so bear with me if that is the case. However I think you can start reviewing the code at this point.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6055#issuecomment-516992042:446,config,configuration,446,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6055#issuecomment-516992042,1,['config'],['configuration']
Modifiability,"@ldgauthier @jamesemery If `PossibleDeNovo` can't work with just founderIDs, then the constructor that takes a set of founderIDs should be removed (??) Or better yet, should the base `Pedigree` class be refactored to make more explicit the distinction between annotation subclasses that require the full pedigree and those that just require the founderIDs (perhaps as a separate PR) ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5663#issuecomment-463263757:203,refactor,refactored,203,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5663#issuecomment-463263757,1,['refactor'],['refactored']
Modifiability,"@ldgauthier Also, try running with the prebuilt jars (https://github.com/broadinstitute/gatk/releases/download/4.1.8.1/gatk-4.1.8.1.zip) and/or the latest docker image to eliminate your local build environment as a variable.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5568#issuecomment-663152910:215,variab,variable,215,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5568#issuecomment-663152910,1,['variab'],['variable']
Modifiability,"@ldgauthier As discussed in person, could you please pull out a `GnarlyGenotyperEngine` with a `VariantContext finalizeGenotypes(VariantContext)` public entry point, so that we can do gnarly genotyping in BigQuery? You can see the version @jonn-smith wrote here: . https://github.com/broadinstitute/gatk/blob/jts_bigquery_spark_example/src/main/java/org/broadinstitute/hellbender/tools/evoquer/GnarlyGenotyperEngine.java. (But note that the version above does not exactly match the latest version of your code -- it's just an example of the refactoring we'll need in this branch). I think once this is done, and the few comments I add just now are addressed, and this branch is updated to the latest and greatest version of your tool, this can be merged",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4947#issuecomment-495344843:541,refactor,refactoring,541,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4947#issuecomment-495344843,1,['refactor'],['refactoring']
Modifiability,"@ldgauthier I think at some point we removed example JSONs in both the CNV and M2 WDL directories. I believe the reasoning was that those JSONs were mostly non-informative templates that could just as easily be generated with `womtool inputs`; since they were also not tested (in contrast to the JSONs used by the Travis WDL tests), they had to be kept in sync manually. @davidbenjamin @LeeTL1220 can correct me if I'm wrong. In contrast, providing Jack's hyperparameters for WES via JSONs will actually be informative! However, we will inevitably run into some issues touched upon in https://github.com/broadinstitute/gatk/issues/4719. I agree that it would be desirable to set some default WES/WGS hyperparameters in the featured workspace. However, I hope this wouldn't require two separate workspaces for WES/WGS or any shenanigans like that. Ideally, this sort of thing could be covered at the tool level with argsets, as mentioned in that issue. @droazen any updates there?. In any case, I'm not sure having the JSON in this repo and not covered by any tests is what we want. ; Maybe @bshifaw can chime in? Are the featured workspaces covered by tests elsewhere? What is the current SOP for taking workflows from this repo, turning them into featured workspaces, and populating their configurations?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6017#issuecomment-505971851:1290,config,configurations,1290,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6017#issuecomment-505971851,1,['config'],['configurations']
Modifiability,"@ldgauthier Let me know once you've had the chance to do that `GnarlyGenotyperEngine` refactoring discussed above, and I'd be happy to give this a (hopefully) final look.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4947#issuecomment-499240904:86,refactor,refactoring,86,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4947#issuecomment-499240904,1,['refactor'],['refactoring']
Modifiability,"@ldgauthier Some parts of taking splitting MNPs at the end of HaplotypeCaller are easy: breaking eg one DNP at position n into a SNP at n and a SNP at n + 1, letting the SNPs inherit the PLs, AF, and AD (okay, this isn't quite right because a read might end in the middle of the MNP, but close enough) of the parent MNP. . . but the general problem of splitting annotations seems like it might be too tricky. I'm leaning toward instead just modifying `AssemblyBasedCallerGenotypingEngine.phaseCalls()`. It seems that this phasing relies very heavily on perfect phasing or anti-phasing and that even one questionable haplotype with incorrect phasing can spoil things. I would guess that we could improve the phasing by making some simple guess as to which haplotypes are real. Basically, the problem is that while HaplotypeCaller imposes ploidy on alleles, it does not do so on haplotypes, and so phasing information is diluted. With your permission I would like to merge this PR and open a new issue for improving `phaseCalls`. After all, the issue is fixed in M2, and HC now has a perfectly good MNP mode, with the caveat that it doesn't interact nicely with GVCF mode.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4650#issuecomment-384836262:175,inherit,inherit,175,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4650#issuecomment-384836262,1,['inherit'],['inherit']
Modifiability,"@ldgauthier Tests are still failing because I haven't updated test vcfs. Before I do so could I get your opinion? Here's a summary of the rationale behind each commit:. * fa34758 When we get to `realignReadToBestHaplotype` we have discarded bases outside of the assembly region but we still have hard clips in the read Cigar. This change forces us to keep those Cigar elements after realigning to the best haplotype so that we know how far past the assembly window the read extended.; * 6af7ad4 After the above change, `BaseQualityRankSum` was liable to look for discarded bases in the hard-clipped part of the read. This fixes that.; * 952d217 The `Clipping` annotation doesn't do anything. It counts the number of hard clips, but pre-this PR there are no hard clips because those bases don't get realigned to the best haplotype and post-this PR the ""hard clips"" are just bases outside the assembly region. As a placeholder I'm setting it to zero (note how this doesn't break any tests!) but really I think we should just get rid of it.; * This PR introduced some off-by-one errors in the depth annotation (but not the ADs). While looking into this I found an apparent bug where some reads that don't overlap a variant get counted in the depth. The issue was that we were counting clipped bases in the overlap. I don't think this is correct because by this point in the code we have unclipped soft clips and gone through local reassembly. Therefore, anything clipped here is a part of the read we truly don't believe belongs anywhere near the assembly region. This change alone breaks the tests with a few off-by-one DP fields.; * 8c51c0a Uses hard clips in the Cigar to correct read position annotations.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4956#issuecomment-400755806:474,extend,extended,474,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4956#issuecomment-400755806,1,['extend'],['extended']
Modifiability,"@ldgauthier Well, there is always the `@ExperimentalFeature` tag if you think it's appropriate in this case. But if @meganshand did a pass to clean up / refactor the old code, `@BetaFeature` might be the right label....",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8933#issuecomment-2261126897:153,refactor,refactor,153,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8933#issuecomment-2261126897,1,['refactor'],['refactor']
Modifiability,"@ldgauthier defer to you on this, but agree that it seems confusing/misleading, especially in the case of large ref blocks with highly variable depth",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7185#issuecomment-824151659:135,variab,variable,135,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7185#issuecomment-824151659,1,['variab'],['variable']
Modifiability,"@lucidtronix . Thanks for the explanation. Here are a few lines with the FILTER and INFO fields. Let me know if you need a larger subset. I would guess that since DP is the total depth for 5000 subjects instead of just 1 that would likely be the culprit. Maybe average depth across samples could be used for a replacement variable when there are multiple samples. . ```; VQSRTrancheSNP99.70to99.80 AC=8;AF=0.0008391;AN=9532;BaseQRankSum=-0.915;CNN_1D=-16.118;ClippingRankSum=0;DP=177452;ExcessHet=3.0267;FS=6.543;InbreedingCoeff=-0.002;MLEAC=8;MLEAF=0.0008391;MQ=71.41;MQRankSum=0.005;NEGATIVE_TRAIN_SITE;QD=10.23;ReadPosRankSum=0.483;SOR=1.039;VQSLOD=-3.08;culprit=MQRankSum; VQSRTrancheSNP99.70to99.80 AC=62,81;AF=0.006477,0.008462;AN=9570;BaseQRankSum=0.398;CNN_1D=-16.118;ClippingRankSum=0;DP=196764;ExcessHet=2.802;FS=0;InbreedingCoeff=-0.0026;MLEAC=61,67;MLEAF=0.006373,0.007;MQ=68.23;MQRankSum=-0.961;NEGATIVE_TRAIN_SITE;QD=3.96;ReadPosRankSum=-0.318;SOR=0.666;VQSLOD=-5.206;culprit=MQRankSum; VQSRTrancheSNP99.70to99.80 AC=117;AF=0.012;AN=9574;BaseQRankSum=0;CNN_1D=-16.118;ClippingRankSum=0;DP=203481;ExcessHet=1.8042;FS=0.593;InbreedingCoeff=0.0034;MLEAC=117;MLEAF=0.012;MQ=55.51;MQRankSum=1.32;NEGATIVE_TRAIN_SITE;QD=6.25;ReadPosRankSum=0.087;SOR=0.642;VQSLOD=-5.629;culprit=MQRankSum; VQSRTrancheSNP99.70to99.80 AC=21;AF=0.002193;AN=9574;BaseQRankSum=0;CNN_1D=-16.118;ClippingRankSum=0;DP=209533;ExcessHet=3.1253;FS=0;InbreedingCoeff=-0.0027;MLEAC=21;MLEAF=0.002193;MQ=57.8;MQRankSum=1.19;NEGATIVE_TRAIN_SITE;QD=4.35;ReadPosRankSum=-0.075;SOR=0.695;VQSLOD=-5.57;culprit=DP; VQSRTrancheSNP99.80to99.90 AC=1;AF=0.0001044;AN=9572;BaseQRankSum=-2.379;CNN_1D=-16.118;ClippingRankSum=0;DP=212253;ExcessHet=3.0108;FS=0;InbreedingCoeff=-0.0003;MLEAC=1;MLEAF=0.0001044;MQ=186.25;MQRankSum=0.084;QD=0.28;ReadPosRankSum=-0.743;SOR=0.798;VQSLOD=-17.7;culprit=MQ; VQSRTrancheSNP99.80to99.90 AC=5214;AF=0.545;AN=9570;BaseQRankSum=-1.071;CNN_1D=-16.118;ClippingRankSum=0;DP=276543;ExcessHet=160;FS=10.66;",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5101#issuecomment-412996579:322,variab,variable,322,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5101#issuecomment-412996579,1,['variab'],['variable']
Modifiability,"@lucidtronix Are the environment variables that you added to the Docker env essential to realize the speed 2x improvement ? I'm reluctant to just add them to the Docker env without understanding what they're doing and whether/how they impact other components. i.e., changing OPEN_MP thread affinity/pinning params etc. might impact the native Intel PairHMM implementation (also @samuelklee will these impact CNV) ? Another option is reduce the scope of them and set them only for the specific tool(s), possibly exposed as command line arguments. The ScriptExecutor has control over the python process' environment and could easily propagate them to the so they only affect the particular Python process. But the values would have to be provided somehow.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5725#issuecomment-475614790:33,variab,variables,33,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5725#issuecomment-475614790,1,['variab'],['variables']
Modifiability,"@lucidtronix Could you try this branch out by running your CNN tool and confirming that it runs to completion, produces correct output, and doesn't time out? It would be helpful if you could run it in a configuration that prior to this PR would reliably time out.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4757#issuecomment-388099208:203,config,configuration,203,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4757#issuecomment-388099208,1,['config'],['configuration']
Modifiability,"@lucidtronix It would be great to get this into the upcoming release later this week, but see my questions above about the intent of the environment variable settings, and whether they're essential to achieve the speedup. Setting OPEN_MP_NUM_THREADS for the entire docker will potentially impact the native PairHMM code, so we'll either need to remove these, narrow the scope to tool/WDL, or better understand the intent/impact of these on the native code.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5725#issuecomment-476605177:149,variab,variable,149,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5725#issuecomment-476605177,1,['variab'],['variable']
Modifiability,@lucidtronix Probably something's up with my configuration...; `; $ gcc -v; Using built-in specs.; COLLECT_GCC=gcc; COLLECT_LTO_WRAPPER=/Users/markw/anaconda/envs/py27/bin/../libexec/gcc/x86_64-apple-darwin11.4.2/4.8.5/lto-wrapper; Target: x86_64-apple-darwin11.4.2; Configured with: ./configure --prefix=/Users/ray/mc-x64-3.5/conda-bld/gcc-4.8_1477649012852/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac --with-gxx-include-dir=/Users/ray/mc-x64-3.5/conda-bld/gcc-4.8_1477649012852/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac/gcc/include/c++ --bindir=/Users/ray/mc-x64-3.5/conda-bld/gcc-4.8_1477649012852/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac/bin --datarootdir=/Users/ray/mc-x64-3.5/conda-bld/gcc-4.8_1477649012852/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac/share --libdir=/Users/ray/mc-x64-3.5/conda-bld/gcc-4.8_1477649012852/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac/lib --with-gmp=/Users/ray/mc-x64-3.5/conda-bld/gcc-4.8_1477649012852/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac --with-mpfr=/Users/ray/mc-x64-3.5/conda-bld/gcc-4.8_147764901285,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4742#issuecomment-391065177:45,config,configuration,45,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4742#issuecomment-391065177,3,"['Config', 'config']","['Configured', 'configuration', 'configure']"
Modifiability,"@lydiarck - Not yet our priorities shifted and I haven't had time to address this. . @jnktsj That's great to hear! I'm surprised because this is the first I've heard of it. If you have some time, I'd love to discuss it with you (and / or Niall and / or Carrie). I'm particularly interested in how you will incorporate new versions of the software as updates are made. Things are starting to slow down, so I should *actually* be able to start taking a look next or the following week. It's going to require refactoring several things deep in the Funcotator Engine, which is why it hasn't happened yet.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6289#issuecomment-913047919:506,refactor,refactoring,506,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6289#issuecomment-913047919,1,['refactor'],['refactoring']
Modifiability,"@magicDGS - thanks again for the help on this. yes, GATKReadFilterPluginDescriptor seems somewhat like the same concept as GATK3's PluginManager. Some of the not-yet-ported tools like VariantAnnotator used the plugin idea more heavily as well, so perhaps that'll get built out more when those are ported? I will work with that and perhaps try to pull out an AbstractPluginDescriptor. @droazen - in the earlier post you mentioned support on porting this. i think we're almost done, but it would be really helpful to test this using the same test data as GATK3 to ensure identical behavior. I dont know if there's any issues preventing sharing those files, but it seems like sharing those, at least privately wit us, should be a relatively straightforward thing. We're happy to do all that testing, if we could just get that test data. Sorry to keep bugging you on this point, but it'd be helpful if someone from GATK could reply on whether this is a possibility. Thanks.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/616#issuecomment-344022553:131,Plugin,PluginManager,131,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/616#issuecomment-344022553,2,"['Plugin', 'plugin']","['PluginManager', 'plugin']"
Modifiability,"@magicDGS Args like the config file that are truly optional (have no default value at all) do not show up in the command line or headers unless they're populated with some value. It should be pretty easy for ReadTools (which I think already has a common base class for its tools), to ensure a config file is never accepted by just precluding it via custom command line validation, or arg preprocessing. BTW, all tools built with GATK already have numerous common args that may or may not apply in a given tool context. For example, all of the ReadWalkers have a `--lenientVCFProcessing` arg. So I'm not even sure we need to make this hidden, since it will hide it from gatk users. My 2 cents. Others may feel differently.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4474#issuecomment-371819413:24,config,config,24,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4474#issuecomment-371819413,2,['config'],['config']
Modifiability,"@magicDGS Can you move the make*Transformer methods up to GATKTool (at some point we'll have a plugin at that level), and then also integrate these with AssemblyRegionWalker ? We'll want to do the Spark tools as well, but we leave that for a separate PR.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2085#issuecomment-289924338:95,plugin,plugin,95,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2085#issuecomment-289924338,1,['plugin'],['plugin']
Modifiability,@magicDGS Do you have an example of something you need to configure that doesn't work well with java properties?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3078#issuecomment-307798804:58,config,configure,58,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3078#issuecomment-307798804,1,['config'],['configure']
Modifiability,"@magicDGS Esotericsoftware's minLog is used by Esotericsoftware's kryo. So, they would have to extend the class.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2751#issuecomment-315767184:95,extend,extend,95,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2751#issuecomment-315767184,1,['extend'],['extend']
Modifiability,"@magicDGS I agree that a fully implemented AbstractPluginDescritor class is a good idea. Why not do this in a separate PR, and would this be most appropriate in Barclay? I would be happy to take a stab or let you. We should look at the usage pattern of ReadFilters, VariantEval's two plugins, VariantAnnotation and perhaps others I'm not thinking about.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-407469692:284,plugin,plugins,284,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-407469692,1,['plugin'],['plugins']
Modifiability,"@magicDGS I like your idea of making `TwoPassReadWalker` more configurable, with the ability to specify different filters/transformers/intervals per-pass, provided that tools that don't need this level of configuration don't have to override any additional methods.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4849#issuecomment-394446988:62,config,configurable,62,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4849#issuecomment-394446988,2,['config'],"['configurable', 'configuration']"
Modifiability,"@magicDGS I like your suggestion of factoring out a CountingFilter class that can be reused for both filter types. Ideally, I think we should first get https://github.com/broadinstitute/gatk/pull/2218/files in first (it has some minor changes to CountingReadFilter). Hopefully that will be soon. In the meantime, if you're so inclined, you might try doing the CountingFilter refactoring as part of this PR (which still needs tests), since you'll have two clients for it. That will definitely require some careful refactoring of the existing classes in order to retain the current CountingFilter behavior).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2195#issuecomment-261546961:375,refactor,refactoring,375,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2195#issuecomment-261546961,2,['refactor'],['refactoring']
Modifiability,"@magicDGS I made the return type `List<Object>` because the existing consumers that I know of (clp and docgen) have no explicit knowledge of the actual type used for any given plugin descriptor. The proper way to handle this would be to use the bounded type `List<? super T>`; if you just make it `List<T>`, then casual consumers like the clp would get a compile error for this:. List<Object> plugins = descriptor.getDefaultInstances();. However, other similar methods in the descriptor class aren't bounded (a couple of them should be); and I didn't want to include that change in the docgen PR so I stayed consistent with the existing methods. I really should fix this in a separate PR.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2362#issuecomment-276224076:176,plugin,plugin,176,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2362#issuecomment-276224076,2,['plugin'],"['plugin', 'plugins']"
Modifiability,"@magicDGS I'd strongly prefer not to introduce a read filter descriptor hierarchy if we can avoid it, as it will be tricky to get right, and add complexity. We definitely need to be able to extend the package list used by the descriptor to find plugins, but as you point out we'll be able to use the configuration mechanism for that. For before/after-analysis filters, I expect that we'll just add that directly to the existing plugin once we resolve https://github.com/broadinstitute/gatk/pull/2085 (which I hope to get to this week). I think the rest of the cases can be addressed by overriding makeReadFilter and providing custom behavior of filter merging. If this turns out to be something truly common, we could consider allowing the tool to inject an argument collection into the plugin.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2353#issuecomment-274970451:190,extend,extend,190,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2353#issuecomment-274970451,5,"['config', 'extend', 'plugin']","['configuration', 'extend', 'plugin', 'plugins']"
Modifiability,"@magicDGS I'm still missing why this wouldn't work for downstream projects (as long as they load Main or some Main-derived class). I think the owner config issue is different; for locale, we need to always force US. Can you verify this, or maybe provide more details about what case doesn't work ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3483#issuecomment-324622945:149,config,config,149,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3483#issuecomment-324622945,1,['config'],['config']
Modifiability,"@magicDGS It looks like you have triggered a few new compiler errors in the last branch, namely in the following places:. ```; /gatk/src/test/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/SVDiscoveryTestDataProvider.java:33: error: cannot find symbol; BaseTest.b38_reference_20_21, ReferenceWindowFunctions.IDENTITY_FUNCTION);; ^; symbol: variable BaseTest; location: class SVDiscoveryTestDataProvider; /gatk/src/test/java/org/broadinstitute/hellbender/tools/copynumber/formats/SampleLocatableCollectionUnitTest.java:30: error: cannot find symbol; private static final String TEST_SUB_DIR = toolsTestDir + ""copynumber/formats"";; ^; symbol: variable toolsTestDir; location: class SampleLocatableCollectionUnitTest; /gatk/src/test/java/org/broadinstitute/hellbender/tools/copynumber/utils/annotatedregion/SimpleAnnotatedGenomicRegionUnitTest.java:18: error: cannot find symbol; private static final String TEST_FILE = publicTestDir + ""org/broadinstitute/hellbender/tools/copynumber/utils/combine-segment-breakpoints-with-legacy-header-learning-combined-copy-number.tsv"";; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3475#issuecomment-341143259:353,variab,variable,353,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3475#issuecomment-341143259,2,['variab'],['variable']
Modifiability,"@magicDGS My take on these: I think the read filter plugin descriptor shouldn't be removed, since it also does filter merging, header propagation, etc. which need to be done even if you want to disable user command line control. (Also, if you remove it I would expect you'd get an NPE). So I would say that we should update the doc to say that you shouldn't remove it from the list, and should override `makeReadFilter` in the case where you want finer control. I know we talked a lot about the transformer issue a while back; I think the intention was that we would do the integration with the rest of the tool types as part of https://github.com/broadinstitute/gatk/issues/2160, but @droazen may recall differently.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4651#issuecomment-381279453:52,plugin,plugin,52,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4651#issuecomment-381279453,1,['plugin'],['plugin']
Modifiability,@magicDGS Perhaps a better solution to this problem would be to refactor the -L code to be able to handle queryname sorted files (we could produce a very similar filter to your filter and add it to the traversal early),MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5367#issuecomment-435175872:64,refactor,refactor,64,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5367#issuecomment-435175872,1,['refactor'],['refactor']
Modifiability,"@magicDGS Rather than have a special case for doc-only args, I think we should close this PR and include the config file arg as part of the arg collection you added in https://github.com/broadinstitute/gatk/pull/3998.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4474#issuecomment-377341373:109,config,config,109,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4474#issuecomment-377341373,1,['config'],['config']
Modifiability,"@magicDGS Sorry for the delay on these AssemblyRegion-related PRs. There is an effort at the Broad right now to validate the GATK4 `HaplotypeCaller` against the GATK3 version. Until this is complete, we're not accepting even minor changes to code on the critical path for the `HaplotypeCaller`, except for bug fixes that arise from the validation work. It's still possible that as a result of this validation work `AssemblyRegionWalker` may get refactored/altered to address problems discovered, so until we have a final version that produces acceptable results for `HaplotypeCaller` (and we're not quite there yet) other changes to that part of the codebase will have to wait. Sorry for the inconvenience -- once GATK4's `HaplotypeCaller` gets the official stamp of approval we will certainly find a way to get all of your changes in. In the mean time we have to ask you to be patient a little longer!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2371#issuecomment-287447471:445,refactor,refactored,445,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2371#issuecomment-287447471,1,['refactor'],['refactored']
Modifiability,"@magicDGS The GATK versioning scheme is not related to the API -- it is targeted at end users rather than projects using GATK as a library. Here's a slide that explains it:. <img width=""824"" alt=""gatk_versioning"" src=""https://user-images.githubusercontent.com/798637/38042254-e5bb85a4-3281-11e8-8d83-017bb6b73fda.png"">. As the slide mentions, we have given some thought to supplementing the main version number with an ""API version number"", but we'd have to more clearly define what constitutes the official public API for the GATK before doing so. On a side note, now that we're in general release it may be easier for you to get PRs for things like new walker types merged into the GATK proper, particularly if they are fairly self-contained and don't involve refactoring lots of engine classes. I was planning to ask whether you wanted to resurrect your `SlidingWindowWalker` PR at some point.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4603#issuecomment-376946968:762,refactor,refactoring,762,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4603#issuecomment-376946968,1,['refactor'],['refactoring']
Modifiability,"@magicDGS The HaplotypeCaller traversal has undergone some changes in the past few weeks to improve performance and bring the output of the tool closer to GATK3. There is now an `AssemblyRegionWalker` that divides the intervals into active and inactive regions, in a greatly simplified version of the GATK3 traversal. Initially, I did plan on having `AssemblyRegionWalker` extend the former `ReadWindowWalker`, or an adapted version of your `SlidingWindowWalker`, and I did implement it like this at first, but ultimately I collapsed it into a single class for several reasons:; - Inheriting from a more generic traversal type caused usability issues and confusion with respect to the command-line arguments. The `ReadWindow` was the unit of processing for the superclass, but for `AssemblyRegionWalker` it was the unit of I/O and `AssemblyRegion` was the unit of processing, and I couldn't update the docs for `ReadWindowWalker` to clear up the confusion without mentioning `AssemblyRegion`-specific concepts.; - The `ReadShard` / `ReadWindow` was/is **only** there to prove that we can shard the data without introducing calling artifacts, and to provide a unit of parallelism for the upcoming Spark implementation. It's not something we really want to expose to users as a prominent knob, and we may hide it completely in the future once the shard size is tuned for performance.; - Inheriting from a more abstract walker type caused a number of other problems as well: methods that should have been final in the supertype could no longer be made final, with the result that tool implementations could inappropriately override engine initialization/shutdown routines. Also, there were issues with the progress meter, since both the supertype traversal and subtype traversal needed their own progress meter for their different units of processing. Ultimately it was just too awkward and forced, and the read shard is something that we eventually want to make an internal/encapsulated implementation d",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210806513:373,extend,extend,373,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210806513,3,"['Inherit', 'adapt', 'extend']","['Inheriting', 'adapted', 'extend']"
Modifiability,"@magicDGS The problem with exposing the datasources to walkers is that they would be able to invalidate the entire traversal. For example, a `ReadWalker` could alter the traversal intervals on the reads datasource mid-way through traversal from within `apply()`, or it could cause the reads iterator used by the engine to get closed by issuing a separate `iterator()` call on the datasource, which would cause the rest of the traversal to fail. This is why I feel strongly that the datasource objects should not be directly accessible to walker-based tools. Note that it's still possible for walkers to create their own, separate datasources without reaching into the ones used by the engine, or a tool author can extend `GATKTool` directly rather than one of the walker base classes and have the freedom to access everything (which was not possible before this PR).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4964#issuecomment-401423305:714,extend,extend,714,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4964#issuecomment-401423305,1,['extend'],['extend']
Modifiability,"@magicDGS This seems like a good idea. I have a few comments.; 1. It seems like there should be an analogous function to `getPackageList()` where you set the list of individual classes, maybe `getNamedToolList()`.; 2. It seems like there's a design flaw in the `Main` class. I thought that the intent was that people could inherit from `Main` and override `getPackageList()` in order to substitute their own packages. But for some reason `getPackageList()` is static which is preventing that from happening. I think we should make it non-static which would allow a subclass of main to just override `getPackageList` and `getNamedToolList()`. What does your derived main class look like now?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2204#issuecomment-255832583:323,inherit,inherit,323,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2204#issuecomment-255832583,1,['inherit'],['inherit']
Modifiability,"@magicDGS We like the idea of making this configurable. We're not sure that this is the best mechanism for doing so though. We've been thinking about possibly including a standardized mechanism for configuring properties, i.e. some property file in the jar that could then be overridden by downstream projects. We don't know a great mechanism for doing that though. . Do you have any thoughts? Maybe using something like [commons-configuration](https://commons.apache.org/proper/commons-configuration/userguide/user_guide.html). It seems like we have a number of places that need this sort of configuration, and it would be good if we had a standard way of doing so instead of relying on little hacks for each instance.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2156#issuecomment-265240247:42,config,configurable,42,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2156#issuecomment-265240247,5,['config'],"['configurable', 'configuration', 'configuring']"
Modifiability,"@magicDGS We talked about this a bit today. I think we should get https://github.com/broadinstitute/gatk/pull/4469 finished and merged, and then rebase this on top of that. Then we should put the remaining arguments, including the ""doc-only"" config file arg, as well as the special arguments collection, into the interface and default implementation class that you've defined here. Then we can do a detailed review.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3998#issuecomment-377344569:242,config,config,242,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3998#issuecomment-377344569,1,['config'],['config']
Modifiability,"@magicDGS We talked with our ops people and it looks like we periodically purge anything older than 60 days, so this snapshot is gone. We're going to try to streamline what we upload, which will then allow us to extend the retention time for these things. Hopefully in the meantime you can rebuild what you need.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4565#issuecomment-375725088:212,extend,extend,212,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4565#issuecomment-375725088,1,['extend'],['extend']
Modifiability,"@magicDGS We will definitely be keeping the `GATKRead` interface around for the foreseeable future. When the HTSJDK 3.0 interfaces materialize we'll re-evaluate, but it's possible that we would continue to code against `GATKRead` even then, as our `SAMRecord` -> `GATKRead` adapter layer does some useful caching, and having our own interface has certain advantages (as well as disadvantages).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4166#issuecomment-358325854:274,adapt,adapter,274,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4166#issuecomment-358325854,1,['adapt'],['adapter']
Modifiability,"@magicDGS We've inherited customCommandLineValidation() from picard, but it never really caught on as a way to do things in GATK. The fact that it's output is inconsistent with the rest of the command line parsing is an accident and should be fixed. . I think the right thing to do here would be to make `customCommandLineValidation()` into a `void` method that either throws a `CommandLineException` or doesn't. It will take some changes to a few tools but it will make things less confusing in the long run. Then you can move the call to customCommandLineValidation to just after the call to `parseArgs` like you've done, but we can remove the custom code to print error messages since it will just be handled by the regular exception handling.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2226#issuecomment-255845846:16,inherit,inherited,16,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2226#issuecomment-255845846,1,['inherit'],['inherited']
Modifiability,"@magicDGS Yes, I think this should be closed in favor of a PR to make `ReadTransformers` into plugins after the merge of https://github.com/broadinstitute/gatk/pull/2085. The plugin framework implemented by @cmnbroad allows for plugin classes to themselves contain command-line arguments, so you could add an argument directly to the `MisencodedBaseQualityReadTransformer` to control whether misencoded quals should be fixed or generate an error.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2084#issuecomment-245992067:94,plugin,plugins,94,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2084#issuecomment-245992067,3,['plugin'],"['plugin', 'plugins']"
Modifiability,"@magicDGS after I did some refactoring for Mutect2 mitochondria GVCF output, this should be pretty easily doable by overriding the GVCFBlockCombiner. Can we close this?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1883#issuecomment-590349072:27,refactor,refactoring,27,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1883#issuecomment-590349072,1,['refactor'],['refactoring']
Modifiability,"@magicDGS is correct that no annotations are tagged as `@DocumentedFeature` yet, but thats just because nobody has done it. The plugin (which I think @jamesemery is planning to implement soon), is a completely separate issue - the `@DocumentedFeature` annotations can be added either way.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3809#issuecomment-342823633:128,plugin,plugin,128,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3809#issuecomment-342823633,1,['plugin'],['plugin']
Modifiability,"@magicDGS, is this the same Mac as the one in #1985? Assuming yes, this crash happens because AVX is not supported. We had a check for AVX in GKL 0.2.0, but the call was removed by mistake in 0.3.0. . Sorry about that, we'll fix it in the next GKL release. In the meantime, you can use the workaround from #2302 and define this environment variable: `export GKL_USE_LIB_PATH=1`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2315#issuecomment-267115817:340,variab,variable,340,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2315#issuecomment-267115817,1,['variab'],['variable']
Modifiability,@magicDGS. It looks like there's some sort of compilation error here:. ```; /gatk/src/test/java/org/broadinstitute/hellbender/tools/walkers/markduplicates/MarkDuplicatesGATKIntegrationTest.java:180: error: incompatible types: inference variable T has incompatible bounds; ((GATKDefaultCLPConfigurationArgumentCollection) markDuplicatesGATK.configArgs).TMP_DIR = CollectionUtil.makeList(outputDir);; ^; equality constraints: String; lower bounds: File; where T is a type-variable:; T extends Object declared in method <T>makeList(T...); /gatk/src/test/java/org/broadinstitute/hellbender/tools/walkers/markduplicates/MarkDuplicatesGATKIntegrationTest.java:256: error: incompatible types: inference variable T has incompatible bounds; ((GATKDefaultCLPConfigurationArgumentCollection) markDuplicatesGATK.configArgs).TMP_DIR = CollectionUtil.makeList(outputDir);; ^; equality constraints: String; lower bounds: File; where T is a type-variable:; T extends Object declared in method <T>makeList(T...); ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3998#issuecomment-355606477:236,variab,variable,236,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3998#issuecomment-355606477,8,"['config', 'extend', 'variab']","['configArgs', 'extends', 'variable']"
Modifiability,"@magicdgs Do you get all your dependencies from there, or only gatk? I assumed you had multiple repos configured so that it first resolves from central and then from artifactory. I'm not sure how to configure that sanely in maven, although I'm sure it's doable to someone who is less ignorant of maven then me.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3724#issuecomment-340474332:102,config,configured,102,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3724#issuecomment-340474332,2,['config'],"['configure', 'configured']"
Modifiability,"@magicdgs You could include this filter in ReadTools and the plugin would discover it - after all, thats part of the purpose of plugins ;-). Anyway, at a minimum we should make sure the doc clearly explains when/how to use this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5367#issuecomment-434681393:61,plugin,plugin,61,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5367#issuecomment-434681393,2,['plugin'],"['plugin', 'plugins']"
Modifiability,"@mattsooknah Are there any places you'd like us to look more carefully? I'm tempted to just inhale this as is and deal with refactoring later. But if there are places you modified more heavily, we should look a bit at them.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/347#issuecomment-89308937:124,refactor,refactoring,124,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/347#issuecomment-89308937,1,['refactor'],['refactoring']
Modifiability,"@mbabadi I've updated my PR to use miniconda3. @mbabadi @lucidtronix @samuelklee I think we should aim for tools that at least run out-of-box, without depending on any out-of-band configuration other than the conda env. On top of that we can provide guidance/configs for users on how to enable further optimizations, like g++. Does that sound like an achievable goal ?. As for the docker, we're going to have strike the right balance between image bloat and performance(including test performance). I think we're around 4+ gig now, and counting. Before the Python integration we were at 1.9G, and trying to find ways to reduce it. So lets see where we wind up but keep that in mind. Finally, we need to find a way to install the (GATK) python package(s) without depending on access to the GATK repo. Right now I think the gCNV branch has a ""pip install from source"" added to the conda env .yml. That will work on the docker at the moment (and thus on travis), but that won't work for non-docker users how don't have source/repo access. Also, one of the proposals to reduce the size of the docker is to remove the repo clone that is currently there. My proposal is that we change the gradle build to create an archive/zip of the python source (this would include the VQSR-CNN package code as well as gCNV kernel). We can then copy that on to the docker image, and pip-install it from the copy. That would retain the ability to always run travis tests based on the code in the repo, and also keep the nightly docker image in sync. We'll also have deliver the archive as an artifact somehow (perhaps including PyPi) for non-docker users.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3912#issuecomment-350303277:180,config,configuration,180,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3912#issuecomment-350303277,2,['config'],"['configs', 'configuration']"
Modifiability,"@mbabadi You can directly register more classes by adding them in GATKRegistrator. It doesn't fix the problem of needing custom configuration, but that's how we've been dealing with custom serializers, just put them all in all the time.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2337#issuecomment-272290322:128,config,configuration,128,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2337#issuecomment-272290322,1,['config'],['configuration']
Modifiability,"@mcovarr Follow-up on the codecov thing. Apparently codecov is just not configured correctly? I asked at an Engine Team meeting and it looks like we're just not excluding test files for some reason, so that explains why it's calling out my test file as not covered. We could change it to ignore that directory, since it makes sense to not check for test coverage on test files, but I'm hesitant to include it here because it's sort of outside the scope of this PR.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8708#issuecomment-2000123077:72,config,configured,72,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8708#issuecomment-2000123077,1,['config'],['configured']
Modifiability,"@mcovarr added two tests (and refactored a dataprovider, because I'm a good person) -- take another look?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7864#issuecomment-1152529144:30,refactor,refactored,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7864#issuecomment-1152529144,1,['refactor'],['refactored']
Modifiability,"@mdayii I'd suggest that you search the gatk [forum](https://gatk.broadinstitute.org/hc/en-us/community/topics) for similar topics, or post this issue there, to see if anyone there can help with your configuration.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7397#issuecomment-896003059:200,config,configuration,200,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7397#issuecomment-896003059,1,['config'],['configuration']
Modifiability,"@meganshand Latest build failed with:. ```; org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorIntegrationTest.testWithAllAnnotations FAILED; java.lang.AssertionError: Iterators differ at element [9]: FORMAT=<ID=PL,Number=G,Type=Integer,Description=""Normalized, Phred-scaled likelihoods for genotypes as defined in the VCF specification""> != FORMAT=<ID=NUMT,Number=1,Type=String,Description=""Potentially a polymorphic NuMT false positive rather than a real mitochondrial variant.""> expected [FORMAT=<ID=PL,Number=G,Type=Integer,Description=""Normalized, Phred-scaled likelihoods for genotypes as defined in the VCF specification"">] but found [FORMAT=<ID=NUMT,Number=1,Type=String,Description=""Potentially a polymorphic NuMT false positive rather than a real mitochondrial variant."">]; at org.testng.Assert.fail(Assert.java:93); at org.testng.Assert.failNotEquals(Assert.java:512); at org.testng.Assert.assertEqualsImpl(Assert.java:134); at org.testng.Assert.assertEquals(Assert.java:610); at org.testng.Assert.assertEquals(Assert.java:578); at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorIntegrationTest.assertHeadersMatch(VariantAnnotatorIntegrationTest.java:66); at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorIntegrationTest.runVariantAnnotatorAndAssertSomething(VariantAnnotatorIntegrationTest.java:92); at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorIntegrationTest.assertVariantContextsMatch(VariantAnnotatorIntegrationTest.java:55); at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorIntegrationTest.assertVariantContextsMatch(VariantAnnotatorIntegrationTest.java:49); at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorIntegrationTest.testWithAllAnnotations(VariantAnnotatorIntegrationTest.java:224); Exception in thread ""Thread-5"" java.nio.file.ClosedFileSystemException; 	at com.google.common.jimfs.FileSystemState.checkOpen(FileSystemState.java:64); ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5842#issuecomment-477769414:425,polymorphi,polymorphic,425,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5842#issuecomment-477769414,2,['polymorphi'],['polymorphic']
Modifiability,@mr-c I'm sorry this PR was forgotten about. Could you explain what it does? None of us are familiar with the `JAVAPATH` environment variable and googling didn't reveal anything useful.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3866#issuecomment-453586187:133,variab,variable,133,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3866#issuecomment-453586187,1,['variab'],['variable']
Modifiability,"@munrosa @ldgauthier Possible breakthrough. . First, what's definitely true about the het at 169510380 in 55_55003_F5region.bam when I reproduce the bug with `-L chr1:169510380 -ip 100`:. * The variant is considered active and triggers assembly, as it should.; * For every kmer size there are non-unique kmers in the reference, so it increases up to k = 85, the last attempt at which the engine relaxes the unique kmers requirement. (See `ReadThreadingAssembler` line 425).; * Once it reaches this kmer size, there are cycles in the graph and so no assembly is returned. (See `ReadThreadingAssembler` line 464). Thus no alt haplotype is discovered and the variant is missed. I believe there are two possible solutions.; * The assembly engine looks for cycles before pruning, but this order could be switched with no ill effects. In the case of this het there are no cycles after pruning because the apparent cycle was a poorly-supported path due to sequencing error. Here regular pruning works but the new `--adaptive-pruning` option would give a bit more security against false cycles.; * We don't actually have to check for cycles, especially in the last, desperate kmer attempt. Well, we do with the current recursive implementation of `KBestHaplotypeFinder`, but we *don't* in the Dijkstra's algorithm implementation currently under review: #5462. (Technical note: @ldgauthier I know I promised that this PR gives entirely equivalent results to the existing implementation, but technically this is only true if the existing implementation finishes in finite time. Due to the greedy -- but optimal -- nature of Dijkstra's algorithm, cycles do not cause issues). Personally, I am in favor of *both* solutions -- looking for cycles after pruning, and waiving the no-cycle requirement on the last attempt. They are complementary.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3697#issuecomment-446465913:1009,adapt,adaptive-pruning,1009,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3697#issuecomment-446465913,1,['adapt'],['adaptive-pruning']
Modifiability,"@mwalker174 Hi Mark, I've refactored the code significantly, would you take a look again? Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4602#issuecomment-386752928:26,refactor,refactored,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4602#issuecomment-386752928,1,['refactor'],['refactored']
Modifiability,"@mwalker174 Is encountering the same problem in the wild. He's reporting that it goes away if you specify the environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY. he's seeing the warning message:; ```; 16:55:09.480 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; ```; which *should* only appear during tests, so something is strange.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3591#issuecomment-330650894:122,variab,variables,122,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3591#issuecomment-330650894,3,"['config', 'variab']","['configured', 'variables']"
Modifiability,"@nalinigans: We now believe this is actually a GenomicsDB issue (or possibly an issue in the JNI layer.). @gokalpcelik was able to reproduce this problem on a set of 330 whole exomes. He found that if he ran GenotypeGenotypeGVCs from a GenomicsDB the memory usage climbed up to 10s of GB, but the java heap memory remained constant. He then tested firt extracting the combined GVCF from genomics db and then running GenotypeGVCFs and saw that memory usage for GenotypeGVCFs remained constant at 1 G. So we think this is probably a GenomicsDB issue. . GenomicsDBImport > GenotypeGVCFs ---- Memory ramps up immediately to 10s of gigabytes; GenomicsDBImport > SelectVariants to GVCF > GenotypeGVCFs ---- Memory is fixed at 1.1 GB. He can fill in more detail about the exact configuration if it helps.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8989#issuecomment-2432001934:771,config,configuration,771,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8989#issuecomment-2432001934,1,['config'],['configuration']
Modifiability,"@olavurmortensen, looks like TILEDB_DISABLE_FILE_LOCKING=1 did not get passed to the tool. Did you use `export TILEDB_DISABLE_FILE_LOCKING=1` as the command to set the environment variable?; If you have and see the issue, please try the attached zip that contains a shared library with some debug/tracing messages, so we can pinpoint the issue a little more.; [libgenomicsdb.zip](https://github.com/broadinstitute/gatk/files/2922767/libgenomicsdb.zip); To use the zip, from a bash shell:. ```; %: tar zxf libgenomicsdb.zip; %: export LD_LIBRARY_PATH=`pwd`:$LD_LIBRARY_PATH; %: export TILEDB_DISABLE_FILE_LOCKING=1; %: gatk --java-options ""-Xmx4g -Xms4g"" GenomicsDBImport ...; ```; Please attach the log if you still see the issue.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5740#issuecomment-468989614:180,variab,variable,180,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5740#issuecomment-468989614,1,['variab'],['variable']
Modifiability,"@olavurmortensen, yes please file a new issue. Please include your OS/platform version, your command, all the logs, any core dumps and steps to create a CIFS mount with your configuration in the issue.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5608#issuecomment-468371003:174,config,configuration,174,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5608#issuecomment-468371003,1,['config'],['configuration']
Modifiability,"@olavurmortensen, yes, this has been tested on our systems and the fix will only mitigate NFS type of issues. However, there is improved logging of system errors now and we would like your inputs with logs and the exact NFS/CIFS configuration if you still run into issues to help us further debug what is happening. Please note that TILEDB_DISABLE_FILE_LOCKING env variable has to be set to 1 when doing the GenomicsDBImport.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5608#issuecomment-458665946:229,config,configuration,229,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5608#issuecomment-458665946,2,"['config', 'variab']","['configuration', 'variable']"
Modifiability,@psfoley The branch is failing tests after your latest commit with the error:. ```; > Could not resolve all dependencies for configuration ':runtime'.; > Could not find com.intel:genomicsdb:0.9.2-proto-3.0.0-beta-1+uuid-static.; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4261#issuecomment-360870965:125,config,configuration,125,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4261#issuecomment-360870965,1,['config'],['configuration']
Modifiability,"@rdbremel This got missed in the churn of issues. Does this happen repeatedly or is it a 1 time occurrence? We've seen similar issues in the past and tried to wrap them all in layers of retries, but sometimes things slip through.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6182#issuecomment-547962085:176,layers,layers,176,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6182#issuecomment-547962085,1,['layers'],['layers']
Modifiability,"@rickymagner I'd recommend postponing any further refactoring for future PRs post-release, in the interests of checkpointing this feature -- assuming that we're confident the feature works as intended and there are no side effects in other HaplotypeCaller codepaths. If the new argument is not already marked as Experimental, perhaps it should be until it's seen some more usage?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8609#issuecomment-1847920221:50,refactor,refactoring,50,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8609#issuecomment-1847920221,1,['refactor'],['refactoring']
Modifiability,@ronlevine @lbergelson Have you guys thought about the Spark case - I don't think this will capture logging output from Spark workers unless the settings get propagated through the config (we also don't propagate the verbosity IIRC). It might be a little misleading to allow specification of a file for Spark tools if it doesn't capture all of the output.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2751#issuecomment-315897839:181,config,config,181,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2751#issuecomment-315897839,1,['config'],['config']
Modifiability,"@ronlevine I know this a port from gatk3, but I think theres a bit of refactoring that can be done. It seems like it's more complicated than it needs to be. Could you take a look and see? In particular I'm not sure why things get converted to a bitset, it looks like you should just be able to derive the indecies directly and avoid creating a bitset. If I'm missing some detail and it can't be simplified let me know.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1852#issuecomment-242102049:70,refactor,refactoring,70,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1852#issuecomment-242102049,1,['refactor'],['refactoring']
Modifiability,@ronlevine It looks to me that there could be some refactoring done here to separate out the AF update logic into a single function call that could be used in multiple places rather than having two sets of fairly complicated and highly similar logic. Could you see if that's doable?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1871#issuecomment-242121998:51,refactor,refactoring,51,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1871#issuecomment-242121998,1,['refactor'],['refactoring']
Modifiability,"@ronlevine Sorry for the slow response. I like it much more with the refactoring you did. I had a few minor comments. I want to ask @vdauwera about the names on monday, I find them confusing, but if it's standard nomenclature then we should keep them. So don't do any renaming until I get back to you about that I guess.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1871#issuecomment-246046362:69,refactor,refactoring,69,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1871#issuecomment-246046362,1,['refactor'],['refactoring']
Modifiability,"@ronlevine, I think that the following should work for output the Esotericsoft's MinLog to write to a file:; * Implement in `LoggerUtils` a class extending `com.esotericsoftware.minlog.Log.Logger` that overrides the print method to append to the provided file.; * Use`Log.setLogger()` with an instance of that class.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2751#issuecomment-315706353:146,extend,extending,146,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2751#issuecomment-315706353,1,['extend'],['extending']
Modifiability,"@samuelklee ; My understanding: the code that **can** (and I think should) be borrowed from VCF is `CHROM`, `POS`, `ID`, `INFO`, with `END` from `INFO` extracted to be its own column. ; Then; * `FILTER` can be optional.; * `QUAL` can be optional but it is a nice-to-have feature as a quick-glance confidence measure, if that applies.; * `FORMAT` is going to be hard, because I understand the complaint that they can be wasting space, but I have seen VCF files that have rows with different numbers of fields in `FORMAT`, and that is spec-compliant. If this flexibility is allowed, i.e. allowing sample specific information to be missing on several rows, then the `FORMAT` column can be shared. Recap: only `REF`, `ALT` are missing, which is not much code I believe. I think VCF just happens to have a name that starts with V. Stripping out the `REF`, `ALT`, it is quite flexible for describing any annotated interval (OK, 0-length is up for debate) on a piecewise linear coordinate. And I just made myself sound like a VCF-lover. I simply think much of it can be reused.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-481753416:870,flexible,flexible,870,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-481753416,1,['flexible'],['flexible']
Modifiability,"@samuelklee @lucidtronix @asmirnov239 @davidbenjamin This is a good time for all of us to decide on GATK-style python coding (how much type hinting? how much documentation? etc.). I am adding you to this thread so that we can all have a say. **Code style**: The standard practice is to strictly follow [PEP-0008](https://www.python.org/dev/peps/pep-0008/). The python plugin for IntelliJ IDEA (=PyCharm) bark loudly when one deviates, which is pretty useful. **Docstring style**: I have written most of it in reST-style. In retrospect, perhaps [NumPyStyle](https://github.com/numpy/numpy/blob/master/doc/HOWTO_DOCUMENT.rst.txt) would have been a better choice. If I have time now, I will switch. Definitely at some point. You may want to wait until I check the I tick off ""fill in missing docs"" and ""further code cleanup"" boxes before you take a look at the code.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3838#issuecomment-344508765:368,plugin,plugin,368,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3838#issuecomment-344508765,1,['plugin'],['plugin']
Modifiability,"@samuelklee DRAGEN STRE model doesn't actually make any alterations to the smith waterman parameters or how they work, it just works by adjusting the indel gap penalties that are used for the PairHMM. At one point we were concerned about SW parameters being different with dragen but as it turns out the biggest visible effect of the SW parameters on the output (the alignment we perform after haplotypes discovery) is irrelevant since they don't realign their reads internally. We kept the default gatk alignment behavior and thus the SW parameters that are used (for dangling head recovery which I believe are the old arguments) still match. As far as unifying the parameters I suspect it could be done though one wonders if there aren't risks where the different contexts in which we use the parameters will not perform as well with a unified set. Speculation on my part though. I agree with David that we should be cautious about making changes that will affect the HaplotypeCaller before November. . I support including an argument in any case (possibly multiple) to include the SW parameters. I would actually advocate we read these files in as tables of parameters where you simply point to on the command line to configure new parameters.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6863#issuecomment-705611993:1221,config,configure,1221,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6863#issuecomment-705611993,1,['config'],['configure']
Modifiability,"@samuelklee I'm not making any changes to the CNV collection classes. I; think none of this PR affects those classes. On Thu, Mar 1, 2018 at 4:19 PM, samuelklee <notifications@github.com> wrote:. > Sorry @droazen <https://github.com/droazen> @LeeTL1220; > <https://github.com/leetl1220>, can you give me a bit more context?; > @LeeTL1220 <https://github.com/leetl1220> is no longer using any of the; > CNV-specific collections classes that I had hoped might be Tribble-ized in; > the future, so I'm OK with any decisions you guys make that is specific to; > his classes (does @jonn-smith <https://github.com/jonn-smith> have an; > opinion?) I think that moving towards storing the config in the header is a; > good thing, in general.; >; > If we need to make corresponding changes to the CNV-specific collections; > classes, then we should talk more. Not all of those collections describe; > locatables, so I'm not sure how we could fit them in the Tribble framework.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/pull/4276#issuecomment-369734330>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXkzoWV1fcDEucTdcNZ_DggL0UW4M9ks5taGXhgaJpZM4Ru2it>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 8011A; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4276#issuecomment-369735007:681,config,config,681,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4276#issuecomment-369735007,1,['config'],['config']
Modifiability,"@samuelklee If there are CNV tools that can't comfortably extend `GATKTool` as things stand now, then I think that we should adjust `GATKTool` to be more flexible until they can do so. This would help with certain long-term goals that the engine team has (such as all tools supporting NIO for all inputs, consistent sequence dictionary validation, etc.)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2471#issuecomment-358040921:58,extend,extend,58,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2471#issuecomment-358040921,2,"['extend', 'flexible']","['extend', 'flexible']"
Modifiability,"@samuelklee It wasn't just a rebase, it was a complete rewrite because the old code had since become completely entangled with DRAGEN code. But I did it! Everything is passing, the code is dramatically simpler, and it's even a bit faster. I have done my best to make a coherent commit history. I would recommend reviewing one commit at a time in side-by-side diff mode. Note that some commits rip out old code and replace it with pseudocode, deferring the new code to a later commit. Other commits tell a story of what all the different caches meant in order to motivate the simpification of later commits. The baroqueness of the old code was motivated by three considerations:; * cache-friendliness -- traversing all arrays by incrementing the innermost index, reads. This is absolutely essential.; * flattening 3D arrays into 1D arrays. This was a premature optimization.; * Precomputing addition operations -- this was misguided. The DRAGEN code relied on these caches in a rather complex way, which fortunately turned out not to be necessary and which could be dramatically simplified. My notes on tracking all the variables from the parent genotype calculator down to the DRAGEN calculator are in this google doc: https://docs.google.com/document/d/1v6s57mUAwfj38nL3VdktjA059kYBkJfokq18IDy79E8/edit?usp=sharing. Good luck and don't hesitate to ask me to explain anything.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1023647476:55,rewrite,rewrite,55,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1023647476,2,"['rewrite', 'variab']","['rewrite', 'variables']"
Modifiability,@samuelklee Removed references to union and replaced with combine; @droazen Undid the IDE formatting changes.; @droazen Refactored to use `IntervalUtils.compareLocatables(....)`,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3713#issuecomment-338739173:120,Refactor,Refactored,120,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3713#issuecomment-338739173,1,['Refactor'],['Refactored']
Modifiability,"@samuelklee Yes, setting the `GATK_LOCAL_JAR` and/or `GATK_SPARK_JAR` environment variables will cause the gatk script to use that jar, instead of looking in its directory for a jar. The naming of the jar itself also doesn't matter if you use the environment variable method.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3968#issuecomment-351734787:82,variab,variables,82,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3968#issuecomment-351734787,2,['variab'],"['variable', 'variables']"
Modifiability,@samuelklee correct me if I'm wrong - I should manually pass old default values to this set of commands?; ```; /soft/gatk-4.5.0.0/gatk PreprocessIntervals -R /ref/GRCh38.d1.vd1/GRCh38.d1.vd1.fa --padding 0 -L chr1:10000-35000 -L chr22:198477-20003000 -imr OVERLAPPING_ONLY -O /outputs/gatk_intervals.interval_list. /soft/gatk-4.5.0.0/gatk AnnotateIntervals -L /outputs/gatk_intervals.interval_list -R /ref/GRCh38.d1.vd1/GRCh38.d1.vd1.fa -imr OVERLAPPING_ONLY -O /outputs/gatk_intervals.interval_list.annotated.tsv. /soft/gatk-4.5.0.0/gatk CollectReadCounts -I /inputs/E07002_normal_alignment.bam -R /ref/GRCh38.d1.vd1/GRCh38.d1.vd1.fa -L /outputs/gatk_intervals.interval_list --interval-merging-rule OVERLAPPING_ONLY -O /outputs/E07002_normal_alignment.bam.counts.hdf5; /soft/gatk-4.5.0.0/gatk CollectReadCounts -I /inputs/E07002_tumor_alignment.bam -R /ref/GRCh38.d1.vd1/GRCh38.d1.vd1.fa -L /outputs/gatk_intervals.interval_list --interval-merging-rule OVERLAPPING_ONLY -O /outputs/E07002_tumor_alignment.bam.counts.hdf5. /soft/gatk-4.5.0.0/gatk DetermineGermlineContigPloidy -L /outputs/gatk_intervals.interval_list --interval-merging-rule OVERLAPPING_ONLY --contig-ploidy-priors /outputs/a_valid_ploidy_priors_table.tsv.copy.tsv --output /outputs/COHORT_runDir --output-prefix COHORT --input /outputs/E07002_normal_alignment.bam.counts.hdf5 --input /outputs/E07002_tumor_alignment.bam.counts.hdf5. /soft/gatk-4.5.0.0/gatk GermlineCNVCaller --run-mode COHORT -L /outputs/gatk_intervals.interval_list --interval-merging-rule OVERLAPPING_ONLY --annotated-intervals /outputs/gatk_intervals.interval_list.annotated.tsv --contig-ploidy-calls /outputs/COHORT_runDir/COHORT-calls --input /outputs/E07002_normal_alignment.bam.counts.hdf5 --input /outputs/E07002_tumor_alignment.bam.counts.hdf5 --output /outputs/COHORT_runDir --output-prefix COHORT; ```. Or I can change some config file to skip this manual part?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8628#issuecomment-1857396943:1870,config,config,1870,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8628#issuecomment-1857396943,1,['config'],['config']
Modifiability,"@samuelklee os.environ would probably work, though it might be easier to set `theano.config.base_compiledir` directly so you don't have to worry about clobbering any other flags.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-390648848:85,config,config,85,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-390648848,1,['config'],['config']
Modifiability,"@samuelklee, thanks for the update and suggestion. I moved CollectAllelicCounts to the `Coverage Analysis` category. CollectFragmentCounts isn't on the list currently so I added it to the same. I hope I'm not missing a bunch of other new tools given I missed this one. . @yfarjoun ; - You are now in charge of deciding whether we should include authorship in code. What the Comms team wants is for authorship to NOT show up in the gatkDoc/javaDoc. If you want to keep them, author lines should be at the bottom and formatted so they do not show up in the documentation. Geraldine is fine with completely removing them if you prefer that. There is a format trick that has javaDoc skip the author line and @vdauwera would know this or I can get you what I see in other docs. Let either of us know.; - I can help you test your changes. I think the categories are good to go now so I will need to put these into both Picard and GATK HelpConstants.java, with the latter being a placeholder until the new Picard release is incorporated into the next GATK release, with variables that then must be included in each tool doc. I will find an example in a bit. Which tool do you want to test? @cmnbroad can explain the engineering details in engineering lingo if you need more information.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-349404645:1063,variab,variables,1063,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-349404645,1,['variab'],['variables']
Modifiability,"@samuelklee: @LeeTL1220 and I just had a discussion about the writer aspect of this branch, and we agreed on the following:. 1. Lee will introduce a new header type to encapsulate the information that's currently passed in individually to the `writeHeader()` method in `AnnotatedIntervalWriter`. This makes the interface cleaner and more future-proof, since the signature will just become `writeHeader(AnnotatedIntervalHeader)`. 2. Lee will start writing out 3 additional structured header lines (as comment lines) to every header, declaring the names of the chrom, start, and stop columns. These will not be respected on input yet (he will still be relying on a config file to get the names of these 3 columns), but it's the first step in the direction of storing all necessary schema information in the header of each file, rather than separately from each file. 3. Lee will file a github issue to eventually use these 3 header lines on input, when they are present, to get the names of the chrom/start/stop columns (possibly still with a fallback to a separate config file if they aren't, but that is a point we can debate in a future PR).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4276#issuecomment-369709447:663,config,config,663,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4276#issuecomment-369709447,2,['config'],['config']
Modifiability,"@sooheelee . Here is my command:. ```; singularity exec gatk.simg test.pbs; ```; Here is test.pbs:. ```; gatk DetermineGermlineContigPloidy \; -L filtered.interval_list \; --input A1.count.hdf5 --input A2.count.hdf5 \; --contig-ploidy-priors contig_ploidy_priors_homo_sapiens.tsv \; --interval-merging-rule OVERLAPPING_ONLY \; --output out \; --output-prefix exomeseq \; --verbosity DEBUG \; --mean-bias-standard-deviation 0.01 \; --mapping-error-rate 0.01 \; --global-psi-scale 0.001 \; --sample-psi-scale 0.0001; ```. Here is the error message and I think the python environment has been activated by Singularity. The task failed when it tried to create directory of '/root/.theano'. ```; 17:03:28.891 INFO DetermineGermlineContigPloidy - Initializing engine; 17:03:28.896 DEBUG ScriptExecutor - Executing:; 17:03:28.896 DEBUG ScriptExecutor - python; 17:03:28.896 DEBUG ScriptExecutor - -c; 17:03:28.896 DEBUG ScriptExecutor - import gcnvkernel. Traceback (most recent call last):; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configdefaults.py"", line 1738, in filter_compiledir; os.makedirs(path, 0o770) # read-write-execute for user and group; File ""/opt/miniconda/envs/gatk/lib/python3.6/os.py"", line 210, in makedirs; makedirs(head, mode, exist_ok); File ""/opt/miniconda/envs/gatk/lib/python3.6/os.py"", line 220, in makedirs; mkdir(name, mode); PermissionError: [Errno 13] Permission denied: '/root/.theano'; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-433477833:1051,config,configdefaults,1051,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-433477833,1,['config'],['configdefaults']
Modifiability,"@sooheelee As an initial attempt to address this without too much refactoring, what about a two-step process where the user runs M2 with all samples eg `-I normal.bam -I tumor1.bam -I tumor2.bam -tumor sample1 -tumor sample2` (this would require a small code change to specify `-tumor` more than once) and then uses the common set of variants in GGA mode (PR #4601) on each sample individually?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4327#issuecomment-376960734:66,refactor,refactoring,66,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4327#issuecomment-376960734,1,['refactor'],['refactoring']
Modifiability,"@sooheelee It looks like split2_8.vcf.gz and split3_8.vcf.gz were generated with GATK 3 M2. In GATK 4 we only emit calls that are within the unpadded read shards, which I believe do not extend past the input intervals. Thus, as long as `SplitIntervals` returns disjoint subintervals (which it does), different scatters of M2 should produce disjoint calls. Could you try to reproduce the issue with GATK 4 M2?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3061#issuecomment-314866090:186,extend,extend,186,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3061#issuecomment-314866090,1,['extend'],['extend']
Modifiability,"@sooheelee We evaluated the S3 plugin, but found that it always localizes the entire file, which defeats the purpose of NIO. We are currently assessing how difficult it would be to patch the existing plugin. Issue is here: https://github.com/Upplication/Amazon-S3-FileSystem-NIO2/issues/103",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3708#issuecomment-374610893:31,plugin,plugin,31,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3708#issuecomment-374610893,2,['plugin'],['plugin']
Modifiability,"@sooheelee You are right, the python environment is not activated automatically. However, I manually activated the environment and then run my test.pbs, I still got the error. ```; [shengq2@cqs1 singularity]$ singularity shell gatk.simg; Singularity: Invoking an interactive shell within container...; Singularity gatk.simg:/scratch/cqs/softwares/singularity> source activate gatk; (gatk) Singularity gatk.simg:/scratch/cqs/softwares/singularity> sh test.pbs. ...; 21:27:03.205 INFO DetermineGermlineContigPloidy - Initializing engine; 21:27:03.210 DEBUG ScriptExecutor - Executing:; 21:27:03.210 DEBUG ScriptExecutor - python; 21:27:03.210 DEBUG ScriptExecutor - -c; 21:27:03.210 DEBUG ScriptExecutor - import gcnvkernel. Traceback (most recent call last):; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configdefaults.py"", line 1738, in filter_compiledir; os.makedirs(path, 0o770) # read-write-execute for user and group; File ""/opt/miniconda/envs/gatk/lib/python3.6/os.py"", line 210, in makedirs; makedirs(head, mode, exist_ok); File ""/opt/miniconda/envs/gatk/lib/python3.6/os.py"", line 220, in makedirs; mkdir(name, mode); PermissionError: [Errno 13] Permission denied: '/root/.theano'; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-433548346:825,config,configdefaults,825,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-433548346,1,['config'],['configdefaults']
Modifiability,"@sooheelee [Commenting on the forum discussion] `--alleles`, `--genotyping-mode`, and `--consensus` are inherited from a common parent class of Mutect2 and HaplotypeCaller and are inactive in GATK 4 Mutect2. I should fix this misleading situation. @cbao-bi's use case is important and the work-around that I gave on the forum is not satisfactory. I'm convinced that it's worth doing it right. Let me tentatively guess that I can put in a good force-calling mode within two months.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4555#issuecomment-375935335:104,inherit,inherited,104,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4555#issuecomment-375935335,1,['inherit'],['inherited']
Modifiability,"@srikarchamala We haven't heard from enough people to make this a priority. However, we are doing a big refactoring to make all of the Mutect2 annotations and filters inherently multiallelic so that splitting with external tools before or after FilterMutectCalls ought not to cause problems.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3564#issuecomment-570229469:104,refactor,refactoring,104,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3564#issuecomment-570229469,1,['refactor'],['refactoring']
Modifiability,"@t-ogasawara @frank-y-liu @gspowley @paolonarvaez @droazen @lbergelson please comment on the following proposal. The proposal is that we would spin off native PairHMM as a separate project/repo on github and host AVX code there and have alternative implementations extend that project/repo (by creating repos that depend on the AVX one). . In other words, now we have 1 repo, broadinstitute/gatk. After the proposed change we'll have 3 repos (all BSD licensed):; 1) broadinstitute/gatk; 2) broadinstitute/nativePairHMM-AVX; 2) broadinstitute/nativePairHMM-PPC. We will duplicate the native code (AVX and PPC will be separate copies of C++ files etc) to simplify the testing burden. The parties interested in working on a specific architecture will contribute code directly to the respective architecture-specific repo and gatk will take occasional updates of those repos. The gatk repo will depend on the other two. The PPC repo will depend on the AVX repo (and any other native repos will depend on the AVX one). The avx and ppc repos will have their own build systems and unit tests against the new interface. The AVX repo will expose something like the following Java API (to be worked out in detail). ```; //Used to copy references to byteArrays to JNI from reads; public final class JNIReadDataHolderClass {; public byte[] readBases = null;; public byte[] readQuals = null;; public byte[] insertionGOP = null;; public byte[] deletionGOP = null;; public byte[] overallGCP = null;; }. //Used to copy references to byteArrays to JNI from haplotypes; public final class JNIHaplotypeDataHolderClass {; public byte[] haplotypeBases = null;; }. public interface NativePairHMMKernel extends AutoCloseable { . /**; * Function to initialize the fields of JNIReadDataHolderClass and JNIHaplotypeDataHolderClass from JVM.; * C++ code gets FieldIDs for these classes once and re-uses these IDs for the remainder of the program. Field IDs do not; * change per JVM session; *; * @param readDataHolderClass class",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1748#issuecomment-214914864:265,extend,extend,265,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1748#issuecomment-214914864,1,['extend'],['extend']
Modifiability,"@takutosato This is needed to rev gatk public, which in turn is needed for the Mutect2 refactoring I'm working on. Can you review?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2239#issuecomment-257135814:87,refactor,refactoring,87,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2239#issuecomment-257135814,1,['refactor'],['refactoring']
Modifiability,@tedsharpe @SHuang-Broad I've tried to address your comments -- want to have a another look? . Due to issues in the class I backed out my usage and refactoring of SATagAlignmentBuilder and SATagAlignment and just went with my own simple little parser.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2684#issuecomment-301569060:148,refactor,refactoring,148,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2684#issuecomment-301569060,1,['refactor'],['refactoring']
Modifiability,@tedsharpe How much effort do you think it would be to adapt the current BWA bindings to bwa-mem2?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7014#issuecomment-758176744:55,adapt,adapt,55,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7014#issuecomment-758176744,1,['adapt'],['adapt']
Modifiability,"@tedsharpe Thanks for your patience! Review complete for now. I have a lot of stylistic comments and a few request for refactoring / some redesign of what appear to me to be redundant classes. Some stylistic comments:. I didn't comment everywhere that I saw it, but we try to avoid any unbracketed control structures. Even 1-liners should have brackets unless they're so short that the statement fits on the same line as the predicate. We typically have starting brackets at the end of the line though instead of on their own line, which saves some pain associated with single line brackets. I.e. gatk typically uses. ```; if ( something ) {; doThing; } else {; otherThing; }; ```. rather than. ```; if ( something ) ; {; doThing; }; else ; {; otherThing; }; ```. 2) You use a lot of raw iterators, which is fine and is necessary in many cases. In other cases those operations can be written much more succinctly with either a for-each loop, or a stream. i.e. . ```; List<Integer> values;; Iterator itr = iterable.iterator();; while(itr.hasNext()){; Element elem = itr.next();; int value = someFunction(elem); if ( value > SOME_CONSTANT) {; values.append(value); }; }; return values;; ```. can be . ```; return StreamSupport.stream(iterable.spliterator, false); .map( elem -> someFunction(elem)); .filter( value -> value > SOME_CONSTANT ); .collect( Collectors.toList()); ```. We should probably add a utilty function to convert an iterator to a stream directly so we can stream iterators easily even if there is no associated iteratable. . 3) The tools need tests. This is important. 4) It would be good to think about how the tools can be composited into a spark pipline and run without writing intermediate files. . 5) Bitwise operations are a rarity in GATK and many of our users will not be very comfortable with them. Please avoid bit twiddling tricks when possible. When it's not possible (i.e. when you are performing tricks to treat a long as a set of byte pairs) please add detailed explanat",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1435#issuecomment-172985394:119,refactor,refactoring,119,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1435#issuecomment-172985394,1,['refactor'],['refactoring']
Modifiability,"@tedsharpe This looks good to me. In general partition sizes can be much larger than 100kb without problems, so I suspect it's is something funny to do with the task serialization of ctx.paralellize(). . If this is a performance critical tool it would probably be better to rewrite it in a way that it can load the reference in parallel. Since I assume this is something you run essentially once per reference it may not be worth it. . If you're worried about small machines running out of memory, I would expose the parameter that lets you configure how much memory each partition gets. I would expect in any spark configuration each core will have no less than ~1gb to work with and likely 2 -4 on any machines used for biology work.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1505#issuecomment-187387150:274,rewrite,rewrite,274,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1505#issuecomment-187387150,3,"['config', 'rewrite']","['configuration', 'configure', 'rewrite']"
Modifiability,@tedsharpe and I discussed in person. This approach will cause problems for tests that rely on custom registrators. Back to you Ted until it grows additional layers of indirection.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1775#issuecomment-215145245:158,layers,layers,158,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1775#issuecomment-215145245,1,['layers'],['layers']
Modifiability,"@tedsharpe not sure if this is what you meant.; Also, I think the ""ultimate"" solution might be marking `SVInterval` not final, but allows it to be extended, and child classes should name themselves with convention baked in.; What do you think?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5157#issuecomment-418886943:147,extend,extended,147,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5157#issuecomment-418886943,1,['extend'],['extended']
Modifiability,"@tomwhite After spending some time searching for this feature for my testing purposes, it would be helpful to expose the NIO adapter toggle directly from the command line in this branch.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5138#issuecomment-418494235:125,adapt,adapter,125,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5138#issuecomment-418494235,1,['adapt'],['adapter']
Modifiability,"@tomwhite I think the general goal of unifying the Spark/non-Spark tool hierarchies is worthwhile, but I don't like the idea of all tools having `if (sparkArgs.useSpark) {} else {}` boilerplate. If we do this, we should have separate abstract methods that get called automatically in the Spark/non-Spark cases. I also think we should wait to perform this refactoring until later in the quarter, after the Spark evaluation, and after we've standardized more on `Path` for inputs/outputs, as it will break a lot of downstream code in gatk-protected -- and we don't want to break all the tools more than once if we can help it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2217#issuecomment-254228946:355,refactor,refactoring,355,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2217#issuecomment-254228946,1,['refactor'],['refactoring']
Modifiability,"@tomwhite Like we discussed this morning, we can and should get rid of the broadcast code but we should ideally first get some sort of plot we can point to in order to justify the change. This will also be useful for future presentations of our performance improvements. The plot would ideally be a comparison between the new distribution strategy and broadcasting compared across a variable number of cores, so the performance improvement can be better understood.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5127#issuecomment-416327226:383,variab,variable,383,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5127#issuecomment-416327226,1,['variab'],['variable']
Modifiability,@tomwhite Looks great. Does `GATKTestPipeline` need updating to accept the environment variable as well?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/596#issuecomment-114991281:87,variab,variable,87,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/596#issuecomment-114991281,1,['variab'],['variable']
Modifiability,@tomwhite Sorry! I missed your message. This reproduces reliably on our cluster if you run the command I posted above on our current master branch. ```; ./gatk-launch MarkDuplicatesSpark -I file:///home/unix/louisb/flag_stat.bam -O file:///home/unix/louisb/testoutput.bam -- --sparkRunner SPARK --sparkMaster yarn-client; ```. You'll have to adapt it to your own file system or log into ours. Folders under /home/unix/ are visible to every node in our cluster.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1451#issuecomment-188483706:342,adapt,adapt,342,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1451#issuecomment-188483706,1,['adapt'],['adapt']
Modifiability,@tomwhite What do you think about this change? Do you need a scala 10 build of gatk? We could parameterize the build so that it can build both if necessary.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2264#issuecomment-260671311:94,parameteriz,parameterize,94,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2264#issuecomment-260671311,1,['parameteriz'],['parameterize']
Modifiability,@tomwhite suggested looking into the possibility of using java.nio.file.Path + plugins for Hadoop and Google buckets to satisfy this ticket.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/958#issuecomment-169082726:79,plugin,plugins,79,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/958#issuecomment-169082726,1,['plugin'],['plugins']
Modifiability,"@tovanadler I believe the refactoring in question will be minimal (only what is necessary to connect `MarkDuplicatesDataflow` to the `ReadsPreprocessingPipeline`), so the conflict with your branch shouldn't be too bad. If you need a git consultant to assist with the rebase, we're glad to help :)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/759#issuecomment-125624982:26,refactor,refactoring,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/759#issuecomment-125624982,1,['refactor'],['refactoring']
Modifiability,"@tovanadler Review complete. Looks good, just a few comments. I have a few comments about the organization of duplicate marking. I think you've inherited some very old style code that could maybe use some refactoring. I think we do need to also include the histogram and the metrics headers. Those could be done in a separate ticket though. I'm a bit worried that the test is indeterministic. Unless I overlooked something which is likely, it seems like it might depend on the ordering of a PCollection which is undefined. This isn't problematic for the actual metric file, but might be for the tests. What do you think about reorganizing to output an annotation on only 1 of the ""best"" reads with the count of all optical duplicates in it's group. That would simplify the code, and since we only care about the global count it wouldn't change the information content.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/749#issuecomment-126762958:144,inherit,inherited,144,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/749#issuecomment-126762958,2,"['inherit', 'refactor']","['inherited', 'refactoring']"
Modifiability,"@tushu1232 Sorry for the delay. I'm looking into this. I think you've hit a serious bug in the gatk-launch script. Do you have $GATK_LOCAL_JAR set in your environment? . It looks like the case where a local jar is explicitly specified results in the environment variables not being properly defined, which means you end up hitting #2026. ; If they were properly defined you should be seeing the line `Snappy is disabled via system property` included in your standard out. . I've opened #2316 to deal with the issue. Until that's fixed though, the workaround is to invoke the jars directly and add `-Dsnappy.disable=true`. i.e. `java -Dsnappy.disable=true -jar $GATK_LOCAL_JAR SortSam --input BAM_BWA/SRR2.sorted.bam --output hpcinfra/hadoop/test.bam --SORT_ORDER queryname`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2300#issuecomment-267119418:262,variab,variables,262,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2300#issuecomment-267119418,1,['variab'],['variables']
Modifiability,@vdauwera can you clarify? is this a bug or an enhancement?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/263#issuecomment-93479914:47,enhance,enhancement,47,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/263#issuecomment-93479914,1,['enhance'],['enhancement']
Modifiability,@vdauwera is this a bug or enhancement?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/269#issuecomment-114259940:27,enhance,enhancement,27,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/269#issuecomment-114259940,1,['enhance'],['enhancement']
Modifiability,@vdauwera not in my radar either.... the rewrite of the assembly may fix some of these cases where there is actually some variation that we fail to detect (false negative) that would explain those soft clips. However I don't think that would fix all the cases.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/269#issuecomment-279013205:41,rewrite,rewrite,41,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/269#issuecomment-279013205,1,['rewrite'],['rewrite']
Modifiability,"@vdauwera note that this modifies the path of the CNV methods doc (which is mostly out of date, but is still linked to in some Comms materials) from docs/CNVs/CNV-methods.pdf to docs/CNV/archived/archived-CNV-methods.pdf. The extended abstract on gCNV is a very technical description of the probabilistic model, but it might be worth referencing it for interested users until a preprint or publication is available.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5732#issuecomment-467976599:226,extend,extended,226,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5732#issuecomment-467976599,1,['extend'],['extended']
Modifiability,"@vdauwera when running spark tools through the gatk-launch-soon-to-be-just-gatk script, you use a `--` to separate arguments to the specific tool you're running from arguments that describe what sort of Spark setup you're trying to submit the job to. The most important of these is probably `sparkRunner`, which says what type of spark cluster you're using: `LOCAL` indicates that you want to simulate a spark cluster on your local machine using multithreading; `SPARK` indicates that you want to submit to a configured, dedicated spark cluster, in which case you have to pass the url to the master node, and `GCS` indicates that you want to use a cluster managed by Google Cloud Dataproc, in which case you need the name of the cluster. See https://github.com/broadinstitute/gatk#running-gatk4-spark-tools-on-a-spark-cluster for more info. Day-to-day on the SV team we primarily use dataproc, so the example command I was going to use for one tool looks like:. ```; ./gatk ParallelCopyGCSDirectoryIntoHDFSSpark \; --input-gcs-path gs://my-bucket/my-data-directory/ \; --output-hdfs-directory hdfs://my-dataproc-spark-cluster-m:8020/my-data \; -- \; --sparkRunner GCS \; --cluster my-dataproc-spark-cluster; ```. I wasn't sure if it was cool to use the GCS/dataproc version or if we'd rather not tie ourselves to GCP in the examples. In this particular case I feel like it might be appropriate since it's a GCS-specific tool, but my question was more about our general strategy.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-349358724:509,config,configured,509,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-349358724,1,['config'],['configured']
Modifiability,"@vilay-nference Thank you for doing this work. It's nitpicky annoying stuff to figure out.; ; I have one additional request. Instead of addding additional direct implementation dependencies, could we specify the transtive version requirements in a [gradle constraints block](https://docs.gradle.org/current/userguide/dependency_constraints.html)? . That will: ; 1. make it clear that we don't rely on these directly; 2. prevent us from keeping them around if we do something like remove hadoop in the future; 3. lets us rewrite those force blocks to instead define minimum versions so if the libraries move forward in the future we're not accidentally holding on a to an old version",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8950#issuecomment-2297074810:520,rewrite,rewrite,520,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8950#issuecomment-2297074810,1,['rewrite'],['rewrite']
Modifiability,"@vilay-nference Were you able to get this configuration to pass tests on your end? I've attempted to incorporate your changes into https://github.com/broadinstitute/gatk/pull/8998, but I'm running into issues with hadoop and protobuf incompatibilities. I see the same problem with your branch when I try to run tests on it. (I also can't run tests without disabling -Werror on your branch since there are still some unresolved deprecation and other minor issues). Errors look like this:. ```; Caused by: java.lang.ExceptionInInitializerError: Exception java.lang.NoClassDefFoundError: Could not initialize class org.apache.hadoop.security.proto.SecurityProtos [in thread ""IPC Server handler 1 on default port 64812""]; 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos.<clinit>(ClientNamenodeProtocolProtos.java); ```. and you can easily trigger one by running `ParallelCopyGCSDirectoryIntoHDFSSparkIntegrationTest`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8950#issuecomment-2412827680:42,config,configuration,42,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8950#issuecomment-2412827680,1,['config'],['configuration']
Modifiability,@vruano Back to you. This refactoring is starting to look really nice thanks to your review.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1918#issuecomment-232558623:26,refactor,refactoring,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1918#issuecomment-232558623,1,['refactor'],['refactoring']
Modifiability,"@vruano What happens if you make your tool extend `GATKSparkTool` directly, rather than `VariantWalkerSpark`, then do the following in your `runTool()` method?. ```; final VariantsSparkSource variantsSource = new VariantsSparkSource(ctx);; final List<SimpleInterval> intervals = hasIntervals() ? getIntervals() : IntervalUtils.getAllIntervalsForReference(getBestAvailableSequenceDictionary());; final JavaRDD<VariantContext> variants = variantsSource.getParallelVariantContexts(vcf, intervals);; ```. And then do a `variants.mapPartitions()` call on the resulting `variants` RDD to process each variant?. Also, at-mentioning @tomwhite here to comment on the `VariantWalkerSpark` issue, since he wrote that class.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2545#issuecomment-290236139:43,extend,extend,43,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2545#issuecomment-290236139,1,['extend'],['extend']
Modifiability,@vruano You could extend `CommandLineProgram` rather than `GATKTool`,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6390#issuecomment-576783251:18,extend,extend,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6390#issuecomment-576783251,1,['extend'],['extend']
Modifiability,@vruano in addition to refactoring MAthUtils I also engaged in a few boy scout rule activities.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2235#issuecomment-256973513:23,refactor,refactoring,23,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2235#issuecomment-256973513,1,['refactor'],['refactoring']
Modifiability,@vruano refactoring is done -- back to you,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7122#issuecomment-841279671:8,refactor,refactoring,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7122#issuecomment-841279671,1,['refactor'],['refactoring']
Modifiability,"@wangdy12 .I get the same issue: when use HaplotypeCallerSpark on a cluster, It lose a lot of variable sites and the result jitter to the same input bam. and running on local mode, the result is good.; But I fix the code according to your way. It does not work and get the same issue.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4231#issuecomment-371410958:94,variab,variable,94,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4231#issuecomment-371410958,1,['variab'],['variable']
Modifiability,"@wujh2017 Did you set up the appropriate conda environment as described in the README?. > Python 3.6.2, along with a set of additional Python packages, are required to run some tools and workflows. GATK uses the Conda package manager to establish and manage the environment and dependencies required by these tools. The GATK Docker image comes with this environment pre-configured. In order to establish an environment suitable to run these tools outside of the Docker image, the conda gatkcondaenv.yml file is provided. To establish the conda environment locally, Conda must first be installed. Then, create the gatk environment by running the command conda env create -n gatk -f gatkcondaenv.yml (developers should run ./gradlew createPythonPackageArchive, followed by conda env create -n gatk -f scripts/gatkcondaenv.yml from within the root of the repository clone). To activate the environment once it has been created, run the command source activate gatk. See the Conda documentation for additional information about using and managing Conda environments.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4389#issuecomment-364748538:370,config,configured,370,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4389#issuecomment-364748538,1,['config'],['configured']
Modifiability,"@xaviloinaz here's that feature you requested for configurable ordering of funotations via `VariantClassifications`. It's basically what we talked about - you supply a TSV file with `VARIANT_CLASSIFICATION SEVERITY`, and Funcotator will respect that new order. . Let me know if this won't work for what you wanted to do.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7673#issuecomment-1040805481:50,config,configurable,50,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7673#issuecomment-1040805481,1,['config'],['configurable']
Modifiability,"@xysj1989 We primarily run this workflow using the WDL on Terra. In this case, each GermlineCNVCaller shard is run on a separate VM using the GATK Docker. Hopefully, we can always at least guarantee that this default mode of running the workflow is functional and covered by tests. However, if you'd like to instead run multiple instances of GermlineCNVCaller locally, you may need to make sure certain environment variables are set appropriate. For example, I think you can address (2) above (the location of the temporary theano directory) by either setting environment variables or modifying your Theano configuration (see http://deeplearning.net/software/theano/library/config.html) appropriately. You may also want to check the GermlineCNVCaller task in the WDL to see how other variables are set there. Let me look into whether you can also address (1) in this way, or if this will require a GATK code change, and get back to you. (Of course, if you figure it out before me, please follow up!) Thanks again for bringing this to our attention.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6235#issuecomment-548007200:415,variab,variables,415,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6235#issuecomment-548007200,5,"['config', 'variab']","['config', 'configuration', 'variables']"
Modifiability,"@yfarjoun I'm not understanding... If we're on the reverse strand, then we reach the adaptor at the 5' end of the forward strand i.e. at one `getMateStart() + 1`, which is what the code does now. If we're on the forward strand the equivalent logic would be `getMateEnd() + 1`, but no such method exists, so we use `read.getStart() + abs(read.getFragmentLength())`. Why is this not equivalent?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3184#issuecomment-358740758:85,adapt,adaptor,85,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3184#issuecomment-358740758,1,['adapt'],['adaptor']
Modifiability,"A comment from @ldgauthier:. The threshold for PLs to be considered uninformative is the same between GATK3 and 4, but the stack when it gets called is a little bit different. It might be changes in subsetting again because the methods that evaluate the ""informativeness"" in GATK3 are called updateGenotypeAfterSubsetting and createGenotypesWithSubsettedLikelihoods, which appear to have been refactored into AlleleSubsettingUtils in GATK4. I could see how if we calculate the sum before subsetting in GATK4 then it's smaller and uninformative, but that's speculation.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2712#issuecomment-305601294:393,refactor,refactored,393,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2712#issuecomment-305601294,1,['refactor'],['refactored']
Modifiability,A couple of things we can try:. Make sure that the temp directories created by RunSGAViaProcessBuilderOnSpark are actually being created on the /tmp filesystem. Maybe there's some Spark configuration that is overriding that?. Try removing the assembly part and just running writeToLocal() on the text files dataset. Then we can see if something's getting stuck in the assembly process or if it's a file-handling problem.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1877#issuecomment-223582523:186,config,configuration,186,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1877#issuecomment-223582523,1,['config'],['configuration']
Modifiability,"A few minor issues:. - [x] Change `--resource <blah>` to `--resource:<blah>` in tool-level documentation. EDIT: Added to the sl_lite_overlap branch mentioned below.; - [x] The VCF writer in VariantRecalibrator has a few conditionals to allow for VCF headers without contig lines, we could do the same for the writer in LabeledVariantAnnotationsWalker. EDIT: Added to the sl_lite_overlap branch mentioned below.; - [ ] Double check whether we should worry about any differences in extraction on test data (provided via email) from https://gatk.broadinstitute.org/hc/en-us/community/posts/7974912707099-VariantRecalibrator-IndexOutOfBoundsException. Probably nothing to worry about, and at least the error messaging in the new tools is more informative.; - [x] We could change the strategy for checking for resource overlaps to require allele-level matching (rather than only matching on start position, as was inherited from VQSR). A quick test on malaria shows that this can reduce the number of overlaps by O(10%), but performance doesn't really change too much. Branch is already open at https://github.com/broadinstitute/gatk/tree/sl_lite_overlap; - [ ] Expand the exact-match tests to cover some of these strategies, which were added separately in #8049 and merged to make a release deadline.; - [x] Catch the exception in https://github.com/broadinstitute/gatk/blob/fd782504d18b56dbc266c2b3bb4eb32f21916776/src/main/java/org/broadinstitute/hellbender/tools/walkers/vqsr/scalable/LabeledVariantAnnotationsWalker.java#L389 and throw the same message that is thrown in AS mode. Added in #8074.; - [x] Add message to the score tool that the scores HDF5 file will not be out when the input VCF is empty (such a message is already emitted about the annotations HDF5 file). Added in #8074.; - [ ] Megan suggested in the review of #8074 that dynamic disk sizing could be added to the WDL.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7724#issuecomment-1222787946:909,inherit,inherited,909,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7724#issuecomment-1222787946,1,['inherit'],['inherited']
Modifiability,A few tests that are not marked as either `cloud` or `bucket` are now failing in Travis due to undefined environment variables -- should fix this before merge.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/545#issuecomment-109324558:117,variab,variables,117,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/545#issuecomment-109324558,1,['variab'],['variables']
Modifiability,"A fix I found for this issue is to modify `start_session_get_args_and_model` function in models.py from this:. ```; def start_session_get_args_and_model(intra_ops, inter_ops, semantics_json, weights_hd5=None, tensor_type=None):; K.clear_session(); K.get_session().close(); cfg = K.tf.ConfigProto(intra_op_parallelism_threads=intra_ops, inter_op_parallelism_threads=inter_ops); cfg.gpu_options.allow_growth = True; K.set_session(K.tf.Session(config=cfg)); return args_and_model_from_semantics(semantics_json, weights_hd5, tensor_type). ```. To this:. ```; import tensorflow as tf. def start_session_get_args_and_model(intra_ops, inter_ops, semantics_json, weights_hd5=None, tensor_type=None):; tf.keras.backend.clear_session(); tf.keras.backend.get_session().close(); cfg = tf.ConfigProto(intra_op_parallelism_threads=intra_ops, inter_op_parallelism_threads=inter_ops); cfg.gpu_options.allow_growth = True; tf.keras.backend.set_session(tf.Session(config=cfg)); return args_and_model_from_semantics(semantics_json, weights_hd5, tensor_type). ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7250#issuecomment-839720987:284,Config,ConfigProto,284,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7250#issuecomment-839720987,4,"['Config', 'config']","['ConfigProto', 'config']"
Modifiability,A holdover for this is currently in place where we detect if no funcotations were produced at all. In that case we warn the user that they may have configured the reference version and data sources incorrectly.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4978#issuecomment-503219497:148,config,configured,148,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4978#issuecomment-503219497,1,['config'],['configured']
Modifiability,"A logger with configurable verbosity would be great, but low priority is fine. This is a very low priority issue.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2689#issuecomment-300304840:14,config,configurable,14,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2689#issuecomment-300304840,1,['config'],['configurable']
Modifiability,"A user reported this same issue and the error message does not give any location information in GATK 4.1.9.0 with VariantRecalibrator. ; [Link to the forum post](https://gatk.broadinstitute.org/hc/en-us/community/posts/360074618292-New-version-of-GATK-leads-to-VariantRecalibrator-error-?page=1#community_comment_360013419291).; Here is the message:; ```; Using GATK jar ~/bin/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xms24g -jar ~/bin/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar VariantRecalibrator -V temp/vatiant_germline/sites.only.vcf.gz -O temp/vatiant_germline/recaliberation.indel.vcf --tranches-file temp/vatiant_germline/tranches.indel.txt --trust-all-polymorphic -tranche 100.0 -tranche 99.95 -tranche 99.9 -tranche 99.5 -tranche 99.0 -tranche 97.0 -tranche 96.0 -tranche 95.0 -tranche 94.0 -tranche 93.5 -tranche 93.0 -tranche 92.0 -tranche 91.0 -tranche 90.0 -an DP -an FS -an MQRankSum -an QD -an ReadPosRankSum -an SOR -mode INDEL --max-gaussians 4 -resource:mills,known=false,training=true,truth=true,prior=12 ~/db/mutect2_support/b37/Mills_and_1000G_gold_standard.indels.b37.sites.vcf.gz -resource:dbsnp,known=true,training=false,truth=false,prior=2 ~/db/mutect2_support/b37/hg19_v0_dbsnp_138.b37.vcf.gz --use-allele-specific-annotations -resource:axiomPoly,known=false,training=true,truth=false,prior=10 ~/db/mutect2_support/b37/Axiom_Exome_Plus.genotypes.all_populations.poly.b37.vcf.gz. 14:58:10.389 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:~/bin/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Nov 12, 2020 2:58:10 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 14:58:10.555 INFO VariantRecalibrator - --------------",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6701#issuecomment-726406532:837,polymorphi,polymorphic,837,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6701#issuecomment-726406532,1,['polymorphi'],['polymorphic']
Modifiability,"About ""this version also moves the files around so they match the local tree"" what I meant is that it expects the files to be somewhere else. I did the actual shuffling via gsutil.; The change is at BaseRecalibratorDataflowIntegrationTest:21, changing the semantics of the; HELLBENDER_TEST_INPUTS environment variable.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107677548:309,variab,variable,309,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107677548,1,['variab'],['variable']
Modifiability,"According to this paper https://www.nature.com/articles/s41467-018-03590-5. it is: ""Each resulting qualified captured library with the SureSelect Human; All Exon kit (Aglient) was then loaded on *BGISEQ-5000 *sequencing; platforms, and we performed high-throughput sequencing for each captured; library. High-quality reads were aligned to the human reference genome; (GRCh37) using the Burrows-Wheeler Aligner (BWA v0.7.15) software. All; genomic variations, including single-nucleotide polymorphisms and InDels; were detected by *HaplotypeCaller of GATK *(v3.0.0).; "". On Wed, Dec 12, 2018 at 3:18 PM Louis Bergelson <notifications@github.com>; wrote:. > @yfarjoun <https://github.com/yfarjoun> Do you know if BGI's sequencing; > is compatible with our tools without any special treatment?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/5517#issuecomment-446729153>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACnk0mujY7YzxUJ-6IPU8B7jPiZWQuzMks5u4WR3gaJpZM4ZQNxZ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5517#issuecomment-446769514:487,polymorphi,polymorphisms,487,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5517#issuecomment-446769514,1,['polymorphi'],['polymorphisms']
Modifiability,"Actually what you said is correct: it is very painful to extend the `Main` class and use the current implementation. The simplest `Main` class that I'm using it's not extending the GATK one just because of the static methods (see the code [here](https://github.com/magicDGS/thaplv/blob/master/src/main/java/org/magicdgs/thaplv/Main.java)), that's why I would like to include this change. In few minutes I will commit the changes that you propose, including the method and the change from static. Thanks for the feedback, @lbergelson!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2204#issuecomment-255841430:57,extend,extend,57,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2204#issuecomment-255841430,2,['extend'],"['extend', 'extending']"
Modifiability,"Actually, I think this is it: https://github.com/broadinstitute/gatk/compare/sl_het_pulldown_spark. I did take a crack at Spark-ifying the het pulldown in this (ancient) branch, but I can't claim that I knew what I was doing---if I remember correctly, I just tried to adapt some code @akiezun had in https://github.com/broadinstitute/gatk/tree/ak_coverage_spark. I don't think I ever implemented proper CIGAR-string parsing either. Might be better to start from scratch?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1558#issuecomment-223475794:268,adapt,adapt,268,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1558#issuecomment-223475794,1,['adapt'],['adapt']
Modifiability,"Actually, just ran a WGS sample with 250bp bins that took ~4 hours to plot...pretty ridiculous! The R code is neither efficient nor well written, so I'm inclined to completely rewrite plotting in python (for ACNV, as well).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3554#issuecomment-329195610:176,rewrite,rewrite,176,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3554#issuecomment-329195610,1,['rewrite'],['rewrite']
Modifiability,"Added a few comments of my own -- requested that you refactor to check the index modification time in the `FeatureDataSource` constructors, rather than in the sequence dictionary validation routines.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3063#issuecomment-320948324:53,refactor,refactor,53,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3063#issuecomment-320948324,1,['refactor'],['refactor']
Modifiability,"Additional feedback from the user for the mutect2 workflow. > ""Of note, it is really difficult and not really 'user-friendly' to have to predict disc space and runtime for Funcotator, which seem to depend (based on calculations you copied above from other Functotator workflows) on outputs of Mutect2 (eg vcf sizes), when here Mutect and Funcotator and bundled together. So I cannot see output of Mutect to predict values for Funcotator - especially not when I get to run this over hundreds of samples. It is also pricey to have jobs failing because of this. It would be much better to have these variables encoded, so that the algorithm uses Mutect outputs to predict memory etc. that it will need to run Funcotator downstream. If this is really how things work (and this is my current understanding), I really do not know how to estimate this for many samples without 'trial and error' that is both costly and it will take extremely long time....""",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6680#issuecomment-651354230:597,variab,variables,597,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6680#issuecomment-651354230,1,['variab'],['variables']
Modifiability,Agree that we should do the equivalent of what we did for read filters for annotations using the new plugin framework. Thanks for offering to help out on this one @magicDGS -- any contributions you're able to make here are very much appreciated!,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1880#issuecomment-288771998:101,plugin,plugin,101,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1880#issuecomment-288771998,1,['plugin'],['plugin']
Modifiability,"Agree with that, but also it will be nice to be able to configure it if used outside the AnnotationEngine - the same for other usages of OneShotLogger...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3828#issuecomment-345246188:56,config,configure,56,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3828#issuecomment-345246188,1,['config'],['configure']
Modifiability,"Agree with those above arguing that VCF isn't appropriate for this purpose, and would be a very bad fit. I certainly support the goal of adopting a single unified, standard format for tabular data throughout the GATK, however, and would ideally favor a solution at the HTSJDK/tribble level to get all the benefits that that provides, such as support for NIO and indexing (even if it means that engine team has to extend tribble to support records that are not `Locatable`). . We're happy to help with any efforts in the direction of unification, and would be eager to participate in any methods-wide discussions on this topic.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-387100399:413,extend,extend,413,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-387100399,1,['extend'],['extend']
Modifiability,Agreed! And also with default value stored in the configuration file.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3471#issuecomment-324070716:50,config,configuration,50,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3471#issuecomment-324070716,1,['config'],['configuration']
Modifiability,"Ah -- maybe I'm mistaken. Or can it be a difference in how they're; applied/invoked?. Maybe there's some inconsistency in behavior. Would be nice to iron this; all out. On Mon, Feb 23, 2015 at 5:17 PM, Louis Bergelson notifications@github.com; wrote:. > I'm pretty sure they all do... or at least all can depending on how you; > configure your MalformedReadFilter.; > ; > example:; > ; > ```; > private static boolean checkHasReadGroup(final SAMRecord read) {; > if ( read.getReadGroup() == null ) {; > // there are 2 possibilities: either the RG tag is missing or it is not defined in the header; > final String rgID = (String)read.getAttribute(SAMTagUtil.getSingleton().RG);; > if ( rgID == null ); > throw new UserException.ReadMissingReadGroup(read);; > throw new UserException.ReadHasUndefinedReadGroup(read, rgID);; > }; > return true;; > }; > ```; > ; > —; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/hellbender/issues/193#issuecomment-75649333; > . ## . Geraldine A. Van der Auwera, Ph.D.; Bioinformatics Scientist II; GATK Support & Outreach; Broad Institute",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/193#issuecomment-75686467:329,config,configure,329,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/193#issuecomment-75686467,1,['config'],['configure']
Modifiability,"Ah the ""Tool Docs Index"" link thing is because I left out the extension on purpose; our php server is configured to grab whatever file is present with that basename, with a rule of precedence in case there are several with different extensions (which is useful because of reasons). But locally the browser doesn't know to do that. I'm putting in some logic to handle this, thanks for pointing it out.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3165#issuecomment-311098545:102,config,configured,102,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3165#issuecomment-311098545,1,['config'],['configured']
Modifiability,"Ah, ok, so you would extend `Main` anyway even if we had a way to control which packages it looks in? That's good to know. Can you not use base test because of that GenomeLocParser? We should fix that.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2122#issuecomment-242816026:21,extend,extend,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2122#issuecomment-242816026,1,['extend'],['extend']
Modifiability,"Ah, well, if bash is opaque to you then that's not the right file for you! This script has to be adapted before it can be useful, generally, unless I guess you're looking for the exact same bug I was looking for at the time. I'll try to add more comments.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/913#issuecomment-143079634:97,adapt,adapted,97,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/913#issuecomment-143079634,1,['adapt'],['adapted']
Modifiability,Almost all of the CNV tools extend `CommandLineProgram` unless they are walkers or otherwise need to use `-L`.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2471#issuecomment-357534210:28,extend,extend,28,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2471#issuecomment-357534210,1,['extend'],['extend']
Modifiability,"Also as part of the mock-up, we should actually package the mock config files inside of our jar, load them off the classpath, and test file-based override ability.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3126#issuecomment-309543353:65,config,config,65,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3126#issuecomment-309543353,1,['config'],['config']
Modifiability,Also some tests for collection classes. Refactoring may avoid code duplication here.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3916#issuecomment-352080910:40,Refactor,Refactoring,40,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3916#issuecomment-352080910,1,['Refactor'],['Refactoring']
Modifiability,"Also, a necessary functionality is fragment-based counts. Perhaps a good starting point is bringing back Valentin's GATKReadPair which was removed as a part of XHMM-related code cleanup. Eventually, it is also useful to have the option of collecting various summary statistics and empirical distribution for features such as insert length, mapping quality, the position with respect to bins. Other things to consider are summary of orphan reads, reads with mates on other contigs, and reads with multiple alternate alignments. This requires a bit of good software engineering but it's a necessary one-time investment. This is a relatively high-priority refactoring given the growing interest in running gCNV on large datasets (gnomAD, gen psy cohort, TCGA).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3153#issuecomment-310507118:653,refactor,refactoring,653,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3153#issuecomment-310507118,1,['refactor'],['refactoring']
Modifiability,"Also, could you provide at what level is the logging configured @jjfarrell ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4531#issuecomment-373807025:53,config,configured,53,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4531#issuecomment-373807025,1,['config'],['configured']
Modifiability,"Also, good point on the arg name collision. The command line parser handles this (for any args, even across plugins or across anywhere) but I didn't see any tests, so I added tests to both CommandLineParserUnitTest and CommandLineParserPluginUnitTest.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1973#issuecomment-244960357:108,plugin,plugins,108,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1973#issuecomment-244960357,1,['plugin'],['plugins']
Modifiability,"Also, in GATK3, PluginManager was used to lookup plugin w/ Reflection, such as:. new PluginManager<RequiredStratification>(RequiredStratification.class).getPlugins()). Is there a similar concept in GATK4?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/616#issuecomment-343577587:16,Plugin,PluginManager,16,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/616#issuecomment-343577587,3,"['Plugin', 'plugin']","['PluginManager', 'plugin']"
Modifiability,"Also, the cloud tests should be skipped by default (ie., in the case of no environment variables set).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2404#issuecomment-279083759:87,variab,variables,87,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2404#issuecomment-279083759,1,['variab'],['variables']
Modifiability,"Also, we might want to write some tests for gatk-launch, since it's starting to have lots of different invocation configurations and I find that any line of python code I don't actually run tends to have some horrible error that a compiler would have complained about.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2090#issuecomment-239881662:114,config,configurations,114,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2090#issuecomment-239881662,1,['config'],['configurations']
Modifiability,"Although I was again wrong by static-blocks and inheritance (see https://github.com/broadinstitute/gatk/issues/3483), I think that in the case of the tests is better to make overridable this config - thus, I keep it open.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5013#issuecomment-405101030:48,inherit,inheritance,48,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5013#issuecomment-405101030,2,"['config', 'inherit']","['config', 'inheritance']"
Modifiability,"Am going to address part of this ticket in my next branch by giving tools a way to indicate that particular data sources are required or optional, even when the inherited argument itself is marked as optional. Once this is done, all we probably need to do is change the code that displays the tool arguments to the user so that they take each tools requirements into account when displaying args.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/149#issuecomment-76432333:161,inherit,inherited,161,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/149#issuecomment-76432333,1,['inherit'],['inherited']
Modifiability,"And as an addition to the above case, I have found this problematic representation of alleles also in variable sites in newly generated vcf files (gatk version 4.3.0.0):; NC_041772.1 40006060 . GAC G,AAC; NC_041772.1 40006061 . A T,G,C,*; NC_041772.1 40006062 . C T,*",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8030#issuecomment-1420899124:102,variab,variable,102,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8030#issuecomment-1420899124,1,['variab'],['variable']
Modifiability,AnnotationBaseTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5833/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Fubm90YXRvci9hbGxlbGVzcGVjaWZpYy9SZWR1Y2libGVBbm5vdGF0aW9uQmFzZVRlc3QuamF2YQ==) | `2.439% <0%> (-90.244%)` | `1 <0> (-8)` | |; | [...ferenceConfidenceVariantContextMergerUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5833/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL1JlZmVyZW5jZUNvbmZpZGVuY2VWYXJpYW50Q29udGV4dE1lcmdlclVuaXRUZXN0LmphdmE=) | `2.881% <0%> (-94.239%)` | `1 <0> (-25)` | |; | [...stitute/hellbender/tools/HaplotypeCallerSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/5833/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9IYXBsb3R5cGVDYWxsZXJTcGFyay5qYXZh) | `70.115% <0%> (ø)` | `18 <1> (ø)` | :arrow_down: |; | [...er/tools/walkers/GenotypeGVCFsIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5833/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL0dlbm90eXBlR1ZDRnNJbnRlZ3JhdGlvblRlc3QuamF2YQ==) | `3.704% <0%> (-80.408%)` | `2 <0> (-37)` | |; | [...haplotypecaller/HaplotypeCallerEngineUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5833/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9IYXBsb3R5cGVDYWxsZXJFbmdpbmVVbml0VGVzdC5qYXZh) | `3.704% <0%> (-92.593%)` | `1 <0> (-5)` | |; | [...Plugin/GATKAnnotationPluginDescriptorUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5833/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL0dBVEtQbHVnaW4vR0FUS0Fubm90YXRpb25QbHVnaW5EZXNjcmlwdG9yVW5pdFRlc3QuamF2YQ==) | `7.219% <0%> (-81.016%)` | `4 <0> (-54)` | |; | ... and [1286 more](https://codecov.io/gh/broadinstitute/gatk/pull/5833/diff?src=pr&el=tree-more) | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5833#issuecomment-476235495:3841,Plugin,Plugin,3841,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5833#issuecomment-476235495,1,['Plugin'],['Plugin']
Modifiability,"Another con of hacky solution is that it may make calling replicates in T/N configuration difficult. I have not confirmed this, though, so it may not be an issue at all.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3265#issuecomment-315085428:76,config,configuration,76,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3265#issuecomment-315085428,1,['config'],['configuration']
Modifiability,"Another option is to invert the dependency by changing htsjdk.samtools.cram.ref.ReferenceSource to be an interface (which I've lobbyied the htsjdk CRAM developer to do anyway for other reasons) ,and change Hadoop-BAM to instantiate an implementation of that interface on an hdfs file. I made the changes to all three layers locally to do this for non-HDFS references and it works fine. The interface itself is only one method at the moment:; public byte[] getReferenceBases(final SAMSequenceRecord record, final boolean tryNameVariants); . This has the advantage of not requiring a dependency on the jsr203 release, but its a point solution for CRAM references only, and it would require a non BWC change in htsjdk since the ReferenceSource interface is public. However as I mentioned I think the htsjdk change is a good one anyway.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1326#issuecomment-165247142:317,layers,layers,317,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1326#issuecomment-165247142,1,['layers'],['layers']
Modifiability,"Another permutation on this: i dont know if you'll like hooking into the codec, but one could wrap the codec and create a FeatureInputAwareVariantContext. This would go into FeatureDataSource, and would potentially allow Features to be 'aware' of their owning FeatureInput. The code below is not final and needs work, but this or the patch might give an idea:. [FeatureInputAwareVariantContext.patch.txt](https://github.com/broadinstitute/gatk/files/6346205/FeatureInputAwareVariantContext.patch.txt). ```; public static class CodecWrapper<FEATURE_TYPE extends Feature, SOURCE> implements FeatureCodec<FEATURE_TYPE, SOURCE>; {; private final FeatureCodec<FEATURE_TYPE, SOURCE> childCodec;; private final FeatureInput<FEATURE_TYPE> featureInput;. public CodecWrapper(FeatureCodec<FEATURE_TYPE, SOURCE> childCodec, FeatureInput<FEATURE_TYPE> featureInput); {; this.childCodec = childCodec;; this.featureInput = featureInput;; }. @Override; public Feature decodeLoc(SOURCE source) throws IOException {; return childCodec.decodeLoc(source);; }. @Override; public FEATURE_TYPE decode(SOURCE source) throws IOException {; FEATURE_TYPE feature = childCodec.decode(source);. //Either look for marker class or otherwise poke in FeatureInput here:; if (feature instanceof VariantContext); {; feature = new FeatureInputAwareVariantContext(feature, featureInput);; }. return feature;; }. @Override; public FeatureCodecHeader readHeader(SOURCE source) throws IOException {; return childCodec.readHeader(source);; }. @Override; public Class<FEATURE_TYPE> getFeatureType() {; return childCodec.getFeatureType();; }. @Override; public SOURCE makeSourceFromStream(InputStream bufferedInputStream) {; return childCodec.makeSourceFromStream(bufferedInputStream);; }. @Override; public LocationAware makeIndexableSourceFromStream(InputStream inputStream) {; return childCodec.makeIndexableSourceFromStream(inputStream);; }. @Override; public boolean isDone(SOURCE source) {; return childCodec.isDone(source);; }. @Overrid",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-823546766:553,extend,extends,553,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-823546766,1,['extend'],['extends']
Modifiability,"Another thing that just come to my mind is to rely on [SLF4J](https://www.slf4j.org/) for logging - downstream projects can configure which logger they want to use, and they can have their own ways of setting logging verbosity. If the logging system from HTSJDK wants to be maintain, it can also add a simple implementation of SLF4J with the verbosity levels that are in the current implementation.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4340#issuecomment-371401288:124,config,configure,124,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4340#issuecomment-371401288,1,['config'],['configure']
Modifiability,"Apart of the amount of work in both Barclay and GATK, I think that this shouldn't be implemented for 2 reasons:. * After #3486, some tools are hidden from the command line (and they will be most likely undocumented too). If the bash-completion works with undocumented tools that are hidden from the command line, there will appear anyway after pressing tab-tab. If that tools are treated in a different way, then it requires even more work - Barclay does not use the omitFromCommandLine at all, and that means that GATK should extend the bash-completion to take it into account.; * If a tool can bash-complete but it does not show in the online help pages (the main source for help, taking into account that in the CLI is a bit messy when the parameter space grows), then it will be really difficult to really understand how the tool work. Even if it shows the parameters with tab-tab, the only way of checking what the meaning of each of them is look at the CLI help. Because the bash-completion is a sub-type of help-doclet, it should require the `@DocumentedFeature` annotation: that is the marker interface in Barclay for mark classes as parsed/added to the ""help"" generated by doclets....",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3596#issuecomment-331112758:527,extend,extend,527,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3596#issuecomment-331112758,1,['extend'],['extend']
Modifiability,"As an aside, I downloaded the gnomAD VCFs locally using gsutil, modified the `gnomAD_exome.config` to refer to them and it works:. ```; $ cat gnomAD_exome.config; name = gnomAD_exome; version = 2.1; src_file = gnomad.exomes.r2.1.sites.liftoverToHg38.INFO_ANNOTATIONS_FIXED.vcf.gz; # src_file = gs://broad-public-datasets/funcotator/gnomAD_2.1_VCF_INFO_AF_Only/hg38/gnomad.exomes.r2.1.sites.liftoverToHg38.INFO_ANNOTATIONS_FIXED.vcf.gz; # [...]; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6926#issuecomment-723357333:91,config,config,91,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6926#issuecomment-723357333,2,['config'],['config']
Modifiability,"As an update here, we're currently planning an upgrade to the library that we use to read bams into spark. As part of that upgrade we're going to try to fix the issue that requires 2 separate filesystem plugins for some things to work. That should enable people with hdfs file system plugins to work with gatk without a matching NIO plugin. There's no definite timeline, but hopefully within the next quarter.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3936#issuecomment-375371782:203,plugin,plugins,203,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3936#issuecomment-375371782,3,['plugin'],"['plugin', 'plugins']"
Modifiability,"As determined by @davidadamsphd , the `Copyable` interface idea won't work:. The recommendation from the Dataflow team was to make a narrow API and do the copying part of the API. I started down this route, and I think it might be doable for things like the walker interface. The idea is to make a Copyable interface and have our interfaces extend that. . However, we have unsafe code already in the engine. I tried to make this SafeDoFn approach, however it became clear quickly that we'd have a combinatorial explosion of classes because we don't just have `DoFn<GATKRead,POut>`, but also `<Iterable<GATKRead>,POut>`, and many others. So, this approach will not work for the engine. I then tried to make a general purpose solution (using coders to write to bytes and then recreate a new class). This doesn't work for a few reasons, most critical is that the coder registry isn't Serializable, so that can't be passed down deep enough to get this to work. While working on this, I chatted with someone on the Dataflow team who is working on the verification on the direct runner. He has a PR out and likely going to get it approved soon. So, for the engine, we could always test using the direct runner and know for sure there are not issues (once we can use his code). However, there are two downsides:. 1) We will need to wait for a cut of the SDK (which looking at their previous clip is likely ~ two weeks away). . 2) I don't know if we want the direct runner test as our general purpose solution. Can we expect Comp Bios to always test with the direct runner first? Will they write anything more complex than functions that use the Walker interface?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/702#issuecomment-127403661:341,extend,extend,341,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/702#issuecomment-127403661,1,['extend'],['extend']
Modifiability,"As discussed elsewhere, all IGV does is ignore the column headers and take the last column, see https://software.broadinstitute.org/software/igv/SEG. Let's have ModelSegments output two LegacySegmentCollection files with column headers `[SAMPLE, CONTIG, START, END, COPY_RATIO_POSTERIOR_50/MINOR_ALLELE_FRACTION_POSTERIOR_50]`, one for CR_50 (.cr.igv.seg) and the other for MAF_50 (.af.igv.seg). Methods can be added to ModeledSegmentCollection to create these LegacySegmentCollections. LegacySegmentCollection can inherit from AbstractSampleCollection and the write method can be overridden to suppress the SAM-style header, although documentation should be added to explain this idiosyncrasy. Should be no more than a day's work. @LeeTL1220 feel free to take this one if you have time, otherwise I'm happy to take care of it after some more high priority issues.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5037#issuecomment-407150546:515,inherit,inherit,515,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5037#issuecomment-407150546,1,['inherit'],['inherit']
Modifiability,"As discussed with @lbergelson, I tested this patch manually on my local machine as well as on a clean Google Cloud VM and found it to work perfectly in all cases. I believe that the test failures here are artifacts of some configuration issue with the Travis CI VM environment, rather than indicative of a real problem. I'll see if I can recruit an external user who ran into the requester pays issue to test this branch for additional confirmation.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7730#issuecomment-1079415266:223,config,configuration,223,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7730#issuecomment-1079415266,1,['config'],['configuration']
Modifiability,"As for your other question: the inheritance was historical, this class didn't add any feature to the SAMRecord. Yes, AVRO needs some extra work - but you can also send Java-serialized objects via Dataflow if you prefer.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/493#issuecomment-100270176:32,inherit,inheritance,32,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/493#issuecomment-100270176,1,['inherit'],['inheritance']
Modifiability,"As long as we are rolling with a single common environment across all tools, perhaps we could have a documentation annotation for tools that require it?. As I said in #4125, I vote against allowing users to configure their own environment. I really see no benefit to anyone---it's much easier on the users to just use the yml, and it's definitely much easier on us.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4127#issuecomment-357037432:207,config,configure,207,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4127#issuecomment-357037432,1,['config'],['configure']
Modifiability,At some point we deleted the lines in question because the variables are `false` by default.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4259#issuecomment-379872013:59,variab,variables,59,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4259#issuecomment-379872013,1,['variab'],['variables']
Modifiability,"Authorization settings for the connector are described here: https://github.com/GoogleCloudPlatform/bigdata-interop/blob/master/gcs/conf/gcs-core-default.xml#L50. @jamesemery have you been able to get the connector working?. @droazen what configuration improvements did you have in mind?. Also, I'm not sure what the difference between `google.cloud.auth.service.account.json.keyfile` and `fs.gs.auth.service.account.json.keyfile` is (if any).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5996#issuecomment-500755373:239,config,configuration,239,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5996#issuecomment-500755373,1,['config'],['configuration']
Modifiability,"BTW, I wouldn't bother looking at the diff. It's a complete rewrite.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5413#issuecomment-438805556:60,rewrite,rewrite,60,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5413#issuecomment-438805556,1,['rewrite'],['rewrite']
Modifiability,Back to @akiezun for a refactoring before I review,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/661#issuecomment-124222253:23,refactor,refactoring,23,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/661#issuecomment-124222253,1,['refactor'],['refactoring']
Modifiability,Back to @lbergelson for a refactoring,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/682#issuecomment-123811263:26,refactor,refactoring,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/682#issuecomment-123811263,1,['refactor'],['refactoring']
Modifiability,Back to you @lbergelson. I think that we can wait for the htsjdk release for refactoring `CachingIndexedFastaSequenceFile`.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2243#issuecomment-271819769:77,refactor,refactoring,77,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2243#issuecomment-271819769,1,['refactor'],['refactoring']
Modifiability,"Back to you, @akiezun. I think that I did almost everything that you mentioned, and I refactored names in the tool class to fit in the new framework. I believe that for some API users it would be nice to have shortcuts like the `getNumberOfDeletions()`, so I didn't remove it. But I can do it in a later commit if you really think that is unnecessary.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1836#issuecomment-220175654:86,refactor,refactored,86,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1836#issuecomment-220175654,1,['refactor'],['refactored']
Modifiability,"Barclay doesn't have any way to recognize those args as being the same. However, the Pedigree annotation classes already have this problem - its the reason that `pedigreeFile` and `founderIDs` were lifted into `GATKAnnotationPluginDescriptor`, and are not included in the individual annotations - `GATKAnnotationPluginDescriptor` distributes them to the annotation classes as necessary. Its not ideal, but it works. Can you extend that pattern ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7213#issuecomment-823482350:424,extend,extend,424,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7213#issuecomment-823482350,1,['extend'],['extend']
Modifiability,"Based on gs://broad-gotc-test-results/staging/joint_genotyping/exome/scientific/2021-09-03-11-25-15/gather_vcfs_low_memory/small_callset_high_threshold.vcf.gz (from the console output) there are slightly fewer variants filtered with ExcessHet now, which is expected since you said it was an across-the-board shift. Expected (old) has 4335 and actual (new) has 4133 -- no new things, just some now pass. If you can calculate a new equivalent threshold I'd rather use that, but otherwise I'm not overly concerned about the changes. I'm not concerned about the Jenkins call caching unless it's for the GenotypeGVCFs task where ExcessHet actually gets calculated. For the ReducibleAnnotation comments, if you just revert your changes (statics, visibility, etc.) and open an issue I'm fine with that. Admittedly this could be another target for refactoring.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7394#issuecomment-921160043:840,refactor,refactoring,840,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7394#issuecomment-921160043,1,['refactor'],['refactoring']
Modifiability,"Bayesian GMM:. This is essentially an exact port of the sklearn implementation, but only allowing for full covariance matrices. I think it might be good for those in the Bishop reading group to take a look during review. I decided to split this off into its own branch (just updated the existing branch https://github.com/broadinstitute/gatk/tree/sl_sklearn_bgmm_port) and only include stubs for the BGMM backend in the above tools. This is so we can prioritize merging the IsolationForest implementation for @meganshand. We can easily add this module back when it's been reviewed separately. TODOs:. - [x] Class-level docs.; - [x] Method-level docs. I think pointers back to the original sklearn code will suffice for most methods, but I've also included some parameter descriptions. Also note that I've retained original sklearn comments throughout the implementation and have also commented on mathematical expressions where it might be helpful.; - [x] Unit tests. There's already test data (generated using Pyro) checked in and the results match the sklearn implementation to high precision, I just need to write numerical checks. There are also already unit tests for static utility methods. Future work:; - [ ] Expanding unit tests to cover more of the interface. These initial unit tests will almost certainly not completely cover the possibilities allowed by the interface, e.g. warm starts. Could be a good exercise for other developers. EDIT: At least one test of warm starts has been added.; - [ ] As mentioned in the prototyping discussion, expanding this implementation to properly include marginalization might be of future interest. However, I think a very strong case would have to made before proceeding, as I think closely matching the sklearn implementation has obvious benefits for maintainability.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7724#issuecomment-1067948712:1802,maintainab,maintainability,1802,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7724#issuecomment-1067948712,1,['maintainab'],['maintainability']
Modifiability,"Bcftools setGT plugin is probably the best way of fixing this issue currently. ; ```; bcftools plugin setGT -h ; About: Sets genotypes. The target genotypes can be specified as:; ./. .. completely missing (""."" or ""./."", depending on ploidy); ./x .. partially missing (e.g., ""./0"" or "".|1"" but not ""./.""); . .. partially or completely missing; a .. all genotypes; b .. heterozygous genotypes failing two-tailed binomial test (example below); q .. select genotypes using -i/-e options; and the new genotype can be one of:; . .. missing (""."" or ""./."", keeps ploidy); 0 .. reference allele (e.g. 0/0 or 0, keeps ploidy); c:GT .. custom genotype (e.g. 0/0, 0, 0/1, m/M, overrides ploidy); m .. minor (the second most common) allele (e.g. 1/1 or 1, keeps ploidy); M .. major allele (e.g. 1/1 or 1, keeps ploidy); p .. phase genotype (0/1 becomes 0|1); u .. unphase genotype and sort by allele (1|0 becomes 0/1); Usage: bcftools +setGT [General Options] -- [Plugin Options]; Options:; run ""bcftools plugin"" for a list of common options. Plugin options:; -e, --exclude <expr> Exclude a genotype if true (requires -t q); -i, --include <expr> include a genotype if true (requires -t q); -n, --new-gt <type> Genotypes to set, see above; -t, --target-gt <type> Genotypes to change, see above. Example:; # set missing genotypes (""./."") to phased ref genotypes (""0|0""); bcftools +setGT in.vcf -- -t . -n 0p. # set missing genotypes with DP>0 and GQ>20 to ref genotypes (""0/0""); bcftools +setGT in.vcf -- -t q -n 0 -i 'GT=""."" && FMT/DP>0 && GQ>20'. # set partially missing genotypes to completely missing; bcftools +setGT in.vcf -- -t ./x -n . # set heterozygous genotypes to 0/0 if binom.test(nAlt,nRef+nAlt,0.5)<1e-3; bcftools +setGT in.vcf -- -t ""b:AD<1e-3"" -n 0. # force unphased heterozygous genotype if binom.test(nAlt,nRef+nAlt,0.5)>0.1; bcftools +setGT in.vcf -- -t ./x -n c:'m/M'; ```; I was always wondering if GATK will have a plugin interface where people can code their own using groovy, kotlin, javascr",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8328#issuecomment-1556119501:15,plugin,plugin,15,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8328#issuecomment-1556119501,4,"['Plugin', 'plugin']","['Plugin', 'plugin']"
Modifiability,"Because the `reference` variable is package private. If I want to call apply() from onTraversalSuccess, it needs to be protected—will push a commit to shortly to show you what I mean.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6512#issuecomment-618045813:24,variab,variable,24,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6512#issuecomment-618045813,1,['variab'],['variable']
Modifiability,"Because you implemented the original plugin descriptor, can you have a look @cmnbroad?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2353#issuecomment-274528407:37,plugin,plugin,37,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2353#issuecomment-274528407,1,['plugin'],['plugin']
Modifiability,"BedToIntevervalList is printed that way because it doesn't specify a long name, so it defaults to using the variable name. I think this is actually a bug, since it isn't using our standard argument names. ```; @Argument(shortName = StandardArgumentDefinitions.INPUT_SHORT_NAME, doc = ""The input BED file""); public File INPUT;; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1149#issuecomment-157864245:108,variab,variable,108,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1149#issuecomment-157864245,1,['variab'],['variable']
Modifiability,Being handled in a refactoring branch. I can take it out and make a small PR.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3224#issuecomment-313876142:19,refactor,refactoring,19,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3224#issuecomment-313876142,1,['refactor'],['refactoring']
Modifiability,"Blech, I can't figure out how to get it to work properly so you don't have to manually specify the output coder. My implementation:. ```; public static <A,B> PCollection<B> apply(PCollection<? extends A> input, SerializableFunction<A, B> f){; return input.apply(DataflowUtils.lift(f));; }. public static <A,B> PTransform<PCollection<? extends A>, PCollection<B>> lift(SerializableFunction<A, B> f){; return ParDo.of(new DoFn<A, B>() {; @Override; public void processElement(ProcessContext c) throws Exception {; c.output(f.apply(c.element()));; }; });; }; ```. example usage:. ```; @Test; public void testApplyToString(){; Pipeline p = GATKTestPipeline.create();; PCollection<Integer> pints = p.apply(Create.of(Arrays.asList(1, 2, 3)));. PCollection<String> presults = DataflowUtils.apply( pints, Object::toString).setCoder(StringUtf8Coder.of());. DataflowAssert.that(presults).containsInAnyOrder(""1"",""2"",""3"");; p.run();; }; ```. note the `setCoder` call, if you don't include that you get. ```; SLF4J: Class path contains multiple SLF4J bindings.; SLF4J: Found binding in [jar:file:/Users/louisb/.gradle/caches/modules-2/files-2.1/org.slf4j/slf4j-jdk14/1.7.7/25d160723ea37a6cb84e87cd70773ff02997e857/slf4j-jdk14-1.7.7.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: Found binding in [jar:file:/Users/louisb/.gradle/caches/modules-2/files-2.1/org.slf4j/slf4j-log4j12/1.7.10/b3eeae7d1765f988a1f45ea81517191315c69c9e/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: Found binding in [jar:file:/Users/louisb/.gradle/caches/modules-2/files-2.1/org.slf4j/slf4j-log4j12/1.7.5/6edffc576ce104ec769d954618764f39f0f0f10d/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.; SLF4J: Actual binding is of type [org.slf4j.impl.JDK14LoggerFactory]; java.lang.IllegalStateException: Unable to infer a default Coder for AnonymousParDo.out [PCollection]; either correct the root cause below ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/658#issuecomment-122314248:193,extend,extends,193,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/658#issuecomment-122314248,2,['extend'],['extends']
Modifiability,"BucketUtils was a solution before we had Filesystem providers. It's stuck around as a parallel set of code because we couldn't trust the providers at first. In the long run it should be removed and replaced entirely by `Files` operations. We need to test that all the functionality exists / works as expected though, and it hasn't been a high priority to do so. Particularly, I'm not sure we have a lot of faith in the HDFS NIO plugin, so we may need to keep around special cases for that. It could definitely at least be simplified a lot though.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3569#issuecomment-328993020:428,plugin,plugin,428,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3569#issuecomment-328993020,1,['plugin'],['plugin']
Modifiability,But that smoke test doesn't actually test any new functionality. The variable was there in the WDL before and was already being used - it just had no default value.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5872#issuecomment-483668246:69,variab,variable,69,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5872#issuecomment-483668246,1,['variab'],['variable']
Modifiability,"By doing the following, I was able to get a JointGenotyping result for my 343 samples:; - increased the amount of memory allocated to the Java heap in ImportGvcfs to 50000m; - modified the runtime attributes for all the joint genotyping tasks to match the format that Cromwell accepts for HPC environments (https://cromwell.readthedocs.io/en/stable/tutorials/HPCIntro/#specifying-the-runtime-attributes-for-your-hpc-tasks); - increasing the runtime memory attribute for ImportGvcfs and GenotypeGvcfs from 26000 MiB to 60 G; - executing the workflow with the following sbatch parameters:; nodes=4; ntasks=32; mem=248g; tmp=429G; - manually tar'ing up all the genomicsdb directories from the execution directories of all 10 shards of ImportGvcfs after they successfully completed GenomicsDBImport and failed with the error message: ; pure virtual method called ; terminate called without active exception; - running an abbreviated version of JointGenotyping which started at GenotypeGvcfs and executed the remainder of the JointGenotyping workflow unchanged.; ; I think this pretty clearly demonstrates that, whatever is going on, it occurs between GenomicsDBImport's successful creation of genomicsdb and the tar -cf of same. The failure is 100% reproducible with a number of different runtime configurations. The error messages are from C++ and seem to be occurring at the point where native C++ code is handing execution back to Java.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8076#issuecomment-1314069533:1293,config,configurations,1293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8076#issuecomment-1314069533,1,['config'],['configurations']
Modifiability,"By the way, I can take this one. I'm planning to do a big PR with a workaround with some of the problems of the plugin that I found to discuss them there.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2397#issuecomment-278248114:112,plugin,plugin,112,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2397#issuecomment-278248114,1,['plugin'],['plugin']
Modifiability,"By the way, I thought @vdauwera was opposed to using optional inputs in this way at some point (see #3657). Was that question ever decided? (I'm still of the opinion that they *should* be used in this way, but this is one of the reasons I didn't for this iteration of the WDL.). To be clear, the pair WDL right now does not allow all of the workflow paths (tumor-only, no PoN, etc.) that the new tools make possible. It only allows the one that we will most likely run in production (matched-normal + PoN). We should probably make the WDL a little more flexible to cover the most common use cases, but I'm fine if it doesn't completely expose all of the possible workflow paths---this would probably just make the WDL harder to maintain. Users can write their own WDLs in this case.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3983#issuecomment-362696132:553,flexible,flexible,553,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3983#issuecomment-362696132,1,['flexible'],['flexible']
Modifiability,"CKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 /share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-spark.jar CountReadsSpark --input /project/casa/gcad/test/HG04302.alt_bwamem_GRCh38DH.20150718.GBR.low_coverage.cram --reference file:///restricted/projectnb/casa/wgs.hg38/pipelines/hc/cram.test/GRCh38_full_analysis_set_plus_decoy_hla.fa.gz --spark-master yarn; 2019-01-09 13:35:04 WARN SparkConf:66 - The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 2019-01-09 13:35:05 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 13:35:09.640 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 13:35:09.799 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 13:35:11.507 INFO CountReadsSpark - ------------------------------------------------------------; 13:35:11.508 INFO CountReadsSpark - The Genome Analysis Toolkit (GATK) v4.0.12.0; 13:35:11.508 INFO CountReadsSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:35:11.508 INFO CountReadsSpark - Executing as farrell@scc-hadoop.bu.edu on Linux v2.6.32-754.6.3.el6.x86_64 amd64; 13:35:11.508 INFO CountReadsSpark - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 13:35:11.509 INFO CountReadsSpark - Start Date/Time: January 9, 2019 1:35:09 PM EST; 13:35:11.509 INFO CountReadsSpark - ------------------------------------------------------------; 13:35:11.509 INF",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:2095,variab,variables,2095,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,2,"['config', 'variab']","['configured', 'variables']"
Modifiability,"Can you give a bit more information here? If I'm understanding correctly, it's not clear that the same issue is at play here. The original issue was that duplicate/incomplete fragments were causing queries to the workspace to fail. . In this latest instance, it seems you are appending additional samples to the existing workspace. Is that right? If so,; - are you seeing the same/similar error? That is, it's a core dump? Can you share the error messages, any logs, core dump files etc?; - did you clean up the workspace before importing? That is, remove the incomplete fragment @nalinigans identified and the duplicated ones?. My first instinct is that even if the incomplete/duplicated fragments weren't cleaned up, the incremental import shouldn't have an issue -- at least not till it gets to the consolidate phase, which only happens after all batches are imported. Sounds like you were seeing an issue at batch 3 of 4, so might have something to do with the samples in that batch...or some other import issue. You mentioned that previous imports to this particular contig failed -- were those just transient failures that worked when rerun, or was there some configuration that you changed to get that to work?. For completeness, the way I identified duplicate fragments was to do an md5sum check on some of the internal files. If any pair of fragments have the same md5sum they are likely duplicates. So, from the workspace directory, something like:. ```; find . -name ""ALT.tdb"" -exec md5sum {} \;|sort; ```; That will highlight the fragments that are potentially duplicate. To confirm that the fragments are indeed duplicates, you'll then want to take that list of potentially duplicate fragments and check that all corresponding files within each pair of potentially duplicate fragments actually have the same md5sum. I have a crude bash script that I can share if you want.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6910#issuecomment-722541707:1166,config,configuration,1166,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6910#issuecomment-722541707,1,['config'],['configuration']
Modifiability,"Can you make sure that all read filter args have long-ish/unique short and long names? Single-letter names should be reserved only for important universal GATK args, not plugin args.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1973#issuecomment-231199718:170,plugin,plugin,170,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1973#issuecomment-231199718,1,['plugin'],['plugin']
Modifiability,Can you please restore `AlignmentContext`? Removing it would cause issues for my incoming HC branch. We can discuss whether to refactor it away later.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1513#issuecomment-187251531:127,refactor,refactor,127,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1513#issuecomment-187251531,1,['refactor'],['refactor']
Modifiability,"Cannot this use directly `TableReader`? Maybe it should include a `Path` constructor and being refactored a bit... Another option could be the picard `BasicInputParser`, `TabbedInputParser` or `TabbedTextFileWithHeaderParser`...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3757#issuecomment-340715685:95,refactor,refactored,95,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3757#issuecomment-340715685,1,['refactor'],['refactored']
Modifiability,CircleCI isn't configured yet. We're experimenting with both it and codeship.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/519#issuecomment-101738979:15,config,configured,15,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/519#issuecomment-101738979,1,['config'],['configured']
Modifiability,"ClipReads is already ported. Can we resolve whether this ticket is a bug report or an enhancement request or neither?. @vdauwera nope, but there's the code!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/263#issuecomment-130136318:86,enhance,enhancement,86,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/263#issuecomment-130136318,1,['enhance'],['enhancement']
Modifiability,Closing -- we've refactored the README to make it more readable.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1049#issuecomment-319415065:17,refactor,refactored,17,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1049#issuecomment-319415065,1,['refactor'],['refactored']
Modifiability,Closing. I will open a PR between today and tomorrow with all the workaround for this plugin (before the change in Barclay). We can discuss in the new one the details in a better way.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2359#issuecomment-278244313:86,plugin,plugin,86,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2359#issuecomment-278244313,2,['plugin'],['plugin']
Modifiability,"Co-assigning to @jonn-smith, since it's likely to be implemented using the new OWNER configuration facility.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3610#issuecomment-331978713:85,config,configuration,85,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3610#issuecomment-331978713,1,['config'],['configuration']
Modifiability,"Considering that this PR has lasted through my absence and the holiday season, I want to take the chance of summarizing the concerns you have issued here:. --------. Resolved as requested (or at least made efforts to):. * documenting the logic and methods; documented. * emit VCF instead of custom file format; emit both VCF custom file format now. * bug in determining if alignment signature satisfies `allMiddleAlignmentsDisjointFromAlphaOmega`; bug fixed in commit b4f7568b03b91eb77d256bcfe8117001bce040ec. --------. Unresolved yet:; the fact that gap split happens after the alignment configuration scoring step is considered backwards. I agree in principle but due to AS and MQ were used in the scoring step, and split-copy leads to technically wrong AS & MQ, I originally decided to score first, then split. Splitting the gapped alignments was introduced originally to have a centralized logic in inferring type and location of the events. . The tension is that AS is used in the scoring but becomes practically useless after that. >> Correct, but I am having thoughts about this now (not to pick only one—that; would be wrong—but to ditch them altogether probably under some condition; and redo the alignment step), exactly because of this behavior I observe.; Think about the case where one originating gapped (say insertion); alignment, after splitting, has one of the two children contained in; another alignment (not its sibling, that's impossible) in terms of their; read span. Now the originating gapped alignment probably should be filtered; out, or not, because if we keep it, an insertion would be called but; apparently there are alternative explanations due to the other alignment.; I'm not sure how to deal with this case, and if this scenario is common; enough. It probably is the case that such alignments happen mostly in STR; regions, so getting the exact alignments correct there is no easy task.; ; > Is that enough of a concern to worry about. In such a case I feel like we; ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3805#issuecomment-354976980:589,config,configuration,589,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3805#issuecomment-354976980,1,['config'],['configuration']
Modifiability,"Could this be related to having sliced objects in the gsutils buckets but not using a code path that goes through a native CRC implementation? I ask because I noticed that when I try to download the file. ```; gs://hellbender/test/resources/benchmark/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam; ```. with gsutil, I get this error:. ```; CommandException: ; Downloading this composite object requires integrity checking with CRC32c,; but your crcmod installation isn't using the module's C extension, so the; hash computation will likely throttle download performance. For help; installing the extension, please see:. $ gsutil help crcmod. To download regardless of crcmod performance or to skip slow integrity; checks, see the ""check_hashes"" option in your boto config file.; ```. Could the GATK command path be computing all of the CRC hashes in Java code, slowing it down?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1755#issuecomment-223982882:766,config,config,766,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1755#issuecomment-223982882,1,['config'],['config']
Modifiability,"Currently the code for the config file parsing will override values passed in on the command-line, so that explains one reason this is happening. The fix should be pretty straightforward.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4435#issuecomment-367715460:27,config,config,27,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4435#issuecomment-367715460,1,['config'],['config']
Modifiability,"DK Defaults.REFERENCE_FASTA : null; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:39:19.226 DEBUG ConfigFactory - Configuration file values:; 17:39:19.244 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:39:19.244 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:39:19.245 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:39:19.245 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:39:19.245 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:39:19.245 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:39:19.245 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:39:19.245 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:39:19.245 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:39:19.245 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:39:19.245 DEBUG ConfigFactory - createOutputBamIndex = true; 17:39:19.245 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 17:39:19.245 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 17:39:19.246 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 17:39:19.246 INFO PathSeqPipelineSpark - GCS ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:5535,Config,ConfigFactory,5535,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,1,['Config'],['ConfigFactory']
Modifiability,"DOWNLOAD : false; 17:39:19.226 DEBUG ConfigFactory - Configuration file values:; 17:39:19.244 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:39:19.244 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:39:19.245 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:39:19.245 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:39:19.245 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:39:19.245 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:39:19.245 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:39:19.245 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:39:19.245 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:39:19.245 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:39:19.245 DEBUG ConfigFactory - createOutputBamIndex = true; 17:39:19.245 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 17:39:19.245 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 17:39:19.246 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 17:39:19.246 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 17:39:19.246 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 17:39:19.246 INFO PathSeqPipelineSpark - Initializing engine; 17:39:19.246 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 18/04/24 17:39:19 INFO SparkContext: Running Spark version 2.2.0; 18",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:6007,Config,ConfigFactory,6007,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,1,['Config'],['ConfigFactory']
Modifiability,"Dear all,. I am a bit confused why GATK uses `[0,1,2]` for the `GT` files, even though VCF specifications clearly state that the `GT` field is `encoded as allele values separated by either of / or |`. They even say that for `diploid calls examples could be 0/1, 1 | 0, or 1/2, etc`. As it is right now, if I read a VCF from GATK CNV germline pipeline through `bcftools`, the `GT` field is changed to `-65`:; ```; 1 17345376 CNV_1_17345376_161326630 N <DUP> 101.19 . END=161326630 GT:CN:NP:QA:QS:QSE:QSS -65:3:13:11:101:3:18. 1 161332119 CNV_1_161332119_161332223 N <DEL> 3.19 . END=161332223 GT:CN:NP:QA:QS:QSE:QSS -65:1:1:3:3:3:3. 1 193091331 CNV_1_193091331_241683022 N <DUP> 268.21 . END=241683022 GT:CN:NP:QA:QS:QSE:QSS -65:3:27:34:268:36:3. 2 96919546 CNV_2_96919546_96931119 N . 62.93 . END=96931119 GT:CN:NP:QA:QS:QSE:QSS -65:2:3:38:63:38:63. 3 10183532 CNV_3_10183532_69928534 N . 469.93 . END=69928534 GT:CN:NP:QA:QS:QSE:QSS -65:2:22:31:470:19:75. 3 69986973 CNV_3_69986973_70014399 N <DUP> 10.12 . END=70014399 GT:CN:NP:QA:QS:QSE:QSS -65:3:8:4:10:4:10; ```. Any reason to not use the standaed `GT` format?. I have also noticed that GATK outputs some non-variable SVs to the VCF without any ALT allele. Why not remove them if they are actually not SVs, if `GT=0` and `CN=2`?. thanks,",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6167#issuecomment-621738904:1164,variab,variable,1164,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6167#issuecomment-621738904,1,['variab'],['variable']
Modifiability,"Discussed in person with @cmnbroad. We decided to go with a different approach that avoids the need for a central registry + connecting code, would be 100% pluggable, and would be easily extensible to variant annotations if we want to add `@Argument` capability there as well:. -replace lambda read filters with explicit class definitions; -patch the argument system to add the generic ability to discover arguments within plugin classes, and pass on populated instances of those plugin classes to the tool (likely involves only a very small amount of additional code, based on our reading of `CommandLineParser`)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1900#issuecomment-228180490:423,plugin,plugin,423,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1900#issuecomment-228180490,2,['plugin'],['plugin']
Modifiability,"Do you think that this is really necessary for a normal user? I mean, usually `UserException` should have a well-defined message String for point to the user what happened. If the stack traces are necessary, are for debugging and I think that the final user won't benefit for having an extra argument. In addition, it is in the repository README, which made it discoverable for developers and it is what it is really meant for (I guess). Other option may be to print the stack trace for `UserException`only if the verbosity is set to DEBUG, and that will get rid of the environmental variable....",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2443#issuecomment-285282288:584,variab,variable,584,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2443#issuecomment-285282288,1,['variab'],['variable']
Modifiability,Doing some additional refactoring and rebasing.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5434#issuecomment-557828837:22,refactor,refactoring,22,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5434#issuecomment-557828837,1,['refactor'],['refactoring']
Modifiability,"Done with my review. Thanks for doing this much-needed refactor! The BaseRecal stuff looks sound, but I have some other feedback that we should discuss/address.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/911#issuecomment-142750080:55,refactor,refactor,55,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/911#issuecomment-142750080,1,['refactor'],['refactor']
Modifiability,"EATE_MD5 : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.CUSTOM_READER_FACTORY :; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.REFERENCE_FASTA : null; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:39:19.226 DEBUG ConfigFactory - Configuration file values:; 17:39:19.244 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:39:19.244 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:39:19.245 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:39:19.245 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:39:19.245 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:39:19.245 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:39:19.245 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:39:19.245 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:39:19.245 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:39:19.245 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:39:19.245 DEBUG ConfigFactory - createOutputBamIndex = true; 17:39:19.245 DEBUG ConfigFactory - g",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:5066,Config,ConfigFactory,5066,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,2,['Config'],"['ConfigFactory', 'Configuration']"
Modifiability,"Evaluation of THCA/STAD/LUAD TCGA WGS/WES CR concordance with SNP arrays was implemented on FC last summer and showed good performance. For WES, comparisons against GATK CNV and CODEX showed comparable to highly improved performance, respectively, with minimal parameter tuning. WGS comparisons were unavailable due to limitations of competing tools. This evaluation will be expanded to include CR/MAF concordance against PanCanAtlas ABSOLUTE results. Some curation of the samples could be performed; some batch effects were observed in some LC WGS LUAD samples. Comparisons to other tools will probably be removed for ease of maintenance. Will be adapted to fit into whatever framework arises from #4630; same goes for HCC1143 and CRSP validations.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4122#issuecomment-459833697:648,adapt,adapted,648,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4122#issuecomment-459833697,1,['adapt'],['adapted']
Modifiability,"Even if I'll refactore it to be void, the part that I changed is need it: there is no other part which handle the `CommandLineException` and if the `customCommandLineValidation()` throws the exception no error is printed in the terminal. I guess that returning a `String[]` in Picard is done to output several errors in the command line to avoid the user to re-run with another bug not reported. Nevertheless, I prefer you approach. I'm changing now the code in this PR to add what you suggested. Thanks again for make my development smoother, @lbergelson!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2226#issuecomment-255847377:13,refactor,refactore,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2226#issuecomment-255847377,1,['refactor'],['refactore']
Modifiability,"FC was fine with previous model of having normal input bams/bais be optional parameters. I don't think FC would let you do a pair missing a sample. Typically, in FC, you would create two method configurations, one for tumor-only and one for pair. Then the former would be run on samples and the latter run on pairs.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3983#issuecomment-352624267:194,config,configurations,194,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3983#issuecomment-352624267,1,['config'],['configurations']
Modifiability,"FYI, if you set the environment variable TILEDB_DISABLE_FILE_LOCKING=1 before running any GenomicsDB tool, it doesn't try to lock files on POSIX filesystems (Lustre, NFS, xfs, ext4 etc)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4753#issuecomment-437188003:32,variab,variable,32,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4753#issuecomment-437188003,1,['variab'],['variable']
Modifiability,"Finally getting around to this @fleharty, apologies! Decided to punt on most of your requests, but hopefully my reasoning is acceptable. Perhaps we can also get it in by release and @takutosato can use this version of ModelSegments in his CRSP evaluations (and maybe even verify there are no changes from the previous version in typical, single-sample copy-ratio + allele-fraction use---since from that perspective all code changes should just be a refactor---if he has time).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6499#issuecomment-857808000:449,refactor,refactor---if,449,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6499#issuecomment-857808000,1,['refactor'],['refactor---if']
Modifiability,"Finally got the test passing in travis (silly error from refactoring previous stuff). Ready to go, @droazen!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4682#issuecomment-384608688:57,refactor,refactoring,57,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4682#issuecomment-384608688,1,['refactor'],['refactoring']
Modifiability,"Finished refactoring the production code as detailed above, just need to add some tests. Results are exactly the same as before in single-sample mode---except for allele-fraction-only mode. This is because I refactored all existing segmenters (there were separate ones for copy-ratio-only, allele-fraction-only, and copy-ratio + allele-fraction) as special cases of a single multisample segmenter; however, the Gaussian kernel in the old allele-fraction-only segmenter was missing a normalization factor that is now present in the new multisample segmenter. Thus, users who previously ran in allele-fraction-only mode will have to retune parameters to achieve the same level of sensitivity. I expect that this will be a very small number of users (if any---note that allele-fraction-only mode isn't even exposed in the WDL), but we can probably mention it in the release notes. Might need to update a figure, etc. as well in the tutorial.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6499#issuecomment-611833344:9,refactor,refactoring,9,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6499#issuecomment-611833344,2,['refactor'],"['refactored', 'refactoring']"
Modifiability,First candidate is Apache Configuration.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3078#issuecomment-307472726:26,Config,Configuration,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3078#issuecomment-307472726,1,['Config'],['Configuration']
Modifiability,"First cut at a rewrite seems to be working fine and is much, much leaner. Building a small PoN with 4 simulated normals with 5M bins each, CombineReadCounts took ~1 min, CreatePanelOfNormals (with no QC) took ~4.5 minutes (although ~1 minute of this is writing target weights, which I haven't added to the new version yet) and generated a 2.7GB PoN, and NormalizeSomaticReadCounts took ~8 minutes (~7.5 minutes of which was spent composing/writing results, thanks to overhead from ReadCountCollection). In comparison, the new CreateReadCountPanelOfNormals took ~1 minute (which includes combining read-count files, which takes ~30s of I/O) and generated a 520MB PoN, and DenoiseReadCounts took ~30s (~10s of which was composing/writing results, as we are still forced to generate two ReadCountCollections). Resulting PTN and TN copy ratios were identical down to 1E-16 levels. Differences are only due to removing the unnecessary pseudoinverse computation. Results after filtering and before SVD are identical, despite the code being rewritten from scratch to be more memory efficient (e.g., filtering is performed in place)---phew!. If we stored read counts as HDF5 instead of as plain text, this would make things much faster. Perhaps it would be best to make TSV an optional output of the new coverage collection tool. As a bonus, it would then only take a little bit more code to allow previous PoNs to be provided via -I as an additional source of read counts. Remaining TODO's:. - [x] Allow DenoiseReadCounts to be run without a PoN. This will just perform standardization and optional GC correction. This gives us the ability to run the case sample pipeline without a PoN, which will give users more options and might be good enough if the data is not too noisy.; - [x] Actually, I'm not sure why we take perform SVD on the intervals x samples matrix and take the left-singular vectors. <s>Typically, SVD is performed on the samples x intervals matrix and the right-singular vectors are taken, ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351:15,rewrite,rewrite,15,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351,1,['rewrite'],['rewrite']
Modifiability,"Fixed for filtering in #5498. `Mutect2` itself will be trickier because a lot of unused arguments are a result of `SomaticGenotypingEngine`'s inheritance. This will involve significant refactoring, but the genotyping classes will make more sense.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5352#issuecomment-445517991:142,inherit,inheritance,142,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5352#issuecomment-445517991,2,"['inherit', 'refactor']","['inheritance', 'refactoring']"
Modifiability,Fixed missing end-of-line backslashes in commands and made a few additional enhancements to the doc text,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2781#issuecomment-309632487:76,enhance,enhancements,76,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2781#issuecomment-309632487,1,['enhance'],['enhancements']
Modifiability,"Fixing womtool is a bit more complicated, since it affects FireCloud method configuration parameters as well.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4319#issuecomment-362118499:76,config,configuration,76,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4319#issuecomment-362118499,1,['config'],['configuration']
Modifiability,"For now, this request will be treated together in this issue. We may split it later. @vdauwera - i moved it here because it's about ReadTransformers. https://www.pivotaltracker.com/story/show/70281952. ReadTransformers should be able to be added by the command line like; filters. Currently the only way to include or add a new readTransformer is; by adding a new argument to the GATK engine, and there is no way to run; only specific transformer by a walker.; When that will be done, there are readTransformers that are currently; written as filter (for example: ReassignMappingQuality) that should be; refactor to readTransformers.; Also it will be good to put all the ReadTransformers together (like the; filters).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6#issuecomment-66436109:604,refactor,refactor,604,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6#issuecomment-66436109,1,['refactor'],['refactor']
Modifiability,"For running the tests on the cloud, it needs to be configured. Since we don't want to hardcode project details in the code (we don't all write to the same cloud project), I'm using environment variables. For the cloud tests to pass we have to set:TEST_PROJECT and TEST_STAGING_GCS_FOLDER.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/529#issuecomment-105577911:51,config,configured,51,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/529#issuecomment-105577911,2,"['config', 'variab']","['configured', 'variables']"
Modifiability,"For some reason, the environment variable is not getting passed to GenomicsDB at all. To help debug the issue, can you do the following and see if any consolidation lock is created at all? ; ```; find /path/to/genomicsdb_workspace -name "".__consolidation_lock""; ```; Also, what type of Posix filesystem is your GenomicsDB workspace? Is it NFS or Lustre? How is file locking configured on the system?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6627#issuecomment-637237653:33,variab,variable,33,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6627#issuecomment-637237653,2,"['config', 'variab']","['configured', 'variable']"
Modifiability,"For the second point (file format) I would prefer something different than Java Properties, because for lists will be a bit messy (separated by comma in Apache Configuration). Maybe YML or JSON will be better for this purpose?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3078#issuecomment-307729448:160,Config,Configuration,160,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3078#issuecomment-307729448,1,['Config'],['Configuration']
Modifiability,"Found a serious bug in this PR: `passesEmitThreshold()` was calling `passesCallThreshold(configuration.genotypeArgs.STANDARD_CONFIDENCE_FOR_CALLING)` instead of `passesCallThreshold(conf)`, which caused `STANDARD_CONFIDENCE_FOR_CALLING` to get compared against itself! Pushing a fix for this now.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2314#issuecomment-286781671:89,config,configuration,89,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2314#issuecomment-286781671,1,['config'],['configuration']
Modifiability,"From a high level, users really shouldn't be worrying about this. The Hadoop defaults are often adequate. This is not a perfect analogy, but you generally don't think about block sizes for your usual Linux file system. Also, the default 4 MB size that it's currently set to is very small. The individual Spark tasks finish in just a few seconds, which means you're probably paying more overhead than you need. > 1. How do you handle situations where you need different split sizes for different file types?. You can manually specify the block sizes for individual files in HDFS. It's possible that you could also specify the mapreduce split sizes in the `InputFormat`, but I'd need to ask @tomwhite. > 1. How do you handle setting reasonable defaults so it works right out of the box without people having to set their spark configs explicitly?. Have we already decided that the default Spark configs are bad?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1166#issuecomment-158515933:825,config,configs,825,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1166#issuecomment-158515933,2,['config'],['configs']
Modifiability,"Given that no one (not even stackoverflow) seems to know how to set the root logging level programmatically in log4j 2.x (rather than through a config file), perhaps we should consider downgrading to log4j 1.x, where setting the root logging level was a one-liner:. ```; Logger.getRootLogger().setLevel(level);; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/243#issuecomment-76847662:144,config,config,144,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/243#issuecomment-76847662,1,['config'],['config']
Modifiability,"Given that this helps you out, but doesn't change the behavior of our tools, it's pretty much a refactor from our end and I'm happy to give it a 👍",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2534#issuecomment-334156894:96,refactor,refactor,96,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2534#issuecomment-334156894,1,['refactor'],['refactor']
Modifiability,"Glad you were able to resolve your issue. Not sure if this is specific to the CNV tool or if the exception caused by the Spark configuration is more general. Tagging engine team @droazen, but closing for now.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5686#issuecomment-467510488:127,config,configuration,127,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5686#issuecomment-467510488,1,['config'],['configuration']
Modifiability,"Good to know -- forgot that we had `--arguments_file`. . The advantage of the config file approach would be that you wouldn't have to specify any extra args -- it would just look in the current directory or a known location for a file with your dataflow settings. Things like the pipeline runner would probably still be manually specified, though (perhaps with nice aliases like `--cloud` and `--local`)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/508#issuecomment-100311128:78,config,config,78,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/508#issuecomment-100311128,1,['config'],['config']
Modifiability,"HI @lbergelson - ; I'm working on a bug/warning in the variant calling workflow where it's complaining about not finding a logger:. `21:04:59.525 INFO ProgressMeter - Starting traversal; 21:04:59.526 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; log4j:WARN No appenders could be found for logger (io.grpc.netty.shaded.io.netty.util.internal.logging.InternalLoggerFactory).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 21:05:10.018 INFO ProgressMeter - chr1:4642050 0.2 205000 1172992.6`. I found that if I had gatk's build.gradle NOT exclude the log4j.properties file I get rid of that warning, so I'm trying to understand the issue [here](https://github.com/broadinstitute/gatk/blob/33bda5e08b6a09b40a729ee525d2e3083e0ecdf8/build.gradle#L441): (where you found log4j.properties clashed with log4j2.xml) . James Emery is on the git blame for that, but he thinks that's because of the refactoring he did. Thanks in advance - I'm not sure if there's something else I should be doing with the xml version of that file to avoid this warning.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7778#issuecomment-1098029722:1023,refactor,refactoring,1023,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7778#issuecomment-1098029722,1,['refactor'],['refactoring']
Modifiability,"HTSJDK Defaults.CUSTOM_READER_FACTORY :; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.REFERENCE_FASTA : null; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:39:19.226 DEBUG ConfigFactory - Configuration file values:; 17:39:19.244 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:39:19.244 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:39:19.245 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:39:19.245 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:39:19.245 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:39:19.245 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:39:19.245 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:39:19.245 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:39:19.245 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:39:19.245 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:39:19.245 DEBUG ConfigFactory - createOutputBamIndex = true; 17:39:19.245 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:39:19.245 DEBU",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:5129,Config,ConfigFactory,5129,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,1,['Config'],['ConfigFactory']
Modifiability,"HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:39:19.226 DEBUG ConfigFactory - Configuration file values:; 17:39:19.244 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:39:19.244 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:39:19.245 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:39:19.245 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:39:19.245 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:39:19.245 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:39:19.245 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:39:19.245 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:39:19.245 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:39:19.245 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:39:19.245 DEBUG ConfigFactory - createOutputBamIndex = true; 17:39:19.245 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 17:39:19.245 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 17:39:19.246 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 17:39:19.246 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 17:39:19.246 INFO PathSeqPipelineSpark - Using googl",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:5612,Config,ConfigFactory,5612,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,1,['Config'],['ConfigFactory']
Modifiability,"Ha, this PR was much tinier than I expected -- feet are barely damp. My ""wishlist"" would include a much bigger refactor because I made a mess in Java7 and didn't have the time to clean up (especially with generics) after we switched in Java8. I'm still tinkering with the rank sum tests though, so it's not worth tackling the refactor until those are good to go. :+1:",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2657#issuecomment-299206702:111,refactor,refactor,111,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2657#issuecomment-299206702,2,['refactor'],['refactor']
Modifiability,"Hello @SZLux,. This looks suspiciously like #3050. I suspect this isn't a PathSeq issue, but to be sure can you please try to run another GATK Spark tool such as CountReadsSpark? If that does not work, it's likely an issue with your configuration or Spark/Java versions being incompatible. . What kind of environment are you running in? My suspicion is you are running on a cluster and have the correct Spark/Java version on the driver (master node) but perhaps not on the workers.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383712768:233,config,configuration,233,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383712768,1,['config'],['configuration']
Modifiability,"Hello @cmnbroad. My current solution satisfy all the constraints and it's not too complicated, although is not as simple as a common generic class that just need to be extended. Have a look and if you like it I can implement some tests for `CountingVariantFilter`; if not, I could come back to a separate `CountingVariantFilter` with its own and/or/negate inner classes.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2195#issuecomment-272482490:168,extend,extended,168,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2195#issuecomment-272482490,1,['extend'],['extended']
Modifiability,"Hello @nalinigans,. As part of gatk-sv pipeline we are using GATK : v4.1.8.1 which doesn't have bypass-feature-reader option. Also, we didn’t capture strace for the run with just ""--genomicsdb-shared-posixfs-optimizations"" so wont be able to share the FUTEX process counts. So after using v4.2.4.1 we get below results. 	- Using ""--genomicsdb-shared-posixfs-optimizations"" & ""--bypass-feature-reader"" the process took 118 mins.; ""FUTEX_WAIT_PRIVATE, 0, NULL"" : 1266. 	- Using ""--genomicsdb-shared-posixfs-optimizations"" & ""--bypass-feature-reader"" and ; TILEDB_UPLOAD_BUFFER_SIZE=5242880 as env variable the process took 113 mins.; 	""FUTEX_WAIT_PRIVATE, 0, NULL"" : 3. 	- Even using 10 MB as buffer size resulted in same execution time of 113 mins.; 	- Using a buffer size bigger i.e. 50 MBs caused the process to run slower so we aborted it. Please let us know if we can improve it further.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7646#issuecomment-1040947845:595,variab,variable,595,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7646#issuecomment-1040947845,1,['variab'],['variable']
Modifiability,"Hello, . I have the same problem, . path_input_file=/work/gr-fe/archive/sample_repository/all_exome_gvcfs_hg38/FVH/exomes #patient GVCF; path_name_individu=/work/gr-fe/sboutry/excalibur/input/11_12_23_gvcf_FVH/patient_name.txt; path_output=/work/gr-fe/sboutry/excalibur/input/11_12_23_gvcf_FVH/patient_data.vcf.gz; tmp_folder=/scratch/sboutry/logs/combine_gvcf_file; nbr_groups=2. #Path to database and programs; REF=/work/gr-fe/saadat/Reference_Genome/GRCH38_no_alt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa.gz; BCFTOOLS=/work/gr-fe/sboutry/tools/bcftools/install/bin/bcftools; export BCFTOOLS_PLUGINS=/work/gr-fe/sboutry/tools/bcftools/bcftools/plugins; TABIX=/work/gr-fe/sboutry/tools/tabix/tabix-0.2.6/tabix; GATK=/work/gr-fe/sboutry/tools/gatk/gatk-4.2.2.0/gatk. cd ${path_input_file}. ${GATK} --java-options ""-Xmx180G -XX:ParallelGCThreads=36"" CombineGVCFs -R ${REF} --variant ${path_name_individu} -O ${path_output}/patient_data.g.vcf.gz. where all my files are like this . JL0015.g.vcf.gz; JL0016.g.vcf.gz; JL0017.g.vcf.gz; JL0018.g.vcf.gz; JL0019.g.vcf.gz; JL0020.g.vcf.gz; JL0182.g.vcf.gz; JL0183.g.vcf.gz; JL0184.g.vcf.gz; JL0185.g.vcf.gz; JL0186.g.vcf.gz; JL0234.g.vcf.gz; JL0278.g.vcf.gz; JL0412.g.vcf.gz; JL0417.g.vcf.gz; JL0515.g.vcf.gz. A USER ERROR has occurred: Cannot read file:///work/gr-fe/sboutry/excalibur/input/11_12_23_gvcf_FVH/patient_name.txt because no suitable codecs found. Thanks a lot for any help . Best, . Simon",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8255#issuecomment-1918989841:652,plugin,plugins,652,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8255#issuecomment-1918989841,1,['plugin'],['plugins']
Modifiability,"Hello:. Thanks for info. . I don’t suppose it is my role to militate/plead for the solid fix of the this omission. But I must say that it would be appreciated and in its own way advance science. Thanks for any consideration. Cheers,; Chuck. > On 10/Feb/2017, at 9:45 AM, Valentin Ruano Rubio <notifications@github.com> wrote:; > ; > @vruano not in my radar either.... the rewrite of the assembly may fix some of these cases where there is actually some variation that we fail to detect (false negative) that would explain those soft clips. However I don't think that would fix all the cases.; > ; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub, or mute the thread.; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/269#issuecomment-279021246:372,rewrite,rewrite,372,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/269#issuecomment-279021246,1,['rewrite'],['rewrite']
Modifiability,Here's another one with our exact problem (solved largely by putting the config onto HDFS). http://progexc.blogspot.com/2014/12/spark-configuration-mess-solved.html,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3079#issuecomment-322554798:73,config,config,73,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3079#issuecomment-322554798,2,['config'],"['config', 'configuration-mess-solved']"
Modifiability,"Here's another way. It's not perfect either; it doesn't get rid of the DoFn. Type erasure really limits our options here. ``` java; public class MapFn<A,B> extends DoFn<A,B> {; protected SerializableFunction<A,B> fn;. public MapFn(SerializableFunction<A,B> fn) {; this.fn = fn;; }. @Override; public void processElement(DoFn<A, B>.ProcessContext c) throws Exception {; c.output(fn.apply(c.element()));; }; }; ```. Used like this:. ``` java; PCollection<Integer> presults = pipeline; .apply(Create.of(Arrays.asList(1, 2, 3))); .apply(ParDo.of(new MapFn<Integer, Integer>(x -> x + 1) {}));; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/658#issuecomment-122997546:156,extend,extends,156,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/658#issuecomment-122997546,1,['extend'],['extends']
Modifiability,"Here's some code you can add to the test to check that jbwa actually works. ```; @Test; public void testBwaMemAlignSingleRead() throws Exception {; final String libraryPath = NativeUtils.runningOnLinux() ? ""/lib/libbwajni.so"" : ""/lib/libbwajni_mac.jnilib"";; Assert.assertTrue(NativeUtils.loadLibraryFromClasspath(libraryPath), ""jbwa library was not loaded. "" +; ""This could be due to a configuration error, or your system might not support it."");. BwaIndex index= new BwaIndex(new File(b37_reference_20_21));; final BwaMem bwaMem = new BwaMem(index);; //real read taken from src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam; final String name=""20FUKAAXX100202:3:46:9213:168594"";; final byte[] seqs= ""GTTTTGTTTACTACAGCTTTGTAGTAAATTTTGAACTCTAAAGTGTTAGTTCTCTAACTTTGTTTGTTTTTCAAGAGTGTTTTGACTCTTCTTACTGCATC"".getBytes(); ;; final byte[] quals= ""DGFDGFDHFFFFGFEFHEGFFFGGHEHFHGHHGEGGGGGFGFHGHGHEHGGGFGAEFGDACAHHDHGCGFGGGFGDHGHFFHDDCGGDGEE"".getBytes();; final ShortRead read = new ShortRead(name, seqs, quals);; final AlnRgn[] align = bwaMem.align(read);; Assert.assertEquals(align.length, 1);; Assert.assertEquals(align[0].getChrom(), ""20"");; Assert.assertEquals(align[0].getCigar(), ""101M"");; Assert.assertEquals(align[0].getMQual(), 60);; Assert.assertEquals(align[0].getPos(), 9999997-1); #note difference from the bam file (9999997 in bam, 9999996 here); Assert.assertEquals(align[0].getNm(), 0);; bwaMem.dispose();; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1847#issuecomment-220755636:386,config,configuration,386,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1847#issuecomment-220755636,1,['config'],['configuration']
Modifiability,"Here's some old code that uses SamLocusIterator (from tfennel) that AllelicCapseg can adapt for now. From Tim:; ""They key to making this nice and simple is the SamLocusIterator class, which given a BAM file and a list of intervals, will give you pileups at each position in the intervals, filtered how you want them, and even provide convenience methods to access the exact base per read that is piled up at the site etc. The really nice things about doing it this way is that the constructor to SamLocusIterator takes a simple parameter to tell it whether to use an index/query mechanism (similar to what you're doing now) or to just read the BAM serially up until the last interval is reached and output the loci of interest. Running the below with ~100k sites on a standard exome (15GB or so) without using the index took only about 15 minutes."". ```; public void pileup(final File bam, final IntervalList intervals, final int minQ, final File outputFile) {; final int MAX_INTERVALS_FOR_INDEX = 25000; // just a guess, not sure what the right number is. final SamLocusIterator iterator = new SamLocusIterator(new SAMFileReader(bam), intervals, intervals.size() < MAX_INTERVALS_FOR_INDEX);; iterator.setEmitUncoveredLoci(false);; iterator.setQualityScoreCutoff(minQ);. final BufferedWriter out = IoUtil.openFileForBufferedWriting(outputFile); // will automatically gzip if filename ends with .gz; try {; while (iterator.hasNext()) {; final SamLocusIterator.LocusInfo locus = iterator.next();; int a=0, c=0, g=0, t=0;. for (final SamLocusIterator.RecordAndOffset rec : locus.getRecordAndPositions()) {; switch (rec.getReadBase()) {; case 'A' : ++a; break;; case 'C' : ++c; break;; case 'G' : ++g; break;; case 'T' : ++t; break;; }; }. out.append(locus.getSequenceName() + ""\t"" + locus.getPosition() + ""\t"" + a + ""\t"" + c + ""\t"" + g + ""\t"" + t + ""\t"");; }. out.close(); ; }; catch (IOException ioe) { throw new RuntimeIOException(ioe); }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/335#issuecomment-88102420:86,adapt,adapt,86,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/335#issuecomment-88102420,1,['adapt'],['adapt']
Modifiability,"Here’s one example executor from a recent run. This the the variants broadcast (4 seconds using 100MB blocks). ```; 16/04/11 20:55:24 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 3; ...; 16/04/11 20:55:28 INFO broadcast.TorrentBroadcast: Reading broadcast variable 3 took 3913 ms; ...; 16/04/11 20:55:35 INFO storage.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 655.5 MB, free 2.9 GB); ```. This the the reference broadcast (130 seconds using 100MB blocks). ```; 16/04/11 20:55:35 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 4; ...; 16/04/11 20:57:46 INFO broadcast.TorrentBroadcast: Reading broadcast variable 4 took 130726 ms; ...; 16/04/11 20:57:47 INFO storage.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 739.6 MB, free 1505.0 MB); ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1675#issuecomment-208656364:193,variab,variable,193,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1675#issuecomment-208656364,4,['variab'],['variable']
Modifiability,"Hey @bbimber I will have to think on this. The most simple solution might be to add a feature context side input for the annotation in question but looking at how that code is threaded in the variant callers it would take a little bit of work to add it to those tools and probably introduce some complicated questions, (like for example: what is the correct featurecontext to send to annotate a variant that only covers one base of the site in question where the feature context object exists?). Its possible to do something like that for variant annotator a little bit more easily but i guess the question comes down to this: How generalized do you think this annotation will be? Does it need to be annotatable with variant annotator or could you write a separate tool that does the variant -> variant association and calculates the annotation without using the plugin framework? If it needs to be generalizable I would agree with @droazen that the easiest approach would be to add the side input as an argument and make the annotation object responsible for querying the feature context. This is inelegant but might be preferable to putting the entire walker context into the `annotate()` function.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6930#issuecomment-754249851:863,plugin,plugin,863,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6930#issuecomment-754249851,1,['plugin'],['plugin']
Modifiability,"Hey @magicDGS, since I'm on the forum documentation side of things, I'm not familiar with a plugin. Is this something @cmnbroad typically takes care of?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3809#issuecomment-342820936:92,plugin,plugin,92,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3809#issuecomment-342820936,1,['plugin'],['plugin']
Modifiability,Hey @valleema. We don't think this is a bug given that the argument `--max-mnp-distance` is intended to remove Multi-Nucliotide polymorphism which generally are adjacent SNPs(i.e. sites with the pattern ref: AA alt: GT for example). Your example site here doesn't have an MNP but rather is a multi-allelic site. Consequently the flag `--max-mnp-distance` is not doing anything wrong in this case. I would direct future questions about your use case (namely how to un-collapse multi-allelic sites) to our forums: https://gatk.broadinstitute.org/hc/en-us/community/topics. Thank you;,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7782#issuecomment-1115218078:128,polymorphi,polymorphism,128,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7782#issuecomment-1115218078,1,['polymorphi'],['polymorphism']
Modifiability,"Hey all, @vruano / @davidbenjamin, is this on your radar at all? I heard rumblings about a rewrite of the assembly machinery; would that address this?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/269#issuecomment-278955477:91,rewrite,rewrite,91,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/269#issuecomment-278955477,1,['rewrite'],['rewrite']
Modifiability,"Hey all, I'm still interested in supporting this. We don't really have a ""plugin API"", I am in fact the API, but if you give me something usable I'll plug it in. As this is marked ""QuixoticDream"" I don't think that's likely. I'm closing the corresponding IGV issue, too many open issues, but it doesn't mean I've lost interest.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3286#issuecomment-433201230:74,plugin,plugin,74,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3286#issuecomment-433201230,1,['plugin'],['plugin']
Modifiability,"Hi - @kaixinxiaonvwa-hub . I have a couple of questions:. - Can you post your full stack trace with the errors?; - Did you attempt to enable `gnomAD` data sources or is it doing this without any changes to the data sources directory? Did you do any other configuration steps after downloading the datasources and before running funcotator?. If you enable `gnomAD`, the datasources are hosted on google cloud. If you don't have an internet connection or google cloud is blocked, Funcotator will not be able to connect to read the gnomAD data and will show the error in your `1` case above.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8275#issuecomment-1494703773:255,config,configuration,255,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8275#issuecomment-1494703773,1,['config'],['configuration']
Modifiability,"Hi @MattMcL4475 - if you mean the images I used to test these, they are: `terrapublic.azurecr.io/gatk:4.5-squashed` and `terrapublic.azurecr.io/gatk:4.5-min-layers`. Note that these are manually built and pushed for this task and didn't go through any automated tests that are in this repo.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8808#issuecomment-2159093348:157,layers,layers,157,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8808#issuecomment-2159093348,1,['layers'],['layers']
Modifiability,"Hi @MattMcL4475 - so sorry I didn't see your reply until now (likely due to my email filters). Anyway, I don't think we have a squashed version of this in the official GATK image. From the conversation above, I think we decided to go with just the reduced layers version of the image which is what this PR is for. I do, however, have a sample squashed version here: `terrapublic.azurecr.io/gatk:4.5-squashed` - but then again, it's not in the official Docker hub repo.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8808#issuecomment-2231781482:256,layers,layers,256,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8808#issuecomment-2231781482,1,['layers'],['layers']
Modifiability,"Hi @Yyx2626, I'm Geraldine, you may remember me from the Beijing training. It was great visiting your team! I'm sorry it took me so long to follow up on this discussion, and I want to thank you again for reaching out to us about integrating the tool that you developed into GATK. We are certainly very interested in providing this enhancement to the research community, and we are now ready to talk about the next steps. . After examining your paper and the source code in Github, we think that the most efficient way to integrate the functionality you developed would be to adapt the filtering parts of your tool to run on the output of Mutect2. So this would be a standalone tool that you would run after Mutect2, much like the current FilterMutectCalls tool. . If the results are comparable to your current tool, then we would take that into the official distribution of GATK. If somehow that integration does not yield satisfactory results, then we would look at integrating the entire tool, though we're hoping it won't be necessary, so we can avoid maintaining duplicate functionality for some of the boilerplate data transformations. . David @davidbenjamin can provide some advice on how to implement this in GATK4; in brief you would need to write some code that applies the filters you developed to a variant context. Let us know if this is an option you'd like to explore; we'd be happy to help.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4632#issuecomment-403101973:331,enhance,enhancement,331,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4632#issuecomment-403101973,2,"['adapt', 'enhance']","['adapt', 'enhancement']"
Modifiability,"Hi @bhanugandham . I wasn't able to reproduce your problem on my laptop. Based on the error message, it looks like a network config Spark issue. What environment are you running this in? If it's MacOS, can you post the contents of `/etc/hosts`? Also the error message looks like it's truncated. You might be able to get the full stack trace by adding `--verbosity DEBUG`, which would be helpful as well.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5802#issuecomment-473419380:125,config,config,125,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5802#issuecomment-473419380,1,['config'],['config']
Modifiability,"Hi @cwhelan , I've expanded this PR to do more than what it originally was trying to fix, and separated the patches by commits as usual:. * the originally proposed fix, which brings back the annotation that are available to simple variants but go missing due to a careless bug, is now done in commit 50f1b640a31ddb528dc763b83b26a9d98dce8556; this commit also accordingly refactors the giant class `CpxVariantDetector` into three new classes; * in the 2nd commit 734516383fb665a79796de76535560fc03cb754b, I did more refactoring on how we group the descriptions for the annotation keys, and updated the test VCF files accordingly.; * because of the refactoring, the review comments were gone, so I added them back in the 3rd commit b7619c45a949dfba21d65a5ed876bc72e832aa77, which contains the comments and my replies. They come in as TODO's but are going to be removed ultimately; * in the following commits, I added tests for the CPX code path, selecting three representative cases (there's no limit how complex the scenario can go). One particular commit 224c97c7b736e94ed6b4d8b067ec830a9f8f2403 is large but most of it is for adding a flat file that contains the chromosome names in hg38 and their lengths for building a bare bone sequence dictionary used in building test data.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4330#issuecomment-372761525:371,refactor,refactors,371,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4330#issuecomment-372761525,3,['refactor'],"['refactoring', 'refactors']"
Modifiability,"Hi @georgiiprovisor ,. Sorry this fell though the cracks. Did you find a workaround? I suspect some of our I/O refactoring caused this warning to be triggered incorrectly. Happy to take a look if that's still useful. -Laura",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8564#issuecomment-2435404906:111,refactor,refactoring,111,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8564#issuecomment-2435404906,1,['refactor'],['refactoring']
Modifiability,"Hi @hh1985 . Memory tuning is pretty tricky and can depend on a lot of things. How is your cluster configured? ; Are you using YARN? Are you running in client or cluster mode? . I'm assuming you're running with YARN. Mesos should also work but I don't have any experience configuring it. . BQSR should run safely with 4g of memory per core. (It should really work with much less I think, but 4 should definitely be sufficient.) There are a few different parameters that can help you adjust the memory ratios.; A good tuning might be something like; ; ```; --num-executors 5 ; --executor-cores 8 ; --executor-memory 32g ; ```. if you're not running with gatk-launch you'll need to set; ```; --conf spark.yarn.executor.memoryOverhead=600; ```; Without setting a higher than default yarn memory overhead like this we see consistent crashes, it's included in the settings gatk-launch applies already. That should run 5 separate executors with 8 cores each and give each one 32g, so 4g / core. . If you're running in cluster mode you'll have to carve out some memory and cores for the driver. You can set the driver settings with ; ```; --driver-cores 2; --driver-memory 4g; ``` ; or something along those lines. The driver doesn't need much memory or computer for BQSR. In general we've had better luck using the entire cluster for one job and running jobs in sequence rather than trying to run two jobs simultaneously using a subset of the cluster.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3465#issuecomment-324064738:99,config,configured,99,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3465#issuecomment-324064738,2,['config'],"['configured', 'configuring']"
Modifiability,Hi @icemduru ; Looks like your slurm workload manager was configured to have a limit of 48GBs of maximum process memory size per execution. Your java instance is set with -Xmx45G which will cover most of this limit and leaves only a handful of memory space for the native GenomicsDB library. Native libraries work above the heapsize so it is better for you to set your -Xmx to a more sensible size of 8~12GB and leave rest of the memory space to the native library to use. . Keep in mind that this memory limit on slurm could be set per user not per task therefore you may need to run a single contig at a time or maybe 2 of them simultaneously. Otherwise slurm may interefere with all the tasks and cancel all your jobs. . One final reminder. We strongly recommend users to set the temporary directory to somewhere else other than /tmp. Slurm workload manager interferes with this preference and sometimes results in premature termination of the gatk processes due to deletion of extracted native library and accessory files. . I hope this helps.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8918#issuecomment-2283694332:58,config,configured,58,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8918#issuecomment-2283694332,1,['config'],['configured']
Modifiability,"Hi @jean-philippe-martin ,. A `Feature` in our codebase has a specific meaning that is different from ""interval"": it is a record that 1. has a location on the genome plus (typically) some metadata information about that location and 2. is in one of the formats supported by our file-parsing framework tribble and is the product of a tribble codec. A VCF record is an example of a `Feature`. . The common interface between `Feature` and `SimpleInterval` is called `Locatable`. I recommend (for now) that you simply alter your uprooted version of BQSR to take a `List<? extends Locatable>` instead of a `List<? extends Feature>` in `apply()`. This should require no code changes beyond changing method parameter types, and it will allow you to feed BQSR `SimpleIntervals` for the known sites for now, and `Features` like VCF records later on when we're ready for that. Please do return `ArtificialTestFeature` to the `FeatureDataSourceUnitTest` from which it came -- this is a very incomplete class meant only for testing purposes and not for external use.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/511#issuecomment-100393247:568,extend,extends,568,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/511#issuecomment-100393247,2,['extend'],['extends']
Modifiability,Hi @jonn-smith I had some success getting outputs with this. However ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/funcotator/ is no longer accessible. The most recent version of I accessed on 3/19/2018 still contained at least one error. I'm trying to correct them myself as I go using a more recent build of GATK but it would be helpful if the data files required by this program were available. The one I found is in `gencode_xrefseq.config` where it references a source that doesn't exist and I fixed that. After that I was able to get outputs with hg38. Thanks for your work on this!. I'd also point out there are a lot of fields in the MAF with `__UNKNOWN__` as the entry,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4521#issuecomment-383387645:447,config,config,447,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4521#issuecomment-383387645,1,['config'],['config']
Modifiability,Hi @lvzenglei ; This is not an issue that needs any fix in fact this is the default behavior that Mutect2 and HaplotypeCaller will have. Extending MNP distance does not change any of the Smith Waterman or PairHMM parameters that will eventually decide on the model used to genotype the region. If you are interested in getting larger complex events genotyped you may need to get longer reads (preferably 2x300) and use read backed phasing before you try genotyping any such events. In that case you may need to experiment with Smith Waterman and PairHMM parameters to prefer mismatches over SNPs but this still may not result in what you are actually looking for. One better approach would be to use read backed phasing along with post processing your phased variants to convert individual phased SNPs and INDELs into COMPLEX calls by other tools. . I hope this helps. Regards.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8310#issuecomment-2421513998:137,Extend,Extending,137,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8310#issuecomment-2421513998,1,['Extend'],['Extending']
Modifiability,Hi @potter-s ; Our docker image is already built with root account only however PATH is set to be usable by all users so if you wish to keep user priviledges after execution you may add ` -u $UID:$GID` parameter to docker command line therefore the container will run using your user permissions. . This has a catch of course. Temporary folders must be set where your user has RWX permissions therefore we want users to pay attention to that. There is a writing that we posted a while ago which you may refer to for setting up your temporary files for GATK workflows. . [How to setup temporary folder for GATK local executtion](https://gatk.broadinstitute.org/hc/en-us/articles/18965297287067-How-to-setup-and-use-temporary-folder-for-GATK-local-execution). For some of the tools such as gCNV or CNN you may need to setup additional environment variables to locate python compilation directory to a place where you have read and write permissions. . I hope this helps.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8856#issuecomment-2145780965:845,variab,variables,845,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8856#issuecomment-2145780965,1,['variab'],['variables']
Modifiability,"Hi @ruslan-abasov,. I believe your GermlineCNVCaller results should have inherited the correct dictionary from the count files. The issue is you created some GermlineCNVCaller shards (e.g., shard 4) with inappropriately ordered intervals (since these were instead ordered w.r.t. to the idiosyncratic dictionary you attached). However, I think if you just reshard and rerun GermlineCNVCaller for any such shards, you may be able to reuse most of your results. For example, you could take your shard 4 interval list, which contains intervals from chr18, chr19, and chr1, and reshard these intervals into two shards: 4a containing chr18-19 intervals, and 4b containing chr1 intervals. After rerunning 4a and 4b through GermlineCNVCaller, you should be able to use PostprocessGermlineCNVCalls to stitch together shards 4b, 1, 2, 3, and 4a, since these will be ordered w.r.t. the correct dictionary from the count files (i.e, they will contain intervals in the order chr1, chr10-19). Of course, you will want not want to perform this exact procedure; you'll want to generalize it to whatever will yield the correct order for all 10 of your shards across all contigs. Again, this may be error prone and I can't guarantee that it will be successful, since I haven't tried it myself. I would personally just rerun the pipeline. You might be able to cut down on runtime by using smaller shards (I believe we typically shard the entire genome into far more than 10 shards, which we usually run in parallel) and making sure you set parameters appropriately for WGS. @mwalker174 has the most experience running on WGS and should be able to provide you the latest recommendations, or you might be able to find them by searching GitHub or the GATK Forums. Your point is well taken about failing earlier, and I think I've outlined the best strategy above. It is impossible to catch all possible errors early, but for some we can certainly fail before the GermlineCNVCaller step.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6924#issuecomment-720091802:73,inherit,inherited,73,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6924#issuecomment-720091802,1,['inherit'],['inherited']
Modifiability,Hi @yurivict I tested this on my machine and it works for me. Do you see a message saying the version was overridden?; ```; $ ./gradlew printVersion -Drelease=true -DversionOverride=myVersion1.2. > Configure project :; Version number overridden as myVersion1.2. > Task :printVersion; myVersion1.2; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7143#issuecomment-857796023:198,Config,Configure,198,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7143#issuecomment-857796023,1,['Config'],['Configure']
Modifiability,"Hi DarioS. FastaAlternateReferenceMaker is a really simple tool. It actually just looks at the alternate alleles at each site and uses the first non-symbolic one to make the fasta. It doesn't even look at the genotypes. So it should work fine with a multisample vcf but it will give you a mush of samples together as a single fasta. I could be extended to be smarter but it's not a high priority for us right now. . We should improve the documentation, I had to go look in the code to see what it was doing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7557#issuecomment-969237729:344,extend,extended,344,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7557#issuecomment-969237729,1,['extend'],['extended']
Modifiability,"Hi Stefan,. If there is no obvious error (e.g., is `/media/Berechnungen/GATKTest/CN_transition_matrix_autosomal.tsvx` the correct filename, rather than `/media/Berechnungen/GATKTest/CN_transition_matrix_autosomal.tsv`? Does the file exist and is it correctly formatted?), then I would guess that this is likely an error with your nd4j configuration. Just to let you know, we have significantly revamped the both somatic and germline CNV pipelines for the release in January. If you would like a preview of the germline tool, you may want to look at this branch: https://github.com/broadinstitute/gatk/tree/sl_gcnv_ploidy_cli However, be aware that it is still under development.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3996#issuecomment-352760467:335,config,configuration,335,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3996#issuecomment-352760467,1,['config'],['configuration']
Modifiability,"Hi all. Apologies for writing in an old issue. ; has this been fixed?; With java 1.8 and GATK 4.1.3.0 I think I'm getting the same error (in this case with HaplotypeCallerSpark). Any idea on how to extend the size?. The errors are:. `org.broadinstitute.hellbender.exceptions.UserException: Max size of locatable exceeded. Max size is 5000, but locatable size is 8638. Try increasing shard size and/or padding. Locatable: Contig1:65711-74348; 	at org.broadinstitute.hellbender.engine.spark.SparkSharder$5.computeNext(SparkSharder.java:293); 	at org.broadinstitute.hellbender.engine.spark.SparkSharder$5.computeNext(SparkSharder.java:281); 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143); 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138); 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.TransformedIterator.hasNext(TransformedIterator.java:43); 	at java.util.Spliterators$IteratorSpliterator.tryAdvance(Spliterators.java:1811); 	at java.util.stream.StreamSpliterators$WrappingSpliterator.lambda$initPartialTraversalState$0(StreamSpliterators.java:294); 	at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.fillBuffer(StreamSpliterators.java:206); 	at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.doAdvance(StreamSpliterators.java:161); 	at java.util.stream.StreamSpliterators$WrappingSpliterator.tryAdvance(StreamSpliterators.java:300); 	at java.util.Spliterators$1Adapter.hasNext(Spliterators.java:681); 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.Iterators$5.hasNext(Iterators.java:547); 	at java.util.Spliterators$IteratorSpliterator.tryAdvance(Spliterators.java:1811); 	at java.util.stream.StreamSpliterators$WrappingSpliterator.lambda$initPartialTraversalState$0(StreamSpliterators.java:294); 	at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2554#issuecomment-530773994:198,extend,extend,198,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2554#issuecomment-530773994,1,['extend'],['extend']
Modifiability,"Hi, ; for those looking to run containers within a multi-user HPC environment, running a container with default root privileges presents a potential data security risk. Adding something like :. RUN useradd -ms /bin/bash gatk; WORKDIR /home/gatk; USER gatk. to the Docker file would greatly reduce the risk and bring the current containers in line with general best practice, e.g https://medium.com/@mccode/processes-in-containers-should-not-run-as-root-2feae3f0df3b. There should be no downsides to running in this manner. Singularity could help but the current configuration will be picked up and prevented from running by any site using a container security scanner, e.g. Aqua.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3644#issuecomment-494457377:562,config,configuration,562,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3644#issuecomment-494457377,1,['config'],['configuration']
Modifiability,"Hi, I also meet this issue. . The Specificity in `HaplotypeCallerSpark` was a bitter less than local mode in my test. Do you known which configuration can improve the Specificity in `HaplotypeCallerSpark` ? @Atahualkpa",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5323#issuecomment-433815315:137,config,configuration,137,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5323#issuecomment-433815315,1,['config'],['configuration']
Modifiability,"Hi, I think this can be closed now. . I just found that this issue might due to the inconsistency between gCNV version and PostProcessGermlineCNVCalls version. ; I updated my python configuration from 4.1.8.0 to 4.2.2.0 and the issue is gone. . Sorry for the troubles here.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7444#issuecomment-908550017:182,config,configuration,182,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7444#issuecomment-908550017,1,['config'],['configuration']
Modifiability,"Hi, I'm trying to generate a VCF with Mitochondrial mode of Mutect2 based on MT amplicon (PCR) sequencing. But I encountered two problems:; 1. The base site (Pos: MT:16320) has high depth and good quality, but the result of AF is low. In IGV, AF~=0.5; 2. The total depth(DP) is also quite different from the depth in bam; I tried changing various parameters but nothing seems to make a difference? and even tried turn on --disable-tool-default-read-filters. Is there a parameter I'm missing? What are the filtering criteria? How to adapt to PCR data?. version: GATK4.1.4.1; java -Xmx16g -Djava.io.tmpdir=./tmp -jar gatk-package-4.1.4.1-local.jar Mutect2 -I tmp.bam -R hs37d5.fa -L MT.bed -O raw.vcf --min-pruning 5 --mitochondria-mode --max-reads-per-alignment-start 10000. MT 16182 . A AC,ACC . . DP=262;ECNT=7;MBQ=31,26,29;MFRL=0,0,0;MMQ=60,60,60;MPOS=44,44;OCM=0;POPAF=2.40,2.40;TLOD=84.81,46.04 GT:AD:AF:DP:F1R2:F2R1:SB 0/1/2:157,72,31:0.261,0.118:260:122,48,21:0,0,0:0,157,0,103; MT 16183 . A C . . DP=262;ECNT=7;MBQ=30,34;MFRL=0,0;MMQ=60,60;MPOS=45;OCM=0;POPAF=2.40;TLOD=531.07 GT:AD:AF:DP:F1R2:F2R1:SB 0/1:58,204:0.780:262:42,190:0,0:0,58,0,204; MT 16188 . CT C . . DP=262;ECNT=7;MBQ=34,29;MFRL=0,0;MMQ=60,60;MPOS=50;OCM=0;POPAF=2.40;TLOD=19.25 GT:AD:AF:DP:F1R2:F2R1:SB 0/1:243,19:0.070:262:207,15:0,0:0,243,0,19; MT 16189 . T C . . DP=262;ECNT=7;MBQ=35,34;MFRL=0,0;MMQ=60,60;MPOS=51;OCM=0;POPAF=2.40;TLOD=931.43 GT:AD:AF:DP:F1R2:F2R1:SB 0/1:2,260:0.989:262:2,237:0,0:0,2,0,260; MT 16266 . C A . . DP=1073;ECNT=4;MBQ=7,32;MFRL=0,0;MMQ=60,60;MPOS=50;OCM=0;POPAF=2.40;TLOD=3575.47 GT:AD:AF:DP:F1R2:F2R1:SB 0/1:14,1039:0.996:1053:4,469:0,446:0,14,481,558; MT 16274 . G A . . DP=1073;ECNT=4;MBQ=33,12;MFRL=0,0;MMQ=60,60;MPOS=53;OCM=0;POPAF=2.40;TLOD=1.89 GT:AD:AF:DP:F1R2:F2R1:SB 0/1:1057,11:5.214e-03:1068:571,1:337,4:467,590,10,1; MT 16320 . C T . . DP=643;ECNT=4;MBQ=27,36;MFRL=0,0;MMQ=60,60;MPOS=21;OCM=0;POPAF=2.40;TLOD=11.29 GT:AD:AF:DP:F1R2:F2R1:PGT:PID:PS:SB 0|1:564,60:0.017:624:48,60:358,",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5193#issuecomment-575001931:532,adapt,adapt,532,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5193#issuecomment-575001931,1,['adapt'],['adapt']
Modifiability,"Hi,. Thanks for the response. Running with -u isn’t ideal as we can’t control; how the user runs this (unless they do this on their own hardware or say a; cloud instance). However, I managed to convert the docker image into a singularity one and; that runs ‘out of the box’ in user space. Simon. On 3 Jun 2024, at 18:43, Gökalp Çelik ***@***.***> wrote:. Hi @potter-s <https://github.com/potter-s>; Our docker image is already built with root account only however PATH is; set to be usable by all users so if you wish to keep user priviledges after; execution you may add -u $UID:$GID parameter to docker command line; therefore the container will run using your user permissions. This has a catch of course. Temporary folders must be set where your user; has RWX permissions therefore we want users to pay attention to that. There; is a writing that we posted a while ago which you may refer to for setting; up your temporary files for GATK workflows. How to setup temporary folder for GATK local executtion; <https://gatk.broadinstitute.org/hc/en-us/articles/18965297287067-How-to-setup-and-use-temporary-folder-for-GATK-local-execution>. For some of the tools such as gCNV or CNN you may need to setup additional; environment variables to locate python compilation directory to a place; where you have read and write permissions. I hope this helps. —; Reply to this email directly, view it on GitHub; <https://github.com/broadinstitute/gatk/issues/8856#issuecomment-2145780965>,; or unsubscribe; <https://github.com/notifications/unsubscribe-auth/ABU3SAWISO2HSCUNHK3SGIDZFSTK5AVCNFSM6AAAAABIWRNXGKVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDCNBVG44DAOJWGU>; .; You are receiving this because you were mentioned.Message ID:; ***@***.***>",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8856#issuecomment-2155884154:1229,variab,variables,1229,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8856#issuecomment-2155884154,1,['variab'],['variables']
Modifiability,"Hi,; Glad to know that you have tested GATK4 with Amazon S3 using NIO file system plugin. ; I have been stuck on this process for long...I would really appreciate if you could share the work around procedure detail for this.; Thanks in advance !; Senthil",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3708#issuecomment-354368839:82,plugin,plugin,82,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3708#issuecomment-354368839,1,['plugin'],['plugin']
Modifiability,"Hi,; I tried your commands (and many adaptions / changements) but I always get the same problem:; If the command line includes `--`, I get the JNI linkage error as if the spark related parameters were not parsed.; I tried many things, as:; > /home/axverdier/Tools/GATK4/gatk-4.beta.6/gatk-launch CountReadsSpark --programName gatk4-testing --input hdfs://spark01:7222/user/axverdier/data/710-PE-G1.bam --output hdfs://spark01:7222/user/axverdier/testOutGATK_CountReadsSpark --javaOptions -Dmapr.library.flatclass -- --sparkRunner SPARK --sparkMaster yarn --deploy-mode cluster; > /home/axverdier/Tools/GATK4/gatk-4.beta.6/gatk-launch CountReadsSpark --programName gatk4-testing --input hdfs://spark01:7222/user/axverdier/data/710-PE-G1.bam --output hdfs://spark01:7222/user/axverdier/testOutGATK_CountReadsSpark --javaOptions -Dmapr.library.flatclass --sparkRunner SPARK --sparkMaster yarn -- --master yarn --deploy-mode cluster. > /home/axverdier/Tools/GATK4/gatk-4.beta.6/gatk-launch CountReadsSpark --programName gatk4-testing --input hdfs://spark01:7222/user/axverdier/data/710-PE-G1.bam --output hdfs://spark01:7222/user/axverdier/testOutGATK_CountReadsSpark --javaOptions -Dmapr.library.flatclass --sparkRunner SPARK --sparkMaster yarn -- --master yarn --deploy-mode cluster --conf spark.driver.extraJavaOptions='-Dmapr.library.flatclass' --conf spark.executor.extraJavaOptions='-Dmapr.library.flatclass'. > /home/axverdier/Tools/GATK4/gatk-4.beta.6/gatk-launch CountReadsSpark --programName gatk4-testing --input hdfs://spark01:7222/user/axverdier/data/710-PE-G1.bam --output hdfs://spark01:7222/user/axverdier/testOutGATK_CountReadsSpark --javaOptions -Dmapr.library.flatclass --sparkRunner SPARK --sparkMaster yarn -- --master yarn --deploy-mode cluster --driver-java-options '-Dmapr.library.flatclass'. It's a non-exhaustive list, I tried a lot of configurations similar to these ones.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3933#issuecomment-350227061:37,adapt,adaptions,37,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3933#issuecomment-350227061,2,"['adapt', 'config']","['adaptions', 'configurations']"
Modifiability,"Hmm, I am also getting intermittent build errors like the following much more often:. ```; Could not determine the dependencies of task ':sparkJar'.; > Could not resolve all files for configuration ':sparkConfiguration'.; > Could not download gson.jar (com.google.code.gson:gson:2.2.2); > Could not get resource 'https://repo.maven.apache.org/maven2/com/google/code/gson/gson/2.2.2/gson-2.2.2.jar'.; > Could not GET 'https://repo.maven.apache.org/maven2/com/google/code/gson/gson/2.2.2/gson-2.2.2.jar'. Received status code 403 from server: Forbidden; > Could not download core.jar (com.github.fommil.netlib:core:1.1); > Could not get resource 'https://repo.maven.apache.org/maven2/com/github/fommil/netlib/core/1.1/core-1.1.jar'.; > Could not GET 'https://repo.maven.apache.org/maven2/com/github/fommil/netlib/core/1.1/core-1.1.jar'. Received status code 403 from server: Forbidden; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6513#issuecomment-601832142:184,config,configuration,184,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6513#issuecomment-601832142,1,['config'],['configuration']
Modifiability,"Hmm, actually, could this be a problem due to the way the native libraries are loaded in the test code? Note that we first cycle through all implementations in the DataProvider, loading the respective library for each implementation via the `synchronized boolean load` method in the `NativeLibraryLoader`. I'm not really that familiar with concurrency in Java (nor loading native libraries, for that matter), but it seems that the intermittent failure goes away when I refactor the test to remove the DataProvider (by just looping through the implementations in the test method). Perhaps related to https://github.com/broadinstitute/gatk/issues/5339?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607596205:469,refactor,refactor,469,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607596205,1,['refactor'],['refactor']
Modifiability,"How about: first check an environment variable for the location of the GATK jar, and if it's not set check the script's working directory for a jar? Whatever we do should be consistent across the spark and non-spark jars.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1693#issuecomment-207473378:38,variab,variable,38,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1693#issuecomment-207473378,1,['variab'],['variable']
Modifiability,How did you install the GATK Conda environment? Looks like a problem with the conda environment configuration. One possible reason could be that conda environment is not the version 4.3.0.0 is requesting.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8952#issuecomment-2287925550:96,config,configuration,96,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8952#issuecomment-2287925550,1,['config'],['configuration']
Modifiability,"I _believe_ your issue is that you are assigning 600GB to execution of cromwell, but the error is with the call to **VariantRecalibrator** in one of the tasks not having enough memory. A few tasks call **VariantRecalibrator**, do you know which task failed? Can you post the java call from the STDERR file? For me, it was task **SNPsVariantRecalibrator** which was assigned only 3.5GB of memory by default. In [joint-discovery-gatk4.wdl](https://github.com/gatk-workflows/gatk4-germline-snps-indels/blob/master/joint-discovery-gatk4.wdl), the memory assigned for each task can be set via ""machine_mem_gb"", but it looks like the current [input.json](https://github.com/gatk-workflows/gatk4-germline-snps-indels/blob/master/joint-discovery-gatk4.hg38.wgs.inputs.json) does not have that variable, but instead ""mem_size"" for each task. . A simple solution would be to replace ${java_mem} with a static value in calls to **VariantRecalibrator** (lines 564 & 684). For example, replace:. `${gatk_path} --java-options ""-Xmx${java_mem}g -Xms${java_mem}g""`. with. `${gatk_path} --java-options ""-Xmx100g -Xms100g""`. I'm not certain this will help, but I think it's a step in the right direction.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6165#issuecomment-571396381:785,variab,variable,785,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6165#issuecomment-571396381,1,['variab'],['variable']
Modifiability,"I addressed partially your comments (and fixed a compilation error due to the tests using the previous arguments). One of the major points of discussion are the following:. * `Collection` instead of `List`: I think that the first is more flexible, because a client maybe wants to have a `LinkedHashSet` as the argument to avoid repetition of the same filter. I agree that the abstract class should discourage not honoring the user order.; * Access to methods/fields: I think that the plugin could be used outside GATK in a different way by extending it. I explained some of my usage cases in one of the comments in the code, but just by overriding a simple method the whole plugin could be used very nicely in some of them. I would prefer to do that than copy your code and re-implement the bits that I would like to change. Back to you for your ideas on this, @cmnbroad!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2355#issuecomment-275359208:238,flexible,flexible,238,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2355#issuecomment-275359208,4,"['extend', 'flexible', 'plugin']","['extending', 'flexible', 'plugin']"
Modifiability,"I addressed some of your comments, @droazen. If you would like to have a properties file for the configuration, I will need some help on setting it up (although I will try by my own too). Although I still set up some of the environment in `Main`, now the `CommandLineProgram` class have the same instance passed by `Main`. Looking forward for your comments on the updates.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2322#issuecomment-271845715:97,config,configuration,97,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2322#issuecomment-271845715,1,['config'],['configuration']
Modifiability,"I agree with all Chris has said, and think that it's very likely that you're running out of memory on the executors. You might try cutting back on --num-executors, and bumping up --executor-memory.; If you can figure out your adapter sequence, you can specify that as --adaptor-sequence, and sometimes that helps with this stage.; We're laying down asphalt, and you're driving on the hot pavement just behind us. Thanks for trying out this tool.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4635#issuecomment-380144314:226,adapt,adapter,226,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4635#issuecomment-380144314,2,['adapt'],"['adapter', 'adaptor-sequence']"
Modifiability,"I also just noticed that tests are failing on the branch because they still reference the old constants in a number of places:. ```; symbol: variable READ_NAME_LONG_NAME; location: class ReadNameReadFilter; /gatk/src/test/java/org/broadinstitute/hellbender/cmdline/GATKPlugin/GATKReadFilterPluginDescriptorTest.java:117: error: cannot find symbol; { PlatformReadFilter.class.getSimpleName(), ""--"" + PlatformReadFilter.PL_FILTER_NAME_LONG_NAME, ""fakePlatform"" }, ; ```. You'll need to update these references in order to get tests passing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4103#issuecomment-360806542:141,variab,variable,141,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4103#issuecomment-360806542,1,['variab'],['variable']
Modifiability,"I also tried the following approach which did not generate an error:. 1. imported the 10 not-reblocked gvcfs from chr16 into genomicsdb ; 2. GenotypeGVCFs with the same command line as number 3 above. . So the error appears to be related to the reblocking of the gvcfs. ```; gendb:///restricted/projectnb/kageproj/gatk/genomicsdb/genomicsDB.chr16; Using GATK jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx60g -jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar GenotypeGVCFs -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -G StandardAnnotation -G AS_StandardAnnotation -V gendb:///restricted/projectnb/kageproj/gatk/genomicsdb/genomicsDB.chr16 -L chr16:105582-211160 --use-new-qual-calculator --only-output-calls-starting-in-intervals TRUE --genomicsdb-shared-posixfs-optimizations TRUE --tmp-dir tmp -O chr16-105582-211160.vcf.gz; 07:46:18.893 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 07:46:18.944 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Aug 25, 2021 7:46:19 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 07:46:19.128 INFO GenotypeGVCFs - ------------------------------------------------------------; 07:46:19.128 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.0.0; 07:46:19.128 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 07:46:19.129 INFO GenotypeGVCFs - Executing as farre",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7437#issuecomment-905431278:455,variab,variable,455,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7437#issuecomment-905431278,1,['variab'],['variable']
Modifiability,I ask that because for htsjdk defaults they must be system properties and they're final and set statically on load so mucking about resetting system properties after the JVM started already is going to be a bit of a fiddly ordering nightmare. . [Stack overflow](http://stackoverflow.com/questions/6736235/set-java-system-properties-with-a-configuration-file) doesn't seem to think that it's possible to initialize them from a file.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2316#issuecomment-267126704:339,config,configuration-file,339,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2316#issuecomment-267126704,1,['config'],['configuration-file']
Modifiability,"I attended a journal club some months ago where a paper stated most researchers use default settings of tools. The paper benchmarked tool with default settings and with tweaked parameters. I can dig up the paper if anyone is interested. So there is some expectation from the user community that the default parameters reflect some sweet spot parameterization for running the tool. . I highly support @cmnbroad's suggestion for making argument sets callable by one flag. For exomes, please do not label flag as `WES`. We want to refer instead to _targeted exomes_, so `EXOMES` or variation is preferable.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4719#issuecomment-386411104:342,parameteriz,parameterization,342,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4719#issuecomment-386411104,1,['parameteriz'],['parameterization']
Modifiability,I believe I've had this issue before but with different tools as well.; If you are on nextflow below is a config for scope docker. ```; docker.fixOwnership; Fix ownership of files created by the docker container.; ```. There is also another scope that could be set if there is only a single user. ```; docker.runOptions; This attribute can be used to provide any extra command line options supported by the docker run command. See the [Docker documentation](https://docs.docker.com/engine/reference/run/) for details.; ```; This one enables passing -u parameter to docker directly. . If none of them are set in the nextflow config then I would first suggest these options. If not we can escalate this with the team.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8233#issuecomment-2078156593:106,config,config,106,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8233#issuecomment-2078156593,2,['config'],['config']
Modifiability,"I can contribute to this one too if I have time, because I will need it too. My suggestion is to create a plugin for the annotation engine. There are also some issues that will be nice to address/discuss too before I start contributing:. * __Allow custom packages for Annotations in VariantAnnotatorEngine__ (https://github.com/broadinstitute/gatk/issues/2155): I think that `VariantAnnotatorEngine` should be modify directly to use annotation classes and let the plugin deal with the reflection code to pick the instances for each String. This will help in my work trying to use `VariantAnnotatorEngine` with custom annotation classes, and let the package constraint to the plugin itself (and at some point, to the GATK configuration).; * __GenotypeAnnotations require GT, but some of them are computable without them__(https://github.com/broadinstitute/gatk/issues/1730): some of this annotations are useful for my work, which does not include samples with genotypes. Nevertheless, I cannot use them unless I hack my `Genotype` object beforehand. I think that this could be solved by setting a flag in the plugin and set the genotype annotation with it, to allow the computation for non-called samples.; * It would be nice to have in the plugin an argument collection (similar to #2355), allowing other implementations for the arg parsing part. I will appreciate if these things are included/addressed at the same time, and I would love to contribute with some code to help you and be a beta-tester from the API point of view.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1880#issuecomment-288768668:106,plugin,plugin,106,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1880#issuecomment-288768668,6,"['config', 'plugin']","['configuration', 'plugin']"
Modifiability,"I can definitely appreciate that in some cases downstream analysis might be made easier if the original representations of GGA mode alleles were preserved. . Internally, HaplotypeCaller has to convert all variants at a position to share the same reference context so that read alignments can be compared to all possible alternate alleles simultaneously, and it would be a complex and error-prone process to re-map the unified alleles back to their original representations, and would also pose problems in terms of computing the correct values for INFO field annotations such as DP if the output VCF had to be split across multiple lines according to the how things were specified in the input files. I'm going to close this for now unless you strongly object or have an additional case that shows an error in GGA mode output. It's possible that in the future we could implement an enhancement in the form of a mode that preserves the GGA input allele representations, possibly under some stricter conditions upon the input, but that would likely be a tricky implementation. Pinging @ldgauthier to make sure she agrees.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5385#issuecomment-435902950:882,enhance,enhancement,882,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5385#issuecomment-435902950,1,['enhance'],['enhancement']
Modifiability,"I can't use it because of that, and it have lots of variables that I'm not using. I'm doing my own base test class, but I'd love to have a more general base test class in GATK to extend, without that many variables specific for this repository. Thanks a lot for your interest!. Should I do something for this PR? . So should I",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2122#issuecomment-242839085:52,variab,variables,52,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2122#issuecomment-242839085,3,"['extend', 'variab']","['extend', 'variables']"
Modifiability,"I concur, what it looks like we have here is code that switched (accidentally?) from the GCS adapter to GCS-NIO instead.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2287#issuecomment-265014643:93,adapt,adapter,93,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2287#issuecomment-265014643,1,['adapt'],['adapter']
Modifiability,"I definitely like the idea of moving in this direction, and it will become more compelling as we extend the GATKSparkTool hierarchy, which is currently pretty flat and doesn't mirror the GATKTool hierarchy. I think we should also consider using some of the concepts from the metrics refactoring, which introduces a layer that separates the implementation of the processing logic from the containing tool/driver, and allows a single implementation (i.e. metrics collector, but could be FlagStats, CountReads, SelectVariants, whatever) to be independent of the source and/or destination of the data. It adds more moving parts, but has the advantage of allowing a single implementation to be used from any of Spark tool, standalone tool, Spark pipeline, standalone pipeline, etc.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2217#issuecomment-254216118:97,extend,extend,97,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2217#issuecomment-254216118,2,"['extend', 'refactor']","['extend', 'refactoring']"
Modifiability,"I did some additional runs on the Broad cluster on a 14 GB bam, twice as large as the bam used in the plot above. This was with 60 cores, 4 cores per executor, and 16 GB of memory per executor. Results:. Broadcast (3 runs): 10m52.020s, 11m46.975s, 10m17.274s; Sharded (3 runs): 19m33.310s, 13m36.466s, (died from out-of-memory error). Not sure what's going on here, but something about this configuration is favorable to Broadcast and unfavorable to Sharded. We should try to understand what and why. Below are the commands I used to run each implementation on dataflow01, for reference:. ```; spark-submit \; --master yarn-client \; --driver-memory 8G \; --num-executors 16 \; --executor-cores 4 \; --executor-memory 16G \; --conf spark.driver.maxResultSize=0 \; --conf spark.driver.userClassPathFirst=true \; --conf spark.executor.userClassPathFirst=true \; --conf spark.io.compression.codec=lzf \; --conf spark.yarn.executor.memoryOverhead=600 \; $JAR BaseRecalibratorSpark \; --input hdfs:///user/droazen/bqsr/CEUTrio.HiSeq.WGS.b37.NA12878.1m-130m.bam \; --output bqsr_out_${1}.bam \; -R hdfs:///user/droazen/bqsr/human_g1k_v37.2bit \; --knownSites hdfs:///user/droazen/bqsr/dbsnp_138.b37.1m-130m.vcf \; --joinStrategy BROADCAST \; --apiKey $API_KEY \; --sparkMaster yarn-client. spark-submit \; --master yarn-client \; --driver-memory 8G \; --num-executors 16 \; --executor-cores 4 \; --executor-memory 16G \; --conf spark.driver.maxResultSize=0 \; --conf spark.driver.userClassPathFirst=true \; --conf spark.executor.userClassPathFirst=true \; --conf spark.io.compression.codec=lzf \; --conf spark.yarn.executor.memoryOverhead=600 \; --conf spark.akka.frameSize=1024 \; $JAR BaseRecalibratorSparkOptimized \; --input gs://droazen-testing/spark/CEUTrio.HiSeq.WGS.b37.NA12878.1m-130m.bam \; --output bqsr_out_${1}.bam \; -R hdfs:///user/droazen/bqsr/human_g1k_v37.fasta \; --knownSites /local/dev/droazen/new_test/dbsnp_138.b37.1m-130m.vcf \; -L 1:1-130000000 \; --apiKey $API_KEY \; --sparkMaster",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/995#issuecomment-157825077:391,config,configuration,391,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/995#issuecomment-157825077,1,['config'],['configuration']
Modifiability,I did this PR because of the GenomeLocParser; what you suggested was the first thing that I tried. I did this instead of refactoring the BaseTest because I didn't want to cause major changes in the code.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2122#issuecomment-242854061:121,refactor,refactoring,121,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2122#issuecomment-242854061,1,['refactor'],['refactoring']
Modifiability,"I didn't do a line by line review since it seemed like Adam did a very thorough job there. The overall structure looks good to me though. It seems like metrics collection would be a good candidate for being made a plugin system, as in the ReadFilter branch. That should definitely be a different pass on this though if necessary.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1959#issuecomment-233690780:214,plugin,plugin,214,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1959#issuecomment-233690780,1,['plugin'],['plugin']
Modifiability,"I discovered that one of the 345 input gvcfs failed VCF validation. When I removed that file and reran with no other changes, I did not get the ""terminate called without an active exception"" error. However, ImportGvcfs still fails; the failure seems to occur immediately after GenomicsDBImport logs success in importing all batches, in each shard. From all the Cromwell logs it looks like everything is working, but the top level workflow execution fails. I've been trying various configurations of memory, scatter count, and #nodes, so I don't have those log files around still. I can rerun with -DGATK_STACKTRACE_ON_USER_EXCEPTION=true and see if I get anything useful.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8076#issuecomment-1295310651:481,config,configurations,481,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8076#issuecomment-1295310651,1,['config'],['configurations']
Modifiability,"I do not think you should have two versions. Here is a task example from; another workflow:. ```; output {; File cnv_acs_conversion_skew = ""${output_skew_filename}""; Float cnv_acs_conversion_skew_float =; read_float(output_skew_filename); String cnv_acs_conversion_skew_string =; read_string(output_skew_filename); }; ```. While this adds some clutter, at least the task (and workflow) produces a; file, float, or string. Then you can decide which you actually want to; attach to the data model via the method configuration output. Clutter vs. fork? I say ""clutter"". Also, you may only need one alternate; type. On Tue, Jul 2, 2019 at 9:52 AM ldgauthier <notifications@github.com> wrote:. > *@ldgauthier* requested changes on this pull request.; > ------------------------------; >; > In scripts/cnv_wdl/germline/cnv_germline_case_workflow.wdl; > <https://github.com/broadinstitute/gatk/pull/6017#discussion_r299492696>:; >; > > @@ -242,6 +250,7 @@ workflow CNVGermlineCaseWorkflow {; > Array[File] gcnv_tracking_tars = GermlineCNVCallerCaseMode.gcnv_tracking_tar; > Array[File] genotyped_intervals_vcf = PostprocessGermlineCNVCalls.genotyped_intervals_vcf; > Array[File] genotyped_segments_vcf = PostprocessGermlineCNVCalls.genotyped_segments_vcf; > + Array[File] qc_status_files = CollectSampleQualityMetrics.qc_status_files; >; > Ideally I'd want to be able to flag failing samples in an obvious way in; > the workspace, like having new fields in the data model called; > ""sample_quality"" and ""model_quality"" with the QC status reported there. Are; > we violently opposed to having a Cromwell version and a Firecloud version; > of this WDL? (@LeeTL1220 <https://github.com/LeeTL1220>); > ------------------------------; >; > In scripts/cnv_wdl/cnv_common_tasks.wdl; > <https://github.com/broadinstitute/gatk/pull/6017#discussion_r299493991>:; >; > > @@ -453,3 +453,98 @@ task PostprocessGermlineCNVCalls {; > File genotyped_segments_vcf = genotyped_segments_vcf_filename; > }; > }; > +; > +task Col",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6017#issuecomment-507695717:510,config,configuration,510,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6017#issuecomment-507695717,1,['config'],['configuration']
Modifiability,"I don't really like the idea of a `--fix_misencoded_quality_scores` arg at the `GATKTool` level hooked into the data source -- I'd prefer to see your read transformer PR (https://github.com/broadinstitute/gatk/pull/2085) get merged, and then make read transformers a plugin that can be turned on/off on the command line, like we can with read filters. I've asked @cmnbroad to add a comment here outlining what would have to be done to make read transformers a plugin.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2084#issuecomment-245959488:267,plugin,plugin,267,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2084#issuecomment-245959488,2,['plugin'],['plugin']
Modifiability,"I don't think it's much different to have a zip file with 2 jars in it vs a zip file with a directory tree. This will add an additional layer of complexity to testing. I.e. do we test the packaged jars, now our unit tests run with a different configuration than the other tests, etc.. . What we probably really want is some sort of executable archive that wraps whatever we put in it into a single runnable file. I'm not sure how to do that exactly, but if self extracting zip files are a thing there should be a way to do it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1693#issuecomment-215791645:243,config,configuration,243,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1693#issuecomment-215791645,1,['config'],['configuration']
Modifiability,"I don't think that M2 should be spewing out a concatenation of all alleles,; since that does imply ploidy via the VCF spec. Multiallelics on multiple; lines violates the spec too, right?. On Mon, Sep 18, 2017 at 9:28 PM, David Benjamin <notifications@github.com>; wrote:. > @chandrans <https://github.com/chandrans> @chapmanb; > <https://github.com/chapmanb> @ldgauthier <https://github.com/ldgauthier>; > The -ploidy argument is one of several arguments inherited from; > AssemblyRegionWalker that apply only to HaplotypeCaller and do nothing in; > Mutect2. (We should refactor this once the engine team's workload; > lightens enough to review lower-priority things like this.) The GT field; > emitted by Mutect is just the concatenation of all called alleles -- 0/1,; > 0/1/2, 0/1/2/3 etc -- and doesn't imply anything about the ploidy. Maybe we; > should get rid of it entirely since AF is so much more informative.; >; > I like the idea of splitting multiallelics into multiple lines. It would; > make filtering a lot easier. @LeeTL1220 <https://github.com/leetl1220> do; > you have an opinion?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/3564#issuecomment-330401348>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk2Bmhl2i8xOPd4zC_WqQ9GtNbRKNks5sjxi1gaJpZM4PTWbd>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 8011A; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3564#issuecomment-330527699:455,inherit,inherited,455,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3564#issuecomment-330527699,2,"['inherit', 'refactor']","['inherited', 'refactor']"
Modifiability,"I don't think that hiding/disable arguments would work in every case: sometimes, an argument shouldn't be exposed but still available to set programmatically, or maybe just reduce visibility making it `@Hidden` and/or `@Advance`. What is the problem of making an interface for the top-level argument to the GATK? Changing the interface or the `CommadnLineProgram` has the same effect, but the API user can still behave the same as before. It is much more extensible and downstream-friendly. What's about making the `CLPConfigurationArgumentCollection` an interface always returning defaults to be able to change it in a proper way? The cycle of development of a new argument will be: 1) add a new method to the interface with a default returning what will be expected from the previous behaviour, 2) add and return by the argument in the GATK implementation, 3) use the getter in the CLP for perform the operation. This only adds the first point, and operating in 3 classes instead of 3. For API user it is really easy to maintain the previous behavior when upgrading the dependency by just using their own implementation of the class, or include the top-level new arguments by using the GATK implementation. It is much more flexible and extensible (I always think about GATK also as a library). In addition, I think that this approach is also important for evolving GATK. For example, if a new top-level argument is tagged as experimental (still not supported but requested in Barclay), removing it would allow to keep the interface (no version bump) the same and final users can still operate with the experimental argument. The same applies to the `GATKTool` base class (https://github.com/broadinstitute/gatk/issues/4341), and for downstream projects the aim should be to be able to extend safely the `CommandLineProgram` directly to implement their own toolkit using the powerful GATK framework.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3998#issuecomment-366185003:1225,flexible,flexible,1225,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3998#issuecomment-366185003,2,"['extend', 'flexible']","['extend', 'flexible']"
Modifiability,"I don't think the `@author` tags will show up in the doc; they generally don't show up in javadoc, and our doc system doesn't include them in the generated doc.`@author` tags are a bit sketchy to begin with, because they're block tags, so the extend all the way to the start of the next tag. But since they're stripped out, putting such a tag at the start of the javadoc can result in everything after it being silently dropped from the output. We found such a case in GATK a while back.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-349354828:243,extend,extend,243,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-349354828,1,['extend'],['extend']
Modifiability,"I don't understand the filtering approach you're suggesting. ReadSource crashes, adding a filter afterwards wouldn't help. But that's not a problem anymore because those crash-inducing reads weren't supposed to be there in the first place and now we're working with an input that doesn't have them. I looked into DataflowReadsPipeline but can't use it. I would suggest putting together a library of helpful functions rather than a collection of base classes, since the former is more flexible. I can't change to ReadSource because it doesn't return all the reads, breaking ABQSR. So it looks like I have to wait for a fixed ReadSource (incidentally, can we document this limitation in the class docs? In this case it would have saved us all some time).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/556#issuecomment-111206864:484,flexible,flexible,484,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/556#issuecomment-111206864,1,['flexible'],['flexible']
Modifiability,I fixed these by update the dataproc image from 1.0 -> 1.1. . @davidbernick Is there a way to make that into a variable that's settable in one place and shared between every jenkins spark job?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2289#issuecomment-264497363:111,variab,variable,111,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2289#issuecomment-264497363,1,['variab'],['variable']
Modifiability,"I found this online., this might be the reason why use include spark is different from system spark in this case. In spark, when i try to cast the variable to the same 'class' (with exact the same class name, in this case, you could try to cast any ""Variant"" to ""Variant"" in BroadcastJoinReadsWithVariants ), it always throw out the class exception error. It is because that ""The equality of two classes in Java depends on the fully qualified name and the class loader that loaded it.""; In system spark, it might split the JVM, which means the broadcast variable may cause problem to identify if it is the same class?. This is what i test in spark:; List<Variant> vs = variants.take(10);; MinimalVariant v = (MinimalVariant) vs.get(0);. this will throw ClassCastException error... http://stackoverflow.com/questions/826319/classcastexception-when-casting-to-the-same-class",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1386#issuecomment-166160181:147,variab,variable,147,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1386#issuecomment-166160181,2,['variab'],['variable']
Modifiability,"I got it to work by using the runtime switch --disable-sequence-dictionary-validation . . If that is not used it crashes. . . Docker commandline. . /gatk Funcotator --disable-sequence-dictionary-validation \. -R mydata/refs/Homo_sapiens_assembly19.fasta \. -V mydata/P50513_mutect2_filtered.vcf \. -O mydata/P50513_mutect2_funcotator.maf \. --output-file-format MAF \. --data-sources-path mydata/dataSourcesFolder/funcotator_dataSources.v1.6.20190124s/ --ref-version hg19. . . . From: Louis Bergelson <notifications@github.com> ; Sent: Wednesday, October 30, 2019 10:26 AM; To: broadinstitute/gatk <gatk@noreply.github.com>; Cc: rdbremel <rdbremel017@gmail.com>; Mention <mention@noreply.github.com>; Subject: Re: [broadinstitute/gatk] Funcotator shuts down (#6182). . @rdbremel <https://github.com/rdbremel> This got missed in the churn of issues. Does this happen repeatedly or is it a 1 time occurrence? We've seen similar issues in the past and tried to wrap them all in layers of retries, but sometimes things slip through. —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub <https://github.com/broadinstitute/gatk/issues/6182?email_source=notifications&email_token=ANCR2VB4ZCHMAJUHBKE2SP3QRGRQFA5CNFSM4I2MRFQKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOECUTZZI#issuecomment-547962085> , or unsubscribe <https://github.com/notifications/unsubscribe-auth/ANCR2VHRV5JESZYAYX55YHTQRGRQFANCNFSM4I2MRFQA> . <https://github.com/notifications/beacon/ANCR2VAS2WE5TDCUC6G5LETQRGRQFA5CNFSM4I2MRFQKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOECUTZZI.gif>",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6182#issuecomment-548102382:975,layers,layers,975,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6182#issuecomment-548102382,1,['layers'],['layers']
Modifiability,"I grok the refactoring changes and those are 👍, but I think I'm going to need a walkthrough for the core spanning deletion work...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7857#issuecomment-1135250576:11,refactor,refactoring,11,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7857#issuecomment-1135250576,1,['refactor'],['refactoring']
Modifiability,"I guess it depends on how general we want to keep this input path. I think most purely read-depth based callers won't really be able to discover events smaller than 800bp or so (maybe 500bp at the lower limit) with any accuracy. I also don't know of any tools that we're considering that will describe individual breakpoints. . What about this for a rule: create two intervals for the start and end of the CNV interval + or - 151 bases (allowing a read length of slop). If the two intervals overlap, merge them together into a single evidence interval. . We could also make the slop amount parameterizable per input file, since different tools might have different characteristics, although that would be a feature we could just make a ticket for until we need it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3542#issuecomment-327499474:590,parameteriz,parameterizable,590,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3542#issuecomment-327499474,1,['parameteriz'],['parameterizable']
Modifiability,I guess that those problems are all GenomeLoc(Parser) inheritance and since we are breaking away from GenomeLoc perhaps is a good time to lift this restriction as well; I don't think being conservative at this point is necessarily a good thing considering the longer road ahead.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/373#issuecomment-97539279:54,inherit,inheritance,54,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/373#issuecomment-97539279,1,['inherit'],['inheritance']
Modifiability,"I have a PR that fixes this. It's on the branch tws_desparkify_svdiscovery, which is a big refactor on some SV code. Someone could have a look and pull out just that bit of code to make a new, smaller PR.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6709#issuecomment-662574860:91,refactor,refactor,91,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6709#issuecomment-662574860,1,['refactor'],['refactor']
Modifiability,"I have a pull request out for this code. Will you refactoring maybe that; obsolete?; On Jul 28, 2015 9:04 AM, ""Louis Bergelson"" notifications@github.com; wrote:. > @davidadamsphd https://github.com/davidadamsphd 1 comment. Either; > address that now or in your next refactoring commits. Otherwise [image:; > :+1:] merge at will.; > ; > —; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/hellbender/pull/759#issuecomment-125620920; > .",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/759#issuecomment-125621800:50,refactor,refactoring,50,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/759#issuecomment-125621800,2,['refactor'],['refactoring']
Modifiability,"I have pull requests in flight for both (1) and (2). They are 1469; <https://github.com/GoogleCloudPlatform/google-cloud-java/pull/1469> and; 1470 <https://github.com/GoogleCloudPlatform/google-cloud-java/pull/1470>. Cheers,; JP. On Tue, Dec 6, 2016 at 3:54 AM, Tom White <notifications@github.com> wrote:. > Yes, Hadoop-BAM uses the NIO API to do file merging, whereas in GATK we; > were using the Hadoop APIs (and therefore the GCS<->HDFS adapter) to do it.; >; > It looks like there are a couple of things needed in GCS-NIO to use the; > NIO API for this.; >; > 1. GoogleCloudPlatform/google-cloud-java#1450; > <https://github.com/GoogleCloudPlatform/google-cloud-java/issues/1450>; > so that we don't have to special-case gs URIs to remove everything; > except the scheme and host when looking up the filesystem (see; > https://github.com/HadoopGenomics/Hadoop-BAM/; > blob/master/src/main/java/org/seqdoop/hadoop_bam/util/; > NIOFileUtil.java#L40; > <https://github.com/HadoopGenomics/Hadoop-BAM/blob/master/src/main/java/org/seqdoop/hadoop_bam/util/NIOFileUtil.java#L40>; > ); > 2. GoogleCloudPlatform/google-cloud-java#813; > <https://github.com/GoogleCloudPlatform/google-cloud-java/issues/813>; > to support path matching (https://github.com/HadoopGenomics/Hadoop-BAM/; > blob/master/src/main/java/org/seqdoop/hadoop_bam/util/; > NIOFileUtil.java#L90; > <https://github.com/HadoopGenomics/Hadoop-BAM/blob/master/src/main/java/org/seqdoop/hadoop_bam/util/NIOFileUtil.java#L90>; > ); >; > There may be more, as I stopped there. The best way forward is probably to; > go back to the old code in GATK while the deficiencies in GCS-NIO are fixed; > and then released.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2287#issuecomment-266151447:441,adapt,adapter,441,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2287#issuecomment-266151447,1,['adapt'],['adapter']
Modifiability,"I have several use cases for this:; 1. I didn't mention it in the first comment, but if a tool does not want to provide custom `ReadFilter`from the user, but allow to disable the ones applied by the tool, this will keep in sync the parameter name.; 2. I would like to have a `ReadFilter` plugin that is applied ""after analysis"", but the parameters here said that they are applied ""before analysis"". This will be misleading for my users. For this case I also opened #2353, to allow custom parameters based on the same read filter plugin descriptor implemented in GATK.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2352#issuecomment-274753122:288,plugin,plugin,288,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2352#issuecomment-274753122,2,['plugin'],['plugin']
Modifiability,"I haven't been able to reproduce @vdauwera error, but there are issues with the https checkout at the moment. ; There's one annoying issue witwhere it will prompt for a password before every individual file download. This will be fixed in https://github.com/github/git-lfs/issues/755. It can be worked around by using `git config credential.helper cache` but the easiest thing to do at the moment is just using the ssh checkout. . I need to investigate what happens with ssh checkout if you don't have a key set up.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/952#issuecomment-150376513:323,config,config,323,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/952#issuecomment-150376513,1,['config'],['config']
Modifiability,"I havent published to github yet, pending getting these core changes in; however, the purpose is pretty simple: allow VariantEval to inherit from MultiVariantWalker, but not require it to include the required argument -V. this seemed comparable to VariantWalkerBase (no arguments), and VariantWalker (specifies -V). GATK3's VariantEval uses the --eval argument and I generally tried to keep everything in this port in sync with GATK3, within reason. If there is another way to subclasses to negate some @argument defined by a superclass this would work too. If you want to see more I'll push to github.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4495#issuecomment-379803776:133,inherit,inherit,133,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4495#issuecomment-379803776,1,['inherit'],['inherit']
Modifiability,I imagine that @skwalker's scripts could be adapted for the task -- I'll try to set up a meeting with her next week to discuss.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4614#issuecomment-381170947:44,adapt,adapted,44,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4614#issuecomment-381170947,1,['adapt'],['adapted']
Modifiability,I inherited broadinstitute/gatk-protected#795 from @davidbenjamin.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2267#issuecomment-276161535:2,inherit,inherited,2,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2267#issuecomment-276161535,1,['inherit'],['inherited']
Modifiability,"I like option 2. If one of the other options is specified, use the specified value. For example, the following would use 0.1 for interval-psi-scale:. --set-defaults-for-data-type WES. --interval-psi-scale 0.1. On Mon, Apr 30, 2018 at 10:27 PM, samuelklee <notifications@github.com>; wrote:. > Thanks for bringing this up! I actually think that I prefer option 1,; > although not ideal (since, as you say, it places more burden on the user).; > The whole point of having generically parameterized models is that we can; > apply them to many data types. To single out a few with hardcoded sets of; > defaults seems like a slippery slope to me. (Of course, we should; > definitely provide defaults for typical data types in *documentation*.); > And in the end, I think it is beneficial for users that wish to tweak knobs; > to do some work to understand what those knobs actually do (even if just at; > a basic level).; >; > The other downside of option 2 is that it might not be immediately obvious; > from the command line what parameters are being used. For example, if a; > user chooses a set of defaults but then overrides some of them, we should; > make it so they don't have to go digging through the logs to see what; > parameters are actually used in the end. Nor should they have to go back; > and check what the defaults were for whatever version of the jar they were; > using at the time. Option 2 might also make it easier to inadvertently; > override parameters, etc. via command-line typos or copy-and-paste; > errors---it's much more straightforward to require and check that every; > parameter is specified once and fallback to a default if not, as we do now.; > Not to say that we couldn't get around any of these issues in Barclay, but; > I think it'll require some thought and careful design. Would be interested; > to hear Engine team's opinions.; >; > Finally, one point that I think will become more relevant as our tools and; > pipeline become more flexible and parameterized: I t",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385677379:482,parameteriz,parameterized,482,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385677379,1,['parameteriz'],['parameterized']
Modifiability,"I looked at the first new commit and skimmed the second. I'm assuming; tests still pass. If you want another set of eyes on the test data; refactor it'll have to wait until Monday. -L. On Fri, Oct 21, 2016 at 12:19 PM, David Benjamin notifications@github.com; wrote:. > @ldgauthier https://github.com/ldgauthier I _think_ you approved; > without needing it to send back to you, but I'm paranoid and not fully; > confident with the new github review system. Also, the changes to the test; > code to get rid of the duplicated likelihood-setting were pretty big.; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/gatk/pull/2185#issuecomment-255445145,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/AGRhdEDyfInQUcZiSZ5qBFhrAOnJpNZiks5q2RBVgaJpZM4KFNEm; > .",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2185#issuecomment-256139184:139,refactor,refactor,139,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2185#issuecomment-256139184,1,['refactor'],['refactor']
Modifiability,"I made the change you suggested but just realized I forgot to push before; leaving for my flight. I'll push when I get back. On Tue, Nov 20, 2018, 12:04 PM ldgauthier <notifications@github.com wrote:. > *@ldgauthier* commented on this pull request.; >; > Looks lovely, thanks for the quick fix! I have one refactoring suggestion; > -- take it or leave it.; > ------------------------------; >; > In; > src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/RMSMappingQuality.java; > <https://github.com/broadinstitute/gatk/pull/5435#discussion_r235089237>:; >; > > return Arrays.asList(squareSum,totalDP);; > } catch (final NumberFormatException e) {; > throw new UserException.BadInput(""malformed "" + GATKVCFConstants.RAW_RMS_MAPPING_QUALITY_KEY + "" annotation: "" + rawDataString);; > }; > }; >; > + /**; > + * Private getter function to replace VariantContext::getAttributeAsIntList in instances where there is a chance; > + * that ints will overlow beyond Integer.MAX_VALUE; > + * @return VariantContext attribute indexed by key, as list of long.; > + */; > + static private List<Long> getAttributeAsLongList(final VariantContext vc, final String key, final Long defaultValue) {; >; > I agree with your reticence to put this into htsjdk. The type handling; > there is super awkward, but I don't think it's worth dealing with until it; > gets improved in 3.0. However, I might suggest making this method public; > and moving it to a utility class. GATKProtectedVariantContextUtils has a; > lot of similar methods; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/pull/5435#pullrequestreview-176875488>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AGKhCBnSuwz30W-52kw9uZTXywWUCua-ks5uxDYbgaJpZM4Yp2Gl>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5435#issuecomment-440649639:306,refactor,refactoring,306,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5435#issuecomment-440649639,1,['refactor'],['refactoring']
Modifiability,I realized that the `--disable X --disable X` cannot blow up because the argument is a `Set`. Every repeated argument is hidden from the plugin. Either the signature should change to throw the exception or ignore that issue.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2377#issuecomment-276979349:137,plugin,plugin,137,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2377#issuecomment-276979349,1,['plugin'],['plugin']
Modifiability,"I recommend putting that in the GitHub wiki.; On Wed, Dec 23, 2015 at 12:20 PM Adam Kiezun notifications@github.com; wrote:. > scenario: someone wants to use gatk4 as a framework and add new tools.; > They need show on the list of tools etc. They package names are the user's,; > ie not org.broadinstitute.hellbender*. The way to do this is to extend Main; > but we have no example and not documentation of this (other that in the; > Main class, which is not the right place - I think it should be in README; > or some such).; > ; > —; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/gatk/issues/1397.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1397#issuecomment-166961552:344,extend,extend,344,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1397#issuecomment-166961552,1,['extend'],['extend']
Modifiability,I refactored ProgressLogger and I'm pushing it up for review. (See https://github.com/samtools/htsjdk/pull/281.) I will feel much happier just implementing a `log` function than copying that code over and having to test and maintain it.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/612#issuecomment-122434826:2,refactor,refactored,2,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/612#issuecomment-122434826,1,['refactor'],['refactored']
Modifiability,"I refactored some of the stream methods to address @akiezun's (well-founded, as running HaplotypeCaller showed) performance concerns. That, along with @lbergelson's suggestion to lazily evaluate error messages, leaves HaplotypeCaller's performance unaffected. Tests are passing locally but Travis is giving me a mysterious error:. > compileTestJavaerror: error reading /home/travis/.gradle/caches/modules-2/files-2.1/commons-httpclient/commons-httpclient/3.1/964cd74171f427720480efdec40a7c7f6e58426a/commons-httpclient-3.1.jar; error in opening zip file; > error: error reading /home/travis/.gradle/caches/modules-2/files-2.1/commons-httpclient/commons-httpclient/3.1/964cd74171f427720480efdec40a7c7f6e58426a/commons-httpclient-3.1.jar; cannot read zip file. I am fully rebased onto master, including @akiezun's PR from this afternoon. @droazen and @lbergelson do you have insights?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1979#issuecomment-232447710:2,refactor,refactored,2,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1979#issuecomment-232447710,1,['refactor'],['refactored']
Modifiability,"I should have been more specific. `--genotype-given-alleles` is HaplotypeCaller functionality, which is very similar if not outputting GVCFs. If providing dbSNP is similar to something you might want to do, then I think this approach will work. My concern was that if you were interested in a particular locus, but had never seen a variant there in any sample (i.e. didn't have an ALT allele to compare to), that implementation could get tricky. We should be able to output 100% reference sites with what I have in mind. @davidbenjamin while you're waiting for the restoration of the TCGA data, would you be interested in extending GGA mode to GenotypeGVCFs? (Obviously without the graph part.)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6239#issuecomment-549503239:622,extend,extending,622,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6239#issuecomment-549503239,1,['extend'],['extending']
Modifiability,"I still think it's bad that sometimes you have tests for the AS annotations in the same class as its non-AS equivalent, and other times you have the AS tests in a separate class (eg., `AS_QualByDepthUnitTest`). It might be better to have the AS annotations always in a separate class, but extend the non-AS test class to share `DataProviders`, etc. Up to you, but should be consistent at least.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1840#issuecomment-225961429:289,extend,extend,289,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1840#issuecomment-225961429,1,['extend'],['extend']
Modifiability,"I suppose that's true (about always having the dependency) if we want GCS capability in all tools that do I/O (which is all tools, I imagine). I guess I was imagining that there might be a set of Spark-only tools that could have a skinnier dependency tree if we decided to segregate them.; Calling it FileSystemUtils or IOUtils or extending the standard URL resolution framework would ease the cognitive dissonance, though, so thumbs up on that.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1887#issuecomment-224377784:331,extend,extending,331,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1887#issuecomment-224377784,1,['extend'],['extending']
Modifiability,I suspect this is related to the jar configuration (3 separate uber- jars) that is unique to the docker CI tests. I'll debug and resolve this after #6351 is merged.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1251462525:37,config,configuration,37,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1251462525,1,['config'],['configuration']
Modifiability,I tested this successfully on a cluster. The changes make it possible to override Spark configuration - e.g. I was able to override `spark.yarn.executor.memoryOverhead`.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1211#issuecomment-162842908:88,config,configuration,88,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1211#issuecomment-162842908,1,['config'],['configuration']
Modifiability,"I think all of our dataflow / spark code is at least almost entirely using `GATKRead`. GATKRead is designed to not provide access to the header because it's not available from a google `Read` backed `GATKRead`. It sounds like there is some information that `Read` includes that is missing from a headerless `SAMRecord`. I think we could audit the `SAMRecordToGATKReadAdaptor` to find any places it touches the header and then cache that information in the adaptor before stripping the header. We don't need to add back in the headers at any point, because we provide library functions to perform any header related operation with a provided header.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141107659:456,adapt,adaptor,456,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141107659,1,['adapt'],['adaptor']
Modifiability,"I think it's best not to co-opt existing formats for storing *variant calls* and *mutations* if we want to store generic annotations. Furthermore, many of the drawbacks of VCF (e.g, wasted space from repeated tags/unused fields) are really not worth dealing with if our data is strictly tabular and well structured. I think if we can settle on a format internally that satisfies all of our needs, then it'd probably a *very small* amount of effort on the part of external developers to write adapters to consume it. After all, we are only talking about metadata (hopefully in a standardized but suitably flexible format, e.g. SAM/VCF-style header) + tabular data. It may also be that there is a format out there that already fits the bill, in which case we just need to do some more research and discussion. I think this would be better than causing confusion and setting a bad example by co-opting unsuitable formats, even if this would require no additional effort for external developers.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-386578762:492,adapt,adapters,492,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-386578762,2,"['adapt', 'flexible']","['adapters', 'flexible']"
Modifiability,I think now that we have the NIO plugin working we can probably replace everything that used AuthHolder with NIO.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2394#issuecomment-277828180:33,plugin,plugin,33,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2394#issuecomment-277828180,1,['plugin'],['plugin']
Modifiability,"I think that it is necessary to have a way for downstream projects to override some of the top-level arguments in the base CLP class. For example, the config file is for documentation purposes, but I don't want to expose users to that argument because I will set the defaults programmatically. Another example is the GCS retries, which might not be useful for a software that is not planning to support GCS even if it is already implemented (or does not want to expose). As a downstream developer, for me it is important to being able to configure arguments and expose/hide them to my final users; with the current implementation, my main issue is to have an argument that are irrelevant for the toolkit user and that I get questions about why and how to use them (the most clear example, the config file). If the main problem is to change an interface, a default value for new methods can be added to keep the same behavior.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3998#issuecomment-361876183:151,config,config,151,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3998#issuecomment-361876183,3,['config'],"['config', 'configure']"
Modifiability,"I think that no `VariantAnnotation` is documented yet, because there is no even a plugin. It will be nice to have them anyway...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3809#issuecomment-342818332:82,plugin,plugin,82,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3809#issuecomment-342818332,1,['plugin'],['plugin']
Modifiability,I think that the annotation plugin is assigned to @jamesemery here: https://github.com/broadinstitute/gatk/issues/3287. But of course it can be documented before it is a plugin. I just wanted to point out that all of them should be annotated properly...,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3809#issuecomment-342823849:28,plugin,plugin,28,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3809#issuecomment-342823849,2,['plugin'],['plugin']
Modifiability,"I think that this is a nice feature (at least for me) and not a bug. For example, if in GATK someone runs a tool with `-RF read_filters.args`, then the pipeline cannot be reproduced in a different dataset unless the file is accessible. I can understand that it could be also nice to preserve the `-RF read_filters.args` to be able to modify the file an re-run the tool with different parameters, but for me the purpose of storing the command line in the header or other places is keep track of the exact params that I used: if a file is modified, then it is impossible to trace the params. For input files, this is expected (if the input has changed, it is expected that the result change), but for arguments it shouldn't be the case (independently of the file changing, the tool was running with exactly that parameters). I vote for solve this in Barclay in a configurable way, to allow users to decide which kind of verbosity of the command line they want (I definitely prefer to expand as currently).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3797#issuecomment-342798092:861,config,configurable,861,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3797#issuecomment-342798092,1,['config'],['configurable']
Modifiability,"I think that this is prepared for merging, @cmnbroad. As soon as it gets in, I would submit a PR fixing all the problems found in the plugin descriptor for discussion.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2385#issuecomment-278391496:134,plugin,plugin,134,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2385#issuecomment-278391496,1,['plugin'],['plugin']
Modifiability,"I think that this is related with https://github.com/broadinstitute/gatk/issues/1880, and I've already develop a rough system in a small project to have a plugin for variant annotation. I would love to contribute to this, and I kind of started with https://github.com/broadinstitute/gatk/pull/2534",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2542#issuecomment-290173938:155,plugin,plugin,155,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2542#issuecomment-290173938,1,['plugin'],['plugin']
Modifiability,I think the only failure left is a Travis thing. Back to @vruano after heavy refactoring.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5831#issuecomment-519630216:77,refactor,refactoring,77,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5831#issuecomment-519630216,1,['refactor'],['refactoring']
Modifiability,"I think the piece we need is just the parser, though -- we can keep the Picard code that injects values into instance variables & parses the annotations, etc., we just need something to go from the raw args array into a parsed set of names + values.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/72#issuecomment-69632895:118,variab,variables,118,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/72#issuecomment-69632895,1,['variab'],['variables']
Modifiability,"I think the problem is that you need to do https://github.com/broadinstitute/gatk/issues/1130 first, then refactor the tests to use a .2bit reference for the BROADCAST-based tests as part of this ticket (SHUFFLE should still have test coverage, though).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1131#issuecomment-157478894:106,refactor,refactor,106,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1131#issuecomment-157478894,1,['refactor'],['refactor']
Modifiability,"I think this is happening because were trying to serialize the class loader sun.misc.Launcher$AppClassLoader), which appears to be reached through the graph by way of via https://github.com/damiencarol/jsr203-hadoop/blob/master/src/main/java/hdfs/jsr203/HadoopFileSystem.java#L82. We probably need to short circuit that with a custom serializer for one of these:. Serialization trace:; classes (sun.misc.Launcher$AppClassLoader); classLoader (org.apache.hadoop.conf.Configuration); conf (org.apache.hadoop.hdfs.DistributedFileSystem); fs (hdfs.jsr203.HadoopFileSystem); hdfs (hdfs.jsr203.HadoopPath); path (htsjdk.samtools.seekablestream.SeekablePathStream); seekableStream (htsjdk.tribble.TribbleIndexedFeatureReader); featureReader (org.broadinstitute.hellbender.engine.FeatureDataSource); featureSources (org.broadinstitute.hellbender.engine.FeatureManager). See, for instance, https://github.com/dbpedia/distributed-extraction-framework/issues/9.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5680#issuecomment-668654169:466,Config,Configuration,466,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680#issuecomment-668654169,1,['Config'],['Configuration']
Modifiability,I think we can finally unblock this using PAPIv2's ability to request machine types on Google Cloud. We just have to configure our jenkins server to use PAPIv2.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4356#issuecomment-415862887:117,config,configure,117,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4356#issuecomment-415862887,1,['config'],['configure']
Modifiability,I think we will add a logger for GenomicsDB with configurable verbosity - but this is low priority for us.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2689#issuecomment-300298389:49,config,configurable,49,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2689#issuecomment-300298389,1,['config'],['configurable']
Modifiability,"I tried running gatk version 4.0.7.0 with the environment variable: 'TILEDB_DISABLE_FILE_LOCKING=YES' to see if that would fix the issue, but I still get the same error. I am not sure that the patch that enable that was actually in the 4.0.7.0 release.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5024#issuecomment-409070215:58,variab,variable,58,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5024#issuecomment-409070215,1,['variab'],['variable']
Modifiability,"I tried setting that environment variable, but it did not resolve the issue. Please note that I have this filesystem mounted as `CIFS` not `NFS`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5342#issuecomment-453657941:33,variab,variable,33,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5342#issuecomment-453657941,1,['variab'],['variable']
Modifiability,I want to point out a bug regarding the read plugin that I reported in #357 (and fix with in #2359): disable a default filter with arguments blows up as a `CommandLineException` as if the user provided the arguments for a disabled filter. This is quite important in this regard of insane combinations.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2377#issuecomment-276625536:45,plugin,plugin,45,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2377#issuecomment-276625536,1,['plugin'],['plugin']
Modifiability,"I was looking into this because it is useful for me, and I have found that there is going to be redundancy between the `VariantAnnotatorEngine`code and the plugin. Here a couple of suggestions after trying to implement something in this regard time ago:. * Remove/deprecate the private class `AnnotationManager` in favor of the plugin. The current code is performing reflection operations by itself, and this can cause some problems.; * Refactor the `VariantAnnotatorEngine` constructors in favor of a constructor from the barclay plugin and a list of annotations to apply, to avoid the `AnnotationManager` implementation.; * Remove/deprecate static methods for creating an annotator engine (`ofAllMinusExcluded` and `ofSelectedMinusExcluded`) in favor of handling this in the plugin.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3287#issuecomment-316077922:156,plugin,plugin,156,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3287#issuecomment-316077922,5,"['Refactor', 'plugin']","['Refactor', 'plugin']"
Modifiability,"I was thinking after checking your `ReadWindowWalker` implementation and the HaplotypeCaller branch that with this class we can do a general `SlidingWindowWalker`, which could be extended by `ReadWindowWalker` or it could be possible that `HaplotypeCaller` directly implements `SlidingWindowWalker`. @droazen, could you have a look and tell me what do you think about the idea?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210077798:179,extend,extended,179,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210077798,1,['extend'],['extended']
Modifiability,"I was thinking more carefully on this and another option is create methods in `ReadPileup` to fix the overlaps after construction and/or getBaseCounts without overlaps. This won't break the behaviour of LIBS and it is up to the user to change overlaps. But for performance issues, I would like to have a variable in `ReadPileup` for track if the overlaps are corrected/fixed, to avoid recomputation. I can implement this in a different PR or in this one if the basic idea behind this one is not accepted.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2041#issuecomment-235993555:304,variab,variable,304,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2041#issuecomment-235993555,1,['variab'],['variable']
Modifiability,"I was thinking that if we relied on PyPI for distribution, it would only be for released builds, not a release for every repo merge commit. But, I'm increasingly inclined to think that in the short term we should just include the python archive/zip file right in the gatk distribution zip, and modify the env .yml to install from that. Then every configuration (docker image, git clone user, and end user) could use exactly the same method to establish the environment. That seems like the simplest solution for now.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3964#issuecomment-352279343:347,config,configuration,347,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3964#issuecomment-352279343,1,['config'],['configuration']
Modifiability,"I will do a big PR with a commit for each of the issues that we found in the plugin, and including tests. Although the plugin interface is going to change in Barclay, I think that before that the PR should setup all the tests for the issues to ensura that the change does not broke anything. Any opinion on this, @cmnbroad, @droazen, @lbergelson?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2377#issuecomment-278248587:77,plugin,plugin,77,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2377#issuecomment-278248587,2,['plugin'],['plugin']
Modifiability,"I would like to have this feature for a new project that will rely on the built-in walkers of GATK. The main problem that I am facing is while grabbing the info from the METAINF file. For fixing this, I have a proposal:. * Create a new interface with the single method to print the startup message; * Create a base-class with the current code in GATK, which can be extended to override some parts of the startup message, but not all.; * Make a non-final static field in `CommandLineProgram` , which is settable. This could be set in the `Main` for toolkits (similarly to how the config file is set). The default will be the GATK implementation. If you agree with the proposal, let me know to implement it and submit a PR.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4101#issuecomment-381504458:365,extend,extended,365,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4101#issuecomment-381504458,2,"['config', 'extend']","['config', 'extended']"
Modifiability,"I would like to specify what passing a `ReadFilter` to some of my tools means, so maybe passing an `ArgumentCollection` will be simpler than this one, I agree. Although #2085 may solve the issue regarding the `ReadTransformer`/`ReadFilter` ordering, I would like to have in the plugin a way to specify different parameters (maybe some of then hidden before expose to users or advanced in the case of disabling). I will open a new PR for that change, but I will really appreciate if I can get something like that in this and other plugins (if implemented).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2353#issuecomment-275082983:278,plugin,plugin,278,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2353#issuecomment-275082983,2,['plugin'],"['plugin', 'plugins']"
Modifiability,"I'd be OK with adding some sort of experimental tag, but I'm not sure where to put it. Does that goes in the docs or somewhere in the code to post a warning to the log? I think the new tests should cover that it's working as intended. There is definitely room for future work (e.g. paying closer attention around the boundaries, the refactoring ideas, etc), but for now the docs should describe the expected behavior well.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8609#issuecomment-1847926227:333,refactor,refactoring,333,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8609#issuecomment-1847926227,1,['refactor'],['refactoring']
Modifiability,"I'd like to get to the point where most/all GATK tools extend `GATKTool` rather than `CommandLineProgram` directly, so I think we have to keep this one open.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2471#issuecomment-358025246:55,extend,extend,55,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2471#issuecomment-358025246,1,['extend'],['extend']
Modifiability,"I'll also mention that as part of https://github.com/broadinstitute/gatk/issues/4341 we plan to give tools more control over the arguments inherited from `GATKTool`, including selectively disabling and redefining engine arguments, so once a mechanism is in place for that `CalculateGenotypePosteriors` could make `--sequence-dictionary` required. Currently this ability only exists for a few `GATKTool` arguments, and the tool has to override methods like `requiresReads()` to make use of it. Until the ability to do this is generalized, recommend the stopgap solution with the check in `onTraversalStart()` + a note in the tool's docs.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4383#issuecomment-364497042:139,inherit,inherited,139,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4383#issuecomment-364497042,1,['inherit'],['inherited']
Modifiability,I'll take it. The tab completion task is printing out too many warnings for all of the new instances of unresolvable backtrace variables due to Picard. I'll do....something with it.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3710#issuecomment-337386911:127,variab,variables,127,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3710#issuecomment-337386911,1,['variab'],['variables']
Modifiability,"I'm adding some issues and PRs for make the plugin usable in other cases too, @cmnbroad. Maybe you prefer that solution instead of make it extensible. Just let me know.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2355#issuecomment-275376544:44,plugin,plugin,44,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2355#issuecomment-275376544,1,['plugin'],['plugin']
Modifiability,"I'm always confused with static-block initializers, and I just wondered in the config PR if the static block in `Main` it is correctly setting everything in sub-classes. I understood that this won't work in Main-derived classes; if that's not the case, feel free to close this PR...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3483#issuecomment-324627658:79,config,config,79,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3483#issuecomment-324627658,1,['config'],['config']
Modifiability,"I'm assuming you will have the subset of samples before creating a GenomicsDBFeatureReader object (and before creating the corresponding Protobuf export configuration object). More precisely, you are NOT requesting a line by line filter similar to:; At pos 100, compute INFO fields etc including only the samples whose QUAL > 5; At pos 102, compute INFO fields etc including only the samples whose QUAL > 5; ....",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5570#issuecomment-469502322:153,config,configuration,153,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5570#issuecomment-469502322,1,['config'],['configuration']
Modifiability,"I'm glad it's working now, but a PS since you asked about `-independent-mates`: several months ago we made Mutect2 force paired reads to share the latent random variable indicating which haplotype they are derived from in the somatic genotyping model. This is correct because paired reads come from the same molecule of DNA. `-independent-mates` disables this and tells Mutect2 to forget about pairing. We only created the option because some synthetic validation data is generated by spiking in variation without regard to pairing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6230#issuecomment-596165833:161,variab,variable,161,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6230#issuecomment-596165833,1,['variab'],['variable']
Modifiability,"I'm going to close this issue because it's not a bug. Several things in the code of Mutect2 and FilterMutectCalls adapt as they traverse the genome and it's possible that some learned parameter shifts minutely. For example, the assembly graph pruning algorithm uses knowledge of previously assembled regions to better distinguish between errors and somatic variation. It's also possible that somewhere we forgot to give something a fixed random seed. In full honesty, I _wish_ that I knew exactly what causes the 3142 to become 3143, and I regret that I don't have time for it. Nonetheless, in principle it is not cause for alarm.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8152#issuecomment-1983783338:114,adapt,adapt,114,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8152#issuecomment-1983783338,1,['adapt'],['adapt']
Modifiability,"I'm in favor of doing a ruthless refactor involving changing some of the; default behavior and prohibiting contradictory options. On Fri, Dec 12, 2014 at 11:59 AM, lbergelson notifications@github.com; wrote:. > We definitely need a way to combine vcfs that does the right thing and; > isn't horrible to use.; > ; > —; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/hellbender/issues/16#issuecomment-66800878; > .",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/16#issuecomment-66802166:33,refactor,refactor,33,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/16#issuecomment-66802166,1,['refactor'],['refactor']
Modifiability,"I'm more familiar with working with interval data files so I can't speak much to variant call formats except that VCF tends to be a bit wonky with interval data (such as SV calls) because, as @samuelklee mentioned, most of the fields are likely to be irrelevant and thus waste space. I feel strongly that BED-like formats are the way to go for interval data, i.e. #contig start end x1 x2 x3 x4 ...; chr1 2938 3949 3.9 0 + cat ...; ... where x1, x2, x3, x4 ... are columns for variables of different types (which could be specified by additional header lines like in VCF).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-385748457:476,variab,variables,476,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-385748457,1,['variab'],['variables']
Modifiability,"I'm not sure exactly what's happening but I suspect it has something to do with the way the files are mounted. My guess is that there is some sort of transient interruption happening in the connection between the EC2 instance and the file server, and it's causing an error in gatk. When reading from a local file GATK does not expect any errors since errors in local files are usually fatal problems caused by a broken disk. Its probably some sort of bug in amazon's fuse implementation which isn't properly hiding network problems from the software. . I expect that your output is truncated at the point the error occured, and you probably need to rerun those shards. Instead of mounting them with amazon's fuse, you could try to either copy the files to a local disc, or access them using an NIO filesystem plugins like this plugin https://github.com/awslabs/aws-java-nio-spi-for-s3 or as signed URLs using https://github.com/broadinstitute/http-nio/ (included in gatk 4.6).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8735#issuecomment-2214915942:809,plugin,plugins,809,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8735#issuecomment-2214915942,2,['plugin'],"['plugin', 'plugins']"
Modifiability,"I'm not sure what proportion of users leverage the incremental import functionality...it wasn't available when GenomicsDBImport was first made available, but has been around for ~3 years now. As for workspaces with whole chromosomes -- there is no requirement or performance benefits to using whole chromosomes. As you say, subsetting a chromosome to smaller regions will work and make the import and query parallelizable. (if you remember where the advice about whole chromosomes came from, let us know. That might be something that needs to be updated/clarified). Many small contigs does add overhead to import though and, till recently, multiple contigs couldn't be imported together (i.e., each contig would have it's own folder under the GenomicsDB workspace - which gets inefficient with many small contigs). For WGS, probably the best way to create the GenomicsDBImport interval list is to split based on where there are consecutive N's in the reference genome (maybe using [Picard](https://broadinstitute.github.io/picard/command-line-overview.html#ScatterIntervalsByNs)) and/or regions that you are blacklisting. I think you suggested that some of the blacklisted regions were especially gnarly - maybe ploidy or high alternate allele count? - depending on the frequency of those, we may save a bit on space/memory requirements. That may address your concern about overlap between variants and import intervals. In general, any variant that starts in a specified import interval will show up in a query to that workspace. I'm not sure if the blacklist regions contain any variants that start within but extend beyond the blacklist -- those may not show up if the regions are split up in this way.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1212486548:1612,extend,extend,1612,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1212486548,1,['extend'],['extend']
Modifiability,"I'm not sure why the `gs` provider doesn't get installed (I think JP was seeing this before), but in any case there's a workaround in GATK for it: https://github.com/broadinstitute/gatk/blob/master/src/main/java/org/broadinstitute/hellbender/utils/gcs/BucketUtils.java#L380-L390. However, since the Hadoop-BAM code is doing the path lookups, it can't call that code directly (the dependency is the wrong way round). It would be best if we could fix the underlying problem of course, so that it gets picked up properly - I wonder if this can be done by fixing the service provider so it survives relocation (see http://maven.apache.org/plugins/maven-shade-plugin/examples/resource-transformers.html#ServicesResourceTransformer). BTW I'm afraid I'm travelling this week, so I won't have time to look at it until next week either :(",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2287#issuecomment-263997131:635,plugin,plugins,635,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2287#issuecomment-263997131,2,['plugin'],"['plugin', 'plugins']"
Modifiability,I'm not totally clear from your response but I think you've resolved the problem? . If you're encountering a bug merging bai files could you open an issue describing that with your stack trace and any relevant information about the configuration you're running?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6233#issuecomment-547956623:232,config,configuration,232,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6233#issuecomment-547956623,1,['config'],['configuration']
Modifiability,"I'm pretty sure they all do... or at least all can depending on how you configure your MalformedReadFilter. example:. ```; private static boolean checkHasReadGroup(final SAMRecord read) {; if ( read.getReadGroup() == null ) {; // there are 2 possibilities: either the RG tag is missing or it is not defined in the header; final String rgID = (String)read.getAttribute(SAMTagUtil.getSingleton().RG);; if ( rgID == null ); throw new UserException.ReadMissingReadGroup(read);; throw new UserException.ReadHasUndefinedReadGroup(read, rgID);; }; return true;; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/193#issuecomment-75649333:72,config,configure,72,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/193#issuecomment-75649333,1,['config'],['configure']
Modifiability,"I'm still unclear if this is a bug or enhancement. Does the current ClipReads do what it promises to do? If yes, then it's not a bug. If not, what is the test case that shows the bug?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/263#issuecomment-99303878:38,enhance,enhancement,38,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/263#issuecomment-99303878,1,['enhance'],['enhancement']
Modifiability,"I'm thinking about lists such as filters packages (in the filter plugin): I can imagine a list with long package names separated by commas, which might be complicated to read due to the repetition of the same organization name. I think that it might be also more organize if the configuration file is an YML with sections for the different configurations: this will make the file more readable and easier to modify. Something like the codecov configuration will be interesting, separating configurations for spark, plugins, feature codecs, etc. For example, if I want to use a custom codec for BED files while including the HTSJDK codec packages, I would find a problem. Doing it in a granular level may be interesting for having something like:. ```yml; - codecs:; - packages:; - htsjdk.variant; - htsjdk.tribble; - exclude: bed.BEDCodec; - org.broadinstitute.hellbender.utils.codecs; - org.magicdgs.htsjdk.codecs; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3078#issuecomment-307803675:65,plugin,plugin,65,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3078#issuecomment-307803675,6,"['config', 'plugin']","['configuration', 'configurations', 'plugin', 'plugins']"
Modifiability,"I'm using spark 2.1.0. I can confirm it works with the command ; ```; spark-submit \; --deploy-mode client \; --class org.broadinstitute.hellbender.Main \; --master yarn \; /home/hadoop/gatk-package-4.alpha.2-269-gdce8abc-SNAPSHOT-spark.jar BwaSpark \; --bwamemIndexImage /var/tmp/hs38DH-V.fasta.img \; -I hdfs:///unaligned.bam \; -O hdfs:///aligned.bam \; -R hdfs:///hg38/hs38DH-V.fasta \; --disableSequenceDictionaryValidation true \; --sparkMaster yarn; ```; so I guess it's really a minor issue. I can see it confusing other spark users though, who might expect spark configuration arguments to go through `spark-submit` rather than the application args, especially since the --sparkMaster app arg is optional. Just my two cents.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2718#issuecomment-301945697:572,config,configuration,572,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2718#issuecomment-301945697,1,['config'],['configuration']
Modifiability,"I've added a few commits that clean up some of the code inherited from VQSR regarding the use of labeled resources when using allele-specific annotations. This should be ready for review and/or experimentation with importing into the WARP repo, @meganshand. There are a few unrelated failing tests, which I think others are seeing in their branches as well.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8074#issuecomment-1323953256:56,inherit,inherited,56,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8074#issuecomment-1323953256,1,['inherit'],['inherited']
Modifiability,"I've added a new end-to-end test for SelectVariants that writes to GCS. Sadly, the IntegrationTestSpec class uses Files throughout, so it wasn't possible to do this simply without first completely refactoring IntegrationTestSpec (which should probably be its own pull request). . Doing this refactoring would have the advantage that changing existing end-to-end tests from local to GCS would be trivial. For now instead I went with an ad-hoc approach. It works, and the test passes.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5378#issuecomment-455686612:197,refactor,refactoring,197,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5378#issuecomment-455686612,2,['refactor'],['refactoring']
Modifiability,"I've pulled the problem VCF and a couple of successful ones locally and I can confirm that when running with 4 VCFs:. - VCFs that succeeded through JointGermlineCNVSegmentation in our pipeline succeeded for me locally; - The VCF that was flagged in the error message `Exception thrown at chrX:6383391 [VC SAMPLE_ID.segments.vcf.gz ...` completes just fine with other successful partners; - The VCF that was not identified in the error message, but was inferred to be a sex chromosome aneuploidy causes a failure with any combination of other VCFs; - If there are more than 2 VCFs run together, including the failing VCF/aneuploid sample, the error message indicates the problem originates in a non-aneuploid VCF, which misleading and makes this hard to treat. This behaviour was consistent in `4.5.0.0`. Command used in my toy dataset:. ```; gatk --java-options ""-Xms4000M -Xmx6000M"" JointGermlineCNVSegmentation -R /data/Homo_sapiens_assembly38_masked.fasta -O /data/out.vcf.gz -V /data/SAM1.segments.vcf.gz -V /data/SAM2.segments.vcf.gz -V /data/SAM3.segments.vcf.gz -V /data/SAM4.segments.vcf.gz --model-call-intervals /data/preprocessed.interval_list -ped /data/inferred_sex_pedigree.ped; ```. - In this configuration, `SAM4` is aneuploid, and `SAM1` is always the flagged VCF; - If I remove `SAM1` and re-run with 3 VCFs, `SAM3` is mentioned in the error message. It's not derived from alphabetical order, first argument specified with `-V`, or first in the PED file",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8834#issuecomment-2123897736:1208,config,configuration,1208,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8834#issuecomment-2123897736,1,['config'],['configuration']
Modifiability,"ITE_FOR_SAMTOOLS : false; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - Deflater: IntelDeflater; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - Inflater: IntelInflater; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - Initializing engine; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.varia.NullAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.varia.NullAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""NullAppender"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.L",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312229998:4949,variab,variable,4949,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312229998,1,['variab'],['variable']
Modifiability,"If I understand correctly, @bbimber, this sounds a lot like the `--genotype-given-alleles` change David B. made recently -- discover new sites in the provided samples, but _also_ output calls against the variants specified in the resource file. This is a small change from what you propose because the resource file would specify a particular ALT allele, so if you were interested in positions where you haven't seen a variant yet we'd need to extend functionality to call against <NON_REF> and output or you could put some random placeholder allele, although the identity of that allele would affect your likelihoods. On the other hand, having an ALT specified from a previous cohort would improve your likelihoods for all-reference cohorts compared to what you get now for non-variant site output.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6239#issuecomment-549456607:444,extend,extend,444,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6239#issuecomment-549456607,1,['extend'],['extend']
Modifiability,"If it helps, this is a dummy walker that illustrates the kind of parallelization I'm exploring whether I can implement. The main sticking point I think exists relates to different threads interacting with FeatureManager and the feature reading code:. I would appreciate any thoughts on how to approach this in GATK. ```. /**; * This is a contrived example. It is designed to explore multithreading; */; public class DemoMultiVariantEval extends MultiVariantWalkerGroupedOnStart {; protected List<VariantEvalEngine> engines = new ArrayList<>();. @ArgumentCollection; protected VariantEvalArgumentCollection variantEvalArgs = new VariantEvalArgumentCollection();. @Override; protected void initializeDrivingVariants() {; getDrivingVariantsFeatureInputs().addAll(VariantEvalEngine.getFeatureInputsForDrivingVariants(variantEvalArgs));. super.initializeDrivingVariants();; }. private ExecutorService pool = null;; private final int threads = 2;. @Override; public void onTraversalStart() {; //Again, contrived. The real case would set up multiple engines with different parameters:; engines.add(new VariantEvalEngine(variantEvalArgs, this, getSequenceDictionaryForDrivingVariants(), getSamplesForVariants(), logger));; engines.add(new VariantEvalEngine(variantEvalArgs, this, getSequenceDictionaryForDrivingVariants(), getSamplesForVariants(), logger));. final ThreadFactory threadFactory = new ThreadFactoryBuilder(); .setNameFormat(""dummywalker-thread-%d""); .setDaemon(true); .build();. pool = Executors.newFixedThreadPool(threads, threadFactory);; }. @Override; public void apply(final List<VariantContext> variantContexts, final ReferenceContext referenceContext, final List<ReadsContext> readsContexts) {; // Parallelization should help; however, these steps interact with FeatureManager and the FeatureInputs.; // Is there an appropriate way to approach this?; Utils.runInParallel(threads, () -> {; this.engines.parallelStream().forEach(engine -> {; engine.apply(variantContexts, referenceContext);;",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7013#issuecomment-750442132:437,extend,extends,437,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7013#issuecomment-750442132,1,['extend'],['extends']
Modifiability,If it's actually 27**MB** I would just check it into lfs or rewrite to use -L,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1745#issuecomment-212969722:60,rewrite,rewrite,60,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1745#issuecomment-212969722,1,['rewrite'],['rewrite']
Modifiability,"If the encapsulation of the datasources is causing issues for tools that extend `GATKTool` directly, we can relax it -- it was intended to prevent walker authors from directly manipulating the datasources used for the traversal, but it may be doing more harm than good at this point.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2471#issuecomment-358333054:73,extend,extend,73,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2471#issuecomment-358333054,1,['extend'],['extend']
Modifiability,"If you override `makeReadFilter` in a tool with this arguments and the user provides them, either the developer should code a warning message saying that read filters will be ignored or the user will not be aware of what is happening in the processing. @cmnbroad, I think that it could be a good idea to add a `requiresReadFilters` to `GATKTool`, and like that tools which requires reads does not need to implement all this even if they are not extending `ReadWalker` or `LocusWalker`, for instance. What do you think? This is related with #1076",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1900#issuecomment-226123264:445,extend,extending,445,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1900#issuecomment-226123264,1,['extend'],['extending']
Modifiability,"If you set the environment variable `SPARK_LOCAL_IP=""127.0.0.1""` , as suggested by this SO post: https://stackoverflow.com/questions/34601554/mac-spark-shell-error-initializing-sparkcontext, the error goes away, so I think it's related to some change in the localhost network configuration our VPN sets up.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1534#issuecomment-325239463:27,variab,variable,27,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1534#issuecomment-325239463,2,"['config', 'variab']","['configuration', 'variable']"
Modifiability,"In order to get it running though you will need to install the following things on each machine using apt-get: gawk, sysstat, and perf-tools-unstable. Additionally as root, you will have to set the /proc/sys/kernel/perf_event_paranoid variable from 1 to 0. For these tasks it might be possible to automate these steps by updating the system image that is used to setup dataproc clusters. In order to actually run and install PAT, you will need to download it from [here](https://github.com/intel-hadoop/PAT/tree/master/PAT) and add all the machines and ssh ports (including the master) in your cluster to the ""ALL_NODES"" setting in the config.template -> config file. You will also have to setup an SSH key to root on the cluster, which can be done with the command `gcloud compute ssh` and set the ""SSH_KEY"" variable in the config file to point to the google_compute_engine file in roots .ssh directory (public keys should have automatically been distributed to the other nodes). . At this point you need simply input the command line command you wish to run into the ""CMD_PATH"" variable and run ./pat run. I recommend running a spark-submit job using yarn-client as master. NOTE: the output will be a directory containing an excel spreadsheet and a bunch of data for each cluster. You will need to open the spreadsheet on a windows copy of excel and use ""control+q"" to run the macros that load the data.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1986#issuecomment-234947495:235,variab,variable,235,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1986#issuecomment-234947495,6,"['config', 'variab']","['config', 'variable']"
Modifiability,"In other words, saying that broadcast took X minutes is not meaningful unless it's framed as a fraction of total runtime, and we understand whether this is a fixed or variable cost.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1675#issuecomment-208920951:167,variab,variable,167,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1675#issuecomment-208920951,1,['variab'],['variable']
Modifiability,"In terms of the two tools, I don't think it's necessary at this point to make an inheritance structure. `CallVariantsFromAlignedContigsSAMSpark` is more of a one-off for dealing with de novo assembly files and I'm not sure if it will be supported long term. However, I did extract a `callVariantsFromAlignmentRegions` method in `CallVariantsFromAlignedContigsSpark` that `CallVariantsFromAlignedContigsSAMSpark` can use, which reduces code duplication a lot. There's not much left in `CallVariantsFromAlignedContigsSAMSpark` except for the logic to convert GATKReads into AlignmentRegions, which seems appropriate.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2079#issuecomment-240514475:81,inherit,inheritance,81,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2079#issuecomment-240514475,1,['inherit'],['inheritance']
Modifiability,"Interesting! Thanks for generating these. I am already convinced by #4519 we should at least switch over to a ‘CollectReadCounts’ strategy for initial evaluations. A few comments:. -I’m guessing that the equal insert size and uniform sampling is enhancing many of these artifacts to a level that we probably don’t see in the real world. Can we take a look at some real-world examples?. -Same goes for the fact that homs will be unlikely. -Not sure about the dropouts. Might be worth running without SNPs as a confounding factor. -How flexible is SVGen? Might be worth putting together a more realistic simulated data set. Any chance @MartonKN might be able to use it to cook up some realistic tumor data?. -I don’t recall having a `CollectBaseCallCoverage` type tool in beta—which tool are you thinking of? On a related note, it seems there is some demand to port `DepthOfCoverage` from GATK3. However, I’d prefer that we roll a CNV-specific version of the tool even if it does get ported. In any case, I think along with findings from the other issue, we should issue a quick PR for `CollectReadCounts` and go ahead to change the `CollectCounts` WDL task to call it—it’s for this very reason that the task is named generically! @sooheelee note that we may have to update the tutorials, etc. at some point, but perhaps the right time will be until all evaluations are more complete. Speaking of which, this PR should not delay getting the first round of automated evaluations up and running. Again, the whole point of those is to have a reproducible baseline metric against which we can easily experiment with and adopt these sorts of changes. Although these sorts of theoretical/simulated/thought experiments are clearly useful to us, unfortunately, they may not be as compelling to some of our users as demonstrable improvement seems on real data!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4551#issuecomment-375122976:534,flexible,flexible,534,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4551#issuecomment-375122976,1,['flexible'],['flexible']
Modifiability,"Interesting, that's somewhat disturbing news, I wonder if we're paying for ssd's without actually being able to use them... It's also possible there's a different setting that's configuring the ssd's to be used for shuffle output. . We should investigate this further and 1) see if setting spark.local.dir makes a performance difference 2) ask the dataproc team about this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2426#issuecomment-283418564:178,config,configuring,178,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2426#issuecomment-283418564,1,['config'],['configuring']
Modifiability,"Interesting, this is the first time that I have seen a CentOS-7 install without zlib and uuid - even the minimal installations include it. Your options are:; - Install zlib and uuid (yum -y install zlib libuuid); - Ask your admins whether these packages are installed in some other location. For example, if the zlib library is at /opt/my_install/lib64/libz.so, then you can set your environment variable LD_LIBRARY_PATH; ```; export LD_LIBRARY_PATH=/opt/my_install/lib64:/opt/my_install/lib:$LD_LIBRARY_PATH; ```; - Wait for the next GenomicsDB binary jar to show up",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4124#issuecomment-357067214:396,variab,variable,396,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4124#issuecomment-357067214,1,['variab'],['variable']
Modifiability,Is it guaranteed that one of the configurations won't include any of the MQ0 regions? Why is that?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4326#issuecomment-364663550:33,config,configurations,33,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4326#issuecomment-364663550,1,['config'],['configurations']
Modifiability,Is there a way to have java load a config file as system properties on startup?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2316#issuecomment-267124998:35,config,config,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2316#issuecomment-267124998,1,['config'],['config']
Modifiability,"Is this the only `CommandLineException` which should be an `UserException`? If not, how is going to work the development of new exceptions in Barclay. For instance, in https://github.com/broadinstitute/barclay/pull/11 there is a new exception that I made for values out of range, which extends `BadArgumentValue`. I agree that this errors should be decoupled from the ones that are not, but in this case I think that this is already implemented:. * `UserException` are handled in the `mainEntry()`, distinguishing errors that comes from the user's side regarding some specifications in the tools/framework.; * `CommandLineException` are handled in `parseArgs()`, distinguishing errors that comes from the command line from the user side while parsing. I expect that any command line error that is not `CommandLineParserInternalException ` or `ShouldNeverReachHereException` comes from the user's side. The contract in Barclay says that are `CommandLineException` are _""Exceptions thrown by CommandLineParser implementations.""_, and I think that if other parts of the code (outside arg parsing) is throwing this exception is a bug that does not come from the user. I guess that this is the problematic part.; * Any other `Exception` is thrown in `Main.handleNonUserException()`, which may be caused by non-user exceptions. I propose that `CommandLineException` is handled as currently to separate ""errors that are the user's fault regarding input and/or assumptions"" (`UserException`), ""errors that are the user's fault while providing parameters to the command line"" (`CommandLineException`) and ""errors that are not the user's fault"" (other `Exception`s). Actually, this is reasonable because the exit status is different for any kind of errors in the current `Main`. The only problem that I see with this approach is the silently failing of a ""bug"" in tools/engine code, which can be rethrow easily in `CommandLineProgram.instanceMain`` as following:. ```java; public Object instanceMain(final Strin",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2324#issuecomment-268773161:286,extend,extends,286,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2324#issuecomment-268773161,1,['extend'],['extends']
Modifiability,Issues that need to be addressed as part of this ticket:; - support for arguments defined in filter classes; - PluginManager -- to port or not to port?; - performance issues with searching the classpath; - separating out the readmetrics code from the code that applies filters,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6#issuecomment-69800773:111,Plugin,PluginManager,111,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6#issuecomment-69800773,1,['Plugin'],['PluginManager']
Modifiability,"It *looks like* it doesn't. I ran a job and looked at the ""environment"" tab in the Spark page for the job and didn't see ""spark.local.dir"" mentioned in the list of properties or the command line. Based on [the documentation](http://spark.apache.org/docs/latest/configuration.html), the setting must thus still be at its default value of ""/tmp"". . /tmp is on the HDD, the SSD one would have to be on /mnt/1/.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2426#issuecomment-283210934:261,config,configuration,261,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2426#issuecomment-283210934,1,['config'],['configuration']
Modifiability,"It can easely be extended to multiple sequences... it just happened that I didn't need it personally. . I took a quick pick to your branch... do you really care about the contig descriptions?.... I would say that this ""aligner"" class should not be responsible of compose the multi sequence fasta file but rather accept one as a constructor argument and the construction of the fasta is delegated back to the invoker code; in this new more general aligner the current single contig could be implemented as a public static method call.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4780#issuecomment-389762587:17,extend,extended,17,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4780#issuecomment-389762587,1,['extend'],['extended']
Modifiability,"It could just be natural variability in the user's runtime environment, but it's worth doing some longer-running tests to be sure.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3491#issuecomment-324453449:25,variab,variability,25,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3491#issuecomment-324453449,1,['variab'],['variability']
Modifiability,"It depends if the current implemented functionality is useful or not. If yes, then this is an enhancement (to ultimately have both functionalities). If not, then this is a bug (to replace the current functionality by the desired one). . Maybe Yossi could weigh in on use cases?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/263#issuecomment-94342834:94,enhance,enhancement,94,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/263#issuecomment-94342834,1,['enhance'],['enhancement']
Modifiability,"It depends why it fails the filter. One reason is consecutive indel elements, but consider for example:. ref: ACGTTTA; read: AC TTTTA; cigar: 2M1D1I4M. Especially in long-read technologies with a lot of indel errors it seems draconian to throw out reads where this happens once. And okay, I understand that an aligner could represent this as a G->T subsitution, but what about the same thing but with 2D followed by 2I? That strikes me as a much better cigar than calling it a DNP. In general, a bad cigar should mean either that the aligner is bad (in which case why are we filtering isolated reads and not just rejecting the entire BAM?) or the read is malformed. But consecutive indels in a technology with many indel errors is neither of these!. Anyway, I don't think there's a problem with allowing these reads in the GATK, and if there is the refactoring in this #6403 should let us fix any problem easily enough.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6433#issuecomment-583415079:849,refactor,refactoring,849,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6433#issuecomment-583415079,1,['refactor'],['refactoring']
Modifiability,"It looks like all the changes in my original commit with the raw GATK3 code (except for one file) got squashed out somehow, so I can't see just the changes from GATK3 anymore. I'll probably have to go back and re-commit those when you're ready to make this tractable to review. I'll wait to comment on #1 until that happens. As for a default plugin descriptor, I'd prefer not to take one unless its fully implemented, with tests. Plus, although we could develop it here, it should really live in the Barclay repo if its truly generic. More importantly, I'm not sure the plugins in this PR should be plugins at all. Historically, plugins have required a lot of test development and iteration because they have command line arguments (the plugin system is for extending the command line parser with discoverable, re-useable components that are shared across multiple tools, and need shared command line arguments). I haven't looked at the new ones closely, but I'm not sure they're a good fit. As for the files, it look like about 400MB (?) Thats pretty big - you should try to squeeze them down or target some existing files if you can.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-431839008:342,plugin,plugin,342,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-431839008,6,"['extend', 'plugin']","['extending', 'plugin', 'plugins']"
Modifiability,It looks like there's some minor refactoring in your new graph handler. I'm not a real stickler about sneaking that in but just want to check it was intentional.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4622#issuecomment-378055518:33,refactor,refactoring,33,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4622#issuecomment-378055518,1,['refactor'],['refactoring']
Modifiability,"It seems plausible to me, though, that the Google auth library may have been patched to perform checks that it wasn't performing previously. Maybe our project permissions have always been mis-configured :)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3591#issuecomment-330940762:192,config,configured,192,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3591#issuecomment-330940762,1,['config'],['configured']
Modifiability,"It seems that there are a lot of soft clips that aren't bacterial reads.; What's your mean insert size? I've seen lots of aberrant soft clips when; the insert size is small and Picard doesn't catch adapter sequences with; multiple mismatches. Does the Picard percent adapter in alignment summary; metrics seem high? I've also seen lots of soft clips when the chimera rate; is high, sometimes because of bad sample extraction. What's the percent; chimeras in your alignment summary metrics? 5% is bad and I've seen up to; 15%, but that was an FFPE tumor sample. On Mon, Mar 25, 2019 at 8:48 PM jjfarrell <notifications@github.com> wrote:. > When the --dontUseSoftCliiped flag is used, the GQ=0 is much lower- N=1355; > for '0/0' calls.; >; > zcat; > A-ADC-AD004288-BL-NCR-15AD82285.hg38.realign.bqsr.dontUseSoftclipped.g.vcf.gz; > |tr '\t' '\n'|grep '0/0'|tr ':' '\t'|cut -f2,3|awk '$2 == ""0"" {print; > $0}'|cut -f1|sort -n|uniq -c; > 1355 0; > 6 0,0,0; > 7 0,0,0,0; > 602 1; > 537 2; > 520 3; > 595 4; > 441 5; > 511 6; > 583 7; > 701 8; > 403 9; > 468 10; >; > —; > You are receiving this because you were assigned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/5445#issuecomment-476431178>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AGRhdA_gZKYn3vuqNDvvDadvM9tgzQqGks5vaW5IgaJpZM4YxgEF>; > .; >. -- ; Laura Doyle Gauthier, Ph.D.; Associate Director, Germline Methods; Data Sciences Platform; gauthier@broadinstitute.org; Broad Institute of MIT & Harvard; 320 Charles St.; Cambridge MA 0214",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5445#issuecomment-476654990:198,adapt,adapter,198,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5445#issuecomment-476654990,2,['adapt'],['adapter']
Modifiability,"It should be fairly easy to create a read transformer plugin; pretty much follow the pattern of read filter plugins: clone and modify GATKReadFilterPluginDescriptor, and add an instance of the new descriptor to the list of plugins passed in to the command line parser in (appropriate) tool base classes. And of course tests!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2084#issuecomment-245968764:54,plugin,plugin,54,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2084#issuecomment-245968764,3,['plugin'],"['plugin', 'plugins']"
Modifiability,"It should probably be a `ReadWalker`, yes -- but if it can't be one for some reason, it is possible to extend `GATKTool` directly.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1864#issuecomment-222225456:103,extend,extend,103,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1864#issuecomment-222225456,1,['extend'],['extend']
Modifiability,"It turns out I was mistaken that setting the environment variables fixes the problem (stupid error on my part). It's possible the BaseTest message is unrelated. I haven't tested this branch out yet, but building from the commit immediately before the update works. I am going to try the next version to see if it helps. Edit: #3594 does not fix the issue.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3592#issuecomment-330896419:57,variab,variables,57,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3592#issuecomment-330896419,1,['variab'],['variables']
Modifiability,It will be useful if this is added to the configuration system (#2368 and #3081).,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2155#issuecomment-316078724:42,config,configuration,42,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2155#issuecomment-316078724,1,['config'],['configuration']
Modifiability,It will be very useful to have an abstract class for the plugin arguments (as I did for the read filters plugin in #2355) to be able for downstream projects to change default values or hide arguments to the final user.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3292#issuecomment-316079921:57,plugin,plugin,57,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3292#issuecomment-316079921,2,['plugin'],['plugin']
Modifiability,"It won't be able to run any faster than BWA mem does with a similar number of cores, since it is essentially just running bwamem. It's potentially faster as part of a spark pipeline so you can load and process data once instead of saving the data to disk and reloading it repeatedly. . The complete list of spark configuration parameters is available on the [spark docs](https://spark.apache.org/docs/3.5.0/configuration.html). Many of them are not relevant in local mode. From what I understand the local mode is going to execute as a single executor with the number of cores specified in the `local[#]` block ( or the total number of system threads if it's set to `*`) It will use the available memory that java is configured with. I'm pretty sure it's ignoring the memory and configuration parameters you've set. Those will be relevant if you configure a stand alone spark cluster (potentially one running exclusively on your local machine). . Our spark tools are not being actively developed for the most part. We've moved away from them to use single threaded tools widely sharded and managed by cromwell. The additional complexity of the spark environment made it hard to see much benefit when most of the tools are embarassingly parallel and easily shardable.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8897#issuecomment-2214866066:313,config,configuration,313,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8897#issuecomment-2214866066,5,['config'],"['configuration', 'configure', 'configured']"
Modifiability,It would be good for progress meter to be more flexible.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6390#issuecomment-575773895:47,flexible,flexible,47,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6390#issuecomment-575773895,1,['flexible'],['flexible']
Modifiability,It's a little disturbing that we need to use different classpaths for the executor vs. the driver...is this symptomatic of a deeper problem in our configuration? Should we be worried?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1518#issuecomment-190816499:147,config,configuration,147,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1518#issuecomment-190816499,1,['config'],['configuration']
Modifiability,"It's looking like we might have to fix the issues with NIO here after all @tomwhite @jean-philippe-martin, as @lbergelson has been unable to get this working reasonably with the GCS adapter (it runs, but veeeerrryyy slowly).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2306#issuecomment-271691417:182,adapt,adapter,182,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2306#issuecomment-271691417,1,['adapt'],['adapter']
Modifiability,"It's not a surprise that a local disk is faster than a remote one, but the; magnitude of the difference is a lot more than I would expect. I remember; at the time using direct GCS access to get the best possible performance in; the bit I was working on, but I don't remember exactly how much of a; difference it made. From my desktop it takes 2m25s to download the whole file, so the ~6min; difference seems really excessive, something is broken. One thing to look; into is whether the sharding is working correctly (are we getting the; correct number of parallel downloads?). Presumably this code is using the HDFS adapter. It'll be interesting to; compare vs the NIO version (and then the optimized NIO version once I write; it).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1755#issuecomment-213094600:616,adapt,adapter,616,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1755#issuecomment-213094600,1,['adapt'],['adapter']
Modifiability,"It's not clear to me that we want these tools in Gatk4. We deliberately didn't port them because we felt they were unnecessary going forward. . I understand that there are some legitimate use cases that require them: ex low coverage naive variant calling from high ploidy pools which haplotype caller would do poorly on. (Also, do we know that haplotype caller doesn't do well on those sorts of things? Maybe we should consider modifications there if it doesn't?) I'm not sure that supporting that use case is worth the added complexity of maintaining and supporting these tools. Especially since we don't provide a pileup based variant caller as part of gatk4... . @vdauwera Can you comment? . @sooheelee I'm not sure I agree with you that supporting this for mutect 1 is useful. ; A) We don't want to support the use of mutect 1 anymore and would like to encourage people to switch to mutect 2 which I think we now believe is a better variant caller for both snps and indels. ; B) Mutect 1 users are already using gatk3, so they have access to these tools already. Mutect 1 also requires co-cleaning which I believe is a different but related tool to indel realignment. . For the variant review issue, we have thoughts on implementing a much better solution for variant review by creating an assembly plugin for igv.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3104#issuecomment-308451988:1303,plugin,plugin,1303,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3104#issuecomment-308451988,1,['plugin'],['plugin']
Modifiability,"It's not perfect, but one thing we can do is this:. ``` java; /**; * SerializableFunction -> PTransform representing a map.; */; public class Map {; public static <A,B> PTransform<PCollection<? extends A>, PCollection<B>> of(SerializableFunction<A, B> f, B sample) {; return ParDo.of(new DoFn<A, B>() {; @Override; public void processElement(ProcessContext c) throws Exception {; c.output(f.apply(c.element()));; }; @Override; protected TypeDescriptor getOutputTypeDescriptor() {; return TypeDescriptor.of(sample.getClass());; }; });; }; }; ```. You use it like this:. ``` java; PCollection<Integer> p1 = pipeline; .apply(Create.of(Arrays.asList(1, 2, 3))); .apply(Map.of(x -> x + 1, 1);; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/658#issuecomment-122440966:194,extend,extends,194,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/658#issuecomment-122440966,1,['extend'],['extends']
Modifiability,"It's weird, usually java should output an error message if it runs out of memory. The exception would be when java is assigned so much memory that the SYSTEM kills it instead of java killing itself. ; You could try adding `dmesg | tail -100` to your wdl after m2 runs to see if there are any messages from the OOMkiller. . What's your total available memory on the machine and your -Xmx setting? You typically need to leave some memory room for the OS and other native sofware. (although by default our pipelines SHOULD have that configured correctly.)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7494#issuecomment-939070414:530,config,configured,530,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7494#issuecomment-939070414,1,['config'],['configured']
Modifiability,Its use in `ValidateBasicSomaticShortMutations` seems limited to the integration test. Can I rewrite the test to do without `AnnotatedInterval` and call it a day?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3884#issuecomment-526876913:93,rewrite,rewrite,93,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3884#issuecomment-526876913,1,['rewrite'],['rewrite']
Modifiability,"I’ve also revisited this work for MalariaGEN, additionally including further cleanup of the canonical part of the WDLs (mostly low hanging fruit like adding structs, which help a lot for cutting down parameter cruft on Terra). For ease of iteration, this work broke things up into 3 pushes of a button: 1) data collection, 2) preclustering (done in a relatively modular way, so you can swap in whatever clustering script you like, as long as it outputs hard/soft responsibilities) +random selection of training cohorts, and 3) cohort mode + scattered case mode on all clusters. But no reason we couldn’t link some of those up. No problem running 16k samples, with 6 clusters and 300 training samples per, but also note I was only running a single genomic shard containing CNVs of interest for this use case. (I did manage to break Terra for a few days when I tried to attach collected counts to the data model in what I would’ve thought would be a relatively trivial way, but that’s another matter.). I’ve shared some version of these WDLs over Slack previously, but happy to also open up a branch here. I think some of this work may be replicated in GATK-SV and I’m also not sure what we want to make canonical. Surely most users will run only a single cluster. But from the perspective of our MalariaGEN collaborators, the more of what I put together for them being made canonical, the better, as this will ease future maintainability. But will leave it up to other current stakeholders.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5633#issuecomment-894527360:1421,maintainab,maintainability,1421,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5633#issuecomment-894527360,1,['maintainab'],['maintainability']
Modifiability,Just adding a note here that `FeatureCache` should eventually be refactored to use the simplified Interval class (when it exists) to track cache boundaries and compute overlap.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/100#issuecomment-76229630:65,refactor,refactored,65,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/100#issuecomment-76229630,1,['refactor'],['refactored']
Modifiability,"Just an idea: it will be nice to propagate the configuration from `Main` to the tools, and obtain from it the packages/classes to include in the command line tools (this is one of the things that I implemented in #2322).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3126#issuecomment-309680944:47,config,configuration,47,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3126#issuecomment-309680944,1,['config'],['configuration']
Modifiability,"Just checked and this is not new behavior. I was afraid this would be an unintended consequence of the DRAGEN branch but that doesn't seem to be the case. Looking a little closer into the code I actually think the only difference this makes is explicitly for the contamination and nothing else. There are a few layers where we filter reads before the annotations are called (filtered by QC before active region determination, MQ/etc-filtered/un-softclipped/overlap-score-adjusted before assembly, filtered based on poor concordance with existing haplotypes, reads are realigned and re-filtered by position, and again filtered due to contamination downsampling). It seems to me that the two likelihoods arrays fed to the `prepareReadAlleleLikelihoodsForAnnotation()` have had all of the above steps applied to them EXCEPT for the contamination downsampling step applied to them looking thorough the code in the HaplotypeCaller. I guess the question is whether there is a strong reason to make the annotations with/without the contamination adjustment... I think the argument `--use-filtered-reads-for-annotations` is misleadingly labeled though the description looks correct to me since it really does only seem to make a difference for the contamination step. . There is another wrinkle to all of this. For DRAGEN-GATK we evaluated calculating the overlaps/annotations exactly how they do it in DRAGEN and decided against it. In DRAGEN they retain the original reads from the bam (i.e. no realignment/no score adjustments/etc...) and use THOSE for annotation and for genotyping. I would have to pick through their debug output to tell just which subsets get used for genotyping (for example they still use non-haplotype-matching reads for FRD and BQD but I don't remember if those are also used for calculating annotations).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7144#issuecomment-800380324:311,layers,layers,311,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7144#issuecomment-800380324,1,['layers'],['layers']
Modifiability,"Just for posterity:. jhess 1:55 PM; just to clarify: what is the logic behind approximating σ_(τ/min/maj) ≈ (post90 - post10)/2? (edited) ; 1:55; what is the scale factor of 1/2 for?; 1:58; one other thing — how come σ_(min/maj) is the sum of the total CR segment’s variance (i.e. σ_τ) and the allelic segment’s variance?; 2:00; this would imply that the allelic segments are actually a sum of the random variables corresponding to the allelic and total segmentation, which I’m not sure is the case?. slee 5:32 PM; Sorry, just now seeing your questions!; 5:33; The scale factor of 1/2 is pretty arbitrary. Just trying to give an estimate of posterior width when the credible interval might be skewed. A better approach would be to refit posteriors with Gaussians/Betas as mentioned previously.; 5:35; However, I'm not actually convinced that these credible intervals are what we want to pass to the sigmas. As I also mentioned above, if sigma.tau is supposed to be a global quantity, probably the posterior median of the parameter that controls the global variance (given in the .cr.params file) might be a better thing to use. However, I never got a straight answer from anybody about whether this was a segment-level or global quantity---any idea?. slee 5:41 PM; As for using the sum of the CR variance and the MAF variance, you're right---we should be propagating error for the product of the two random variables. Not sure what I was thinking...probably just a brain fart. Not sure if it will make a difference for ABSOLUTE, but thanks for catching that!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5804#issuecomment-652411494:405,variab,variables,405,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5804#issuecomment-652411494,2,['variab'],['variables']
Modifiability,Just noting for posterity that removing build strings in the conda YML seems to have improved portability to different OS environments: https://gatk.broadinstitute.org/hc/en-us/community/posts/360061666671-Broken-conda-env-create-n-gatk-f-gatkcondaenv-yml,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-628860774:94,portab,portability,94,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-628860774,1,['portab'],['portability']
Modifiability,"Just occured to me--Are we saying the application of a _germline workflow_ extends to the creation of a PoN consisting of germline normals for the _somatic workflow_?. If so, we should reorganize the directory structure of the CNV scripts.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3163#issuecomment-310873860:75,extend,extends,75,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3163#issuecomment-310873860,1,['extend'],['extends']
Modifiability,"Just to add some clarity around the the granularity of these log levels: the setLoggingLevel API is setting the level of all of the loggers that inherit from the caller's context configuration, which as I understand it, is essentially global for Hellbender, since it currently has only one configuration.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/603#issuecomment-122898121:145,inherit,inherit,145,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/603#issuecomment-122898121,3,"['config', 'inherit']","['configuration', 'inherit']"
Modifiability,Karthik had suggested setting the environment variable TILEDB_DISABLE_FILE_LOCKING=1 for NFS (see https://github.com/broadinstitute/gatk/issues/4753) -- maybe give that a shot?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5342#issuecomment-453600476:46,variab,variable,46,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5342#issuecomment-453600476,1,['variab'],['variable']
Modifiability,"Let us take an example. Suppose, we configure GenomicsDB with 3 column partitions - 0-10, 10-100, 100-300 and want to run GenomicsDBImport tool with an interval [0,100]. In this case, the import tool will only write contigs between 0 and 100 into first two partitions (according to the loader JSON file). Is this what you had in mind? The command line will look like:; $ gatk-launch GenomicsDBImport -L 0-100 --loaderJSONfile loader.json --streamIdJSONFile stream.json. This can definitely be done. However, the client still needs to know the column partitions.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-277372931:36,config,configure,36,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-277372931,1,['config'],['configure']
Modifiability,"Let's discuss further before you get too far along. The design of the Collections code was intended to ensure that very strict file formats are adhered to within the CNV pipeline. Making it more flexible to accommodate TSVs with arbitrary column headers, relax requirements for sequence dictionaries, etc. undermines that goal. There are also two other issues to consider:. 1) It looks like @jonn-smith has also been putting considerable effort into building a TSV framework for Funcotator. Perhaps CombineSegmentBreakpoints should consider using that framework instead, if it is more appropriate. We can also discuss bringing the CNV pipeline over into that framework, but this should definitely wait until after release. The end goal is for CNV team to spend as little time as possible writing or maintaining any code related to TSV parsing. 2) @mbabadi has put together some python evaluation code for the new gCNV, which makes use of the IntervalTree python package and PyVCF to accomplish some things that are very similar to what CombineSegmentBreakpoints is doing. Perhaps we could implement a similar approach purely in Java by making use of the IntervalTree implementation in htsjdk. I think for now we should treat CombineSegmentBreakpoints as a one-off tool to be used for internal validations. After release, we should design a more generic evaluation tool. This tool could take as input multiple collections of annotated locatables, with a few rigidly defined formats allowed (e.g., VCF, CNV Collection TSVs, perhaps some TSVs from other tools, etc.), with one designated as ground truth. The regions for evaluation could also be specified via -L (since it is possible this might not completely specified by the ground-truth collection). The appropriate intersections and lookups could then be performed.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3995#issuecomment-352860616:195,flexible,flexible,195,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3995#issuecomment-352860616,1,['flexible'],['flexible']
Modifiability,"Let's expand the existing `ReadsPipelineSpark` in both directions, so that it extends from `BWA` to the `HaplotypeCaller`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1646#issuecomment-202432982:78,extend,extends,78,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1646#issuecomment-202432982,1,['extend'],['extends']
Modifiability,"Let's hear what others say, but I think I would strongly prefer to simply take over VariantEval in another repo if this was something you'd consider. I'd likely do much of what you propose anyway (certainly WRT testing); however, perhaps not the microscope we went through with the core GATK changes earlier. On plugins: I like what seems to be shaping up w/ Barclay. I carried over the Stratifier and Evaluator as plugins because it seems like it would make sense to allow tools to provide extensions (VariantEval, our tool, does). If I took this PR a step further, I would have migrated many arguments currently top-level on VariantEval into the plugins themselves (a good feature in Barclay). As an aside: I dont think VariantAnnotator is migrated yet, but we have many GATK3 plugins related to annotation, and hope that tool retains Annotator plugins when it get migrated. My impressions of barclay are probably a little out of date. I agree the main argument parsing framework is pretty robust. Specifically on plugins, it seems a little less so, or at least there are not many tools I visibly see exercising that part of the code. For example, there really should be a default implmentation or base class between Barclay's plugins and ReadFilter plugins. I'm guessing if more tools in GATK4 were using plugins this would have happened. I created something like this for VariantEval, and without a ton of work that could probably get generalized; however, doing so would throw a lot higher bar on me and as noted above I'm trying to take on less, not more at the moment. If we do take over VariantEval, I'm certainly happy to try to contribute code and experiences to improve the core, through more targeted PRs.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-407202501:312,plugin,plugins,312,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-407202501,9,['plugin'],['plugins']
Modifiability,"Looking at MetricFile and with it heavy use of Reflexion looks a bit nasty, if there is a better alternative the better. I guess a refactoring of MetricFile would use annotations to allow one to customize output variable name... force one to have those not-so-good looking CAPITAL_FIELD_NAMES for the sake of it is harsh. Don't understand why One has to commit to ; particular type for all histograms either. . Anyway, only if the use of MetricFile is an overkill I would ask you to do your custom one (i.e if it can be done in a few lines of code).... probably not the case.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3611#issuecomment-336521031:131,refactor,refactoring,131,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3611#issuecomment-336521031,2,"['refactor', 'variab']","['refactoring', 'variable']"
Modifiability,"Looking back into this PR... at some point you are using 'N' to pad what seem to be gaps on the read sequence. Although the end result would be the same perhaps is better to be more explicit and just use '-' instead. In that case my suggestion of using `Nucleotide.intersect` wouldn't cover for the '-' character so you need a explicity ""&&"" or ""II"". When you compare the cost of each different alignment the gap-open and gap-ext are ignored (you only look at base call mismatches). I wonder whether it would be more correct to actually take them in consideration... so imagine that there is no gaps in the original alignment what-soever and that adding a 1bp gap decreases the number of mis-matches by just 1 which is typically Q30 increase in the Lk but the gap itself default penalty is Q45 so can one say that that read wouldn't still support the reference over a 1-bp gap alternative? . Example with a 2-bp gap making the trick:; ```; Ref: ....GCATGTGATATATATATATATATATATATACACACAC....; Read: ....GCATGTG--ATATATATATATATATATATAC <end-of-the-read>; ```. That could happen in STRs with impurities... but if the original alignment did not added itself the gap to reduce the number of mismatches is because precisely due to the added cost of the gap-open and necessary extends that we would be ignoring here. This is all hypothetical until some one quantifies how often this might occur ... so I'm happy to keep ignoring the gaps for now until we get a report on a real-live dataset that would benefit of such a change or some enthusiastic dsde-methods member investigates this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5172#issuecomment-420743269:1270,extend,extends,1270,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5172#issuecomment-420743269,1,['extend'],['extends']
Modifiability,"Looks great!. One quick note: I don't get the idea behind `Poisson` -- shouldn't we simply use negative binomials w/ modeled `mu_sj` and `alpha_sj`, evaluated at observed counts (`tt.arange(min_count, max_count + 1)`), and weighted with the number bins for each count (`_hist_sjm`)? i.e. if one observes an empirical distribution `P_obs(x)` rather than `x` draws, then the appropriate max likelihood objective function is `\sum_x P_obs(x) log P_model(x | \theta)`. Perhaps this is exactly what you've done and I don't get it. Another quick note: what I had in mind was _either_ modeling `mu_sj` at quantized ploidy states, _or_ let the ploidy state be unrestricted w/ a penalty via. a Bernoulli process (possibly w/ different per-contig penalties to account for e.g. higher rate of X/Y loss). We have enough samples in the cohort to select the quantized model (and those samples pin down the per-contig biases `b_j`). The samples that do not conform to quantized ploidy states can then choose whatever (variable) ploidy state they wish by paying a (hefty) price. We would also need to mask contigs that have variable ploidy calls from gCNV.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-376286536:1003,variab,variable,1003,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-376286536,2,['variab'],['variable']
Modifiability,"Lots of refactoring was done for the Segmenter classes in #6499. At least for segmentation, all use cases (CR-only, AF-only, CR+AF, single-sample, multi-sample) now go through `MultisampleMultidimensionalKernelSegmenter`. `AlleleFractionKernelSegmenter` and `CopyRatioKernelSegmenter` classes still exist, but both simply call the `MultisampleMultidimensionalKernelSegmenter` class; this was done so preexisting tests for those two classes could be reused. I'm fine with calling this done. We can always open a new issue in the unlikely event we refactor the modelling code.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5625#issuecomment-900609908:8,refactor,refactoring,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5625#issuecomment-900609908,2,['refactor'],"['refactor', 'refactoring']"
Modifiability,"MTOOLS : true; 14:39:24.083 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 14:39:24.083 INFO DetermineGermlineContigPloidy - Deflater: IntelDeflater; 14:39:24.083 INFO DetermineGermlineContigPloidy - Inflater: IntelInflater; 14:39:24.083 INFO DetermineGermlineContigPloidy - GCS max retries/reopens: 20; 14:39:24.083 INFO DetermineGermlineContigPloidy - Requester pays: disabled; 14:39:24.083 INFO DetermineGermlineContigPloidy - Initializing engine; 14:39:26.111 INFO DetermineGermlineContigPloidy - Shutting down engine; [May 26, 2019 2:39:26 PM UTC] org.broadinstitute.hellbender.tools.copynumber.DetermineGermlineContigPloidy done. Elapsed time: 0.07 minutes.; Runtime.totalMemory()=1511522304; org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException:; python exited with 1; Command Line: python -c import gcnvkernel. Stdout:; Stderr: Traceback (most recent call last):; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configdefaults.py"", line 1738, in filter_compiledir; os.makedirs(path, 0o770) # read-write-execute for user and group; File ""/opt/miniconda/envs/gatk/lib/python3.6/os.py"", line 210, in makedirs; makedirs(head, mode, exist_ok); File ""/opt/miniconda/envs/gatk/lib/python3.6/os.py"", line 220, in makedirs; mkdir(name, mode); PermissionError: [Errno 13] Permission denied: '/root/.theano'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/gcnvkernel/__init__.py"", line 1, in <module>; from pymc3 import __version__ as pymc3_version; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/pymc3/__init__.py"", line 5, in <module>; from .distributions import *; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/pymc3/distributions/__init__.py"", line 1, in <module>; from . import timeseries; File ""/opt/miniconda/envs/gatk/lib/python3.6",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-496007081:2849,config,configdefaults,2849,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-496007081,1,['config'],['configdefaults']
Modifiability,"Mappings = new AssemblyContigWithFineTunedAlignments(contig, tigWithInsMappings.insertionMappings);; > +; > + this.basicInfo = new BasicInfo(contig);; > +; > + annotate(refSequenceDictionary);; > + }; > +; > + private static List<AlignmentInterval> deOverlapAlignments(final List<AlignmentInterval> originalAlignments,; > + final SAMSequenceDictionary refSequenceDictionary) {; > + final List<AlignmentInterval> result = new ArrayList<>(originalAlignments.size());; > + final Iterator<AlignmentInterval> iterator = originalAlignments.iterator();; > + AlignmentInterval one = iterator.next();; > + while (iterator.hasNext()) {; > + final AlignmentInterval two = iterator.next();; > + // TODO: 11/5/17 an edge case is possible where the best configuration contains two alignments,; > + // one of which contains a large gap, and since the gap split happens after the configuration scoring,; > I agree it is backwards. But...; > ; > The reason was that the (naive) alignment configuration scoring module rightnow uses MQ and AS (aligner score) for picking the ""best"" configuration (i.e. sub-list of the alignments given by aligner), which would be technically wrong if we were to split the gap and to simply grab the originating alignment's values.; > ; > This is especially true for AS, whose recomputing takes more time, and code, and forces us to know how AS are computed in the aligner so that there's no bias in computing the scores of naive alignments vs gap-split alignments (may not matter in practice, but still takes more code to compute).; > ; > Lots of the code in the discovery stage was devoted actually to alignment related acrobatics and edge cases so that the breakpoints we could resolve are as accurate as possible.; > I've kept in mind your wisdom that different aligners may be experimented with, but it seems unlikely in the near future (their own quirkiness, lack of API for JNI, etc); it seems more and more likely to me that eventually it's inevitable to have a custom alignment m",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3805#issuecomment-350618009:1740,config,configuration,1740,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3805#issuecomment-350618009,2,['config'],['configuration']
Modifiability,"Maybe I misunderstand the underlying model, but if some Pedigree annotations only need to know which samples are founders (ExcessHet ?) , and some need to know the full relationships (PossibleDeNovo), then I'm suggesting we change the class hierarchy to reflect that:. PedigreeAnnotation; |--TrioAnnotation; |----PossibleDeNovo; |--ExcessHet (assuming ExcessHet only needs founders...); ... Then the plugin could deterministically validate whether the user has provided sufficient args for the set of requested annotations; and if so, propagate them accordingly. A TrioAnnotation could only be populated (from the command line at least) from a file, whereas the others could be populated from either a file or just a set of IDs. I think it would simplify the annotations.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5663#issuecomment-463372550:400,plugin,plugin,400,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5663#issuecomment-463372550,1,['plugin'],['plugin']
Modifiability,Minor enhancements to match VariantRecalibrator tweaks,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2780#issuecomment-309635824:6,enhance,enhancements,6,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2780#issuecomment-309635824,1,['enhance'],['enhancements']
Modifiability,"Modifying what I wrote earlier, got confused with another issue. I am not familiar with Lustre and Lustre configuration. Did the excessive file locking from Lustre(FUTEX_WAIT_PRIVATE?) go away with `--genomicsdb-shared-posixfs-optimizations`? . Is there anyway to configure Lustre buffer sizes for writing? If not, can you try setting environment variable TILEDB_UPLOAD_BUFFER_SIZE to something like 5242880(5M) and try `GenomicsDBImport`? Does it help with performance? Is the amount of file locking lower than before?. If the performance is still not acceptable... What version of gatk are you using? Can you use the latest gatk and try using the `--bypass-feature-reader` option with `GenomicsDBImport`? Does this help with performance?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7646#issuecomment-1039746554:106,config,configuration,106,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7646#issuecomment-1039746554,3,"['config', 'variab']","['configuration', 'configure', 'variable']"
Modifiability,"Most spark tools use the one in GATKSparkTool, but some have some special requirements that make it not work for them. They have to specify different sequence dictionaries or something like that in a way that isn't exposed. Maybe something could be refactored there, but they needed manually adjusting to match the new behavior because of their special handling of the writing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6458#issuecomment-594167389:249,refactor,refactored,249,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6458#issuecomment-594167389,1,['refactor'],['refactored']
Modifiability,Most things were addressed here. A bunch of follow on tickets created to address more complicated refactorings that we don't have time to hit now.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3803#issuecomment-368648535:98,refactor,refactorings,98,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3803#issuecomment-368648535,1,['refactor'],['refactorings']
Modifiability,"Multiple causes can cause closed connections when reading from GCS, almost all of which are outside of our control. This will never be ""completely fixed"" in the sense that even if the code is perfect it's completely possible to send too many requests to GCS, and it'll respond by closing connections. The main factors that I know of are:. - number of concurrent accesses to the GCS bucket in question; - number of concurrent accesses to the GCP project in question; - storage class of the GCS bucket in question (the more expensive ones have more replicas, thus can handle a higher load). If you're running into those difficulties I would suggest trying to reduce the load (reduce the number of concurrent workers or threads) and making sure it's not a single-region storage bucket. If that fails, perhaps try using a different bucket that no one else is also using (to reduce other sources of load). If I understand correctly that you didn't change the version you're using but are suddenly seeing more issues than before, then perhaps the cause is a server-side change from GCS (outside of our control), a change in configuration (are you reading from a bucket of a different class from before), or perhaps just an increase of other activity on the same bucket/project. The current code is very persistent in its retries: as you can see from the messages it spent a whole half hour waiting. If it's an overload situation then you may get better performance by reducing the worker count (as they will have to retry less).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5631#issuecomment-526270716:1118,config,configuration,1118,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5631#issuecomment-526270716,1,['config'],['configuration']
Modifiability,"My $0.02:. 1. In general it's ok with me to not provide a template for WDLs in the GATK repo as long as you guys help us (ie @bshifaw) produce appropriate templates to include in the gatk-workflows repo and in FireCloud. . 2. Re: Picard tools, going forward they should be invoked from the GATK jar by default. Among other benefits, that will reduce support entropy wrt possible combination of versions of tools people might be using. 3. I like the idea of focusing on the auto-generated wrappers for improvements like the string variable for adding arbitrary extra args.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4188#issuecomment-358488159:530,variab,variable,530,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4188#issuecomment-358488159,1,['variab'],['variable']
Modifiability,"My 2 cents: actually the argset strategy would be nice also for plugins. For example, in ReadFilters it might allow to specify ""recommended"" filters but not necessary; and in the annotations to convert the groups to a argument set. +1 to the argset for many use-cases and not only this one!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385879758:64,plugin,plugins,64,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385879758,1,['plugin'],['plugins']
Modifiability,My current plan is to rewrite all the bam/sam files to update them to 1.5.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/758#issuecomment-125371895:22,rewrite,rewrite,22,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/758#issuecomment-125371895,1,['rewrite'],['rewrite']
Modifiability,"My recollection is that this is a use case we never put any priority on so there's no test in the GATK suite for access to private files. There should be, of course. The feature is there and (at least locally) it worked when I tried it. NIO does not use the API_KEY, it uses default credentials. Those are environment variables that are set by the `gcloud` command or pre-set for you in the case of virtual machines on Google. There are two cases: local execution and Spark. . I just tested local execution and it worked fine for me:. ```; $ ./gatk-launch PrintReads -L Broad.human.exome.b37.small.interval_list -I gs://jpmartin-private/bench/WGS-G94982-NA12878.bam -O t_gcs.bam; ```. this command worked even though (unless I'm mistaken) neither the bucket nor the file are public. One challenge however is that the way to set default credentials has changed recently. Calling `gcloud auth login` used to be enough but now we have to call (IIRC) `gcloud auth application-default login`. For Spark, the default credentials are set as whoever owns the dataproc environment that's used to run the show. So it should be set so it has access to the buckets necessary. NIO has mechanisms for accessing buckets that belong to someone other than who is running the Spark job, but they are not hooked into GATK yet.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2394#issuecomment-277832658:318,variab,variables,318,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2394#issuecomment-277832658,1,['variab'],['variables']
Modifiability,"My worry about camel case is that it trips up people a lot, especially those whose native language doesn't have a concept of case (like Chinese). . Maybe long arguments with lots of dashes need to be refactored to have fewer... can you give some examples?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2596#issuecomment-323747625:200,refactor,refactored,200,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2596#issuecomment-323747625,1,['refactor'],['refactored']
Modifiability,Namespaced arguments in barclay are something we've talked about before that could help with this. So multiple argument collections / plugins objects could declare the same argument and it would be de-ambiguated by the full name of the plugin/collection. Something like `--ReadNameFilter.invert --MappingQualityFilter.invert`,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6005#issuecomment-502173995:134,plugin,plugins,134,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6005#issuecomment-502173995,2,['plugin'],"['plugin', 'plugins']"
Modifiability,"No problems. The walkers have no built in parallelism so there's no problem with using state. It makes it harder to adapt to spark, but that's probably not a big deal.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4447#issuecomment-368091726:116,adapt,adapt,116,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4447#issuecomment-368091726,1,['adapt'],['adapt']
Modifiability,No validation here. I was satisfied with the validation from the Palantir report and using this as a robustness test to show that GATK4 HC isn't going to fall over. I have a matched list of GVCFs here: /humgen/gsa-hpprojects/dev/gauthier/scratch/newQualHC/check.list @skwalker could you adapt your analysis to run with this list? I'll need to give you a different jar for the GenotypeGVCFs step on my GVCFs since the annotation format is outdated.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4614#issuecomment-380822981:287,adapt,adapt,287,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4614#issuecomment-380822981,1,['adapt'],['adapt']
Modifiability,"Not a bad idea, will look into that tomorrow. Note that you are using Tensorflow 1.4 or 1.5 and that from v1.6 even the; non-Intel optimized build supports only AVX capable machines. On Thu 11 Oct 2018, 21:07 droazen, <notifications@github.com> wrote:. > *@droazen* commented on this pull request.; > ------------------------------; >; > In; > src/main/java/org/broadinstitute/hellbender/tools/walkers/vqsr/CNNScoreVariants.java; > <https://github.com/broadinstitute/gatk/pull/5291#discussion_r224587026>:; >; > > @@ -198,6 +200,13 @@; > return new String[]{""No default architecture for tensor type:"" + tensorType.name()};; > }; > }; > +; > + IntelGKLUtils utils = new IntelGKLUtils();; > + if (utils.isAvxSupported() == false); > + {; > + return new String[]{CNNScoreVariants.AVXREQUIRED_ERROR};; >; > Maybe the answer is for the conda environments to set an extra environment; > variable that would allow GATK to detect which conda environment it's in.; > Then you could have a check in CNNScoreVariants that aborts the tool only; > if AVX is not present AND you're running in the Intel conda environment,; > and point the user to the non-Intel conda environment in the error message.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/pull/5291#discussion_r224587026>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AG6lr8HM6ItLWqfSaTKeVY4yCp07il29ks5uj6TugaJpZM4XNHdi>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-429109651:881,variab,variable,881,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-429109651,1,['variab'],['variable']
Modifiability,"Not sure if this is outside the scope of a simple port, but I think it would be great if the fitting of a `GaussianMixtureModel` was made a little bit more generic and extracted. Right now the method `maximizeGaussian` takes in `List<VariantDatum>`, but it should be trivial to refactor it to take in a `double[]` or `List<Double>`. Fitting a GMM could be more generally useful for other methods, after all. It might even be useful to extract the k-means clustering code used to initialize the model, if this is retained in the port. Perhaps also outside the scope, but it'd also be nice if variable names were changed to match the notation in Bishop Ch. 10 (on which the variational-Bayes algorithm is based). I think this would make the code much easier to parse from a mathematical standpoint.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2062#issuecomment-236003146:278,refactor,refactor,278,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2062#issuecomment-236003146,2,"['refactor', 'variab']","['refactor', 'variable']"
Modifiability,"Not sure if this is related, but @chandrans and I had some trouble with the dataproc launcher yesterday (didn't recognize some yarn argument). Changing the ""image"" setting in the cluster config solved it, afaiu.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2230#issuecomment-278729124:187,config,config,187,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2230#issuecomment-278729124,1,['config'],['config']
Modifiability,"Note separate method configuration, but uses the same WDL.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3983#issuecomment-362811803:21,config,configuration,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3983#issuecomment-362811803,1,['config'],['configuration']
Modifiability,"Note that before this is merged, we'll need to do a datasource release in which the following property is added to the gencode config files:. ```; # Required field for GENCODE files.; # NCBI build version:; ncbi_build_version = X; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5522#issuecomment-447141409:127,config,config,127,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5522#issuecomment-447141409,1,['config'],['config']
Modifiability,"Note that there is an AnnotateIntervals tool in the CNV pipeline (awaiting review in sl_denoising) that will output a TSV with column headers CONTIG, START, END, and GC_CONTENT. It takes -L, which can do the padding for you. If this doesn't exactly fit the bill for you, then it's probably best if you roll your own implementation rather than modify or refactor that code---should be easy enough.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3859#issuecomment-345807469:353,refactor,refactor,353,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3859#issuecomment-345807469,1,['refactor'],['refactor']
Modifiability,"Note that this is all new code, not directly ported from the old GATK, so should be given appropriate scrutiny. Also note that the initial, primitive walker interface here will certainly change/evolve as we gain a better understanding of what facilities the new GATK engine needs to provide. I will open a separate ticket to port existing hellbender tools to the new ReadWalker interface once this is merged.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/112#issuecomment-70037077:194,evolve,evolve,194,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/112#issuecomment-70037077,1,['evolve'],['evolve']
Modifiability,"Note to self: the gcloud API changes a bit with the new release, apply the changes in [jp_gcloud_17_snapshot](https://github.com/broadinstitute/gatk/tree/jp_gcloud_17_snapshot) to adapt.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2822#issuecomment-306241927:180,adapt,adapt,180,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2822#issuecomment-306241927,1,['adapt'],['adapt']
Modifiability,"Now that there's been some refactoring of this code, it might be relatively straightforward to rewire the GVCFBlockCombiner to take in likelihood data from the pileup without creating a VariantContext, then pass the combined data to a VC and then to the writer.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5618#issuecomment-590953474:27,refactor,refactoring,27,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5618#issuecomment-590953474,1,['refactor'],['refactoring']
Modifiability,"Now, it seems like calling `contaminationDownsampling` right after `retainEvidence` could cause problems if both methods remove reads. However, one might correctly point out that although the cache invalidation I mentioned is not handled systematically, the method `removeEvidenceByIndex` _does_ have some code to update the evidence by sample and the evidence index map. It's possible that this code is totally fine and that this lead is a dead end. However, the code looks like it could be simpler and it's tough to parse. For example, try to track the `to` variable, which determines the determination of the outer `for` loop:. ```; for (int etrIndex = 1, to = nextIndexToRemove, from = to + 1; to < newEvidenceCount; etrIndex++, from++) {; if (etrIndex < evidencesToRemove.length) {; nextIndexToRemove = evidencesToRemove[etrIndex];; evidenceIndex.remove(evidences.get(nextIndexToRemove));; } else {; nextIndexToRemove = oldEvidenceCount;; }; for (; from < nextIndexToRemove; from++) {; final EVIDENCE evidence = evidences.get(from);; evidences.set(to, evidence);; evidenceIndex.put(evidence, to++);; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6586#issuecomment-625030697:560,variab,variable,560,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6586#issuecomment-625030697,1,['variab'],['variable']
Modifiability,"OK so just following along; the problem appears related to the Google Cloud Storage Connector and its configuration. When running on Cloud we need to ask for the `https://www.googleapis.com/auth/devstorage.read_write` scope, as described in [the install docs](https://github.com/GoogleCloudPlatform/bigdata-interop/blob/master/gcs/INSTALL.md). But you're right that `https://www.googleapis.com/auth/cloud-platform` should imply that so it should work... The command line argument is `--scopes` (plural) and not `--scope` but that's probably not the issue, the tool would have complained if you actually typed `scope` in there. . Perhaps the code is trying to do the non-cloud setup and that's what's making it not work on cloud?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3591#issuecomment-331047616:102,config,configuration,102,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3591#issuecomment-331047616,1,['config'],['configuration']
Modifiability,"OK, got lfs working. I understand the reasoning for limiting to 20-21 and will do this when i rewrite the GATK3 tests in the port. However, for the purpose of verifying consistent behavior of this ported tool would be great to use the same resources as GATK3. Since it seems like this is shareable, would it still be possible to get the full genome versions of:. /humgen/1kg/reference/human_g1k_v37.fasta (and FAI, dict); /humgen/gsa-hpprojects/GATK/data/dbsnp_132_b37.vcf + index?. While i can probably figure out the right version off public databases, these are outdated and it would be preferable to get exactly what GATK3 uses just in case there's a difference. . Beyond that, glad to hear it seems like /VariantEval/* is OK to share too. I'm happy to provide a place these could be uploaded, or download from you guys. . Thanks again for the help.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/616#issuecomment-358432986:94,rewrite,rewrite,94,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/616#issuecomment-358432986,1,['rewrite'],['rewrite']
Modifiability,"OK, looks like you can get around the compiler lock issues by pointing each invocation of GermlineCNVCaller to a different compilation directory. For example, invoke `gatk` by. `THEANORC=PATH/TO/THEANORC_# gatk GermlineCNVCaller ...`. This uses the `THEANORC` environment variable to set the `.theanorc` configuration file to `PATH/TO/THEANORC_#` for this instance of GATK (where you should fill in `#` appropriately). Each `PATH/TO/THEANORC_#` should be a file containing the following:. ````; [global]; base_compiledir = PATH/TO/COMPILEDIR_#; ````. Where again, `#` is filled in appropriately. The goal is to point each GermlineCNVCaller instance to a different compilation directory. @xysj1989 can you let me know if this works for you?. This is a bit of a hack. We could probably avoid this by changing the GATK code to use a specified or temporary directory for the theano directory without too much effort. However, there is an upside to using a non-temporary directory to avoid recompilation of the model upon subsequent runs. In this case, we'd just want to let the user be able to specify the theano directory (rather than dump things in `~/.theano` unexpectedly). We should think about whether this should be opt-in, i.e., should we preserve the original behavior of using `~/.theano` by default?. @mwalker174 opinions? @droazen or engine team, thoughts on what the policy should be for python/R scripts doing this sort of thing? Is it generally true that the GATK leaves no trace, other than producing the expected output?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6235#issuecomment-548430809:272,variab,variable,272,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6235#issuecomment-548430809,2,"['config', 'variab']","['configuration', 'variable']"
Modifiability,"OK, thanks. I tried to keep edits here limited and protected. I'm happy to describe more about what I'm trying to do in VariantQC if that's helpful. Also - i have not forgotten about trying to refactor VariantQC to better handle arguments (i.e. dont pass the walker to the VariantEvaluator, and to separate a VariantEvalEngine class, somewhat like VariantAnnotationEngine.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5998#issuecomment-502259266:193,refactor,refactor,193,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5998#issuecomment-502259266,1,['refactor'],['refactor']
Modifiability,"OK. Another thing: since in GATK4 it inherits from LocusWalkerByInterval, -L is now required. the usage examples still say -L is optional. . Tangentially is there a shortcut way to pass ""all intervals in the genome"" to GATK in the -L argument? Is there some trick using the DICT file or something like this? Certainly it's not that hard to convert a .dict file to an interval list, but not automatic.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6617#issuecomment-634784068:37,inherit,inherits,37,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6617#issuecomment-634784068,1,['inherit'],['inherits']
Modifiability,"OK. As a reference, how does GATK deal with max-alternate-alleles for normal human variant calling? Presumably really high alternate alleles would primarily happen in repetitive/index prone-regions? FWIW, When we execute GenotypeGVCFs, we run as ~1000 jobs where each takes an even chunk of the genome, by base pairs. . Yes, I did see the bypass-feature-reader option, but we have jobs in-flight and I'm reluctant to change too many things as once. We will try this when possible though. As far as number of batches imported: I would need to check, but I believe it's only ~5 batches with perhaps 50-100 samples/ea. So I guess it's not that many new batches in the scheme of things, but anecdotally we have noticed that with the last couple rounds of import we needed to reduce batch size to make it work (i.e. not get hung). It is conceivable there is some other factor that is causing that variable performance.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7542#issuecomment-964442581:892,variab,variable,892,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7542#issuecomment-964442581,1,['variab'],['variable']
Modifiability,"OK. I found a potential solution. For this solution, we do not need to add or remove any dependencies. The only change is to the `log4j.properties` file (which configures log4j 1.x) to match the config specified in `log4j2.xml` (which configures log4j2). Now, GKL will use log4j 1.x to log, but the format will match the rest of GATK, which uses log4j2. This means that we have only one GKL for both GATK 3 and 4, at the expense of having to keep to config files, `log4j.properties` and `log4j2.xml`, in sync (which they probably should have been anyway, thought they weren't).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3416#issuecomment-320779413:160,config,configures,160,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3416#issuecomment-320779413,4,['config'],"['config', 'configures']"
Modifiability,"O_BUFFER_SIZE : 131072; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.REFERENCE_FASTA : null; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:39:19.226 DEBUG ConfigFactory - Configuration file values:; 17:39:19.244 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:39:19.244 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:39:19.245 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:39:19.245 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:39:19.245 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:39:19.245 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:39:19.245 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:39:19.245 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:39:19.245 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:39:19.245 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:39:19.245 DEBUG ConfigFactory - createOutputBamIndex = true; 17:39:19.245 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 17:39:19.245 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 17:39:19.246 INFO PathSeqPipelineSpark - I",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:5466,Config,ConfigFactory,5466,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,1,['Config'],['ConfigFactory']
Modifiability,Obviated by ModelSegments rewrite.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3181#issuecomment-356697320:26,rewrite,rewrite,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3181#issuecomment-356697320,1,['rewrite'],['rewrite']
Modifiability,"OfBins) + "" should be >= 0."");; >; > @asmirnov <https://github.com/asmirnov> and @samuelklee; > <https://github.com/samuelklee> are both correct, but for the future in; > cases where you *would* want an IllegalArgumentException you should use; > Utils.validateArg to render this sort of thing a one-liner.; > ------------------------------; >; > In src/main/java/org/broadinstitute/hellbender/tools/copynumber/; > CreateBinningIntervals.java; > <https://github.com/broadinstitute/gatk/pull/3597#discussion_r140646132>:; >; > > + doc = ""width of the bins"",; > + fullName = WIDTH_OF_BINS_LONG_NAME,; > + shortName = WIDTH_OF_BINS_SHORT_NAME,; > + optional = true,; > + minValue = 1; > + ); > + private int widthOfBins = 1;; > +; > + @Argument(; > + doc = ""width of the padding regions"",; > + fullName = PADDING_LONG_NAME,; > + shortName = PADDING_SHORT_NAME,; > + optional = true,; > + minValue = 0; > + ); > + private int padding = 0;; >; > . . . and if this padding is different from the inherited padding then; > this demands a comment to avoid confusion.; > ------------------------------; >; > In src/main/java/org/broadinstitute/hellbender/tools/copynumber/; > CreateBinningIntervals.java; > <https://github.com/broadinstitute/gatk/pull/3597#discussion_r140646146>:; >; > > +; > + // check if the bin widths are set appropriately; > + if(widthOfBins <= 0) {; > + throw new IllegalArgumentException(""Width of bins "" + Integer.toString(widthOfBins) + "" should be >= 0."");; > + }; > +; > + // get the sequence dictionary; > + final SAMSequenceDictionary sequenceDictionary = getBestAvailableSequenceDictionary();; > + final List<SimpleInterval> intervals = hasIntervals() ? intervalArgumentCollection.getIntervals(sequenceDictionary); > + : IntervalUtils.getAllIntervalsForReference(sequenceDictionary);; > +; > + // create an IntervalList by copying all elements of 'intervals' into it; > + IntervalList intervalList = new IntervalList(sequenceDictionary);; > + intervals.stream().map(si -> new Inte",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3597#issuecomment-331744211:4993,inherit,inherited,4993,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3597#issuecomment-331744211,1,['inherit'],['inherited']
Modifiability,"Oh, I hadn't noticed that there was a compilation warning causing the test to fail. ```; /gatk/src/test/java/org/broadinstitute/hellbender/MainTest.java:55: warning: [serial] serializable class ExitNotAllowedExcepion has no definition of serialVersionUID; private static final class ExitNotAllowedExcepion extends SecurityException {; ^; error: warnings found and -Werror specified; ```. Please fix that also :)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4283#issuecomment-361661772:306,extend,extends,306,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4283#issuecomment-361661772,1,['extend'],['extends']
Modifiability,"Oh, also note that there might be some variable-name references in the Javadocs for the VQSR-lite tools that are not rendered properly in online docs; see https://github.com/broadinstitute/gatk/issues/8146 for more context. However, if you're just looking at the Javadocs via IntelliJ, everything should look fine.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8131#issuecomment-1414063049:39,variab,variable-name,39,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8131#issuecomment-1414063049,1,['variab'],['variable-name']
Modifiability,"Oh, interesting. That's a real problem. We inherited that code from picard and I don't think anyone ever paid attention to it. I'm assuming it's in there because someone encountered a tmp dir they couldn't read/write to but could set permissions on at some point, which seems weird. . At most we should be setting it for owner only I think.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4513#issuecomment-371635375:43,inherit,inherited,43,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4513#issuecomment-371635375,1,['inherit'],['inherited']
Modifiability,Ohh. This looks like what we have really wanted when we refactor the test suite to test spark and other tools using the same methods. This should bring restful nights to us all. Unfortunately it looks like most of the docker tests have failed with errors along the lines of this: ; ```; org.gradle.api.internal.tasks.testing.TestSuiteExecutionException: Could not complete execution for Gradle Test Executor 1.; 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:63); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); 	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); 	at com.sun.proxy.$Proxy2.stop(Unknown Source); 	at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:377); 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(Exec,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5787#issuecomment-472107858:56,refactor,refactor,56,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5787#issuecomment-472107858,1,['refactor'],['refactor']
Modifiability,"On the first question, we definitely appreciate how much work this will take. Often, porting the code is the easy part; developing new tests and test data can be a huge effort. I can try to find out if it would be possible for you to take the tool over - I know this kind of thing has come up before for other tools, but I'd have to ask around to find that out. @vdauwera do you have input on this ?. As for the plugins, currently in your branch `VariantStratification` and `VariantEvaluator` are modeled as Barclay command line plugin descriptors, and I was questioning whether thats necessary. Being a plugin is not necessarily required - `ReadFilter` and `Annotation` are both plugins, but they didn't have to be, and it takes quite a bit of work (again, mostly test development) to get a plugin right. Also, I'd consider the Barclay plugin framework to be pretty developed at this point, so I'd be curious to learn more about what issues you see. And yes, definitely don't check any of the large GATK3 test files into the repo, even temporarily. Take a look at [General guidelines for GATK4 developers](https://github.com/broadinstitute/gatk#dev_guidelines) if you haven't already. As you pointed out, new GATK4 tests that use smaller files would have to be developed. We'd want those to be included, and passing tests on the CI server, before we started reviewing the branch, so we know we're reviewing code that works and is covered by tests as much as possible. The second commit in my list above would have only your GATK3 java test files, etc (but not the big files, which you appear to have locally). The third commit would have your ported tool code, as well as the new test code, with the new tests enabled, as well as the smaller input files and expected results files. At the end we'd remove commit #2.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-407185633:412,plugin,plugins,412,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-407185633,6,['plugin'],"['plugin', 'plugins']"
Modifiability,"One concern I have is the maintainability of the test (having been burned by this in other places myself). When we add a new output field, etc we need a very easy way to update/generate these results. At the very least some instructions would be helpful (and imagine someone to follow those as part of a PR)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7192#issuecomment-821234533:26,maintainab,maintainability,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7192#issuecomment-821234533,1,['maintainab'],['maintainability']
Modifiability,"One major goal is to replace all the system properties in `gatk-launch`. Config options would be a combination of:; -Java system properties (like in gatk-launch); -Other engine-wide settings (like codec package names in `FeatureManager`, or NIO retries)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2368#issuecomment-307467920:73,Config,Config,73,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2368#issuecomment-307467920,1,['Config'],['Config']
Modifiability,"One more thing: I'm also wondering if it would be possible to get a quick, preliminary evaluation of such a process without actually doing the work of adding it into the training tool. It's probably possible to do a slightly more ""manual"" validation split (say, using one or a few chromosomes), run the score tool on that validation set, use some external code to calculate the desired threshold from the resulting scores, and then use that threshold going forward. Actually, now that I've written it out, that sounds a lot cleaner and more flexible! Let me try to hack together the corresponding workflow.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1065543611:541,flexible,flexible,541,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1065543611,1,['flexible'],['flexible']
Modifiability,"One observation that illustrates the need for care when optimizing metrics: for a few of the F1 optimizations, the haplotype-to-reference match-value parameter gets driven to its minimal value (1). Not 100% sure, but I'm guessing this might effectively boost precision by somehow cutting down on the complexity of proposed haplotypes---it depends on what the exact behavior of our SW algorithm is for negative scores. @davidbenjamin any thoughts on this behavior?. Something I don't quite understand yet is if we can impose some effective constraints on the parameters or otherwise reduce the number of independent dimensions. For example, it seems reasonable to me to fix the gap-extend penalties to -1 and let all other parameters be defined w.r.t. them. But perhaps we can also fix the match values similarly?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-712268193:681,extend,extend,681,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-712268193,1,['extend'],['extend']
Modifiability,"One part of this ticket is done: https://github.com/broadinstitute/gatk/pull/4964 added accessors that allow direct descendants of `GATKTool` to directly access engine datasources, while still forbidding direct access for tools that extend a Walker base class (except for Walker types living in the engine package, which still have access).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4341#issuecomment-483829878:233,extend,extend,233,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4341#issuecomment-483829878,1,['extend'],['extend']
Modifiability,"One proposal for moving forward would be to have a default properties file with a known name/location that is included in the gatk jar (say, ""gatk.default.properties""), which is always loaded and populates the initial configuration, and then use the classloader getResources method to also load all resources with some other known name (say, ""gatk.properties""). That way any properties files on the classpath with the known name would be automatically discovered and loaded. The apache commons API allows looks like it has good support for handling this using a [composite](http://commons.apache.org/proper/commons-configuration/userguide/howto_compositeconfiguration.html#Composite_Configuration_Details) configuration. We would have to define some rules around override semantics, but it looks like the api provides a lot of control over that as well.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2322#issuecomment-274654954:218,config,configuration,218,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2322#issuecomment-274654954,3,['config'],['configuration']
Modifiability,"One thing to try is to configure cromwell to retain the log directory via a workflow option when we run the tests. Then at the end of the build we can copy them somewhere, either always, or via the travis after_failure entry in the build matrix. Then we'd be able to see exactly what failed in the travis environment.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4130#issuecomment-357059677:23,config,configure,23,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4130#issuecomment-357059677,1,['config'],['configure']
Modifiability,"One variable that we need to control for is OpenJDK vs. Oracle JDK. Apparently these errors happened with OpenJDK, which is known to be flakier in the networking department than Oracle JDK. We should test with Oracle's JDK and see if the errors persist.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300313874:4,variab,variable,4,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300313874,1,['variab'],['variable']
Modifiability,One way to mitigate this problem is to set the GRADLE_USER_HOME environment variable to move the gradle cache onto a shorter path. This solved the problem for me for BaseRecalibratorDataflow. ; In my case I saw about over 100 jars listed.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/580#issuecomment-114291341:76,variab,variable,76,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/580#issuecomment-114291341,1,['variab'],['variable']
Modifiability,"Oof, that's a nasty problem. We can definitely do something about it. It feels more like a Microsoft bug than a GATK one though. It seems crazy that each layer pull has to be a separate web request and there's no batch api for it? Multi layer docker builds are pretty standard from what I understand. . It sounds like your suggestions are talking about 2 slightly different issues to me. 1. Too many layers:. We typically have squashed the GATK docker images, but we recently switched to building our release images with google cloud build. Since squash is *STILL* an experimental feature in docker we've had trouble getting it to work there. Since the size reduction was pretty minimal from squashing we figured it would be ok to not prioritize it. It's definitely possible for us to consolidate various layers in the build. Or manually squash the images. We can take a look for our next release. Wide workflows on azure are something we need to support. 2. Docker size reduction:; I've spend a lot of time looking at this in the past. Our docker image is huge, but it's mostly due to the massive size of our python and R dependencies. I've done a bunch of work reducing temporary files in independent layers and using multiple stages to reduce the size. There's not much low hanging fruit left there. Similarly, moving to alpine is tricky an has limited benefit. GATK packages a number of C libraries which do not work out of the box on alpine due to the different C runtime. (At least that was the case the last time I investigated it a few years ago. ) I suspect there's a way to port things so they work on it, but it's not something we can do now. It also wouldn't be much of a help, the base image is completely dwarfed by piles of python and R dependencies which are very difficult to safely trim. Anyway, that's the state of things. We've considered a java only image for a while which would be much smaller than the current one. (although still fat by most docker standards...). We've never ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8684#issuecomment-1934859427:400,layers,layers,400,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8684#issuecomment-1934859427,2,['layers'],['layers']
Modifiability,"Originally, we just had the normal be optional. You also had automated tests in the WDL Travis. . In FC, for tumor only, you would probably want a separate method configuration that ran on sample entity type. I'm open to other suggestions, but I can't think of another way. This could in theory be used for germline calling, too.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3983#issuecomment-362811723:163,config,configuration,163,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3983#issuecomment-362811723,1,['config'],['configuration']
Modifiability,"Overall the refactoring looks good and makes sense… but I'm not seeing how this fixes the problem of eating exceptions we saw during a recent run. Can you explain what was happening before, and how the new code addresses it?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7480#issuecomment-927995357:12,refactor,refactoring,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7480#issuecomment-927995357,1,['refactor'],['refactoring']
Modifiability,Override mechanisms (in order of priority). -Individual config options specified on the command line manually / Or explicit config file ; -Override config file packaged into a downstream project; -Default GATK config file,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2368#issuecomment-307467520:56,config,config,56,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2368#issuecomment-307467520,4,['config'],['config']
Modifiability,"PPY_COMPRESSOR : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.REFERENCE_FASTA : null; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:39:19.226 DEBUG ConfigFactory - Configuration file values:; 17:39:19.244 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:39:19.244 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:39:19.245 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:39:19.245 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:39:19.245 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:39:19.245 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:39:19.245 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:39:19.245 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:39:19.245 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:39:19.245 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:39:19.245 DEBUG ConfigFactory - createOutputBamIndex = true; 17:39:19.245 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_a",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:5249,Config,ConfigFactory,5249,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,1,['Config'],['ConfigFactory']
Modifiability,"Passing in the properties file did work (with the --gatk-config-file option). However, of the four tools I tested (MarkDuplicates, BaseRecalibrator, ApplyBQSR, and HaplotypeCaller) all of the tools accepted the --gatk-config-file option except for MarkDuplicates, which complains that it is not a recognized option. Perhaps this should be turned into a separate issue?. Thanks",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4435#issuecomment-368036324:57,config,config-file,57,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4435#issuecomment-368036324,2,['config'],['config-file']
Modifiability,"Per discussion with @kgururaj, this will probably take the form of a flag that suppresses materializing the genotype data. Also see the protobuf-based enhancements described [here](https://github.com/broadinstitute/gatk/issues/3689).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3688#issuecomment-336965003:151,enhance,enhancements,151,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3688#issuecomment-336965003,1,['enhance'],['enhancements']
Modifiability,"Please need help : . I ran the script for VQSR . gatk-4.2.0.0/gatk VariantRecalibrator\; -V variants_sitesonly.vcf.gz\; 	-trust-all-polymorphic\; -tranche 100.0 -tranche 99.95 -tranche 99.9 -tranche 99.5 -tranche 99.0 -tranche 97.0 -tranche 96.0 -tranche 95.0 -tranche 94.0 -tranche 93.5 -tranche 93.0 -tranche 92.0 -tranche 91.0 -tranche 90.0\; -an FS -an ReadPosRankSum -an MQRankSum -an QD -an SOR -an DP\ ; -mode INDEL\; -max-gaussians 4\; -resource:mills,known=false,training=true,truth=true,prior=12:Mills_and_1000G_gold_standard.indels.hg38.vcf.gz\; -resource:axiomPoly,known=false,training=true,truth=false,prior=Axiom_Exome_Plus.genotypes.all_populations.poly.hg38.vcf.gz\; -resource:dbsnp,known=true,training=false,truth=false,prior=2:Homo_sapiens_assembly38.dbsnp138.vcf\; -O cohort_indels.recal\; --tranches-file cohort_indels.tranches. ERROR - >. A USER ERROR has occurred: Argument resource was missing: Argument 'resource' is required. Any help would be really great !. thank you; Smeeta",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2199#issuecomment-885465593:132,polymorphi,polymorphic,132,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2199#issuecomment-885465593,1,['polymorphi'],['polymorphic']
Modifiability,Please use the template in the WDL GATK repo doc that was shared. Or we can modify that template. I'd like the document to match what is generated automatically. The template in that document includes optimizations and is quite portable.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2480#issuecomment-358440295:228,portab,portable,228,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2480#issuecomment-358440295,1,['portab'],['portable']
Modifiability,Probably CircleCI might not be at Gradle 2.1 or higher for the [Gradle Download Task](https://github.com/michel-kraemer/gradle-download-task) plugin.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/519#issuecomment-101733759:142,plugin,plugin,142,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/519#issuecomment-101733759,1,['plugin'],['plugin']
Modifiability,Probably git. See `core.autocrlf` at https://git-scm.com/book/id/v2/Customizing-Git-Git-Configuration.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-431474812:88,Config,Configuration,88,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-431474812,1,['Config'],['Configuration']
Modifiability,"RROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.varia.NullAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.varia.NullAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""NullAppender"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.varia.NullAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.varia.NullAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""NullAppender"".; log4j:ERROR A ""org.apache.log4j.varia.NullAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.mis",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312229998:5850,variab,variable,5850,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312229998,1,['variab'],['variable']
Modifiability,"Rather than delaying argument validation to a second pass, I changed the sequence so the tool instantiates the descriptor and gives it the tool defaults right from the start, (and then passes the descriptor instance(s) to the arg parser) so it has all the state it needs to validate at arg parsing time. Also, I reverted the removal of the package limitation for plugins, at least temporarily, since searching through all packages looked like it slowed down the integration tests.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1973#issuecomment-232046070:363,plugin,plugins,363,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1973#issuecomment-232046070,1,['plugin'],['plugins']
Modifiability,"Really we need some tests for gs:// files in ReadsSparkSinkUnitTest - e.g. a GCS version of testWritingToFileURL. This needs knowledge of how to configure the Hadoop GCS connector (outside dataproc), which I lack. Perhaps someone else knows how to do this?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2306#issuecomment-270615942:145,config,configure,145,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2306#issuecomment-270615942,1,['config'],['configure']
Modifiability,Refactoring won't happen,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6877#issuecomment-1377803387:0,Refactor,Refactoring,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6877#issuecomment-1377803387,1,['Refactor'],['Refactoring']
Modifiability,"Removed the inheritance. Moved the file, though I can move it again if someone feels strongly about where it should be.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/487#issuecomment-99599956:12,inherit,inheritance,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/487#issuecomment-99599956,1,['inherit'],['inheritance']
Modifiability,"Results are in:. Using the branch for PR #4971 with the value `ALIGNMENT_LOW_READ_UNIQUENESS_THRESHOLD` set to 10 and 19, while keeping the gap split children together (that is, method ; `private static GoodAndBadMappings splitGaps(final GoodAndBadMappings configuration, final boolean keepSplitChildrenTogether)` is called with `false` for its second parameter). Here are the comparisons:; ```; simple variants unique TP unique FP; size-10 filter: 10756 24 101; size-19 filter: 10755 1 0; ```. So I think your suggestion is a better trade off!. What I'll do is make that parameter an (advanced) CLI argument in PR #4971 , and experiment more to settle on a good default value.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4962#issuecomment-403890890:257,config,configuration,257,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4962#issuecomment-403890890,1,['config'],['configuration']
Modifiability,"Reviving this. This will essentially be a major refactor/rewrite of CreatePanelOfNormals to make it scalable enough to handle WGS. - [x] CombineReadCounts is too cumbersome for large matrices. Change CreatePanelOfNormals to take in multiple -I instead.; - [x] Rename NormalizeSomaticReadCounts to DenoiseReadCounts and require integer read counts as input. These will still be backed by a ReadCountCollection until @asmirnov239's changes are in.; - [x] Remove optional outputs (factor-normalized and beta-hats) from DenoiseReadCounts. For now, TN and PTN output will remain in the same format (log2) to maintain compatibility with downstream tools.; - [x] Maximum number of eigensamples K to retain in the PoN is specified; the smaller of this or the number of samples remaining after filtering is used. The number actually used to denoise can be specified in DenoiseReadCounts. If we are going to spend energy computing K eigensamples, there is no reason we shouldn't expose all of them in the PoN, even if we don't want to use all of them for denoising. (Also, the current SVD utility methods do not allow for specification of K < N when performing SVD on an MxN matrix, even though the backend implementations that are called do allow for this; this is terrible. In any case, randomized SVD should be much faster than the currently available implementations, even when K = N).; - [x] Rename CreatePanelOfNormals to CreateReadCountPanelOfNormals; - [x] Refer to ""targets"" as intervals. See #3246.; - [x] Remove QC.; - [x] Refer to proportional coverage as fractional coverage.; - [x] Perform optional GC-bias correction internally if annotated intervals are passed as input.; - [x] Make standardization process for panel and case samples identical. Currently, a sample mean is taken at one point in the PoN standardization process, while a sample median is taken in the case standardization process.; - [x] HDF5 PoN will store version number, all integer read counts, all/panel intervals, all/panel ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-313921687:48,refactor,refactor,48,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-313921687,2,"['refactor', 'rewrite']","['refactor', 'rewrite']"
Modifiability,"Right, the `try` block needs to catch the `java.lang.UnsatisfiedLinkError` exception. We'll fix that in the next GKL release. As a workaround, you can try defining this environment variable: `export GKL_USE_LIB_PATH=1`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2302#issuecomment-265859639:181,variab,variable,181,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2302#issuecomment-265859639,1,['variab'],['variable']
Modifiability,"Right. I want to subset by sample name, effectively taking a slice of the; position by sample genotype matrix and computing Info annotations based; only in the kept samples. On Mon, Mar 4, 2019, 8:52 PM Karthik Gururaj <notifications@github.com>; wrote:. > I'm assuming you will have the subset of samples before creating a; > GenomicsDBFeatureReader object (and before creating the corresponding; > Protobuf export configuration object).; >; > More precisely, you are NOT requesting a line by line filter similar to:; > At pos 100, compute INFO fields etc including only the samples whose QUAL; > > 5; > At pos 102, compute INFO fields etc including only the samples whose QUAL; > > 5; > ....; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/5570#issuecomment-469502322>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AGRhdMZjIbDJ2eDZcB69XHiUycnumzHrks5vTc3PgaJpZM4Z7pF2>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5570#issuecomment-469680991:416,config,configuration,416,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5570#issuecomment-469680991,1,['config'],['configuration']
Modifiability,"Running a particular bam sort takes ~20minutes with hdd and 16 minutes with ssd. So it's definitely being used somehow. It looks like spark.local.dir is over ridden by the environment variable LOCAL_DIRS, and I don't see that set, but it's possible it's being set but not recorded correctly in the UI or something like that. Someone will need to poke at a bit more to be more clear about what's happening.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2426#issuecomment-283481370:184,variab,variable,184,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2426#issuecomment-283481370,1,['variab'],['variable']
Modifiability,Saw it again [here](https://travis-ci.com/broadinstitute/gatk/jobs/180435353) (now restarted). If I'm reading the serialization stack in the right order:. ```; Serialization trace:; classes (sun.misc.Launcher$AppClassLoader); classLoader (org.apache.hadoop.conf.Configuration); conf (org.apache.hadoop.hdfs.DistributedFileSystem); fs (hdfs.jsr203.HadoopFileSystem); hdfs (hdfs.jsr203.HadoopPath); path (htsjdk.samtools.seekablestream.SeekablePathStream); seekableStream (htsjdk.tribble.TribbleIndexedFeatureReader); featureReader (org.broadinstitute.hellbender.engine.FeatureDataSource); featureSources (org.broadinstitute.hellbender.engine.FeatureManager); ```. it looks like we're trying to serialize a ClassLoader. The FieldSerializer does appear to use a ClassLoader to load classes during serialization.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5680#issuecomment-467462740:262,Config,Configuration,262,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680#issuecomment-467462740,1,['Config'],['Configuration']
Modifiability,"ScoreVariantAnnotations:. Scores variant calls in a VCF file based on site-level annotations using a previously trained model. TODOs:. - [x] Integration tests. Exact-match tests for (non-exhaustive) configurations given by the Cartesian product of the following options:; * Java Bayesian Gaussian Mixture Model (BGMM) backend vs. python sklearn IsolationForest backend; (BGMM tests to be added once PR for the backend goes in.); * non-allele-specific vs. allele-specific; * SNP-only vs. SNP+INDEL (for both of these options, we use trained models that contain both SNP and INDEL scorers as input) ; - [x] Tool-level docs. Minor TODOs:. - [x] Parameter-level docs.; - [x] Parameter/mode validation.; - [x] Double check or add behavior for handling previously filtered input, clearing present filters, etc. Future work:. - [ ] The `score_samples` method of the sklearn IsolationForest is single-threaded. See (possibly stalled) PR at https://github.com/scikit-learn/scikit-learn/pull/14001 and some workarounds using e.g. `multiprocessing` ibid.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7724#issuecomment-1067948563:199,config,configurations,199,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7724#issuecomment-1067948563,1,['config'],['configurations']
Modifiability,"See https://github.com/broadinstitute/gatk/issues/4888, which is an older report for the same issue. As mentioned there, I think we should patch our fork of `google-cloud-java` to do a channel reopen on `UnknownHostException` for now as a quick fix. @jean-philippe-martin is eventually going to add an official configuration mechanism for clients of `google-cloud-java` to customize which errors should trigger a retry/reopen, which should provide a better way to deal with these errors as they crop up without having to modify the NIO library itself.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-412134420:311,config,configuration,311,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-412134420,1,['config'],['configuration']
Modifiability,Seems like something like https://github.com/broadinstitute/gatk/issues/4794 could be avoided if we rewrote this. It seems like a pretty simple rewrite too...,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4535#issuecomment-391044481:144,rewrite,rewrite,144,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4535#issuecomment-391044481,1,['rewrite'],['rewrite']
Modifiability,"Several backwards-incompatible changes in VCF 4.3 (eg., escape sequences) have made it difficult to update without first doing a major refactoring in HTSJDK to better version/isolate our parsers. @cmnbroad can provide further details.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2602#issuecomment-471719969:135,refactor,refactoring,135,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2602#issuecomment-471719969,1,['refactor'],['refactoring']
Modifiability,"Should this be closed so that your other code can be refactored to use `GenomicsConverter` for now?. Let's not move either `GenomicsConverter` or `ReadConverter` to hellbender just yet. It's unclear whether they'll be needed at all once the read common interfaces branch is in, and also they have some issues (eg., `ReadConverter` ""encodes"" attribute values by calling `toString()` on them, which is bad for certain attribute types such as byte arrays).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/493#issuecomment-100273393:53,refactor,refactored,53,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/493#issuecomment-100273393,1,['refactor'],['refactored']
Modifiability,"Since @davidadamsphd is out today, would either @tomwhite or @laserson be available to review JUST the spark part of this branch (specifically, just the classes `BaseRecalibratorSpark`, `BaseRecalibratorSparkFn`, `ApplyBQSRSpark`, `ApplyBQSRSparkFn`, `ReadsPipelineSpark`, `BaseRecalibratorSparkIntegrationTest`, and `ApplyBQSRSparkIntegrationTest`)? . The rest of the branch is just a BQSR engine refactoring that will be reviewed by @jean-philippe-martin as soon as we do another rebase on top of master (we'll ping him as soon as that part of the branch is ready for review).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/911#issuecomment-142645705:398,refactor,refactoring,398,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/911#issuecomment-142645705,1,['refactor'],['refactoring']
Modifiability,"Since Geraldine is away till the end of the week, and we are under the Nov 23 deadline for review, I will proceed with changes. I think it useful for me to go through the motions and see what other discussion items turn up. Notes on factors I think are of interest to users re annotators:; - cohort vs sample level annotation; - InfoFieldAnnotation; - GenotypeAnnotation; - minimum number of samples, e.g. 10 for inbreedingcoefficient; - standard annotations for each tool (HC, M2 and VariantAnnotator), standard allele-specific annotations.; - StandardMutectAnnotation; - PerAlleleAnnotation; - StandardAnnotation (extends Annotation); - StandardHCAnnotation; - VariantAnnotation; - noticing `public class` vs `public final class`. Not annotating `abstract` class nor `public interface`.; - What is a reducible annotation?; - I would really find helpful the acronym for the annotation, e.g. MBQ, be listed with the annotation summary, e.g. Median base quality of bases supporting each allele.; - Annotations that are specific to a tool. E.g. DepthPerSampleHC can only be used by HaplotypeCaller and not VariantAnnotator. Doc doesn't say anything about Mutect2. ; - Not sure what VariantOverlapAnnotator does ~~but went ahead and summarized as ""Annotate ID field and attribute overlap FLAG"".~~ `did not tag`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3809#issuecomment-344427246:616,extend,extends,616,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3809#issuecomment-344427246,1,['extend'],['extends']
Modifiability,"Since not all Picard tools operate on Sam files, and some GATK tools do (but won't extend this new class), I'd advocate for a more generalized name.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/83#issuecomment-69979216:83,extend,extend,83,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/83#issuecomment-69979216,1,['extend'],['extend']
Modifiability,"Since we are going to change many of those argument names (camel-back to kebab-case) I think we should take this opportunity to use constants to specify argument names in the code and use them in our test code so further changes in argument names don't break tests. . Take as an example [CombineReadCounts](https://github.com/broadinstitute/gatk/blob/3ec7399a54ccf89d2b323b2be71b8b7e4931174c/src/main/java/org/broadinstitute/hellbender/tools/exome/CombineReadCounts.java). Extract enclosed below. It might be also beneficial to add public constant for the default values. ```java; public final class CombineReadCounts extends CommandLineProgram {. public static final String READ_COUNT_FILES_SHORT_NAME = StandardArgumentDefinitions.INPUT_SHORT_NAME;; public static final String READ_COUNT_FILES_FULL_NAME = StandardArgumentDefinitions.INPUT_LONG_NAME;; public static final String READ_COUNT_FILE_LIST_SHORT_NAME = ""inputList"";; public static final String READ_COUNT_FILE_LIST_FULL_NAME = READ_COUNT_FILE_LIST_SHORT_NAME;; public static final String MAX_GROUP_SIZE_SHORT_NAME = ""MOF"";; public static final String MAX_GROUP_SIZE_FULL_NAME = ""maxOpenFiles"";; public static final int DEFAULT_MAX_GROUP_SIZE = 100;. @Argument(; doc = ""Coverage files to combine, they must contain all the targets in the input file ("" +; TargetArgumentCollection.TARGET_FILE_LONG_NAME + "") and in the same order"",; shortName = READ_COUNT_FILE_LIST_SHORT_NAME,; fullName = READ_COUNT_FILE_LIST_FULL_NAME,; optional = true; ); protected File coverageFileList;. @Argument(; doc = READ_COUNT_FILES_DOCUMENTATION,; shortName = READ_COUNT_FILES_SHORT_NAME,; fullName = READ_COUNT_FILES_FULL_NAME,; optional = true; ); protected List<File> coverageFiles = new ArrayList<>();. @Argument(; doc = ""Maximum number of files to combine simultaneously."",; shortName = MAX_GROUP_SIZE_SHORT_NAME,; fullName = MAX_GROUP_SIZE_FULL_NAME,; optional = false; ); protected int maxMergeSize = DEFAULT_MAX_GROUP_SIZE;. @ArgumentCollection; protect",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-346175904:618,extend,extends,618,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-346175904,1,['extend'],['extends']
Modifiability,"Size=0,spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 ,spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 ,spark.kryoserializer.buffer.max=512m,spark.yarn.executor.memoryOverhead=600,spark.executor.cores=2,spark.executor.instances=2 --jar /Users/droazen/src/hellbender/build/libs/gatk-package-4.beta.6-54-g0ee99da-SNAPSHOT-spark.jar -- CountReadsSpark -I gs://hellbender/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam --sparkMaster yarn; Job [acdae2af-e0ce-4822-87f5-dcd165d85cf4] submitted.; Waiting for job output...; 20:39:42.869 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 20:39:43.053 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/tmp/acdae2af-e0ce-4822-87f5-dcd165d85cf4/gatk-package-4.beta.6-54-g0ee99da-SNAPSHOT-spark.jar!/com/intel/gkl/native/libgkl_compression.so; [November 27, 2017 8:39:43 PM UTC] CountReadsSpark --input gs://hellbender/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam --sparkMaster yarn --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --interval_merging_rule ALL --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --gcs_max_retries 20 --disableToolDefaultReadFilters false; [November 27, 2017 8:39:43 PM UTC] Executing as root@droazen-test-cluster-m on Linux 3.16.0-4-amd64",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3855#issuecomment-347320994:2637,variab,variables,2637,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3855#issuecomment-347320994,2,"['config', 'variab']","['configured', 'variables']"
Modifiability,"So I just updated one of the newer tests, and now all of the tests for HaplotypeCaller seem to be passing locally. The previous commits updating the copy code were preserved when Louis reverted, so there were basically no changes I had to make to get this ""working."" That does leave us with one question now:. When looking into this a little with James and Louis earlier, we realized that the code for setting up the ActiveRegionGenotyper uses a weird partial copy of the standard CLI args method that has existed in the code for whoever knows how long. Conceptually this seems like a bad idea, but changing it now would possibly cause some older tests to fail, if they were based on this faulty method reasoning. Should we try to merge the PR as it is now, with all tests passing, and hopefully consistency with previous behavior, or try to update the logic around this genotyper as well at the same time? It's possible we can try to address the latter point as well at some point in the future when we try to get Louis's refactor code actually working. Maybe there could be some quarter goal around a HaplotypeCaller code revamp sometime inspired by some of these ideas?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8609#issuecomment-1847916216:1023,refactor,refactor,1023,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8609#issuecomment-1847916216,1,['refactor'],['refactor']
Modifiability,"So I've tracked this down to being a problem with line 193 in `ReadSparkSink`. . ```; finalOut.saveAsNewAPIHadoopFile(outputFile, SAMRecord.class, SAMRecordWritable.class, SparkHeaderlessBAMOutputFormat.class);; ```. saveAsNewAPIHadoopFile is not writing any part files when `outputFile` (a `String`) is a relative path. i.e. `test.bam`. . If you change it to a fully specified path i.e. `/Users/louisb/Workspace/gatk/testoutput.bam` then it works as expected. I discovered this by running the driver in the debugger which is possible using . ```; --driver-java-options -agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=5005; ```. as an option to spark-submit (or gatk-launch). The quick fix is to have input paths be converted into absolute paths always. Also to add a check in `mergeBam()` to make sure that there's at least 1 part file specified. The longer fix is to ; 1. understand why this is happening only in certain spark configurations; 2. figure out how to unit test for this",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1448#issuecomment-175192707:943,config,configurations,943,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1448#issuecomment-175192707,1,['config'],['configurations']
Modifiability,So we typically override the config files on the command line. We'll have to make sure we wire the log4j 1.x logger to respect our command line overrides if it doesn't already. You can check that by testing if you can control the log output with the --verbosity command. If not we'll have to update `LoggingUtils.setLoggingLevel()`,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3416#issuecomment-320787794:29,config,config,29,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3416#issuecomment-320787794,1,['config'],['config']
Modifiability,"Some always throw, some always filter, some depend on the arguments you; pass to the filter constructor.; On Feb 23, 2015 9:19 PM, ""Geraldine Van der Auwera"" <; notifications@github.com> wrote:. > Ah -- maybe I'm mistaken. Or can it be a difference in how they're; > applied/invoked?; > ; > Maybe there's some inconsistency in behavior. Would be nice to iron this; > all out.; > ; > On Mon, Feb 23, 2015 at 5:17 PM, Louis Bergelson <notifications@github.com; > ; > > wrote:; > > ; > > I'm pretty sure they all do... or at least all can depending on how you; > > configure your MalformedReadFilter.; > > ; > > example:; > > ; > > private static boolean checkHasReadGroup(final SAMRecord read) {; > > if ( read.getReadGroup() == null ) {; > > // there are 2 possibilities: either the RG tag is missing or it is not; > > defined in the header; > > final String rgID =; > > (String)read.getAttribute(SAMTagUtil.getSingleton().RG);; > > if ( rgID == null ); > > throw new UserException.ReadMissingReadGroup(read);; > > throw new UserException.ReadHasUndefinedReadGroup(read, rgID);; > > }; > > return true;; > > }; > > ; > > —; > > Reply to this email directly or view it on GitHub; > > <; > > https://github.com/broadinstitute/hellbender/issues/193#issuecomment-75649333; > > ; > > .; > ; > ## ; > ; > Geraldine A. Van der Auwera, Ph.D.; > Bioinformatics Scientist II; > GATK Support & Outreach; > Broad Institute; > ; > —; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/hellbender/issues/193#issuecomment-75686467; > .",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/193#issuecomment-75686777:562,config,configure,562,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/193#issuecomment-75686777,1,['config'],['configure']
Modifiability,"Some changes after the comments from @droazen (in commit #2360):; * Change the argument name from `--disableAllReadFilters` to `--disableToolDefaultReadFilters`; * Make `--disableToolDefaultReadFilters` mutex w.r.t. `--disableReadFilter`; * As pointed out in a different PR (#2355), make all the plugin arguments common.; * Make `isDisabled()` and `getAllInstances()` honor the `--disableToolDefaultReadFilters` argument (solves #2363). I think that this makes more sense than disable absolutely all the read filters, including the ones provided by the user. The cases where this is more useful are:; - Process the data without filters: provide just `--disableToolDefaultReadFilters`; - Process the data without default filters and add any other: provide `--disableToolDefaultReadFilters` and `--readFilter` with the rest of filters; - Process the data with the default filters and include more: as in the previous behaviour, provide `--readFilter` with the rest of filters; - Process the data with some default filters but change the order: provide `--disableToolDefaultReadFilters` and the list of filters in the new order. . Can you have a look to this one, @cmnbroad and/or @droazen?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2365#issuecomment-275633852:296,plugin,plugin,296,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2365#issuecomment-275633852,1,['plugin'],['plugin']
Modifiability,"Some comments/questions for the review:; - I'll add a separate ticket to rewrite the integration tests, all of which pass and most of which are disabled since they require access to large files on the broad file system. In the meantime I need to add a couple of small tests to get the coverage back up, and would like to get the CR process started.; - I ported a bunch of support files but need feedback on whether they're in the right location.; - Somewhere I saw something that said GATK no longer supports .ped files ? If not, what should the replacement be in the tests require pedigree input?; - Is it a requirement to support Ploidy > 2 ? The current GATK tool, and thus the HB tool, do not; - I did not port the WalkerTestSpec.disableShadowVCF? Is that needed in Hellbender ?; - Are there other headers I should be applying to the output variant file ?. Command Line Arguments:; - I didn't port the GATK command line argument ""-no_cmd_line_in_header"". Should I ? And if not, should the command line args automatically be propagated to the output vcf file ? I didn't see GATK do this anywhere.; - There was one test that used --variant:dbsnp on the command line but I couldn't find the code that processed that in GATK, not sure what the means on the command line.; - I replaced ""-U LENIENT_VCF_PROCESSING"" with ""--lenient"" (testFileWithoutInfoLineInHeaderWithOverride needs this to pass).; - I replaced ""-L"" with --interval since HB seems to use -L for ""lane"" ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/792#issuecomment-128798027:73,rewrite,rewrite,73,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/792#issuecomment-128798027,1,['rewrite'],['rewrite']
Modifiability,Some info on Spark configurations:. https://stackoverflow.com/questions/29441316/specifying-an-external-configuration-file-for-apache-spark,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3079#issuecomment-322552565:19,config,configurations,19,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3079#issuecomment-322552565,2,['config'],"['configuration-file-for-apache-spark', 'configurations']"
Modifiability,"Some offline discussions have led us to the conclusion that this is best handled by tools upstream. Adapters should not be simply soft-clipped, so it shouldn't be the responsibility of M2 or HC to include logic to remove adapters.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6346#issuecomment-575334816:100,Adapt,Adapters,100,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6346#issuecomment-575334816,2,"['Adapt', 'adapt']","['Adapters', 'adapters']"
Modifiability,Someday we could even figure out how to configure it at will!,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/295#issuecomment-78489115:40,config,configure,40,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/295#issuecomment-78489115,1,['config'],['configure']
Modifiability,"Sorry @droazen @LeeTL1220, can you give me a bit more context? @LeeTL1220 is no longer using any of the CNV-specific collections classes that I had hoped might be Tribble-ized in the future, so I'm OK with any decisions you guys make that are specific to his classes (does @jonn-smith have an opinion?) I think that moving towards storing the config in the header is a good thing, in general. If we need to make corresponding changes to the CNV-specific collections classes, then we should talk more. Not all of those collections describe locatables, so I'm not sure how we could fit them in the Tribble framework.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4276#issuecomment-369734330:343,config,config,343,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4276#issuecomment-369734330,1,['config'],['config']
Modifiability,"Sorry, again confounded by static-blocks and inheritance (dup of my own issue https://github.com/broadinstitute/gatk/issues/3483)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5014#issuecomment-405100946:45,inherit,inheritance,45,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5014#issuecomment-405100946,1,['inherit'],['inheritance']
Modifiability,"Sorry, but this bug still isn't fixed as of v4.2.6.1. Reproduce as follows:. ```; --read-filter MateDistantReadFilter; --mate-too-distant-length 1500; ```. Instead of a run-time exception (as in v4.2.5.0), HaplotypeCaller simply produces no variant calls at all. Expected behavior would be to exclude paired-end mappings whose TLEN exceeds the parameterized value. Perhaps there is an implementation bug, unrelated to the original problem, that contains faulty logic for doing this. Thanks...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7701#issuecomment-1102943692:344,parameteriz,parameterized,344,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7701#issuecomment-1102943692,1,['parameteriz'],['parameterized']
Modifiability,"Sorry, it's difficult for me to spot git notifications in my email. . > Maybe @bshifaw can chime in? Are the featured workspaces covered by tests elsewhere? What is the current SOP for taking workflows from this repo, turning them into featured workspaces, and populating their configurations?. Example JSONs with input test data are usually introduced in the gatk-workflows git repos and carried over to the featured workspaces. That isn't to say they are not welcomed from the gatk repo. > @bshifaw related to what Sam was saying - we also have a few standard resources needed to run the workflows that we would like to share with users. What is the standard procedure for doing so? Ideally they would be bundled with featured workspaces, but also accessible from outside of Terra. Workflow resources files that are not already in [broad-references](https://console.cloud.google.com/storage/browser/broad-references) would be saved in the [gatk-best-practices](https://console.cloud.google.com/storage/browser/gatk-best-practices) bucket. In the past i've separated the resources files per workflow directory (e.g. pathseq, cnn-hg38) but you can organize them a different way if the resources files would be shared by other workflows (e.g. somatic-hg38, somatic-b37).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6017#issuecomment-507703719:278,config,configurations,278,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6017#issuecomment-507703719,1,['config'],['configurations']
Modifiability,"Sorry, just saw this now. We still don't have a simple solution for training models without pysam. We can probably do something similar to what we do with inference, but I think the current priority is to improve inference throughput so it will probably be a little while before we get to re-writing the training code. If people feel we should re-prioritize please let me know.; I have installed the conda environment on the same OSX version, without seeing this issue.; Which gcc version are you using @mwalker174 ? ; My `gcc -v` output is:; ```; Configured with: --prefix=/Applications/Xcode.app/Contents/Developer/usr --with-gxx-include-dir=/usr/include/c++/4.2.1; Apple LLVM version 8.0.0 (clang-800.0.42.1); Target: x86_64-apple-darwin15.6.0; Thread model: posix; InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4742#issuecomment-391014193:548,Config,Configured,548,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4742#issuecomment-391014193,1,['Config'],['Configured']
Modifiability,"Sounds like a good idea, thanks! I will use a copy of the current plugin in my project til all these changes of the plugin descriptor are in (both Barclay and my proposals here).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2362#issuecomment-275707975:66,plugin,plugin,66,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2362#issuecomment-275707975,2,['plugin'],['plugin']
Modifiability,Stack trace from failed job:. ```; Caused by: org.broadinstitute.hellbender.exceptions.GATKException: Null value when trying to read system resource. Cannot find: org/broadinstitute/hellbender/tools/copynumber/utils/annotatedinterval/annotated_region_default.config; 	at org.broadinstitute.hellbender.utils.io.Resource.getResourceContentsAsFile(Resource.java:90); 	at org.broadinstitute.hellbender.utils.codecs.AnnotatedIntervalCodec.<init>(AnnotatedIntervalCodec.java:55); 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423); 	at java.lang.Class.newInstance(Class.java:442); 	at org.broadinstitute.hellbender.engine.FeatureManager.getCandidateCodecsForFile(FeatureManager.java:511); 	at org.broadinstitute.hellbender.engine.FeatureManager.getCodecForFile(FeatureManager.java:464); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getCodecForFeatureInput(FeatureDataSource.java:324); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getFeatureReader(FeatureDataSource.java:304); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:256); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:230); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:214); 	at org.broadinstitute.hellbender.utils.spark.JoinReadsWithVariants.openFeatureSource(JoinReadsWithVariants.java:63); 	at org.broadinstitute.hellbender.utils.spark.JoinReadsWithVariants.lambda$null$0(JoinReadsWithVariants.java:44); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 	at java.util.stream.Abstra,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5979#issuecomment-498620174:259,config,config,259,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5979#issuecomment-498620174,1,['config'],['config']
Modifiability,"Subsampling seems to be the way to go, see #2858. For the record, I did try to implement caching, but this results in excessive cache checking. In general, I think a better solution is to structure code so that expensive global quantities are not unnecessarily recomputed locally. At some point, this sort of undesirable recomputation snuck in during a refactoring of the allele-fraction likelihood code, probably when we tried to make the method for computing site likelihoods pull double duty based on the presence or absence of an allelic PoN. With an allelic PoN, we need to compute a log gamma at each site based on the site-specific bias hyperparameters; without a PoN, we only need to do this once for all sites, since the bias hyperparameters are now global, but the code naively recomputes it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2860#issuecomment-335621709:353,refactor,refactoring,353,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2860#issuecomment-335621709,1,['refactor'],['refactoring']
Modifiability,"Substituting `STANDARD_CONFIDENCE_FOR_CALLING/3` for `STANDARD_CONFIDENCE_FOR_EMITTING` seems wrong, given that `STANDARD_CONFIDENCE_FOR_CALLING` is a user-configurable value. We talked to @vdauwera just now and she agrees -- we're going to close this PR here and open a ticket against GATK3 to fix this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2194#issuecomment-259794842:156,config,configurable,156,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2194#issuecomment-259794842,1,['config'],['configurable']
Modifiability,"SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1199); at org.testng.TestNG.runSuitesLocally(TestNG.java:1124); at org.testng.TestNG.run(TestNG.java:1032); at org.testng.remote.RemoteTestNG.run(RemoteTestNG.java:111); at org.testng.remote.RemoteTestNG.initAndRun(RemoteTestNG.java:204); at org.testng.remote.RemoteTestNG.main(RemoteTestNG.java:175); at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:125); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at com.intellij.rt.execution.application.AppMain.main(AppMain.java:140); Caused by: com.google.cloud.dataflow.sdk.coders.CannotProvideCoderException: Cannot provide a Coder for type variable B because the actual type is unknown due to erasure.; at com.google.cloud.dataflow.sdk.coders.CoderRegistry.getDefaultCoder(CoderRegistry.java:545); at com.google.cloud.dataflow.sdk.coders.CoderRegistry.getDefaultCoder(CoderRegistry.java:516); at com.google.cloud.dataflow.sdk.coders.CoderRegistry.getDefaultCoder(CoderRegistry.java:165); at com.google.cloud.dataflow.sdk.transforms.ParDo$Bound.getDefaultOutputCoder(ParDo.java:741); at com.google.cloud.dataflow.sdk.transforms.ParDo$Bound.getDefaultOutputCoder(ParDo.java:660); at com.google.cloud.dataflow.sdk.transforms.PTransform.getDefaultOutputCoder(PTransform.java:334); at com.google.cloud.dataflow.sdk.values.TypedPValue.inferCoderOrFail(TypedPValue.java:152); at com.google.cloud.dataflow.sdk.values.TypedPValue.getCoder(TypedPValue.java:46); ... 46 more. ===============================================; Custom suite; Total tests run: 1, Failures: 1, Skips: 0; ===============================================; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/658#issuecomment-122314248:5698,variab,variable,5698,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/658#issuecomment-122314248,1,['variab'],['variable']
Modifiability,"Sure thing. I can find out which changes I needed to make in gatk to get certain tools to work, like `PrintReads` and `MarkDuplicates`, though they were certainly not exhaustive. We will also see about open sourcing our s3 nio library which is basically a rewrite of https://github.com/Upplication/Amazon-S3-FileSystem-NIO2 with changes for handling s3 endpoints.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3102#issuecomment-319206378:256,rewrite,rewrite,256,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3102#issuecomment-319206378,1,['rewrite'],['rewrite']
Modifiability,"THe only issue with running PRs on Jenkins is that I would prefer if an ""admin"" (via the comment section -- can be any Broad employee) actually activate the test. Why? Because essentially this enables someone to run arbitrary code on OUR environment. While it's fairly locked down, we want some sort of verification that, at first glance, the code doesn't look like it's going to hack us. Jenkins itself is full of holes that can probably be exploited from a well-crafted job. I'd prefer to mitigate that. Will you or any Broad person be able to type, ""Test Please"" in the comments of a PR to kick off a PR? Otherwise I'll be doing this on Travis which has better sandboxing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1758#issuecomment-287538625:664,sandbox,sandboxing,664,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1758#issuecomment-287538625,1,['sandbox'],['sandboxing']
Modifiability,TODO: refactor duplicated VC generation code from PosteriorProbabilitiesUtilsUnitTest in ReblockGVCFUnitTest by extracting to VariantContextTestUtils,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4947#issuecomment-403224932:6,refactor,refactor,6,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4947#issuecomment-403224932,1,['refactor'],['refactor']
Modifiability,"Testing the tool behavior when given an incomplete PED file. PED; <img width=""847"" alt=""screenshot 2019-01-22 16 42 07"" src=""https://user-images.githubusercontent.com/11543866/51567234-b42e3200-1e64-11e9-942c-2934980dc04a.png"">. Command; ```; gatk CalculateGenotypePosteriors \; -V precomputed/trioGGVCF.vcf.gz \; -ped duo.ped \; --skip-population-priors \; -O sandbox/duoCGP.vcf.gz; ```. Results; <img width=""842"" alt=""screenshot 2019-01-22 16 44 01"" src=""https://user-images.githubusercontent.com/11543866/51567337-ef306580-1e64-11e9-8ca6-051ccb3fa18d.png"">. The line of interest reads:; ```; 16:27:00.401 INFO CalculateGenotypePosteriors - No PED file passed or no *non-skipped* trios found in PED file. Skipping family priors.; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5409#issuecomment-456575220:361,sandbox,sandbox,361,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5409#issuecomment-456575220,1,['sandbox'],['sandbox']
Modifiability,"Thank you @kshakir. What I see there is that the code sets the default NIO option, and as part of this is creates a google cloud `StorageOptions` object. Sadly for us, when this object is created it determines which Google credentials to use, and if nothing was specified by the user it will send some network messages to try to figure out whether it's running on a Google Compute Engine machine. When we wrote the default-setting code we didn't realize that setting the number of retries was going to cause a network message to be sent, with the associated potential retries and delays. We can't change the way Google Compute Engine works, or how the Google authentication works either. Ideally we'd want some way to only search for credentials when we know NIO is going to be used. The point of these defaults is that they're used for anything that uses NIO, including third-party library code. We can't fully replicate this behavior in a different way from the outside. So I think the ""correct"" fix would be to go deep inside the Google NIO library and change it so that instead of providing a default configuration (that the user would have to put together, causing the problem you've seen), we can provide a *callback* that sets the configuration when the Google Cloud NIO provider is loaded. This is harder for future developers to wrap their heads around, but at least it would prevent this delay if NIO is not used. I'd like to think about this some more before doing something quite this drastic, though.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3491#issuecomment-443837504:1105,config,configuration,1105,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3491#issuecomment-443837504,2,['config'],['configuration']
Modifiability,"Thank you @mwalker174 for the suggestions. I ended up writing for loops to test which configurations work. Driver memory: 2-50g; executor memory: 2-50g; executor cores: 1-20; bamPartitionSize: 1-64m. Some combinations failed in minutes, some failed in hours, and some finished without errors. Bellow are three of which work for a ~33X WGS data:; ```; ../gatk-4.beta.1/gatk-launch BwaAndMarkDuplicatesPipelineSpark ; --bamPartitionSize 4000000 ; -I hdfs://bigdata/user/myname/gatk4test/wgs.sub4.unaligned.bam ; -O hdfs://bigdata/user/myname/gatk4test/wgs.sub4.BwaAndMarkDuplicatesPipelineSpark_out.bam ; -R hdfs://bigdata/user/myname/genomes/Hsapiens/GRCh37/seq/GRCh37.2bit ; --bwamemIndexImage /hadoop/myname/GRCh37.fa.img ; --disableSequenceDictionaryValidation ; -- --sparkRunner SPARK ; --sparkMaster spark://ln16:7077 ; --conf spark.cores.max=600 ; --executor-cores 20 ; --executor-memory 10g ; --conf spark.driver.memory=50g. ../gatk-4.beta.1/gatk-launch BwaAndMarkDuplicatesPipelineSpark ; --bamPartitionSize 4000000 ; -I hdfs://bigdata/user/myname/gatk4test/wgs.sub4.unaligned.bam ; -O hdfs://bigdata/user/myname/gatk4test/wgs.sub4.BwaAndMarkDuplicatesPipelineSpark_out.bam ; -R hdfs://bigdata/user/myname/genomes/Hsapiens/GRCh37/seq/GRCh37.2bit ; --bwamemIndexImage /hadoop/myname/GRCh37.fa.img ; --disableSequenceDictionaryValidation ; -- --sparkRunner SPARK ; --sparkMaster spark://ln16:7077 ; --conf spark.cores.max=600 ; --executor-cores 5 ; --executor-memory 50g ; --conf spark.driver.memory=50g. ../gatk-4.beta.1/gatk-launch BwaAndMarkDuplicatesPipelineSpark ; --bamPartitionSize 64000000 ; -I hdfs://bigdata/user/myname/gatk4test/wgs.sub4.unaligned.bam ; -O hdfs://bigdata/user/myname/gatk4test/wgs.sub4.BwaAndMarkDuplicatesPipelineSpark_out.bam ; -R hdfs://bigdata/user/myname/genomes/Hsapiens/GRCh37/seq/GRCh37.2bit ; --bwamemIndexImage /hadoop/myname/GRCh37.fa.img ; --disableSequenceDictionaryValidation ; -- --sparkRunner SPARK ; --sparkMaster spark://ln16:7077 ; --conf spark.core",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-313981314:86,config,configurations,86,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-313981314,1,['config'],['configurations']
Modifiability,"Thank you @vruano for your diligent review. I've implemented logger classes to encapsulate the metrics classes. Unfortunately the metrics classes must remain public in order to write output using `MetricsUtils.saveMetrics()`, but at least the tools aren't using them directly. There are two logging class groups - one for Filter and one Score. For Filter, there is an interface `PSFilterLogger` that is implemented by a file-logging class `PSFilterFileLogger` and a dummy class `PSFilterEmptyLogger` that does nothing. There are analogous classes for Score, but there is no Empty logger because it's not actually necessary. This adds a lot of new classes (maybe you can think of a better way) but usage has been greatly simplified. As we discussed in person, I don't think there is a faster way to count the reads in Spark. If you wanted to count the reads as they pass through, you would have to use some kind of atomic type that would be slow. Also it may be impossible to account for cases when tasks fail and restart. @lbergelson @droazen In this PR, I wanted to use htsjdk's MetricsFile and MetricBase classes for writing metrics to a file. I notice that these classes are mostly used for picard-related things. Is this the preferred way to do things? They do force you to expose public variables and also use an upper-case naming convention. On the other hand, they are somewhat convenient.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3611#issuecomment-334308160:1292,variab,variables,1292,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3611#issuecomment-334308160,1,['variab'],['variables']
Modifiability,"Thank you. From: Adam Kiezun ; Sent: Monday, May 11, 2015 5:07 PM; To: broadinstitute/hellbender ; Cc: nenewell ; Subject: Re: [hellbender] #253 - Added ""final"" keyword to classes that are not inherited, added… (#512). Merged #512. —; Reply to this email directly or view it on GitHub.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/512#issuecomment-101100346:193,inherit,inherited,193,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/512#issuecomment-101100346,1,['inherit'],['inherited']
Modifiability,"Thanks @davidbenjamin for essentially refactoring this code three times now!. Looking forward to reviewing your latest changes, but it may have to wait until early next week. Apologies for the delay. In the meantime, I see that there is a minor rebase conflict—up to you if you want to address it now, no biggie if you want to wait until after review.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1070960072:38,refactor,refactoring,38,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1070960072,1,['refactor'],['refactoring']
Modifiability,"Thanks @droazen. Because you assigned it to me, I would like to know a couple of details on how this should be implemented in GATK4:. * GATK3 use to have a the `MisencodedBaseQualityReadTransformer` always on, with a switch for checking/fixing the qualities. If we follow this approach in GATK, the only change for this is to include the checking step every n reads and then #2160 will do the rest. Nevertheles, I think that it's quite dangerous to allow an user to disable it with the plugin (because the name suggest that it is only fixing the qualities), so I suggest to integrate in the read data source an iterator for checking every x reads if the qualities are misencoded, independently on the transformer.; * GATK3 throws an UserException for ""putatively misencoded"" qualities, using 60 as maximum base quality for throwing. I think that in the case of GATK4 could be more useful to use a warning if it is over 60 (I do not know what is the reasoning behind this value), and use `SAMUtils.MAX_PHRED_SCORE` for throwing. I'd be happy to implement this if there is a consensus about what to do here, so I'll wait for your ideas...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2082#issuecomment-288760814:486,plugin,plugin,486,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2082#issuecomment-288760814,1,['plugin'],['plugin']
Modifiability,"Thanks @lbergelson for looking into this. Users can definitely squash the image after pulling, and then push it to their private registries - that's the best workaround here, so this is likely a low-priority issue. Docker images can only be pulled by layers currently; there's no way to pull an image that has multiple layers with one HTTP request. In the [TES runner](https://github.com/microsoft/ga4gh-tes), we are also increasing the docker pull retry count to help. I'll try to update the `dockerfile` and send a PR, thank you!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8684#issuecomment-1935007776:251,layers,layers,251,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8684#issuecomment-1935007776,2,['layers'],['layers']
Modifiability,"Thanks @lbergelson! I agree that it might be good to break into more layers—could be worth talking to SV team and seeing what lessons they learned in putting together their hierarchy of images. Also, note that I pushed the install of miniconda into the base, but I did not push down the setup of the GATK conda environment itself (which takes the bulk of the time during the main-image build, as it requires lots of downloading). I think I commented elsewhere that a good strategy might be to set up the conda environment with the non-GATK python dependencies in the base, and then update the environment via a pip install of the GATK python packages in the main image. This would let us make python code changes without having to rebuild the base, but might require a bit of scripting to create a final yml for non-Docker users. I also agree that it would be nice to cut down the Travis time, might be worth taking a look at other strategies to do that—could save everyone a lot of time!. Will try to add the test you suggested sometime tomorrow.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-621487662:69,layers,layers,69,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-621487662,1,['layers'],['layers']
Modifiability,"Thanks @lbergelson. There are some issues with the current implementation of the filter plugin and barclay that is preventing me to use the latest master, but once they are solved I will try it out and if I find any problem I will report it. For me, as a solo developer, this is going to be amazing for keeping everything up-to-date without breaking compatibility before releases of my own software.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1995#issuecomment-279985534:88,plugin,plugin,88,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1995#issuecomment-279985534,1,['plugin'],['plugin']
Modifiability,"Thanks @mwalker174! I think I responded to or addressed everything. The code paths for reading TSVs all go through the abstract CNV collection classes. Those require a bit of boilerplate, but were IMO a huge improvement over the horrowshow of utility methods from the old code... Happy to discuss possible further refactoring and improvement (and there are already catch-all issues open), if needed. If we decide to stream other locatable collections, we can start to extract more of these streaming/subsetting methods to `AbstractLocatableCollection`, which would give us something like the `LocatableTableReader` you're envisioning in your edit. We've discussed using @jonn-smith's `XSVLocatableTable` machinery as well. I think the only downsides are the conventional reliance on extensions/config files for decoding, as well as the need to accommodate CNV headers. Encoding is also not handled. We also still need to represent non-Locatable TSVs, ideally with a minimal number of code paths, although that probably won't present any major refactoring issues. Also recall that we discussed moving from Files -> Paths in previous PRs, so we should instead go from Files -> FeatureDataSources where it makes sense.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6266#issuecomment-558720770:314,refactor,refactoring,314,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6266#issuecomment-558720770,3,"['config', 'refactor']","['config', 'refactoring']"
Modifiability,"Thanks @ruqianl, you may want to read through the comments at https://github.com/broadinstitute/gatk/issues/6235 and the corresponding PR https://github.com/broadinstitute/gatk/pull/6244, which both address this issue. See also the following bit of documentation added in that PR:. > Advanced users may wish to set the THEANO_FLAGS environment variable to override the GATK theano configuration. For example, by running THEANO_FLAGS=""base_compiledir=PATH/TO/BASE_COMPILEDIR"" gatk GermlineCNVCaller ..., users can specify the theano compilation directory (which is set to $HOME/.theano by default). See theano documentation at https://theano-pymc.readthedocs.io/en/latest/library/config.html. So you can specify a unique compilation directory for each of your jobs to avoid the compilelock, e.g., `THEANO_FLAGS=""base_compiledir=PATH/TO/BASE_COMPILEDIR/FOR/JOB/0"" gatk GermlineCNVCaller ...`, `THEANO_FLAGS=""base_compiledir=PATH/TO/BASE_COMPILEDIR/FOR/JOB/1"" gatk GermlineCNVCaller ...`, etc. Alternatively, you can increase `config.compile.timeout` as discussed in those comments.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7411#issuecomment-905070899:344,variab,variable,344,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7411#issuecomment-905070899,4,"['config', 'variab']","['config', 'configuration', 'variable']"
Modifiability,"Thanks for bringing this to our attention, @Tintest. I think that we may be able to address this by setting `base_compiledir` via `os.environ[""THEANO_FLAGS""]` appropriately (see http://deeplearning.net/software/theano/library/config.html). @mbabadi @cmnbroad any thoughts? . In any case, thanks for trying out the GermlineCNVCaller pipeline. You may have to tune some parameters, depending on your data type. You may find the following discussions helpful:. https://gatkforums.broadinstitute.org/gatk/discussion/11711/germlinecnvcaller-interval-merging-rule-error. https://github.com/broadinstitute/gatk/issues/4719. Note that we're still in beta, but our preliminary evaluations have demonstrated improved performance over other callers in both WES and WGS.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-390303432:226,config,config,226,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-390303432,1,['config'],['config']
Modifiability,"Thanks for bringing this up! I actually think that I prefer option 1, although not ideal (since, as you say, it places more burden on the user). The whole point of having generically parameterized models is that we can apply them to many data types. To single out a few with hardcoded sets of defaults seems like a slippery slope to me. (Of course, we should definitely provide defaults for typical data types in *documentation*.) And in the end, I think it is beneficial for users that wish to tweak knobs to do some work to understand what those knobs actually do (even if just at a basic level). The other downside of option 2 is that it might not be immediately obvious from the command line what parameters are being used. For example, if a user chooses a set of defaults but then overrides some of them, we should make it so they don't have to go digging through the logs to see what parameters are actually used in the end. Nor should they have to go back and check what the defaults were for whatever version of the jar they were using at the time. Option 2 might also make it easier to inadvertently override parameters, etc. via command-line typos or copy-and-paste errors---it's much more straightforward to require and check that every parameter is specified once and fallback to a default if not, as we do now. Not to say that we couldn't get around any of these issues in Barclay, but I think it'll require some thought and careful design. Would be interested to hear Engine team's opinions. Finally, one point that I think will become more relevant as our tools and pipelines become more flexible and parameterized: I think we should start thinking of ""Best Practices Recommendations"" less as ""here is the best set of parameters to use with your data"" and more as ""here is *how to find* the best set of parameters to use with your data (for a given truth set, sensitivity requirement, etc.)"". After all, if we are putting together pipelines to do hyperparameter optimization, there is n",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385584289:183,parameteriz,parameterized,183,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385584289,1,['parameteriz'],['parameterized']
Modifiability,"Thanks for indulging me on this. To me it seems like `UnfilledReadsLikelihoods` diverges too much from `ReadsLikelihoods` to extend it. In effect it's letting `ReadsLikelihoods` sometimes be a wrapper for something that is not a `ReadsLikelihoods`. I haven't worked this out but I would hope that it's possible to construct a `ReadsLikelihoods` from a pileup. I mean, the idea of pileup calling is that you use just a single base for the likelihoods and not the whole read (via Pair-HMM), so we should be able to fill the likelihoods from the base qualities.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4865#issuecomment-396369856:125,extend,extend,125,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4865#issuecomment-396369856,1,['extend'],['extend']
Modifiability,"Thanks for looking into this @davidbenjamin. I followed the best practices using bwa mem, mark duplicates etc., to create these input bams for HaplotypeCaller. This is Novaseq 2 x 150 data, I ran Fastqc on the reads and everything looks really good, the only thing I can find that might explain the soft-clipping is that there's some Nextera adapter read through on a small percentage of the reads. I haven't been using -Y with bwa (I see it's used in GATK 4 wdls), so it seems like there should be less soft-clipping than normal. I'll admit these are definitely messy regions we're dealing with, but we really need to make the F5 calls for our clinical pipeline. I just tried --dont-use-soft-clipped-bases and I wasn't able to pick the SNP up in the 55-55003_F5_region.bam, but using forceActive/dontTrimActiveRegions does work on this call.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3697#issuecomment-402690747:342,adapt,adapter,342,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3697#issuecomment-402690747,1,['adapt'],['adapter']
Modifiability,"Thanks for reporting this, @Stikus! That change you highlighted indeed fixes the issue. There was an oblique mention of issues with the previously specified version of pip in the comments of that PR. Note that we now use conda 23.10.0 with the libmamba solver in the GATK Docker image. Please feel free to reopen if you have issues with that specific configuration!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8618#issuecomment-1851818671:351,config,configuration,351,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8618#issuecomment-1851818671,1,['config'],['configuration']
Modifiability,"Thanks for that info and for sharing the files, @asmirnov239. I suspect that there are essentially two types of bins: ""nice"" and ""not so nice"". The sampling noise in the former is determined by Poisson observation noise, whereas that in the latter is determined by uncertainty in the bias posteriors. This is a bit hard to see in the plots above, and even in this version where I tried to adjust the point size and alpha:. ![image](https://user-images.githubusercontent.com/11076296/137733810-16a79ea9-ea7b-47cc-a42f-40130a949015.png). However, plotting a measure of the difference in the dCRs (from 20 and 200 posterior samples) vs. the dCR is more suggestive:. ![image](https://user-images.githubusercontent.com/11076296/137734587-1b9f6551-74b2-4097-a02c-f51d7341251c.png). As are the dCR histograms:. ![image](https://user-images.githubusercontent.com/11076296/137733867-ce0f5573-a5cc-412c-9060-56fbb09d1ef0.png). I would guess that the nice spike around CR ~ 2 and the fatter base extending up to dCR ~ 100 are distinct populations of bins. So the punchline would be that differences at high dCR are probably just noise within the noise. For ""nice"" bins at dCR ~ few, the sampling noise looks to be <1%. Not really sure what's going on at very high dCR, but I think it's safe to say that these are ""not so nice"" bins!. I've seen this pattern in other WES cohorts when plotting the posterior means vs. std devs for the biases; tried to dig up the plots on Slack, but I can't find them at the moment. Perhaps something along those lines might be worth visualizing in your model-criticism notebooks, if you don't already?. Again, hard to say this is indeed the case from the dCRs alone, but if so, it might be worth baking this sort of mixture into future versions of the model or coming up with other strategies to deal with such bins.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5754#issuecomment-945731946:985,extend,extending,985,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5754#issuecomment-945731946,1,['extend'],['extending']
Modifiability,"Thanks for the analysis @laserson! Not sure I fully understand it yet, but will re-read it until I do :) By tomorrow I should have plots for https://github.com/broadinstitute/gatk/issues/995 comparing JP's existing code against the broadcast approach, and will link them here -- let's decide whether or not to devote more effort to refactoring JP's code after seeing those numbers. In the meantime, recommend focusing on https://github.com/broadinstitute/gatk/issues/1015, which is probably the most urgent blocking issue for spark at the moment.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1007#issuecomment-151881484:332,refactor,refactoring,332,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1007#issuecomment-151881484,1,['refactor'],['refactoring']
Modifiability,"Thanks for the feedback, @cmnbroad.  @droazen, should I open a ticket for implement the plugin and close this issue? What's about the checking of the quals?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2084#issuecomment-245969652:88,plugin,plugin,88,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2084#issuecomment-245969652,1,['plugin'],['plugin']
Modifiability,"Thanks for the review.; Regarding the code complexity, I definitely agree. I was trying to not do too much code surgery. For your suggestion regarding just outputting the total, perhaps if I name it something like total optical duplicates? The issue is that you could have two optical duplicate clusters, but I guess we don't really care, right? We just need the total?; Also, I can rewrite the test to load the information in as a map and then just make sure the sizes are the same and query the map. Would that be better?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/749#issuecomment-127370106:383,rewrite,rewrite,383,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/749#issuecomment-127370106,1,['rewrite'],['rewrite']
Modifiability,"Thanks for the suggestion, @droazen! I did this PR before the read filter plugin was included, and actually I was thinking about remove this PR because it is very clunky. Should I close this and open a discussion about the plugin?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2084#issuecomment-245960477:74,plugin,plugin,74,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2084#issuecomment-245960477,2,['plugin'],['plugin']
Modifiability,"Thanks guys!. On Sat, Sep 23, 2017 at 11:38 PM, David Benjamin <notifications@github.com>; wrote:. > *@davidbenjamin* requested changes on this pull request.; >; > Done with my review. Mainly the usual stuff about writing more idiomatic; > Java that all C++ coders go through!; > ------------------------------; >; > In src/main/java/org/broadinstitute/hellbender/tools/copynumber/; > CreateBinningIntervals.java; > <https://github.com/broadinstitute/gatk/pull/3597#discussion_r140646010>:; >; > > +import org.broadinstitute.hellbender.utils.IntervalUtils;; > +import org.broadinstitute.hellbender.utils.SimpleInterval;; > +; > +import java.io.File;; > +import java.util.List;; > +; > +; > +; > +@CommandLineProgramProperties(; > + summary = ""Split intervals into sub-interval files."",; > + oneLineSummary = ""Split intervals into sub-interval files."",; > + programGroup = VariantProgramGroup.class; > +); > +@DocumentedFeature; > +public class CreateBinningIntervals extends GATKTool {; > + public static final String WIDTH_OF_BINS_SHORT_NAME = ""bw"";; >; > @samuelklee <https://github.com/samuelklee> is the boss of the copy; > number code, but personally I don't see the need to be extremely concise; > with short names and would prefer width.; > ------------------------------; >; > In src/main/java/org/broadinstitute/hellbender/tools/copynumber/; > CreateBinningIntervals.java; > <https://github.com/broadinstitute/gatk/pull/3597#discussion_r140646054>:; >; > > +@DocumentedFeature; > +public class CreateBinningIntervals extends GATKTool {; > + public static final String WIDTH_OF_BINS_SHORT_NAME = ""bw"";; > + public static final String WIDTH_OF_BINS_LONG_NAME = ""binwidths"";; > +; > + public static final String PADDING_SHORT_NAME = ""pad"";; > + public static final String PADDING_LONG_NAME = ""padding"";; > +; > + @Argument(; > + doc = ""width of the bins"",; > + fullName = WIDTH_OF_BINS_LONG_NAME,; > + shortName = WIDTH_OF_BINS_SHORT_NAME,; > + optional = true,; > + minValue = 1; > + ); > + pri",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3597#issuecomment-331744211:967,extend,extends,967,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3597#issuecomment-331744211,1,['extend'],['extends']
Modifiability,"Thanks. This shows that bcast can go faster when given more RAM: going from; the earlier experimental envelope of 192GB of RAM to 256GB instead, the; bcast version can go from 19 min to 10min (sharded takes 7min with the; small amount of memory). This is consistent with my earlier experiments; that also showed that increasing the RAM-per-core sped up the bcast; computation. I wouldn't worry about having found a configuration where sharded performs; poorly. My experiments so far have shown that there are lots of; configuration points where one or the other performs poorly; the onus is on; finding the good configuration values that result in better performance, or; identifying which algorithms may be performant for a larger range of; parameter choices. To maximize understanding I would suggest that we not change too many; parameters at a time. For example it seems unnecessary to have multiple; variants of the 128Mbp input. I found there can be significant (>50%) variation between identical runs.; One way to reduce this is to set spark.task.maxFailures=1 which I strongly; recommend for all experiments going forward. One concern I have is that our cluster memory is already far larger than; the input size. In fact here each machine can fit the whole input; comfortably in RAM. This is not going to be the case for the full input.; Since it would take too long to iterate using the full input, it seems wise; instead to reduce both the input size and to keep a close eye on the amount; of memory we're using to make sure we're not going down a path that would; not be able to cope with the full input. On Wed, Nov 18, 2015 at 11:08 AM, droazen notifications@github.com wrote:. > I did some additional runs on the Broad cluster on a 14 GB bam, twice as; > large as the bam used in the plot above. This was with 60 cores, 4 cores; > per executor, and 16 GB of memory per executor. Results:; > ; > Broadcast (3 runs): 10m52.020s, 11m46.975s, 10m17.274s; > Sharded (3 runs): 19m33.310s, 13m3",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/995#issuecomment-157838734:415,config,configuration,415,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/995#issuecomment-157838734,3,['config'],['configuration']
Modifiability,That code checks whether DP is 0 or the maximum PL value is 0. If any of the conditions is satisfied then plugin will assign nocall ./. to that site.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8328#issuecomment-2117184307:106,plugin,plugin,106,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8328#issuecomment-2117184307,1,['plugin'],['plugin']
Modifiability,"That should work for both my cases. It could be nice for SelectVariants to; be able to specify whether genotypes should be called or not too. Other; tools might want the sites-only option. On Mon, Mar 4, 2019 at 12:40 PM droazen <notifications@github.com> wrote:. > *@droazen* commented on this pull request.; > ------------------------------; >; > In; > src/main/java/org/broadinstitute/hellbender/tools/genomicsdb/GenomicsDBUtils.java; > <https://github.com/broadinstitute/gatk/pull/4947#discussion_r262167602>:; >; > > @@ -40,7 +40,7 @@; > */; > public static GenomicsDBExportConfiguration.ExportConfiguration createExportConfiguration(final File reference, final String workspace,; > final String callsetJson, final String vidmapJson,; > - final String vcfHeader) {; > + final String vcfHeader, final boolean doGnarlyGenotyping) {; >; > @lbergelson <https://github.com/lbergelson> @ldgauthier; > <https://github.com/ldgauthier> If tools had a way to inject custom GDB; > config (eg., via an overridable method in GATKTool), and the engine used; > this config when creating the Feature Manager on startup, would that solve; > the problem here?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/pull/4947#discussion_r262167602>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AGRhdOOjGpZBu39mqk7jekA7iOzWDTFrks5vTVqFgaJpZM4U4KK0>; > .; >. -- ; Laura Doyle Gauthier, Ph.D.; Associate Director, Germline Methods; Data Sciences Platform; gauthier@broadinstitute.org; Broad Institute of MIT & Harvard; 320 Charles St.; Cambridge MA 0214",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4947#issuecomment-469412816:975,config,config,975,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4947#issuecomment-469412816,2,['config'],['config']
Modifiability,"That sounds like a good thing to look at. If someone has already written it, it would be great to not have to rewrite it..",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4086#issuecomment-356366388:110,rewrite,rewrite,110,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4086#issuecomment-356366388,1,['rewrite'],['rewrite']
Modifiability,"That's a good sign. However the code above only checks the providers at the client. It'd be interesting to also check what happens at the workers. I wrote ExampleNioCheckFS for this purpose earlier, you can use it. It's only in a test branch of mine (since it's just test code) but it's pretty short. Looks like this:. ````java; /**; * Example of how to use Spark on Google Cloud Storage directly, without using the GCS Hadoop Connector.; */; @CommandLineProgramProperties(; summary = ""Example of how to use Spark on Google Cloud Storage directly, without using the GCS Hadoop Connector"",; oneLineSummary = ""Example of how to use Spark on Google Cloud Storage directly, without using the GCS Hadoop Connector"",; programGroup = ReadProgramGroup.class; ); public class ExampleNioCheckFS extends SparkCommandLineProgram {; private static final long serialVersionUID = 1L;. @Argument(fullName = StandardArgumentDefinitions.OUTPUT_LONG_NAME, shortName = StandardArgumentDefinitions.OUTPUT_SHORT_NAME, doc = ""Output file (if not provided, defaults to STDOUT)"", common = false, optional = true); private File OUTPUT_FILE = null;. @Argument(fullName = ""inputPath"", shortName = ""P"", doc = ""Input path (eg. gs://foo/bar.bam)"", optional = false); private String path = null;. // Typically set to number of executors times number of cores per executor.; @Argument(fullName = ""parts"", doc = ""number of partitions"", optional = false); private int parts = 3;. private void countReads(JavaSparkContext ctx) {; PrintStream outputStream;. try {; outputStream = OUTPUT_FILE != null ? new PrintStream(OUTPUT_FILE) : System.out;; }; catch ( FileNotFoundException e ) {; throw new UserException.CouldNotReadInputFile(OUTPUT_FILE, e);; }. NioBam input = new NioBam(path, path + "".bai"");; List<String> ret = input.getReads(ctx, parts).mapPartitions(ExampleNioCheckFS::getFS).collect();; outputStream.println(""**** Results **** : "" + String.join("", "", ret));; }. private static Iterator<String> getFS(Iterator<SAMRecord> rs) {",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2312#issuecomment-267424466:785,extend,extends,785,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2312#issuecomment-267424466,1,['extend'],['extends']
Modifiability,"That's what we do for the spark jar, easy to extend it to work for a non-spark jar too. What I mean though, is when someone downloads a packaged gatk, what do they get/ how do they install it?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1693#issuecomment-207474072:45,extend,extend,45,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1693#issuecomment-207474072,1,['extend'],['extend']
Modifiability,"That's why I am not using in ReadTools and other developmental toolkit the base class from GATK, due to the polluted command line with unused arguments. I think that for give flexibility, some of that arguments should be configurable by extending classes. For example, some tools that does not require reads at all should be able to turn off the read arguments. That will be very useful, although I am not sure how to do it in a proper way without adding more and more interfaces for argument collections. In context case of this PR, I think that adding it does not have any real effect on the GATK codebase, and a lot is gained by downstream projects. For example, if the wrapper script adds another argument that should be parsed in `Main` and documented, the GATK team just add it to its class. If a toolkit has a similar wrapper script, it can also add its own only-doc argument by simply overriding the method...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4474#issuecomment-371822090:221,config,configurable,221,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4474#issuecomment-371822090,2,"['config', 'extend']","['configurable', 'extending']"
Modifiability,The Engine Team discussed this internally and we're going to pull out a subset of all the configuration options into the config file. These options should be those that will change only infrequently (like the data sources directory).,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4960#issuecomment-461937685:90,config,configuration,90,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4960#issuecomment-461937685,2,['config'],"['config', 'configuration']"
Modifiability,The actual code for the funcotation factories is all set up for this. The required update is that `GencodeFuncotationFactory` needs to be refactored to take in the name of the data source. Right now it's assumed that it can only be `Gencode`.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3956#issuecomment-378314286:138,refactor,refactored,138,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3956#issuecomment-378314286,1,['refactor'],['refactored']
Modifiability,"The behavior of the GATK3 CombineVariants was very inconsistent and the arguments weren't entirely clear. I also suspect that some operations weren't possible with the arguments given. Rather than port that old broken version, I would advocate for an overhaul or rewrite. @bhanugandham it's going to be a big project to collect requirements and expected behavior for this tool. For example, what should the MQ be for the combined VCF for two different input VCFs with different MQ values? Much of the confusion stemmed from the old ability to merge VCFs containing the same sample. In the case where we take one genotype for each sample name (e.g. the old ` -genotypeMergeOptions PRIORITIZE`) then I believe the old behavior was wrong in some cases, taking the filter status from an input VCF at random. We also need to clarify `FilteredRecordMergeType` options, e.g. https://github.com/broadinstitute/gsa-unstable/issues/935",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/17#issuecomment-430229167:263,rewrite,rewrite,263,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/17#issuecomment-430229167,1,['rewrite'],['rewrite']
Modifiability,"The code difference between OSX and Linux is pretty small. . Here's the change in the native code. ; - Linux will define the `threads` variable and use it in the OpenMP pragma.; - OSX will not see the `threads` declaration and the OpenMP pragma will be a comment. ``` C; #ifdef linux; int threads = min((int)maxNumThreadsToUse, omp_get_max_threads());; #endif; #pragma omp parallel for schedule(dynamic, 1) num_threads(threads); ```. We also need a minor change in `build.gradle`. I'll submit the changes to this PR and you can decide from there.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1813#issuecomment-218857915:135,variab,variable,135,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1813#issuecomment-218857915,1,['variab'],['variable']
Modifiability,"The code is hard to read and I don't know how to get that info from IntelliJ, but I think we need all of the subclasses in walkers/na12878kb/ including all in assess/ and probably most in core/:. gsa-unstable/private/gatk-tools-private/src/main/java/org/broadinstitute/gatk/tools/walkers/na12878kb/:; ExportReviews.java; ExtractConsensusSites.java; ImportCallset.java; ImportReviews.java; NA12878DBWalker.java; NA12878KnowledgeBaseServer.java; SummarizeConsensus.java; UpdateConsensus.java. gsa-unstable/private/gatk-tools-private/src/main/java/org/broadinstitute/gatk/tools/walkers/na12878kb/assess:; AllSitesWriter.java; AssessNA12878.java; Assessment.java; AssessmentType.java; Assessor.java; BadSitesWriter.java; ROCCurveNA12878.java (a different walker, but something else we use often); SitesWriter.java. gsa-unstable/private/gatk-tools-private/src/main/java/org/broadinstitute/gatk/tools/walkers/na12878kb/core:; CallSet.java; ConsensusMaker.java; ConsensusSummarizer.java; MongoDBManager.java; MongoGenotype.java; MongoVariantContext.java; NA12878DBArgumentCollection.java; NA12878KBMain.java; NA12878KnowledgeBase.java; *NewlyAddedSites.java; *OneChunkIterator.java; PolymorphicStatus.java; *RawSiteIterator.java; SiteIterator.java; SiteManager.java; SiteSelector.java; TruthStatus.java; - I haven't come across these classes before, so I'm not sure if they're essential. (I haven't explored this code, but it's probably necessary); gsa-unstable/private/gatk-tools-private/src/main/java/org/broadinstitute/gatk/tools/walkers/na12878kb/core/errors:; InvalidRecordHandler.java; InvalidRecordsLogError.java; InvalidRecordsRemove.java; InvalidRecordsThrowError.java; MongoVariantContextException.java",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/55#issuecomment-94767432:1176,Polymorphi,PolymorphicStatus,1176,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/55#issuecomment-94767432,1,['Polymorphi'],['PolymorphicStatus']
Modifiability,The concise message is:. ```; cb2@cb2-VirtualBox:~/gatk$ ./gradlew bundle; > Configure project :; Executing: git lfs pull --include src/main/resources/large. > Task :condaStandardEnvironmentDefinition; Created standard Conda environment yml file: gatkcondaenv.yml. > Task :pythonPackageArchive; Created GATK Python package archive in /home/cb2/gatk/build/gatkPythonPackageArchive.zip. > Task :gatkDoc FAILED; Unable to find the 'javadoc' executable. Tried the java home: /usr/lib/jvm/java-11-openjdk-amd64 and the PATH. We will assume the executable can be ran in the current working folder. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':gatkDoc'.; > Javadoc generation failed. Generated Javadoc options file (useful for troubleshooting): '/home/cb2/gatk/build/tmp/gatkDoc/javadoc.options'. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. Run with --scan to get full insights. * Get more help at https://help.gradle.org; ```. And stacktrace flag output looks like:. ```; `cb2@cb2-VirtualBox:~/gatk$ ./gradlew bundle --stacktrace; > Task :gatkDoc FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':gatkDoc'.; > Javadoc generation failed. Generated Javadoc options file (useful for troubleshooting): '/home/cb2/gatk/build/tmp/gatkDoc/javadoc.options'. * Try:; Run with --info or --debug option to get more log output. Run with --scan to get full insights. * Exception is:; org.gradle.api.tasks.TaskExecutionException: Execution failed for task ':gatkDoc'.; at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter$3.accept(ExecuteActionsTaskExecuter.java:166); at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter$3.accept(ExecuteActionsTaskExecuter.java:163); at org.gradle.internal.Try$Failure.ifSuccessfulOrElse(Try.java:191); at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(Execu,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4155#issuecomment-566796716:77,Config,Configure,77,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4155#issuecomment-566796716,1,['Config'],['Configure']
Modifiability,"The fact that it's circular means that there are reads and bases right up against the edge of the contig and often have soft clips that extend beyond the contig. So it looks a bit like this:. <img width=""945"" alt=""screen shot 2018-07-19 at 11 37 45 am"" src=""https://user-images.githubusercontent.com/13020550/42953344-3ba4f544-8b48-11e8-9250-db75e56c2069.png"">. So yes, circular means it's probably throwing in a weird edge case.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5036#issuecomment-406322137:136,extend,extend,136,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5036#issuecomment-406322137,1,['extend'],['extend']
Modifiability,"The file size only went from 1.9MB to 3MB -- there's no perceptible; difference in test runtime that I can see. On Wed, Apr 5, 2017 at 2:04 PM, droazen <notifications@github.com> wrote:. > *@droazen* commented on this pull request.; > ------------------------------; >; > In src/test/java/org/broadinstitute/hellbender/tools/spark/; > ParallelCopyGCSDirectoryIntoHDFSSparkIntegrationTest.java; > <https://github.com/broadinstitute/gatk/pull/2540#discussion_r109987028>:; >; > > +; > +; > +public class ParallelCopyGCSDirectoryIntoHDFSSparkIntegrationTest extends CommandLineProgramTest {; > +; > + @Override; > + public String getTestedToolName() {; > + return ParallelCopyGCSDirectoryIntoHDFSSpark.class.getSimpleName();; > + }; > +; > + @Test(groups = {""spark"", ""bucket""}); > + public void testCopyFile() throws Exception {; > + MiniDFSCluster cluster = null;; > + try {; > + final Configuration conf = new Configuration();; > + // set the minicluster to have a very low block size so that we can test transfering a file in chunks without actually needing to move a big file; > + conf.set(""dfs.blocksize"", ""1048576"");; >; > Instead of switching to a larger file, is it possible to just decrease the; > block size further? (thinking about test runtimes here); >; > —; > You are receiving this because you were assigned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/pull/2540#discussion_r109987028>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AArTZYV_vD1XwS5IPvZiNZKOe6QzDJDVks5rs9e2gaJpZM4MtGXX>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2540#issuecomment-291948506:555,extend,extends,555,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2540#issuecomment-291948506,3,"['Config', 'extend']","['Configuration', 'extends']"
Modifiability,"The htsjdk and Hadoop-BAM parts of this are done (some refactoring will be needed for ReadSparkSink). The gating factor is getting Hadoop-BAM to move to Java 8 (this will be a topic for the call with Hadoop-BAM scheduled for 1/21) and upgrading to a newer htsjdk with ref-factored CRAM support. It would possible to implement this without that upgrade, but the it would require each partition to write a complete CRAM file, which would make the merging at the end much less efficient.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1270#issuecomment-171421991:55,refactor,refactoring,55,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1270#issuecomment-171421991,1,['refactor'],['refactoring']
Modifiability,The idea behind this branch: make the output to readsSparkSort consistent and configurable. So that if a tool alters reads without changing their sort order then no sort will be performed by default. It also means that if you request sharded output there is the ability to ask reasSparkSource to sort the file for you.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4874#issuecomment-416339183:78,config,configurable,78,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4874#issuecomment-416339183,1,['config'],['configurable']
Modifiability,The main issue with this task was that the query results were being limited to 100 by default. So we use the -n param now in the query. Another issue was that we were running bq show on a table variable $TABLE which is never defined.; I also changed this because the approach (returning all the samples names of the samples that have been loaded) didn't seem scalable. I wanted to only return at most the number of samples we are trying to ingest.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7470#issuecomment-921024230:194,variab,variable,194,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7470#issuecomment-921024230,1,['variab'],['variable']
Modifiability,"The master branch failed on BaseRealibratorSpark when running WGS. Try to test this branch, but got hit by a strange error message. The jar file looks right to me. @tomwhite did you have some environment variables? . ````Using GATK jar /home/genomics/Projects/TomWhitePatches/gatk/build/libs/gatk-package-4.alpha.2-230-g19db939-SNAPSHOT-spark.jar; Running:; /home/genomics/Projects/spark/bin/spark-submit --master spark://n001:7077 --conf spark.kryoserializer.buffer.max=512m --conf spark.driver.maxResultSize=0 --conf spark.driver.userClassPathFirst=true --conf spark.io.compression.codec=lzf --conf spark.yarn.executor.memoryOverhead=600 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true --executor-memory 25G --driver-memory 5G /home/genomics/Projects/TomWhitePatches/gatk/build/libs/gatk-package-4.alpha.2-230-g19db939-SNAPSHOT-spark.jar BaseRecalibratorSpark -I hdfs://n001:54310/GATK4TEST/LargeBroadData/WGS-G94982-NA12878.bam -knownSites hdfs://n001:54310/GATK4TEST/DBSNP/dbsnp_138.hg19.vcf.gz -R hdfs://n001:54310/GATK4TEST/OldData/human_g1k_v37.2bit -O hdfs://n001:54310/GATK4TEST/LargeOutput/WGS_BQSR --sparkMaster spark://n001:7077; Picked up JAVA_TOOL_OPTIONS: -XX:+UseG1GC -XX:ParallelGCThreads=4; Picked up JAVA_TOOL_OPTIONS: -XX:+UseG1GC -XX:ParallelGCThreads=4; java.lang.ClassNotFoundException: org.broadinstitute.hellbender.Main; at java.lang.ClassLoader.findClass(ClassLoader.java:530); at org.apache.spark.util.ParentClassLoader.findClass(ParentClassLoader.scala:26); at java.lang.ClassLoader.loadClass(ClassLoader.ja",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2620#issuecomment-299259877:204,variab,variables,204,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2620#issuecomment-299259877,1,['variab'],['variables']
Modifiability,"The naming of the different classes (`AlignmentRegion`, `AssembledBreakpoint`, `BreakpointAllele`) was very confusing especially with the id variable `breakpointId` in several classes. I've renamed `AssembledBreakpoint` to `BreakpointAlignment`, since I think that the main thing it's trying to capture is a split alignment that's indicating there might be a breakpoint at a given location. Then, the actual breakpoint is represented by `BreakpointAllele` which the information about the breakpoint junction, but not the alignment-related fields. Does that make more sense?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2079#issuecomment-240475247:141,variab,variable,141,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2079#issuecomment-240475247,1,['variab'],['variable']
Modifiability,"The problem is that htsjdk supports reading multiple versions of vcf, but only knows how to write the current version. Traditionally this has worked because older vcf versions could be trivially written out as a newer version. But v4.3 restricts some values to a narrower range than previous versions, so its not always possible to write out a pre-v4.3 version as a v4.3 compliant file in a non-destructive way. Hence the need to refactor to better support full versioning.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2602#issuecomment-472037659:430,refactor,refactor,430,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2602#issuecomment-472037659,1,['refactor'],['refactor']
Modifiability,The problem looks like it's due to the Hadoop version. Hadoop 2.7.0 or later is required for https://issues.apache.org/jira/browse/HDFS-3689. We rely on this change to concatenate the VCF parts together (which are variable lengths).,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6218#issuecomment-546884621:214,variab,variable,214,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6218#issuecomment-546884621,1,['variab'],['variable']
Modifiability,The problem seems to be fixed in picard with @cmnbroad's change to the cloud configuration. Thew pom for 2.18.1+ looks like it won't include nio.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4556#issuecomment-375432298:77,config,configuration,77,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4556#issuecomment-375432298,1,['config'],['configuration']
Modifiability,"The problem seems to be that some of the ""deep"" filters inherited from ReadWalker require to look into the read-base/qualities (just to compare their lenghts) which seems to be quite costly to decode and some of the read-attributes when this particular tool does not need to look into those fields. . It seems to me that we could override these check with a more lightweight alternative. . <img width=""1009"" alt=""screen shot 2018-09-27 at 3 03 36 pm"" src=""https://user-images.githubusercontent.com/791104/46168553-d32a0800-c266-11e8-96fd-d60d7c0380d5.png"">",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5233#issuecomment-425207954:56,inherit,inherited,56,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5233#issuecomment-425207954,1,['inherit'],['inherited']
Modifiability,"The properties needed are listed here (for testing in this case): https://github.com/broadinstitute/gatk/blob/master/src/main/java/org/broadinstitute/hellbender/engine/spark/SparkContextFactory.java#L83-L86. They are; * `fs.gs.impl`; * `fs.AbstractFileSystem.gs.impl`; * `fs.gs.project.id`; * `google.cloud.auth.service.account.json.keyfile`. Note that to set them as Spark configuration values, they need to be prefixed with `spark.hadoop`. So from the `spark-submit` command line you would write. ```; --conf spark.hadoop.fs.gs.impl=com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5996#issuecomment-500745498:374,config,configuration,374,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5996#issuecomment-500745498,1,['config'],['configuration']
Modifiability,The rebasing is non-trivial due to the [Allele Subsetting Refactoring](https://github.com/broadinstitute/gatk/commit/df9ad9b56c9eeaabb8b39ad63731edcf7aaf3a70). I will add unit tests for the new SAC methods once this is complete.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1852#issuecomment-238874246:58,Refactor,Refactoring,58,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1852#issuecomment-238874246,1,['Refactor'],['Refactoring']
Modifiability,"The regression tests added as part of #4344 fulfill this requirement. However, they need to be refactored to take advantage of the newly-included full `hg19` and `hg38` reference sequences.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5295#issuecomment-430312265:95,refactor,refactored,95,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5295#issuecomment-430312265,1,['refactor'],['refactored']
Modifiability,"The snapshot builds get published to an artifact repository, but I don't think those are accessible from outside of Broad. The build from this morning with your branch is [here](https://broadinstitute.jfrog.io/broadinstitute/libs-snapshot-local/org/broadinstitute/gatk/4.0.11.0-30-g9c4a27b-SNAPSHOT/) if you can access it. Otherwise, for local development, you can do the following:. - pull gatk master from today so it includes your commit; - run `git fetch --tags` (this is optional but it will give your local build a more reasonable version tag); - run `./gradlew install printVersion` to install the locally built gatk into your local machine's maven repository; - change your VariantQC gradle project to include the `maven` gradle plugin if its not already there; - add `mavenLocal()` to your projects' `repositories `closure; - change your gatk dependency to the version number printed out by 'printVersion'; - rebuild VariantQC. Having said all that, what code are you dependent on ? I expect the command line interface to VariantEval, and the VariantUtils and StratificationManager and friends classes all to undergo some refactoring and evolve a bit before the tool has the beta tag removed and the interfaces are stabilized. See https://github.com/broadinstitute/gatk/issues/5439 and https://github.com/broadinstitute/gatk/issues/5440.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-440782148:737,plugin,plugin,737,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-440782148,3,"['evolve', 'plugin', 'refactor']","['evolve', 'plugin', 'refactoring']"
Modifiability,"The task here is to simply move the code while changing as little as possible, and then validate that. Once that's done, we can do whatever refactoring/changes we want to VQSR, or replace it completely.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2062#issuecomment-236014525:140,refactor,refactoring,140,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2062#issuecomment-236014525,1,['refactor'],['refactoring']
Modifiability,"The underlying issue here is is that the GATK conda env environment isn't established since bioconda doesn't appear to configure it. The NPE needs is fixed by #7816. In this particular case it appears that some of the requirements are satisfied, since the code gets past the initial check to see if the GATK python code is available. But then the actual CNN code can't be loaded.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7811#issuecomment-1110010269:119,config,configure,119,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7811#issuecomment-1110010269,1,['config'],['configure']
Modifiability,"Then we need to refactor the test to write the file itself, since HTJDK won't.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/526#issuecomment-104392000:16,refactor,refactor,16,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/526#issuecomment-104392000,1,['refactor'],['refactor']
Modifiability,"Theory from @cmnbroad is below:. ```; I think this is happening because were trying to serialize the class loader sun.misc.Launcher$AppClassLoader), which appears to be reached through the graph by way of via https://github.com/damiencarol/jsr203-hadoop/blob/master/src/main/java/hdfs/jsr203/HadoopFileSystem.java#L82. We probably need to short circuit that with a custom serializer for one of these:. Serialization trace:; classes (sun.misc.Launcher$AppClassLoader); classLoader (org.apache.hadoop.conf.Configuration); conf (org.apache.hadoop.hdfs.DistributedFileSystem); fs (hdfs.jsr203.HadoopFileSystem); hdfs (hdfs.jsr203.HadoopPath); path (htsjdk.samtools.seekablestream.SeekablePathStream); seekableStream (htsjdk.tribble.TribbleIndexedFeatureReader); featureReader (org.broadinstitute.hellbender.engine.FeatureDataSource); featureSources (org.broadinstitute.hellbender.engine.FeatureManager). See, for instance, dbpedia/distributed-extraction-framework#9.; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6730#issuecomment-671508579:504,Config,Configuration,504,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6730#issuecomment-671508579,1,['Config'],['Configuration']
Modifiability,"There are currently a few known issues that reviewers should bear in mind (these will become tickets shortly):. -All codecs marked as implementing `ReferenceDependentFeatureCodec` are currently non-functional. I need to either refactor them to not require a GenomeLocParser or delete them entirely. -The `IndexFeatureFile` tool is not currently working on block-compressed files -- will fix this soon. -`IndexFeatureFile` needs integration tests (will work on this during code review). -`BQSR` needs integration tests for the case of multiple simultaneous known sites files (now that it supports them!). -All codecs should move to tribble, and must implement `canDecode()` correctly (this is a new requirement, since it's no longer possible to manually request a particular codec). Most `canDecode()` implementations can be file-extension-based; only things like VCF format detection need to examine file contents to determine file type. -`FeatureDataSource` supports querying by interval, as well as full traversals, but not full traversal by a set of intervals (yet). This latter feature will be needed for the `VariantWalker` traversal.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/224#issuecomment-75658867:227,refactor,refactor,227,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/224#issuecomment-75658867,1,['refactor'],['refactor']
Modifiability,"There are several parts to this that all exist, but need to be committed in this order:. -Minor refactoring of CRAM container writing code in htsjdk to allow separation of the writing of CRAM containers from the writing of the CRAM file header, SAM file header, and EOF container for part-merging.; -Hadoop-BAM needs to go to Java8 and upgrade to newer htsjdk.; -Add CRAMOutputFormat, CRAMRecordWriter, etc. to Hadoop-BAM (these depend on the htsjdk code mentioned above).; -Modify ReadSparkSink to be CRAM aware.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1270#issuecomment-169339543:96,refactor,refactoring,96,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1270#issuecomment-169339543,1,['refactor'],['refactoring']
Modifiability,"There are still too many variables here. Do you know that the input bams are the same? Are you using BWA-MEM? My theory is that it's choosing different secondary alignments in MQ0 cases, preferring bases that are capital. If you can show that the same reads going in produce different variants with your two different references then this will be a lot easier to debug.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6825#issuecomment-707755358:25,variab,variables,25,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6825#issuecomment-707755358,1,['variab'],['variables']
Modifiability,"There has been no activity on this for two years, and the two classes already inherit from different superclasses, and the current ""has a"" implementation avoids code duplication nicely.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4580#issuecomment-592146189:78,inherit,inherit,78,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4580#issuecomment-592146189,1,['inherit'],['inherit']
Modifiability,"There is a first attempt to have some configurable settings in #2322. If in that PR this could be included, feel free to let me know how do you want to do this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2337#issuecomment-272485359:38,config,configurable,38,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2337#issuecomment-272485359,1,['config'],['configurable']
Modifiability,"There is no conceivable worst case for this PR -- the only reason for having a `CountSet` was to be be able to have quick `min` and `max` operations (in log(n) time), *but*. * these operations are not used anywhere outside of unit tests, so to make an illuminating worst case you would have to rewrite the assembly engine.; * Even if we did use these operations they would be done once per assembly region, and therefore we could make this class 1000x slower and we would add about a second to the run time of a WGS bam.; * a plain old `TreeSet`, which is what this PR replaces the `CountSet` with, also has these operations in log time.; * the number of kmer sizes used is usually 2, and will be up to 6 in very rare cases.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5467#issuecomment-443463884:294,rewrite,rewrite,294,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5467#issuecomment-443463884,1,['rewrite'],['rewrite']
Modifiability,"There were a few issues with this case. First, the data source was not constructed 100% correctly. The config file is correct. . The index file is for the tar.gz version of the source data and not for the uncompressed version that they're using. The index should correspond to the source data in the file referenced by the config file itself (not a zipped or otherwise transformed version). Secondly, the source `tsv` data file has the header line for the table commented out. The Xsv codec is aware of leading hash marks as comments and will ignore any such lines. Because of this, the leading hash in the table header is ignored and the file cannot be properly parsed. The fix is simple - just remove the leading hash from the table header (the preceding line with the two hash marks is correctly interpreted as a file header because of the leading hashes acting as comments). Lastly, even if the user fixed the file they would still need to index it with`IndexFeatureFile`. At some point the code underlying this in `HTSJDK` was broken such that no Xsv files can currently be indexed. I have submitted a pull request in `HTSJDK` (https://github.com/samtools/htsjdk/pull/1429) for this and have another ready to go in GATK (#6224) that includes a test for this case so this reversion cannot happen again.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6223#issuecomment-545186183:103,config,config,103,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6223#issuecomment-545186183,2,['config'],['config']
Modifiability,"There were some good basic examples in the original ticket:. - get all the contiguously aligned reads (e.g, xxM); - get reads with soft clipping (e.g., xSxxM, could be reads with partial adapter sequence still left after trimming); - get reads with insertions (e.g., xxMxxIxxM, could be spliced reads, e.g., reads spanning exon-exon, or intron-intron junction); - get reads with deletions (g.g., xxMxxDxxM, could point at SV). Those would be the basic must-haves. Then the next step of nice-to-haves would be to be able to find specific patterns like ""D followed by I"" or specific numbers of operators like ""exactly five D in a row"" or ""five D in total, not necessarily in consecutive order"". Do you need me to be more specific than that?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/588#issuecomment-307890525:187,adapt,adapter,187,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/588#issuecomment-307890525,1,['adapt'],['adapter']
Modifiability,"There's a lot of room for improvement in the ported code, but as you said we can deal with refactoring later.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/347#issuecomment-89331480:91,refactor,refactoring,91,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/347#issuecomment-89331480,1,['refactor'],['refactoring']
Modifiability,"These downstream protected tests only need to run on each merge into master, not when a PR is opened, so that should address the security issue. (I misspoke earlier, sorry -- only the tests in https://github.com/broadinstitute/gatk/issues/2298 need to run for each PR). The goal is just to have a badge on github that lets us know whether protected is working with the current build of public/master (and it often won't be in the course of development). Workflow would be:; 1. A merge goes into GATK public/master; 2. Job grabs the HEAD of public/master, and builds and installs a snapshot into the local maven repository using `./gradlew install printVersion`; 3. Job needs to save the last line of output from `./gradlew install printVersion` in a `gatkPublicVersion` variable -- this is the version of the snapshot of public that was installed to maven local.; 4. Job checks out latest GATK protected/master and executes the command to build it and run the full test suite, but as part of that command it overrides the GATK public version to point to the snapshot version installed locally and saved in the `gatkPublicVersion` variable above. It is trivial to patch protected's `build.gradle` to add the ability to override the GATK public version when running gradle commands (we can help with this part).; 5. New github badge for ""Downstream GATK protected tests"" on the front page of the GATK public github repo gets updated with the results of step 4, and can be clicked on to view the full test output.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1758#issuecomment-287543585:770,variab,variable,770,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1758#issuecomment-287543585,2,['variab'],['variable']
Modifiability,"These kind of errors are typically seen when the field description in the VCF header is incorrect. For example, describing the field length to be a fixed integer when the field is really variable length. I would recommend closely scanning the VCF header for inconsistencies first.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5045#issuecomment-407450035:187,variab,variable,187,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5045#issuecomment-407450035,1,['variab'],['variable']
Modifiability,"Things left for later:; * `GenotypeIndexCalculator` sometimes interacts with primitive arrays, sometimes with `GenotypeAlleleCounts`; * `GenotypeLikelihoodCalculator` has extraneous responsibilities and doesn't interact with `GenotypeAlleleCounts` as well as it should.; * `alleleCountsToIndex(final GenotypeAlleleCounts newGAC, final int[] newToOldAlleleMap)` in `GenotypeIndexCalculator` needs refactoring.; * `GenotypeLikelihoodCalculators` is really just a cache of `GenotypeAlleleCounts`.; * `GenotypeAlleleCounts` has some unused and barely-used methods, and it precomputes a lot of quantities that are not often needed and could be computed on-the-fly without difficulty or expense.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1066400217:396,refactor,refactoring,396,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1066400217,1,['refactor'],['refactoring']
Modifiability,This also means the `GeneListOutputRenderer` will need to accept a config file as a parameter. `SimpleTsvOutputRenderer` already does this.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5962#issuecomment-494907060:67,config,config,67,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5962#issuecomment-494907060,1,['config'],['config']
Modifiability,"This bug has become part of a bigger effort to address how configure the gatk. We're working on a general solution to avoid this sort of issue in the future. We haven't addressed this specific subcase yet though. For now the workaround I described above should work for you. If it doesn't, let me know.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2300#issuecomment-274899565:59,config,configure,59,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2300#issuecomment-274899565,1,['config'],['configure']
Modifiability,"This code comes with no tests and it duplicates the code we have in BQSR, which is not the right way to do it I think. . I think there are 2 ways to proceed. One is to refactor the current BQSR code to use an abstraction like the Recalibration Table builder (which may not be a bad idea by itself) - then you can reuse existing tests. Another way it to treat this a from-scratch reimplementation - but then we need all this new code fully tested. back to @jean-philippe-martin",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/511#issuecomment-101329709:168,refactor,refactor,168,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/511#issuecomment-101329709,1,['refactor'],['refactor']
Modifiability,"This error message is related to GATK's ability to load files on Google buckets (""gcs://bucket/file.bam""). This ability is enabled even when running locally (this aspect is intentional, because it's useful to be able to run a local GATK instance to process remote data without having to fire up a VM). As the bucket-reading code (""NIO"") initializes, it looks for credentials to use. Those can be set via an environment variable or via `gcloud auth`, as described in GATK's README. If neither of these are set, it checks whether it's currently running in a Google virtual machine (so it can figure out who owns the virtual machine that it's running on, and use those credentials). Apparently this code throws an exception if it runs out of ways to find credentials, and our code prints it out and moves on. The message is useful, for if we *were* running in a google VM and the credential-finding failed, we'd certainly like to know. Whether we need the full stack trace, now, that's a choice we have to make.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4369#issuecomment-424038095:419,variab,variable,419,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4369#issuecomment-424038095,1,['variab'],['variable']
Modifiability,"This happens for me also. I will modify and set the environment variable SPARK_LOCAL_IP=""127.0.0.1"". Thanks @cwhelan and @SHuang-Broad. The environmental variable fixes the error.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1534#issuecomment-356030921:64,variab,variable,64,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1534#issuecomment-356030921,2,['variab'],['variable']
Modifiability,This is fixed for now with a travis environment variable. I'm testing lb_add_region_to_dataproc to make sure that the fix works before removing that variable and merging a change to the dataproc code.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6129#issuecomment-525941840:48,variab,variable,48,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6129#issuecomment-525941840,2,['variab'],['variable']
Modifiability,"This is not ready for merge -- I just want to see if tests pass with this configuration. There are still some unresolved vulnerabilities:. ```; [1/7] - pkg:maven/com.google.protobuf/protobuf-java@4.0.0-rc-2 - 3 vulnerabilities found!; [2/7] - pkg:maven/log4j/log4j@1.2.17 - 6 vulnerabilities found!; [3/7] - pkg:maven/org.codehaus.janino/janino@3.1.9 - 1 vulnerability found!; [4/7] - pkg:maven/net.minidev/json-smart@2.4.7 - 1 vulnerability found!; [5/7] - pkg:maven/org.codehaus.jettison/jettison@1.1 - 3 vulnerabilities found!; [6/7] - pkg:maven/org.eclipse.jetty/jetty-util@9.4.48.v20220622 - 1 vulnerability found!; [7/7] - pkg:maven/org.eclipse.jetty/jetty-http@9.4.48.v20220622 - 1 vulnerability found!; ```. Some of these we may be unable to resolve. Eg., the `protobuf-java` version in this branch appears to be the most recent one, but still has open vulnerabilities filed against it. The ancient log4j 1.x version is used by two of our dependencies (`hdf5-java-bindings` and `spark-mllib_2.12`), and is the most recent version. Note that this is completely unrelated to the infamous log4j 2.x vulnerability, which was patched in GATK a long time ago.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8352#issuecomment-1581408853:74,config,configuration,74,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8352#issuecomment-1581408853,1,['config'],['configuration']
Modifiability,This is now configurable via Owner -- closing.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2139#issuecomment-358072659:12,config,configurable,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2139#issuecomment-358072659,1,['config'],['configurable']
Modifiability,"This is only changing codepaths related with the help. So this change will change the usage to say that all filters are valid for disabeFilter to say that only the available ones are valid. The only problem could be in the docgen code, but not in the behavior of the plugin.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2360#issuecomment-275496845:267,plugin,plugin,267,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2360#issuecomment-275496845,1,['plugin'],['plugin']
Modifiability,"This is ready for review again, @droazen. I split the walkers in two: `ReadSliderWalker` and `VariantSliderWalker`, both based on `ReadWalker` and `VariantWalker` to traverse in a sliding-window approach. I also implemented some `ArgumentCollectionDefinition` for the three parameters (window-size, window-step and window-padding). There are some changes that I would like to discuss with you:; - `ShardSource<T>`: a generic class for lazily load sources of certain type. Now `ReadShard` extends this class, but there is actually no change in the definitions. I wonder if it is worthy to maintain `ReadShard` as a separate class, because it seems that the only usage is in `AssemblyRegionWalker` and it could be easily changed by a `ShardSource<GATKRead>`.; - `FilteringIterator<T>`: another generic class for filter on iteration records. Now `ReadFilteringIterator` extends this class, but again I don't find any usage that requires an specific case instead of a generic.; - There is some repetition in the test code: `ShardSourceUnitTest` and `ReadShardUnitTest` are exactly the same. I didn't want to remove the later, but I think that it is redundant; the same for the `FilteringIteratorUnitTest` adn `ReadFilteringIterator.; - Because of the inclusion of `ArgumentCollectionDefinition`s for window-related parameters, I just want to know if I should include them in `AssemblyRegionWalker` to maintain consistency in the user options. Back to you @droazen, and thanks again for all the help.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-224348995:488,extend,extends,488,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-224348995,2,['extend'],['extends']
Modifiability,"This is resolved -- we are going to take in the Picard tools as a dependency, and refactor both projects to depend on a shared arg parsing/documentation system (https://github.com/broadinstitute/barclay).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1210#issuecomment-265904560:82,refactor,refactor,82,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1210#issuecomment-265904560,1,['refactor'],['refactor']
Modifiability,"This is the set of fixes for the filter plugin, @cmnbroad. The first commit is the change introduced in #2385 for less verbose test output, so it should be drop once it is accepted.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2401#issuecomment-278899912:40,plugin,plugin,40,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2401#issuecomment-278899912,1,['plugin'],['plugin']
Modifiability,"This likely has to do with your spark configuration. Check on the Spark job's progress through the web interface, which should be something like http://<driver_address>:4040 (see https://spark.apache.org/docs/latest/monitoring.html). . If your BAM is very small, you can also try increasing the number of partitions by reducing --bamPartitionSize.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312316932:38,config,configuration,38,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312316932,1,['config'],['configuration']
Modifiability,This list was generated using a not-yet merged version of my CRAM metadata tool that uses my not-yet-merged refactored CRAM code.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6018#issuecomment-505925995:108,refactor,refactored,108,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6018#issuecomment-505925995,1,['refactor'],['refactored']
Modifiability,"This looks promising, at a minimum we should try setting up the docker with hadoop native libraries so the performance gains can be extended to most use cases. This might also include adding some magic to the gatk launch script inside the docker to detect and run with the correct version of the hadoop libraries.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4746#issuecomment-387519906:132,extend,extended,132,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4746#issuecomment-387519906,1,['extend'],['extended']
Modifiability,This must be the most discussed tool in the GATK. I am relieved to close the book on this PR. Very nice work @mwalker174 -- thanks for all the refactoring!,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7243#issuecomment-936636793:143,refactor,refactoring,143,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7243#issuecomment-936636793,1,['refactor'],['refactoring']
Modifiability,This needs to happen for MAF too. . We should add a method `OutputRenderer::sanitizeField` that we can plug into the annotation process that will automatically sanitize each field for illegal characters as they are added to the output. This will require refactoring the `OutputRenderer::write` method to be concrete with a call to another write method and this sanitizeField method to get the benefits automatically for all OutputRenderers.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4693#issuecomment-383709501:254,refactor,refactoring,254,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4693#issuecomment-383709501,1,['refactor'],['refactoring']
Modifiability,"This requires also a finer control for the codecs, once the configuration-code is implemented, to ignore default packages, and include/exclude single classes.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2139#issuecomment-324272895:60,config,configuration-code,60,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2139#issuecomment-324272895,1,['config'],['configuration-code']
Modifiability,"This seems like a consequence of the fact that we use `java.nio.file.Path`for a lot of things in gatk. This requires a custom `java.nio.file.spi.FileSystemProvider` to be available for each type of path you want to be able to resolve. Spark native uses `org.apache.hadoop.fs.Path` for a lot of things. It's seems likely that that maprfs provides a hadoop file system plugin, which many spark applications can consume, but it's unlikely that it also provides a java.nio.file.Path implementation. ; ; I don't think we'd be able to implement a provider for maprfs ourselves. We don't have any systems with maprfs and don't have the bandwidth to take it on right now. Implementing a file system provider isn't a terribly complicated project, but it's not a trivial one either. However, there's an implementation for hadoop here https://github.com/damiencarol/jsr203-hadoop which is sufficient for what gatk does. If maprfs provides a hadoop file system, it would probably not be too difficult to take that project as a template and modify it to use the maprfs implementation. . I think the only things you'd have to implement for the spark tools to work are the basic Path operations that support the simple operations like `Paths.get()`,`Files.exists()`, and `Path.resolve()`. (although that's not a complete list. . If you are interested in writing a plugin like that, you can add it to the gatk class path at runtime. We might also be open to packaging such a plugin with the gatk if there was wide demand for it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3936#issuecomment-350070555:367,plugin,plugin,367,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3936#issuecomment-350070555,3,['plugin'],['plugin']
Modifiability,This seems like a good idea. I have never liked the -1 magic values. It's definitely a non-trivial refactoring effort though.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1445#issuecomment-174612131:99,refactor,refactoring,99,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1445#issuecomment-174612131,1,['refactor'],['refactoring']
Modifiability,"This seems like a lot of machinery (introducing two new types and a new method) just to hide the config file argument. What if we just mark it `@Hidden` (I know thats prohibited, but this is kind of a special case). The only reason it even exists is because we wanted it to appear in the command lines we display on output and embed in output files. If its `@Hidden` it will still be reflected there when it's used, but it wouldn't be displayed in tool help/usage. Its already always displayed in help as an arg for the gatk wrapper.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4474#issuecomment-371570897:97,config,config,97,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4474#issuecomment-371570897,1,['config'],['config']
Modifiability,"This seems to be a regression with GATK 4.1.0.0. The code does check for compatible versions before beginning traversal. However, the following log was reported using the M2 WDL:; ```; Runtime.totalMemory()=58851328; ***********************************************************************. A USER ERROR has occurred: Bad input: Config file for datasource (file:///cromwell_root/funcotator_dataSources.v1.4.20180615/gencode/hg19/gencode.config) does not contain required key: ""ncbi_build_version"". ***********************************************************************; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5660#issuecomment-463020005:328,Config,Config,328,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5660#issuecomment-463020005,2,"['Config', 'config']","['Config', 'config']"
Modifiability,"This seems to happen in the cloud auth layers, which I don't control. . One potential workaround would be to add a command-line option to disable GCS support. This would only help the original reporter if they don't use GCS paths, of course. Is this something we think may be worth doing at all?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3491#issuecomment-427413074:39,layers,layers,39,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3491#issuecomment-427413074,1,['layers'],['layers']
Modifiability,This seems to have already been refactored at some point.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2102#issuecomment-590488457:32,refactor,refactored,32,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2102#issuecomment-590488457,1,['refactor'],['refactored']
Modifiability,"This ticket is just to make the codec packages configurable, which would be resolved by https://github.com/broadinstitute/gatk/pull/3447. If you need more fine-grained control than this, we could discuss as part of a separate ticket.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2139#issuecomment-337651849:47,config,configurable,47,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2139#issuecomment-337651849,1,['config'],['configurable']
Modifiability,"This ticket is silly -- these are argument collections, not actual code that needs tests. Each of these classes just contains an annotated argument variable. Each class also has comments explaining when a tool would want to use each argument collection. Closing!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/451#issuecomment-97182591:148,variab,variable,148,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/451#issuecomment-97182591,1,['variab'],['variable']
Modifiability,This ties into the URI class design meeting we're having next week -- I'd say wait until then before starting any refactor of this part of the code.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4480#issuecomment-369943177:114,refactor,refactor,114,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4480#issuecomment-369943177,1,['refactor'],['refactor']
Modifiability,"Those sound like issues with the IndelRealigner tool from GATK3, which is; not part of our pipeline anymore. Is this still a problem with 4.1.4.1?. On Wed, Feb 19, 2020, 1:00 AM Dario Strbenac <notifications@github.com>; wrote:. > In the news file of a structural variant software I use, I read; >; > Added FIX_SA and FIX_MISSING_HARD_CLIP; > FIX_SA: rewrites split read SA tags; > corrects GATK indel realignment SA tag data inconsistency; > FIX_MISSING_HARD_CLIP: infers missing hard clipping if split read records; > have different lengths; > corrects for GATK indel realignment stripping hard clipping when realigning; >; > Could such issues perhaps be resolved in an update to GATK?; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/6459?email_source=notifications&email_token=ABSGC5E7CIUF53HYCPS76FDRDTDHBA5CNFSM4KXSMK22YY3PNVWWK3TUL52HS4DFUVEXG43VMWVGG33NNVSW45C7NFSM4IOQ3U6A>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ABSGC5DYKL5KH6EZS5ZU66DRDTDHBANCNFSM4KXSMK2Q>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6459#issuecomment-588488049:351,rewrite,rewrites,351,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6459#issuecomment-588488049,1,['rewrite'],['rewrites']
Modifiability,To add to this ticket. In #7876 we have had to expand the JumboAnnotations to work in the HaplotypeCaller as well. Unfortunately this has created problems since there aren't evidences objects in the HC so we have had to change the erasure of the annotate() methods somewhat and some hacky code is now part of the `VariantAnnotatorEngine` which currently has some code in the `addInfoAnnotations()` method that has to resolve the complicated spiderwebs of which likelihoods objects do or don't exist at any given time and then cast them to what they likely are. This really needs to be revisited and refactored to handle the extra annotation inputs more gracefully.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7543#issuecomment-1191802150:599,refactor,refactored,599,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7543#issuecomment-1191802150,1,['refactor'],['refactored']
Modifiability,"To clarify what needs to be done here:. -Add a new `--javaOptions` argument to `gatk-launch`. -When running with a packaged local jar, the value of `--javaOptions` should be injected into the command line built by `formatLocalJarCommand()`. -When running with the ""wrapper script"" (as a result of building with `./gradlew installDist` instead of `./gradlew localJar`), propagate the value of `--javaOptions` to the `JAVA_OPTS` environment variable the wrapper script expects. You can inspect the wrapper script itself by running `./gradlew installDist` and then examining `build/install/gatk/bin/gatk`. -When running on Spark, you'll need to add the `--javaOptions` to `spark.driver.extraJavaOptions` and `spark.executor.extraJavaOptions`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2694#issuecomment-305007868:439,variab,variable,439,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2694#issuecomment-305007868,1,['variab'],['variable']
Modifiability,"To clarify, the tests are being run. It appears to be a bug in how we have configured the jacocoTestReport job that gets executed inside the docker image which seems to result some missing xml files that codeCoverage uses to build its reports. Since we have our integration and cloud tests outside of the docker image the coverage didn't drop to zero. I am looking into reconfiguring the jacocoTestReport task to behave correctly.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5001#issuecomment-404629551:75,config,configured,75,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5001#issuecomment-404629551,1,['config'],['configured']
Modifiability,"To provide some more background, the idea is to generate output as generated by [CollectAllelicCounts](https://software.broadinstitute.org/gatk/documentation/tooldocs/current/org_broadinstitute_hellbender_tools_copynumber_CollectAllelicCounts.php) for a pool of normals so that we can correct allelic biases in tumor-only. Would it be possible that CreateSomaticPanelofNormals is extended to cover the CollectAllelicCounts ""special case""? . @samuelklee @davidbenjamin.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5649#issuecomment-462058940:380,extend,extended,380,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5649#issuecomment-462058940,1,['extend'],['extended']
Modifiability,"TrainVariantAnnotationsModel:. Trains a model for scoring variant calls based on site-level annotations. TODOs:. - [x] Integration tests. Exact-match tests for (non-exhaustive) configurations given by the Cartesian product of the following options:; * non-allele-specific vs. allele-specific; * SNP-only vs. SNP+INDEL (for both of these options, we use extracted annotations that contain both SNP and INDEL variants as input); * positive (training with *.annot.hdf5) vs. positive-unlabeled (training with *.annot.hdf5 and *.unlabeled.annot.hdf5); * Java Bayesian Gaussian Mixture Model (BGMM) backend vs. python sklearn IsolationForest backend; (BGMM tests to be added once PR for the backend goes in.); - [x] Tool-level docs. Minor TODOs:. - [x] Parameter-level docs.; - [x] Parameter/mode validation.; - [x] Refactor main code block for model training; it's a bit monolithic and procedural now.; - [x] Decide on behavior for ill-behaved annotations. E.g., all missing, zero variance. Future work:. - [ ] We could allow subsetting of annotations here, which might allow for easier treatment of ill-behaved annotations. However, I'd say enabling workflows where the set of annotations is fixed is the priority.; - [ ] We could do positive-unlabeled training more rigorously or iteratively. Right now, we essentially do a single iteration to determine negative data. This could perhaps be preceded by a round of refactoring to clean up model training and make it less procedural.; - [ ] Automatic threshold tuning could be built into the tool, see #7711. We'd probably have to introduce a ""validation"" label. Perhaps it makes sense to keep this sort of thing at the workflow level?; - [ ] In the positive-negative framework enforced by the Java code in this tool, a ""model"" is anything that assigns a score, we fit two models to different subsets of the data, and then take the difference of the two scores. While the python backend does give some freedom to specify a model, future developers may want",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7724#issuecomment-1067948369:177,config,configurations,177,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7724#issuecomment-1067948369,2,"['Refactor', 'config']","['Refactor', 'configurations']"
Modifiability,"Turns out a typo prevents running the ""manage_sv_pipeline"" script, saying GATK_DIR is an unbound variable. Please fix.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3370#issuecomment-318746083:97,variab,variable,97,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3370#issuecomment-318746083,1,['variab'],['variable']
Modifiability,Unbound variable bug fixed.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3370#issuecomment-318751586:8,variab,variable,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3370#issuecomment-318751586,1,['variab'],['variable']
Modifiability,"Use SampleLocatableMetadata if you want CombineSegmentBreakpoints to only operate on segment files from a single sample. It's conceivable that you want it to be more flexible, in which case I would use LocatableMetadata. Also, go ahead and move the collection class into the collection package, rather than expose the abstract classes.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3995#issuecomment-352762883:166,flexible,flexible,166,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3995#issuecomment-352762883,1,['flexible'],['flexible']
Modifiability,"Valentin & I discussed this in person just now, with the following results:. -The various *Context objects should probably be refactored to return empty lists upon lack of input, as Valentin suggested, instead of being `Optional`. -There may be a need to allow tools to request additional context around the current locus/interval, but tools should probably not be performing arbitrary queries as a general rule, since it would be difficult or impossible to optimize a traversal in which the access pattern is random. If a tool needs to group disparate data items together (eg., mates on different contigs), there should be an initial grouping step to prepare the required data for the main analysis, instead of random queries within the main analysis. -apply()/map() should take its inputs as parameters instead of directly accessing member variables into which input data has been injected.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/242#issuecomment-76805735:126,refactor,refactored,126,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/242#issuecomment-76805735,2,"['refactor', 'variab']","['refactored', 'variables']"
Modifiability,"VariantQC is a tool we made that is somewhat analogous to FastQC. Given an input VCF, it runs VariantEval to generate various summary tables of data, and then makes an HTML report (borrowing a lot from the tool MultiQC) summarizing that VCF. . I wrote this originally by forking GATK3 and wrote a new walker that internally called and run VariantEval. That was never the final plan. I dont know what this will need to look like in GATK4 yet. I'm fine with the expectation that GATK4 VariantEval will evolve and we'd need to update our code wrapping it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-440806347:500,evolve,evolve,500,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-440806347,1,['evolve'],['evolve']
Modifiability,"VariantsSpark - Done initializing engine; 19/02/18 16:58:10 WARN org.apache.spark.SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 19/02/18 16:58:10 INFO org.spark_project.jetty.util.log: Logging initialized @8431ms; 19/02/18 16:58:11 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown; 19/02/18 16:58:11 INFO org.spark_project.jetty.server.Server: Started @8536ms; 19/02/18 16:58:11 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@45c90a05{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 19/02/18 16:58:11 WARN org.apache.spark.scheduler.FairSchedulableBuilder: Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.; 19/02/18 16:58:12 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at gatk-test-2495f43b-04fc-49e7-aa0a-7108cc876246-m/10.240.0.11:8032; 19/02/18 16:58:13 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at gatk-test-2495f43b-04fc-49e7-aa0a-7108cc876246-m/10.240.0.11:10200; 19/02/18 16:58:15 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1550508751046_0004; WARNING	2019-02-18 16:58:23	AsciiLineReader	Creating an indexable source for an AsciiFeatureCodec using a stream that is neither a PositionalBufferedStream nor a BlockCompressedInputStream; WARNING	2019-02-18 16:58:23	AsciiLineReader	Creating an indexable source for an AsciiFeatureCodec using a stream that is neither a PositionalBufferedStream nor a BlockCompressedInputStream; 19/02/18 16:58:25 INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat: Total input files to process : 1; 19/02/18 16:58",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3840#issuecomment-464825765:4890,config,configuration,4890,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3840#issuecomment-464825765,1,['config'],['configuration']
Modifiability,"WIth the 4.2.2.0 ReblockGVCF it is running fine. This was without rerunning the HaplotypeCaller to create the gvcf just the reblock. . ```; Using GATK jar /share/pkg.7/gatk/4.2.2.0/install/gatk-4.2.2.0/gatk-package-4.2.2.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /share/pkg.7/gatk/4.2.2.0/install/gatk-4.2.2.0/gatk-package-4.2.2.0-local.jar ReblockGVCF -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -V gvcf.gather/GARDWGSN00001.autosome.g.vcf.gz -drop-low-quals -rgq-threshold 20 -do-qual-approx -O g1.test.reblock.g.vcf.gz; 00:54:40.318 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.2.0/install/gatk-4.2.2.0/gatk-package-4.2.2.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Aug 25, 2021 12:54:40 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 00:54:40.501 INFO ReblockGVCF - ------------------------------------------------------------; 00:54:40.501 INFO ReblockGVCF - The Genome Analysis Toolkit (GATK) v4.2.2.0; 00:54:40.501 INFO ReblockGVCF - For support and documentation go to https://software.broadinstitute.org/gatk/; 00:54:40.501 INFO ReblockGVCF - Executing as farrell@scc-hadoop.bu.edu on Linux v3.10.0-1160.36.2.el7.x86_64 amd64; 00:54:40.502 INFO ReblockGVCF - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 00:54:40.502 INFO ReblockGVCF - Start Date/Time: August 25, 2021 12:54:40 AM EDT; 00:54:40.502 INFO ReblockGVCF - ------------------------------------------------------------; 00:54:40.502 INFO ReblockGVCF - ------------------------------------------------------------; 00:54:40.503 INFO ReblockGVCF - HTSJDK Version: 2.24.1; 00:54:40.503 INFO ReblockGVCF - Picard",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7334#issuecomment-905183643:256,variab,variable,256,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7334#issuecomment-905183643,1,['variab'],['variable']
Modifiability,"We build GATK-SV docker images on GitHub runners. We use the following to set up the environment to use `--squash` flag. https://github.com/broadinstitute/gatk-sv/blob/52813222b64bf2d15fb9a1aae068590bee184511/.github/workflows/sv_pipeline_docker.yml#L199-L204. I am unsure if you can configure the runtime env on Google Cloud build, but if you can, hopefully, the above can hint some directions.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8684#issuecomment-1979099671:284,config,configure,284,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8684#issuecomment-1979099671,1,['config'],['configure']
Modifiability,We can either add `to*LegacySegmentCollection` methods to ModeledSegmentCollection or add static utility methods to ModelSegments. No real preference here as the coupling is minimal.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5037#issuecomment-407170543:162,coupling,coupling,162,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5037#issuecomment-407170543,1,['coupling'],['coupling']
Modifiability,We could combine the filters and transformers into 1 plugin that can intersperse them. It would be more complicated but be maximally expressive.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2085#issuecomment-246002976:53,plugin,plugin,53,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2085#issuecomment-246002976,1,['plugin'],['plugin']
Modifiability,"We generate indices on output files, but we decided not to auto-generate indices on input files in GATK4. We included tools `IndexFeatureFile` and `BuildBamIndex` that can generate these indices on-demand. Indexing inputs automatically is inherently racy/dangerous in the face of multiple processes sharing inputs unless you do things like file locking, which comes with all sorts of portability issues.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2647#issuecomment-298768350:384,portab,portability,384,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2647#issuecomment-298768350,1,['portab'],['portability']
Modifiability,"We might want to add a separate mechanism for this as well that doesn't involve extending `Main`, since using a different main class makes packaging slightly awkward. We could add an editable config file that `Main`reads from to find the list of packages to search.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1396#issuecomment-166960036:80,extend,extending,80,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1396#issuecomment-166960036,2,"['config', 'extend']","['config', 'extending']"
Modifiability,"We recommend backing up data just because it is the ""cleanest"" way to roll back. If backing up data is really such a pain point, you could skip doing that. Just back up the callset.json file, and don't turn on `--consolidate` when you're doing incremental import. If a failure happens, just roll back the callset.json and re-do the import. The downside is that the failed import will hang around and take up disk space, but hopefully it is a rare enough occurrence that it doesn't matter - and you will have saved yourself backing up the data. In response to 2) - I guess you're implying that the overhead of cluster/job scheduling won't amortize any benefits from parallelism there? I suppose that could be true, but doesn't seem to be worth optimizing towards that. What I'm asking is whether split and merge are purely an instrument to allow you to choose the granularity of parallelism you want to use? Or is there something else? As I said before, we are considering enabling other ways to do distributed import which would work for the former. It might go something like:; - Create a workspace/initialize configuration+intervals to be imported; - Actually do the import by kicking off (multiple) import(s). User can pick the number of intervals each import is responsible for. User must ensure that no interval gets specified in multiple import processes. P.S: regarding 1000s of small contigs - the current GenomicsDBImport doesn't so so well with large number of contigs (unless you do concatenate the contigs into fewer groups). We hope to have some changes coming soon that will help with that by adding an option for the tool to merge multiple contigs into a single folder in the workspace.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6620#issuecomment-641037548:1111,config,configuration,1111,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6620#issuecomment-641037548,1,['config'],['configuration']
Modifiability,We should definitely try to centralize setting of the system properties if possible (perhaps using a master config file) -- though I vote that we put in a quick fix for this first.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2316#issuecomment-267123755:108,config,config,108,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2316#issuecomment-267123755,1,['config'],['config']
Modifiability,"We've filed a ticket with github support -- however, the branch has been cleared to merge in its current state, as it's had more than enough reviews. We can file tickets to improve/refactor once it's in master.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3945#issuecomment-351092625:181,refactor,refactor,181,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3945#issuecomment-351092625,1,['refactor'],['refactor']
Modifiability,"Weird, I would have that that forcing the htsjdk version like we already do would have done it... I don't see anything wrong with adding that exclusion, but I'm confused why we need it. ` force 'com.github.samtools:htsjdk:' + htsjdkVersion`. It sounds like a gradle bug in building the final pom file. I wonder if switching to the javaLibrary plugin would fix it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2578#issuecomment-292579154:343,plugin,plugin,343,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2578#issuecomment-292579154,1,['plugin'],['plugin']
Modifiability,"Well, I guess that part of the contract for GATKTool is that GATK tools should report progress as they go. I agree that the ProgressMeter should be made more flexible and allow reporting of progress in terms of things other than genomic location.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6390#issuecomment-577262636:158,flexible,flexible,158,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6390#issuecomment-577262636,1,['flexible'],['flexible']
Modifiability,"Well, that explains that, sort of. The code snippet you're providing looks like it ought to do what you say it does (i.e., the mates have to be paired, not unmapped, mapped to the same contig, and have a difference in their start positions that is at least `mateTooDistantLength`). . But there are two problems with this:. 1) This filter's behavior is unexpected wrt HaplotypeCaller. It seems to me that an inclusive filter (i.e., process only paired-end mappings whose TLEN falls within a specified range) would be more usable. That would imply a filter implementation that accepts a pair of integers, but the expected behavior would be more obvious and in line with GATK's other range-limited parameterizations (e.g., `MappingQualityReadFilter` comes immediately to mind). 2) I can't tell from where I sit, but the code snippet looks correct only if `getStart()` and `getMateStart()` return a zero-based start position of each mate relative to the start of the strand to which the mate is mapped. If the code is just computing the difference between POS for the mates, the computation is incorrect for forward + reverse-complement (Illumina-style) pairs. In addition, computing TLEN requires not only that you consider the orientation of the individual mate mappings, but also that you make an arbitrary decision about how to handle soft-clipped reads. I hate to say this, but I think this parameter needs some attention. Its potential utility with HaplotypeCaller seems evident to me (i.e., it would be good to be able to exclude outliers with unreasonable TLENs) but its implementation and frugal documentation make it unusable in practice.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7701#issuecomment-1103199220:695,parameteriz,parameterizations,695,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7701#issuecomment-1103199220,1,['parameteriz'],['parameterizations']
Modifiability,What is the progress on this @cmnbroad? Is this waiting for the new Barclay plugin interface?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2401#issuecomment-282995363:76,plugin,plugin,76,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2401#issuecomment-282995363,1,['plugin'],['plugin']
Modifiability,"What is the timeline for portable WDL-NIO?. This would make things like performing preliminary analyses on subsets of contigs, etc. go a bit faster. I agree that this is not a common use case, but since it's such a small amount of work, I don't see the harm. Would be nice to be consistent with other Featured WDLs, if they're all using NIO as well (is this true?)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4806#issuecomment-391724363:25,portab,portable,25,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4806#issuecomment-391724363,1,['portab'],['portable']
Modifiability,"What is your cluster configuration? . That's a lot of memory for one executor, it may be having trouble allocating workers with that much memory, or using all the memory on 1 very large executor.; Have you tried setting executor cores as well? I would usually set it to something like `--executor-cores 4 --executor-memory 16G` . You want to design your executors so they fit evenly into the worker nodes on your cluster but don't have too many cores per executor. . An aside, you *should* be able to create a bam index as part of SortSamSpark now, we have support for generating it in parallel and merging the indexes.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6233#issuecomment-547077382:21,config,configuration,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6233#issuecomment-547077382,1,['config'],['configuration']
Modifiability,"What the rules for when a tool is allowed to be a `CommandLineProgram`? Most of the CNV tools extend `CommandLineProgram` rather than `GATKTool` for various reasons, including: 1) they use sequence-dictionary input in a way that requires custom argument documentation, 2) they use `-I` to specify non-BAM/SAM/CRAM input, and 3) they don't really make use of the argument collections available in `GATKTool` or otherwise fall under the walker paradigm. These reasons are admittedly minor, but they do make the tools a bit nicer to use in the end. Otherwise, whenever it makes sense for a tool to extend `GATKTool`, it does (4 out of 12 of the CNV tools). (A bit of a tangent: in all the cases where we do extend `GATKTool` to e.g. make use of the `-L` functionality, we still have to jump through some extra hoops to make sure we don't get tripped up. For example, the default `--interval-merging-rule` behavior is incorrect for most CNV analyses, so the user has to set this to `OVERLAPPING_ONLY` manually, otherwise we throw an exception---which is quite awkward. Ideally, we'd have some option to not modify the incoming intervals at all, as well.). So I'm comfortable with closing this issue, but we can discuss the pros and cons of moving more of the tools over if necessary.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2471#issuecomment-358038497:94,extend,extend,94,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2471#issuecomment-358038497,3,['extend'],['extend']
Modifiability,"While we normally don't recommend ignoring that wrapper, this seems like a good reason to do so. . The wrapper is pretty simple, most of what it's doing is some munging of the input to allow it to be more standardized in several different gatk use cases. The only thing I can think of that you would want to be sure to copy is that it sets a number of properties. . We set these spark `--conf` properties with the wrapper. I don't actually know how important some of them are anymore. If it works without them then you're probably good.; ```; ""spark.kryoserializer.buffer.max"" : ""512m"",; ""spark.driver.maxResultSize"" : ""0"",; ""spark.driver.userClassPathFirst"" : ""false"",; ""spark.io.compression.codec"" : ""lzf"",; ""spark.executor.memoryOverhead"" : ""600"",; ""spark.driver.extraJavaOptions"" : EXTRA_JAVA_OPTIONS_SPARK,; ""spark.executor.extraJavaOptions"" : EXTRA_JAVA_OPTIONS_SPARK; ```. These are htsjdk properties we want to set for spark. ; ```; EXTRA_JAVA_OPTIONS_SPARK= ""-DGATK_STACKTRACE_ON_USER_EXCEPTION=true "" \; ""-Dsamjdk.use_async_io_read_samtools=false "" \; ""-Dsamjdk.use_async_io_write_samtools=false "" \; ""-Dsamjdk.use_async_io_write_tribble=false "" \; ""-Dsamjdk.compression_level=2 ""; ```. If you can get this value into your spark environment variables it prevents and anying warning output. `SUPPRESS_GCLOUD_CREDS_WARNING=true`. Let us know how it works for you.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6198#issuecomment-539073054:1251,variab,variables,1251,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6198#issuecomment-539073054,1,['variab'],['variables']
Modifiability,Will add a variable to our Protobuf configuration object - the JSON already an option to set this.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2687#issuecomment-300298863:11,variab,variable,11,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2687#issuecomment-300298863,2,"['config', 'variab']","['configuration', 'variable']"
Modifiability,Will refactor and re-open as a different PR,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8642#issuecomment-1937109021:5,refactor,refactor,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8642#issuecomment-1937109021,1,['refactor'],['refactor']
Modifiability,"With a service account key set, it worked like a charm:. ```; $ ./gatk-launch PrintReadsSpark -I gs://jpmartin-testing-project/hellbender-test-inputs/CEUTrio.HiSeq.WGS.b37.ch20.1m-2m.NA12878.bam -O gs://jpmartin-testing-project/test-output/readcount --shardedOutput true -- --sparkRunner GCS --cluster jps-test-cluster; (...); [November 20, 2017 6:17:08 PM UTC] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.72 minutes.; Runtime.totalMemory()=670040064; Job [13c93a62-96d0-456e-91d1-ef7b20f1236b] finished successfully.; ```. Though I understand that [this is expected](https://github.com/broadinstitute/gatk/issues/3591#issuecomment-330650894). So next I tried it without any `HELLBEND*` environment variable and it worked as well!. ```; Job [6e2f2c6b-921a-4fdf-a42e-0706216b2098] finished successfully.; (...); $ gsutil ls -lh gs://jpmartin-testing-project/test-output/readcount/; 0 B 2017-11-20T18:28:27Z gs://jpmartin-testing-project/test-output/readcount/; 0 B 2017-11-20T18:28:52Z gs://jpmartin-testing-project/test-output/readcount/_SUCCESS; 120.25 MiB 2017-11-20T18:28:51Z gs://jpmartin-testing-project/test-output/readcount/part-r-00000.bam; ```. This is with `GOOGLE_APPLICATION_CREDENTIALS` set, as I believe is part of the GATK README instructions. Next I went to my repro code and tried it again with v30. It failed (`StorageException: Error code 404 trying to get security access token from Compute Engine metadata for the default service account.`) I'm not sure why but the new version is certainly an improvement over the previous one since it fixes `PrintReadsSpark`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3855#issuecomment-345788205:745,variab,variable,745,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3855#issuecomment-345788205,1,['variab'],['variable']
Modifiability,"With the exception of the HMM package, all of our R dependencies are available through the conda R or bioconda channels; the HMM package is available only through a user's custom channel. However, the HMM package is only used to generate truth for testing the Java HMM code by @vruano (which is currently unused, but we thought was worth keeping around). I'm sure we could easily rewrite the tests to load the truth from a file. I think we should get rid of the install_R_packages.R script altogether and just roll all of these dependencies into the conda environment.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4250#issuecomment-406067920:380,rewrite,rewrite,380,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4250#issuecomment-406067920,1,['rewrite'],['rewrite']
Modifiability,"Works with local files, and a PR for cloud functionality (complete with passing test) is under review (it's PR #595). The one thing left to do is to adapt to the skeleton once that's merged in.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/426#issuecomment-114635562:149,adapt,adapt,149,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/426#issuecomment-114635562,1,['adapt'],['adapt']
Modifiability,Would also be good to identify what changes we'd need to make to the `broad-dsde-dev` firewall config to make this work there so that we can file a request with IT.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/975#issuecomment-148461723:95,config,config,95,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/975#issuecomment-148461723,1,['config'],['config']
Modifiability,"Would it be helpful to discuss this in person? Filter behavior is an important design decision and I'm not sure I have a good handle on the current state + context. On Mon, Feb 23, 2015 at 9:22 PM, Louis Bergelson notifications@github.com wrote: Some always throw, some always filter, some depend on the arguments you; pass to the filter constructor.; On Feb 23, 2015 9:19 PM, ""Geraldine Van der Auwera"" <; notifications@github.com> wrote:. > Ah -- maybe I'm mistaken. Or can it be a difference in how they're; > applied/invoked?; > ; > Maybe there's some inconsistency in behavior. Would be nice to iron this; > all out.; > ; > On Mon, Feb 23, 2015 at 5:17 PM, Louis Bergelson <notifications@github.com; > ; > > wrote:; > > ; > > I'm pretty sure they all do... or at least all can depending on how you; > > configure your MalformedReadFilter.; > > ; > > example:; > > ; > > private static boolean checkHasReadGroup(final SAMRecord read) {; > > if ( read.getReadGroup() == null ) {; > > // there are 2 possibilities: either the RG tag is missing or it is not; > > defined in the header; > > final String rgID =; > > (String)read.getAttribute(SAMTagUtil.getSingleton().RG);; > > if ( rgID == null ); > > throw new UserException.ReadMissingReadGroup(read);; > > throw new UserException.ReadHasUndefinedReadGroup(read, rgID);; > > }; > > return true;; > > }; > > ; > > —; > > Reply to this email directly or view it on GitHub; > > <; > > https://github.com/broadinstitute/hellbender/issues/193#issuecomment-75649333; > > ; > > .; > ; > ## ; > ; > Geraldine A. Van der Auwera, Ph.D.; > Bioinformatics Scientist II; > GATK Support & Outreach; > Broad Institute; > ; > —; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/hellbender/issues/193#issuecomment-75686467; > . —Reply to this email directly or view it on GitHub.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/193#issuecomment-76679907:808,config,configure,808,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/193#issuecomment-76679907,1,['config'],['configure']
Modifiability,"Wow, thanks for the detailed comments so far, @davidbenjamin! But perhaps let's quickly chat before you go any further?. There are a lot of things you commented on---temporary integration tests using local files, lots of code/arguments/etc. intentionally copied verbatim over from VQSR/tranches, and entire tools (the ""monolithic"" GMMVariantTrain and ScikitLearnVariantTrain)---that are rather in flux or will be scrapped/cleaned up shortly. That said, the comments on the code inherited from VQSR will certainly be useful in this process!. But it might save you some time if we could chat so I can give you a rough orientation and perhaps point out where the vestigial VQSR code remains. I think focusing discussion on the high level design of the tools that are likely to stay would also be most useful at this stage. Feel free to throw something on my calendar!. In the end, I think we will probably just retain the BGMM backend + the versions of the tools in the ""scalable"" package. I left the ""monolithic"" GMMVariantTrain and ScikitLearnVariantTrain tools in this branch so I could do one round of tieout. That tieout came out OK, so I think we'll abandon the monolithic tools, along with all the associated code outside of the scalable package. If it helps, I can go ahead and remove that stuff from this draft PR.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7659#issuecomment-1029393942:478,inherit,inherited,478,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7659#issuecomment-1029393942,1,['inherit'],['inherited']
Modifiability,YXZh) | `0% <0%> (-100%)` | `0% <0%> (-8%)` | |; | [...ct/CreateSomaticPanelOfNormalsIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5803/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL211dGVjdC9DcmVhdGVTb21hdGljUGFuZWxPZk5vcm1hbHNJbnRlZ3JhdGlvblRlc3QuamF2YQ==) | `3.448% <0%> (-94.253%)` | `2% <0%> (-8%)` | |; | [...ls/walkers/mutect/CreateSomaticPanelOfNormals.java](https://codecov.io/gh/broadinstitute/gatk/pull/5803/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL211dGVjdC9DcmVhdGVTb21hdGljUGFuZWxPZk5vcm1hbHMuamF2YQ==) | `0% <0%> (-91.429%)` | `0% <0%> (-23%)` | |; | [.../org/broadinstitute/hellbender/utils/IGVUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/5803/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9JR1ZVdGlscy5qYXZh) | `0% <0%> (-88.889%)` | `0% <0%> (-3%)` | |; | [...alkers/mutect/filtering/PolymorphicNuMTFilter.java](https://codecov.io/gh/broadinstitute/gatk/pull/5803/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL211dGVjdC9maWx0ZXJpbmcvUG9seW1vcnBoaWNOdU1URmlsdGVyLmphdmE=) | `0% <0%> (-88.235%)` | `0% <0%> (-9%)` | |; | [...aplotypecaller/HaplotypeCallerIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5803/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9IYXBsb3R5cGVDYWxsZXJJbnRlZ3JhdGlvblRlc3QuamF2YQ==) | `0.431% <0%> (-87.5%)` | `2% <0%> (-85%)` | |; | [...alkers/mutect/SomaticReferenceConfidenceModel.java](https://codecov.io/gh/broadinstitute/gatk/pull/5803/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL211dGVjdC9Tb21hdGljUmVmZXJlbmNlQ29uZmlkZW5jZU1vZGVsLmphdmE=) | `12.5% <0%> (-84.375%)` | `1% <0%> (-7%)` | |; | [...tils/variant/writers/SomaticGVCFBlockCombiner.ja,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5803#issuecomment-473417970:2816,Polymorphi,PolymorphicNuMTFilter,2816,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5803#issuecomment-473417970,1,['Polymorphi'],['PolymorphicNuMTFilter']
Modifiability,"Yeah, I don't like these new interface methods -- they make `GATKRead` significantly worse. We should cache `isUnmapped`, etc. in the adapter to accomplish the same thing, as @lbergelson suggests. Not that hard, and we can just unconditionally invalidate the cached values (using `Boolean` fields set to null) whenever the read is mutated in any way in order to simplify the logic.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235102282:134,adapt,adapter,134,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235102282,1,['adapt'],['adapter']
Modifiability,"Yeah, I expect to need to rewrite tests for the final port. Thanks for any help on the validation data.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/616#issuecomment-358036823:26,rewrite,rewrite,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/616#issuecomment-358036823,1,['rewrite'],['rewrite']
Modifiability,"Yeah, it would be useful (see https://github.com/broadinstitute/gatk/issues/2582). Not sure if/when we'll ever get around to the Barclay changes though. Another simple option that wouldn't require Barclay changes would be to implement it as just another (plugin descriptor) command line argument that could be sued alongside `--read-filter`'. So if you wanted a `ReadNameFilter` and an inverted `ReadLengthFilter`, the syntax would be:. `--read-filter ReadNameFilter --invert-read-filter ReadLengthReadFilter`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6005#issuecomment-502231306:255,plugin,plugin,255,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6005#issuecomment-502231306,1,['plugin'],['plugin']
Modifiability,"Yeah, refactoring this class is pretty essential to our ability to write good tests.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/641#issuecomment-146230791:6,refactor,refactoring,6,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/641#issuecomment-146230791,1,['refactor'],['refactoring']
Modifiability,"Yeah, the workaround was simply to add the library jar to the classpath and not try to compile them together. I created the issue to soon, Sorry. . As for the NIO library, it is for AWS S3. We are adapting this one https://github.com/Upplication/Amazon-S3-FileSystem-NIO2 to meet our needs. We didn't like the way it handles s3 endpoints because AWS EMR Spark clusters don't support s3 uri's with that particular syntax. Our version modifies it to support normal s3 uri's without endpoints, instead setting the endpoint with a configuration parameter.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3102#issuecomment-308161431:197,adapt,adapting,197,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3102#issuecomment-308161431,2,"['adapt', 'config']","['adapting', 'configuration']"
Modifiability,"Yep, that will definitely cause a crash - variable length fields need a length value stored while fixed length fields don't. I can add checks for this scenario in GenomicsDB. Please let me know if the following make sense:; Header says that the field F is a fixed length field with length = N. In the data section, if; * length(F) < N - pad with missing values, no error message, continue; * length(F) > N - error, throw exception and print descriptive error message",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5045#issuecomment-407501684:42,variab,variable,42,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5045#issuecomment-407501684,1,['variab'],['variable']
Modifiability,"Yes it is. Honestly not sure on the u/g config, as an end user I'd really rather not have to care about that 😅 ; This is the only tool causing this kind of issue so it's got to be the tool itself, no?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8233#issuecomment-2078145966:40,config,config,40,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8233#issuecomment-2078145966,1,['config'],['config']
Modifiability,"Yes, Hadoop-BAM uses the NIO API to do file merging, whereas in GATK we were using the Hadoop APIs (and therefore the GCS<->HDFS adapter) to do it. It looks like there are a couple of things needed in GCS-NIO to use the NIO API for this.; 1. https://github.com/GoogleCloudPlatform/google-cloud-java/issues/1450 so that we don't have to special-case `gs` URIs to remove everything except the scheme and host when looking up the filesystem (see https://github.com/HadoopGenomics/Hadoop-BAM/blob/master/src/main/java/org/seqdoop/hadoop_bam/util/NIOFileUtil.java#L40); 2. https://github.com/GoogleCloudPlatform/google-cloud-java/issues/813 to support path matching (https://github.com/HadoopGenomics/Hadoop-BAM/blob/master/src/main/java/org/seqdoop/hadoop_bam/util/NIOFileUtil.java#L90). There may be more, as I stopped there. The best way forward is probably to go back to the old code in GATK while the deficiencies in GCS-NIO are fixed and then released. The stacktrace I got for 1 was:. ```; java.lang.IllegalArgumentException: GCS FileSystem URIs mustn't have: port, userinfo, path, query, or fragment: gs://gatk-demo-tom/TEST/markdups.parts/_SUCCESS; 	at shaded.cloud-nio.com.google.common.base.Preconditions.checkArgument(Preconditions.java:146); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.newFileSystem(CloudStorageFileSystemProvider.java:192); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.newFileSystem(CloudStorageFileSystemProvider.java:83); 	at java.nio.file.FileSystems.newFileSystem(FileSystems.java:336); 	at org.seqdoop.hadoop_bam.util.NIOFileUtil.asPath(NIOFileUtil.java:40); 	at org.seqdoop.hadoop_bam.util.NIOFileUtil.asPath(NIOFileUtil.java:54); 	at org.seqdoop.hadoop_bam.util.SAMFileMerger.mergeParts(SAMFileMerger.java:51); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReadsSingle(ReadsSparkSink.java:230); ```. And for 2:. ```; java.lang.UnsupportedOperationException; 	at com.google.cloud.s",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2287#issuecomment-265132050:129,adapt,adapter,129,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2287#issuecomment-265132050,1,['adapt'],['adapter']
Modifiability,"Yes, thank you for jogging my memory @bbimber. @davidbenjamin adding something to that extend in the BadArgumentException message would be helpful.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6263#issuecomment-558740872:87,extend,extend,87,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6263#issuecomment-558740872,1,['extend'],['extend']
Modifiability,"Yes, with GATK4 we made the decision early on that all test data must be publicly-shareable and checked into the repo. This has often meant that when we port a tool from GATK3 to GATK4, we need to rewrite the tests to use different test data.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/616#issuecomment-358035278:197,rewrite,rewrite,197,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/616#issuecomment-358035278,1,['rewrite'],['rewrite']
Modifiability,YnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zdi9jbHVzdGVyL1NWQ2x1c3RlckVuZ2luZS5qYXZh) | `93.269% <0.000%> (-1.002%)` | :arrow_down: |; | [...stitute/hellbender/tools/walkers/sv/SVCluster.java](https://codecov.io/gh/broadinstitute/gatk/pull/7858/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3N2L1NWQ2x1c3Rlci5qYXZh) | `89.773% <0.000%> (-0.881%)` | :arrow_down: |; | [...tools/walkers/sv/JointGermlineCNVSegmentation.java](https://codecov.io/gh/broadinstitute/gatk/pull/7858/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3N2L0pvaW50R2VybWxpbmVDTlZTZWdtZW50YXRpb24uamF2YQ==) | `86.047% <0.000%> (-0.752%)` | :arrow_down: |; | [...der/tools/walkers/sv/SVClusterIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/7858/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3N2L1NWQ2x1c3RlckludGVncmF0aW9uVGVzdC5qYXZh) | `99.496% <0.000%> (-0.004%)` | :arrow_down: |; | [...rs/haplotypecaller/graphs/AdaptiveChainPruner.java](https://codecov.io/gh/broadinstitute/gatk/pull/7858/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9ncmFwaHMvQWRhcHRpdmVDaGFpblBydW5lci5qYXZh) | `97.368% <0.000%> (+0.035%)` | :arrow_up: |; | ... and [3 more](https://codecov.io/gh/broadinstitute/gatk/pull/7858/diff?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7858#issuecomment-1130438520:4985,Adapt,AdaptiveChainPruner,4985,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7858#issuecomment-1130438520,1,['Adapt'],['AdaptiveChainPruner']
Modifiability,"You can currently specify additional spark configuration with the --conf argument, so you could override the registrator that way. I think you'd have to update gatk-launch as well as build.gradle to get it to work, and currently gatk-launch is shared with gatk-protected. Probably the best solution is going to be to extract all the hardcoded configurations into a configuration file that can be changed on a per project basis. There's some movement to this in gatk public at the moment, but we haven't settled on a solution yet I think.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2337#issuecomment-272484533:43,config,configuration,43,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2337#issuecomment-272484533,3,['config'],"['configuration', 'configurations']"
Modifiability,"You can detect whether a tool has failed in bash by checking whether the exit status code is non-zero. In bash, the exit status code of the last command run is stored in the variable `$?`; ; In general, you should ask questions like these on the GATK forum (https://gatkforums.broadinstitute.org/gatk) instead of here, however -- this is for bug reports rather than support requests.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4242#issuecomment-359955349:174,variab,variable,174,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4242#issuecomment-359955349,1,['variab'],['variable']
Modifiability,You have a compile error btw @jamesemery:. ```; :compileTestJava/home/travis/build/broadinstitute/gatk/src/test/java/org/broadinstitute/hellbender/engine/spark/SparkCommandLineProgramUnitTest.java:20: warning: [serial] serializable class TestSparkCommandLineProgram has no definition of serialVersionUID; private static class TestSparkCommandLineProgram extends SparkCommandLineProgram {; ^; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1947#issuecomment-228763924:354,extend,extends,354,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1947#issuecomment-228763924,1,['extend'],['extends']
Modifiability,You would not have access to docker container options when using the Google backend because the running of your image is all controlled by Pipelines API. You would be able to set that value when running on a local backend but thats probably not portable enough for your workflow.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4140#issuecomment-357313468:245,portab,portable,245,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4140#issuecomment-357313468,1,['portab'],['portable']
Modifiability,"You'd better verify the consistency of the results step by step, then you can find which step's result become diffierent, finally fix it.; For example, you want to see if the `JavaRDD<Shard<GATKRead>> readShards`results are always same. You can add debug info in `callVariantsWithHaplotypeCaller` just like this:. ```; ...... final JavaRDD<Shard<GATKRead>> readShards = SparkSharder.shard ....; //Debug; String path = ""./dubug/readShardsTest1"";; java.io.File debug = new File(path);; if (!debug.getParentFile().exists()); debug.getParentFile().mkdir();; try (PrintStream printStream = new PrintStream(debug)) {; printStream.println(""List<ShardBoundary> shardBoundaries size : "" + shardBoundaries.size());; printStream.printf(""NumPartitions : %d\n"", readShards.getNumPartitions());. List<ShardDebug> shardDebugs = readShards.mapToPair(shard -> new Tuple2<>(new ShardDebug(shard), null)); .sortByKey((Comparator<ShardDebug> & Serializable) (o1, o2) ->; IntervalUtils.compareLocatables(o1, o2, header.getSequenceDictionary()); ).keys().collect();; printStream.printf(""NumShard : %d\n"", shardDebugs.size());; for (ShardDebug shardDebug : shardDebugs) {; printStream.println(shardDebug.toString());; }; }catch (Exception e){; e.printStackTrace();; }; ```. ```; static class ShardDebug extends ShardBoundary{; int size;. public ShardDebug(Shard<GATKRead> shard) {; super(shard.getInterval(),shard.getPaddedInterval());; size = Iterators.size(shard.iterator());; }. @Override; public String toString() {; return this.getInterval().toString() + ""\t"" + size;; }; }; ```; ; Run twice and compare the differences, do this step by step, you will find the bug. Gook Luck!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4231#issuecomment-371415511:1280,extend,extends,1280,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4231#issuecomment-371415511,1,['extend'],['extends']
Modifiability,"Your solution doesn't address your third listed drawback to the current; approach, though I'm not sure there's any way to do that that wouldn't; require a pretty dramatic change. It's not obvious to me why we wanted the given alleles in the graph; originally. Maybe the use case was variants from UG that we didn't; necessarily believe were aligned properly?. I don't have any objections, but I'd feel better if we had a better guess; at what the original method was trying to do. On Wed, Apr 3, 2019 at 9:56 PM David Benjamin <notifications@github.com>; wrote:. > In Mutect2 and HaplotypeCaller, we force-call alleles by injecting them; > into the ref haplotype, then threading these constructed haplotypes into; > the assembly graph with a large edge weight. There are several drawbacks to; > this approach:; >; > - The strange edge weights interfere with the AdaptiveChainPruner.; > - The large edge weights may not be large enough to avoid pruning when; > depth is extremely high.; > - The alleles may be lost if assembly fails.; > - If the alleles actually exist but are in phase with another variant; > we end up putting an enormous amount of weight on a false haplotype.; >; > We can get around these issue with the following method:; >; > - assemble haplotypes without regard to the force-called alleles.; > - if an allele is present in these haplotypes, do nothing further.; > - otherwise, add a haplotype in which the allele is injected into the; > reference haplotype.; >; > @LeeTL1220 <https://github.com/LeeTL1220> I prototyped this and it seems; > to resolve the missed forced alleles that Ziao found.; >; > @ldgauthier <https://github.com/ldgauthier> Can you think of any; > objections to making this change in HaplotypeCaller?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/5857>, or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AGRhdMcaTJg47gn",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5857#issuecomment-479916767:862,Adapt,AdaptiveChainPruner,862,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5857#issuecomment-479916767,1,['Adapt'],['AdaptiveChainPruner']
Modifiability,"[![Coverage Status](https://coveralls.io/builds/6585478/badge)](https://coveralls.io/builds/6585478). Coverage decreased (-0.5%) to 81.546% when pulling **f80168755f991400a5028c9525468650f253751b on DevFactory:release/array-designators-should-be-on-the-type,not-the-variable-fix-1** into **f4a4a6f1f13886d05a03c4b13244c3339145a6e5 on broadinstitute:master**.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1915#issuecomment-225816392:266,variab,variable-fix-,266,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1915#issuecomment-225816392,1,['variab'],['variable-fix-']
Modifiability,"[Broadcast data presentation.pdf](https://github.com/broadinstitute/gatk/files/369684/Broadcast.data.presentation.pdf). Here is a link to the presentation I gave on the broadcasting profiling data that I collected. The big takeaways are:; - Broadcasting is relatively time efficient and appears to scale linearly with filesize; - Reference broadcasting takes longer and is much more variable compared to the variants which get broadcast immediately before; - There doesn't seem to be any memory shared between executors for broadcast variables, making it very memory inefficient when there are many executors ; - The brodcast block size is best kept small, (in the order of 4-10MB) as it can explode the broadcast time to be very slow; - Having files in HDFS is significantly faster than using the GCS adapter for large file sizes",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1675#issuecomment-233396177:383,variab,variable,383,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1675#issuecomment-233396177,3,"['adapt', 'variab']","['adapter', 'variable', 'variables']"
Modifiability,"_FIELD_FORMAT : DECIMAL; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Defaults.USE_CRAM_REF_DOWNLOAD : false; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Deflater IntelDeflater; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Inflater IntelInflater; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Initializing engine; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4769b07b] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@5ef60048].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4769b07b] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@5ef60048].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2618#issuecomment-296871363:2279,variab,variable,2279,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2618#issuecomment-296871363,1,['variab'],['variable']
Modifiability,"_MODE(_MM_FLUSH_ZERO_ON);. //Profiling: times for compute and transfer (either bytes copied or pointers copied); m_compute_time = 0;; diff --git i/src/main/cpp/VectorLoglessPairHMM/org_broadinstitute_hellbender_utils_pairhmm_VectorLoglessPairHMM.cc w/src/main/cpp/VectorLoglessPairH; index f45153e..70cf54f 100644; --- i/src/main/cpp/VectorLoglessPairHMM/org_broadinstitute_hellbender_utils_pairhmm_VectorLoglessPairHMM.cc; +++ w/src/main/cpp/VectorLoglessPairHMM/org_broadinstitute_hellbender_utils_pairhmm_VectorLoglessPairHMM.cc; @@ -6,7 +6,7 @@. using namespace std;. -bool use_double = false;; +bool use_double = true;. //Should be called only once for the whole Java process - initializes field ids for the classes JNIReadDataHolderClass; //and JNIHaplotypeDataHolderClass; diff --git i/src/main/java/org/broadinstitute/hellbender/utils/pairhmm/VectorLoglessPairHMM.java w/src/main/java/org/broadinstitute/hellbender/utils; index 18370b0..74a1dad 100644; --- i/src/main/java/org/broadinstitute/hellbender/utils/pairhmm/VectorLoglessPairHMM.java; +++ w/src/main/java/org/broadinstitute/hellbender/utils/pairhmm/VectorLoglessPairHMM.java; @@ -1,6 +1,7 @@; package org.broadinstitute.hellbender.utils.pairhmm;. -import org.apache.log4j.Logger;; +import org.apache.logging.log4j.LogManager;; +import org.apache.logging.log4j.Logger;; import org.broadinstitute.hellbender.exceptions.UserException;; import org.broadinstitute.hellbender.utils.genotyper.ReadLikelihoods;; import org.broadinstitute.hellbender.utils.genotyper.LikelihoodMatrix;; @@ -20,7 +21,7 @@ import java.util.Map;; */; public final class VectorLoglessPairHMM extends LoglessPairHMM {. - final static Logger logger = Logger.getLogger(VectorLoglessPairHMM.class);; + private static final Logger logger = LogManager.getLogger(VectorLoglessPairHMM.class);; final static Boolean runningOnMac = System.getProperty(""os.name"", ""unknown"").toLowerCase().startsWith(""mac"");; long threadLocalSetupTimeDiff = 0;; long pairHMMSetupTime = 0;; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1572#issuecomment-195496083:2741,extend,extends,2741,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1572#issuecomment-195496083,1,['extend'],['extends']
Modifiability,_broadinstitute_hellbender_utils_pairhmm_VectorLoglessPairHMM.cc:1:; /Users/louisb/Workspace/gatk/src/VectorLoglessPairHMM/headers/headers.h:16:10: fatal error: 'omp.h' file not found; #include <omp.h>; ^; 1 error generated. In file included from /Users/louisb/Workspace/gatk/src/VectorLoglessPairHMM/cpp/baseline.cc:1:; /Users/louisb/Workspace/gatk/src/VectorLoglessPairHMM/headers/headers.h:16:10: fatal error: 'omp.h' file not found; #include <omp.h>; ^; 1 error generated. In file included from /Users/louisb/Workspace/gatk/src/VectorLoglessPairHMM/cpp/LoadTimeInitializer.cc:1:; In file included from /Users/louisb/Workspace/gatk/src/VectorLoglessPairHMM/headers/utils.h:4:; In file included from /Users/louisb/Workspace/gatk/src/VectorLoglessPairHMM/headers/common_data_structure.h:4:; /Users/louisb/Workspace/gatk/src/VectorLoglessPairHMM/headers/headers.h:16:10: fatal error: 'omp.h' file not found; #include <omp.h>; ^; 1 error generated. :compileVectorLoglessPairHMMSharedLibraryVectorLoglessPairHMMCpp FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':compileVectorLoglessPairHMMSharedLibraryVectorLoglessPairHMMCpp'.; > Multiple build operations failed.; C++ compiler failed while compiling avx_function_instantiations.cc.; C++ compiler failed while compiling utils.cc.; C++ compiler failed while compiling org_broadinstitute_hellbender_utils_pairhmm_VectorLoglessPairHMM.cc.; C++ compiler failed while compiling baseline.cc.; C++ compiler failed while compiling LoadTimeInitializer.cc.; See the complete log at: file:///Users/louisb/Workspace/gatk/build/tmp/compileVectorLoglessPairHMMSharedLibraryVectorLoglessPairHMMCpp/output.txt; ```. I'm using clang 6.0. Would you expect it to build on that?. ```; Configured with: --prefix=/Library/Developer/CommandLineTools/usr --with-gxx-include-dir=/usr/include/c++/4.2.1; Apple LLVM version 6.0 (clang-600.0.57) (based on LLVM 3.5svn); Target: x86_64-apple-darwin13.4.0; Thread model: posix; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1504#issuecomment-185809068:2599,Config,Configured,2599,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1504#issuecomment-185809068,1,['Config'],['Configured']
Modifiability,"_epochs=10 --max_training_epochs=50 --initial_temperature=1.500000e+00 --num_thermal_advi_iters=2500 --convergence_snr_averaging_window=500 --convergence_snr_trigger_threshold=1.000000e-01 --convergence_snr_countdown_window=10 --max_calling_iters=10 --caller_update_convergence_threshold=1.000000e-03 --caller_internal_admixing_rate=7.500000e-01 --caller_external_admixing_rate=1.000000e+00 --disable_caller=false --disable_sampler=false --disable_annealing=false; Stdout: 14:13:50.032 INFO cohort_denoising_calling - Loading 24 read counts file(s)...; 14:13:53.719 INFO gcnvkernel.io.io_metadata - Loading germline contig ploidy and global read depth metadata...; 14:13:58.626 INFO gcnvkernel.tasks.task_cohort_denoising_calling - Instantiating the denoising model (warm-up)...; 14:14:04.543 INFO gcnvkernel.models.fancy_model - Global model variables: {'W_tu', 'psi_t_log__', 'ard_u_log__', 'log_mean_bias_t'}; 14:14:04.544 INFO gcnvkernel.models.fancy_model - Sample-specific model variables: {'z_su', 'psi_s_log__', 'read_depth_s_log__'}; 14:14:04.544 WARNING gcnvkernel.tasks.inference_task_base - No log emission sampler given; skipping the sampling step; 14:14:04.544 WARNING gcnvkernel.tasks.inference_task_base - No caller given; skipping the calling step; 14:14:04.544 INFO gcnvkernel.tasks.inference_task_base - Instantiating the convergence tracker...; 14:14:04.544 INFO gcnvkernel.tasks.inference_task_base - Setting up DA-ADVI...; 14:14:10.902 INFO gcnvkernel.tasks.inference_task_base - (denoising (warm-up)) starting...: 0%| | 0/5000 [00:00<?, ?it/s]; 14:14:12.877 INFO gcnvkernel.tasks.inference_task_base - (denoising (warm-up) epoch 1) ELBO: N/A, SNR: N/A, T: 1.50: 0%| | 1/5000 [00:01<2:44:32, 1.97s/it]; 14:14:14.753 INFO gcnvkernel.tasks.inference_task_base - (denoising (warm-up) epoch 1) ELBO: -145.294 +/- 0.000, SNR: 35869952999211676.0, T: 1.50: 0%| | 2/5000 [00:03<2:40:21, 1.93s/it]; 14:14:16.609 INFO gcnvkernel.tasks.inference_task_base - (denoising (warm-up) epoch 1) E",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4825#issuecomment-467406398:3698,variab,variables,3698,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4825#issuecomment-467406398,1,['variab'],['variables']
Modifiability,"`FilterMutectCalls` has always and continues to filter multiallelics by default, except in mitochondria mode. You may adjust this with the `max-alt-allele-count` argument. `-max-alternate-alleles` is a `HaplotypeCaller` argument that appeared in `Mutect2` due to excess inheritance in the class hierarchy. I don't believe it ever had any effect in `Mutect2`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6603#issuecomment-629897864:270,inherit,inheritance,270,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6603#issuecomment-629897864,1,['inherit'],['inheritance']
Modifiability,"```; $ git remote show origin; fatal: 'origin' does not appear to be a git repository; fatal: Could not read from remote repository. Please make sure you have the correct access rights; and the repository exists.; $ cat .git/config ; [core]; 	repositoryformatversion = 0; 	filemode = true; 	bare = false; 	logallrefupdates = true; $; ```. Hmm, here is the full log, actually I see some shared library errors at the top. Grr, I have `ncurses-6` library only. Why doesn't the build system die immediately upon an error? Anyway, this is exactly why Gentoo does not like executing zillions of evil jar files and other executables. As I said in the past, your step away from Apache ant build system was a very bad decision. You can see in the log the git tag too. I am not sure if the build system used `master` instead of `gatk` branch. Is that a problem?. [build.log.txt](https://github.com/broadinstitute/gatk/files/1933626/build.log.txt)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687#issuecomment-383221183:225,config,config,225,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687#issuecomment-383221183,1,['config'],['config']
Modifiability,"aJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 ,spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 ,spark.kryoserializer.buffer.max=512m,spark.yarn.executor.memoryOverhead=600 --jar gs://hellbender-test-logs/staging/gatk-package-4.1.0.0-24-g18a95c7-SNAPSHOT-spark_3e9078b7e67707952fa12a0c5c4d2b71.jar -- PrintVariantsSpark --V gs://hellbender/test/resources/large/gvcfs/gatk3.7_30_ga4f720357.24_sample.21.expected.vcf --output gs://hellbender-test-logs/staging/12dc38b0-0b40-49d5-a98e-fe83ca658003.vcf --spark-master yarn; Job [654b5b8e01de4c60bd87d941d4ec8831] submitted.; Waiting for job output...; 19/02/18 16:58:03 WARN org.apache.spark.SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 16:58:09.526 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 16:58:09.705 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/tmp/654b5b8e01de4c60bd87d941d4ec8831/gatk-package-4.1.0.0-24-g18a95c7-SNAPSHOT-spark_3e9078b7e67707952fa12a0c5c4d2b71.jar!/com/intel/gkl/native/libgkl_compression.so; 16:58:10.112 INFO PrintVariantsSpark - ------------------------------------------------------------; 16:58:10.113 INFO PrintVariantsSpark - The Genome Analysis Toolkit (GATK) v4.1.0.0-24-g18a95c7-SNAPSHOT; 16:58:10.113 INFO PrintVariantsSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 16:58:10.113 INFO PrintVariantsSpark - E",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3840#issuecomment-464825765:1272,config,configuration,1272,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3840#issuecomment-464825765,1,['config'],['configuration']
Modifiability,"al String WIDTH_OF_BINS_LONG_NAME = ""binwidths"";; > +; > + public static final String PADDING_SHORT_NAME = ""pad"";; > + public static final String PADDING_LONG_NAME = ""padding"";; > +; > + @Argument(; > + doc = ""width of the bins"",; > + fullName = WIDTH_OF_BINS_LONG_NAME,; > + shortName = WIDTH_OF_BINS_SHORT_NAME,; > + optional = true,; > + minValue = 1; > + ); > + private int widthOfBins = 1;; >; > binWidth would be a more readable variable name. There's nothing wrong; > with the command line argument and the variable being identical.; > ------------------------------; >; > In src/main/java/org/broadinstitute/hellbender/tools/copynumber/; > CreateBinningIntervals.java; > <https://github.com/broadinstitute/gatk/pull/3597#discussion_r140646097>:; >; > > + doc = ""width of the bins"",; > + fullName = WIDTH_OF_BINS_LONG_NAME,; > + shortName = WIDTH_OF_BINS_SHORT_NAME,; > + optional = true,; > + minValue = 1; > + ); > + private int widthOfBins = 1;; > +; > + @Argument(; > + doc = ""width of the padding regions"",; > + fullName = PADDING_LONG_NAME,; > + shortName = PADDING_SHORT_NAME,; > + optional = true,; > + minValue = 0; > + ); > + private int padding = 0;; >; > This tool extends GATKTool, which means that it inherits an; > IntervalArgumentCollection that already includes a padding argument. A; > new one is not needed. BTW @samuelklee <https://github.com/samuelklee>; > does this come up elsewhere in the CNV code? It could be a holdover from; > the days of porting ReCapSeg when I feel we used to write more; > CommandLinePrograms.; > ------------------------------; >; > In src/main/java/org/broadinstitute/hellbender/tools/copynumber/; > CreateBinningIntervals.java; > <https://github.com/broadinstitute/gatk/pull/3597#discussion_r140646119>:; >; > > + createBins();; > + }; > +; > + /**; > + * Generates binning coverage in the intervals given by the user.; > + * The width of bins, the intervals and the output file's path are given by the user.; > + */; > + public void createBin",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3597#issuecomment-331744211:2816,extend,extends,2816,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3597#issuecomment-331744211,2,"['extend', 'inherit']","['extends', 'inherits']"
Modifiability,"am/md5/%s; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.REFERENCE_FASTA : null; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:39:19.226 DEBUG ConfigFactory - Configuration file values:; 17:39:19.244 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:39:19.244 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:39:19.245 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:39:19.245 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:39:19.245 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:39:19.245 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:39:19.245 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:39:19.245 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:39:19.245 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:39:19.245 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:39:19.245 DEBUG ConfigFactory - createOutputBamIndex = true; 17:39:19.245 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 17:39:19.245 INFO PathSeqPipeline",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:5391,Config,ConfigFactory,5391,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,1,['Config'],['ConfigFactory']
Modifiability,another executor: ; `Reading broadcast variable 3 took 34917 ms`; `Reading broadcast variable 4 took 121334 ms`. and another one:; `Reading broadcast variable 3 took 35354 ms`; `Reading broadcast variable 4 took 76535 ms`. Changing `spark.broadcast.blockSize` to 400M may slow it down:; `Reading broadcast variable 3 took 16235 ms`; `Reading broadcast variable 4 took 140846 ms`. And some with 10MB. ```; Reading broadcast variable 3 took 29083 ms; Reading broadcast variable 4 took 104675 ms; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1675#issuecomment-208658975:39,variab,variable,39,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1675#issuecomment-208658975,8,['variab'],['variable']
Modifiability,"ark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 --num-executors 20 --executor-cores 6 --executor-memory 6g /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/gatk-4.1.0.0/gatk-package-4.1.0.0-spark.jar CountReadsSpark --input /project/casa/gcad/adsp.cc/cram/A-ADC-AD010072-BL-NCR-11AD44210.hg38.realign.bqsr.cram --reference ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --spark-master yarn; 23:10:10.737 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 23:10:10.965 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/gatk-4.1.0.0/gatk-package-4.1.0.0-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 23:10:12.679 INFO CountReadsSpark - ------------------------------------------------------------; 23:10:12.680 INFO CountReadsSpark - The Genome Analysis Toolkit (GATK) v4.1.0.0; 23:10:12.680 INFO CountReadsSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 23:10:12.680 INFO CountReadsSpark - Executing as farrell@scc-hadoop.bu.edu on Linux v2.6.32-754.6.3.el6.x86_64 amd64; 23:10:12.681 INFO CountReadsSpark - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 23:10:12.681 INFO CountReadsSpark - Start Date/Time: February 5, 2019 11:10:10 PM EST; 23:10:12.681 INFO CountReadsSpark - -------------------------------------------------------",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-460895912:3169,variab,variables,3169,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-460895912,2,"['config', 'variab']","['configured', 'variables']"
Modifiability,"athSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:39:19.226 DEBUG ConfigFactory - Configuration file values:; 17:39:19.244 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:39:19.244 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:39:19.245 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:39:19.245 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:39:19.245 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:39:19.245 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:39:19.245 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:39:19.245 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:39:19.245 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:39:19.245 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:39:19.245 DEBUG ConfigFactory - createOutputBamIndex = true; 17:39:19.245 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 17:39:19.245 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 17:39:19.246 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 17:39:19.246 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 17:39:19.246 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from htt",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:5680,Config,ConfigFactory,5680,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,1,['Config'],['ConfigFactory']
Modifiability,"aults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.REFERENCE_FASTA : null; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:39:19.226 DEBUG ConfigFactory - Configuration file values:; 17:39:19.244 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:39:19.244 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:39:19.245 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:39:19.245 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:39:19.245 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:39:19.245 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:39:19.245 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:39:19.245 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:39:19.245 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:39:19.245 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:39:19.245 DEBUG ConfigFactory - createOutputBamIndex = true; 17:39:19.245 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:39:19.245 DEBUG ConfigFactory - samjdk.us",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:5324,Config,ConfigFactory,5324,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,1,['Config'],['ConfigFactory']
Modifiability,ava:648); 	at org.gradle.api.internal.file.copy.DefaultCopySpec$DefaultCopySpecResolver.walk(DefaultCopySpec.java:650); 	at org.gradle.api.internal.file.copy.DefaultCopySpec.walk(DefaultCopySpec.java:458); 	at org.gradle.api.internal.file.copy.CopySpecBackedCopyActionProcessingStream.process(CopySpecBackedCopyActionProcessingStream.java:38); 	at org.gradle.api.internal.file.copy.DuplicateHandlingCopyActionDecorator$1.process(DuplicateHandlingCopyActionDecorator.java:44); 	at org.gradle.api.internal.file.copy.NormalizingCopyActionDecorator$1.process(NormalizingCopyActionDecorator.java:57); 	at org.gradle.api.internal.file.copy.CopyActionProcessingStream$process.call(Unknown Source); 	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:48); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:113); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:125); 	at com.github.jengelman.gradle.plugins.shadow.tasks.ShadowCopyAction$1.execute(ShadowCopyAction.groovy:78); 	at com.github.jengelman.gradle.plugins.shadow.tasks.ShadowCopyAction$1$execute.call(Unknown Source); 	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:48); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:113); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:125); 	at com.github.jengelman.gradle.plugins.shadow.tasks.ShadowCopyAction.withResource(ShadowCopyAction.groovy:109); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:93); 	at org.codehaus.groovy.runtime.callsite.StaticMetaMethodSite$Static,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5499#issuecomment-446253445:4713,plugin,plugins,4713,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5499#issuecomment-446253445,1,['plugin'],['plugins']
Modifiability,b3e464b642?src=pr&el=desc) will **increase** coverage by `0.036%`.; > The diff coverage is `84.211%`. ```diff; @@ Coverage Diff @@; ## master #4960 +/- ##; ==============================================; + Coverage 80.784% 80.82% +0.036% ; - Complexity 17957 17967 +10 ; ==============================================; Files 1095 1095 ; Lines 64587 64600 +13 ; Branches 10392 10394 +2 ; ==============================================; + Hits 52176 52210 +34 ; + Misses 8388 8372 -16 ; + Partials 4023 4018 -5; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/4960?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...ools/funcotator/FuncotatorArgumentDefinitions.java](https://codecov.io/gh/broadinstitute/gatk/pull/4960/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9mdW5jb3RhdG9yL0Z1bmNvdGF0b3JBcmd1bWVudERlZmluaXRpb25zLmphdmE=) | `86.364% <ø> (ø)` | `1 <0> (ø)` | :arrow_down: |; | [...stitute/hellbender/utils/config/ConfigFactory.java](https://codecov.io/gh/broadinstitute/gatk/pull/4960/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9jb25maWcvQ29uZmlnRmFjdG9yeS5qYXZh) | `77.64% <100%> (+1.242%)` | `45 <0> (ø)` | :arrow_down: |; | [...titute/hellbender/tools/funcotator/Funcotator.java](https://codecov.io/gh/broadinstitute/gatk/pull/4960/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9mdW5jb3RhdG9yL0Z1bmNvdGF0b3IuamF2YQ==) | `90.556% <81.25%> (+4.927%)` | `53 <7> (+6)` | :arrow_up: |; | [...e/hellbender/tools/funcotator/FuncotatorUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4960/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9mdW5jb3RhdG9yL0Z1bmNvdGF0b3JVdGlscy5qYXZh) | `80.491% <0%> (+0.546%)` | `170% <0%> (+2%)` | :arrow_up: |; | [...nder/utils/runtime/StreamingProcessController.java](https://codecov.io/gh/broadinstitute/gatk/pull/4960/diff?src=,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4960#issuecomment-400812242:1267,config,config,1267,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4960#issuecomment-400812242,2,"['Config', 'config']","['ConfigFactory', 'config']"
Modifiability,"back to @tomwhite - some small refactor+code duplication, build and testing suggestions. I ran it and it works. Let's put it in soon.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1750#issuecomment-219536914:31,refactor,refactor,31,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1750#issuecomment-219536914,1,['refactor'],['refactor']
Modifiability,broadinstitute) (9aa31e4) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/72684d0fae3326398c80e2f47d78eeff1fcc14fe?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) (72684d0) will **decrease** coverage by `0.001%`.; > The diff coverage is `100.000%`. ```diff; @@ Coverage Diff @@; ## master #7851 +/- ##; ===============================================; - Coverage 86.948% 86.947% -0.001% ; Complexity 36927 36927 ; ===============================================; Files 2219 2219 ; Lines 173673 173674 +1 ; Branches 18755 18755 ; ===============================================; - Hits 151006 151005 -1 ; + Misses 16055 16054 -1 ; - Partials 6612 6615 +3 ; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/7851?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) | Coverage Δ | |; |---|---|---|; | [...rs/haplotypecaller/graphs/AdaptiveChainPruner.java](https://codecov.io/gh/broadinstitute/gatk/pull/7851/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9ncmFwaHMvQWRhcHRpdmVDaGFpblBydW5lci5qYXZh) | `97.368% <100.000%> (+0.035%)` | :arrow_up: |; | [.../hellbender/utils/python/PythonUnitTestRunner.java](https://codecov.io/gh/broadinstitute/gatk/pull/7851/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9weXRob24vUHl0aG9uVW5pdFRlc3RSdW5uZXIuamF2YQ==) | `75.410% <0.000%> (-3.279%)` | :arrow_down: |; | [...itute/hellbender/tools/LocalAssemblerUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/7851/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7851#issuecomment-1126424538:1373,Adapt,AdaptiveChainPruner,1373,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7851#issuecomment-1126424538,1,['Adapt'],['AdaptiveChainPruner']
Modifiability,builds are now failing due to the changes in the artifical reads generator...I'll make them more flexible so that I can still get more random reads without breaking the other tests....,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6280#issuecomment-559244663:97,flexible,flexible,97,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6280#issuecomment-559244663,1,['flexible'],['flexible']
Modifiability,"c 5 12:51:17 2017 -0500. Updates to handle SAM header changes from sl_wgs_acnv_headers and updates to mb_gcnv_python_kernel. commit d02d04df684a2820308a1d1c2bfda4b7d1c5f05e; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Mon Nov 13 12:52:33 2017 -0500. Added CLIs and WDL for python gCNV pipeline. commit 66ed74b68375d43514ef84658e7a6c771ed9053c; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Wed Nov 15 01:50:03 2017 -0500. Polished code, ready for review; ; gCNV computational kernel (initial release); ; renaming gammas_s to psi_s to uniformity (sample-specific unexplained variance); ; renamed determine_ploidy_and_depth.py to cohort_determine_ploidy_and_depth.py; finite-temperature forward-backward algorithm; in the ploidy model, replaced alpha_j (NB over-dispersion) with psi_j (unexplained variance) for uniformity. Also, added the possibility of sample-specific unexplained variance in the germline contig ploidy model; ; updated I/O routines and CLIs according to team discussion; ; updated I/O routines and CLIs according to team discussion; ; changed the output layout of the ploidy determination tool; refactored parts of io.py; upped the version to 0.3 as it is not backwards compatible anymore; ; case ploidy determination tool from a given ploidy model; major code cleanup and refactoring of I/O module; refactoring of common CLI script snippets; ; removed all ""targets""; some code cleanup; ; pad flat class bitmask w/ a given padding value in the hybrid q_c_expectation_mode; option to disable annealing and keep the temperature fixed; ; bugfix in finite-temperature forward-backward; further refactoring of model I/O; ; the option to take a previously trained model as starting point in cohort CLI; the option to take previous calls as a starting point in cohort CLI; ; option to save and load adamax moments; ; import/export adamax bias correction tensor; ; refactoring related to fancy opt I/O; added average ploidy column to read depth; updated docs of hybrid ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598:11083,refactor,refactored,11083,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598,1,['refactor'],['refactored']
Modifiability,c=pr&el=desc) will **increase** coverage by `0.012%`.; > The diff coverage is `86.42%`. ```diff; @@ Coverage Diff @@; ## master #5462 +/- ##; ===============================================; + Coverage 87.075% 87.087% +0.012% ; + Complexity 31334 31225 -109 ; ===============================================; Files 1921 1915 -6 ; Lines 144602 144079 -523 ; Branches 15951 15891 -60 ; ===============================================; - Hits 125912 125474 -438 ; + Misses 12896 12834 -62 ; + Partials 5794 5771 -23; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/5462?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...s/walkers/haplotypecaller/graphs/PathUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5462/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9ncmFwaHMvUGF0aFVuaXRUZXN0LmphdmE=) | `93.258% <ø> (-0.22%)` | `7 <0> (ø)` | |; | [...rs/haplotypecaller/graphs/AdaptiveChainPruner.java](https://codecov.io/gh/broadinstitute/gatk/pull/5462/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9ncmFwaHMvQWRhcHRpdmVDaGFpblBydW5lci5qYXZh) | `95.349% <100%> (ø)` | `16 <0> (ø)` | :arrow_down: |; | [...ller/readthreading/ReadThreadingGraphUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5462/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9yZWFkdGhyZWFkaW5nL1JlYWRUaHJlYWRpbmdHcmFwaFVuaXRUZXN0LmphdmE=) | `95.238% <100%> (+0.018%)` | `55 <0> (ø)` | :arrow_down: |; | [...rs/haplotypecaller/graphs/ChainPrunerUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5462/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9ncmFwaHMvQ2hhaW5QcnVuZXJVbml0VGVzdC5qYXZh) | `99.194% <100%> (-0.006%)` | `40 <0> (ø)` | |; | [...der/t,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5462#issuecomment-450062027:1281,Adapt,AdaptiveChainPruner,1281,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5462#issuecomment-450062027,1,['Adapt'],['AdaptiveChainPruner']
Modifiability,ch 52/65; 19:36:40.808 INFO  GenomicsDBImport - Done importing batch 53/65; 20:18:42.274 INFO  GenomicsDBImport - Done importing batch 54/65; 21:01:51.304 INFO  GenomicsDBImport - Done importing batch 55/65; 21:36:00.458 INFO  GenomicsDBImport - Done importing batch 56/65; 22:08:38.587 INFO  GenomicsDBImport - Done importing batch 57/65; 22:40:44.082 INFO  GenomicsDBImport - Done importing batch 58/65; 23:14:11.202 INFO  GenomicsDBImport - Done importing batch 59/65; 23:48:23.805 INFO  GenomicsDBImport - Done importing batch 60/65; 00:20:35.869 INFO  GenomicsDBImport - Done importing batch 61/65; 00:51:47.408 INFO  GenomicsDBImport - Done importing batch 62/65; 01:25:23.587 INFO  GenomicsDBImport - Done importing batch 63/65; 01:59:03.103 INFO  GenomicsDBImport - Done importing batch 64/65; Using GATK jar /share/pkg.7/gatk/[4.2.6.1/install/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar](http://4.2.6.1/install/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar) defined in environment variable GATK_LOCAL_JAR; Running:;     java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx150g -Xms16g -jar /share/pkg.7/gatk/[4.2.6.1/install/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar](http://4.2.6.1/install/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar) GenomicsDBImport --sample-name-map sample_map.chr3 --genomicsdb-workspace-path genomicsDB.rb.chr3 --genomicsdb-shared-posixfs-optimizations True --tmp-dir tmp --L chr3 --batch-size 50 --bypass-feature-reader --reader-threads 5 --merge-input-intervals --overwrite-existing-genomicsdb-workspace --consolidate; [farrell@scc-hadoop genomicsdb]$ ls genomicsDB.rb.chr3; __tiledb_workspace.tdb  chr3$1$198295559  vcfheader.vcf  vidmap.json. ```; It never indicates that it imported batch 65/65. No error and the  callset.json is missing which we found in chr4 to chr22. ;   ; ls genomicsDB.rb.chr4. __tiledb_workspace.tdb  callset.json  chr4$1$1902145,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1246785232:3638,variab,variable,3638,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1246785232,1,['variab'],['variable']
Modifiability,closing - refactoring to one WDL,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7112#issuecomment-786760463:10,refactor,refactoring,10,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7112#issuecomment-786760463,1,['refactor'],['refactoring']
Modifiability,"commend for all experiments going forward. One concern I have is that our cluster memory is already far larger than; the input size. In fact here each machine can fit the whole input; comfortably in RAM. This is not going to be the case for the full input.; Since it would take too long to iterate using the full input, it seems wise; instead to reduce both the input size and to keep a close eye on the amount; of memory we're using to make sure we're not going down a path that would; not be able to cope with the full input. On Wed, Nov 18, 2015 at 11:08 AM, droazen notifications@github.com wrote:. > I did some additional runs on the Broad cluster on a 14 GB bam, twice as; > large as the bam used in the plot above. This was with 60 cores, 4 cores; > per executor, and 16 GB of memory per executor. Results:; > ; > Broadcast (3 runs): 10m52.020s, 11m46.975s, 10m17.274s; > Sharded (3 runs): 19m33.310s, 13m36.466s, (died from out-of-memory error); > ; > Not sure what's going on here, but something about this configuration is; > favorable to Broadcast and unfavorable to Sharded. We should try to; > understand what and why.; > ; > Below are the commands I used to run each implementation on dataflow01,; > for reference:; > ; > spark-submit \; > --master yarn-client \; > --driver-memory 8G \; > --num-executors 16 \; > --executor-cores 4 \; > --executor-memory 16G \; > --conf spark.driver.maxResultSize=0 \; > --conf spark.driver.userClassPathFirst=true \; > --conf spark.executor.userClassPathFirst=true \; > --conf spark.io.compression.codec=lzf \; > --conf spark.yarn.executor.memoryOverhead=600 \; > $JAR BaseRecalibratorSpark \; > --input hdfs:///user/droazen/bqsr/CEUTrio.HiSeq.WGS.b37.NA12878.1m-130m.bam \; > --output bqsr_out_${1}.bam \; > -R hdfs:///user/droazen/bqsr/human_g1k_v37.2bit \; > --knownSites hdfs:///user/droazen/bqsr/dbsnp_138.b37.1m-130m.vcf \; > --joinStrategy BROADCAST \; > --apiKey $API_KEY \; > --sparkMaster yarn-client; > ; > spark-submit \; > --master yarn-c",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/995#issuecomment-157838734:2104,config,configuration,2104,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/995#issuecomment-157838734,1,['config'],['configuration']
Modifiability,"config file is good for convenience, but I find that they cause lots of confusion and errors because people forget their existence",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/508#issuecomment-100312587:0,config,config,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/508#issuecomment-100312587,1,['config'],['config']
Modifiability,"cotationFactory.createDefaultFuncotationsOnVariant(GencodeFuncotationFactory.java:499); 22 Jun 2023 14:54:27,163 DEBUG: 		at org.broadinstitute.hellbender.tools.funcotator.DataSourceFuncotationFactory.createFuncotations(DataSourceFuncotationFactory.java:217); 22 Jun 2023 14:54:27,164 DEBUG: 		at org.broadinstitute.hellbender.tools.funcotator.DataSourceFuncotationFactory.createFuncotations(DataSourceFuncotationFactory.java:182); 22 Jun 2023 14:54:27,166 DEBUG: 		at org.broadinstitute.hellbender.tools.funcotator.FuncotatorEngine.lambda$createFuncotationMapForVariant$0(FuncotatorEngine.java:152); 22 Jun 2023 14:54:27,167 DEBUG: 		at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197); 22 Jun 2023 14:54:27,168 DEBUG: 		at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:179); 22 Jun 2023 14:54:27,170 DEBUG: 		at java.base/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1625); 22 Jun 2023 14:54:27,171 DEBUG: 		at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509); 22 Jun 2023 14:54:27,172 DEBUG: 		at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499); 22 Jun 2023 14:54:27,174 DEBUG: 		at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:921); 22 Jun 2023 14:54:27,175 DEBUG: 		at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 22 Jun 2023 14:54:27,177 DEBUG: 		at java.base/java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:682); 22 Jun 2023 14:54:27,178 DEBUG: 		at org.broadinstitute.hellbender.tools.funcotator.FuncotatorEngine.createFuncotationMapForVariant(FuncotatorEngine.java:162); 22 Jun 2023 14:54:27,180 DEBUG: 		at com.github.discvrseq.walkers.ExtendedFuncotator.enqueueAndHandleVariant(ExtendedFuncotator.java:209); 22 Jun 2023 14:54:27,181 DEBUG: 		at org.broadinstitute.hellbender.tools.funcotator.Funcotator.apply(Funcotator.java:878); ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8363#issuecomment-1603412226:3423,Extend,ExtendedFuncotator,3423,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8363#issuecomment-1603412226,2,['Extend'],['ExtendedFuncotator']
Modifiability,"ctory (Picard) Asserts the validity for specified Illumina basecalling data.; CollectIlluminaBasecallingMetrics (Picard) Collects Illumina Basecalling metrics for a sequencing run.; CollectIlluminaLaneMetrics (Picard) Collects Illumina lane metrics for the given BaseCalling analysis directory.; ExtractIlluminaBarcodes (Picard) Tool determines the barcode for each read in an Illumina lane.; IlluminaBasecallsToFastq (Picard) Generate FASTQ file(s) from Illumina basecall read data. ...; ```. With this change it instead prints the gatk launcher help, which is not the intended result. ; ```; Usage template for all tools (uses --spark-runner LOCAL when used with a Spark tool); gatk AnyTool toolArgs. Usage template for Spark tools (will NOT work on non-Spark tools); gatk SparkTool toolArgs [ -- --spark-runner <LOCAL | SPARK | GCS> sparkArgs ]. Getting help; gatk --list Print the list of available tools. gatk Tool --help Print help on a particular tool. Configuration File Specification; --gatk-config-file PATH/TO/GATK/PROPERTIES/FILE. gatk forwards commands to GATK and adds some sugar for submitting spark jobs. --spark-runner <target> controls how spark tools are run; valid targets are:; LOCAL: run using the in-memory spark runner; SPARK: run using spark-submit on an existing cluster; --spark-master must be specified; --spark-submit-command may be specified to control the Spark submit command; arguments to spark-submit may optionally be specified after --; GCS: run using Google cloud dataproc; commands after the -- will be passed to dataproc; --cluster <your-cluster> must be specified after the --; spark properties and some common spark-submit parameters will be translated; to dataproc equivalents. --dry-run may be specified to output the generated command line without running it; --java-options 'OPTION1[ OPTION2=Y ... ]' optional - pass the given string of options to the; java JVM at runtime.; Java options MUST be passed inside a single string with space-separated values.; ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5541#issuecomment-449068030:1429,Config,Configuration,1429,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5541#issuecomment-449068030,2,"['Config', 'config']","['Configuration', 'config-file']"
Modifiability,"cuting as myname@ln14 on Linux 3.10.0-514.16.1.el7.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_112-b15; Version: 4.alpha.2-1125-g27b5190-SNAPSHOT; 16:55:20.229 INFO BwaAndMarkDuplicatesPipelineSpark - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 16:55:20.229 INFO BwaAndMarkDuplicatesPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:55:20.229 INFO BwaAndMarkDuplicatesPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - Deflater: IntelDeflater; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - Inflater: IntelInflater; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - Initializing engine; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.varia.NullAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312229998:4504,variab,variable,4504,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312229998,1,['variab'],['variable']
Modifiability,cy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvVHJhaW5WYXJpYW50QW5ub3RhdGlvbnNNb2RlbC5qYXZh) | `77.778% <0.000%> (-2.991%)` | :arrow_down: |; | [...bender/utils/runtime/AsynchronousStreamWriter.java](https://codecov.io/gh/broadinstitute/gatk/pull/8092?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9ydW50aW1lL0FzeW5jaHJvbm91c1N0cmVhbVdyaXRlci5qYXZh) | `81.633% <0.000%> (-2.041%)` | :arrow_down: |; | [...ct/CreateSomaticPanelOfNormalsIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/8092?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL211dGVjdC9DcmVhdGVTb21hdGljUGFuZWxPZk5vcm1hbHNJbnRlZ3JhdGlvblRlc3QuamF2YQ==) | `96.396% <0.000%> (-1.305%)` | :arrow_down: |; | [...stitute/hellbender/utils/config/ConfigFactory.java](https://codecov.io/gh/broadinstitute/gatk/pull/8092?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9jb25maWcvQ29uZmlnRmFjdG9yeS5qYXZh) | `73.750% <0.000%> (-1.250%)` | :arrow_down: |; | [...ools/walkers/annotator/VariantAnnotatorEngine.java](https://codecov.io/gh/broadinstitute/gatk/pull/8092?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Fubm90YXRvci9WYXJpYW50QW5ub3RhdG9yRW5naW5lLmphdmE=) | `86.260% <0.000%> (-0.999%)` | :arrow_down: |; | [...org/broadinstitute/hellbender/utils/MathUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/8092?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadins,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8092#issuecomment-1374581874:4557,config,config,4557,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8092#issuecomment-1374581874,2,"['Config', 'config']","['ConfigFactory', 'config']"
Modifiability,"d I think things look good from this perspective, at least. This tieout uses a subset of the Pf7 samples containing 300 cohort and 1683 case samples (which were indeed treated as a cohort-case cluster in the original Pf7 CNV genotyping analysis). ~4k genomic bins are covered. We compare this branch against 4.5.0.0, as well as this branch against itself (checking for reproducibility). Costs for this branch ($10.92) and 4.5.0.0 ($10.96) were quite comparable. Note that a small portion of these costs derives from Pf7-specific genotyping steps, which I did not bother to remove from the workflow. Runtime for the ploidy modeling and postprocessing steps were comparable. Interestingly, **runtime for the gCNV was ~20-25% longer with this branch than with 4.5.0.0, but memory usage fell by a factor of ~3 (~6GB to ~2GB)!** I am not sure if we could recoup the runtime with some more tweaking of the environment (perhaps double checking that optimized BLAS/MKL/etc. packages are properly used, changing environment variables/flags, etc.), but I think the decrease in memory usage is quite nice. Concordance was checked for the following quantities (4.5.0.0 is on the x-axis and this branch is on the y-axis in all plots below):. 1) Variational posterior means (`mu_*`) and standard deviations (`std_*`) for all analogous variables in the ploidy and gCNV models. There were some slight changes to the gCNV model in this branch (e.g., the functional form of the ARD prior was changed), which means some variables are no longer directly comparable. Furthermore, some variables (such as the bias factors W) are degenerate and cannot be immediately compared. Otherwise, there is good concordance between the remaining variables, e.g.:. ![image](https://github.com/broadinstitute/gatk/assets/11076296/614cf501-ca31-4199-badb-3194b7f78154); ![image](https://github.com/broadinstitute/gatk/assets/11076296/f615084d-d0bf-44e9-bcf5-98abd26ceb06); ![image](https://github.com/broadinstitute/gatk/assets/11076296/",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8561#issuecomment-2079695268:1085,variab,variables,1085,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8561#issuecomment-2079695268,1,['variab'],['variables']
Modifiability,"d be in a separate commit that we can just delete at the end, but that can get unwieldy if the files in the commit need to change as we go along. Alternatively you could isolate them into a separate directory. They should either be disabled or made dependent on a test method (see the `@Test` annotation properties `enabled` and `dependsOn`) that is easily toggled so they can be run locally, but don't run on the CI server. Otherwise the CI server build will always fail. In general, its really helpful to have the first commit in the PR contain the completely unmodified GATK3 source files. It makes it much easier for the reviewer to see what changed for the port. I noticed that you have 2 new plugins included in this. I'm not sure if that was suggested by someone on the GATK team (I'm wondering if we want to go down that path...) but I can tell you that the existing plugins required an enormous amount of test development and review iteration. If we do decide to make them plugins, I think it would be a good idea to do so in a separate PR. Also, if we choose to make an AbstractPlugin base class, we may want that to live in the Barclay repo. As @magicdgs points out, master already has your previous commits, so you should start by rebasing on that. Ideally, the branch would have the following commits before we start the first review cycle:. 1. A single commit containing the unmodified GATK3 source (unmodified with the exception that if a file is renamed for GATK4, its helpful to rename the GATK3 version in this commit so it's easy to compare in the next commit). This commit doesn't have to compile or run - its just to make the review process easier for us, and will be deleted at some point. I can help with how to get this into your branch if you like.; 2. Your modified GATK3 tests in a single commit. This will also be removed before merge.; 3. A single commit with all of your ""minimal"" changes for the port, including the real, new tests. This should compile, and tests should",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-407089352:1888,plugin,plugins,1888,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-407089352,1,['plugin'],['plugins']
Modifiability,"dGraph only allows a single annotation/track. I'm not sure if the track definition line is intended to hold any metadata other than display parameters, either? https://genome.ucsc.edu/goldenPath/help/bedgraph.html. As for the unmarked column header line, the reason I decided this would be useful in the CNV TSV formats is that it's very easy to throw the table into a pandas or R dataframe for quick analysis, where you can then use the column names to manipulate the table. Typically, pandas/R TSV loading methods let you specify the `@` comment character to strip the SAM header (although we recently ran into some trouble with this in https://github.com/broadinstitute/gatk/pull/581). Note that we *require* a single unmarked column header, which is easy enough to skip (in the case you don't want to use it) if you know it's there. On the other hand, one could argue that if we store the type of each column in the metadata, then any analysis code should technically use that to parse the table (rather than letting pandas/R automatically infer the type of each column). So a marked column header line would make quick analyses a bit more difficult (as users would need to write parsing code), but could encourage more careful downstream code practices. @SHuang-Broad Just to be clear, the way I originally used ""annotation"" refers to any quantity that could be represented by a single type in a column (not in the sense of variant annotation). If string types are allowed, this is indeed pretty flexible! All I care about extracting is the common functionality related to the fact that we have locatable columns. I think the concerns you raise about e.g. SV representation in VCF are a separate matter, but happy to discuss further. I think once we decide what the header needs to be able to represent and what it should look like, this problem is mostly solved. There may be some things to decide about e.g. representation of doubles, NaNs, etc. but I don't think we need to be too rigid here.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-480917329:1644,flexible,flexible,1644,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-480917329,1,['flexible'],['flexible']
Modifiability,"da_read_pipeline is the right spot. If it's OK, I suggest holding off for a little bit. That branch is next in queue to get merged. I plan to have it merged by next Friday. Until then, I'll be doing (hopefully small) cleanup. I don't expect any serious refactors, but it hasn't gone for review, so I can't promise anything. (Read: it probably won't change much, but I can't guarantee it, so using it may waste some of your time). Right after that goes in, I want to work with you Tom on a refactor that'll work well for everyone. There many also be other places where we're overly tied to Cloud Dataflow right now.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/567#issuecomment-120037353:253,refactor,refactors,253,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/567#issuecomment-120037353,2,['refactor'],"['refactor', 'refactors']"
Modifiability,"e GATK/Picard (`IndexFeatureFile `), but they would print inconsistent logs with the rest of my toolkits and they aren't overridable because the classes are final; thus, I would use a decorator over this tools to print the proper startup messages. After a while, I might implement a `VariantWalker`, which will require that I implement another layer (`MyVariantWalker`). Thus, I end up with a lot of naive classes implemented on top of the base walkers and wrappers around bundled GATK/Picard tools. This is very difficult to maintain, because if a change is done at the `CommandLineProgram` abstract class for the logging output (a new method, for example), I will need to update every naive class and wrapper if I bump the GATK version. In addition, extensions of my own toolkit (if any) would need to do the same, making the class-dependency tree so deep that it is difficult to follow (with GATK3, this problem was really driving me crazy when I tried to implement custom tools). On the other hand, there is another use case for the GATK itself: once barclay has a common class for CLP, GATK would be able to run directly Picard tools without the decorator; nevertheless, they will still need it for the log output. This also gives me the impression that the configuration for the CLP output should be at the barclay level, to be shared between Picard/GATK/downstream toolkits to be able to combine them. I think that a way of managing that woul be a new field in the CLP consisting on an interface/abstract class, `CommandLineStartupFormatter`, with the same CLP methods for this kind of operations, that will be passed to the CLP on construction (in `Main`) and defaults to whatever base class is chosen. This will allow custom toolkits to override in their `Main` the formatter and thus make consistent the output of every tool. Another option is to use directly something like the Spring framework, but I think that it is quite complicated for API users without knowledge of Spring (like me).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4101#issuecomment-382994646:2071,config,configuration,2071,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4101#issuecomment-382994646,1,['config'],['configuration']
Modifiability,"eCaller.java b/protected/gatk-tools-protected/src/main/java/org/broadinstitute/gatk/tools/walkers/haplotypecaller/HaplotypeCaller.java; index cf34b1fb4e..2ee5752f04 100644; --- a/protected/gatk-tools-protected/src/main/java/org/broadinstitute/gatk/tools/walkers/haplotypecaller/HaplotypeCaller.java; +++ b/protected/gatk-tools-protected/src/main/java/org/broadinstitute/gatk/tools/walkers/haplotypecaller/HaplotypeCaller.java; @@ -497,10 +497,11 @@ public class HaplotypeCaller extends ActiveRegionWalker<List<VariantContext>, In; protected boolean mergeVariantsViaLD = false;; ; @Advanced; - @Argument(fullName=""tryPhysicalPhasing"", shortName=""tryPhysicalPhasing"", doc=""If specified, we will add physical (read-based) phasing information"", required = false); - protected boolean tryPhysicalPhasing = false;; + @Argument(fullName=""doNotRunPhysicalPhasing"", shortName=""doNotRunPhysicalPhasing"", doc=""If specified, we will not try to add physical (read-based) phasing information"", required = false); + protected boolean doNotRunPhysicalPhasing = false;; ; - public static final String HAPLOTYPE_CALLER_PHASING_KEY = ""HCP"";; + public static final String HAPLOTYPE_CALLER_PHASING_ID_KEY = ""PID"";; + public static final String HAPLOTYPE_CALLER_PHASING_GT_KEY = ""PGT"";; ; // -----------------------------------------------------------------------------------------------; // arguments for debugging / developing the haplotype caller; @@ -634,12 +635,11 @@ public class HaplotypeCaller extends ActiveRegionWalker<List<VariantContext>, In; if ( emitReferenceConfidence() ) {; ; if (SCAC.genotypingOutputMode == GenotypingOutputMode.GENOTYPE_GIVEN_ALLELES); - throw new UserException.BadArgumentValue(""ERC/gt_mode"",""you cannot request reference confidence output and Genotyping Giving Alleles at the same time"");; + throw new UserException.BadArgumentValue(""ERC/gt_mode"",""you cannot request reference confidence output and GENOTYPE_GIVEN_ALLELES at the same time"");; ; SCAC.genotypeArgs.STANDARD_CONFIDENCE_FO",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5727#issuecomment-470679237:1232,extend,extends,1232,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5727#issuecomment-470679237,2,['extend'],['extends']
Modifiability,"e_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 ,spark.kryoserializer.buffer.max=512m,spark.yarn.executor.memoryOverhead=600 --jar gs://hellbender-test-logs/staging/gatk-package-4.1.0.0-24-g18a95c7-SNAPSHOT-spark_3e9078b7e67707952fa12a0c5c4d2b71.jar -- PrintVariantsSpark --V gs://hellbender/test/resources/large/gvcfs/gatk3.7_30_ga4f720357.24_sample.21.expected.vcf --output gs://hellbender-test-logs/staging/12dc38b0-0b40-49d5-a98e-fe83ca658003.vcf --spark-master yarn; Job [654b5b8e01de4c60bd87d941d4ec8831] submitted.; Waiting for job output...; 19/02/18 16:58:03 WARN org.apache.spark.SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 16:58:09.526 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 16:58:09.705 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/tmp/654b5b8e01de4c60bd87d941d4ec8831/gatk-package-4.1.0.0-24-g18a95c7-SNAPSHOT-spark_3e9078b7e67707952fa12a0c5c4d2b71.jar!/com/intel/gkl/native/libgkl_compression.so; 16:58:10.112 INFO PrintVariantsSpark - ------------------------------------------------------------; 16:58:10.113 INFO PrintVariantsSpark - The Genome Analysis Toolkit (GATK) v4.1.0.0-24-g18a95c7-SNAPSHOT; 16:58:10.113 INFO PrintVariantsSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 16:58:10.113 INFO PrintVariantsSpark - Executing as root@gatk-test-2495f43b-04fc-49e7-aa0a-7108cc876246-m on Linux v4.9.0-8-amd64 amd64; 16:58:10.114 INFO PrintVariantsSpark - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_181-8u181-b13-2~deb9u1-b13; 16:58:10.114 INFO PrintVariantsSpark - Start Date/Time: February 18, 2019 4:58:09 PM",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3840#issuecomment-464825765:1514,variab,variables,1514,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3840#issuecomment-464825765,2,"['config', 'variab']","['configured', 'variables']"
Modifiability,"eano/compile/__init__.py"", line 10, in <module>; from theano.compile.function_module import *; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/compile/function_module.py"", line 21, in <module>; import theano.compile.mode; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/compile/mode.py"", line 10, in <module>; import theano.gof.vm; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/vm.py"", line 662, in <module>; from . import lazylinker_c; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 42, in <module>; location = os.path.join(config.compiledir, 'lazylinker_ext'); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configparser.py"", line 333, in __get__; self.__set__(cls, val_str); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configparser.py"", line 344, in __set__; self.val = self.filter(val); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configdefaults.py"", line 1745, in filter_compiledir; "" '%s'. Check the permissions."" % path); ValueError: Unable to create the compiledir directory '/root/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-stretch-sid-x86_64-3.6.2-64'. Check the permissions. at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:126); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeArgs(PythonScriptExecutor.java:170); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeCommand(PythonScriptExecutor.java:79); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.checkPythonEnvironmentForPackage(PythonScriptExecutor.java:192); at org.broadinstitute.hellbender.tools.copynumber.DetermineGermlineContigPloidy.onStartup(DetermineGermlineContigPloidy.java:269); at org.broadinstitute.hell",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-496007081:5142,config,configdefaults,5142,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-496007081,1,['config'],['configdefaults']
Modifiability,ed[m[50D[1B[1m> :testOnPackagedReleaseJar > Executing test org...help.DocumentationGeneration[m[79D[1B[3A src/main/java/org/broadinstitute/hellbender/utils/variant/writers/SomaticGVCFWriter.java:4: error: package com.google.common.collect does not exist[0K; 2022-08-16T00:09:07.4435974Z src/main/java/org/broadinstitute/hellbender/cmdline/CommandLineProgram.java:479: error: cannot find symbol; 2022-08-16T00:09:07.4436105Z @VisibleForTesting; 2022-08-16T00:09:07.4436380Z symbol: class VisibleForTesting; 2022-08-16T00:09:07.4436641Z location: class CommandLineProgram; 2022-08-16T00:09:07.4436930Z src/main/java/org/broadinstitute/hellbender/engine/FeatureInput.java:120: error: cannot find symbol; 2022-08-16T00:09:07.4437094Z @VisibleForTesting; 2022-08-16T00:09:07.4437369Z symbol: class VisibleForTesting; 2022-08-16T00:09:07.4437519Z location: class FeatureInput<T>; 2022-08-16T00:09:07.4437725Z where T is a type-variable:; 2022-08-16T00:09:07.4437925Z T extends Feature declared in class FeatureInput; 2022-08-16T00:09:07.4438276Z src/main/java/org/broadinstitute/hellbender/tools/walkers/variantutils/PosteriorProbabilitiesUtils.java:251: error: cannot find symbol; 2022-08-16T00:09:07.4438417Z @VisibleForTesting; 2022-08-16T00:09:07.4438677Z symbol: class VisibleForTesting; 2022-08-16T00:09:07.4438873Z location: class PosteriorProbabilitiesUtils; 2022-08-16T00:09:07.4439223Z src/main/java/org/broadinstitute/hellbender/tools/walkers/variantutils/PosteriorProbabilitiesUtils.java:271: error: cannot find symbol; 2022-08-16T00:09:07.4439362Z @VisibleForTesting; 2022-08-16T00:09:07.4439618Z symbol: class VisibleForTesting; 2022-08-16T00:09:07.4439806Z location: class PosteriorProbabilitiesUtils; 2022-08-16T00:09:07.4465668Z src/main/java/org/broadinstitute/hellbender/cmdline/GATKPlugin/DefaultGATKVariantAnnotationArgumentCollection.java:3: error: package com.google.common.collect does not exist; 2022-08-16T00:09:07.4466113Z [done in 2417 ms]; 2022-08-16T00:09:07.4466222Z ,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217242480:20885,extend,extends,20885,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217242480,1,['extend'],['extends']
Modifiability,ed-websocket\9.0.35\tomcat-embed-websocket-9.0.35.jar;E:\repository\org\springframework\spring-web\5.2.6.RELEASE\spring-web-5.2.6.RELEASE.jar;E:\repository\org\springframework\spring-webmvc\5.2.6.RELEASE\spring-webmvc-5.2.6.RELEASE.jar;E:\repository\org\springframework\spring-aop\5.2.6.RELEASE\spring-aop-5.2.6.RELEASE.jar;E:\repository\org\springframework\spring-context\5.2.6.RELEASE\spring-context-5.2.6.RELEASE.jar;E:\repository\org\springframework\spring-expression\5.2.6.RELEASE\spring-expression-5.2.6.RELEASE.jar;E:\repository\org\mybatis\spring\boot\mybatis-spring-boot-starter\2.1.2\mybatis-spring-boot-starter-2.1.2.jar;E:\repository\org\mybatis\spring\boot\mybatis-spring-boot-autoconfigure\2.1.2\mybatis-spring-boot-autoconfigure-2.1.2.jar;E:\repository\org\mybatis\mybatis\3.5.4\mybatis-3.5.4.jar;E:\repository\org\mybatis\mybatis-spring\2.0.4\mybatis-spring-2.0.4.jar;E:\repository\mysql\mysql-connector-java\8.0.20\mysql-connector-java-8.0.20.jar;E:\repository\org\springframework\boot\spring-boot-configuration-processor\2.3.0.RELEASE\spring-boot-configuration-processor-2.3.0.RELEASE.jar;E:\repository\org\springframework\spring-core\5.2.6.RELEASE\spring-core-5.2.6.RELEASE.jar;E:\repository\org\springframework\spring-jcl\5.2.6.RELEASE\spring-jcl-5.2.6.RELEASE.jar;E:\repository\com\google\firebase\firebase-admin\6.8.1\firebase-admin-6.8.1.jar;E:\repository\com\google\api-client\google-api-client\1.25.0\google-api-client-1.25.0.jar;E:\repository\com\google\oauth-client\google-oauth-client\1.25.0\google-oauth-client-1.25.0.jar;E:\repository\com\google\http-client\google-http-client-jackson2\1.25.0\google-http-client-jackson2-1.25.0.jar;E:\repository\com\google\api-client\google-api-client-gson\1.25.0\google-api-client-gson-1.25.0.jar;E:\repository\com\google\http-client\google-http-client-gson\1.25.0\google-http-client-gson-1.25.0.jar;E:\repository\com\google\code\gson\gson\2.8.6\gson-2.8.6.jar;E:\repository\com\google\http-client\google-http-client\1.25.0\google-http-,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-635805233:5458,config,configuration-processor,5458,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-635805233,1,['config'],['configuration-processor']
Modifiability,"er and -consolidate; 2. With --bypass-feature-reader; 3. With --consolidate without --bypass-feature-reader (This ended up on a node with 384gb.) The other ran on 256GB nodes. . Test 2 ran the fastest with the lowest memory requirements (Wall clock 76 hours); Test 1 ran slower and required more memory 40-50% of 256GB (Wall Clock 94 hours); Test 3 ran initially faster with less memory than test 1 but by batch 65 it was using 75% of 384 GB. This job has not finished and appears stuck on importing batch 65. So the consolidate option appears to have a memory leak or using just requiring too much memory. The -consolidate option was the culprit. So rerunning chr1-3 with just the --bypass-feature-reader option (test2) ran fine without lots of memory being used. Below is the time output from chr1. The output shows the Maximum resident set size (kbytes): **2630440**. Using GATK jar /share/pkg.7/gatk/4.2.6.1/install/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar defined in environment variable GATK_LOCAL_JAR; ```; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx200g -Xms16g -jar /share/pkg.7/gatk/4.2.6.1/install/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar GenomicsDBImport --sample-name-map sample_map.chr1 --genomicsdb-workspace-path genomicsDB.rb.bypass.time.chr1 --genomicsdb-shared-posixfs-optimizations True --tmp-dir tmp --bypass-feature-reader --L chr1 --batch-size 50 --reader-threads 4 --overwrite-existing-genomicsdb-workspace; Command being timed: ""gatk --java-options -Xmx200g -Xms16g GenomicsDBImport --sample-name-map sample_map.chr1 --genomicsdb-workspace-path genomicsDB.rb.bypass.time.chr1 --genomicsdb-shared-posixfs-optimizations True --tmp-dir tmp --bypass-feature-reader --L chr1 --batch-size 50 --reader-threads 4 --overwrite-existing-genomicsdb-workspace""; User time (seconds): 270716.45; System time (seconds): 1723.34; Percent of CPU this job go",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1252598687:1474,variab,variable,1474,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1252598687,1,['variab'],['variable']
Modifiability,erArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/5544/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9SZWFkVGhyZWFkaW5nQXNzZW1ibGVyQXJndW1lbnRDb2xsZWN0aW9uLmphdmE=) | `94.118% <ø> (ø)` | `1 <0> (ø)` | :arrow_down: |; | [...kers/haplotypecaller/AssemblyBasedCallerUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/5544/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9Bc3NlbWJseUJhc2VkQ2FsbGVyVXRpbHMuamF2YQ==) | `76.923% <ø> (-0.946%)` | `34 <0> (-1)` | |; | [...walkers/haplotypecaller/HaplotypeCallerEngine.java](https://codecov.io/gh/broadinstitute/gatk/pull/5544/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9IYXBsb3R5cGVDYWxsZXJFbmdpbmUuamF2YQ==) | `78.425% <100%> (ø)` | `76 <0> (ø)` | :arrow_down: |; | [...rs/haplotypecaller/graphs/AdaptiveChainPruner.java](https://codecov.io/gh/broadinstitute/gatk/pull/5544/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9ncmFwaHMvQWRhcHRpdmVDaGFpblBydW5lci5qYXZh) | `95.349% <100%> (+0.111%)` | `16 <0> (ø)` | :arrow_down: |; | [...hellbender/tools/walkers/mutect/Mutect2Engine.java](https://codecov.io/gh/broadinstitute/gatk/pull/5544/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL211dGVjdC9NdXRlY3QyRW5naW5lLmphdmE=) | `90.173% <100%> (ø)` | `65 <0> (ø)` | :arrow_down: |; | [...otypecaller/HaplotypeCallerArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/5544/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9IYXBsb3R5cGVDYWxsZXJBcmd1bWVudENvbGxlY3Rpb24uamF2YQ==) | `100% <100%> (ø)` | `3 <1> (+1)` | :arrow_up: |; | [...r/tools/walkers/mutect/Mutect2Integrati,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5544#issuecomment-449424951:2272,Adapt,AdaptiveChainPruner,2272,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5544#issuecomment-449424951,1,['Adapt'],['AdaptiveChainPruner']
Modifiability,erce.invoke(StaticMetaMethodSite.java:151); 	at org.codehaus.groovy.runtime.callsite.StaticMetaMethodSite.callStatic(StaticMetaMethodSite.java:102); 	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallStatic(CallSiteArray.java:56); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callStatic(AbstractCallSite.java:194); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callStatic(AbstractCallSite.java:214); 	at com.github.jengelman.gradle.plugins.shadow.tasks.ShadowCopyAction.execute(ShadowCopyAction.groovy:75); 	at org.gradle.api.internal.file.copy.NormalizingCopyActionDecorator.execute(NormalizingCopyActionDecorator.java:53); 	at org.gradle.api.internal.file.copy.DuplicateHandlingCopyActionDecorator.execute(DuplicateHandlingCopyActionDecorator.java:42); 	at org.gradle.api.internal.file.copy.CopyActionExecuter.execute(CopyActionExecuter.java:40); 	at org.gradle.api.tasks.AbstractCopyTask.copy(AbstractCopyTask.java:174); 	at com.github.jengelman.gradle.plugins.shadow.tasks.ShadowJar.copy(ShadowJar.java:70); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.reflect.JavaMethod.invoke(JavaMethod.java:73); 	at org.gradle.api.internal.project.taskfactory.StandardTaskAction.doExecute(StandardTaskAction.java:46); 	at org.gradle.api.internal.project.taskfactory.StandardTaskAction.execute(StandardTaskAction.java:39); 	at org.gradle.api.internal.project.taskfactory.StandardTaskAction.execute(StandardTaskAction.java:26); 	at org.gradle.api.internal.AbstractTask$TaskActionWrapper.execute(AbstractTask.java:788); 	at org.gradle.api.internal.AbstractTask$TaskActionWrapper.execute(AbstractTask.java:755); 	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter$1.run(ExecuteActi,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5499#issuecomment-446253445:6739,plugin,plugins,6739,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5499#issuecomment-446253445,1,['plugin'],['plugins']
Modifiability,ering/Mutect2FilteringEngine.java](https://codecov.io/gh/broadinstitute/gatk/pull/5842/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL211dGVjdC9maWx0ZXJpbmcvTXV0ZWN0MkZpbHRlcmluZ0VuZ2luZS5qYXZh) | `97.115% <100%> (+0.057%)` | `43 <0> (ø)` | :arrow_down: |; | [.../mutect/filtering/M2FiltersArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/5842/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL211dGVjdC9maWx0ZXJpbmcvTTJGaWx0ZXJzQXJndW1lbnRDb2xsZWN0aW9uLmphdmE=) | `93.75% <100%> (+0.417%)` | `6 <0> (ø)` | :arrow_down: |; | [...kers/mutect/filtering/MinAlleleFractionFilter.java](https://codecov.io/gh/broadinstitute/gatk/pull/5842/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL211dGVjdC9maWx0ZXJpbmcvTWluQWxsZWxlRnJhY3Rpb25GaWx0ZXIuamF2YQ==) | `100% <100%> (ø)` | `7 <7> (?)` | |; | [...e/hellbender/utils/variant/GATKVCFHeaderLines.java](https://codecov.io/gh/broadinstitute/gatk/pull/5842/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy92YXJpYW50L0dBVEtWQ0ZIZWFkZXJMaW5lcy5qYXZh) | `94.886% <100%> (+0.029%)` | `11 <0> (ø)` | :arrow_down: |; | [...alkers/mutect/filtering/PolymorphicNuMTFilter.java](https://codecov.io/gh/broadinstitute/gatk/pull/5842/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL211dGVjdC9maWx0ZXJpbmcvUG9seW1vcnBoaWNOdU1URmlsdGVyLmphdmE=) | `88.235% <88.235%> (ø)` | `9 <9> (?)` | |; | [...lbender/utils/variant/GATKVariantContextUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/5842/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy92YXJpYW50L0dBVEtWYXJpYW50Q29udGV4dFV0aWxzLmphdmE=) | `87.309% <0%> (-0.306%)` | `244% <0%> (-2%)` | |; | ... and [10 more](https://codecov.io/gh/broadinstitute/gatk/pull/5842/diff?src=pr&el=tree-more) | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5842#issuecomment-477635851:3450,Polymorphi,PolymorphicNuMTFilter,3450,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5842#issuecomment-477635851,1,['Polymorphi'],['PolymorphicNuMTFilter']
Modifiability,"erval_list=/tmp/intervals9016836733228000464.tsv --contig_ploidy_prior_table=/home/n.liorni/snakemake_cnv_gatk/resources/contig_ploidy_priors.tsv --output_model_path=/home/n.liorni/snakemake_cnv_gatk/results/cnv/ploidy/ploidy-model; Stdout: 15:09:46.970 INFO cohort_determine_ploidy_and_depth - THEANO_FLAGS environment variable has been set to: device=cpu,floatX=float64,optimizer=fast_run,compute_test_value=ignore,openmp=true,blas.ldflags=-lmkl_rt,openmp_elemwise_minsize=10; 15:09:47.017 INFO gcnvkernel.structs.metadata - Generating intervals metadata...; 15:09:47.024 INFO gcnvkernel.tasks.task_cohort_ploidy_determination - Instantiating the germline contig ploidy determination model...; 15:09:50.320 INFO gcnvkernel.tasks.task_cohort_ploidy_determination - Instantiating the ploidy emission sampler...; 15:09:50.321 INFO gcnvkernel.tasks.task_cohort_ploidy_determination - Instantiating the ploidy caller...; 15:09:50.957 INFO gcnvkernel.models.fancy_model - Global model variables: {'psi_j_log__', 'mean_bias_j_lowerbound__'}; 15:09:50.957 INFO gcnvkernel.models.fancy_model - Sample-specific model variables: {'psi_s_log__'}; 15:09:50.957 INFO gcnvkernel.tasks.inference_task_base - Instantiating the convergence tracker...; 15:09:50.958 INFO gcnvkernel.tasks.inference_task_base - Setting up DA-ADVI...; 15:10:03.310 INFO gcnvkernel.tasks.inference_task_base - (denoising) starting...: 0%| | 0/1000 [00:00<?, ?it/s]; 15:10:03.410 INFO gcnvkernel.tasks.inference_task_base - (denoising epoch 1) ELBO: -1038.498 +/- 431.707, SNR: 71.3, T: 1.98: 8%|8 | 83/1000 [00:00<00:01, 826.53it/s]; 15:10:03.522 INFO gcnvkernel.tasks.inference_task_base - (denoising epoch 1) ELBO: -821.262 +/- 327.042, SNR: 38.9, T: 1.97: 17%|#6 | 166/1000 [00:00<00:01, 776.56it/s]; 15:10:03.636 INFO gcnvkernel.tasks.inference_task_base - (denoising epoch 1) ELBO: -727.432 +/- 277.971, SNR: 29.4, T: 1.95: 24%|##4 | 244/1000 [00:00<00:01, 732.12it/s]; 15:10:03.754 INFO gcnvkernel.tasks.inference_task_base - (deno",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7444#issuecomment-945753905:7364,variab,variables,7364,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7444#issuecomment-945753905,1,['variab'],['variables']
Modifiability,"es can be specified as:; ./. .. completely missing (""."" or ""./."", depending on ploidy); ./x .. partially missing (e.g., ""./0"" or "".|1"" but not ""./.""); . .. partially or completely missing; a .. all genotypes; b .. heterozygous genotypes failing two-tailed binomial test (example below); q .. select genotypes using -i/-e options; and the new genotype can be one of:; . .. missing (""."" or ""./."", keeps ploidy); 0 .. reference allele (e.g. 0/0 or 0, keeps ploidy); c:GT .. custom genotype (e.g. 0/0, 0, 0/1, m/M, overrides ploidy); m .. minor (the second most common) allele (e.g. 1/1 or 1, keeps ploidy); M .. major allele (e.g. 1/1 or 1, keeps ploidy); p .. phase genotype (0/1 becomes 0|1); u .. unphase genotype and sort by allele (1|0 becomes 0/1); Usage: bcftools +setGT [General Options] -- [Plugin Options]; Options:; run ""bcftools plugin"" for a list of common options. Plugin options:; -e, --exclude <expr> Exclude a genotype if true (requires -t q); -i, --include <expr> include a genotype if true (requires -t q); -n, --new-gt <type> Genotypes to set, see above; -t, --target-gt <type> Genotypes to change, see above. Example:; # set missing genotypes (""./."") to phased ref genotypes (""0|0""); bcftools +setGT in.vcf -- -t . -n 0p. # set missing genotypes with DP>0 and GQ>20 to ref genotypes (""0/0""); bcftools +setGT in.vcf -- -t q -n 0 -i 'GT=""."" && FMT/DP>0 && GQ>20'. # set partially missing genotypes to completely missing; bcftools +setGT in.vcf -- -t ./x -n . # set heterozygous genotypes to 0/0 if binom.test(nAlt,nRef+nAlt,0.5)<1e-3; bcftools +setGT in.vcf -- -t ""b:AD<1e-3"" -n 0. # force unphased heterozygous genotype if binom.test(nAlt,nRef+nAlt,0.5)>0.1; bcftools +setGT in.vcf -- -t ./x -n c:'m/M'; ```; I was always wondering if GATK will have a plugin interface where people can code their own using groovy, kotlin, javascript or python plugins to extend some of the functionality where developers may not reach immediately. Personally I use htsjdk extensively (and sometimes p",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8328#issuecomment-1556119501:1030,Plugin,Plugin,1030,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8328#issuecomment-1556119501,1,['Plugin'],['Plugin']
Modifiability,eshold 0.5 --pileup-detection-absolute-alt-depth 0.0 --pileup-detection-snp-adjacent-to-assembled-indel-range 5 --pileup-detection-bad-read-tolerance 0.0 --pileup-detection-pro; per-pair-read-badness true --pileup-detection-edit-distance-read-badness-threshold 0.08 --pileup-detection-chimeric-read-badness true --pileup-detection-template-mean-badness-threshold 0.0; --pileup-detection-template-std-badness-threshold 0.0 --bam-writer-type CALLED_HAPLOTYPES --dont-use-soft-clipped-bases false --override-fragment-softclip-check false --min-base-quality-s; core 10 --smith-waterman JAVA --max-mnp-distance 0 --force-call-filtered-alleles false --reference-model-deletion-quality 30 --soft-clip-low-quality-ends false --allele-informative-reads-o; verlap-margin 2 --smith-waterman-dangling-end-match-value 25 --smith-waterman-dangling-end-mismatch-penalty -50 --smith-waterman-dangling-end-gap-open-penalty -110 --smith-waterman-danglin; g-end-gap-extend-penalty -6 --smith-waterman-haplotype-to-reference-match-value 200 --smith-waterman-haplotype-to-reference-mismatch-penalty -150 --smith-waterman-haplotype-to-reference-ga; p-open-penalty -260 --smith-waterman-haplotype-to-reference-gap-extend-penalty -11 --smith-waterman-read-to-haplotype-match-value 10 --smith-waterman-read-to-haplotype-mismatch-penalty -15; --smith-waterman-read-to-haplotype-gap-open-penalty -30 --smith-waterman-read-to-haplotype-gap-extend-penalty -5 --flow-assembly-collapse-hmer-size 0 --flow-assembly-collapse-partial-mode; false --flow-filter-alleles false --flow-filter-alleles-qual-threshold 30.0 --flow-filter-alleles-sor-threshold 3.0 --flow-filter-lone-alleles false --flow-filter-alleles-debug-graphs fal; se --min-assembly-region-size 50 --max-assembly-region-size 300 --active-probability-threshold 0.002 --max-prob-propagation-distance 50 --force-active false --assembly-region-padding 100 -; -padding-around-indels 75 --padding-around-snps 20 --padding-around-strs 75 --max-extension-into-assembly-region-pa,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8574#issuecomment-1793390789:7749,extend,extend-penalty,7749,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8574#issuecomment-1793390789,3,['extend'],['extend-penalty']
Modifiability,feCycle.doStart(ContainerLifeCycle.java:105); at org.eclipse.jetty.server.handler.AbstractHandler.doStart(AbstractHandler.java:61); at org.eclipse.jetty.server.Server.doStart(Server.java:394); at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68); at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:1155); at org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer.start(NameNodeHttpServer.java:181); at org.apache.hadoop.hdfs.server.namenode.NameNode.startHttpServer(NameNode.java:885); at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:707); at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:953); at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:926); at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1692); at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNode(MiniDFSCluster.java:1314); at org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1083); at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:958); at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:890); at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:518); at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:477); at org.broadinstitute.hellbender.testutils.MiniClusterUtils.getMiniCluster(MiniClusterUtils.java:30); at org.broadinstitute.hellbender.testutils.MiniClusterUtils.getMiniCluster(MiniClusterUtils.java:38); at org.broadinstitute.hellbender.metrics.MetricsUtilsTest.setupMiniCluster(MetricsUtilsTest.java:24). Caused by:; java.lang.IllegalArgumentException: Invalid Java version 11.0.16.1; at org.eclipse.jetty.util.JavaVersion.parseJDK9(JavaVersion.java:71); at org.eclipse.jetty.util.JavaVersion.parse(JavaVersion.java:49); at org.eclipse.jetty.util.JavaVersion.<clinit>(JavaVersion.java:[43](https://github.com/broadinstitute/gatk/action,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8098#issuecomment-1320505279:2385,config,configureNameService,2385,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8098#issuecomment-1320505279,1,['config'],['configureNameService']
Modifiability,"g.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.varia.NullAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.varia.NullAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""NullAppender"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.varia.NullAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.varia.NullAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""NullAppender"".; log4j:ERROR A ""org.apache.log4j.varia.NullAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.varia.NullAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""NullAppender"".; ^C; ####################### Ctrl-C after 16 hours ##############; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312229998:6298,variab,variable,6298,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312229998,2,['variab'],['variable']
Modifiability,ge Δ | Complexity Δ | |; |---|---|---|---|; | [...r/tools/walkers/mutect/Mutect2FilteringEngine.java](https://codecov.io/gh/broadinstitute/gatk/pull/5566/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL211dGVjdC9NdXRlY3QyRmlsdGVyaW5nRW5naW5lLmphdmE=) | `80.743% <0%> (-4.581%)` | `89% <0%> (ø)` | |; | [...ute/hellbender/utils/test/FuncotatorTestUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/5566/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0Z1bmNvdGF0b3JUZXN0VXRpbHMuamF2YQ==) | `95.161% <0%> (-3.084%)` | `7% <0%> (+1%)` | |; | [...GATKPlugin/GATKReadFilterPluginDescriptorTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5566/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL0dBVEtQbHVnaW4vR0FUS1JlYWRGaWx0ZXJQbHVnaW5EZXNjcmlwdG9yVGVzdC5qYXZh) | `88.62% <0%> (-1.76%)` | `48% <0%> (+1%)` | |; | [...Plugin/GATKAnnotationPluginDescriptorUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5566/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL0dBVEtQbHVnaW4vR0FUS0Fubm90YXRpb25QbHVnaW5EZXNjcmlwdG9yVW5pdFRlc3QuamF2YQ==) | `88.235% <0%> (-1.43%)` | `58% <0%> (+1%)` | |; | [.../tools/walkers/haplotypecaller/RefVsAnyResult.java](https://codecov.io/gh/broadinstitute/gatk/pull/5566/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9SZWZWc0FueVJlc3VsdC5qYXZh) | `100% <0%> (ø)` | `3% <0%> (+1%)` | :arrow_up: |; | [...ools/walkers/annotator/VariantAnnotatorEngine.java](https://codecov.io/gh/broadinstitute/gatk/pull/5566/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Fubm90YXRvci9WYXJpYW50QW5ub3RhdG9yRW5naW5lLmphdmE=) | `91.304% <0%> (ø)` | `70% <0%> (ø)` | :arrow_down: |; | [...line/GATKPlugin/testpluggables/TestAnnotation.java](https:,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5566#issuecomment-452843310:1862,Plugin,Plugin,1862,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5566#issuecomment-452843310,1,['Plugin'],['Plugin']
Modifiability,git-blame says that this is quite old code by MdP 2013... and is difficult to recover the history since is before the maven refactoring. Here I would just apply amnesty and remove one of the tests; let's the lack of coverage do the talking. . Of the two test method names `testStartInMiddleWithBubble` seems the most plausible one given the code but I'm not 100% sure about that.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1081#issuecomment-166013743:124,refactor,refactoring,124,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1081#issuecomment-166013743,1,['refactor'],['refactoring']
Modifiability,gradle.api.internal.file.copy.NormalizingCopyActionDecorator$1.process(NormalizingCopyActionDecorator.java:57); 	at org.gradle.api.internal.file.copy.CopyActionProcessingStream$process.call(Unknown Source); 	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:48); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:113); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:125); 	at com.github.jengelman.gradle.plugins.shadow.tasks.ShadowCopyAction$1.execute(ShadowCopyAction.groovy:78); 	at com.github.jengelman.gradle.plugins.shadow.tasks.ShadowCopyAction$1$execute.call(Unknown Source); 	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:48); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:113); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:125); 	at com.github.jengelman.gradle.plugins.shadow.tasks.ShadowCopyAction.withResource(ShadowCopyAction.groovy:109); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:93); 	at org.codehaus.groovy.runtime.callsite.StaticMetaMethodSite$StaticMetaMethodSiteNoUnwrapNoCoerce.invoke(StaticMetaMethodSite.java:151); 	at org.codehaus.groovy.runtime.callsite.StaticMetaMethodSite.callStatic(StaticMetaMethodSite.java:102); 	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallStatic(CallSiteArray.java:56); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callStatic(AbstractCallSite.java:194); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callStatic(AbstractCallSite.java:214); 	at com.github.jen,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5499#issuecomment-446253445:5197,plugin,plugins,5197,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5499#issuecomment-446253445,1,['plugin'],['plugins']
Modifiability,hadowCopyAction.withResource(ShadowCopyAction.groovy:109); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:93); 	at org.codehaus.groovy.runtime.callsite.StaticMetaMethodSite$StaticMetaMethodSiteNoUnwrapNoCoerce.invoke(StaticMetaMethodSite.java:151); 	at org.codehaus.groovy.runtime.callsite.StaticMetaMethodSite.callStatic(StaticMetaMethodSite.java:102); 	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallStatic(CallSiteArray.java:56); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callStatic(AbstractCallSite.java:194); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callStatic(AbstractCallSite.java:214); 	at com.github.jengelman.gradle.plugins.shadow.tasks.ShadowCopyAction.execute(ShadowCopyAction.groovy:75); 	at org.gradle.api.internal.file.copy.NormalizingCopyActionDecorator.execute(NormalizingCopyActionDecorator.java:53); 	at org.gradle.api.internal.file.copy.DuplicateHandlingCopyActionDecorator.execute(DuplicateHandlingCopyActionDecorator.java:42); 	at org.gradle.api.internal.file.copy.CopyActionExecuter.execute(CopyActionExecuter.java:40); 	at org.gradle.api.tasks.AbstractCopyTask.copy(AbstractCopyTask.java:174); 	at com.github.jengelman.gradle.plugins.shadow.tasks.ShadowJar.copy(ShadowJar.java:70); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.reflect.JavaMethod.invoke(JavaMethod.java:73); 	at org.gradle.api.internal.project.taskfactory.StandardTask,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5499#issuecomment-446253445:6215,plugin,plugins,6215,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5499#issuecomment-446253445,1,['plugin'],['plugins']
Modifiability,hc2ljU29tYXRpY1Nob3J0TXV0YXRpb25zLmphdmE=) | `80.172% <ø> (ø)` | `19 <0> (ø)` | :arrow_down: |; | [...tute/hellbender/tools/AnnotatePairOrientation.java](https://codecov.io/gh/broadinstitute/gatk/pull/5551/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9Bbm5vdGF0ZVBhaXJPcmllbnRhdGlvbi5qYXZh) | `96.429% <ø> (ø)` | `8 <0> (ø)` | :arrow_down: |; | [...nder/tools/copynumber/utils/TagGermlineEvents.java](https://codecov.io/gh/broadinstitute/gatk/pull/5551/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL3V0aWxzL1RhZ0dlcm1saW5lRXZlbnRzLmphdmE=) | `100% <ø> (ø)` | `3 <0> (ø)` | :arrow_down: |; | [...t/java/org/broadinstitute/hellbender/MainTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5551/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9NYWluVGVzdC5qYXZh) | `85.714% <90.909%> (+2.787%)` | `15 <9> (+9)` | :arrow_up: |; | [...utils/smithwaterman/SmithWatermanIntelAligner.java](https://codecov.io/gh/broadinstitute/gatk/pull/5551/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9zbWl0aHdhdGVybWFuL1NtaXRoV2F0ZXJtYW5JbnRlbEFsaWduZXIuamF2YQ==) | `50% <0%> (-30%)` | `1% <0%> (-2%)` | |; | [...ender/tools/walkers/annotator/PolymorphicNuMT.java](https://codecov.io/gh/broadinstitute/gatk/pull/5551/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Fubm90YXRvci9Qb2x5bW9ycGhpY051TVQuamF2YQ==) | `92.593% <0%> (-3.704%)` | `8% <0%> (-1%)` | |; | [...r/tools/walkers/mutect/Mutect2IntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5551/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL211dGVjdC9NdXRlY3QySW50ZWdyYXRpb25UZXN0LmphdmE=) | `87.586% <0%> (-0.517%)` | `89% <0%> (-2%)` | |; | ... and [13 more](https://codecov.io/gh/broadinstitute/gatk/pull/5551/diff?src=pr&el=tree-more) | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5551#issuecomment-450184780:3447,Polymorphi,PolymorphicNuMT,3447,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5551#issuecomment-450184780,1,['Polymorphi'],['PolymorphicNuMT']
Modifiability,"http://docs.travis-ci.com/user/build-timeouts/#Build-times-out-because-no-output-was-received; use that to extend the waiting time.; On Aug 18, 2015 9:13 PM, ""JP Martin"" notifications@github.com wrote:. > Marking cloud test as ""todo"" for now so I can merge.; > ; > —; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/hellbender/pull/812#issuecomment-132408527; > .",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/812#issuecomment-132409715:107,extend,extend,107,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/812#issuecomment-132409715,1,['extend'],['extend']
Modifiability,"ies = 20; 17:39:19.244 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:39:19.245 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:39:19.245 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:39:19.245 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:39:19.245 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:39:19.245 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:39:19.245 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:39:19.245 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:39:19.245 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:39:19.245 DEBUG ConfigFactory - createOutputBamIndex = true; 17:39:19.245 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 17:39:19.245 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 17:39:19.246 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 17:39:19.246 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 17:39:19.246 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 17:39:19.246 INFO PathSeqPipelineSpark - Initializing engine; 17:39:19.246 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 18/04/24 17:39:19 INFO SparkContext: Running Spark version 2.2.0; 18/04/24 17:39:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes whe",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:6149,Config,ConfigFactory,6149,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,1,['Config'],['ConfigFactory']
Modifiability,"ility). Costs for this branch ($10.92) and 4.5.0.0 ($10.96) were quite comparable. Note that a small portion of these costs derives from Pf7-specific genotyping steps, which I did not bother to remove from the workflow. Runtime for the ploidy modeling and postprocessing steps were comparable. Interestingly, **runtime for the gCNV was ~20-25% longer with this branch than with 4.5.0.0, but memory usage fell by a factor of ~3 (~6GB to ~2GB)!** I am not sure if we could recoup the runtime with some more tweaking of the environment (perhaps double checking that optimized BLAS/MKL/etc. packages are properly used, changing environment variables/flags, etc.), but I think the decrease in memory usage is quite nice. Concordance was checked for the following quantities (4.5.0.0 is on the x-axis and this branch is on the y-axis in all plots below):. 1) Variational posterior means (`mu_*`) and standard deviations (`std_*`) for all analogous variables in the ploidy and gCNV models. There were some slight changes to the gCNV model in this branch (e.g., the functional form of the ARD prior was changed), which means some variables are no longer directly comparable. Furthermore, some variables (such as the bias factors W) are degenerate and cannot be immediately compared. Otherwise, there is good concordance between the remaining variables, e.g.:. ![image](https://github.com/broadinstitute/gatk/assets/11076296/614cf501-ca31-4199-badb-3194b7f78154); ![image](https://github.com/broadinstitute/gatk/assets/11076296/f615084d-d0bf-44e9-bcf5-98abd26ceb06); ![image](https://github.com/broadinstitute/gatk/assets/11076296/48570e53-024c-44b5-8835-3fd40b4c5866); ![image](https://github.com/broadinstitute/gatk/assets/11076296/99100e5d-05e2-4a5c-9d68-57db1b734029); ![image](https://github.com/broadinstitute/gatk/assets/11076296/abae09e1-70a5-4213-95a2-0cb10f9db192); ![image](https://github.com/broadinstitute/gatk/assets/11076296/ef68d0da-90df-4c4b-9802-97988a498280). 2) ... Will update more later!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8561#issuecomment-2079695268:1571,variab,variables,1571,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8561#issuecomment-2079695268,3,['variab'],['variables']
Modifiability,"ingframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:550); 	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:143); 	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:758); 	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:750); 	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:397); 	at org.springframework.boot.SpringApplication.run(SpringApplication.java:315); 	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1237); 	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1226); 	at com.luz.push.PushApplication.main(PushApplication.java:10). java.io.IOException: The Application Default Credentials are not available. They are available if running in Google Compute Engine. Otherwise, the environment variable GOOGLE_APPLICATION_CREDENTIALS must be defined pointing to a file defining the credentials. See https://developers.google.com/accounts/docs/application-default-credentials for more information.; 	at com.google.auth.oauth2.DefaultCredentialsProvider.getDefaultCredentials(DefaultCredentialsProvider.java:131); 	at com.google.auth.oauth2.GoogleCredentials.getApplicationDefault(GoogleCredentials.java:127); 	at com.google.auth.oauth2.GoogleCredentials.getApplicationDefault(GoogleCredentials.java:100); 	at com.luz.push.utils.GcmUtils.init(GcmUtils.java:31); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.springframework.beans.factory.annotation.InitDestroyAnnotationBeanPostProcessor$LifecycleElement.invoke(InitDestroyAnnotationBeanPostProcessor.java:389); 	at org.springf",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-635805233:28788,variab,variable,28788,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-635805233,1,['variab'],['variable']
Modifiability,"ission_sampling_median_rel_error=5.000000e-03 --max_advi_iter_first_epoch=5000 --max_advi_iter_subsequent_epochs=200 --min_training_epochs=10 --max_training_epochs=50 --initial_temperature=1.500000e+00 --num_thermal_advi_iters=2500 --convergence_snr_averaging_window=500 --convergence_snr_trigger_threshold=1.000000e-01 --convergence_snr_countdown_window=10 --max_calling_iters=10 --caller_update_convergence_threshold=1.000000e-03 --caller_internal_admixing_rate=7.500000e-01 --caller_external_admixing_rate=1.000000e+00 --disable_caller=false --disable_sampler=false --disable_annealing=false; Stdout: 14:13:50.032 INFO cohort_denoising_calling - Loading 24 read counts file(s)...; 14:13:53.719 INFO gcnvkernel.io.io_metadata - Loading germline contig ploidy and global read depth metadata...; 14:13:58.626 INFO gcnvkernel.tasks.task_cohort_denoising_calling - Instantiating the denoising model (warm-up)...; 14:14:04.543 INFO gcnvkernel.models.fancy_model - Global model variables: {'W_tu', 'psi_t_log__', 'ard_u_log__', 'log_mean_bias_t'}; 14:14:04.544 INFO gcnvkernel.models.fancy_model - Sample-specific model variables: {'z_su', 'psi_s_log__', 'read_depth_s_log__'}; 14:14:04.544 WARNING gcnvkernel.tasks.inference_task_base - No log emission sampler given; skipping the sampling step; 14:14:04.544 WARNING gcnvkernel.tasks.inference_task_base - No caller given; skipping the calling step; 14:14:04.544 INFO gcnvkernel.tasks.inference_task_base - Instantiating the convergence tracker...; 14:14:04.544 INFO gcnvkernel.tasks.inference_task_base - Setting up DA-ADVI...; 14:14:10.902 INFO gcnvkernel.tasks.inference_task_base - (denoising (warm-up)) starting...: 0%| | 0/5000 [00:00<?, ?it/s]; 14:14:12.877 INFO gcnvkernel.tasks.inference_task_base - (denoising (warm-up) epoch 1) ELBO: N/A, SNR: N/A, T: 1.50: 0%| | 1/5000 [00:01<2:44:32, 1.97s/it]; 14:14:14.753 INFO gcnvkernel.tasks.inference_task_base - (denoising (warm-up) epoch 1) ELBO: -145.294 +/- 0.000, SNR: 35869952999211676.0, T: 1.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4825#issuecomment-467406398:3556,variab,variables,3556,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4825#issuecomment-467406398,1,['variab'],['variables']
Modifiability,it was coming from the adam project - log4j was picking up that config file because it was the first log4j.properties file it could find. Fixed by providing our own,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1123#issuecomment-185537487:64,config,config,64,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1123#issuecomment-185537487,1,['config'],['config']
Modifiability,java:650); 	at org.gradle.api.internal.file.copy.DefaultCopySpec.walk(DefaultCopySpec.java:458); 	at org.gradle.api.internal.file.copy.CopySpecBackedCopyActionProcessingStream.process(CopySpecBackedCopyActionProcessingStream.java:38); 	at org.gradle.api.internal.file.copy.DuplicateHandlingCopyActionDecorator$1.process(DuplicateHandlingCopyActionDecorator.java:44); 	at org.gradle.api.internal.file.copy.NormalizingCopyActionDecorator$1.process(NormalizingCopyActionDecorator.java:57); 	at org.gradle.api.internal.file.copy.CopyActionProcessingStream$process.call(Unknown Source); 	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:48); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:113); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:125); 	at com.github.jengelman.gradle.plugins.shadow.tasks.ShadowCopyAction$1.execute(ShadowCopyAction.groovy:78); 	at com.github.jengelman.gradle.plugins.shadow.tasks.ShadowCopyAction$1$execute.call(Unknown Source); 	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:48); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:113); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:125); 	at com.github.jengelman.gradle.plugins.shadow.tasks.ShadowCopyAction.withResource(ShadowCopyAction.groovy:109); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:93); 	at org.codehaus.groovy.runtime.callsite.StaticMetaMethodSite$StaticMetaMethodSiteNoUnwrapNoCoerce.invoke(StaticMetaMethodSite.java:151); 	at org.codehaus.groovy.runtime.callsit,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5499#issuecomment-446253445:4822,plugin,plugins,4822,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5499#issuecomment-446253445,1,['plugin'],['plugins']
Modifiability,jdk1.8.0_121\jre\lib\ext\cldrdata.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\ext\dnsns.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\ext\jaccess.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\ext\jfxrt.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\ext\localedata.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\ext\nashorn.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\ext\sunec.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\ext\sunjce_provider.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\ext\sunmscapi.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\ext\sunpkcs11.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\ext\zipfs.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\javaws.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\jce.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\jfr.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\jfxswt.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\jsse.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\management-agent.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\plugin.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\resources.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\rt.jar;C:\project\push\target\classes;E:\repository\org\springframework\boot\spring-boot-starter-jdbc\2.3.0.RELEASE\spring-boot-starter-jdbc-2.3.0.RELEASE.jar;E:\repository\org\springframework\boot\spring-boot-starter\2.3.0.RELEASE\spring-boot-starter-2.3.0.RELEASE.jar;E:\repository\org\springframework\boot\spring-boot\2.3.0.RELEASE\spring-boot-2.3.0.RELEASE.jar;E:\repository\org\springframework\boot\spring-boot-autoconfigure\2.3.0.RELEASE\spring-boot-autoconfigure-2.3.0.RELEASE.jar;E:\repository\org\springframework\boot\spring-boot-starter-logging\2.3.0.RELEASE\spring-boot-starter-logging-2.3.0.RELEASE.jar;E:\repository\ch\qos\logback\logback-classic\1.2.3\logback-classic-1.2.3.jar;E:\repository\ch\qos\logback\logback-core\1.2.3\logback-core-1.2.3.jar;E:\repository\org\apache\logging\log4j\log4j-to-slf4j\2.13.2\log4j-to-slf4j-2.13.2.jar;E:\repository\org\apache\logging\log,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-635805233:1669,plugin,plugin,1669,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-635805233,1,['plugin'],['plugin']
Modifiability,"k/resources/contig_ploidy_priors.tsv --output_model_path=/home/n.liorni/snakemake_cnv_gatk/results/cnv/ploidy/ploidy-model; Stdout: 15:09:46.970 INFO cohort_determine_ploidy_and_depth - THEANO_FLAGS environment variable has been set to: device=cpu,floatX=float64,optimizer=fast_run,compute_test_value=ignore,openmp=true,blas.ldflags=-lmkl_rt,openmp_elemwise_minsize=10; 15:09:47.017 INFO gcnvkernel.structs.metadata - Generating intervals metadata...; 15:09:47.024 INFO gcnvkernel.tasks.task_cohort_ploidy_determination - Instantiating the germline contig ploidy determination model...; 15:09:50.320 INFO gcnvkernel.tasks.task_cohort_ploidy_determination - Instantiating the ploidy emission sampler...; 15:09:50.321 INFO gcnvkernel.tasks.task_cohort_ploidy_determination - Instantiating the ploidy caller...; 15:09:50.957 INFO gcnvkernel.models.fancy_model - Global model variables: {'psi_j_log__', 'mean_bias_j_lowerbound__'}; 15:09:50.957 INFO gcnvkernel.models.fancy_model - Sample-specific model variables: {'psi_s_log__'}; 15:09:50.957 INFO gcnvkernel.tasks.inference_task_base - Instantiating the convergence tracker...; 15:09:50.958 INFO gcnvkernel.tasks.inference_task_base - Setting up DA-ADVI...; 15:10:03.310 INFO gcnvkernel.tasks.inference_task_base - (denoising) starting...: 0%| | 0/1000 [00:00<?, ?it/s]; 15:10:03.410 INFO gcnvkernel.tasks.inference_task_base - (denoising epoch 1) ELBO: -1038.498 +/- 431.707, SNR: 71.3, T: 1.98: 8%|8 | 83/1000 [00:00<00:01, 826.53it/s]; 15:10:03.522 INFO gcnvkernel.tasks.inference_task_base - (denoising epoch 1) ELBO: -821.262 +/- 327.042, SNR: 38.9, T: 1.97: 17%|#6 | 166/1000 [00:00<00:01, 776.56it/s]; 15:10:03.636 INFO gcnvkernel.tasks.inference_task_base - (denoising epoch 1) ELBO: -727.432 +/- 277.971, SNR: 29.4, T: 1.95: 24%|##4 | 244/1000 [00:00<00:01, 732.12it/s]; 15:10:03.754 INFO gcnvkernel.tasks.inference_task_base - (denoising epoch 1) ELBO: -662.875 +/- 251.403, SNR: 23.4, T: 1.94: 32%|###1 | 318/1000 [00:00<00:00, 689.38it/s]; ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7444#issuecomment-945753905:7492,variab,variables,7492,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7444#issuecomment-945753905,1,['variab'],['variables']
Modifiability,"le>; from . import timeseries; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/pymc3/distributions/timeseries.py"", line 1, in <module>; import theano.tensor as tt; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/__init__.py"", line 66, in <module>; from theano.compile import (; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/compile/__init__.py"", line 10, in <module>; from theano.compile.function_module import *; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/compile/function_module.py"", line 21, in <module>; import theano.compile.mode; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/compile/mode.py"", line 10, in <module>; import theano.gof.vm; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/vm.py"", line 662, in <module>; from . import lazylinker_c; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 42, in <module>; location = os.path.join(config.compiledir, 'lazylinker_ext'); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configparser.py"", line 333, in __get__; self.__set__(cls, val_str); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configparser.py"", line 344, in __set__; self.val = self.filter(val); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configdefaults.py"", line 1745, in filter_compiledir; "" '%s'. Check the permissions."" % path); ValueError: Unable to create the compiledir directory '/root/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-stretch-sid-x86_64-3.6.2-64'. Check the permissions. at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:126); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeArgs(PythonScriptExecutor.java:170); at org.broadinstitute.hellbender.u",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-496007081:4769,config,config,4769,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-496007081,1,['config'],['config']
Modifiability,lizer.buffer.max=512m --conf spark.driver.maxResultSize=0 --conf spark.driver.userClassPathFirst=false --conf spark.io.compression.codec=lzf --conf spark.yarn.executor.memoryOverhead=600 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 /share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-spark.jar CountReadsSpark --input /project/casa/gcad/test/HG04302.alt_bwamem_GRCh38DH.20150718.GBR.low_coverage.cram --reference file:///restricted/projectnb/casa/wgs.hg38/pipelines/hc/cram.test/GRCh38_full_analysis_set_plus_decoy_hla.fa.gz --spark-master yarn; 2019-01-09 13:35:04 WARN SparkConf:66 - The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 2019-01-09 13:35:05 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 13:35:09.640 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 13:35:09.799 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 13:35:11.507 INFO CountReadsSpark - ------------------------------------------------------------; 13:35:11.508 INFO CountReadsSpark - The Genome Analysis Toolkit (GATK) v4.0.12.0; 13:35:11.508 INFO CountReadsSpark - For support and documentat,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:1703,config,configuration,1703,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,1,['config'],['configuration']
Modifiability,"ly braces can wait for a separate pass (we will want to do those in this PR though). If you're not sure what to include or not just ask. I like the idea of keeping the GATK3 tests working as we go along. We should make a clear distinction between the old and new tests though. Ideally the GATK3 tests would be in a separate commit that we can just delete at the end, but that can get unwieldy if the files in the commit need to change as we go along. Alternatively you could isolate them into a separate directory. They should either be disabled or made dependent on a test method (see the `@Test` annotation properties `enabled` and `dependsOn`) that is easily toggled so they can be run locally, but don't run on the CI server. Otherwise the CI server build will always fail. In general, its really helpful to have the first commit in the PR contain the completely unmodified GATK3 source files. It makes it much easier for the reviewer to see what changed for the port. I noticed that you have 2 new plugins included in this. I'm not sure if that was suggested by someone on the GATK team (I'm wondering if we want to go down that path...) but I can tell you that the existing plugins required an enormous amount of test development and review iteration. If we do decide to make them plugins, I think it would be a good idea to do so in a separate PR. Also, if we choose to make an AbstractPlugin base class, we may want that to live in the Barclay repo. As @magicdgs points out, master already has your previous commits, so you should start by rebasing on that. Ideally, the branch would have the following commits before we start the first review cycle:. 1. A single commit containing the unmodified GATK3 source (unmodified with the exception that if a file is renamed for GATK4, its helpful to rename the GATK3 version in this commit so it's easy to compare in the next commit). This commit doesn't have to compile or run - its just to make the review process easier for us, and will be delete",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-407089352:1604,plugin,plugins,1604,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-407089352,1,['plugin'],['plugins']
Modifiability,"ly. I am busy with graduation in recent days, and will work on visa application for the further postdoc position at Harvard Medical School. Thank you for the interests in our tool, MosaicHunter. The option you suggest looks great. Do you mean that I should establish the GATK 4 developing environment and develop the MosaicHunterFilter tool? I may do that when I have some time. I found the document of GATK 4 at https://github.com/broadinstitute/gatk. Do you have any further advices?. Best regards,; Adam Yongxin Ye; Center for Bioinformatics; Peking University. At 2018-07-07 01:43:05, ""Geraldine Van der Auwera"" <notifications@github.com> wrote:. Hi @Yyx2626, I'm Geraldine, you may remember me from the Beijing training. It was great visiting your team! I'm sorry it took me so long to follow up on this discussion, and I want to thank you again for reaching out to us about integrating the tool that you developed into GATK. We are certainly very interested in providing this enhancement to the research community, and we are now ready to talk about the next steps. After examining your paper and the source code in Github, we think that the most efficient way to integrate the functionality you developed would be to adapt the filtering parts of your tool to run on the output of Mutect2. So this would be a standalone tool that you would run after Mutect2, much like the current FilterMutectCalls tool. If the results are comparable to your current tool, then we would take that into the official distribution of GATK. If somehow that integration does not yield satisfactory results, then we would look at integrating the entire tool, though we're hoping it won't be necessary, so we can avoid maintaining duplicate functionality for some of the boilerplate data transformations. David @davidbenjamin can provide some advice on how to implement this in GATK4; in brief you would need to write some code that applies the filters you developed to a variant context. Let us know if this is an opt",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4632#issuecomment-404104349:1033,enhance,enhancement,1033,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4632#issuecomment-404104349,1,['enhance'],['enhancement']
Modifiability,"makeBefore/makeAfter seems more flexible in that it makes before/after a property of the way the transformer is applied, rather than of the transformer itself. When we write the plugin descriptor, it can maintain two separate argument lists (for ""--preFilterTransformer ..."" and ""--postFilterTransformer ...""), and then merge them accordingly. It does complicate the tool structure, but its the price for flexibility, and the pattern is not that complex. I think we should be explicit about what ""pre"" and ""post"" are relative to in the method and argument names, i.e., makePreReadFilterTransformer and makePostReadFilterTransformer, or maybe even one makeReadFilterTransformer method that takes a pre/post argument.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2085#issuecomment-246000255:32,flexible,flexible,32,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2085#issuecomment-246000255,2,"['flexible', 'plugin']","['flexible', 'plugin']"
Modifiability,"mbler.java:4: error: package com.google.common.collect does not exist; 2022-08-16T00:09:07.3891049Z src/main/java/org/broadinstitute/hellbender/utils/variant/writers/GVCFBlockCombiner.java:32: error: cannot find symbol; 2022-08-16T00:09:07.3891593Z final RangeMap<Integer, Range<Integer>> gqPartitions;; 2022-08-16T00:09:07.3892257Z symbol: class RangeMap; 2022-08-16T00:09:07.3892601Z location: class GVCFBlockCombiner; 2022-08-16T00:09:07.3893126Z src/main/java/org/broadinstitute/hellbender/utils/variant/writers/GVCFBlockCombiner.java:32: error: cannot find symbol; 2022-08-16T00:09:07.3893670Z final RangeMap<Integer, Range<Integer>> gqPartitions;; 2022-08-16T00:09:07.3894352Z symbol: class Range; 2022-08-16T00:09:07.3894678Z location: class GVCFBlockCombiner; 2022-08-16T00:09:07.3897711Z src/main/java/org/broadinstitute/hellbender/utils/variant/writers/GVCFBlockCombiner.java:62: error: cannot find symbol; 2022-08-16T00:09:07.3902203Z RangeMap<Integer,Range<Integer>> parsePartitions(final List<? extends Number> gqPartitions) ***; 2022-08-16T00:09:07.3902980Z symbol: class RangeMap; 2022-08-16T00:09:07.3903340Z location: class GVCFBlockCombiner; 2022-08-16T00:09:07.3903864Z src/main/java/org/broadinstitute/hellbender/utils/variant/writers/GVCFBlockCombiner.java:62: error: cannot find symbol; 2022-08-16T00:09:07.3904505Z RangeMap<Integer,Range<Integer>> parsePartitions(final List<? extends Number> gqPartitions) ***; 2022-08-16T00:09:07.3905250Z symbol: class Range; 2022-08-16T00:09:07.3905751Z location: class GVCFBlockCombiner; 2022-08-16T00:09:07.3906273Z src/main/java/org/broadinstitute/hellbender/utils/variant/writers/GVCFBlockCombiner.java:101: error: cannot find symbol; 2022-08-16T00:09:07.3906908Z static VCFHeaderLine rangeToVCFHeaderLine(Range<Integer> genotypeQualityBand) ***; 2022-08-16T00:09:07.3907793Z symbol: class Range; 2022-08-16T00:09:07.3908125Z location: class GVCFBlockCombiner; 2022-08-16T00:09:07.3910592Z src/main/java/org/broadinstitute/hellbender/to",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217242480:6286,extend,extends,6286,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217242480,1,['extend'],['extends']
Modifiability,"mbler.java:4: error: package com.google.common.collect does not exist; 2022-08-16T22:45:53.7690890Z src/main/java/org/broadinstitute/hellbender/utils/variant/writers/GVCFBlockCombiner.java:32: error: cannot find symbol; 2022-08-16T22:45:53.7738985Z final RangeMap<Integer, Range<Integer>> gqPartitions;; 2022-08-16T22:45:53.7739852Z symbol: class RangeMap; 2022-08-16T22:45:53.7740332Z location: class GVCFBlockCombiner; 2022-08-16T22:45:53.7740892Z src/main/java/org/broadinstitute/hellbender/utils/variant/writers/GVCFBlockCombiner.java:32: error: cannot find symbol; 2022-08-16T22:45:53.7741707Z final RangeMap<Integer, Range<Integer>> gqPartitions;; 2022-08-16T22:45:53.7743523Z symbol: class Range; 2022-08-16T22:45:53.7743866Z location: class GVCFBlockCombiner; 2022-08-16T22:45:53.7747579Z src/main/java/org/broadinstitute/hellbender/utils/variant/writers/GVCFBlockCombiner.java:62: error: cannot find symbol; 2022-08-16T22:45:53.7748444Z RangeMap<Integer,Range<Integer>> parsePartitions(final List<? extends Number> gqPartitions) ***; 2022-08-16T22:45:53.7776218Z symbol: class RangeMap; 2022-08-16T22:45:53.7776715Z location: class GVCFBlockCombiner; 2022-08-16T22:45:53.7777389Z src/main/java/org/broadinstitute/hellbender/utils/variant/writers/GVCFBlockCombiner.java:62: error: cannot find symbol; 2022-08-16T22:45:53.7778220Z RangeMap<Integer,Range<Integer>> parsePartitions(final List<? extends Number> gqPartitions) ***; 2022-08-16T22:45:53.7779110Z symbol: class Range; 2022-08-16T22:45:53.7779574Z location: class GVCFBlockCombiner; 2022-08-16T22:45:53.7780209Z src/main/java/org/broadinstitute/hellbender/utils/variant/writers/GVCFBlockCombiner.java:101: error: cannot find symbol; 2022-08-16T22:45:53.7780965Z static VCFHeaderLine rangeToVCFHeaderLine(Range<Integer> genotypeQualityBand) ***; 2022-08-16T22:45:53.7781896Z symbol: class Range; 2022-08-16T22:45:53.7782232Z location: class GVCFBlockCombiner; 2022-08-16T22:45:53.7785096Z src/main/java/org/broadinstitute/hellbender/to",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217253370:8324,extend,extends,8324,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217253370,1,['extend'],['extends']
Modifiability,"ms that we will want to run the filter with more stringent parameters, as higher base error rates are causing homs to leak past the filter, which in turn affects the fit of the allele-fraction model (which only attempts to model hets) by biasing normal segments towards unbalanced, and 2) we now want to run ModelSegments separately on the normal to allow for the filtering of germline events. So we want to be more stringent with low-coverage normals without affecting our high-coverage tumors. For example, here's some hg38 NovaSeq FFPE WGS data from a ~40x normal:. ![download](https://user-images.githubusercontent.com/11076296/43977946-9bd0a1bc-9cb3-11e8-9d7f-016a99c1c173.png). Compare to an hg19 TCGA WGS ~40x normal:. ![download 1](https://user-images.githubusercontent.com/11076296/43978051-f8820770-9cb3-11e8-8e16-13b51792614f.png). The hom-ref tail in the first plot is much fatter and clearly leaks into the het cloud. Also curious is that the het cloud is far less binomial (or even beta-binomial---note also the absence of the tail extending to the origin). I am still not sure why the incoming data looks different. There are several confounding factors: NovaSeq vs. HiSeq, hg38 vs. hg19, AF > 2% gnomAD sites vs. AF > 10% 1000G sites, FFPE vs. frozen, etc. I have not seen enough examples/combinations to be able to say which are the most important factors. Changing the genotyping/filtering strategy can get around this change in the data without a corresponding change in the allele-fraction model for now, but getting the data to look as good as possible upstream would be even better. Another thought: would be nice if the strategy was easily compatible with an eventual implementation of multi-sample segmentation, which would require that the same sites are used in both the tumor and the normal. We would want to strike a balance between maximizing the number of sites and including questionable sites from the normal. Will add more details later. @davidbenjamin @LeeTL1220 @eit",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3915#issuecomment-412189218:1597,extend,extending,1597,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3915#issuecomment-412189218,1,['extend'],['extending']
Modifiability,"n Tools; #; # Only update this environment if there is a *VERY* good reason to do so!; # If the build is broken but could be fixed by doing something else, then do that thing instead.; # Ensuring the correct environment for canonical (or otherwise reasonable) usage of our standard Docker takes precedence over edge cases.; # If you break the environment, you are responsible for fixing it and also owe the last developer who left this in a reasonable state a beverage of their choice.; # (This may be yourself, and you'll appreciate that beverage while you tinker with dependencies!); #; # When changing dependencies or versions in this file, check to see if the ""supportedPythonPackages"" DataProvider; # used by the testGATKPythonEnvironmentPackagePresent test in PythonEnvironmentIntegrationTest needs to be updated; # to reflect the changes.; #; name: gatk; channels:; # if channels other than conda-forge are added and the channel order is changed (note that conda channel_priority is currently set to flexible),; # verify that key dependencies are installed from the correct channel and compiled against MKL; - conda-forge; - defaults; dependencies:. # core python dependencies; - conda-forge::python=3.6.10 # do not update; - pip=20.0.2 # specifying channel may cause a warning to be emitted by conda; - conda-forge::mkl=2019.5 # MKL typically provides dramatic performance increases for theano, tensorflow, and other key dependencies; - conda-forge::mkl-service=2.3.0; - conda-forge::numpy=1.17.5 # do not update, this will break scipy=0.19.1; # verify that numpy is compiled against MKL (e.g., by checking *_mkl_info using numpy.show_config()); # and that it is used in tensorflow, theano, and other key dependencies; - conda-forge::theano=1.0.4 # it is unlikely that new versions of theano will be released; # verify that this is using numpy compiled against MKL (e.g., by the presence of -lmkl_rt in theano.config.blas.ldflags); - defaults::tensorflow=1.15.0 # update only if absolutely nec",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6656#issuecomment-643526868:1270,flexible,flexible,1270,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6656#issuecomment-643526868,1,['flexible'],['flexible']
Modifiability,ncotator/funcotator_dataSources.v1.6.20190124s/gencode_xrefseq/hg38/gencode_xrefseq_v90_38.tsv; > 12:28:17.939 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/hgnc_download_Nov302017.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/hgnc/hg38/hgnc_download_Nov302017.tsv; > 12:28:17.939 INFO Funcotator - Finalizing data sources (this step can be long if data sources are cloud-based)...; > 12:28:17.940 INFO DataSourceUtils - Setting lookahead cache for data source: chr1_b_bed : 100000; > 12:28:17.951 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/chr1_b_bed.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/chr1_b_bed/hg38/chr1_b_bed.tsv; > 12:28:17.967 INFO FeatureManager - Using codec XsvLocatableTableCodec to read file file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/chr1_b_bed/hg38/chr1_b_bed.config; > 12:28:17.995 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/chr1_b_bed.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/chr1_b_bed/hg38/chr1_b_bed.tsv; > 12:28:17.997 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/chr1_b_bed.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/chr1_b_bed/hg38/chr1_b_bed.tsv; > WARNING 2020-07-21 12:28:17 AsciiLineReader Creating an indexable source for an AsciiFeatureCodec using a stream that is neither a PositionalBufferedStream nor a BlockCompressedInputStream; > 12:28:18.002 INFO DataSourceUtils - Setting lookahead cache for data source: Oreganno : 100000; > 12:28:18.009 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/oreganno.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/oreganno/hg38/oreganno.tsv; > 12:28:18.020 INFO Fe,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6708#issuecomment-661776975:8858,config,config,8858,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6708#issuecomment-661776975,1,['config'],['config']
Modifiability,"nd other key dependencies; - conda-forge::theano=1.0.4 # it is unlikely that new versions of theano will be released; # verify that this is using numpy compiled against MKL (e.g., by the presence of -lmkl_rt in theano.config.blas.ldflags); - defaults::tensorflow=1.15.0 # update only if absolutely necessary, as this may cause conflicts with other core dependencies; # verify that this is using numpy compiled against MKL (e.g., by checking tensorflow.pywrap_tensorflow.IsMklEnabled()); - conda-forge::scipy=1.0.0 # do not update, this will break a scipy.misc.logsumexp import (deprecated in scipy=1.0.0) in pymc3=3.1; - conda-forge::pymc3=3.1 # do not update, this will break gcnvkernel; - conda-forge::keras=2.2.4 # updated from pip-installed 2.2.0, which caused various conflicts/clobbers of conda-installed packages; # conda-installed 2.2.4 appears to be the most recent version with a consistent API and without conflicts/clobbers; # if you wish to update, note that versions of conda-forge::keras after 2.2.5; # undesirably set the environment variable KERAS_BACKEND = theano by default; - defaults::intel-openmp=2019.4; - conda-forge::scikit-learn=0.22.2; - conda-forge::matplotlib=3.2.1; - conda-forge::pandas=1.0.3. # core R dependencies; these should only be used for plotting and do not take precedence over core python dependencies!; - r-base=3.6.2; - r-data.table=1.12.8; - r-dplyr=0.8.5; - r-getopt=1.20.3; - r-ggplot2=3.3.0; - r-gplots=3.0.3; - r-gsalib=2.1; - r-optparse=1.6.4. # other python dependencies; these should be removed after functionality is moved into Java code; - biopython=1.76; - pyvcf=0.6.8; - bioconda::pysam=0.15.3 # using older conda-installed versions may result in libcrypto / openssl bugs. # pip installs should be avoided, as pip may not respect the dependencies found by the conda solver; - pip:; - gatkPythonPackageArchive.zip; ```. It seems to successfully create the environment. I'd still recommend updating the information on your README.md and the file.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6656#issuecomment-643526868:3013,variab,variable,3013,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6656#issuecomment-643526868,1,['variab'],['variable']
Modifiability,"nd the channel order is changed (note that conda channel_priority is currently set to flexible),; # verify that key dependencies are installed from the correct channel and compiled against MKL; - conda-forge; - defaults; dependencies:. # core python dependencies; - conda-forge::python=3.6.10 # do not update; - pip=20.0.2 # specifying channel may cause a warning to be emitted by conda; - conda-forge::mkl=2019.5 # MKL typically provides dramatic performance increases for theano, tensorflow, and other key dependencies; - conda-forge::mkl-service=2.3.0; - conda-forge::numpy=1.17.5 # do not update, this will break scipy=0.19.1; # verify that numpy is compiled against MKL (e.g., by checking *_mkl_info using numpy.show_config()); # and that it is used in tensorflow, theano, and other key dependencies; - conda-forge::theano=1.0.4 # it is unlikely that new versions of theano will be released; # verify that this is using numpy compiled against MKL (e.g., by the presence of -lmkl_rt in theano.config.blas.ldflags); - defaults::tensorflow=1.15.0 # update only if absolutely necessary, as this may cause conflicts with other core dependencies; # verify that this is using numpy compiled against MKL (e.g., by checking tensorflow.pywrap_tensorflow.IsMklEnabled()); - conda-forge::scipy=1.0.0 # do not update, this will break a scipy.misc.logsumexp import (deprecated in scipy=1.0.0) in pymc3=3.1; - conda-forge::pymc3=3.1 # do not update, this will break gcnvkernel; - conda-forge::keras=2.2.4 # updated from pip-installed 2.2.0, which caused various conflicts/clobbers of conda-installed packages; # conda-installed 2.2.4 appears to be the most recent version with a consistent API and without conflicts/clobbers; # if you wish to update, note that versions of conda-forge::keras after 2.2.5; # undesirably set the environment variable KERAS_BACKEND = theano by default; - defaults::intel-openmp=2019.4; - conda-forge::scikit-learn=0.22.2; - conda-forge::matplotlib=3.2.1; - conda-forge::pandas=1.0.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6656#issuecomment-643526868:2181,config,config,2181,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6656#issuecomment-643526868,1,['config'],['config']
Modifiability,"nd, so it cannot localize gs; paths. In other words, the WDL tests in travis need a local instance of; the file and to use that path in the json. On Wed, Apr 10, 2019 at 3:49 PM Jonn Smith <notifications@github.com> wrote:. > @LeeTL1220 <https://github.com/LeeTL1220>; >; > This seems to be running into a cromwell / WDL error:; >; > java.lang.IllegalArgumentException: Could not build the path ""gs://broad-public-datasets/funcotator/transcriptList.exact_uniprot_matches.AKT1_CRLF2_FGFR1.txt"". It may refer to a filesystem not supported by this instance of Cromwell. Supported filesystems are: HTTP, LinuxFileSystem. Failures: HTTP: gs://broad-public-datasets/funcotator/transcriptList.exact_uniprot_matches.AKT1_CRLF2_FGFR1.txt does not have an http or https scheme (IllegalArgumentException); > LinuxFileSystem: Cannot build a local path from gs://broad-public-datasets/funcotator/transcriptList.exact_uniprot_matches.AKT1_CRLF2_FGFR1.txt (RuntimeException) Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems; > 	Could not build the path ""gs://broad-public-datasets/funcotator/transcriptList.exact_uniprot_matches.AKT1_CRLF2_FGFR1.txt"". It may refer to a filesystem not supported by this instance of Cromwell. Supported filesystems are: HTTP, LinuxFileSystem. Failures: HTTP: gs://broad-public-datasets/funcotator/transcriptList.exact_uniprot_matches.AKT1_CRLF2_FGFR1.txt does not have an http or https scheme (IllegalArgumentException); >; > Isn't cromwell supposed to handle gs:// URLs for localizing files? Do you; > have any thoughts?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/pull/5872#issuecomment-481836556>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk7Wd-RgMx2g-UPLNrvjettNMf9ixks5vfkA3gaJpZM4clLLK>; > .; >. -- ; Lee Lichtenstein; Br",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5872#issuecomment-481853426:1069,config,configure,1069,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5872#issuecomment-481853426,1,['config'],['configure']
Modifiability,nds 11 --gvcf-gq-bands 12 --gvcf-gq-bands 13 --gvcf-gq-bands 14 --gvcf-gq-bands 15 --gvcf-gq-bands 16 --gvcf-gq-bands 17 --gvc; f-gq-bands 18 --gvcf-gq-bands 19 --gvcf-gq-bands 20 --gvcf-gq-bands 21 --gvcf-gq-bands 22 --gvcf-gq-bands 23 --gvcf-gq-bands 24 --gvcf-gq-bands 25 --gvcf-gq-bands 26 --gvcf-gq-bands 27 --g; vcf-gq-bands 28 --gvcf-gq-bands 29 --gvcf-gq-bands 30 --gvcf-gq-bands 31 --gvcf-gq-bands 32 --gvcf-gq-bands 33 --gvcf-gq-bands 34 --gvcf-gq-bands 35 --gvcf-gq-bands 36 --gvcf-gq-bands 37 -; -gvcf-gq-bands 38 --gvcf-gq-bands 39 --gvcf-gq-bands 40 --gvcf-gq-bands 41 --gvcf-gq-bands 42 --gvcf-gq-bands 43 --gvcf-gq-bands 44 --gvcf-gq-bands 45 --gvcf-gq-bands 46 --gvcf-gq-bands 47; --gvcf-gq-bands 48 --gvcf-gq-bands 49 --gvcf-gq-bands 50 --gvcf-gq-bands 51 --gvcf-gq-bands 52 --gvcf-gq-bands 53 --gvcf-gq-bands 54 --gvcf-gq-bands 55 --gvcf-gq-bands 56 --gvcf-gq-bands ; 57 --gvcf-gq-bands 58 --gvcf-gq-bands 59 --gvcf-gq-bands 60 --gvcf-gq-bands 70 --gvcf-gq-bands 80 --gvcf-gq-bands 90 --gvcf-gq-bands 99 --floor-blocks false --indel-size-to-eliminate-in-re; f-model 10 --disable-optimizations false --dragen-mode false --flow-mode NONE --apply-bqd false --apply-frd false --disable-spanning-event-genotyping false --transform-dragen-mapping-quali; ty false --mapping-quality-threshold-for-genotyping 20 --max-effective-depth-adjustment-for-frd 0 --just-determine-active-regions false --dont-genotype false --do-not-run-physical-phasing ; false --do-not-correct-overlapping-quality false --use-filtered-reads-for-annotations false --use-flow-aligner-for-stepwise-hc-filtering false --adaptive-pruning false --do-not-recover-dan; gling-branches false --recover-dangling-heads false --kmer-size 10 --kmer-size 25 --dont-increase-kmer-sizes-for-cycles false --allow-non-unique-kmers-in-ref false --num-pruning-samples 1 ; --min-dangling-branch-length 4 --recover-all-dangling-branches false --max-num-haplotypes-in-population 128 --min-pruning 2 --adaptive-pruning-initial-error-rate 0.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8574#issuecomment-1793390789:4845,adapt,adaptive-pruning,4845,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8574#issuecomment-1793390789,2,['adapt'],"['adaptive-pruning', 'adaptive-pruning-initial-error-rate']"
Modifiability,"ntervals of two jobs, and whether separating the jobs would impact calls. In this example, GenotypeGVCFs would run over 1:1050-1150. For example, if we had a multi-NT variant that spanned 1148-1052, we'd want that called correctly no matter what intervals were used for the jobs. I tried using running GenomicsDBImport with -L over a small region, or I ran SelectVariants on the gVCF first (which behaves a little differently), and then used that subset gVCF as input to GenomicsDBImport, where GenomicsDBImport is given the entire contig as the interval. The resulting workspaces will be slightly different, with the latter containing information over a wider region (GenomicsDBIport truncates start/end of the input records to just the target interval). . So if either of these workspaces is passed to GenotypeGVCFs, using --only-output-calls-starting-in-intervals and -L 1:1050-1150:. I think any upstream padding doesnt matter. If you have a multi-nucleotide polymorphism that starts upstream of 1050 but spans 1050, this job wouldnt be responsible for calling that. The prior job, which has an interval set upstream of this one should call it. I think GenomicsDbImport's behavior is fine here. If you have a multi-NT variant that starts within 1050-1150, but extends outside (i.e. deletion or insertion starting at 1148), this could be a problem. The GenomicsDB workspace created with the interval 1:1050-1150 lacks the information to score that, right? The workspace created using the more permissive SelectVariants->GenomicsDBImport contains that downstream information and presumably would make the same call as if GenotypeGVCFs was given the intact chromosome as input, right?. However, it seems that if I simply create the workspace with a reasonably padded interval (adding 1kb should be more than enough for Illumina, right?), and then run GenotypeGVCFs with the original, unpassed interval, then the resulting workspace should contain all available information and GenotypeGVCFs should be",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1221558244:1317,polymorphi,polymorphism,1317,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1221558244,1,['polymorphi'],['polymorphism']
Modifiability,"o go ahead and add this option, I would probably keep the directory structure of the GermlineCNVCaller output the same (i.e., with folders named ""SAMPLE_#""), and just check the sample_name.txt files at the PostprocessGermlineCNVCalls step. I don't think this should require GermlineCNVCaller code changes, right?. 4) We may require additional code at the WDL level if we want to both switch over to primarily using sample names but also get rid of bundling (i.e., by passing only the calls for each sample when needed). Locally, you can always just search all output for directories containing the appropriate sample_name.txt. But on the cloud, you'd want to make sure that the postprocessing step for a particular sample gets only its corresponding directories, which would have to happen at the WDL level; the check against sample_name.txt at the tool level would just be a formality. I can foresee headaches with globbing and funky sample names. I'm not sure I understand your point about extending PostprocessGermlineCNVCalls to run on all samples. The point of that tool is to take results from all genomic shards for a single sample and stitch them together, right? Even if we extend this to run on a batch of multiple samples (which would just be moving the loop over samples at the WDL level to some lower level, i.e., Java or python), we still need to see all shards for those samples. Perhaps I'm misunderstanding---can you clarify?. @mwalker174 can we once and for all clearly document the issue with the transpose? Perhaps by pointing to specific WGS runs that have issues with call caching? I think being able to pinpoint the exact issue will help us identify the right solution---whether that be choosing an appropriate bundling scheme, taking advantage of #5781 to reduce the number of shards, batching during the postprocessing step, removing unnecessary outputs, etc. Recall that we'd like to be able to use the same WDL locally (when you have easy access to all GermlineCNVCaller re",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6659#issuecomment-644829765:2180,extend,extending,2180,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6659#issuecomment-644829765,1,['extend'],['extending']
Modifiability,"o the right processing methods in a single pass over the RDD. Reply by @SHuang-Broad. > I tried to fix it in this PR, but that seems to be a big task,; and probably is impossible to achieve in a single pass,; because currently each class of contig ends up producing a different type of object; (3 general classes: simple -> SimpleNovelAdjacency, complex -> ComplexVariantCanonicalRepresentation, and unknown -> SAM records of the contigs); and a groupBy() operation is necessary in the middle using these objects as keys; due to the fact that different contigs may produce the same variant; So what I'm thinking about, is two pass:; one pass for splitting them up into the 3 classes,; then another pass on each of those 3 RDD's to turn them into VariantContext's.; Any better idea?. Reply by @cwhelan ; > That would be better, and yeah you don't have to do it in this PR.; In theory you could make the keys for the groupByKey() (ie NovelAdjacencyAndAltHaplotype, CpxVariantCanonicalRepresentation, right?) all inherit from the same superclass and do a single group by, couldn't you? Then you could do everything in a single pass. Reply by @SHuang-Broad; > Yes, that is what I'm planning but I'm not sure yet about how to approach that (I actually tried it, before putting in the above comment, and quickly ran into the problem of mixing Java serialization and Kryo serialization, so a larger re-structuring might be needed, and not just a inheritance structure). ------------; ### On the problem of having a confusing TODO for ; `boolean SimpleChimera.isCandidateInvertedDuplication()`. The todo message. > TODO: 5/5/18 Note that the use of the following predicate is currently obsoleted by; {@link AssemblyContigWithFineTunedAlignments#hasIncompletePictureFromTwoAlignments()}; because the contigs with this alignment signature is classified as ""incomplete"",; hence will NOT sent here for constructing SimpleChimera's.; But we may want to keep the code (and related code in BreakpointComplications) ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4663#issuecomment-387899030:1548,inherit,inherit,1548,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4663#issuecomment-387899030,1,['inherit'],['inherit']
Modifiability,"ode:. -`BaseRecalibratorSpark` is the standalone BQSR tool, and calls into the `BaseRecalibratorSparkFn` (which is also called from `ReadsPipelineSpark`). -`ApplyBQSRSpark` is the standalone ApplyBQSR tool, and calls into the `ApplyBQSRSparkFn` (also called from `ReadsPipelineSpark`). -Integration tests for the above are in `BaseRecalibratorSparkIntegrationTest` and `ApplyBQSRSparkIntegrationTest`. -Almost all other changes in the branch are related to the BQSR engine refactoring, which I summarize below:; - We pulled out the guts of the walker `BaseRecalibrator` tool, combined it with all of the code from the former `RecalibrationEngine` class (now deleted) to make a new `BaseRecalibrationEngine` class under `utils/recalibration`.; - We stripped out all copies of the code in `BaseRecalibrationEngine` from the walker, dataflow, and spark versions of BQSR, and modified them to call into `BaseRecalibrationEngine`.; - We moved all auxiliary classes needed by the `BaseRecalibrationEngine` (eg., the covariates, etc.) into `utils/recalibration`.; - We refactored the argument collections. Now there is a single shared `RecalibrationArgumentCollection` that contains **only** the parameters for the `BaseRecalibrationEngine` itself, and this argument collection is exposed by all 3 versions of the tool. Input/output arguments have been removed from this argument collection and put into the individual implementations of BQSR, since they vary between the walker, dataflow, and spark versions of the tool. This eliminates awkward problems such as having both a `knownSites` argument AND a `BQSRKnownVariants` exposed at the same time, with only 1 of them usable for a given version of a tool. The dataflow-only `BaseRecalibrationArgumentCollection` has been deleted completely as no longer needed.; - We tweaked the names of some tool arguments to enforce consistency between the 3 versions of the tool as well as the rest of hellbender (eg., output arg for BQSR is now a more standard `-O`)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/911#issuecomment-142340073:1113,refactor,refactored,1113,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/911#issuecomment-142340073,1,['refactor'],['refactored']
Modifiability,odecov.io/gh/broadinstitute/gatk/pull/3447?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [.../broadinstitute/hellbender/utils/LoggingUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/3447/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9Mb2dnaW5nVXRpbHMuamF2YQ==) | `82.222% <ø> (ø)` | `11 <0> (ø)` | :arrow_down: |; | [...ellbender/cmdline/StandardArgumentDefinitions.java](https://codecov.io/gh/broadinstitute/gatk/pull/3447/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL1N0YW5kYXJkQXJndW1lbnREZWZpbml0aW9ucy5qYXZh) | `0% <ø> (ø)` | `0 <0> (ø)` | :arrow_down: |; | [...broadinstitute/hellbender/utils/test/BaseTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/3447/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0Jhc2VUZXN0LmphdmE=) | `65.926% <ø> (ø)` | `35 <0> (ø)` | :arrow_down: |; | [...ellbender/utils/config/CustomBooleanConverter.java](https://codecov.io/gh/broadinstitute/gatk/pull/3447/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9jb25maWcvQ3VzdG9tQm9vbGVhbkNvbnZlcnRlci5qYXZh) | `100% <100%> (ø)` | `2 <2> (?)` | |; | [...rg/broadinstitute/hellbender/utils/io/IOUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/3447/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9pby9JT1V0aWxzLmphdmE=) | `60.104% <100%> (+0.418%)` | `50 <2> (+1)` | :arrow_up: |; | [...stitute/hellbender/cmdline/CommandLineProgram.java](https://codecov.io/gh/broadinstitute/gatk/pull/3447/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL0NvbW1hbmRMaW5lUHJvZ3JhbS5qYXZh) | `86.408% <100%> (+0.408%)` | `29 <0> (ø)` | :arrow_down: |; | [...oadinstitute/hellbender/engine/FeatureManager.java](https://codecov.io/gh/broadinstitute/gatk/pull/3447/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vc,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3447#issuecomment-323474032:1818,config,config,1818,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3447#issuecomment-323474032,1,['config'],['config']
Modifiability,odehaus.groovy.runtime.callsite.CallSiteArray.defaultCallConstructor(CallSiteArray.java:60); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callConstructor(AbstractCallSite.java:235); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callConstructor(AbstractCallSite.java:255); 	at com.github.jengelman.gradle.plugins.shadow.tasks.ShadowCopyAction$StreamAction.visitFile(ShadowCopyAction.groovy:185); 	at sun.reflect.GeneratedMethodAccessor34.invoke(Unknown Source); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.codehaus.groovy.runtime.callsite.PogoMetaMethodSite$PogoCachedMethodSiteNoUnwrapNoCoerce.invoke(PogoMetaMethodSite.java:210); 	at org.codehaus.groovy.runtime.callsite.PogoMetaMethodSite.callCurrent(PogoMetaMethodSite.java:59); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:166); 	at com.github.jengelman.gradle.plugins.shadow.tasks.ShadowCopyAction$StreamAction.processFile(ShadowCopyAction.groovy:151); 	at org.gradle.api.internal.file.copy.NormalizingCopyActionDecorator$1$1.processFile(NormalizingCopyActionDecorator.java:66); 	at org.gradle.api.internal.file.copy.DuplicateHandlingCopyActionDecorator$1$1.processFile(DuplicateHandlingCopyActionDecorator.java:60); 	at org.gradle.api.internal.file.copy.CopyFileVisitorImpl.processFile(CopyFileVisitorImpl.java:62); 	at org.gradle.api.internal.file.copy.CopyFileVisitorImpl.visitFile(CopyFileVisitorImpl.java:46); 	at org.gradle.api.internal.file.collections.jdk7.Jdk7DirectoryWalker$1.visitFile(Jdk7DirectoryWalker.java:86); 	at org.gradle.api.internal.file.collections.jdk7.Jdk7DirectoryWalker$1.visitFile(Jdk7DirectoryWalker.java:59); 	at java.nio.file.Files.walkFileTree(Files.java:2670); 	at org.gradle.api.internal.file.collections.jdk7.Jdk7DirectoryWalker.walkDir(Jdk7DirectoryWalker.java:59); 	at org.gradle.api.internal.file.collections.DirectoryFileTree,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5499#issuecomment-446253445:2008,plugin,plugins,2008,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5499#issuecomment-446253445,1,['plugin'],['plugins']
Modifiability,"ok, got it. sorry, i missed 'install' in that command. my initial impression is that VariantQC will be able to adapt fine to VariantEvalEngine. I wrote VariantEvalEngine with this is mind, but it's good to formally test it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-759645374:111,adapt,adapt,111,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-759645374,1,['adapt'],['adapt']
Modifiability,"olders named ""SAMPLE_#""), and just check the sample_name.txt files at the PostprocessGermlineCNVCalls step. I don't think this should require GermlineCNVCaller code changes, right?. 4) We may require additional code at the WDL level if we want to both switch over to primarily using sample names but also get rid of bundling (i.e., by passing only the calls for each sample when needed). Locally, you can always just search all output for directories containing the appropriate sample_name.txt. But on the cloud, you'd want to make sure that the postprocessing step for a particular sample gets only its corresponding directories, which would have to happen at the WDL level; the check against sample_name.txt at the tool level would just be a formality. I can foresee headaches with globbing and funky sample names. I'm not sure I understand your point about extending PostprocessGermlineCNVCalls to run on all samples. The point of that tool is to take results from all genomic shards for a single sample and stitch them together, right? Even if we extend this to run on a batch of multiple samples (which would just be moving the loop over samples at the WDL level to some lower level, i.e., Java or python), we still need to see all shards for those samples. Perhaps I'm misunderstanding---can you clarify?. @mwalker174 can we once and for all clearly document the issue with the transpose? Perhaps by pointing to specific WGS runs that have issues with call caching? I think being able to pinpoint the exact issue will help us identify the right solution---whether that be choosing an appropriate bundling scheme, taking advantage of #5781 to reduce the number of shards, batching during the postprocessing step, removing unnecessary outputs, etc. Recall that we'd like to be able to use the same WDL locally (when you have easy access to all GermlineCNVCaller results from all genomic shards) and in the cloud, with minimal duplication of output from bundling when running locally, if possible.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6659#issuecomment-644829765:2371,extend,extend,2371,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6659#issuecomment-644829765,1,['extend'],['extend']
Modifiability,"ollapsed it into a single class for several reasons:; - Inheriting from a more generic traversal type caused usability issues and confusion with respect to the command-line arguments. The `ReadWindow` was the unit of processing for the superclass, but for `AssemblyRegionWalker` it was the unit of I/O and `AssemblyRegion` was the unit of processing, and I couldn't update the docs for `ReadWindowWalker` to clear up the confusion without mentioning `AssemblyRegion`-specific concepts.; - The `ReadShard` / `ReadWindow` was/is **only** there to prove that we can shard the data without introducing calling artifacts, and to provide a unit of parallelism for the upcoming Spark implementation. It's not something we really want to expose to users as a prominent knob, and we may hide it completely in the future once the shard size is tuned for performance.; - Inheriting from a more abstract walker type caused a number of other problems as well: methods that should have been final in the supertype could no longer be made final, with the result that tool implementations could inappropriately override engine initialization/shutdown routines. Also, there were issues with the progress meter, since both the supertype traversal and subtype traversal needed their own progress meter for their different units of processing. Ultimately it was just too awkward and forced, and the read shard is something that we eventually want to make an internal/encapsulated implementation detail anyway. GATK3 made the mistake, I think, of using long, confusing inheritance chains for its walker types, with the result that you got awkward and forced relationships like `RodWalker` inheriting from `LocusWalker`. It's better, I think, to make each traversal as standalone as possible, especially given the simplicity of writing a new walker type in GATK4. For all of these reasons we don't want `AssemblyRegionWalker` to inherit from a more abstract traversal type -- it's just going to be its own standalone thing",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210806513:1385,Inherit,Inheriting,1385,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210806513,1,['Inherit'],['Inheriting']
Modifiability,"on: class GVCFBlockCombiner; 2022-08-16T00:09:07.3893126Z src/main/java/org/broadinstitute/hellbender/utils/variant/writers/GVCFBlockCombiner.java:32: error: cannot find symbol; 2022-08-16T00:09:07.3893670Z final RangeMap<Integer, Range<Integer>> gqPartitions;; 2022-08-16T00:09:07.3894352Z symbol: class Range; 2022-08-16T00:09:07.3894678Z location: class GVCFBlockCombiner; 2022-08-16T00:09:07.3897711Z src/main/java/org/broadinstitute/hellbender/utils/variant/writers/GVCFBlockCombiner.java:62: error: cannot find symbol; 2022-08-16T00:09:07.3902203Z RangeMap<Integer,Range<Integer>> parsePartitions(final List<? extends Number> gqPartitions) ***; 2022-08-16T00:09:07.3902980Z symbol: class RangeMap; 2022-08-16T00:09:07.3903340Z location: class GVCFBlockCombiner; 2022-08-16T00:09:07.3903864Z src/main/java/org/broadinstitute/hellbender/utils/variant/writers/GVCFBlockCombiner.java:62: error: cannot find symbol; 2022-08-16T00:09:07.3904505Z RangeMap<Integer,Range<Integer>> parsePartitions(final List<? extends Number> gqPartitions) ***; 2022-08-16T00:09:07.3905250Z symbol: class Range; 2022-08-16T00:09:07.3905751Z location: class GVCFBlockCombiner; 2022-08-16T00:09:07.3906273Z src/main/java/org/broadinstitute/hellbender/utils/variant/writers/GVCFBlockCombiner.java:101: error: cannot find symbol; 2022-08-16T00:09:07.3906908Z static VCFHeaderLine rangeToVCFHeaderLine(Range<Integer> genotypeQualityBand) ***; 2022-08-16T00:09:07.3907793Z symbol: class Range; 2022-08-16T00:09:07.3908125Z location: class GVCFBlockCombiner; 2022-08-16T00:09:07.3910592Z src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/ChainPruner.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T00:09:07.3914013Z src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/BaseVertex.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T00:09:07.3921838Z src/main/java/org/broadinstitute/hellbender/tools/walkers",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217242480:6678,extend,extends,6678,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217242480,1,['extend'],['extends']
Modifiability,"on: class GVCFBlockCombiner; 2022-08-16T22:45:53.7740892Z src/main/java/org/broadinstitute/hellbender/utils/variant/writers/GVCFBlockCombiner.java:32: error: cannot find symbol; 2022-08-16T22:45:53.7741707Z final RangeMap<Integer, Range<Integer>> gqPartitions;; 2022-08-16T22:45:53.7743523Z symbol: class Range; 2022-08-16T22:45:53.7743866Z location: class GVCFBlockCombiner; 2022-08-16T22:45:53.7747579Z src/main/java/org/broadinstitute/hellbender/utils/variant/writers/GVCFBlockCombiner.java:62: error: cannot find symbol; 2022-08-16T22:45:53.7748444Z RangeMap<Integer,Range<Integer>> parsePartitions(final List<? extends Number> gqPartitions) ***; 2022-08-16T22:45:53.7776218Z symbol: class RangeMap; 2022-08-16T22:45:53.7776715Z location: class GVCFBlockCombiner; 2022-08-16T22:45:53.7777389Z src/main/java/org/broadinstitute/hellbender/utils/variant/writers/GVCFBlockCombiner.java:62: error: cannot find symbol; 2022-08-16T22:45:53.7778220Z RangeMap<Integer,Range<Integer>> parsePartitions(final List<? extends Number> gqPartitions) ***; 2022-08-16T22:45:53.7779110Z symbol: class Range; 2022-08-16T22:45:53.7779574Z location: class GVCFBlockCombiner; 2022-08-16T22:45:53.7780209Z src/main/java/org/broadinstitute/hellbender/utils/variant/writers/GVCFBlockCombiner.java:101: error: cannot find symbol; 2022-08-16T22:45:53.7780965Z static VCFHeaderLine rangeToVCFHeaderLine(Range<Integer> genotypeQualityBand) ***; 2022-08-16T22:45:53.7781896Z symbol: class Range; 2022-08-16T22:45:53.7782232Z location: class GVCFBlockCombiner; 2022-08-16T22:45:53.7785096Z src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/ChainPruner.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T22:45:53.7789228Z src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/BaseVertex.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T22:45:53.7798240Z src/main/java/org/broadinstitute/hellbender/tools/walkers",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217253370:8716,extend,extends,8716,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217253370,1,['extend'],['extends']
Modifiability,"ong. We should make a clear distinction between the old and new tests though. Ideally the GATK3 tests would be in a separate commit that we can just delete at the end, but that can get unwieldy if the files in the commit need to change as we go along. Alternatively you could isolate them into a separate directory. They should either be disabled or made dependent on a test method (see the `@Test` annotation properties `enabled` and `dependsOn`) that is easily toggled so they can be run locally, but don't run on the CI server. Otherwise the CI server build will always fail. In general, its really helpful to have the first commit in the PR contain the completely unmodified GATK3 source files. It makes it much easier for the reviewer to see what changed for the port. I noticed that you have 2 new plugins included in this. I'm not sure if that was suggested by someone on the GATK team (I'm wondering if we want to go down that path...) but I can tell you that the existing plugins required an enormous amount of test development and review iteration. If we do decide to make them plugins, I think it would be a good idea to do so in a separate PR. Also, if we choose to make an AbstractPlugin base class, we may want that to live in the Barclay repo. As @magicdgs points out, master already has your previous commits, so you should start by rebasing on that. Ideally, the branch would have the following commits before we start the first review cycle:. 1. A single commit containing the unmodified GATK3 source (unmodified with the exception that if a file is renamed for GATK4, its helpful to rename the GATK3 version in this commit so it's easy to compare in the next commit). This commit doesn't have to compile or run - its just to make the review process easier for us, and will be deleted at some point. I can help with how to get this into your branch if you like.; 2. Your modified GATK3 tests in a single commit. This will also be removed before merge.; 3. A single commit with all o",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-407089352:1781,plugin,plugins,1781,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-407089352,1,['plugin'],['plugins']
Modifiability,onnection.java:966); 	at shaded.cloud_nio.com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93); 	at shaded.cloud_nio.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:981); 	at shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials.runningOnComputeEngine(ComputeEngineCredentials.java:176); 	at shaded.cloud_nio.com.google.auth.oauth2.DefaultCredentialsProvider.tryGetComputeCredentials(DefaultCredentialsProvider.java:270); 	at shaded.cloud_nio.com.google.auth.oauth2.DefaultCredentialsProvider.getDefaultCredentialsUnsynchronized(DefaultCredentialsProvider.java:194); 	at shaded.cloud_nio.com.google.auth.oauth2.DefaultCredentialsProvider.getDefaultCredentials(DefaultCredentialsProvider.java:112); 	at shaded.cloud_nio.com.google.auth.oauth2.GoogleCredentials.getApplicationDefault(GoogleCredentials.java:113); 	at shaded.cloud_nio.com.google.auth.oauth2.GoogleCredentials.getApplicationDefault(GoogleCredentials.java:86); 	at com.google.cloud.ServiceOptions.defaultCredentials(ServiceOptions.java:277); 	at com.google.cloud.ServiceOptions.<init>(ServiceOptions.java:252); 	at com.google.cloud.storage.StorageOptions.<init>(StorageOptions.java:82); 	at com.google.cloud.storage.StorageOptions.<init>(StorageOptions.java:30); 	at com.google.cloud.storage.StorageOptions$Builder.build(StorageOptions.java:77); 	at org.broadinstitute.hellbender.utils.gcs.BucketUtils.setGlobalNIODefaultOptions(BucketUtils.java:361); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:195); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:131); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:152); 	at org.broadinstitute.hellbender.Main.main(Main.java:233); ```; I was able to fix the issue by setting the environment variable `NO_GCE_CHECK=true` in my shell though,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3591#issuecomment-331269235:6606,variab,variable,6606,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3591#issuecomment-331269235,1,['variab'],['variable']
Modifiability,org\springframework\spring-web\5.2.6.RELEASE\spring-web-5.2.6.RELEASE.jar;E:\repository\org\springframework\spring-webmvc\5.2.6.RELEASE\spring-webmvc-5.2.6.RELEASE.jar;E:\repository\org\springframework\spring-aop\5.2.6.RELEASE\spring-aop-5.2.6.RELEASE.jar;E:\repository\org\springframework\spring-context\5.2.6.RELEASE\spring-context-5.2.6.RELEASE.jar;E:\repository\org\springframework\spring-expression\5.2.6.RELEASE\spring-expression-5.2.6.RELEASE.jar;E:\repository\org\mybatis\spring\boot\mybatis-spring-boot-starter\2.1.2\mybatis-spring-boot-starter-2.1.2.jar;E:\repository\org\mybatis\spring\boot\mybatis-spring-boot-autoconfigure\2.1.2\mybatis-spring-boot-autoconfigure-2.1.2.jar;E:\repository\org\mybatis\mybatis\3.5.4\mybatis-3.5.4.jar;E:\repository\org\mybatis\mybatis-spring\2.0.4\mybatis-spring-2.0.4.jar;E:\repository\mysql\mysql-connector-java\8.0.20\mysql-connector-java-8.0.20.jar;E:\repository\org\springframework\boot\spring-boot-configuration-processor\2.3.0.RELEASE\spring-boot-configuration-processor-2.3.0.RELEASE.jar;E:\repository\org\springframework\spring-core\5.2.6.RELEASE\spring-core-5.2.6.RELEASE.jar;E:\repository\org\springframework\spring-jcl\5.2.6.RELEASE\spring-jcl-5.2.6.RELEASE.jar;E:\repository\com\google\firebase\firebase-admin\6.8.1\firebase-admin-6.8.1.jar;E:\repository\com\google\api-client\google-api-client\1.25.0\google-api-client-1.25.0.jar;E:\repository\com\google\oauth-client\google-oauth-client\1.25.0\google-oauth-client-1.25.0.jar;E:\repository\com\google\http-client\google-http-client-jackson2\1.25.0\google-http-client-jackson2-1.25.0.jar;E:\repository\com\google\api-client\google-api-client-gson\1.25.0\google-api-client-gson-1.25.0.jar;E:\repository\com\google\http-client\google-http-client-gson\1.25.0\google-http-client-gson-1.25.0.jar;E:\repository\com\google\code\gson\gson\2.8.6\gson-2.8.6.jar;E:\repository\com\google\http-client\google-http-client\1.25.0\google-http-client-1.25.0.jar;E:\repository\com\google\code\findbugs\jsr305\3.0,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-635805233:5508,config,configuration-processor-,5508,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-635805233,1,['config'],['configuration-processor-']
Modifiability,"ould like to address are similar to yours, with some inclussions. * Regarding NIO support, I would go to remove completely `File` support. If API users need to use the `File` abstraction, they should convert to a `java.nio.Path` using the `toPath` method.; * In addition, I would like that HTTP/S and FTP is handled also with NIO. For HTTP/S, I am working in a simple `FileSystemProvider` that should be good enough for using in combination with HTSJDK ([jsr203-http](https://github.com/magicDGS/jsr203-http)), and I can speed up the development there for needs in HTSJDK; for FTP, maybe [ftp-fs](https://github.com/robtimus/ftp-fs) can be used or a simple implementation can be derived from the HTTP/S implementation (without credentials). This will remove the special handling of HTTP/S and FTP paths in HTSJDK in favor of a consistent and pluggable manner.; * Interfaces for the data types are great, and maybe it will be good to have codec interfaces for both encoding and decoding. For example, I am missing encoders in tribble (an attempt in https://github.com/samtools/htsjdk/pull/822 for writing support).; * For VCF, I would like to have a less diploid-centric interface and design, or at least a way of configure the catching of genotype-related attributes. Currently there are methods for homozygotes/heterozygotes that aren't really useful for triploids or even VCFs without variation (for example, in Pool-Seq data).; * Modular design for artifacts: thus, a project with only SAM/BAM requirements will require only `htsjdk-sam`, and if they also want CRAM support, `htsjdk-cram`. See https://github.com/samtools/htsjdk/issues/896 for more info about it.; * Common license for all HTSJDK, or at least for each module. This will be good for taking into account legal concerns when including the library, because now there is a mixture depending on the files that are used. This is what is coming to my mind now. Maybe I added something else in https://github.com/samtools/htsjdk/issues/520",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4340#issuecomment-363390940:1281,config,configure,1281,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4340#issuecomment-363390940,1,['config'],['configure']
Modifiability,"ouldn't update the docs for `ReadWindowWalker` to clear up the confusion without mentioning `AssemblyRegion`-specific concepts.; - The `ReadShard` / `ReadWindow` was/is **only** there to prove that we can shard the data without introducing calling artifacts, and to provide a unit of parallelism for the upcoming Spark implementation. It's not something we really want to expose to users as a prominent knob, and we may hide it completely in the future once the shard size is tuned for performance.; - Inheriting from a more abstract walker type caused a number of other problems as well: methods that should have been final in the supertype could no longer be made final, with the result that tool implementations could inappropriately override engine initialization/shutdown routines. Also, there were issues with the progress meter, since both the supertype traversal and subtype traversal needed their own progress meter for their different units of processing. Ultimately it was just too awkward and forced, and the read shard is something that we eventually want to make an internal/encapsulated implementation detail anyway. GATK3 made the mistake, I think, of using long, confusing inheritance chains for its walker types, with the result that you got awkward and forced relationships like `RodWalker` inheriting from `LocusWalker`. It's better, I think, to make each traversal as standalone as possible, especially given the simplicity of writing a new walker type in GATK4. For all of these reasons we don't want `AssemblyRegionWalker` to inherit from a more abstract traversal type -- it's just going to be its own standalone thing, so that it can evolve freely without affecting anyone else. For `SlidingWindowWalker`, which we still want to merge, I recommend making the traversal do **exactly** what you want for your use case, as clearly and simply as possible, without worrying about serving as a base class for other traversals. Ping me once you're happy with it, and I'll re-review.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210806513:2073,inherit,inheritance,2073,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210806513,4,"['evolve', 'inherit']","['evolve', 'inherit', 'inheritance', 'inheriting']"
Modifiability,overall this looks reasonable to me. Adding an integration test is important - at least lock in the variants that are concordant with gatk3 so that we dont lose those while refactoring and fixing etc.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1567#issuecomment-196949010:173,refactor,refactoring,173,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1567#issuecomment-196949010,1,['refactor'],['refactoring']
Modifiability,"ozygous genotypes failing two-tailed binomial test (example below); q .. select genotypes using -i/-e options; and the new genotype can be one of:; . .. missing (""."" or ""./."", keeps ploidy); 0 .. reference allele (e.g. 0/0 or 0, keeps ploidy); c:GT .. custom genotype (e.g. 0/0, 0, 0/1, m/M, overrides ploidy); m .. minor (the second most common) allele (e.g. 1/1 or 1, keeps ploidy); M .. major allele (e.g. 1/1 or 1, keeps ploidy); p .. phase genotype (0/1 becomes 0|1); u .. unphase genotype and sort by allele (1|0 becomes 0/1); Usage: bcftools +setGT [General Options] -- [Plugin Options]; Options:; run ""bcftools plugin"" for a list of common options. Plugin options:; -e, --exclude <expr> Exclude a genotype if true (requires -t q); -i, --include <expr> include a genotype if true (requires -t q); -n, --new-gt <type> Genotypes to set, see above; -t, --target-gt <type> Genotypes to change, see above. Example:; # set missing genotypes (""./."") to phased ref genotypes (""0|0""); bcftools +setGT in.vcf -- -t . -n 0p. # set missing genotypes with DP>0 and GQ>20 to ref genotypes (""0/0""); bcftools +setGT in.vcf -- -t q -n 0 -i 'GT=""."" && FMT/DP>0 && GQ>20'. # set partially missing genotypes to completely missing; bcftools +setGT in.vcf -- -t ./x -n . # set heterozygous genotypes to 0/0 if binom.test(nAlt,nRef+nAlt,0.5)<1e-3; bcftools +setGT in.vcf -- -t ""b:AD<1e-3"" -n 0. # force unphased heterozygous genotype if binom.test(nAlt,nRef+nAlt,0.5)>0.1; bcftools +setGT in.vcf -- -t ./x -n c:'m/M'; ```; I was always wondering if GATK will have a plugin interface where people can code their own using groovy, kotlin, javascript or python plugins to extend some of the functionality where developers may not reach immediately. Personally I use htsjdk extensively (and sometimes pysam) to code a new personal tool each time I need something that I cannot find exactly what I look for. But a generic gatk plugin interface would be really useful and may provide means to extend the community support.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8328#issuecomment-1556119501:1923,plugin,plugin,1923,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8328#issuecomment-1556119501,5,"['extend', 'plugin']","['extend', 'plugin', 'plugins']"
Modifiability,"provements that were discovered while reviewing the variants ; (https://github.com/SHuang-Broad/GATK-SV-callset-regressionTest/tree/master/Evaluation/Analysis/masterVSfeature/notes.xlsx); The implemented fixes are:; * for removing the hard-coded/explicit mentioning of ""chr"" in non-canonical versions, it is now fixed in 5eff782e4d582d516004fba2cee7535d984b1540; * for contigs whose alignments paint ambiguous picture, i.e. multiple alignment configurations offer equally good explanation:; 	1. if only one configuration has all alignment with MQ above a specified threshold, it is favored; this is implemented in ecc31f5fbec4e524b401fc9474a3a1b7ab08c561; 	2. if one configuration has alignment to non-canonical chromosome that explains the contig better than would-be-event-inducing mappings to canonical chromosomes, the canonical mappings are saved but the better non-canonical mappings are saved as SA tag as in SAM spec, and the VCF record produced is annotated accordingly; this is implemented in 65cdb523a2f9fa2026334713fed45381d76ffc82; * fixed a bug where sometimes an assembly contig as several alignments, only one of which has non-mediocre MQ but at the sametime this alignment contains a large gap, such contigs were previously incorrectly filtered away, they are now salvaged by commit b6b2f197b112981e00efd9d415f010c024d31b36. So, for the FN variants (FN in the sense that they are captured in the stable version of our interpretation tool but now goes missing in the experimental interpretation tool); that were curated in the above-mentioned review, only the following ones are not salvaged, with plans or comments attached. ```; asm012854:tig00000	missing	classified as ""incomplete""; fixable by finishing the last TODO in AssemblyContigAlignmentSignatureClassifier (same problem as face by group represented by asm002398:tig00001); asm014580:tig00018	missing	classified as ""incomplete""; fixable by finishing the last TODO in AssemblyContigAlignmentSignatureClassifier (same problem ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4326#issuecomment-370923522:863,config,configuration,863,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4326#issuecomment-370923522,1,['config'],['configuration']
Modifiability,"put); * positive (training with *.annot.hdf5) vs. positive-unlabeled (training with *.annot.hdf5 and *.unlabeled.annot.hdf5); * Java Bayesian Gaussian Mixture Model (BGMM) backend vs. python sklearn IsolationForest backend; (BGMM tests to be added once PR for the backend goes in.); - [x] Tool-level docs. Minor TODOs:. - [x] Parameter-level docs.; - [x] Parameter/mode validation.; - [x] Refactor main code block for model training; it's a bit monolithic and procedural now.; - [x] Decide on behavior for ill-behaved annotations. E.g., all missing, zero variance. Future work:. - [ ] We could allow subsetting of annotations here, which might allow for easier treatment of ill-behaved annotations. However, I'd say enabling workflows where the set of annotations is fixed is the priority.; - [ ] We could do positive-unlabeled training more rigorously or iteratively. Right now, we essentially do a single iteration to determine negative data. This could perhaps be preceded by a round of refactoring to clean up model training and make it less procedural.; - [ ] Automatic threshold tuning could be built into the tool, see #7711. We'd probably have to introduce a ""validation"" label. Perhaps it makes sense to keep this sort of thing at the workflow level?; - [ ] In the positive-negative framework enforced by the Java code in this tool, a ""model"" is anything that assigns a score, we fit two models to different subsets of the data, and then take the difference of the two scores. While the python backend does give some freedom to specify a model, future developers may want to go beyond the framework itself. For example, more traditional classification frameworks, etc. could be explored. As an intermediate step, one could perhaps use the positive/negative scores from the current framework in a more sophisticated way (e.g., using them as features), rather than just taking their difference. This sort of future work could be developed completely independently of the codebase associated wit",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7724#issuecomment-1067948369:1411,refactor,refactoring,1411,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7724#issuecomment-1067948369,1,['refactor'],['refactoring']
Modifiability,r/ports/biology/gatk/work/gatk-4.0.11.0/build/libs/gatk-package-1.0-SNAPSHOT-local.jar'.; 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423); 	at org.codehaus.groovy.reflection.CachedConstructor.invoke(CachedConstructor.java:83); 	at org.codehaus.groovy.runtime.callsite.ConstructorSite$ConstructorSiteNoUnwrapNoCoerce.callConstructor(ConstructorSite.java:105); 	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallConstructor(CallSiteArray.java:60); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callConstructor(AbstractCallSite.java:235); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callConstructor(AbstractCallSite.java:255); 	at com.github.jengelman.gradle.plugins.shadow.tasks.ShadowCopyAction$StreamAction.visitFile(ShadowCopyAction.groovy:185); 	at sun.reflect.GeneratedMethodAccessor34.invoke(Unknown Source); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.codehaus.groovy.runtime.callsite.PogoMetaMethodSite$PogoCachedMethodSiteNoUnwrapNoCoerce.invoke(PogoMetaMethodSite.java:210); 	at org.codehaus.groovy.runtime.callsite.PogoMetaMethodSite.callCurrent(PogoMetaMethodSite.java:59); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:166); 	at com.github.jengelman.gradle.plugins.shadow.tasks.ShadowCopyAction$StreamAction.processFile(ShadowCopyAction.groovy:151); 	at org.gradle.api.internal.file.copy.NormalizingCopyActionDecorator$1$1.processFile(NormalizingCopyActionDecorator.java:66); 	at org.gradle.api.internal.file.copy.DuplicateHandlingCopyActionDecorator$1$1.processFile(DuplicateHandlingCopyAction,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5499#issuecomment-446253445:1341,plugin,plugins,1341,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5499#issuecomment-446253445,1,['plugin'],['plugins']
Modifiability,"r_round=2000 --log_emission_sampling_rounds=100 --log_emission_sampling_median_rel_error=5.000000e-04 --max_advi_iter_first_epoch=1000 --max_advi_iter_subsequent_epochs=1000 --min_training_epochs=20 --max_training_epochs=100 --initial_temperature=2.000000e+00 --num_thermal_advi_iters=5000 --convergence_snr_averaging_window=5000 --convergence_snr_trigger_threshold=1.000000e-01 --convergence_snr_countdown_window=10 --max_calling_iters=1 --caller_update_convergence_threshold=1.000000e-03 --caller_internal_admixing_rate=7.500000e-01 --caller_external_admixing_rate=7.500000e-01 --disable_caller=false --disable_sampler=false --disable_annealing=false --interval_list=/tmp/intervals9016836733228000464.tsv --contig_ploidy_prior_table=/home/n.liorni/snakemake_cnv_gatk/resources/contig_ploidy_priors.tsv --output_model_path=/home/n.liorni/snakemake_cnv_gatk/results/cnv/ploidy/ploidy-model; Stdout: 15:09:46.970 INFO cohort_determine_ploidy_and_depth - THEANO_FLAGS environment variable has been set to: device=cpu,floatX=float64,optimizer=fast_run,compute_test_value=ignore,openmp=true,blas.ldflags=-lmkl_rt,openmp_elemwise_minsize=10; 15:09:47.017 INFO gcnvkernel.structs.metadata - Generating intervals metadata...; 15:09:47.024 INFO gcnvkernel.tasks.task_cohort_ploidy_determination - Instantiating the germline contig ploidy determination model...; 15:09:50.320 INFO gcnvkernel.tasks.task_cohort_ploidy_determination - Instantiating the ploidy emission sampler...; 15:09:50.321 INFO gcnvkernel.tasks.task_cohort_ploidy_determination - Instantiating the ploidy caller...; 15:09:50.957 INFO gcnvkernel.models.fancy_model - Global model variables: {'psi_j_log__', 'mean_bias_j_lowerbound__'}; 15:09:50.957 INFO gcnvkernel.models.fancy_model - Sample-specific model variables: {'psi_s_log__'}; 15:09:50.957 INFO gcnvkernel.tasks.inference_task_base - Instantiating the convergence tracker...; 15:09:50.958 INFO gcnvkernel.tasks.inference_task_base - Setting up DA-ADVI...; 15:10:03.310 INFO gcnvkern",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7444#issuecomment-945753905:6703,variab,variable,6703,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7444#issuecomment-945753905,1,['variab'],['variable']
Modifiability,"ranch against 4.5.0.0, as well as this branch against itself (checking for reproducibility). Costs for this branch ($10.92) and 4.5.0.0 ($10.96) were quite comparable. Note that a small portion of these costs derives from Pf7-specific genotyping steps, which I did not bother to remove from the workflow. Runtime for the ploidy modeling and postprocessing steps were comparable. Interestingly, **runtime for the gCNV was ~20-25% longer with this branch than with 4.5.0.0, but memory usage fell by a factor of ~3 (~6GB to ~2GB)!** I am not sure if we could recoup the runtime with some more tweaking of the environment (perhaps double checking that optimized BLAS/MKL/etc. packages are properly used, changing environment variables/flags, etc.), but I think the decrease in memory usage is quite nice. Concordance was checked for the following quantities (4.5.0.0 is on the x-axis and this branch is on the y-axis in all plots below):. 1) Variational posterior means (`mu_*`) and standard deviations (`std_*`) for all analogous variables in the ploidy and gCNV models. There were some slight changes to the gCNV model in this branch (e.g., the functional form of the ARD prior was changed), which means some variables are no longer directly comparable. Furthermore, some variables (such as the bias factors W) are degenerate and cannot be immediately compared. Otherwise, there is good concordance between the remaining variables, e.g.:. ![image](https://github.com/broadinstitute/gatk/assets/11076296/614cf501-ca31-4199-badb-3194b7f78154); ![image](https://github.com/broadinstitute/gatk/assets/11076296/f615084d-d0bf-44e9-bcf5-98abd26ceb06); ![image](https://github.com/broadinstitute/gatk/assets/11076296/48570e53-024c-44b5-8835-3fd40b4c5866); ![image](https://github.com/broadinstitute/gatk/assets/11076296/99100e5d-05e2-4a5c-9d68-57db1b734029); ![image](https://github.com/broadinstitute/gatk/assets/11076296/abae09e1-70a5-4213-95a2-0cb10f9db192); ![image](https://github.com/broadinstitute/gatk/a",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8561#issuecomment-2079695268:1391,variab,variables,1391,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8561#issuecomment-2079695268,1,['variab'],['variables']
Modifiability,ree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...bender/tools/spark/pathseq/PathSeqFilterSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/3354?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9wYXRoc2VxL1BhdGhTZXFGaWx0ZXJTcGFyay5qYXZh) | `70.968% <ø> (ø)` | `7 <0> (ø)` | :arrow_down: |; | [...itute/hellbender/tools/spark/pathseq/PSFilter.java](https://codecov.io/gh/broadinstitute/gatk/pull/3354?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9wYXRoc2VxL1BTRmlsdGVyLmphdmE=) | `92.617% <100%> (+0.531%)` | `33 <1> (+1)` | :arrow_up: |; | [...ools/spark/pathseq/PSFilterArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/3354?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9wYXRoc2VxL1BTRmlsdGVyQXJndW1lbnRDb2xsZWN0aW9uLmphdmE=) | `80% <100%> (+1.429%)` | `2 <0> (ø)` | :arrow_down: |; | [...ellbender/transformers/AdapterTrimTransformer.java](https://codecov.io/gh/broadinstitute/gatk/pull/3354?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90cmFuc2Zvcm1lcnMvQWRhcHRlclRyaW1UcmFuc2Zvcm1lci5qYXZh) | `92.857% <92.857%> (ø)` | `12 <12> (?)` | |; | [...nder/transformers/SimpleRepeatMaskTransformer.java](https://codecov.io/gh/broadinstitute/gatk/pull/3354?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90cmFuc2Zvcm1lcnMvU2ltcGxlUmVwZWF0TWFza1RyYW5zZm9ybWVyLmphdmE=) | `94.286% <94.286%> (ø)` | `11 <11> (?)` | |; | [...nstitute/hellbender/utils/clipping/ClippingOp.java](https://codecov.io/gh/broadinstitute/gatk/pull/3354?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9jbGlwcGluZy9DbGlwcGluZ09wLmphdmE=) | `84.365% <0%> (+1.629%)` | `91% <0%> (+2%)` | :arrow_up: |; | [...stitute/hellbender/utils/clipping/ReadClipper.java](https://codecov.io/gh/broadinstitute/gatk/pull/3354?src=pr&el=tree#diff-c3JjL21haW4vamF,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3354#issuecomment-317586310:1867,Adapt,AdapterTrimTransformer,1867,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3354#issuecomment-317586310,1,['Adapt'],['AdapterTrimTransformer']
Modifiability,"rg\apache\commons\commons-pool2\2.8.0\commons-pool2-2.8.0.jar;C:\Program Files\JetBrains\IntelliJ IDEA 2020.1\lib\idea_rt.jar"" com.luz.push.PushApplication; Connected to the target VM, address: '127.0.0.1:62530', transport: 'socket'. . ____ _ __ _ _; /\\ / ___'_ __ _ _(_)_ __ __ _ \ \ \ \; ( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \; \\/ ___)| |_)| | | | | || (_| | ) ) ) ); ' |____| .__|_| |_|_| |_\__, | / / / /; =========|_|==============|___/=/_/_/_/; :: Spring Boot :: (v2.3.0.RELEASE). 2020-05-29 15:14:30.695 INFO 12904 --- [ main] com.luz.push.PushApplication : Starting PushApplication on DESKTOP-05L3FQL with PID 12904 (C:\project\push\target\classes started by Sweet in C:\project\push); 2020-05-29 15:14:30.712 INFO 12904 --- [ main] com.luz.push.PushApplication : No active profile set, falling back to default profiles: default; 2020-05-29 15:14:32.088 WARN 12904 --- [ main] o.m.s.mapper.ClassPathMapperScanner : No MyBatis mapper was found in '[com.luz.push]' package. Please check your configuration.; 2020-05-29 15:14:32.662 INFO 12904 --- [ main] o.s.b.w.embedded.tomcat.TomcatWebServer : Tomcat initialized with port(s): 8282 (http); 2020-05-29 15:14:32.675 INFO 12904 --- [ main] o.a.coyote.http11.Http11NioProtocol : Initializing ProtocolHandler [""http-nio-8282""]; 2020-05-29 15:14:32.676 INFO 12904 --- [ main] o.apache.catalina.core.StandardService : Starting service [Tomcat]; 2020-05-29 15:14:32.677 INFO 12904 --- [ main] org.apache.catalina.core.StandardEngine : Starting Servlet engine: [Apache Tomcat/9.0.35]; 2020-05-29 15:14:32.802 INFO 12904 --- [ main] o.a.c.c.C.[Tomcat].[localhost].[/] : Initializing Spring embedded WebApplicationContext; 2020-05-29 15:14:32.802 INFO 12904 --- [ main] o.s.web.context.ContextLoader : Root WebApplicationContext: initialization completed in 1944 ms; 2020-05-29 15:14:32.899 INFO 12904 --- [ main] com.luz.push.utils.GcmUtils : start init gcm server; 2020-05-29 15:14:33.029 WARN 12904 --- [ main] c.g.a.oauth2.ComputeEngineCredenti",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-635805233:11692,config,configuration,11692,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-635805233,1,['config'],['configuration']
Modifiability,"roadinstitute/hellbender/engine/filters/CountingVariantFilter.java:197: error: cannot find symbol; 2022-08-16T00:09:07.4040311Z @VisibleForTesting; 2022-08-16T00:09:07.4040921Z symbol: class VisibleForTesting; 2022-08-16T00:09:07.4041294Z location: class CountingVariantFilter; 2022-08-16T00:09:07.4054361Z src/main/java/org/broadinstitute/hellbender/cmdline/GATKPlugin/GATKReadFilterPluginDescriptor.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T00:09:07.4060164Z src/main/java/org/broadinstitute/hellbender/engine/filters/ReadFilter.java:75: error: cannot find symbol; 2022-08-16T00:09:07.4060614Z @VisibleForTesting; 2022-08-16T00:09:07.4061233Z symbol: class VisibleForTesting; 2022-08-16T00:09:07.4061591Z location: class ReadFilter; 2022-08-16T00:09:07.4083439Z src/main/java/org/broadinstitute/hellbender/utils/config/ConfigFactory.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T00:09:07.4092135Z src/main/java/org/broadinstitute/hellbender/utils/config/GATKConfig.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T00:09:07.4107682Z src/main/java/org/broadinstitute/hellbender/utils/variant/GATKVariantContextUtils.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T00:09:07.4116317Z src/main/java/org/broadinstitute/hellbender/utils/LoggingUtils.java:3: error: package com.google.common.collect does not exist; 2022-08-16T00:09:07.4117746Z src/main/java/org/broadinstitute/hellbender/utils/LoggingUtils.java:4: error: package com.google.common.collect does not exist; 2022-08-16T00:09:07.4124264Z src/main/java/org/broadinstitute/hellbender/utils/LoggingUtils.java:32: error: cannot find symbol; 2022-08-16T00:09:07.4124816Z private static BiMap<Log.LogLevel, Level> loggingLevelNamespaceMap;; 2022-08-16T00:09:07.4125855Z symbol: class BiMap; 2022-08-16T00:09:07.4126189Z location: class LoggingUtils; 2022-08-16T00:09:07.4126674Z src/main/java/org/broadinstitute/he",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217242480:12376,config,config,12376,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217242480,1,['config'],['config']
Modifiability,"roadinstitute/hellbender/engine/filters/CountingVariantFilter.java:197: error: cannot find symbol; 2022-08-16T22:45:53.8024772Z @VisibleForTesting; 2022-08-16T22:45:53.8025036Z symbol: class VisibleForTesting; 2022-08-16T22:45:53.8025212Z location: class CountingVariantFilter; 2022-08-16T22:45:53.8032154Z src/main/java/org/broadinstitute/hellbender/cmdline/GATKPlugin/GATKReadFilterPluginDescriptor.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T22:45:53.8035089Z src/main/java/org/broadinstitute/hellbender/engine/filters/ReadFilter.java:75: error: cannot find symbol; 2022-08-16T22:45:53.8035234Z @VisibleForTesting; 2022-08-16T22:45:53.8035505Z symbol: class VisibleForTesting; 2022-08-16T22:45:53.8035658Z location: class ReadFilter; 2022-08-16T22:45:53.8087327Z src/main/java/org/broadinstitute/hellbender/utils/config/ConfigFactory.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T22:45:53.8103864Z src/main/java/org/broadinstitute/hellbender/utils/config/GATKConfig.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T22:45:53.8113680Z src/main/java/org/broadinstitute/hellbender/utils/variant/GATKVariantContextUtils.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T22:45:53.8117654Z src/main/java/org/broadinstitute/hellbender/utils/LoggingUtils.java:3: error: package com.google.common.collect does not exist; 2022-08-16T22:45:53.8118430Z src/main/java/org/broadinstitute/hellbender/utils/LoggingUtils.java:4: error: package com.google.common.collect does not exist; 2022-08-16T22:45:53.8124030Z src/main/java/org/broadinstitute/hellbender/utils/LoggingUtils.java:32: error: cannot find symbol; 2022-08-16T22:45:53.8124383Z private static BiMap<Log.LogLevel, Level> loggingLevelNamespaceMap;; 2022-08-16T22:45:53.8124657Z symbol: class BiMap; 2022-08-16T22:45:53.8124810Z location: class LoggingUtils; 2022-08-16T22:45:53.8125227Z src/main/java/org/broadinstitute/he",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217253370:14414,config,config,14414,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217253370,1,['config'],['config']
Modifiability,"roblem with the following location: '/home/jeremie/GATK/build/classes/java/main'. Reason: Task ':gatkDoc' uses this output of task ':condaStandardEnvironmentDefinition' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem.; - Gradle detected a problem with the following location: '/home/jeremie/GATK/build/resources/main'. Reason: Task ':gatkDoc' uses this output of task ':condaStandardEnvironmentDefinition' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':gatkDoc'.; > Javadoc generation failed. Generated Javadoc options file (useful for troubleshooting): '/home/jeremie/GATK/build/tmp/gatkDoc/javadoc.options'. * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full insights. * Get more help at https://help.gradle.org. Deprecated Gradle features were used in this build, making it incompatible with Gradle 8.0. You can use '--warning-mode all' to show the individual deprecation warnings and determine if they come from your own scripts or plugins. See https://docs.gradle.org/7.3.2/userguide/command_line_interface.html#sec:command_line_warnings. Execution optimizations have been disabled for 1 invalid unit(s) of work during this build to ensure correctness.; Please consult deprecation warnings for more details. BUILD FAILED in 33s; 5 actionable tasks: 5 executed; ```; which does not seem related to any changes I made.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7936#issuecomment-1202544500:1832,plugin,plugins,1832,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7936#issuecomment-1202544500,1,['plugin'],['plugins']
Modifiability,"s where results changed:. - For the snpeff test, since the behavior on this branch seems more correct to me than master, I tried running the GATK4 test case inputs with GATK3, and it produces exactly the same results as this branch does. So I think that issue was introduced by the original GATK4 port, and is fixed in this branch.; - The rest of the tests with changed results don't seem to hit your breakpoint, though. So I think we need to figure out why they changed, and maybe also compare them with GATK3 (which can be a pain because the output format is slightly different).; - As you mentioned, you changed the reference for testEvalTrackWithoutGenotypesWithSampleFields, which seems to have only affected the number of loci processed. So I'm unclear why that change was necessary. If the test truly should have been failing without this change, will it still fail if the change is reverted ? If not, can we fix it, and either way there should be a negative test for that case. A few other general comments:. - I changed this PR to `draft` mode for now, which just better categorizes it for our internal workflow purposes. When its ready for a detailed code review we can remove the `draft` status.; - The `HashMap<FeatureInput<VariantContext>, HashMap<String, Collection<VariantContext>>>` can be wrapped in a class with just a couple of methods, so we don't have to manifest that long type all over the place.; - I know this PR still in an interim state, but passing the VariantWalker in as an argument to the comp methods doesn't seem like a step forward to me. If we can't solve that problem completely in this PR (which is fine, I'm all for trying to contain this), are those changes necessary ? Perhaps that part should just wait for the next round.; - Any new classes/methods should use `final` for variables and parameters wherever applicable, and public classes and methods should have javadoc.; - Finally, I'm curious if you've tried any perf testing on this branch ? Is it better ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-744689987:1888,variab,variables,1888,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-744689987,1,['variab'],['variables']
Modifiability,"s(DefaultCredentialsProvider.java:124); at shaded.cloud_nio.com.google.auth.oauth2.GoogleCredentials.getApplicationDefault(GoogleCredentials.java:127); at shaded.cloud_nio.com.google.auth.oauth2.GoogleCredentials.getApplicationDefault(GoogleCredentials.java:100); at com.google.cloud.ServiceOptions.defaultCredentials(ServiceOptions.java:304); at com.google.cloud.ServiceOptions.<init>(ServiceOptions.java:278); at com.google.cloud.storage.StorageOptions.<init>(StorageOptions.java:83); at com.google.cloud.storage.StorageOptions.<init>(StorageOptions.java:31); at com.google.cloud.storage.StorageOptions$Builder.build(StorageOptions.java:78); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.setGlobalNIODefaultOptions(BucketUtils.java:382); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:183); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. Produced by pulling the docker image, **shutting off the internet connection**, mounting [helloHaplotypeCaller](https://drive.google.com/file/d/0B7akc6CTmxIHdy11R1M3ZjJJdUU/view), and running:. ```shell; docker run \; --rm \; -v /Users/kshakir/Downloads/helloHaplotypeCaller:/data \; broadinstitute/gatk:4.0.11.0 \; gatk \; HaplotypeCaller \; -R /data/ref/human_g1k_b37_20.fasta \; -I /data/inputs/NA12878_wgs_20.bam \; -O test.vcf; ```. Adding in a `GOOGLE_APPLICATION_CREDENTIALS` environment variable short circuits the above stack trace. ```shell; docker run \; -e GOOGLE_APPLICATION_CREDENTIALS=whatever; --rm \; -v /Users/kshakir/Downloads/helloHaplotypeCaller:/data \; broadinstitute/gatk:4.0.11.0 \; gatk \; HaplotypeCaller \; -R /data/ref/human_g1k_b37_20.fasta \; -I /data/inputs/NA12878_wgs_20.bam \; -O test.vcf; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3491#issuecomment-443830843:4199,variab,variable,4199,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3491#issuecomment-443830843,1,['variab'],['variable']
Modifiability,"s/theano/__init__.py"", line 66, in <module>; from theano.compile import (; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/compile/__init__.py"", line 10, in <module>; from theano.compile.function_module import *; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/compile/function_module.py"", line 21, in <module>; import theano.compile.mode; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/compile/mode.py"", line 10, in <module>; import theano.gof.vm; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/vm.py"", line 662, in <module>; from . import lazylinker_c; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 42, in <module>; location = os.path.join(config.compiledir, 'lazylinker_ext'); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configparser.py"", line 333, in __get__; self.__set__(cls, val_str); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configparser.py"", line 344, in __set__; self.val = self.filter(val); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configdefaults.py"", line 1745, in filter_compiledir; "" '%s'. Check the permissions."" % path); ValueError: Unable to create the compiledir directory '/root/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-stretch-sid-x86_64-3.6.2-64'. Check the permissions. at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:126); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeArgs(PythonScriptExecutor.java:170); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeCommand(PythonScriptExecutor.java:79); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.checkPythonEnvironmentForPackage(PythonScriptExecutor.java:192); at org.broadinstitute",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-496007081:5007,config,configparser,5007,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-496007081,1,['config'],['configparser']
Modifiability,seems like a good candidate to be moved into the config files...,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3552#issuecomment-327585040:49,config,config,49,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3552#issuecomment-327585040,1,['config'],['config']
Modifiability,"serException.BadArgumentValue(""ERC/gt_mode"",""you cannot request reference confidence output and GENOTYPE_GIVEN_ALLELES at the same time"");; ; SCAC.genotypeArgs.STANDARD_CONFIDENCE_FOR_EMITTING = -0.0;; SCAC.genotypeArgs.STANDARD_CONFIDENCE_FOR_CALLING = -0.0;; ; -; // also, we don't need to output several of the annotations; annotationsToExclude.add(""ChromosomeCounts"");; annotationsToExclude.add(""FisherStrand"");; @@ -651,6 +651,9 @@ public class HaplotypeCaller extends ActiveRegionWalker<List<VariantContext>, In; if (!SCAC.annotateAllSitesWithPLs); logger.info(""All sites annotated with PLs forced to true for reference-model confidence output"");; SCAC.annotateAllSitesWithPLs = true;; + } else if ( ! doNotRunPhysicalPhasing ) {; + doNotRunPhysicalPhasing = true;; + logger.info(""Disabling physical phasing, which is supported only for reference-model confidence output"");; }; ; if ( SCAC.AFmodel == AFCalcFactory.Calculation.EXACT_GENERAL_PLOIDY ); @@ -678,7 +681,7 @@ public class HaplotypeCaller extends ActiveRegionWalker<List<VariantContext>, In; if( SCAC.genotypingOutputMode == GenotypingOutputMode.GENOTYPE_GIVEN_ALLELES && consensusMode ); throw new UserException(""HaplotypeCaller cannot be run in both GENOTYPE_GIVEN_ALLELES mode and in consensus mode. Please choose one or the other."");; ; - genotypingEngine = new HaplotypeCallerGenotypingEngine( getToolkit(), SCAC, tryPhysicalPhasing);; + genotypingEngine = new HaplotypeCallerGenotypingEngine( getToolkit(), SCAC, !doNotRunPhysicalPhasing);; // initialize the output VCF header; final VariantAnnotatorEngine annotationEngine = new VariantAnnotatorEngine(Arrays.asList(annotationClassesToUse), annotationsToUse, annotationsToExclude, this, getToolkit());; ; @@ -699,8 +702,10 @@ public class HaplotypeCaller extends ActiveRegionWalker<List<VariantContext>, In; VCFConstants.DEPTH_KEY,; VCFConstants.GENOTYPE_PL_KEY);; ; - if ( tryPhysicalPhasing ); - headerInfo.add(new VCFFormatHeaderLine(HAPLOTYPE_CALLER_PHASING_KEY, VCFHeaderL",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5727#issuecomment-470679237:3579,extend,extends,3579,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5727#issuecomment-470679237,1,['extend'],['extends']
Modifiability,"sh Babadi <mehrtash@broadinstitute.org>; Date: Wed Nov 15 01:50:03 2017 -0500. Polished code, ready for review; ; gCNV computational kernel (initial release); ; renaming gammas_s to psi_s to uniformity (sample-specific unexplained variance); ; renamed determine_ploidy_and_depth.py to cohort_determine_ploidy_and_depth.py; finite-temperature forward-backward algorithm; in the ploidy model, replaced alpha_j (NB over-dispersion) with psi_j (unexplained variance) for uniformity. Also, added the possibility of sample-specific unexplained variance in the germline contig ploidy model; ; updated I/O routines and CLIs according to team discussion; ; updated I/O routines and CLIs according to team discussion; ; changed the output layout of the ploidy determination tool; refactored parts of io.py; upped the version to 0.3 as it is not backwards compatible anymore; ; case ploidy determination tool from a given ploidy model; major code cleanup and refactoring of I/O module; refactoring of common CLI script snippets; ; removed all ""targets""; some code cleanup; ; pad flat class bitmask w/ a given padding value in the hybrid q_c_expectation_mode; option to disable annealing and keep the temperature fixed; ; bugfix in finite-temperature forward-backward; further refactoring of model I/O; ; the option to take a previously trained model as starting point in cohort CLI; the option to take previous calls as a starting point in cohort CLI; ; option to save and load adamax moments; ; import/export adamax bias correction tensor; ; refactoring related to fancy opt I/O; added average ploidy column to read depth; updated docs of hybrid inference; ; modeling intervals can span multiple contigs now; ploidy can change; across contigs with no issue; ; save/load adamax state to .npy instead of .tsv for speed; ; part 1 of doc updates; ; part 2 of doc updates; ; part 3 of doc updates; ; part 4 of doc updates; ; bumped version to 0.5; readme; ; update readme; ; last minute stylistic doc updates.; ````",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598:11261,refactor,refactoring,11261,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598,4,['refactor'],['refactoring']
Modifiability,"sing ) {; + doNotRunPhysicalPhasing = true;; + logger.info(""Disabling physical phasing, which is supported only for reference-model confidence output"");; }; ; if ( SCAC.AFmodel == AFCalcFactory.Calculation.EXACT_GENERAL_PLOIDY ); @@ -678,7 +681,7 @@ public class HaplotypeCaller extends ActiveRegionWalker<List<VariantContext>, In; if( SCAC.genotypingOutputMode == GenotypingOutputMode.GENOTYPE_GIVEN_ALLELES && consensusMode ); throw new UserException(""HaplotypeCaller cannot be run in both GENOTYPE_GIVEN_ALLELES mode and in consensus mode. Please choose one or the other."");; ; - genotypingEngine = new HaplotypeCallerGenotypingEngine( getToolkit(), SCAC, tryPhysicalPhasing);; + genotypingEngine = new HaplotypeCallerGenotypingEngine( getToolkit(), SCAC, !doNotRunPhysicalPhasing);; // initialize the output VCF header; final VariantAnnotatorEngine annotationEngine = new VariantAnnotatorEngine(Arrays.asList(annotationClassesToUse), annotationsToUse, annotationsToExclude, this, getToolkit());; ; @@ -699,8 +702,10 @@ public class HaplotypeCaller extends ActiveRegionWalker<List<VariantContext>, In; VCFConstants.DEPTH_KEY,; VCFConstants.GENOTYPE_PL_KEY);; ; - if ( tryPhysicalPhasing ); - headerInfo.add(new VCFFormatHeaderLine(HAPLOTYPE_CALLER_PHASING_KEY, VCFHeaderLineCount.UNBOUNDED, VCFHeaderLineType.String, ""Physical phasing information, each unique ID within a given sample (but not across samples) connects alternate alleles as occurring on the same haplotype""));; + if ( ! doNotRunPhysicalPhasing ) {; + headerInfo.add(new VCFFormatHeaderLine(HAPLOTYPE_CALLER_PHASING_ID_KEY, 1, VCFHeaderLineType.String, ""Physical phasing ID information, where each unique ID within a given sample (but not across samples) connects records within a phasing group""));; + headerInfo.add(new VCFFormatHeaderLine(HAPLOTYPE_CALLER_PHASING_GT_KEY, 1, VCFHeaderLineType.String, ""Physical phasing haplotype information, describing how the alternate alleles are phased in relation to one another""));; + }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5727#issuecomment-470679237:4352,extend,extends,4352,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5727#issuecomment-470679237,1,['extend'],['extends']
Modifiability,"st and there's no batch api for it? Multi layer docker builds are pretty standard from what I understand. . It sounds like your suggestions are talking about 2 slightly different issues to me. 1. Too many layers:. We typically have squashed the GATK docker images, but we recently switched to building our release images with google cloud build. Since squash is *STILL* an experimental feature in docker we've had trouble getting it to work there. Since the size reduction was pretty minimal from squashing we figured it would be ok to not prioritize it. It's definitely possible for us to consolidate various layers in the build. Or manually squash the images. We can take a look for our next release. Wide workflows on azure are something we need to support. 2. Docker size reduction:; I've spend a lot of time looking at this in the past. Our docker image is huge, but it's mostly due to the massive size of our python and R dependencies. I've done a bunch of work reducing temporary files in independent layers and using multiple stages to reduce the size. There's not much low hanging fruit left there. Similarly, moving to alpine is tricky an has limited benefit. GATK packages a number of C libraries which do not work out of the box on alpine due to the different C runtime. (At least that was the case the last time I investigated it a few years ago. ) I suspect there's a way to port things so they work on it, but it's not something we can do now. It also wouldn't be much of a help, the base image is completely dwarfed by piles of python and R dependencies which are very difficult to safely trim. Anyway, that's the state of things. We've considered a java only image for a while which would be much smaller than the current one. (although still fat by most docker standards...). We've never released one publicly because it seemed like it might cause confusion, but it's a reasonable possibility. . If you have any secret methods to reduce the size of python or R installations we're ha",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8684#issuecomment-1934859427:1203,layers,layers,1203,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8684#issuecomment-1934859427,1,['layers'],['layers']
Modifiability,"stributions/timeseries.py"", line 1, in <module>; import theano.tensor as tt; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/__init__.py"", line 66, in <module>; from theano.compile import (; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/compile/__init__.py"", line 10, in <module>; from theano.compile.function_module import *; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/compile/function_module.py"", line 21, in <module>; import theano.compile.mode; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/compile/mode.py"", line 10, in <module>; import theano.gof.vm; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/vm.py"", line 662, in <module>; from . import lazylinker_c; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 42, in <module>; location = os.path.join(config.compiledir, 'lazylinker_ext'); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configparser.py"", line 333, in __get__; self.__set__(cls, val_str); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configparser.py"", line 344, in __set__; self.val = self.filter(val); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configdefaults.py"", line 1745, in filter_compiledir; "" '%s'. Check the permissions."" % path); ValueError: Unable to create the compiledir directory '/root/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-stretch-sid-x86_64-3.6.2-64'. Check the permissions. at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:126); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeArgs(PythonScriptExecutor.java:170); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeCommand(PythonScriptExecutor.java:79); at org.broadinstitu",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-496007081:4873,config,configparser,4873,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-496007081,1,['config'],['configparser']
Modifiability,"t$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:546); at java.util.stream.AbstractPipeline.evaluateToArrayNode(AbstractPipeline.java:260); at java.util.stream.DoublePipeline.toArray(DoublePipeline.java:530); at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticGenotypingEngine.getGermlineAltAlleleFrequencies(SomaticGenotypingEngine.java:354); at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticGenotypingEngine.getNegativeLogPopulationAFAnnotation(SomaticGenotypingEngine.java:337); at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticGenotypingEngine.callMutations(SomaticGenotypingEngine.java:155); at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2Engine.callRegion(Mutect2Engine.java:259); at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2.apply(Mutect2.java:306); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:200); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:173); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1085); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Using GATK jar /root/gatk.jar defined in environment variable GATK_LOCAL_JAR; ```. one happened while working on chr21, the other on chr9",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7494#issuecomment-936771625:2971,variab,variable,2971,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7494#issuecomment-936771625,1,['variab'],['variable']
Modifiability,t&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9kcmFnc3RyL0NhbGlicmF0ZURyYWdzdHJNb2RlbC5qYXZh) | `70.345% <ø> (ø)` | |; | [...r/utils/fasta/CachingIndexedFastaSequenceFile.java](https://codecov.io/gh/broadinstitute/gatk/pull/7920/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9mYXN0YS9DYWNoaW5nSW5kZXhlZEZhc3RhU2VxdWVuY2VGaWxlLmphdmE=) | `70.330% <ø> (-1.099%)` | :arrow_down: |; | [...t/java/org/broadinstitute/hellbender/MainTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/7920/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9NYWluVGVzdC5qYXZh) | `2.564% <ø> (-82.182%)` | :arrow_down: |; | [...Plugin/GATKAnnotationPluginDescriptorUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/7920/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL0dBVEtQbHVnaW4vR0FUS0Fubm90YXRpb25QbHVnaW5EZXNjcmlwdG9yVW5pdFRlc3QuamF2YQ==) | `7.219% <ø> (-81.016%)` | :arrow_down: |; | [...GATKPlugin/GATKReadFilterPluginDescriptorTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/7920/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL0dBVEtQbHVnaW4vR0FUS1JlYWRGaWx0ZXJQbHVnaW5EZXNjcmlwdG9yVGVzdC5qYXZh) | `0.484% <ø> (-88.136%)` | :arrow_down: |; | [...lbender/engine/AssemblyRegionIteratorUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/7920/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_c,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7920#issuecomment-1239413884:2777,Plugin,Plugin,2777,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7920#issuecomment-1239413884,1,['Plugin'],['Plugin']
Modifiability,"te). Strands of the intervals indicate whether the distal target intervals are; * upstream or downstream of their proposed breakpoints: true indicates that the breakpoint is upstream of the interval; * start position; false indicates that the breakpoint is downstream of the interval end position; */; ```. What else would you like to see documented there? . - The use of the word strand in this case is largely driven by a mapping of these data structures to the BEDPE format, which is the older format for representing breakpoints implied by paired-end mapping data without assembly. If you only consider read pair mappings, strand has the natural interpretation of being the strand to which reads aligned. For example, a deletion's two intervals have strands `+` and `-` because the `+` reads align at left breakpoint and `-` reads align near the right breakpoint. Extending the concept to supplementary mappings of split reads muddies the concept a bit, which made me change the definition of strand to the existing one: whether the evidence suggests a breakpoint upstream of the interval start or downstream of the interval end. . - I created `StrandedInterval` mostly just as a data container since I was often passing around an interval and an associated strand, and using them in conjunction with the `PairedStrandedIntervalTree` data structure. My goal with those was to have them be utility classes that could be used by anyone without regards to the particular mechanics of imprecise evidence clustering I've implemented here. I'd prefer to put the definition of how we're interpreting the interval and strand in our logic classes (`BreakpointEvidence`, `EvidenceTargetLink`, and EvidenceTargetLinkClusterer`). Does that make sense?. - A ""distal target region"" can be represented by a `StrandedInterval`. So can the original, proximal (non-distal) location of the breakpoint evidence. An `EvidenceTargetLink` has the two `StrandedInterval` objects representing the proximal and distal loca",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3628#issuecomment-333857471:1758,Extend,Extending,1758,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3628#issuecomment-333857471,1,['Extend'],['Extending']
Modifiability,ter yarn --conf spark.kryoserializer.buffer.max=512m --conf spark.driver.maxResultSize=0 --conf spark.driver.userClassPathFirst=false --conf spark.io.compression.codec=lzf --conf spark.yarn.executor.memoryOverhead=600 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 /share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-spark.jar CountReadsSpark --input /project/casa/gcad/test/HG04302.alt_bwamem_GRCh38DH.20150718.GBR.low_coverage.cram --reference file:///restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --spark-master yarn; 2019-01-07 11:33:18 WARN SparkConf:66 - The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 2019-01-07 11:33:19 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 11:33:24.377 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 11:33:24.549 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 11:33:26.271 INFO CountReadsSpark - ------------------------------------------------------------; 11:33:26.272 INFO CountReadsSpark - The Genome Analysis Toolkit (GATK) v4.0.12.0; 11:33:26.272 INFO CountReadsSpark - For support and documentat,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:1963,config,configuration,1963,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,1,['config'],['configuration']
Modifiability,thanks for the review @kcibul. I made some changes accordingly. re: PrepareCallset file of sample names. That would be nice! It would make this workflow simpler and it also simplifies the access requirements for PrepareCallset. re: Dockstore. We actually ruled this out because Terra says that the definition of a method configuration can change automatically if its updated in dockstore. Which can be useful but it adds a security risk since a compromised Dockstore can change the definition of the production AoU extraction WDL which runs with highly elevated permissions. We already have a script that creates method configurations from github so I can probably add something a little hacky to resolve relative imports to the raw github file that it refers to.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7242#issuecomment-846494686:321,config,configuration,321,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7242#issuecomment-846494686,2,['config'],"['configuration', 'configurations']"
Modifiability,"that worked, thanks. I had a typo in the variable ""recaFile"" vs ""recalFile"". . how does this group want to proceed with alerting on failures? email? hipchat? slack? carrier pigeon? and who should be on the list?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1609#issuecomment-227783654:41,variab,variable,41,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1609#issuecomment-227783654,1,['variab'],['variable']
Modifiability,"the implementation of posterior sampling, 3) some shape/dimshuffle operations, and other things along these lines. Using a single test shard of 20 1kGP WES samples x 1000 intervals, I have verified determinism/reproducibility for DetermineGermlineContigPloidy COHORT/CASE modes, GermlineCNVCaller COHORT/CASE modes, and PostprocessGermlineCNVCalls. Numerical results are also relatively close to those from 4.4.0.0 for all identifiable call and model quantities (albeit far outside any reasonable exact-match thresholds, most likely due to differences in RNG, sampling, and the aforementioned priors). Some remaining TODOs:. - [x] Rebuild and push the base Docker. EDIT: Mostly covered by #8610, but this also includes an addition of `libblas-dev`.; - [x] Update expected results for integration tests, perhaps add any that might be missing. EDIT: These were generated on WSL Ubuntu 20.04.2, we'll see if things pass on 22.04. Note that changing the ARD priors does change the *names* of the expected files, since the transform is appended to the corresponding variable name. DetermineGermlineContigPloidy and PostprocessGermlineCNVCalls are missing exact-match tests and should probably have some, but I'll leave that to someone else.; - [x] Update other python integration tests.; - [x] Clean up some of the changes to the priors.; - [x] Clean up some TODO comments that I left to track code changes that might result in changed numerics. I'll try to go through and convert these to PR comments in an initial review pass.; - [x] Test over multiple shards on WGS and WES. Probably some scientific tests on ~100 samples in both cohort and case mode would do the trick. We should also double check runtime/memory performance (I noted ~1.5x speedups, but didn't measure carefully; I also want to make sure the changes to posterior sampling didn't introduce any memory issues). @mwalker174 will ping you when a Docker is ready! Might be good to loop in Isaac and/or Jack as well.; - [x] Perhaps add back ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8561#issuecomment-1847549285:2232,variab,variable,2232,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8561#issuecomment-1847549285,1,['variab'],['variable']
Modifiability,this is a bigger project - tribble async reading is very broken. we need to rewrite that part and maybe expose sync/async via factory methods.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1597#issuecomment-198510701:76,rewrite,rewrite,76,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1597#issuecomment-198510701,1,['rewrite'],['rewrite']
Modifiability,this is fine - all those used a variable as a second argument to the semantics is equivalent and this change improves readability (less surpirse to the reader),MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1616#issuecomment-200638990:32,variab,variable,32,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1616#issuecomment-200638990,1,['variab'],['variable']
Modifiability,this is pretty scary looking code and it's not tied with the dataflow 'walker' interface and so i'm assuming it's just a temporary step to putting all of this in and start refactoring. fine to live in the dev package for now. It will show up on the commandline though - maybe we need a way to hide those dev tools. ; @droazen your turn,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/523#issuecomment-103708285:172,refactor,refactoring,172,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/523#issuecomment-103708285,1,['refactor'],['refactoring']
Modifiability,"toHDFSSpark - Defaults.REFERENCE_FASTA : null; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Defaults.USE_CRAM_REF_DOWNLOAD : false; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Deflater IntelDeflater; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Inflater IntelInflater; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Initializing engine; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4769b07b] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@5ef60048].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4769b07b] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@5ef60048].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase).; log4j:WARN Please initia",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2618#issuecomment-296871363:1834,variab,variable,1834,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2618#issuecomment-296871363,1,['variab'],['variable']
Modifiability,tools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 /scratch/home/int/eva/username/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-spark.jar PathSeqPipelineSpark --spark-master spark://xx.xx.xx.xx:7077 --input test_sample.bam --filter-bwa-image hg19mini.fasta.img --kmer-file hg19mini.hss --min-clipped-read-length 70 --microbe-fasta e_coli_k12.fasta --microbe-bwa-image e_coli_k12.fasta.img --taxonomy-file e_coli_k12.db --output output.pathseq.bam --verbosity DEBUG --scores-output output.pathseq.txt; 17:39:18.382 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 17:39:18.825 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/scratch/home/int/eva/username/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 17:39:18.857 DEBUG NativeLibraryLoader - Extracting libgkl_compression.so to /tmp/username/libgkl_compression3681606702485397808.so; 17:39:19.218 INFO PathSeqPipelineSpark - ------------------------------------------------------------; 17:39:19.218 INFO PathSeqPipelineSpark - The Genome Analysis Toolkit (GATK) v4.0.3.0; 17:39:19.218 INFO PathSeqPipelineSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:39:19.219 INFO PathSeqPipelineSpark - Executing as username@node016 on Linux v2.6.32-220.4.1.el6.x86_64 amd64; 17:39:19.220 INFO PathSeqPipelineSpark - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_131-b11; 17:39:19.220 INFO PathSeqPipelineSpark - ,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:2348,variab,variables,2348,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,2,"['config', 'variab']","['configured', 'variables']"
Modifiability,"tor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 /share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-spark.jar CountReadsSpark --input /project/casa/gcad/test/HG04302.alt_bwamem_GRCh38DH.20150718.GBR.low_coverage.cram --reference file:///restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --spark-master yarn; 2019-01-07 11:33:18 WARN SparkConf:66 - The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 2019-01-07 11:33:19 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 11:33:24.377 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 11:33:24.549 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 11:33:26.271 INFO CountReadsSpark - ------------------------------------------------------------; 11:33:26.272 INFO CountReadsSpark - The Genome Analysis Toolkit (GATK) v4.0.12.0; 11:33:26.272 INFO CountReadsSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:33:26.272 INFO CountReadsSpark - Executing as farrell@scc-hadoop.bu.edu on Linux v2.6.32-754.6.3.el6.x86_64 amd64; 11:33:26.273 INFO CountReadsSpark - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 11:33:26.273 INFO CountReadsSpark - Start Date/Time: January 7, 2019 11:33:24 AM EST; 11:33:26.273 INFO CountReadsSpark - ------------------------------------------------------------; 11:33:26.273 IN",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:2355,variab,variables,2355,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,2,"['config', 'variab']","['configured', 'variables']"
Modifiability,tractPipeline.java:472); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485); at org.broadinstitute.hellbender.engine.VariantWalker.traverse(VariantWalker.java:102); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1085); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Using GATK jar /root/gatk.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx3500m -jar /root/gatk.jar Funcotator --data-sources-path /cromwell_root/datasources_dir --ref-version hg38 --output-file-format VCF -R gs://genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.fasta -V gs://fc-secure-d2a2d895-a7af-4117-bdc7-652d7d268324/94e769a1-28e1-4bd7-b09f-9e47fb7d8352/omics_mutect2/14fe5685-740c-4e09-9d1a-8c8d14c0ae5b/call-mutect2/Mutect2/2de52f4f-eea0-4ec7-acc1-f47b1a2d1e6c/call-Filter/attempt-2/CDS-2jucw0.hg38-filtered.vcf.gz -O CDS-2jucw0.hg38-filtered.vcf.gz.annotated.vcf.gz -L /cromwell_root/ccleparams/region_file_wgs.list --annotation-default normal_barcode: --annotation-default tumor_barcode:NP5 --annotation-default Center:DEPMAP --annotation-default source:Unknown; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6651#issuecomment-1182102653:7041,variab,variable,7041,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6651#issuecomment-1182102653,1,['variab'],['variable']
Modifiability,"ts header (either proactively or on demand), but transparently to ; application code that is running against the SAMRecord API.; This would allow SAM headers to be transmitted out-of-band in a way that ; depends on the execution environment. Depending on the environment, ; this might be done by proactive broadcast, or you could think of the ; header tag as a promise to retrieve the header if/when it is needed. ; The size and complexity of the header tag might also depend on the ; execution environment. If the execution environment only supports a ; small finite number of headers, the header tag could be a small integer, ; or in a different execution environment it could be; a unique hash of the header or something like that. Memory footprint in ; the receiver is minimized because many SAMRecords can all share the same ; header object.; This requires more work to support in each execution environment, but it ; seems like it could be efficient and allows application code written to ; operate on SAMRecords to be portable; across different execution environments without having to contend with ; the possible presence of headerless SAMRecords. -Bob. On 9/17/15 4:28 PM, droazen wrote:. > @davidadamsphd https://github.com/davidadamsphd, @lbergelson ; > https://github.com/lbergelson, and myself met for an hour or two ; > just now to discuss this issue, and after reviewing all the options I ; > think we were convinced by the following argument:; > ; > The |SAMRecord| class currently allows its header to be set to null, ; > so if there are cases where the class won't function properly or can ; > enter into an inconsistent state when a header is not present these ; > should be treated as bugs and patched, and we should add unit tests to ; > htsjdk to prove that headerless |SAMRecords| function properly. Then ; > in hellbender we can freely use headerless |SAMRecords| everywhere, ; > only restoring the header to the record when writing out the final bam ; > (since our bam writers",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141451518:2594,portab,portable,2594,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141451518,1,['portab'],['portable']
Modifiability,tureManager - Using codec VCFCodec to read file gs://fc-ac4624cb-a8fc-49a2-b071-d3a0ae799418/cb1feccb-0a69-42bf-ba5f-fde762934a59/Mutect2/fe3623c8-0eaf-4cd4-9f81-d1fda4073f2e/call-Filter/22.hg38-filtered.vcf; 01:39:08.399 INFO FilterAlignmentArtifacts - Done initializing engine; 01:39:09.523 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 01:39:09.565 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 01:39:09.566 INFO IntelPairHmm - Available threads: 4; 01:39:09.566 INFO IntelPairHmm - Requested threads: 4; 01:39:09.566 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 01:39:09.567 INFO ProgressMeter - Starting traversal; 01:39:09.567 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; munmap_chunk(): invalid pointer; Using GATK jar /root/gatk.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx11500m -jar /root/gatk.jar FilterAlignmentArtifacts -R gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta -V gs://fc-ac4624cb-a8fc-49a2-b071-d3a0ae799418/cb1feccb-0a69-42bf-ba5f-fde762934a59/Mutect2/fe3623c8-0eaf-4cd4-9f81-d1fda4073f2e/call-Filter/22.hg38-filtered.vcf -I gs://fc-ac4624cb-a8fc-49a2-b071-d3a0ae799418/209d1183-ed9a-4755-a4b3-d595797640ea/PreProcessingForVariantDiscovery_GATK4/9f7c0ab6-b61b-4797-92f1-7929bbf677d8/call-GatherBamFiles/22.hg38.bam --bwa-mem-index-image /cromwell_root/gatk-test-data/mutect2/Homo_sapiens_assembly38.index_bundle -O 22.hg38-filtered.vcf; 2020/07/25 01:46:01 Starting delocalization.; 2020/07/25 01:46:02 Delocalization script execution started...; 2020/07/25 01:46:02 Delocalizing output /cromwell_root/memory_retry_rc -> gs://fc-ac4624cb-a,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-664539860:5507,variab,variable,5507,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-664539860,1,['variab'],['variable']
Modifiability,typingEngineUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5365/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9Bc3NlbWJseUJhc2VkQ2FsbGVyR2Vub3R5cGluZ0VuZ2luZVVuaXRUZXN0LmphdmE=) | `100% <100%> (ø)` | `31 <0> (ø)` | :arrow_down: |; | [...ypecaller/AssemblyBasedCallerGenotypingEngine.java](https://codecov.io/gh/broadinstitute/gatk/pull/5365/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9Bc3NlbWJseUJhc2VkQ2FsbGVyR2Vub3R5cGluZ0VuZ2luZS5qYXZh) | `89.45% <100%> (+0.049%)` | `89 <0> (+1)` | :arrow_up: |; | [...stitute/hellbender/cmdline/CommandLineProgram.java](https://codecov.io/gh/broadinstitute/gatk/pull/5365/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL0NvbW1hbmRMaW5lUHJvZ3JhbS5qYXZh) | `83.117% <0%> (+1.948%)` | `43% <0%> (+1%)` | :arrow_up: |; | [...stitute/hellbender/utils/config/ConfigFactory.java](https://codecov.io/gh/broadinstitute/gatk/pull/5365/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9jb25maWcvQ29uZmlnRmFjdG9yeS5qYXZh) | `76.398% <0%> (+3.727%)` | `45% <0%> (+2%)` | :arrow_up: |; | [...r/arguments/CopyNumberArgumentValidationUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/5365/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL2FyZ3VtZW50cy9Db3B5TnVtYmVyQXJndW1lbnRWYWxpZGF0aW9uVXRpbHMuamF2YQ==) | `77.778% <0%> (+6.173%)` | `20% <0%> (+1%)` | :arrow_up: |; | [...tute/hellbender/utils/runtime/ProcessSettings.java](https://codecov.io/gh/broadinstitute/gatk/pull/5365/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9ydW50aW1lL1Byb2Nlc3NTZXR0aW5ncy5qYXZh) | `93.75% <0%> (+6.25%)` | `18% <0%> (+2%)` | :arrow_up: |; | [...te/hellbender/utils/python/PythonExecutorBase.java](https://codecov.io/gh/br,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5365#issuecomment-433471265:1961,config,config,1961,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5365#issuecomment-433471265,2,"['Config', 'config']","['ConfigFactory', 'config']"
Modifiability,"ub.com> wrote:; > ; > @SHuang-Broad commented on this pull request.; > ; > In src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/prototype/CpxVariantDetector.java:; > ; > > + this.tigWithInsMappings = new AssemblyContigWithFineTunedAlignments(contig, tigWithInsMappings.insertionMappings);; > +; > + this.basicInfo = new BasicInfo(contig);; > +; > + annotate(refSequenceDictionary);; > + }; > +; > + private static List<AlignmentInterval> deOverlapAlignments(final List<AlignmentInterval> originalAlignments,; > + final SAMSequenceDictionary refSequenceDictionary) {; > + final List<AlignmentInterval> result = new ArrayList<>(originalAlignments.size());; > + final Iterator<AlignmentInterval> iterator = originalAlignments.iterator();; > + AlignmentInterval one = iterator.next();; > + while (iterator.hasNext()) {; > + final AlignmentInterval two = iterator.next();; > + // TODO: 11/5/17 an edge case is possible where the best configuration contains two alignments,; > + // one of which contains a large gap, and since the gap split happens after the configuration scoring,; > I agree it is backwards. But...; > ; > The reason was that the (naive) alignment configuration scoring module rightnow uses MQ and AS (aligner score) for picking the ""best"" configuration (i.e. sub-list of the alignments given by aligner), which would be technically wrong if we were to split the gap and to simply grab the originating alignment's values.; > ; > This is especially true for AS, whose recomputing takes more time, and code, and forces us to know how AS are computed in the aligner so that there's no bias in computing the scores of naive alignments vs gap-split alignments (may not matter in practice, but still takes more code to compute).; > ; > Lots of the code in the discovery stage was devoted actually to alignment related acrobatics and edge cases so that the breakpoints we could resolve are as accurate as possible.; > I've kept in mind your wisdom that different aligners may ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3805#issuecomment-350618009:1509,config,configuration,1509,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3805#issuecomment-350618009,2,['config'],['configuration']
Modifiability,"ugh, made some progress but got stuck because that code requires that penalties are less than 256 and match/mismatch are below 128. Or default values are match 200, mismatch -150, open -260, extend -11. work in progress is in branch ak_native_smithwaterman_sse . @gspowley FYI",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1812#issuecomment-218537669:191,extend,extend,191,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1812#issuecomment-218537669,1,['extend'],['extend']
Modifiability,urces.v1.6.20190124s/chr1_b_bed/hg38/chr1_b_bed.tsv; > 12:28:17.997 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/chr1_b_bed.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/chr1_b_bed/hg38/chr1_b_bed.tsv; > WARNING 2020-07-21 12:28:17 AsciiLineReader Creating an indexable source for an AsciiFeatureCodec using a stream that is neither a PositionalBufferedStream nor a BlockCompressedInputStream; > 12:28:18.002 INFO DataSourceUtils - Setting lookahead cache for data source: Oreganno : 100000; > 12:28:18.009 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/oreganno.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/oreganno/hg38/oreganno.tsv; > 12:28:18.020 INFO FeatureManager - Using codec XsvLocatableTableCodec to read file file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/oreganno/hg38/oreganno.config; > 12:28:18.120 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/oreganno.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/oreganno/hg38/oreganno.tsv; > 12:28:18.121 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/oreganno.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/oreganno/hg38/oreganno.tsv; > WARNING 2020-07-21 12:28:18 AsciiLineReader Creating an indexable source for an AsciiFeatureCodec using a stream that is neither a PositionalBufferedStream nor a BlockCompressedInputStream; > 12:28:18.125 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/simple_uniprot_Dec012014.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/simple_uniprot/hg38/simple_uniprot_Dec012014.tsv; > 12:28:18.424 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6708#issuecomment-661776975:10036,config,config,10036,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6708#issuecomment-661776975,1,['config'],['config']
Modifiability,"ureInput;. public CodecWrapper(FeatureCodec<FEATURE_TYPE, SOURCE> childCodec, FeatureInput<FEATURE_TYPE> featureInput); {; this.childCodec = childCodec;; this.featureInput = featureInput;; }. @Override; public Feature decodeLoc(SOURCE source) throws IOException {; return childCodec.decodeLoc(source);; }. @Override; public FEATURE_TYPE decode(SOURCE source) throws IOException {; FEATURE_TYPE feature = childCodec.decode(source);. //Either look for marker class or otherwise poke in FeatureInput here:; if (feature instanceof VariantContext); {; feature = new FeatureInputAwareVariantContext(feature, featureInput);; }. return feature;; }. @Override; public FeatureCodecHeader readHeader(SOURCE source) throws IOException {; return childCodec.readHeader(source);; }. @Override; public Class<FEATURE_TYPE> getFeatureType() {; return childCodec.getFeatureType();; }. @Override; public SOURCE makeSourceFromStream(InputStream bufferedInputStream) {; return childCodec.makeSourceFromStream(bufferedInputStream);; }. @Override; public LocationAware makeIndexableSourceFromStream(InputStream inputStream) {; return childCodec.makeIndexableSourceFromStream(inputStream);; }. @Override; public boolean isDone(SOURCE source) {; return childCodec.isDone(source);; }. @Override; public void close(SOURCE source) {; childCodec.close(source);; }. @Override; public boolean canDecode(String path) {; return childCodec.canDecode(path);; }; }. public static interface FeatureInputAware<FEATURE_TYPE extends Feature>; {; public FeatureInput<FEATURE_TYPE> getFeatureInput();; }. public static class FeatureInputAwareVariantContext extends VariantContext implements FeatureInputAware<VariantContext>; {; private FeatureInput<VariantContext> featureInput;. public FeatureInputAwareVariantContext(VariantContext parent, FeatureInput<VariantContext> featureInput); {; super(parent);; this.featureInput = featureInput;; }. @Override; public FeatureInput<VariantContext> getFeatureInput() {; return featureInput;; }; }. ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-823546766:2219,extend,extends,2219,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-823546766,2,['extend'],['extends']
Modifiability,va:471); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.collectCaseStatsSequencial(CalibrateDragstrModel.java:459); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.traverse(CalibrateDragstrModel.java:159); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1058); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Using GATK jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx16G -jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar CalibrateDragstrModel --tmp-dir tmp -R /restricte; d/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --str-table-path gvcf.STR/HGDP00001.alt_bwamem_GRCh38DH.20181023.Brahui/HGDP00001.alt_bwamem_GRCh38DH.20181023.Brahui.STR.table -O gvcf.STR/HGDP00001.alt_bwamem_GRCh38DH.20181023.Brahui/HGDP00001.alt_bwamem_GRCh38DH.20181023.Brahui; .Dragstr.model -I ../pop/Brahui/HGDP00001/alignment/HGDP00001.alt_bwamem_GRCh38DH.20181023.Brahui.cram; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7182#issuecomment-821876394:6783,variab,variable,6783,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7182#issuecomment-821876394,1,['variab'],['variable']
Modifiability,we need to keep track of what `GATKReads` correspond to which `ShortReads` and the `Strings` returned from jbwa. One way may be to extend `ShortRead` with a data slot to keep the back pointer to the original `GATKRead` (the jni layer would ignore it and copying 64 bits per read is not a big deal) and then copy the auxiliary data (like read groups etc) back from the `GATKRead` to the newly created `SAMRecord`,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1904#issuecomment-230863254:131,extend,extend,131,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1904#issuecomment-230863254,1,['extend'],['extend']
Modifiability,"we've run all the relevant tools recently. What else is needed here? Can we close this? If not, please make an explicit list of things to check (tools, configuration, etc). Back to @droazen for clarifications.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/968#issuecomment-152306914:152,config,configuration,152,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/968#issuecomment-152306914,1,['config'],['configuration']
Modifiability,"willing to give it a try but I need minimal info to do so. * can I add `library(""ggplot2"")` to the code or will this new library not be supported by the package?. * what value (are these defaults?) do. ```; targetTITV = as.numeric(args[2]); targetSensitivity = as.numeric(args[3]) ; ```. take for a command like this. > cmd=""java ${javaopts} -jar $GATK/gatk.jar \; > 	VariantRecalibrator \; > 	-R ${reference_fa} \; > 	-V ${outfolder}/gatk_variants_excesshet_sitesonly.vcf.gz \; > 	-O ${outfolder}/gatk_variants_recalibrate_SNP.recal.vcf.gz \; > 	${intervals} \; > 	--resource:1001Gsnp,known=true,training=true,truth=true,prior=12.0 ${knownsnps} \; > 	--trust-all-polymorphic \; > 	--use-annotation DP \; > 	--use-annotation QD \; > 	--use-annotation FS \; > 	--use-annotation SOR \; > 	--use-annotation MQ \; > 	--use-annotation MQRankSum \; > 	--use-annotation ReadPosRankSum \; > 	--mode SNP \; > 	--max-gaussians ${maxSNPgaussians} \; > 	--tranches-file ${outfolder}/gatk_variants_recalibrate_snp.tranches \; > 	--tranche 100.0 \; > 	--tranche 99.95 \; > 	--tranche 99.9 \; > 	--tranche 99.8 \; > 	--tranche 99.6 \; > 	--tranche 99.5 \; > 	--tranche 99.4 \; > 	--tranche 99.3 \; > 	--tranche 99.0 \; > 	--tranche 98.0 \; > 	--tranche 97.0 \; > 	--tranche 90.0 \; > 	--rscript-file ${outfolder}/gatk_variantsgatk_variants_recalibrate_snp_plots.R \; > 	--tmp-dir ${basedir}/tmpfiles/"". Sorry but I do not know where to look in the java code for this. Thanks in advance",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6585#issuecomment-624680466:664,polymorphi,polymorphic,664,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6585#issuecomment-624680466,1,['polymorphi'],['polymorphic']
Modifiability,"windows in my implementation is different from the one in `ReadWindowWalker`: first, the overlap between windows is only in one direction; second, `SlidingWindowWalker` is more like a reference/interval walker, from the beginning of the reference (or interval) till the end, it walks in overlapping windows. One example is the following (window-size 10, window-step 5, the - represent the window):. ```; Reference: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _; Windows1: _ _ _ _ _ _ _ _ _ _; Windows2: _ _ _ _ _ _ _ _ _ _; Windows3: _ _ _ _ _ _ _ _ _ _; Windows4: _ _ _ _ _ _ _ _ _ _; Windows5: _ _ _ _ _ _ _ _ _ _; ```. Of course, after having a look to `ReadWindowWalker` I think that several things could be improved in my implementation for a general `SlidingWindowWalker`:; - Apply function similar to the `ReadWindowWalker`, with `ReadWindow` being empty if reads are not provided.; - Three window options: `windowSize` (the actual size of the window), `windowStep` (how much advance for the following window) and `windowPadding` (how much extend the window in both directions). Using this abstraction, `ReadWindowWalker` could be implemented setting `windowSize=windowStep`, and the problem that I need to solve could be implemented setting `windowPadding=0`. The simplest way to acomplish this is to use the current implementation of `ReadWindowWalker` to develop a `SlidingWindowWalker` adding three abstract methods for the three parameters (`getWindowSize()`, `getWindowStep()` and `getWindowPadding()`, and implement `ReadWindowWalker` as a extension of this interface setting `getWindowStep()` to return `getWindowSize()` and `requiresReads()` to true. I can do this once the PR #1567 is accepted and generate the two interfaces (to be sure that the integration with the HC engine is working as expected with the changes), or just implement the `SlidingWindowWalker` and you can include it in the HC PR, or update afterwards to avoid redundancy in the code. What do you thi",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1528#issuecomment-198438775:1629,extend,extend,1629,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1528#issuecomment-198438775,1,['extend'],['extend']
Modifiability,"yf_documentation_update we; can use that for initial testing. On Tue, Dec 5, 2017 at 1:56 PM, sooheelee <notifications@github.com> wrote:. > @samuelklee <https://github.com/samuelklee>, thanks for the update and; > suggestion. I moved CollectAllelicCounts to the Coverage Analysis; > category. CollectFragmentCounts isn't on the list currently so I added it; > to the same. I hope I'm not missing a bunch of other new tools given I; > missed this one.; >; > @yfarjoun <https://github.com/yfarjoun>; >; > - You are now in charge of deciding whether we should include; > authorship in code. What the Comms team wants is for authorship to NOT show; > up in the gatkDoc/javaDoc. If you want to keep them, author lines should be; > at the bottom and formatted so they do not show up in the documentation.; > Geraldine is fine with completely removing them if you prefer that. There; > is a format trick that has javaDoc skip the author line and I can get that; > to you if you decide to keep some of these and @vdauwera; > <https://github.com/vdauwera> would know this or I can get you what I; > see in other docs. Let either of us know.; > - I can help you test your changes. I think the categories are good to; > go now so I will need to put these into both Picard and GATK; > HelpConstants.java, with the latter being a placeholder until the new; > Picard release is incorporated into the next GATK release, with variables; > that then must be included in each tool doc. I will find an example in a; > bit. Which tool do you want to test? @cmnbroad; > <https://github.com/cmnbroad> can explain the engineering details in; > engineering lingo if you need more information.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/3853#issuecomment-349404645>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACnk0jIdprE580XBgq1jL-EIV1hFOcDyks5s9ZHAgaJpZM4QitCF>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-349407253:1482,variab,variables,1482,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-349407253,1,['variab'],['variables']
Performance, 	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.read(DefaultArraySerializers.java:396); 	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.read(DefaultArraySerializers.java:307); 	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790); 	at org.apache.spark.serializer.KryoSerializerInstance.deserialize(KryoSerializer.scala:362); 	at org.apache.spark.scheduler.DirectTaskResult.value(TaskResult.scala:88); 	at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$1.apply$mcV$sp(TaskResultGetter.scala:72); 	at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$1.apply(TaskResultGetter.scala:63); 	at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$1.apply(TaskResultGetter.scala:63); 	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992); 	at org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:62); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: java.lang.NullPointerException; 	at htsjdk.variant.variantcontext.LazyGenotypesContext.decode(LazyGenotypesContext.java:158); 	at htsjdk.variant.variantcontext.LazyGenotypesContext.invalidateSampleOrdering(LazyGenotypesContext.java:205); 	at htsjdk.variant.variantcontext.GenotypesContext.add(GenotypesContext.java:353); 	at htsjdk.variant.variantcontext.GenotypesContext.add(GenotypesContext.java:46); 	at com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:134); 	at com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:40); 	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708); 	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125); 	... 18 more; 19/02/18 16:58:29 INFO org.spark_pro,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3840#issuecomment-464825765:7675,concurren,concurrent,7675,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3840#issuecomment-464825765,1,['concurren'],['concurrent']
Performance, 	at com.github.discvrseq.walkers.BackportLiftedVcf.apply(BackportLiftedVcf.java:156); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.lambda$traverse$0(VariantWalkerBase.java:110); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.traverse(VariantWalkerBase.java:108); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:838); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:115); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:131); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:152); 	at com.github.discvrseq.Main.main(Main.java:51); Caused by: java.lang.ClassNotFoundException: org.xerial.snappy.LoadSnappy; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:381); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:424); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357); 	... 26 more,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2300#issuecomment-333579182:2635,Load,LoadSnappy,2635,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2300#issuecomment-333579182,4,"['Load', 'load']","['LoadSnappy', 'loadClass']"
Performance," ""3.701625"",; ""NIST controlMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/6ea2705f-a3fa-41fc-8d17-a2c55d875eab/call-NISTSampleHeadToHead/BenchmarkComparison/a39481f5-0969-4891-a843-f3c3fd7437d1/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""NIST controlindelF1Score"": ""0.9902"",; ""NIST controlindelPrecision"": ""0.9903"",; ""NIST controlsnpF1Score"": ""0.9899"",; ""NIST controlsnpPrecision"": ""0.9887"",; ""NIST controlsnpRecall"": ""0.9911"",; ""NIST controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/6ea2705f-a3fa-41fc-8d17-a2c55d875eab/call-NISTSampleHeadToHead/BenchmarkComparison/a39481f5-0969-4891-a843-f3c3fd7437d1/call-BenchmarkVCFControlSample/Benchmark/0c99102a-bca1-4426-97c6-5a311ace93c1/call-CombineSummaries/summary.csv"",; ""NIST evalHCprocesshours"": ""95.62183055555556"",; ""NIST evalHCsystemhours"": ""0.18361111111111117"",; ""NIST evalHCwallclockhours"": ""64.22846111111112"",; ""NIST evalHCwallclockmax"": ""3.3683277777777776"",; ""NIST evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/6ea2705f-a3fa-41fc-8d17-a2c55d875eab/call-NISTSampleHeadToHead/BenchmarkComparison/a39481f5-0969-4891-a843-f3c3fd7437d1/call-EVALRuntimeTask/cacheCopy/monitoring.pdf"",; ""NIST evalindelF1Score"": ""0.9902"",; ""NIST evalindelPrecision"": ""0.9903"",; ""NIST evalsnpF1Score"": ""0.9899"",; ""NIST evalsnpPrecision"": ""0.9887"",; ""NIST evalsnpRecall"": ""0.9911"",; ""NIST evalsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/6ea2705f-a3fa-41fc-8d17-a2c55d875eab/call-NISTSampleHeadToHead/BenchmarkComparison/a39481f5-0969-4891-a843-f3c3fd7437d1/call-BenchmarkVCFTestSample/Benchmark/a3925c8a-7e0a-4fec-8507-f885061b69c3/call-CombineSummaries/summary.csv"",; ""ROC_Plots_Reported"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/6ea2705f-a3fa-41fc-8d17-a2c55d875eab/call-CreateHTMLReport/cacheCopy/report.html""; }; } ; </pre> </details>",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7651#issuecomment-1069766207:14465,cache,cacheCopy,14465,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7651#issuecomment-1069766207,2,['cache'],['cacheCopy']
Performance," - 1:210675831 4.2 5438000 1282169.2; 15:39:25.463 INFO ProgressMeter - 10:119579965 4.4 5479000 1242549.2; 15:39:35.700 INFO ProgressMeter - 11:118752077 4.6 5530000 1207397.2; 15:39:46.028 INFO ProgressMeter - 12:58875536 4.8 5592000 1176709.9; 15:39:56.079 INFO ProgressMeter - 13:37022394 4.9 5628000 1143956.7; 15:40:06.117 INFO ProgressMeter - 16:14727212 5.1 5728000 1125992.7; 15:40:16.383 INFO SplitNCigarReads - Shutting down engine; [March 2, 2023 3:40:16 PM EST] org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads done. Elapsed time: 5.27 minutes.; Runtime.totalMemory()=3432513536; java.lang.ClassCastException: htsjdk.samtools.BAMRecord cannot be cast to java.lang.Comparable; 	at java.util.Arrays$NaturalOrder.compare(Arrays.java:102); 	at java.util.TimSort.countRunAndMakeAscending(TimSort.java:355); 	at java.util.TimSort.sort(TimSort.java:234); 	at java.util.ArraysParallelSortHelpers$FJObject$Sorter.compute(ArraysParallelSortHelpers.java:145); 	at java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:731); 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); 	at java.util.concurrent.ForkJoinTask.doInvoke(ForkJoinTask.java:401); 	at java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:734); 	at java.util.Arrays.parallelSort(Arrays.java:1180); 	at htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:247); 	at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:182); 	at htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:202); 	at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:36); 	at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:16); 	at htsjdk.samtools.util.AbstractAsyncWriter$WriterRunnable.run(AbstractAsyncWriter.java:123); 	at java.lang.Thread.run(Thread.java:750); 	Suppressed: htsjdk.samtools.util.RuntimeIOException: Attempt to add record to closed writer.; 		at htsjdk.samtools.util.AbstractAs",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452525485:6103,concurren,concurrent,6103,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452525485,1,['concurren'],['concurrent']
Performance," --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 /share/pkg/gatk/4.0.3.0/install/bin/gatk-package-4.0.3.0-spark.jar CountReadsSpark --input /project/casa/gcad/adsp.cc/cram/A-ADC-AD010072-BL-NCR-11AD44210.hg38.realign.bqsr.cram --reference file:///restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --spark-master yarn; 13:48:31.261 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 13:48:31.426 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg/gatk/4.0.3.0/install/bin/gatk-package-4.0.3.0-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 13:48:31.693 INFO CountReadsSpark - ------------------------------------------------------------; 13:48:31.693 INFO CountReadsSpark - The Genome Analysis Toolkit (GATK) v4.0.3.0; 13:48:31.693 INFO CountReadsSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:48:31.694 INFO CountReadsSpark - Executing as farrell@scc-hadoop.bu.edu on Linux v2.6.32-754.6.3.el6.x86_64 amd64; 13:48:31.694 INFO CountReadsSpark - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 13:48:31.694 INFO CountReadsSpark - Start Date/Time: December 21, 2018 1:48:31 PM EST; 13:48:31.694 INFO CountReadsSpark - ------------------------------------------------------------; 13:48:31.694 INFO CountReadsSpark - ------------------------------------------------------------; 13:48:31.695 INFO CountReadsSpark - HTSJDK Vers",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-449510725:1878,Load,Loading,1878,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-449510725,1,['Load'],['Loading']
Performance," 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, xx.xx.xx.xx, executor 0): java.lang.IllegalStateException: unread block data; at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2740); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1567); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:309); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 18/04/24 14:34:27 INFO DAGScheduler: Job 0 failed: first at ReadsSparkSource.java:221, took 4.816635 s; ```; Our system is an HPC, where all the nodes share the same file system. I run my SPARK on only one node to test the software. I red elesewhere that this might be aproblem of missing jars, so I tried to inlcude these libraries in the SPARK jar folder and added the option:; `; --conf [--jars=""~/bin/spark-2.2.0-bin-hadoop2.7/jars/hbase-client-1.4.3.jar, ~/bin/spark-2.2.0-bin-hadoop2.7/jars/hbase-common-1.4.3.jar, ~/bin/spark-2.2.0-bin-hadoop2.7/jars/hbase-hadoop2-compat-1.4.3.jar, ~/bin/spark-2.2.0-bin-hadoop2.7/jars/hive-hbase-handler-1.2.1.spark2.jar"" ]`. But I still get the error. Is GATK using hbase? If yes shall some jars be included to a local SPARK system to enable it to run GATK tools? Thank you!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383916494:1991,concurren,concurrent,1991,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383916494,1,['concurren'],['concurrent']
Performance," 11 hours to get to the point where the error occurs it has been difficult to trouble shoot, I am hoping that I can fix this without rebuilding everything which is why I decided to write. Thanks for any information or suggestions you may have. . Dan; ; Using GATK jar /home/dan_vanderpool/src/gatk-4.2.5.0/gatk-package-4.2.5.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx1600g -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -jar /home/dan_vanderpool/src/gatk-4.2.5.0/gatk-package-4.2.5.0-local.jar GenotypeGVCFs -R /home/dan_vanderpool/Wolf_raw_reads/Wolf_genome/GCA_905319855.2_mCanLor1.2_genomic.fa -V gendb://Wolf_Genome_Variantsdb -O All_Wolf_Samples_Joint_Genotypes_Raw.vcf.gz -L /scratch/dan/Wolf_reads_raw/Wolf_GenCov300_Q20_Merged.interval_list -imr ALL --genomicsdb-max-alternate-alleles 10 --max-alternate-alleles 6; 17:49:29.781 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/dan_vanderpool/src/gatk-4.2.5.0/gatk-package-4.2.5.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 23, 2022 5:49:30 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 17:49:30.164 INFO GenotypeGVCFs - ------------------------------------------------------------; 17:49:30.165 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.5.0; 17:49:30.165 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:49:30.165 INFO GenotypeGVCFs - Executing as dan_vanderpool@0e07622619ad on Linux v4.4.0-210-generic amd64; 17:49:30.165 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v10.0.2+13; 17:49:30.166 INFO GenotypeGVCFs - Start Date/Time: February 23, 2022 at 5:49:29 PM UTC; 17:49:30.166 INFO GenotypeGVCFs - -------------------------------------------------",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7639#issuecomment-1049112454:4188,Load,Loading,4188,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7639#issuecomment-1049112454,1,['Load'],['Loading']
Performance," 15:39:35.700 INFO ProgressMeter - 11:118752077 4.6 5530000 1207397.2; > 15:39:46.028 INFO ProgressMeter - 12:58875536 4.8 5592000 1176709.9; > 15:39:56.079 INFO ProgressMeter - 13:37022394 4.9 5628000 1143956.7; > 15:40:06.117 INFO ProgressMeter - 16:14727212 5.1 5728000 1125992.7; > 15:40:16.383 INFO SplitNCigarReads - Shutting down engine; > [March 2, 2023 3:40:16 PM EST]; > org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads done.; > Elapsed time: 5.27 minutes.; > Runtime.totalMemory()=3432513536; > java.lang.ClassCastException: htsjdk.samtools.BAMRecord cannot be cast to; > java.lang.Comparable; > at java.util.Arrays$NaturalOrder.compare(Arrays.java:102); > at java.util.TimSort.countRunAndMakeAscending(TimSort.java:355); > at java.util.TimSort.sort(TimSort.java:234); > at; > java.util.ArraysParallelSortHelpers$FJObject$Sorter.compute(ArraysParallelSortHelpers.java:145); > at java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:731); > at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); > at java.util.concurrent.ForkJoinTask.doInvoke(ForkJoinTask.java:401); > at java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:734); > at java.util.Arrays.parallelSort(Arrays.java:1180); > at; > htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:247); > at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:182); > at; > htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:202); > at; > htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:36); > at; > htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:16); > at; > htsjdk.samtools.util.AbstractAsyncWriter$WriterRunnable.run(AbstractAsyncWriter.java:123); > at java.lang.Thread.run(Thread.java:750); > Suppressed: htsjdk.samtools.util.RuntimeIOException: Attempt to add record; > to closed writer.; > at; > htsjdk.samtools.util.AbstractAsyncWriter.write(AbstractAsyncWriter.java:57",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452528344:6619,concurren,concurrent,6619,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452528344,1,['concurren'],['concurrent']
Performance," 2019-01-07 11:33:27 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3688baab{/,null,AVAILABLE,@Spark}; 2019-01-07 11:33:27 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4fe2dd02{/api,null,AVAILABLE,@Spark}; 2019-01-07 11:33:27 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@726a8729{/jobs/job/kill,null,AVAILABLE,@Spark}; 2019-01-07 11:33:27 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@1a2724d3{/stages/stage/kill,null,AVAILABLE,@Spark}; 2019-01-07 11:33:27 INFO SparkUI:54 - Bound SparkUI to 0.0.0.0, and started at http://scc-hadoop.bu.edu:4041; 2019-01-07 11:33:27 INFO SparkContext:54 - Added JAR file:/share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-spark.jar at spark://scc-hadoop.bu.edu:46828/jars/gatk-package-4.0.12.0-spark.jar with timestamp 1546878807984; 2019-01-07 11:33:28 INFO GoogleHadoopFileSystemBase:607 - GHFS version: 1.6.3-hadoop2; 2019-01-07 11:33:29 WARN DomainSocketFactory:117 - The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.; 2019-01-07 11:33:30 INFO Client:54 - Requesting a new application from cluster with 21 NodeManagers; 2019-01-07 11:33:30 INFO Client:54 - Verifying our application has not requested more than the maximum memory capability of the cluster (204800 MB per container); 2019-01-07 11:33:30 INFO Client:54 - Will allocate AM container, with 896 MB memory including 384 MB overhead; 2019-01-07 11:33:30 INFO Client:54 - Setting up container launch context for our AM; 2019-01-07 11:33:30 INFO Client:54 - Setting up the launch environment for our AM container; 2019-01-07 11:33:30 INFO Client:54 - Preparing resources for our AM container; 2019-01-07 11:33:30 INFO HadoopFSDelegationTokenProvider:54 - getting token for: DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_-1883879239_1, ugi=farrell@AD.BU.EDU (auth:KERBEROS)]]; 2019-01-07 11:33:30 INFO DFSClient:1023 - Created HDFS_DELEGATION_TOKEN token 11334 for farrell on ha-hdfs",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:10619,load,loaded,10619,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,1,['load'],['loaded']
Performance," 2019-01-09 13:35:12 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@16b64a03{/,null,AVAILABLE,@Spark}; 2019-01-09 13:35:12 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@1584c019{/api,null,AVAILABLE,@Spark}; 2019-01-09 13:35:12 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@5817f1ca{/jobs/job/kill,null,AVAILABLE,@Spark}; 2019-01-09 13:35:12 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2b395581{/stages/stage/kill,null,AVAILABLE,@Spark}; 2019-01-09 13:35:12 INFO SparkUI:54 - Bound SparkUI to 0.0.0.0, and started at http://scc-hadoop.bu.edu:4041; 2019-01-09 13:35:12 INFO SparkContext:54 - Added JAR file:/share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-spark.jar at spark://scc-hadoop.bu.edu:42689/jars/gatk-package-4.0.12.0-spark.jar with timestamp 1547058912934; 2019-01-09 13:35:13 INFO GoogleHadoopFileSystemBase:607 - GHFS version: 1.6.3-hadoop2; 2019-01-09 13:35:13 WARN DomainSocketFactory:117 - The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.; 2019-01-09 13:35:14 INFO Client:54 - Requesting a new application from cluster with 21 NodeManagers; 2019-01-09 13:35:14 INFO Client:54 - Verifying our application has not requested more than the maximum memory capability of the cluster (204800 MB per container); 2019-01-09 13:35:14 INFO Client:54 - Will allocate AM container, with 896 MB memory including 384 MB overhead; 2019-01-09 13:35:14 INFO Client:54 - Setting up container launch context for our AM; 2019-01-09 13:35:14 INFO Client:54 - Setting up the launch environment for our AM container; 2019-01-09 13:35:14 INFO Client:54 - Preparing resources for our AM container; 2019-01-09 13:35:14 INFO HadoopFSDelegationTokenProvider:54 - getting token for: DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_-682487019_1, ugi=farrell@AD.BU.EDU (auth:KERBEROS)]]; 2019-01-09 13:35:14 INFO DFSClient:1023 - Created HDFS_DELEGATION_TOKEN token 11353 for farrell on ha-hdfs:",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:10359,load,loaded,10359,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,1,['load'],['loaded']
Performance," 26|2, 2], DP=94, ECNT=1, GERMQ=93, MBQ=[31, 20], MFRL=[288, 110], MMQ=[60, 60], MPOS=56, NALOD=1.37, NLOD=6.17, POPAF=4.6, ROQ=93, TLOD=10.97} GT=GT:AD:AF:DP:F1R2:F2R1:SB 0/1:46,4:0.07:50:14,3:10,0:28,18,2,2 0/0:23,0:0.041:23:8,0:5,0:15,8,0,0 filters=; 11:43:25.661 WARN GencodeFuncotationFactory - Creating default GencodeFuncotation on transcript ENST00000441716.2 for problem variant: chr6:167976552-167976594(ACAGTGGGGGTCATTCCCCCTGCAGTGTGTTGGGAGGAGGAGG* -> A); 11:44:04.904 INFO ProgressMeter - chr8:677091 4.5 3000 666.0; 11:45:35.226 INFO ProgressMeter - chr11:62279639 6.0 4000 665.6; 11:46:54.284 INFO ProgressMeter - chr15:19905537 7.3 5000 682.4; 11:48:12.767 WARN FuncotatorUtils - createAminoAcidSequence given a coding sequence of length not divisible by 3. Dropping bases from the end: 2 (size=293, ref allele: G); 11:48:16.949 ERROR GencodeFuncotationFactory - Problem creating a GencodeFuncotation on transcript ENST00000379751.5 for variant: chr20:3786474-3786537(TGGGGCCCATCCCGGCGCGCCCCCCGCCCCGGGGCCCGGCGCCGCCGCCGCCGCCCCGGGGCGG* -> T): Cannot yet handle indels starting outside an exon and ending within an exon.; 11:48:16.949 WARN GencodeFuncotationFactory - Creating default GencodeFuncotation on transcript ENST00000379751.5 for problem variant: chr20:3786474-3786537(TGGGGCCCATCCCGGCGCGCCCCCCGCCCCGGGGCCCGGCGCCGCCGCCGCCGCCCCGGGGCGG* -> T); 11:48:31.506 INFO ProgressMeter - chr21:18282114 8.9 6000 670.6; 11:49:08.210 INFO ProgressMeter - chr21:18282114 9.6 6888 720.6; 11:49:08.210 INFO ProgressMeter - Traversal complete. Processed 6888 total variants in 9.6 minutes.; 11:49:08.210 INFO VcfFuncotationFactory - ClinVar_VCF 20180429_hg38 cache hits/total: 0/2; 11:49:08.211 INFO VcfFuncotationFactory - dbSNP 9606_b151 cache hits/total: 0/4781; 11:49:08.230 INFO Funcotator - Shutting down engine; [July 7, 2021 11:49:08 AM GMT] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 9.72 minutes.; Runtime.totalMemory()=4879548416; Tool returned:; true",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6651#issuecomment-887961422:2306,cache,cache,2306,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6651#issuecomment-887961422,2,['cache'],['cache']
Performance," > 15:39:46.028 INFO ProgressMeter - 12:58875536 4.8 5592000 1176709.9; > 15:39:56.079 INFO ProgressMeter - 13:37022394 4.9 5628000 1143956.7; > 15:40:06.117 INFO ProgressMeter - 16:14727212 5.1 5728000 1125992.7; > 15:40:16.383 INFO SplitNCigarReads - Shutting down engine; > [March 2, 2023 3:40:16 PM EST]; > org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads done.; > Elapsed time: 5.27 minutes.; > Runtime.totalMemory()=3432513536; > java.lang.ClassCastException: htsjdk.samtools.BAMRecord cannot be cast to; > java.lang.Comparable; > at java.util.Arrays$NaturalOrder.compare(Arrays.java:102); > at java.util.TimSort.countRunAndMakeAscending(TimSort.java:355); > at java.util.TimSort.sort(TimSort.java:234); > at; > java.util.ArraysParallelSortHelpers$FJObject$Sorter.compute(ArraysParallelSortHelpers.java:145); > at java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:731); > at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); > at java.util.concurrent.ForkJoinTask.doInvoke(ForkJoinTask.java:401); > at java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:734); > at java.util.Arrays.parallelSort(Arrays.java:1180); > at; > htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:247); > at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:182); > at; > htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:202); > at; > htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:36); > at; > htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:16); > at; > htsjdk.samtools.util.AbstractAsyncWriter$WriterRunnable.run(AbstractAsyncWriter.java:123); > at java.lang.Thread.run(Thread.java:750); > Suppressed: htsjdk.samtools.util.RuntimeIOException: Attempt to add record; > to closed writer.; > at; > htsjdk.samtools.util.AbstractAsyncWriter.write(AbstractAsyncWriter.java:57); > at; > htsjdk.samtools.AsyncSAMFileWriter.addAlignment(AsyncSAMFil",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452528344:6689,concurren,concurrent,6689,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452528344,1,['concurren'],['concurrent']
Performance," HTSJDK Defaults.COMPRESSION_LEVEL : 2; 11:19:40.101 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 11:19:40.101 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 11:19:40.101 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 11:19:40.101 INFO GenomicsDBImport - Deflater: IntelDeflater; 11:19:40.101 INFO GenomicsDBImport - Inflater: IntelInflater; 11:19:40.101 INFO GenomicsDBImport - GCS max retries/reopens: 20; 11:19:40.102 INFO GenomicsDBImport - Requester pays: disabled; 11:19:40.102 INFO GenomicsDBImport - Initializing engine; 11:19:40.385 INFO FeatureManager - Using codec BEDCodec to read file file:///mnt/data/project/reseq/KPSNY042021067K/result/03.bwa_dup_gvcf/geno/chr33.bed; 11:19:40.390 INFO IntervalArgumentCollection - Processing 10664 bp from intervals; 11:19:40.391 WARN GenomicsDBImport - A large number of intervals were specified. Using more than 100 intervals in a single import is not recommended and can cause performance to suffer. If GVCF data only exists within those intervals, performance can be improved by aggregating intervals with the merge-input-intervals argument.; 11:19:40.429 INFO GenomicsDBImport - Done initializing engine; 11:19:40.624 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.3.0-e701905; 11:19:40.625 INFO GenomicsDBImport - Vid Map JSON file will be written to /mnt/data/chr33.db/vidmap.json; 11:19:40.625 INFO GenomicsDBImport - Callset Map JSON file will be written to /mnt/data/chr33.db/callset.json; 11:19:40.625 INFO GenomicsDBImport - Complete VCF Header will be written to /mnt/data/chr33.db/vcfheader.vcf; 11:19:40.625 INFO GenomicsDBImport - Importing to workspace - /mnt/data/chr33.db; 11:19:40.625 INFO ProgressMeter - Starting traversal; 11:19:40.625 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 11:19:49.073 INFO GenomicsDBImport - Importing batch 1 with 1115 samples; 11:20:12.073 I",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7952#issuecomment-1196217460:3311,perform,performance,3311,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7952#issuecomment-1196217460,1,['perform'],['performance']
Performance," INFO Mutect2 - -----------------------------------------------------------; 08:27:10.887 INFO Mutect2 - HTSJDK Version: 2.19.; 08:27:10.887 INFO Mutect2 - Picard Version: 2.19.; 08:27:10.887 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 2. 08:27:10.888 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : fals; 08:27:10.888 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : tru; 08:27:10.888 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : fals; 08:27:10.888 INFO Mutect2 - Deflater: IntelDeflate; 08:27:10.889 INFO Mutect2 - Inflater: IntelInflate; 08:27:10.889 INFO Mutect2 - GCS max retries/reopens: 2; 08:27:10.889 INFO Mutect2 - Requester pays: disable; 08:27:10.889 INFO Mutect2 - Initializing engin; 08:27:11.333 INFO Mutect2 - Done initializing engin; 08:27:11.381 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/gatk/gatk-package-4.1.1.0-local.jar!/com/intel/gkl/native/libgkl_utils.s; 08:27:11.383 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk/gatk-package-4.1.1.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.s; 08:27:11.426 INFO **IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHM**; 08:27:11.427 INFO IntelPairHmm - Available threads: 4; 08:27:11.428 INFO IntelPairHmm - Requested threads: 4; 08:27:11.428 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementatio; 08:27:11.432 INFO Mutect2 - Shutting down engin; [April 23, 2019 8:27:11 AM UTC] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.09 minutes.; Runtime.totalMemory()=190840832; java.lang.IllegalArgumentException: samples cannot be empt; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:724); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.ReferenceConfidenceModel.<init>(ReferenceConfidenceModel.java:116); 	at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticReferenceConfidenceModel.<init>",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4665#issuecomment-485729136:2568,Load,Loading,2568,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4665#issuecomment-485729136,1,['Load'],['Loading']
Performance," ProgressMeter - 3:1 2531.4 1 0.0; 05:39:42.051 INFO GenomicsDBImport - Done importing batch 1/1; 05:39:42.060 INFO ProgressMeter - 3:1 2531.4 1 0.0; 05:39:42.061 INFO ProgressMeter - Traversal complete. Processed 1 total batches in 2531.4 minutes.; 05:39:42.061 INFO GenomicsDBImport - Import completed!; 05:39:42.061 INFO GenomicsDBImport - Shutting down engine; [January 16, 2021 5:39:42 AM CST] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 2,531.64 minutes.; Runtime.totalMemory()=9711910912; Tool returned:; true; **Calling Variants Attempt**; Using GATK jar /data1/_software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Xmx32g -jar /data1/_software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar GenotypeGVCFs --genomicsdb-shared-posixfs-optimizations --reference /data1/EquCab/_ECA30/Equus_caballus.EquCab3.0.dna_sm.toplevel.fa/ -V gendb://ECA3_GenomicsDB_260/3 -O ECA3_GenomicsDB_260.3.g.vcf.gz; 21:16:35.251 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data1/_software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 17, 2021 9:16:35 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 21:16:35.496 INFO GenotypeGVCFs - ------------------------------------------------------------; 21:16:35.497 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.1.8.1; 21:16:35.497 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 21:16:35.497 INFO GenotypeGVCFs - Executing as ccastane9@andersserver-01.cvm.tamu.edu on Linux v3.10.0-1127.19.1.el7.x86_64 amd64; 21:16:35.497 INFO GenotypeGVCFs - Java runtime: OpenJ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-761953839:2237,optimiz,optimizations,2237,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-761953839,1,['optimiz'],['optimizations']
Performance, [...ragemodel/cachemanager/ComputableNodeFunction.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9Db21wdXRhYmxlTm9kZUZ1bmN0aW9uLmphdmE=) | `100% <100%> (+66.667%)` | `4 <1> (+2)` | :arrow_up: |; | [.../coveragemodel/cachemanager/DuplicableNDArray.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9EdXBsaWNhYmxlTkRBcnJheS5qYXZh) | `81.818% <100%> (+38.068%)` | `6 <2> (+2)` | :arrow_up: |; | [...s/coveragemodel/cachemanager/DuplicableNumber.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9EdXBsaWNhYmxlTnVtYmVyLmphdmE=) | `80% <100%> (+80%)` | `5 <2> (+5)` | :arrow_up: |; | [...coveragemodel/cachemanager/PrimitiveCacheNode.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9QcmltaXRpdmVDYWNoZU5vZGUuamF2YQ==) | `83.333% <71.429%> (+30.702%)` | `10 <7> (+3)` | :arrow_up: |; | [...er/tools/coveragemodel/cachemanager/CacheNode.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9DYWNoZU5vZGUuamF2YQ==) | `80.645% <76.923%> (+30.645%)` | `9 <8> (+4)` | :arrow_up: |; | [...overagemodel/cachemanager/ComputableCacheNode.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9Db21wdXRhYmxlQ2FjaGVOb2RlLmphdmE=) | `89.189% <80%> (+32.779%)` | `18 <17> (+2)` | :arrow_up: |; | [...ols/coveragemodel/CoverageModelEMCompute,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3035#issuecomment-306412418:2568,cache,cachemanager,2568,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3035#issuecomment-306412418,1,['cache'],['cachemanager']
Performance," accounting for 1) sample-specific depth (which determines the means of the negative-binomial distributions), 2) multiplicative contig-specific bias (which is mild, at least for WGS), and 3) additive sample-contig-specific mosaicism or bias (note that the above genotype priors imply that mosaicism/bias on top of a baseline of CN = 2 is the only deviation allowed for the autosomes, which is somewhat restrictive but greatly aids convergence). I put together a pure PyMC3 prototype that seems to work relatively well. Here are the per-contig coverage histograms (unfiltered bins in blue, bins retained after filtering in red, and negative-binomial fit in green) and a heatmap of per-contig ploidy probabilities. Both the panel (first 20) and case (remaining) samples are shown:. ![prototype-result](https://user-images.githubusercontent.com/11076296/37938642-e9fbd804-312c-11e8-8a6c-02ea4e4fa704.png). Although the prototype model is clearly a good fit to the filtered data, some care in choosing the optimizer and its learning parameters is required to achieve convergence to the correct solution. This is because the problem is inherently multimodal and thus there are many local minima. I found that using AdaMax with a naive strategy of warm restarts (to help kick us out of local minima) worked decently; we can achieve convergence in <10 minutes for 60 samples x 24 contigs x 250 count bins:. ![elbo](https://user-images.githubusercontent.com/11076296/37938658-fc176f12-312c-11e8-89e2-40c68e0f9953.png). I expect that @mbabadi's annealing implementation in the gcnvkernel package will handle the local minima much better. The course of action needed to implement this model should be as follows:. 1) Alter Java code to emit per-contig histograms. Change python code to consume histograms, perform filtering, and fit using the above model (or some variation).; 2) Choose learning parameters appropriate with annealing and check that results are still good.; 3) Update gCNV model to consume the d",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-376278271:2495,optimiz,optimizer,2495,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-376278271,1,['optimiz'],['optimizer']
Performance," acls to: hdfs; 17/10/13 18:11:37 INFO spark.SecurityManager: Changing view acls groups to: ; 17/10/13 18:11:37 INFO spark.SecurityManager: Changing modify acls groups to: ; 17/10/13 18:11:37 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hdfs); groups with view permissions: Set(); users with modify permissions: Set(hdfs); groups with modify permissions: Set(); 17/10/13 18:11:37 INFO yarn.Client: Submitting application application_1507856833944_0003 to ResourceManager; 17/10/13 18:11:37 INFO impl.YarnClientImpl: Submitted application application_1507856833944_0003; 17/10/13 18:11:37 INFO cluster.SchedulerExtensionServices: Starting Yarn extension services with app application_1507856833944_0003 and attemptId None; 17/10/13 18:11:38 INFO yarn.Client: Application report for application_1507856833944_0003 (state: ACCEPTED); 17/10/13 18:11:38 INFO yarn.Client: ; 	 client token: N/A; 	 diagnostics: N/A; 	 ApplicationMaster host: N/A; 	 ApplicationMaster RPC port: -1; 	 queue: root.users.hdfs; 	 start time: 1507889497661; 	 final status: UNDEFINED; 	 tracking URL: http://mg:8088/proxy/application_1507856833944_0003/; 	 user: hdfs; 17/10/13 18:11:39 INFO yarn.Client: Application report for application_1507856833944_0003 (state: ACCEPTED); 17/10/13 18:11:40 INFO yarn.Client: Application report for application_1507856833944_0003 (state: ACCEPTED); 17/10/13 18:11:41 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM); 17/10/13 18:11:41 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> mg, PROXY_URI_BASES -> http://mg:8088/proxy/application_1507856833944_0003), /proxy/application_1507856833944_0003; 17/10/13 18:11:41 INFO ui.JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter; 17/10/13 18:11:41 INFO yarn.C",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-336412775:11718,queue,queue,11718,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-336412775,1,['queue'],['queue']
Performance," advance. ```. Using GATK jar /home/fmbuga/.conda/envs/gatk4/share/gatk4-4.2.6.1-1/gatk-package-4.2.6.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/fmbuga/.conda/envs/gatk4/share/gatk4-4.2.6.1-1/gatk-package-4.2.6.1-local.jar CNNScoreVariants --version; Using GATK jar /home/fmbuga/.conda/envs/gatk4/share/gatk4-4.2.6.1-1/gatk-package-4.2.6.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/fmbuga/.conda/envs/gatk4/share/gatk4-4.2.6.1-1/gatk-package-4.2.6.1-local.jar CNNScoreVariants -R /home/fmbuga/tools/hg38/hg38.fa -V /home/fmbuga/gatk4_gcp_wgs/06_vcf_raw/SRR16299720_dedup_AORRG_recal_raw.vcf -O ./08_vcf_1dCNN/SRR16299720_dedup_AORRG_recal_raw_1dCNN_scored.vcf; 05:39:39.149 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/fmbuga/.conda/envs/gatk4/share/gatk4-4.2.6.1-1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 05:39:39.304 INFO CNNScoreVariants - ------------------------------------------------------------; 05:39:39.305 INFO CNNScoreVariants - The Genome Analysis Toolkit (GATK) v4.2.6.1; 05:39:39.305 INFO CNNScoreVariants - For support and documentation go to https://software.broadinstitute.org/gatk/; 05:39:39.305 INFO CNNScoreVariants - Executing as fmbuga@node05.cluster on Linux v3.10.0-1062.18.1.el7.x86_64 amd64; 05:39:39.305 INFO CNNScoreVariants - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_332-b09; 05:39:39.305 INFO CNNScoreVariants - Start Date/Time: October 9, 2022 5:39:39 AM PDT; 05:39:39.305 INFO CNNScoreVariants - ------------------------------------------------------------; 05:39:39.306 INFO CNNScoreVariants - ------------------------------------------------------------; 05:39:39.306 INFO CNNScoreVariants ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7811#issuecomment-1274925490:1114,Load,Loading,1114,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7811#issuecomment-1274925490,1,['Load'],['Loading']
Performance, at org.gradle.internal.execution.steps.ResolveInputChangesStep.execute(ResolveInputChangesStep.java:48); at org.gradle.internal.execution.steps.ResolveInputChangesStep.execute(ResolveInputChangesStep.java:33); at org.gradle.internal.execution.steps.CancelExecutionStep.execute(CancelExecutionStep.java:39); at org.gradle.internal.execution.steps.TimeoutStep.executeWithoutTimeout(TimeoutStep.java:73); at org.gradle.internal.execution.steps.TimeoutStep.execute(TimeoutStep.java:54); at org.gradle.internal.execution.steps.CatchExceptionStep.execute(CatchExceptionStep.java:35); at org.gradle.internal.execution.steps.CreateOutputsStep.execute(CreateOutputsStep.java:51); at org.gradle.internal.execution.steps.SnapshotOutputsStep.execute(SnapshotOutputsStep.java:45); at org.gradle.internal.execution.steps.SnapshotOutputsStep.execute(SnapshotOutputsStep.java:31); at org.gradle.internal.execution.steps.CacheStep.executeWithoutCache(CacheStep.java:208); at org.gradle.internal.execution.steps.CacheStep.execute(CacheStep.java:70); at org.gradle.internal.execution.steps.CacheStep.execute(CacheStep.java:45); at org.gradle.internal.execution.steps.BroadcastChangingOutputsStep.execute(BroadcastChangingOutputsStep.java:49); at org.gradle.internal.execution.steps.StoreSnapshotsStep.execute(StoreSnapshotsStep.java:43); at org.gradle.internal.execution.steps.StoreSnapshotsStep.execute(StoreSnapshotsStep.java:32); at org.gradle.internal.execution.steps.RecordOutputsStep.execute(RecordOutputsStep.java:38); at org.gradle.internal.execution.steps.RecordOutputsStep.execute(RecordOutputsStep.java:24); at org.gradle.internal.execution.steps.SkipUpToDateStep.executeBecause(SkipUpToDateStep.java:96); at org.gradle.internal.execution.steps.SkipUpToDateStep.lambda$execute$0(SkipUpToDateStep.java:89); at org.gradle.internal.execution.steps.SkipUpToDateStep.execute(SkipUpToDateStep.java:54); at org.gradle.internal.execution.steps.SkipUpToDateStep.execute(SkipUpToDateStep.java:38); at org.gradle.inter,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6466#issuecomment-590387973:9165,Cache,CacheStep,9165,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6466#issuecomment-590387973,2,['Cache'],['CacheStep']
Performance," compare the result of MarkDuplicates and MarkDuplicatesSpark.; the same input SAM file and the default parameter, the MarkDuplicatesSpark have more data marked as duplicated.; Can you give me any suggest how to debug it, why the Spark version have more data marked?. READ_PAIR_DUPLICATES; **11933661 (MarkDuplicates); 11974162 (MarkDuplicatesSpark)**. Here is the metric file; ```. MarkDuplicatesSpark --output hdfs://wolfpass-aep:9000/user/test/spark_412.MarkDuplicates.bam --metrics-file hdfs://wolfpass-aep:9000/user/test/spark_412.MarkDuplicates-metrics.txt --input hdfs://wolfpass-aep:9000/user/test/spark_412.bowtie2.bam --spark-master yarn --duplicate-scoring-strategy SUM_OF_BASE_QUALITIES --do-not-mark-unmapped-mates false --read-name-regex <optimized capture of last three ':' separated fields as numeric values> --optical-duplicate-pixel-distance 100 --read-validation-stringency SILENT --interval-set-rule UNION --interval-padding 0 --interval-exclusion-padding 0 --interval-merging-rule ALL --bam-partition-size 0 --disable-sequence-dictionary-validation false --add-output-vcf-command-line true --sharded-output false --num-reducers 0 --help false --version false --showHidden false --verbosity INFO --QUIET false --use-jdk-deflater false --use-jdk-inflater false --gcs-max-retries 20 --gcs-project-for-requester-pays --disable-tool-default-read-filters false. METRICS CLASS	org.broadinstitute.hellbender.utils.read.markduplicates.GATKDuplicationMetrics LIBRARY	UNPAIRED_READS_EXAMINED	READ_PAIRS_EXAMINED	SECONDARY_OR_SUPPLEMENTARY_RDS	UNMAPPED_READS	UNPAIRED_READ_DUPLICATES READ_PAIR_DUPLICATES	READ_PAIR_OPTICAL_DUPLICATES	PERCENT_DUPLICATION ESTIMATED_LIBRARY_SIZE; lib1	173613	53799913	0	7610605	81003	11974162	585768	0.222961	05870713. MarkDuplicates --INPUT /home/test/WGS_pipeline/TEST/output/orig_412.bowtie2.bam --OUTPUT /home/test/WGS_pipeline/TEST/output/orig_412.MarkDuplicates.bam --METRICS_FILE /home/test/WGS_pipeline/TEST/output/orig_412.MarkDuplicates-metrics.txt -",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427229905:759,optimiz,optimized,759,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427229905,1,['optimiz'],['optimized']
Performance," go back; > and check what the defaults were for whatever version of the jar they were; > using at the time. Option 2 might also make it easier to inadvertently; > override parameters, etc. via command-line typos or copy-and-paste; > errors---it's much more straightforward to require and check that every; > parameter is specified once and fallback to a default if not, as we do now.; > Not to say that we couldn't get around any of these issues in Barclay, but; > I think it'll require some thought and careful design. Would be interested; > to hear Engine team's opinions.; >; > Finally, one point that I think will become more relevant as our tools and; > pipeline become more flexible and parameterized: I think we should start; > thinking of ""Best Practices Recommendations"" less as ""here is the best set; > of parameters to use with your data"" and more as ""here is *how to find*; > the best set of parameters to use with your data (for a given truth set,; > sensitivity requirement, etc.)"". After all, if we are putting together; > pipelines to do hyperparameter optimization, there is no reason not to; > share them with the community.; >; > This would also relax the requirement that the defaults in the WDL (which; > have to be kept in sync with those in the GATK jar) represent some sort of; > Best Practices Recommendation, which is awkward in exactly scenarios like; > the one you highlight.; >; > @vdauwera <https://github.com/vdauwera> @LeeTL1220; > <https://github.com/LeeTL1220> @sooheelee <https://github.com/sooheelee>; > might have some thoughts.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385584289>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk2h5MhZ7nXrNgo6MrFpMD-TGiAE8ks5tt8gjgaJpZM4TtVZZ>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 8011A; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385677379:2359,optimiz,optimization,2359,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385677379,1,['optimiz'],['optimization']
Performance," in the sampling of denoised copy ratios, fixes a memory leak by updating theano, and also adds some theano flags that typically yield a factor of ~2 speedup (notably, the OpenMP elemwise flag, although we also get a slight boost from using numpy MKL). This allows us to run, e.g.: . 2 shards of 50 samples by 100000 intervals on n1-standard-8s (8 CPU, 30GB memory, $0.08 / hr) each taking ~5 hours = ~1.6 cents / sample; 4 shards of 50 samples by 50000 intervals on n1-highmem-4s (4 CPU, 26GB memory, $0.05 / hr) each taking ~3.25 hours = ~1.3 cents / sample; 45 shards of 50 samples by 5000 intervals on *n1-standard-1s* (1CPU, 3.75GB memory, $0.01 / hr) each taking ~0.5 hours = ~0.5 cents / sample. For these runs, we used a slightly larger interval list and 1/4 the number of samples than in the first example, but because everything scales linearly, it's probably fair to compare the per-sample-and-interval costs. So we get a factor of ~8 savings if we keep the shard size the same. The cost was already satisfactory, but fixing the leak allows us to more easily run scatters that are not so wide, which may be crucial for running the megaWDL. Adding the OpenMP flag also lets CPU scalability work as intended. We can do a more systematic optimization for cost if desired, and we should also revalidate to make sure performance doesn't vary too much with shard size (from spot checking, it looks like marginal and/or single-bin calls may flicker on and off). Note that we have still not optimized inference for WES, although I believe @vruano has done some optimizations for WGS. @mwalker174 @vruano for WGS with 2kb bins, I would expect the cost of the gCNV step to be ~10 cents in cohort mode before inference optimizations, assuming we address #5716 to minimize disk costs. @asmirnov239 can you review? And maybe you can address dCR output in PostprocessGermlineCNVCalls and expose the number of samples in a separate PR? We can make some further changes to the dCR format there if we need.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5781#issuecomment-471570697:1475,scalab,scalability,1475,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5781#issuecomment-471570697,6,"['optimiz', 'perform', 'scalab']","['optimization', 'optimizations', 'optimized', 'performance', 'scalability']"
Performance," increased dictionary size, because they are more repetitive than the DNA data. And shorter BAMs would be different because they are less repetitive (usually less coverage), so their compression relies more on CPU-expensive crunching of the ""2bit nature"" of the DNA.; So they might logically suffer more from a lower compression level.; It might be instructive to compare compression sizes of raw sequence data of two BAM files with output of faToTwoBit / 2.bit files.; 2bit files are compressed by a factor of around four, which gzip often does not reach (because it doesn't know ahead of time that DNA has only four letters).; Use reference genome fasta as proxy for nearly no repetition at all. It doesn't compress much beyond 2bit. Tweaking of the Huffmann coding etc. might have influenced the compression level much in this case, by ""giving the compressor a subtle hint about the four letters"".; Paradoxically, Intel might have optimized for average data and thus brought a disadvantage for the four letter nature of DNA (and also the few letters used in quality data encoding compared to text). 3. BQSR:; When I did interleaving compression experiments, I noticed that the BQSR step decreases compressiblity considerably.; In this example I had the same BAM file in different versions that were aligned to hs38DH, hs38, hs37d5 and could compress them to nearly the size of one, by putting similar pieces of the files after one another.; Adding the same BAM with BQSR increased final file size more than several pre-BQSR versions together.; Note: This piece-meal packing might be useful for different BAMs mostly only with many BAMs where similar regions accumulate. 4. Even faster:; In my experience, level 0 (no compression) (with samtools view -u) increases speed even more, if files are on a lz4 encrypted disk (such as with ZFS).; The speed-up of lz4 over even level 1 of any gzip-like compression is substantial.; With data on SSDs or similarly fast storage, that can make a huge differenc",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3413#issuecomment-360179673:2700,optimiz,optimized,2700,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3413#issuecomment-360179673,1,['optimiz'],['optimized']
Performance, java.io.InputStreamReader.read(InputStreamReader.java:184); 	at htsjdk.tribble.readers.LongLineBufferedReader.fill(LongLineBufferedReader.java:140); 	at htsjdk.tribble.readers.LongLineBufferedReader.readLine(LongLineBufferedReader.java:298); 	at htsjdk.tribble.readers.LongLineBufferedReader.readLine(LongLineBufferedReader.java:354); 	at htsjdk.tribble.readers.SynchronousLineReader.readLine(SynchronousLineReader.java:51); 	at htsjdk.tribble.readers.LineIteratorImpl.advance(LineIteratorImpl.java:24); 	at htsjdk.tribble.readers.LineIteratorImpl.advance(LineIteratorImpl.java:11); 	at htsjdk.samtools.util.AbstractIterator.hasNext(AbstractIterator.java:44); 	at htsjdk.variant.vcf.VCFCodec.readActualHeader(VCFCodec.java:89); 	at htsjdk.tribble.AsciiFeatureCodec.readHeader(AsciiFeatureCodec.java:83); 	at htsjdk.tribble.AsciiFeatureCodec.readHeader(AsciiFeatureCodec.java:36); 	at htsjdk.tribble.TabixFeatureReader.readHeader(TabixFeatureReader.java:100); 	... 12 more; Caused by: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: Connection has been shutdown: javax.net.ssl.SSLException: java.net.SocketException: Connection reset; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.getBuf(SeekableByteChannelPrefetcher.java:136); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.fetch(SeekableByteChannelPrefetcher.java:255); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:300); 	... 41 more; Caused by: com.google.cloud.storage.StorageException: Connection has been shutdown: javax.net.ssl.SSLException: java.net.SocketException: Connection reset; 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:186); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.jav,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-301610931:4625,concurren,concurrent,4625,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-301610931,1,['concurren'],['concurrent']
Performance," normals with 5M bins each, CombineReadCounts took ~1 min, CreatePanelOfNormals (with no QC) took ~4.5 minutes (although ~1 minute of this is writing target weights, which I haven't added to the new version yet) and generated a 2.7GB PoN, and NormalizeSomaticReadCounts took ~8 minutes (~7.5 minutes of which was spent composing/writing results, thanks to overhead from ReadCountCollection). In comparison, the new CreateReadCountPanelOfNormals took ~1 minute (which includes combining read-count files, which takes ~30s of I/O) and generated a 520MB PoN, and DenoiseReadCounts took ~30s (~10s of which was composing/writing results, as we are still forced to generate two ReadCountCollections). Resulting PTN and TN copy ratios were identical down to 1E-16 levels. Differences are only due to removing the unnecessary pseudoinverse computation. Results after filtering and before SVD are identical, despite the code being rewritten from scratch to be more memory efficient (e.g., filtering is performed in place)---phew!. If we stored read counts as HDF5 instead of as plain text, this would make things much faster. Perhaps it would be best to make TSV an optional output of the new coverage collection tool. As a bonus, it would then only take a little bit more code to allow previous PoNs to be provided via -I as an additional source of read counts. Remaining TODO's:. - [x] Allow DenoiseReadCounts to be run without a PoN. This will just perform standardization and optional GC correction. This gives us the ability to run the case sample pipeline without a PoN, which will give users more options and might be good enough if the data is not too noisy.; - [x] Actually, I'm not sure why we take perform SVD on the intervals x samples matrix and take the left-singular vectors. <s>Typically, SVD is performed on the samples x intervals matrix and the right-singular vectors are taken, which saves some extra computation that is necessary to calculate the left-singular vectors.</s> (EDIT: Actuall",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351:1105,perform,performed,1105,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351,1,['perform'],['performed']
Performance," phasing, which is supported only for reference-model confidence output; 14:50:19.280 WARN PossibleDeNovo - Annotation will not be calculated, must provide a valid PED file (-ped) from the command line.; 14:50:19.481 WARN PossibleDeNovo - Annotation will not be calculated, must provide a valid PED file (-ped) from the command line.; 14:50:19.776 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/humgen/gsa-hpprojects/GATK/gatk4/gatk-4.beta.5/gatk-package-4.beta.5-local.jar!/com/intel/gkl/native/libgkl_utils.so; 14:50:19.795 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/humgen/gsa-hpprojects/GATK/gatk4/gatk-4.beta.5/gatk-package-4.beta.5-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 14:50:19.847 WARN IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 14:50:19.848 INFO IntelPairHmm - Available threads: 48; 14:50:19.848 INFO IntelPairHmm - Requested threads: 4; 14:50:19.848 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 14:50:19.926 INFO ProgressMeter - Starting traversal; 14:50:19.926 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 14:50:30.309 INFO ProgressMeter - chr17:740224 0.2 3010 17395.5; 14:50:41.016 INFO ProgressMeter - chr17:1675683 0.4 7020 19973.4; 14:50:51.041 INFO ProgressMeter - chr17:2415218 0.5 10100 19477.4; 14:51:01.041 INFO ProgressMeter - chr17:3591332 0.7 14920 21773.6; 14:51:11.059 INFO ProgressMeter - chr17:4574538 0.9 19100 22412.6; 14:51:21.089 INFO ProgressMeter - chr17:5381890 1.0 22460 22033.3; 14:51:31.097 INFO ProgressMeter - chr17:6474462 1.2 27070 22821.4; 14:51:41.535 INFO ProgressMeter - chr17:7455949 1.4 31150 22902.4; 14:51:51.542 INFO ProgressMeter - chr17:8073825 1.5 33820 22149.2; 14:52:01.549 INFO ProgressMeter - chr17:9138632 1.7 38220 22566.0; 14:52:11.962 INFO ProgressMeter - chr17:10514361 1.9 43840 23478.4; 14:52:21.975 INFO ProgressMeter - chr17:11679575 2.0 48560 23872.6; 1",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3154#issuecomment-334840678:7233,multi-thread,multi-threaded,7233,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3154#issuecomment-334840678,1,['multi-thread'],['multi-threaded']
Performance," physical phasing, which is supported only for reference-model confidence output; >; > 16:17:06.588 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; >; > 16:17:06.589 **WARN** NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils347167544598047196.so: /tmp/libgkl_utils347167544598047196.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > 16:17:06.589 **WARN** IntelPairHmm - Intel GKL Utils not loaded; >; > 16:17:06.589 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; >; > 16:17:06.589 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; >; > 16:17:06.590 **WARN** NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils6186849302609329058.so: /tmp/libgkl_utils6186849302609329058.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > 16:17:06.590 **WARN** IntelPairHmm - Intel GKL Utils not loaded; >; > 16:17:06.591 **WARN** PairHMM - ***WARNING: Machine does not have the AVX instruction set support needed for the accelerated AVX PairHmm. Falling back to the MUCH slower LOGLESS_CACHING implementation!; >; > Since the calculation takes quite long, I checked the WARN messages of the; > output above. Especially the last one about the AVX instruction set where; > it says that a *MUCH* slower implementation will be used. From the few; > WARN messages it seems like the root cause is the failure to load libgkl; > and that again seems to be related to my platform. Does anyone know more; > about this issue or how to work around it?; >; > Best ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600:5872,load,load,5872,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600,1,['load'],['load']
Performance," remove the awkwardness required by `PadTargets` and `CalculateTargetCoverage`/`SparkGenomeReadCounts`. Denoising:; - All parameters are exposed in the PoN creation tool (#3356).; - Without a PoN, standardization and optional GC correction are performed (#3570).; - Other than the minor point about sample mean/median being used inconsistently noted above, the denoising process is essentially exactly the same mathematically as before (""superficial"" differences include the vastly improved memory and I/O optimizations, the ability to adjust number of principal components used, the removal of redundant SVDs, the enforcement of consistent GC-bias correction).; - [ ] That said, I'll carry over this TODO from above: Revisit standardization procedure by checking with simulated data. We should make sure that the centering of the data does not rescale the true copy ratio.; - The only major difference is we no longer make a QC PoN or check for large events. This was performed awkwardly in the old pipeline, so I'd rather not port it over. Eventually we will do all denoising with the gCNV coverage model anyway.; - Pre/tangent-normalization copy ratio are now referred to as standardized/denoised copy ratio.; - [x] Old code is still used for GC-bias correction in `CreateReadCountPanelOfNormals`, and we still use the `AnnotateTargets` tool. We should port this over (possibly as part of `PreprocessIntervals`) at some point (actually, I think we will be forced to, since `PreprocessIntervals` will output a Picard interval list, and `AnnotateTargets` outputs a target file).; - [x] Integration tests are still needed for `CreateReadCountPanelOfNormals`. These might not test for correctness, but we could possibly compare to old PoNs. Segmentation/modeling:; - Instead of separate tools for copy-ratio segmentation (`PerformSegmentation`) and allele-fraction segmentation/union/modeling (`AllelicCNV`), there is now just a single segmentation/modeling tool (`ModelSegments`).; - Input is denoise",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:2269,perform,performed,2269,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828,1,['perform'],['performed']
Performance," reports that this error still occurs even after the patch in https://github.com/broadinstitute/gatk/pull/5099. With that patch, we are now retrying on `UnknownHostException`, but the retries are all failing: . ```; [August 14, 2018 7:09:18 AM UTC] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 896.64 minutes.; Runtime.totalMemory()=3966238720; java.util.concurrent.CompletionException: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: Failure while waiting for FeatureReader to initialize with exception: com.google.cloud.storage.StorageException: All 20 reopens failed. Waited a total of 1918000 ms between attempts; at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273); at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1592); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: Failure while waiting for FeatureReader to initialize with exception: com.google.cloud.storage.StorageException: All 20 reopens failed. Waited a total of 1918000 ms between attempts; at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lambda$getFeatureReadersInParallel$614(GenomicsDBImport.java:605); at java.util.LinkedHashMap.forEach(LinkedHashMap.java:684); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getFeatureReadersInParallel(GenomicsDBImport.java:600); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.createSampleToReaderMap(GenomicsDBImport.java:491); at com.intel.genomicsdb.importer.GenomicsDBImporter.lambda$null$2(Geno",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-412904420:1019,concurren,concurrent,1019,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-412904420,1,['concurren'],['concurrent']
Performance," sc.setLogLevel(newLevel).; Failed to created SparkJLineReader: java.io.IOException: Permission denied; Falling back to SimpleReader.; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /___/ .__/\_,_/_/ /_/\_\ version 1.6.0; /_/. Using Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_91); Type in expressions to have them evaluated.; Type :help for more information.; Spark context available as sc (master = yarn-client, app id = application_1507683879816_0007).; Wed Oct 11 14:25:24 CST 2017 Thread[main,5,main] java.io.FileNotFoundException: derby.log (Permission denied); ----------------------------------------------------------------; Wed Oct 11 14:25:24 CST 2017:; Booting Derby version The Apache Software Foundation - Apache Derby - 10.11.1.1 - (1616546): instance a816c00e-015f-0a1b-f1bd-00002ce33928 ; on database directory /tmp/spark-98953d35-8594-4907-b4a5-0870f1d17b3e/metastore with class loader sun.misc.Launcher$AppClassLoader@5c647e05 ; Loaded from file:/opt/cloudera/parcels/CDH-5.12.1-1.cdh5.12.1.p0.3/jars/derby-10.11.1.1.jar; java.vendor=Oracle Corporation; java.runtime.version=1.8.0_91-b14; user.dir=/opt/Software/gatk; os.name=Linux; os.arch=amd64; os.version=3.10.0-514.el7.x86_64; derby.system.home=null; Database Class Loader started - derby.database.classpath=''; 17/10/11 14:25:33 WARN metastore.ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.1.0-cdh5.12.1; 17/10/11 14:25:33 WARN metastore.ObjectStore: Failed to get database default, returning NoSuchObjectException; SQL context available as sqlContext. **./gradlew bundle**; **[root@com1 gatk]# ./gradlew bundle; when I executed the command ”./gradlew bundle”， it appeared the error in the last ，did this matter？**. .......; [loading ZipFileIndexFileObject[/root/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.core/jackson-databind/2.6.5/d50be1723a09be903887099ff2014ea9020",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-335696240:2049,Load,Loaded,2049,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-335696240,1,['Load'],['Loaded']
Performance, shaded.cloud_nio.com.google.common.util.concurrent.Futures.addCallback(Futures.java:1713); 	at shaded.cloud_nio.com.google.api.gax.core.ApiFutures.addCallback(ApiFutures.java:47); 	at shaded.cloud_nio.com.google.api.gax.retrying.RetryingFutureImpl.setAttemptFuture(RetryingFutureImpl.java:107); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:100); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:47); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:125); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:109); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:131); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:104); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: java.net.SocketTimeoutException: Read timed out; 	at java.net.SocketInputStream.socketRead0(Native Method); 	at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); 	at java.net.SocketInputStream.read(SocketInputStream.java:171); 	at java.net.SocketInputStream.read(SocketInputStream.java:141); 	at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); 	at sun.security.ssl.InputRecord.read(InputRecord.java:503); 	at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973); 	at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:930); 	at sun.security.ssl.AppInputStream.read(AppInputStream.java:105); 	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286); 	at java.i,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300298180:5992,concurren,concurrent,5992,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300298180,1,['concurren'],['concurrent']
Performance," should definitely provide defaults for typical data types in *documentation*.) And in the end, I think it is beneficial for users that wish to tweak knobs to do some work to understand what those knobs actually do (even if just at a basic level). The other downside of option 2 is that it might not be immediately obvious from the command line what parameters are being used. For example, if a user chooses a set of defaults but then overrides some of them, we should make it so they don't have to go digging through the logs to see what parameters are actually used in the end. Nor should they have to go back and check what the defaults were for whatever version of the jar they were using at the time. Option 2 might also make it easier to inadvertently override parameters, etc. via command-line typos or copy-and-paste errors---it's much more straightforward to require and check that every parameter is specified once and fallback to a default if not, as we do now. Not to say that we couldn't get around any of these issues in Barclay, but I think it'll require some thought and careful design. Would be interested to hear Engine team's opinions. Finally, one point that I think will become more relevant as our tools and pipelines become more flexible and parameterized: I think we should start thinking of ""Best Practices Recommendations"" less as ""here is the best set of parameters to use with your data"" and more as ""here is *how to find* the best set of parameters to use with your data (for a given truth set, sensitivity requirement, etc.)"". After all, if we are putting together pipelines to do hyperparameter optimization, there is no reason not to share them with the community. This would also relax the requirement that the defaults in the WDL (which have to be kept in sync with those in the GATK jar) represent some sort of Best Practices Recommendation, which is awkward in exactly scenarios like the one you highlight. @vdauwera @LeeTL1220 @sooheelee might have some thoughts.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385584289:1977,optimiz,optimization,1977,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385584289,1,['optimiz'],['optimization']
Performance," should proceed here, if at all? @ldgauthier reminded me that this story was unfinished and is getting a little stale. @fleharty take note if we want to report progress on this front to our MalariaGEN collaborators. On my end, there are a couple of things to do:; - [x] rebase and resolve conflicts; - [x] change TSV input as discussed above; - [x] add doc strings for new arguments; - [x] add integration tests to make absolutely sure exposure was done correctly, perhaps? I'm open to discussion about how this should be done. Complete coverage here will be difficult and perhaps not worth the effort, but I can probably put in a few tests that make sure changing the hard-coded values in master and doing the same via the exposed parameters in this branch have the same effect on a few existing test cases. However, while I'm doing the last three, I wonder if we could run whatever canonical evaluations/optimizations we have to see whether it's worth consolidating some of the parameter sets at this stage? I think there's an argument for having at least two sets (haplotype-to-reference + read-to-haplotype), but I'm not sure how to justify having a separate set for dangling heads/tails. But also not sure which set the latter should be consolidated with---@jamesemery thoughts? Again, let me reiterate that it seems that many of these parameter values were chosen arbitrarily (or, if not, that the procedure for choosing them has been lost). As a start, you can see the results of some optimizations I did on the CHM mix on slide 15 at https://docs.google.com/presentation/d/1zGuquAZWSUQ-wNxp8D6HhGNjIaFcV0_X9WAS4LODbEo/edit?usp=sharing Here, I optimized over haplotype-to-reference + read-to-haplotype SW parameters on various metrics after variant normalization using vcfeval. These optimizations were done using the Bayesian optimization framework I prototyped long ago (see https://github.com/broadinstitute/gatk-evaluation/tree/master/pipeline-optimizer and https://docs.google.com/present",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6885#issuecomment-891907471:934,optimiz,optimizations,934,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6885#issuecomment-891907471,1,['optimiz'],['optimizations']
Performance," still generating the error which it does not (even on my side). However, my few tests made today resulted in interesting observations. ## Traces. Below the command and trace produced from my real case. I annonymized it but the number of characters in path were kept. ```java; (cerc_prod) [16:48 xxxxxxx@yyyyyy:test a]$ gatk MergeVcfs -I data/calling/cerc_prod2.SM_V7_1.vcf.gz -I data/calling/cerc_prod2.SM_V7_ZW.vcf.gz -O out.vcf.gz; Using GATK jar /master/xxxxxxx/local/pckg/python/miniconda3/envs/cerc_prod/share/gatk4-4.1.7.0-0/gatk-package-4.1.7.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /master/xxxxxxx/local/pckg/python/miniconda3/envs/cerc_prod/share/gatk4-4.1.7.0-0/gatk-package-4.1.7.0-local.jar MergeVcfs -I data/calling/cerc_prod2.SM_V7_1.vcf.gz -I data/calling/cerc_prod2.SM_V7_ZW.vcf.gz -O out.vcf.gz; 16:48:58.710 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/master/xxxxxxx/local/pckg/python/miniconda3/envs/cerc_prod/share/gatk4-4.1.7.0-0/gatk-package-4.1.7.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; [Mon Jun 22 16:48:58 CDT 2020] MergeVcfs --INPUT data/calling/cerc_prod2.SM_V7_1.vcf.gz --INPUT data/calling/cerc_prod2.SM_V7_ZW.vcf.gz --OUTPUT out.vcf.gz --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX true --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false; Jun 22, 2020 4:48:58 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; [Mon Jun 22 16:48:58 CDT 2020] Executing as xxxxxxx@yyyyyy on Linux 3.10.0-693.11.1.el7.x86_64 amd64; OpenJDK 64-Bit Server VM 1.8.0_152-release-1056-b12; D",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6664#issuecomment-647808241:1199,Load,Loading,1199,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6664#issuecomment-647808241,1,['Load'],['Loading']
Performance," the last developer who left this in a reasonable state a beverage of their choice.; # (This may be yourself, and you'll appreciate that beverage while you tinker with dependencies!); #; # When changing dependencies or versions in this file, check to see if the ""supportedPythonPackages"" DataProvider; # used by the testGATKPythonEnvironmentPackagePresent test in PythonEnvironmentIntegrationTest needs to be updated; # to reflect the changes.; #; name: gatk; channels:; # if channels other than conda-forge are added and the channel order is changed (note that conda channel_priority is currently set to flexible),; # verify that key dependencies are installed from the correct channel and compiled against MKL; - conda-forge; - defaults; dependencies:. # core python dependencies; - conda-forge::python=3.6.10 # do not update; - pip=20.0.2 # specifying channel may cause a warning to be emitted by conda; - conda-forge::mkl=2019.5 # MKL typically provides dramatic performance increases for theano, tensorflow, and other key dependencies; - conda-forge::mkl-service=2.3.0; - conda-forge::numpy=1.17.5 # do not update, this will break scipy=0.19.1; # verify that numpy is compiled against MKL (e.g., by checking *_mkl_info using numpy.show_config()); # and that it is used in tensorflow, theano, and other key dependencies; - conda-forge::theano=1.0.4 # it is unlikely that new versions of theano will be released; # verify that this is using numpy compiled against MKL (e.g., by the presence of -lmkl_rt in theano.config.blas.ldflags); - defaults::tensorflow=1.15.0 # update only if absolutely necessary, as this may cause conflicts with other core dependencies; # verify that this is using numpy compiled against MKL (e.g., by checking tensorflow.pywrap_tensorflow.IsMklEnabled()); - conda-forge::scipy=1.0.0 # do not update, this will break a scipy.misc.logsumexp import (deprecated in scipy=1.0.0) in pymc3=3.1; - conda-forge::pymc3=3.1 # do not update, this will break gcnvkernel; - conda-forge:",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6656#issuecomment-643526868:1632,perform,performance,1632,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6656#issuecomment-643526868,1,['perform'],['performance']
Performance," the log file: ```*** Error in `java’: munmap_chunk(): invalid pointer: 0x00007f685d06c840 ***```. The respective backtraces: . ```; *** Error in `java': double free or corruption (out): 0x00007f6364699340 ***; ======= Backtrace: =========; /lib/x86_64-linux-gnu/libc.so.6(+0x777e5)[0x7f636ba307e5]; /lib/x86_64-linux-gnu/libc.so.6(+0x8037a)[0x7f636ba3937a]; /lib/x86_64-linux-gnu/libc.so.6(cfree+0x4c)[0x7f636ba3d53c]; /cromwell_root/tmp.7626fbcf/libgkl_smithwaterman1454827346682980108.so(_Z19runSWOnePairBT_avx2iiiiPhS_iiaPcPs+0x338)[0x7f63123c8fa8]; /cromwell_root/tmp.7626fbcf/libgkl_smithwaterman1454827346682980108.so(Java_com_intel_gkl_smithwaterman_IntelSmithWaterman_alignNative+0xd8)[0x7f63123c8bf8]; [0x7f6355bff192]; ```. ```; *** Error in `java': munmap_chunk(): invalid pointer: 0x00007f685d06c840 ***; ======= Backtrace: =========; /lib/x86_64-linux-gnu/libc.so.6(+0x777e5)[0x7f68634c37e5]; /lib/x86_64-linux-gnu/libc.so.6(cfree+0x1a8)[0x7f68634d0698]; /cromwell_root/tmp.4eeeda3c/libgkl_smithwaterman7538158038428947321.so(_Z19runSWOnePairBT_avx2iiiiPhS_iiaPcPs+0x338)[0x7f6830cf2fa8]; /cromwell_root/tmp.4eeeda3c/libgkl_smithwaterman7538158038428947321.so(Java_com_intel_gkl_smithwaterman_IntelSmithWaterman_alignNative+0xd8)[0x7f6830cf2bf8]; [0x7f684dc31f92]; ```. In each of these occurrences, the filtered vcf file was produced, but the vcf.idx file was missing. Although the java errors occur, the last line of the log denotes the step as a success: (This might be true, but only when the option --create-output-variant-index is set to false.; `SetOperationStatus(copied 0 file(s) to <destinations_folder> succeeded""`. I also performed a test based on machine type. (outside of the full workflow, starting the steps on my own on a separate instance & replicating the steps of the workflow); - Using an instance with 2 vCPU's, 7.5 GB of ram, just ran out of memory.; - Using an instance with 8 vCPU's, 30 GB of ram finished successfully, producing both the filtered vcf & vcf.idx",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-652696262:2481,perform,performed,2481,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-652696262,1,['perform'],['performed']
Performance," this; I now just take a transpose before performing the SVD, so that we still operate on the same intervals x samples matrix.) This will also save us some transposing, which we do anyway to make HDF5 writes faster.; - [x] Change HDF5 matrix writing to allow matrices with NxM > MAX_INT, which can be done naively by chunking and writing to multiple HDF5 subdirectories. This will allow for smaller bin sizes. (EDIT: I implemented this in a way that allows one to set the maximum number of values allowed per chunk, so that heap usage can be controlled, but the downside is that this translates into a corresponding limit on the number of columns (i.e., intervals). On the other hand, you could theoretically crank this number up to Integer.MAX_VALUE, as long as you set -Xmx high enough... In practice, it's very unlikely that we'll need to go to bins smaller than a read length.); - [ ] <s>Check that CreatePanelOfNormals works correctly on Spark cluster.</s> Implement Randomized SVD, which should give better performance on large matrices. See https://arxiv.org/pdf/1007.5510.pdf and https://research.fb.com/fast-randomized-svd/. For now, I'll require that the coverage matrix can fit in RAM, but more sophisticated versions of the algorithm could be implemented in the future.; - [ ] Update methods doc. Note that some of the CNV section is out of date and incorrect. In particular, we have been taking in PCOV as input to CreatePanelOfNormals for some time now, but the doc states that we take integer read counts. This already yields different results by the first filtering step (on intervals by interval median). @LeeTL1220 @davidbenjamin what is the ""official ReCapSeg"" behavior, and do we want to keep the current behavior? In general, I think all of the standardization (i.e., filtering/imputation/truncation/transformation) steps could stand some revisiting. Evaluation:. - [ ] Revisit standardization procedure by checking with simulated data. We should make sure that the centering of ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351:3383,perform,performance,3383,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351,1,['perform'],['performance']
Performance," to messages in stdout. Includes # total records, number of records that were trimmed, # variant records skipped due to ref allele being too long and finally the max-indel-length value that needs to be set to include these in the leftalignandtrim. This is an improvement to previous stdout messaging. Upping max-indel-length; ```; WMCF9-CB5:shlee$ ./gatk LeftAlignAndTrimVariants -R ~/Documents/ref/hg38/Homo_sapiens_assembly38.fasta -V ~/Downloads/zeta_snippet_shlee/zeta_snippet.vcf.gz --max-indel-length 250 -O zeta_snippet_leftalign_250_96branch.vcf.gz; Using GATK wrapper script /Users/shlee/Documents/branches/hellbender/build/install/gatk/bin/gatk; Running:; /Users/shlee/Documents/branches/hellbender/build/install/gatk/bin/gatk LeftAlignAndTrimVariants -R /Users/shlee/Documents/ref/hg38/Homo_sapiens_assembly38.fasta -V /Users/shlee/Downloads/zeta_snippet_shlee/zeta_snippet.vcf.gz --max-indel-length 250 -O zeta_snippet_leftalign_250_96branch.vcf.gz; 14:03:44.243 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/shlee/Documents/branches/hellbender/build/install/gatk/lib/gkl-0.8.5.jar!/com/intel/gkl/native/libgkl_compression.dylib; Sep 06, 2018 2:03:44 PM shaded.cloud_nio.com.google.auth.oauth2.DefaultCredentialsProvider warnAboutProblematicCredentials; WARNING: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a ""quota exceeded"" or ""API not enabled"" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/.; 14:03:44.358 INFO LeftAlignAndTrimVariants - ------------------------------------------------------------; 14:03:44.358 INFO LeftAlignAndTrimVariants - The Genome Analysis Toolkit (GATK) v4.0.8.1-25-g0c6f06f-SNAPSHOT; 14:03:44.359 INFO LeftAlignAndTrimVariants - For support and documentation go ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3487#issuecomment-419190326:6768,Load,Loading,6768,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3487#issuecomment-419190326,1,['Load'],['Loading']
Performance, | [...s/coveragemodel/cachemanager/DuplicableNumber.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9EdXBsaWNhYmxlTnVtYmVyLmphdmE=) | `80% <100%> (+80%)` | `5 <2> (+5)` | :arrow_up: |; | [...coveragemodel/cachemanager/PrimitiveCacheNode.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9QcmltaXRpdmVDYWNoZU5vZGUuamF2YQ==) | `83.333% <71.429%> (+30.702%)` | `10 <7> (+3)` | :arrow_up: |; | [...er/tools/coveragemodel/cachemanager/CacheNode.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9DYWNoZU5vZGUuamF2YQ==) | `80.645% <76.923%> (+30.645%)` | `9 <8> (+4)` | :arrow_up: |; | [...overagemodel/cachemanager/ComputableCacheNode.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9Db21wdXRhYmxlQ2FjaGVOb2RlLmphdmE=) | `89.189% <80%> (+32.779%)` | `18 <17> (+2)` | :arrow_up: |; | [...ols/coveragemodel/CoverageModelEMComputeBlock.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL0NvdmVyYWdlTW9kZWxFTUNvbXB1dGVCbG9jay5qYXZh) | `77.617% <82.558%> (-1.61%)` | `49 <2> (-1)` | |; | [...dinstitute/hellbender/utils/MathObjectAsserts.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9NYXRoT2JqZWN0QXNzZXJ0cy5qYXZh) | `63.636% <84.615%> (+15.249%)` | `9 <3> (+4)` | :arrow_up: |; | ... and [6 more](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree-more) | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3035#issuecomment-306412418:3220,cache,cachemanager,3220,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3035#issuecomment-306412418,1,['cache'],['cachemanager']
Performance,!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: FilterAlignmentArtifacts is an EXPERIMENTAL tool and should not be used for production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!![0m. 01:39:03.364 INFO FilterAlignmentArtifacts - Initializing engine; 01:39:07.644 INFO FeatureManager - Using codec VCFCodec to read file gs://fc-ac4624cb-a8fc-49a2-b071-d3a0ae799418/cb1feccb-0a69-42bf-ba5f-fde762934a59/Mutect2/fe3623c8-0eaf-4cd4-9f81-d1fda4073f2e/call-Filter/22.hg38-filtered.vcf; 01:39:08.399 INFO FilterAlignmentArtifacts - Done initializing engine; 01:39:09.523 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 01:39:09.565 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 01:39:09.566 INFO IntelPairHmm - Available threads: 4; 01:39:09.566 INFO IntelPairHmm - Requested threads: 4; 01:39:09.566 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 01:39:09.567 INFO ProgressMeter - Starting traversal; 01:39:09.567 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; munmap_chunk(): invalid pointer; Using GATK jar /root/gatk.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx11500m -jar /root/gatk.jar FilterAlignmentArtifacts -R gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta -V gs://fc-ac4624cb-a8fc-49a2-b071-d3a0ae799418/cb1feccb-0a69-42bf-ba5f-fde762934a59/Mutect2/fe3623c8-0eaf-4cd4-9f81-d1fda4073f2e/call-Filter/22.hg38-filtered.vcf -I gs://fc-ac4624cb-a8fc-49a2-b071-d3a0ae799418/209d1183-ed9a-4755-a4b3-d595797640ea/PreProcessingForVariantDiscovery_GATK4/9f7c0ab6-b61b-4797-92f1-7929bbf677d8/call-GatherBamFiles/22.hg38.bam --bwa-mem-index-image /cromwe,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-664539860:5205,multi-thread,multi-threaded,5205,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-664539860,1,['multi-thread'],['multi-threaded']
Performance,"!!!!!!!!!!!!!!!!. Warning: FilterAlignmentArtifacts is an EXPERIMENTAL tool and should not be used for production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!![0m. 20:12:42.725 INFO FilterAlignmentArtifacts - Initializing engine; 20:12:48.403 INFO FeatureManager - Using codec VCFCodec to read file gs://fc-secure-024a1aae-a4f9-4025-aa93-f759f93a8203/50383670-4607-4e59-9bfc-4db970980f0e/Mutect2/773a91ea-25be-4d49-b97c-16527076250c/call-Filter/cacheCopy/TN-20-36-filtered.vcf; 20:12:50.117 INFO FilterAlignmentArtifacts - Done initializing engine; 20:12:51.042 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk/gatk-package-4.1.9.0-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 20:12:51.099 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 20:12:51.100 INFO IntelPairHmm - Available threads: 14; 20:12:51.100 INFO IntelPairHmm - Requested threads: 4; 20:12:51.100 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 20:12:51.100 INFO ProgressMeter - Starting traversal; 20:12:51.100 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 20:20:25.766 INFO ProgressMeter - chr3:104142090 7.6 1000 132.0; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007efc9818177e, pid=24, tid=0x00007f13b3c76700; #; # JRE version: OpenJDK Runtime Environment (8.0_242-b08) (build 1.8.0_242-8u242-b08-0ubuntu3~18.04-b08); # Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 ); # Problematic frame:; # C [libgkl_smithwaterman1809483713436863458.so+0x177e] smithWatermanBackTrack(dnaSeqPair*, int, int, int, int, int*, int)+0x60e; #; # Core dump written. Default location: /cromwell_root/core or core.24; #; # An error report file with more information is saved as:; # /cromwell_root/hs_err_pid24.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.jav",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-781673098:1341,multi-thread,multi-threaded,1341,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-781673098,1,['multi-thread'],['multi-threaded']
Performance,"![throughput vs core count](https://cloud.githubusercontent.com/assets/10458949/11080726/940ccfd2-87cb-11e5-99b8-a1ebc4bb6949.png). Thanks to @droazen for doing those measurements. I've taken the input and plotted the per-core throughput numbers above. What we expect to see is a drop after the non-distributed version, and then a steady decline as we add cores. This matches what we're seeing here: the walker throughput is 145 MB/min/core, and then we quickly drop to 50, 32, then finally 15 MB/min/core. This matches well with my earlier informal measurement on the cloud where I saw 15 MB/min/core on 128 cores. It looks like there is room for improvement.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/995#issuecomment-155619576:2,throughput,throughput,2,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/995#issuecomment-155619576,3,['throughput'],['throughput']
Performance,"![weighted](https://user-images.githubusercontent.com/11076296/97032266-8ab9a300-152f-11eb-8d73-148ff99963be.png). Here is the result of optimizing for sensitivity in the high-confidence, low-compexity region of chr22 in CHM, allowing haplotype-to-reference and read-to-haplotype (match, mismatch, gap open) to range over ([1, 20], [-20, -1], [-20, -1]) and fixing gap extend penalties to -1. The optimal (match, mismatch, gap open) parameters found in this run appear to be:. haplotype-to-reference: 2, -8, -19; read-to-haplotype: 1, -4, -3. I wouldn't put much stock in interpreting these parameters or their exact values for now, but it does appear that the match values and the haplotype-to-reference gap-open penalty might be saturating the bounds of the search. Plots of the type suggested by @dalessioluca might be more illuminating. Compare with default performance:. ````; Threshold True-pos-baseline True-pos-call False-pos False-neg Precision Sensitivity F-measure; ----------------------------------------------------------------------------------------------------; 9.000 4003 4019 494 1036 0.8905 0.7944 0.8397; None 4009 4025 511 1030 0.8873 0.7956 0.8390; ````. That the corresponding curve with a precision/sensitivity endpoint of (0.8873, 0.7956) above isn't at the top of the pack means that we could squeeze out some extra calls by varying the SW parameters. Of course, this doesn't account for negative impact elsewhere. One could imagine writing a loss where this sensitivity is optimized while putting minimum constraints on precision, sensitivity, and/or F1 in the high-confidence, high-complexity regions (the assumption being the truth set is complete in those regions), or some weightings/variations thereof. EDIT: Actually, looks like overall performance in the high-confidence region improves:. ````; Threshold True-pos-baseline True-pos-call False-pos False-neg Precision Sensitivity F-measure; ----------------------------------------------------------------------------",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-715465692:137,optimiz,optimizing,137,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-715465692,2,"['optimiz', 'perform']","['optimizing', 'performance']"
Performance,"# Steps to reproduce; Try to do a PostprocessGermlineCNVCalls with not all the autosomal chromosomes. #### Command. ```/home/tintest/miniconda2/bin/java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/tintest/miniconda2/share/gatk4-4.0.5.1-0/gatk-package-4.0.5.1-local.jar PostprocessGermlineCNVCalls -L Refseq_GrCh38_1-3-9-17-Y-M.bed -R hg38_1-3-9-17-Y-M.fasta --calls-shard-path GermlineCNVCaller/GermlineCNVCaller-calls/ --contig-ploidy-calls DetermineGermlineContigPloidy/DetermineGermlineContigPloidy-calls/ --model-shard-path GermlineCNVCaller/GermlineCNVCaller-model --sample-index 274 --autosomal-ref-copy-number 2 --allosomal-contig Y --output-genotyped-intervals intervals/SAMPLE_274_PostprocessGermlineCNVCalls_interval.vcf --output-genotyped-segments segments/SAMPLE_274_PostprocessGermlineCNVCalls_segments.vcf```. #### Output; ```14:21:35.446 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/tintest/miniconda2/share/gatk4-4.0.5.1-0/gatk-package-4.0.5.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 14:21:35.534 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------; 14:21:35.536 INFO PostprocessGermlineCNVCalls - The Genome Analysis Toolkit (GATK) v4.0.5.1; 14:21:35.537 INFO PostprocessGermlineCNVCalls - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:21:35.538 INFO PostprocessGermlineCNVCalls - Executing as tintest@dahu63 on Linux v4.9.0-6-amd64 amd64; 14:21:35.539 INFO PostprocessGermlineCNVCalls - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_121-b15; 14:21:35.541 INFO PostprocessGermlineCNVCalls - Start Date/Time: August 1, 2018 2:21:35 PM CEST; 14:21:35.542 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------; 14:21:35.543 INFO PostprocessGermlineCNVCalls - ------------------------------------",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5053#issuecomment-409558231:1968,Load,Loading,1968,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5053#issuecomment-409558231,1,['Load'],['Loading']
Performance,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2811?src=pr&el=h1) Report; > Merging [#2811](https://codecov.io/gh/broadinstitute/gatk/pull/2811?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/56e6baa79b4e56ebee5fb8d2b2288373a4269fa8?src=pr&el=desc) will **increase** coverage by `0.022%`.; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #2811 +/- ##; =============================================; + Coverage 79.978% 80% +0.022% ; - Complexity 16726 16795 +69 ; =============================================; Files 1139 1139 ; Lines 60894 61155 +261 ; Branches 9436 9497 +61 ; =============================================; + Hits 48702 48924 +222 ; - Misses 8396 8422 +26 ; - Partials 3796 3809 +13; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2811?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...egmentation/PerformAlleleFractionSegmentation.java](https://codecov.io/gh/broadinstitute/gatk/pull/2811?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9zZWdtZW50YXRpb24vUGVyZm9ybUFsbGVsZUZyYWN0aW9uU2VnbWVudGF0aW9uLmphdmE=) | `88.889% <ø> (ø)` | `2 <0> (ø)` | :arrow_down: |; | [...ender/tools/spark/pipelines/BQSRPipelineSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/2811?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvQlFTUlBpcGVsaW5lU3BhcmsuamF2YQ==) | `100% <0%> (ø)` | `15% <0%> (+7%)` | :arrow_up: |; | [...nstitute/hellbender/utils/help/GATKHelpDoclet.java](https://codecov.io/gh/broadinstitute/gatk/pull/2811?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9oZWxwL0dBVEtIZWxwRG9jbGV0LmphdmE=) | `100% <0%> (ø)` | `9% <0%> (+3%)` | :arrow_up: |; | [...stitute/hellbender/cmdline/CommandLineProgram.java](https://codecov.io/gh/broadinstitute/gatk/pull/2811?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGU,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2811#issuecomment-306008892:929,Perform,PerformAlleleFractionSegmentation,929,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2811#issuecomment-306008892,1,['Perform'],['PerformAlleleFractionSegmentation']
Performance,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/3036?src=pr&el=h1) Report; > Merging [#3036](https://codecov.io/gh/broadinstitute/gatk/pull/3036?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/84fbda69bf9528059777496a415be8eb6db63e61?src=pr&el=desc) will **increase** coverage by `0.331%`.; > The diff coverage is `88.554%`. ```diff; @@ Coverage Diff @@; ## master #3036 +/- ##; ===============================================; + Coverage 79.973% 80.304% +0.331% ; - Complexity 16727 17771 +1044 ; ===============================================; Files 1139 1152 +13 ; Lines 60902 65165 +4263 ; Branches 9437 10284 +847 ; ===============================================; + Hits 48705 52330 +3625 ; - Misses 8401 8900 +499 ; - Partials 3796 3935 +139; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/3036?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...ome/segmentation/PerformCopyRatioSegmentation.java](https://codecov.io/gh/broadinstitute/gatk/pull/3036?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9zZWdtZW50YXRpb24vUGVyZm9ybUNvcHlSYXRpb1NlZ21lbnRhdGlvbi5qYXZh) | `86.667% <ø> (+6.667%)` | `4 <0> (+2)` | :arrow_up: |; | [...institute/hellbender/tools/exome/ACNVModeller.java](https://codecov.io/gh/broadinstitute/gatk/pull/3036?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9BQ05WTW9kZWxsZXIuamF2YQ==) | `97.143% <ø> (-0.079%)` | `17 <0> (ø)` | |; | [...ellbender/tools/exome/copyratio/CopyRatioData.java](https://codecov.io/gh/broadinstitute/gatk/pull/3036?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9jb3B5cmF0aW8vQ29weVJhdGlvRGF0YS5qYXZh) | `95.349% <0%> (-2.27%)` | `14 <1> (+1)` | |; | [...nder/tools/exome/segmentation/AFCRHiddenState.java](https://codecov.io/gh/broadinstitute/gatk/pull/3036?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3036#issuecomment-306513201:960,Perform,PerformCopyRatioSegmentation,960,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3036#issuecomment-306513201,1,['Perform'],['PerformCopyRatioSegmentation']
Performance,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/3157?src=pr&el=h1) Report; > Merging [#3157](https://codecov.io/gh/broadinstitute/gatk/pull/3157?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/eab8761cbdfdaf24a5bf7551172b9f262d26d8cf?src=pr&el=desc) will **not change** coverage.; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #3157 +/- ##; ===========================================; Coverage 80.132% 80.132% ; Complexity 16993 16993 ; ===========================================; Files 1145 1145 ; Lines 61641 61641 ; Branches 9606 9606 ; ===========================================; Hits 49394 49394 ; Misses 8419 8419 ; Partials 3828 3828; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/3157?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...te/hellbender/tools/exome/PerformSegmentation.java](https://codecov.io/gh/broadinstitute/gatk/pull/3157?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9QZXJmb3JtU2VnbWVudGF0aW9uLmphdmE=) | `100% <ø> (ø)` | `3 <0> (ø)` | :arrow_down: |; | [...bender/tools/exome/NormalizeSomaticReadCounts.java](https://codecov.io/gh/broadinstitute/gatk/pull/3157?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9Ob3JtYWxpemVTb21hdGljUmVhZENvdW50cy5qYXZh) | `79.167% <ø> (ø)` | `6 <0> (ø)` | :arrow_down: |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3157#issuecomment-311469829:887,Perform,PerformSegmentation,887,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3157#issuecomment-311469829,1,['Perform'],['PerformSegmentation']
Performance,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/3183?src=pr&el=h1) Report; > Merging [#3183](https://codecov.io/gh/broadinstitute/gatk/pull/3183?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/64eba53c96ea739638d34222f0f2c61c39153a64?src=pr&el=desc) will **increase** coverage by `0.05%`.; > The diff coverage is `89.931%`. ```diff; @@ Coverage Diff @@; ## master #3183 +/- ##; ==============================================; + Coverage 80.415% 80.465% +0.05% ; - Complexity 17294 17368 +74 ; ==============================================; Files 1165 1165 ; Lines 62573 62785 +212 ; Branches 9763 9789 +26 ; ==============================================; + Hits 50318 50520 +202 ; - Misses 8350 8353 +3 ; - Partials 3905 3912 +7; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/3183?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...gemodel/cachemanager/ComputableGraphStructure.java](https://codecov.io/gh/broadinstitute/gatk/pull/3183?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9Db21wdXRhYmxlR3JhcGhTdHJ1Y3R1cmUuamF2YQ==) | `100% <ø> (ø)` | `63 <0> (ø)` | :arrow_down: |; | [...nder/cmdline/ExomeStandardArgumentDefinitions.java](https://codecov.io/gh/broadinstitute/gatk/pull/3183?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL0V4b21lU3RhbmRhcmRBcmd1bWVudERlZmluaXRpb25zLmphdmE=) | `0% <ø> (ø)` | `0 <0> (ø)` | :arrow_down: |; | [...der/tools/coveragemodel/nd4jutils/Nd4jIOUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/3183?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL25kNGp1dGlscy9OZDRqSU9VdGlscy5qYXZh) | `81.731% <ø> (ø)` | `19 <0> (ø)` | :arrow_down: |; | [...bender/tools/exome/TargetAnnotationCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/3183?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmc,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3183#issuecomment-314620643:932,cache,cachemanager,932,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3183#issuecomment-314620643,1,['cache'],['cachemanager']
Performance,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/3515?src=pr&el=h1) Report; > Merging [#3515](https://codecov.io/gh/broadinstitute/gatk/pull/3515?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/a218b6bbf6b3f243bce34a30f2458308319aadf3?src=pr&el=desc) will **increase** coverage by `0.012%`.; > The diff coverage is `84.946%`. ```diff; @@ Coverage Diff @@; ## master #3515 +/- ##; ===============================================; + Coverage 79.905% 79.917% +0.012% ; - Complexity 17918 17945 +27 ; ===============================================; Files 1199 1200 +1 ; Lines 65102 65195 +93 ; Branches 10142 10160 +18 ; ===============================================; + Hits 52020 52102 +82 ; - Misses 9042 9049 +7 ; - Partials 4040 4044 +4; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/3515?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...umber/utils/optimization/PersistenceOptimizer.java](https://codecov.io/gh/broadinstitute/gatk/pull/3515?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL3V0aWxzL29wdGltaXphdGlvbi9QZXJzaXN0ZW5jZU9wdGltaXplci5qYXZh) | `84.946% <84.946%> (ø)` | `27 <27> (?)` | |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/3515?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `78.571% <0%> (+0.649%)` | `39% <0%> (ø)` | :arrow_down: |; | [...e/hellbender/engine/spark/SparkContextFactory.java](https://codecov.io/gh/broadinstitute/gatk/pull/3515?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvU3BhcmtDb250ZXh0RmFjdG9yeS5qYXZh) | `73.973% <0%> (+2.74%)` | `11% <0%> (ø)` | :arrow_down: |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3515#issuecomment-325021094:944,optimiz,optimization,944,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3515#issuecomment-325021094,1,['optimiz'],['optimization']
Performance,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/3590?src=pr&el=h1) Report; > Merging [#3590](https://codecov.io/gh/broadinstitute/gatk/pull/3590?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/58108d0f3f1a760884201a62469105bc55c09a29?src=pr&el=desc) will **increase** coverage by `0.358%`.; > The diff coverage is `93.939%`. ```diff; @@ Coverage Diff @@; ## master #3590 +/- ##; ===============================================; + Coverage 79.736% 80.094% +0.358% ; - Complexity 18148 18799 +651 ; ===============================================; Files 1217 1226 +9 ; Lines 66602 69015 +2413 ; Branches 10429 11073 +644 ; ===============================================; + Hits 53106 55277 +2171 ; - Misses 9289 9415 +126 ; - Partials 4207 4323 +116; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/3590?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...umber/utils/optimization/PersistenceOptimizer.java](https://codecov.io/gh/broadinstitute/gatk/pull/3590?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL3V0aWxzL29wdGltaXphdGlvbi9QZXJzaXN0ZW5jZU9wdGltaXplci5qYXZh) | `84.946% <ø> (ø)` | `27 <0> (ø)` | :arrow_down: |; | [...copynumber/utils/segmentation/KernelSegmenter.java](https://codecov.io/gh/broadinstitute/gatk/pull/3590?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL3V0aWxzL3NlZ21lbnRhdGlvbi9LZXJuZWxTZWdtZW50ZXIuamF2YQ==) | `93.939% <93.939%> (ø)` | `44 <44> (?)` | |; | [.../tools/spark/sv/evidence/QNamesForKmersFinder.java](https://codecov.io/gh/broadinstitute/gatk/pull/3590?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9ldmlkZW5jZS9RTmFtZXNGb3JLbWVyc0ZpbmRlci5qYXZh) | `83.333% <0%> (-16.667%)` | `7% <0%> (ø)` | |; | [...nder/tools/spark/pathseq/PSPathogenTaxonScore.java](https://codecov.io/gh/broadinstitute/gatk/pull/3590?src=pr&el=,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3590#issuecomment-330916077:954,optimiz,optimization,954,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3590#issuecomment-330916077,1,['optimiz'],['optimization']
Performance,"$Proxy2.stop(Unknown Source); 	at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:132); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:182); 	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:164); 	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:412); 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64); 	at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 5.0 failed 1 times, most recent failure: Lost task 1.0 in stage 5.0 (TID 12, localhost, executor driver): java.util.ConcurrentModificationException; 	at java.util.ArrayList.sort(ArrayList.java:1464); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.<init>(ReadThreadingAssembler.java:81); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCal",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429:7330,concurren,concurrent,7330,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429,1,['concurren'],['concurrent']
Performance,$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.utils.spark.JoinReadsWithVariants.lambda$join$60e5b476$1(JoinReadsWithVariants.java:44); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$7$1.apply(JavaRDDLike.scala:186); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$7$1.apply(JavaRDDLike.scala:186); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5979#issuecomment-498620174:3959,concurren,concurrent,3959,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5979#issuecomment-498620174,2,['concurren'],['concurrent']
Performance,% +0.051% ; - Complexity 38899 38963 +64 ; ===============================================; Files 2336 2336 ; Lines 182709 182730 +21 ; Branches 20060 20066 +6 ; ===============================================; + Hits 158213 158325 +112 ; + Misses 17441 17365 -76 ; + Partials 7055 7040 -15 ; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/8074?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) | Coverage Δ | |; |---|---|---|; | [.../scalable/data/LabeledVariantAnnotationsDatum.java](https://codecov.io/gh/broadinstitute/gatk/pull/8074/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvZGF0YS9MYWJlbGVkVmFyaWFudEFubm90YXRpb25zRGF0dW0uamF2YQ==) | `68.421% <45.455%> (-3.801%)` | :arrow_down: |; | [...vqsr/scalable/LabeledVariantAnnotationsWalker.java](https://codecov.io/gh/broadinstitute/gatk/pull/8074/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvTGFiZWxlZFZhcmlhbnRBbm5vdGF0aW9uc1dhbGtlci5qYXZh) | `86.822% <46.154%> (+0.208%)` | :arrow_up: |; | [...rs/vqsr/scalable/TrainVariantAnnotationsModel.java](https://codecov.io/gh/broadinstitute/gatk/pull/8074/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvVHJhaW5WYXJpYW50QW5ub3RhdGlvbnNNb2RlbC5qYXZh) | `77.778% <66.667%> (-2.991%)` | :arrow_down: |; | [...lkers/vqsr/scalable/ExtractVariantAnnotations.java](https://codecov.io/gh/broadinstitute/gatk/pull/8074/diff?src=pr&el=tree&utm_medium=referral&utm_sourc,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8074#issuecomment-1294055323:1878,scalab,scalable,1878,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8074#issuecomment-1294055323,1,['scalab'],['scalable']
Performance,"(1) I am studying GMS mappability scores. To the best of my knowledge, it is the only such analysis that considers both paired-end reads and base calling error rate characteristic of Illumina machines. We could feed the GMS score as a feature file to the coverage collector tool for filtering. (2) I am also working on the ""optimal strategy"" for different SV types. (3) @samuelklee, do we get the same wavy pattern in other samples in the same region? in other words, it is sample-specific or region-specific?. (4) While fragment-based GC correction is difficult (and probably unnecessary) to perform without keeping a full index of aligned reads (like GS), it might be worthwhile to at least collect per-sample per-interval fragment-based average GC content (perhaps along with other summaries such as average fragment length, MQ, etc). It is easy to show that that the difference between full fragment-based GC correction and correction only using the observed average fragment GC content for the pile-up is of the order of the curvature of the GC curve, which is presumably small. We could collect these statistics either on-the-go during coverage collection, or from the sparse counts table as you suggested before (most sensible approach, once we figure out a way to represent sparse tensors).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4519#issuecomment-372413152:593,perform,perform,593,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4519#issuecomment-372413152,1,['perform'],['perform']
Performance,"(90 and counting, ~1% of jobs) which seems to match with the above numbers:. `htsjdk.tribble.TribbleException$MalformedFeatureFile: Unable to parse header with error: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: Connection has been shutdown: javax.net.ssl.SSLException: java.net.SocketException: Connection reset, for input source: gs://fc-4c1c7765-2de2-4214-ac41-dc10bbcbb55b/1e300bb3-6990-4342-8959-118826efb3dd/PairedEndSingleSampleWorkflow/3b32519a-f910-49a6-a5fc-b7ec9700d281/call-GatherVCFs/S153-2.g.vcf.gz; 	at htsjdk.tribble.TabixFeatureReader.readHeader(TabixFeatureReader.java:102); 	at htsjdk.tribble.TabixFeatureReader.<init>(TabixFeatureReader.java:86); 	at htsjdk.tribble.AbstractFeatureReader.getFeatureReader(AbstractFeatureReader.java:106); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getReaderFromVCFUri(GenomicsDBImport.java:437); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.loadHeaderFromVCFUri(GenomicsDBImport.java:252); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.initializeHeaderAndSampleMappings(GenomicsDBImport.java:223); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.onStartup(GenomicsDBImport.java:202); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:114); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:220); Caused by: java.lang.RuntimeException: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: Connection has been shutdown: javax.net.ssl.SSLException: java.net.SocketException: Conne",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-301610931:1014,load,loadHeaderFromVCFUri,1014,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-301610931,1,['load'],['loadHeaderFromVCFUri']
Performance,(DefaultTaskExecutionGraph.java:355); at org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$InvokeNodeExecutorsAction.execute(DefaultTaskExecutionGraph.java:343); at org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$BuildOperationAwareExecutionAction.execute(DefaultTaskExecutionGraph.java:336); at org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$BuildOperationAwareExecutionAction.execute(DefaultTaskExecutionGraph.java:322); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker$1.execute(DefaultPlanExecutor.java:134); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker$1.execute(DefaultPlanExecutor.java:129); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker.execute(DefaultPlanExecutor.java:202); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker.executeNextNode(DefaultPlanExecutor.java:193); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker.run(DefaultPlanExecutor.java:129); at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64); at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48); at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56); Caused by: org.gradle.api.GradleException: Javadoc generation failed. Generated Javadoc options file (useful for troubleshooting): '/home/cb2/gatk/build/tmp/gatkDoc/javadoc.options'; at org.gradle.api.tasks.javadoc.internal.JavadocGenerator.execute(JavadocGenerator.java:58); at org.gradle.api.tasks.javadoc.internal.JavadocGenerator.execute(JavadocGenerator.java:31); at org.gradle.api.tasks.javadoc.Javadoc.executeExternalJavadoc(Javadoc.java:158); at org.gradle.api.tasks.javadoc.Javadoc.generate(Javadoc.java:146); at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at java.base/jdk.intern,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4155#issuecomment-566796716:5748,concurren,concurrent,5748,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4155#issuecomment-566796716,1,['concurren'],['concurrent']
Performance,(DefaultTaskExecutionGraph.java:355); at org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$InvokeNodeExecutorsAction.execute(DefaultTaskExecutionGraph.java:343); at org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$BuildOperationAwareExecutionAction.execute(DefaultTaskExecutionGraph.java:336); at org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$BuildOperationAwareExecutionAction.execute(DefaultTaskExecutionGraph.java:322); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker$1.execute(DefaultPlanExecutor.java:134); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker$1.execute(DefaultPlanExecutor.java:129); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker.execute(DefaultPlanExecutor.java:202); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker.executeNextNode(DefaultPlanExecutor.java:193); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker.run(DefaultPlanExecutor.java:129); at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64); at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48); at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56); Caused by: org.gradle.api.GradleException: Javadoc generation failed. Generated Javadoc options file (useful for troubleshooting): '/usr/bin/gatk/build/tmp/gatkDoc/javadoc.options'; at org.gradle.api.tasks.javadoc.internal.JavadocGenerator.execute(JavadocGenerator.java:58); at org.gradle.api.tasks.javadoc.internal.JavadocGenerator.execute(JavadocGenerator.java:31); at org.gradle.api.tasks.javadoc.Javadoc.executeExternalJavadoc(Javadoc.java:158); at org.gradle.api.tasks.javadoc.Javadoc.generate(Javadoc.java:146); at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at java.base/jdk.interna,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6466#issuecomment-590387973:4622,concurren,concurrent,4622,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6466#issuecomment-590387973,1,['concurren'],['concurrent']
Performance,(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); 17:43:23.161 INFO FeatureManager - Using codec VCFCodec to read file file:///scratch/tmp/spark-ecd63991-68be-4879-b481-68e6789a2004/userFiles-b72d4821-5e36-4d36-aa79-aa6263768669/1000G_phase1.indels.hg19.sites.vcf; 20/01/05 17:43:23 INFO NewHadoopRDD: Input split: file:/panfs/roc/groups/6/clinicalmdl/shared/wgs_exome_v1.0/projects/BT_WGS_Flex_S1/data/exome_dedup_reads.bam:167436615680+33554432; 20/01/05 17:43:23 ERROR Executor: Exception in task 4990.0 in stage 0.0 (TID 4990); java.io.FileNotFoundException: /panfs/roc/groups/6/clinicalmdl/shared/v1.0/projects/BT_WGS_Flex_S1/data/exome_dedup_reads.bam (Too many open files); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.<init>(RawLocalFileSystem.j,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5316#issuecomment-570992855:4867,concurren,concurrent,4867,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5316#issuecomment-570992855,1,['concurren'],['concurrent']
Performance,"(also, rebase to pick up the fix for the race condition you see)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1701#issuecomment-211044105:41,race condition,race condition,41,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1701#issuecomment-211044105,1,['race condition'],['race condition']
Performance,") get rid of the warnings about missing .so files. As an aside, I'm curious whether PowerPC architecture has an instruction; set similar to AVX. This is something I might actually be able to; contribute to the project so I'm excited by the prospect!. -Dan. On Fri, Sep 4, 2020, 11:53 AM R-obert <notifications@github.com> wrote:. > Hello,; >; > I'm trying to use GATK4 (4.1.8.1) on an Ubuntu (16.04) machine. The; > machine is a ""PowerLinux"" machine and I'm guessing that the most relevant; > info for the following problem is that it is a ppc64le system. When I use; > HaplotypeCaller, I see the following messages on the screen:; >; > Running:; > java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx50G -jar /home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar HaplotypeCaller -R ref.fa -I mybam.bam -O mycalls.vcf.gz -L snps.vcf -ip 100; >; > 16:17:04.377 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; >; > 16:17:04.397 **WARN** NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (/tmp/libgkl_compression3825249225068031371.so: /tmp/libgkl_compression3825249225068031371.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > 16:17:04.402 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; >; > 16:17:04.407 **WARN** NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (/tmp/libgkl_compression7506152962158874866.so: /tmp/libgkl_compression7506152962158874866.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-b",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600:1427,Load,Loading,1427,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600,1,['Load'],['Loading']
Performance,"* The performance should be fine - TileDB/GenomicsDB stores each field in a separate file (columnar storage) and so adding MIN_DP file to the list of queried fields (~5-10 INFO fields) should be fine.; * One possible source of performance improvement - I was querying the PL field in the sites only query (not producing it in the output VariantContext objects). I think it can be dropped from the query. I was assuming that the PL field would be needed to correctly handle spanning deletions (spanning deletion corresponds to deletion allele with min PL). However, for spanning deletions, all INFO fields are dropped. Hence, any INFO fields that depend on the allele order (allele specific annotations) would be dropped for the spanning deletion. Hence, the exact deletion allele corresponding to the spanning deletion is irrelevant making it possible to drop the PL field from the query as well. Is that correct?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3688#issuecomment-377305568:6,perform,performance,6,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3688#issuecomment-377305568,2,['perform'],['performance']
Performance,"* _Large number of open file handles_: this was an issue in TileDB which got fixed as part of the restructuring that @nalinigans did for supporting HDFS/S3/GCS (#5017). I was too lazy to fix this again. If it's going to take some time for PR #5017 to be merged, I can submit a separate fix for this. This would fix any crashes/termination issues.; * _Performance of a single import process with a large number of intervals_; * Restating the obvious, but this is a single process (and by default, a single thread) with many intervals to import. As you increase the number of samples, this will become a performance pain point.; * More important than the number of intervals is the amount of data imported per interval. Each interval import involves opening the VCF files (loading index structures while creating FeatureReader objects), writing to TileDB/GenomicsDB. and closing the VCF file handles (destroying FeatureReader objects). If the amount of data written for each interval is sufficiently large, the cost of opening/closing the VCF files (creating/destroying FeatureReaders) is small relative to the total time taken.; * In the test cases I and Chris were trying, the amount of data written per interval was small (or 0 in many cases). The time taken in opening/closing the VCF files (and loading/destroying the index) dominates the total time.; * For a single import process (single thread), creating a large interval is better (or no worse) than passing several small intervals. TileDB/GenomicsDB has 0 overhead for regions with no data (for example, WES gVCFs). Having larger intervals will likely avoid issues described above. Hence, an advisory message will be beneficial.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5066#issuecomment-410576757:602,perform,performance,602,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5066#issuecomment-410576757,3,"['load', 'perform']","['loading', 'performance']"
Performance,"************************************************. A USER ERROR has occurred: Couldn't create GenomicsDBFeatureReader. ***********************************************************************; Set the system property GATK_STACKTRACE_ON_USER_EXCEPTION (--java-options '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true') to print the stack trace.; [ccastane9@andersserver-01 GenomicsDB]$ bash *_genotype.3.sh; Using GATK jar /data1/_software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Xmx16g -jar /data1/_software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar GenotypeGVCFs --genomicsdb-shared-posixfs-optimizations --reference /data1/EquCab/_ECA30/Equus_caballus.EquCab3.0.dna_sm.toplevel.fa/ -V gendb://ECA3_GenomicsDB_260/3 -O ECA3_GenomicsDB_260.3.g.vcf.gz; 16:27:53.573 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data1/_software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 06, 2021 4:27:54 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 16:27:54.132 INFO GenotypeGVCFs - ------------------------------------------------------------; 16:27:54.133 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.1.8.1; 16:27:54.133 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 16:27:54.143 INFO GenotypeGVCFs - Executing as ccastane9@andersserver-01.cvm.tamu.edu on Linux v3.10.0-1127.19.1.el7.x86_64 amd64; 16:27:54.143 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_275-b01; 16:27:54.144 INFO GenotypeGVCFs - Start Date/Time: January 6, 2021 4:27:53 PM CST; 16:27:54.144 INFO GenotypeGVCFs - -------------------------------------",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-755760402:4325,Load,Loading,4325,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-755760402,1,['Load'],['Loading']
Performance,"**The following work has been done:**; - We performed a round of evaluations against XHMM and cn.MOPS on a cohort of 160 samples from SFARI project (which is described in our ASHG poster). For ground truth we used a callset generated from Talkowski lab SV pipeline on matched whole genome samples. Unfortunately, SFARI cohort is not public and cannot be used for public facing evaluations.; - Some hyperparameter tweaking was necessary to achieve good performance. Hyperparameters changed were contained mostly only to `psi_t` parameter.; - We developed a clustering procedure that is based on coverage profile at the set of targets that are highly variable across different capture kits. ; - We found that filtering on a QS metric on a final callset significantly boosted the specificity while lowering sensitivity insignificantly.; - We developed a hyperparameter optimization framework prototype that could be used in a future for general optimizations of cost/performance parameters for all GATK pipelines.; - We resolved several memory issues that came up during validations. **A few issues were encountered along the way:**; - The sensitivity and specificity on multiallellic (common) sites was significantly lower than on rare events.; - Single target calling sensitivity was lower than 20%.; - Pipeline WDL required optimization in order to handle whole genome data, however these changes were not consolidated in the official WDL. **Currently the ongoing work is focused on the following:**; - Improving sensitivity/specificity of calls on common regions. One solution being tested involves setting a prior for common regions derived from a high quality callset. Second solution is to set a different filtering threshold for common regions.; - Consolidating validation scripts to process gCNV output and outputs of competing tools measure their performances against ground truth.; - Analyzing 1000 Genomes exomes, which could be potentially used for public facing automatic evaluations. **The",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4123#issuecomment-532500502:44,perform,performed,44,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4123#issuecomment-532500502,5,"['optimiz', 'perform']","['optimization', 'optimizations', 'performance', 'performed']"
Performance,+32 ; Lines 146768 147415 +647 ; Branches 16223 16225 +2 ; ================================================; - Hits 127666 55100 -72566 ; - Misses 13189 87388 +74199 ; + Partials 5913 4927 -986; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/5732?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...s/copynumber/models/AlleleFractionInitializer.java](https://codecov.io/gh/broadinstitute/gatk/pull/5732/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL21vZGVscy9BbGxlbGVGcmFjdGlvbkluaXRpYWxpemVyLmphdmE=) | `89.063% <ø> (ø)` | `17 <0> (ø)` | :arrow_down: |; | [...r/tools/copynumber/models/AlleleFractionState.java](https://codecov.io/gh/broadinstitute/gatk/pull/5732/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL21vZGVscy9BbGxlbGVGcmFjdGlvblN0YXRlLmphdmE=) | `100% <ø> (ø)` | `7 <0> (ø)` | :arrow_down: |; | [...umber/utils/optimization/PersistenceOptimizer.java](https://codecov.io/gh/broadinstitute/gatk/pull/5732/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL3V0aWxzL29wdGltaXphdGlvbi9QZXJzaXN0ZW5jZU9wdGltaXplci5qYXZh) | `77.419% <ø> (-10.753%)` | `24 <0> (-4)` | |; | [...bender/tools/copynumber/models/CopyRatioState.java](https://codecov.io/gh/broadinstitute/gatk/pull/5732/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL21vZGVscy9Db3B5UmF0aW9TdGF0ZS5qYXZh) | `100% <ø> (ø)` | `5 <0> (ø)` | :arrow_down: |; | [...copynumber/utils/segmentation/KernelSegmenter.java](https://codecov.io/gh/broadinstitute/gatk/pull/5732/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL3V0aWxzL3NlZ21lbnRhdGlvbi9LZXJuZWxTZWdtZW50ZXIuamF2YQ==) | `95.671% <ø> (-2.164%)` | `45 <0> (-3)` | |; | [...s/copynumber/models/AlleleFractionLikelihoods.java](https://codecov.io/gh/br,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5732#issuecomment-470293496:1595,optimiz,optimization,1595,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5732#issuecomment-470293496,1,['optimiz'],['optimization']
Performance,+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9OYXR1cmFsTG9nVXRpbHMuamF2YQ==) | `77.143% <0.000%> (ø)` | |; | [...ls/clustering/BayesianGaussianMixtureModeller.java](https://codecov.io/gh/broadinstitute/gatk/pull/7954/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9jbHVzdGVyaW5nL0JheWVzaWFuR2F1c3NpYW5NaXh0dXJlTW9kZWxsZXIuamF2YQ==) | `0.000% <0.000%> (ø)` | |; | [.../tools/walkers/vqsr/scalable/data/VariantType.java](https://codecov.io/gh/broadinstitute/gatk/pull/7954/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvZGF0YS9WYXJpYW50VHlwZS5qYXZh) | `60.000% <60.000%> (ø)` | |; | [.../walkers/vqsr/scalable/SystemCommandUtilsTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/7954/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvU3lzdGVtQ29tbWFuZFV0aWxzVGVzdC5qYXZh) | `60.870% <60.870%> (ø)` | |; | [.../scalable/data/LabeledVariantAnnotationsDatum.java](https://codecov.io/gh/broadinstitute/gatk/pull/7954/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvZGF0YS9MYWJlbGVkVmFyaWFudEFubm90YXRpb25zRGF0dW0uamF2YQ==) | `72.222% <72.222%> (ø)` | |; | ... and [20 more](https://codecov.io/gh/broadinstitute/gatk/pull/7954/diff?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadi,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7954#issuecomment-1191010834:4733,scalab,scalable,4733,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7954#issuecomment-1191010834,1,['scalab'],['scalable']
Performance,", et al. Nature. 2022 Jul;607(7920):732-740. doi: 10.1038/s41586-022-04965-x. Epub 2022 Jul 20.PMID: 35859178. On page 69+ of this pdf, they describe the problem and how they cleverly worked around it. ; ; https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-022-04965-x/MediaObjects/41586_2022_4965_MOESM1_ESM.pdf. _It should be noted that running GATK out of the box will cause every job to read the entire; gVCF index file (.tbi) for each of the 150,119 samples. The average size of the index files is ; 4.15MB, so each job would have to read 4.15*150,126 = 623GB of data on top of the actual; gVCF slice data. For 60,000 jobs, this would amount to 623GB*60,000 = 37PB or 25.2GB/sec; of additional read overhead if the jobs are run on 20,000 cores in 17 days. This read; overhead will definitely prevent 20,000 cores from being used simultaneously. However,; this problem was avoided by pre-processing the .tbi files and modifying the software; reading the gVCF files from the central storage in a similar fashion as we did for GraphTyper; and the CRAM index files (.crai)._. This explains why chr1 requires more memory than chr22 despite running on the same number of samples. The larger chr1 tbi index is the source of the memory problem. The Decode solution is too limit the reading of the tbi index to the part that indexes the scattered region. There is a long pause at the beginning of the running GenotypeGVCFs which I never understood. GATK must be the reading of all the sample's gvcfs tbi into memory during that pause. So the reblocking of the gvcfs above reduced the memory foot print by decreasing the tbi size. Decode reduced it by chopping up the index so for each scattered region, GATK could only read a small subset of the index needed for that region. The combination of reblocking and chopping up the tbi would help with the memory requirements even more. However, it is clear that GATK's present reading of the full tbi is not scalable given the memory requirements.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1374348579:2214,scalab,scalable,2214,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1374348579,1,['scalab'],['scalable']
Performance,"- Initializing engine; > 25 15:07:52.848 INFO FeatureManager - Using codec VCFCodec to read file file://ref_nobackup/af-only-gnomad.hg38.vcf.gz; > 26 15:07:53.126 INFO Mutect2 - Done initializing engine; > 27 15:07:53.196 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/scicore/soft/apps/GATK/4.4.0.0-GCCcore-10.3.0-Java-17/gatk-package-4.4.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; > 28 15:07:53.201 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/scicore/soft/apps/GATK/4.4.0.0-GCCcore-10.3.0-Java-17/gatk-package-4.4.0.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; > 29 15:07:53.223 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; > 30 15:07:53.223 INFO IntelPairHmm - Available threads: 2; > 31 15:07:53.224 INFO IntelPairHmm - Requested threads: 4; > 32 15:07:53.224 WARN IntelPairHmm - Using 2 available threads, but 4 were requested; > 33 15:07:53.224 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; > 34 15:07:53.231 WARN Mutect2 - Note that the Mutect2 reference confidence mode is in BETA -- the likelihoods model and output format are subject to change in subsequent versions.; > 35 15:07:53.314 INFO ProgressMeter - Starting traversal; > 36 15:07:53.314 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; > 37 15:07:54.410 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 1.8392900000000002E-4; > 38 15:07:54.412 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 0.03020143; > 39 15:07:54.412 INFO SmithWatermanAligner - Total compute time in java Smith-Waterman : 0.05 sec; > 40 15:07:54.413 INFO Mutect2 - Shutting down engine; > 41 [June 19, 2023 at 3:07:54 PM CEST] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.03 minutes.; > 42 Runtime.totalMemory()=285212672; > 43 java.lang.IndexOutOfBoundsException: Index -1 out of bounds for length 1; > 4",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7849#issuecomment-1597198632:3628,multi-thread,multi-threaded,3628,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7849#issuecomment-1597198632,1,['multi-thread'],['multi-threaded']
Performance,"- Shutdown hook called; 2019-01-09 13:35:56 INFO ShutdownHookManager:54 - Deleting directory /tmp/spark-69cc5c72-eff6-4259-8b3b-12fa6f8c42b0; 2019-01-09 13:35:56 INFO ShutdownHookManager:54 - Deleting directory /tmp/spark-0bd07e00-4f6d-43bd-b9d2-b1999376c72b; ```. Just to verify, the non-spark version still runs fine with the compressed fasta.... ```; gatk CountReads --input HG04302.alt_bwamem_GRCh38DH.20150718.GBR.low_coverage.cram --reference GRCh38_full_analysis_set_plus_decoy_hla.fa.gz; Using GATK jar /share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-local.jar CountReads --input HG04302.alt_bwamem_GRCh38DH.20150718.GBR.low_coverage.cram --reference GRCh38_full_analysis_set_plus_decoy_hla.fa.gz; 13:38:54.168 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 13:38:55.869 INFO CountReads - ------------------------------------------------------------; 13:38:55.870 INFO CountReads - The Genome Analysis Toolkit (GATK) v4.0.12.0; 13:38:55.870 INFO CountReads - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:38:55.871 INFO CountReads - Executing as farrell@scc-hadoop.bu.edu on Linux v2.6.32-754.6.3.el6.x86_64 amd64; 13:38:55.871 INFO CountReads - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 13:38:55.871 INFO CountReads - Start Date/Time: January 9, 2019 1:38:54 PM EST; 13:38:55.871 INFO CountReads - ------------------------------------------------------------; 13:38:55.871 INFO CountReads - ------------------------------------------------------------; 13:38:55.872 INFO CountReads - HTSJDK Version: 2.18.1; 13:38:55.873 INFO CountReads - ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:44003,Load,Loading,44003,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,1,['Load'],['Loading']
Performance,"-------------------------------------------; 16:26:35.422 INFO GenotypeGVCFs - HTSJDK Version: 2.23.0; 16:26:35.423 INFO GenotypeGVCFs - Picard Version: 2.22.8; 16:26:35.423 INFO GenotypeGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 16:26:35.423 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:26:35.426 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:26:35.426 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:26:35.427 INFO GenotypeGVCFs - Deflater: IntelDeflater; 16:26:35.427 INFO GenotypeGVCFs - Inflater: IntelInflater; 16:26:35.427 INFO GenotypeGVCFs - GCS max retries/reopens: 20; 16:26:35.427 INFO GenotypeGVCFs - Requester pays: disabled; 16:26:35.427 INFO GenotypeGVCFs - Initializing engine; 16:26:37.201 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.3.0-e701905; [TileDB::Buffer] Error: Cannot read from buffer; End of buffer reached.; [TileDB::BookKeeping] Error: Cannot load book-keeping; Reading MBR failed.; 16:26:39.459 INFO GenotypeGVCFs - Shutting down engine; [January 6, 2021 4:26:39 PM CST] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 0.08 minutes.; Runtime.totalMemory()=2303197184; ***********************************************************************. A USER ERROR has occurred: Couldn't create GenomicsDBFeatureReader. ***********************************************************************; Set the system property GATK_STACKTRACE_ON_USER_EXCEPTION (--java-options '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true') to print the stack trace.; [ccastane9@andersserver-01 GenomicsDB]$ bash *_genotype.3.sh; Using GATK jar /data1/_software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Xmx16g -jar /data1/_software/gatk-",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-755760402:3044,load,load,3044,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-755760402,1,['load'],['load']
Performance,"-------------------------------------------; 16:27:54.145 INFO GenotypeGVCFs - HTSJDK Version: 2.23.0; 16:27:54.145 INFO GenotypeGVCFs - Picard Version: 2.22.8; 16:27:54.145 INFO GenotypeGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 16:27:54.145 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:27:54.145 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:27:54.146 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:27:54.146 INFO GenotypeGVCFs - Deflater: IntelDeflater; 16:27:54.146 INFO GenotypeGVCFs - Inflater: IntelInflater; 16:27:54.146 INFO GenotypeGVCFs - GCS max retries/reopens: 20; 16:27:54.146 INFO GenotypeGVCFs - Requester pays: disabled; 16:27:54.146 INFO GenotypeGVCFs - Initializing engine; 16:27:55.873 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.3.0-e701905; [TileDB::Buffer] Error: Cannot read from buffer; End of buffer reached.; [TileDB::BookKeeping] Error: Cannot load book-keeping; Reading MBR failed.; 16:27:58.483 INFO GenotypeGVCFs - Shutting down engine; [January 6, 2021 4:27:58 PM CST] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 0.08 minutes.; Runtime.totalMemory()=2231894016; ***********************************************************************. A USER ERROR has occurred: Couldn't create GenomicsDBFeatureReader. ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException: Couldn't create GenomicsDBFeatureReader; 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getGenomicsDBFeatureReader(FeatureDataSource.java:410); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getFeatureReader(FeatureDataSource.java:326); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:282); 	at org.broadinstitute.hellbender.engine.VariantLocusWalker.initializeDrivingVariants(VariantLocusWalker.java:76); 	at",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-755760402:6399,load,load,6399,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-755760402,1,['load'],['load']
Performance,"-------------------------------------------; 21:16:35.498 INFO GenotypeGVCFs - HTSJDK Version: 2.23.0; 21:16:35.498 INFO GenotypeGVCFs - Picard Version: 2.22.8; 21:16:35.498 INFO GenotypeGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 21:16:35.498 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 21:16:35.498 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 21:16:35.498 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 21:16:35.498 INFO GenotypeGVCFs - Deflater: IntelDeflater; 21:16:35.499 INFO GenotypeGVCFs - Inflater: IntelInflater; 21:16:35.499 INFO GenotypeGVCFs - GCS max retries/reopens: 20; 21:16:35.499 INFO GenotypeGVCFs - Requester pays: disabled; 21:16:35.499 INFO GenotypeGVCFs - Initializing engine; 21:16:36.737 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.3.0-e701905; [TileDB::Buffer] Error: Cannot read from buffer; End of buffer reached.; [TileDB::BookKeeping] Error: Cannot load book-keeping; Reading MBR failed.; 21:16:38.472 INFO GenotypeGVCFs - Shutting down engine; [January 17, 2021 9:16:38 PM CST] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 0.06 minutes.; Runtime.totalMemory()=2551709696; ***********************************************************************. A USER ERROR has occurred: Couldn't create GenomicsDBFeatureReader. ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException: Couldn't create GenomicsDBFeatureReader; 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getGenomicsDBFeatureReader(FeatureDataSource.java:410); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getFeatureReader(FeatureDataSource.java:326); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:282); 	at org.broadinstitute.hellbender.engine.VariantLocusWalker.initializeDrivingVariants(VariantLocusWalker.java:76); 	a",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-761953839:4512,load,load,4512,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-761953839,1,['load'],['load']
Performance,---------------; 09:49:05.901 INFO Mutect2 - HTSJDK Version: 2.18.2; 09:49:05.901 INFO Mutect2 - Picard Version: 2.18.25; 09:49:05.902 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 09:49:05.902 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 09:49:05.902 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 09:49:05.902 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 09:49:05.902 INFO Mutect2 - Deflater: IntelDeflater; 09:49:05.902 INFO Mutect2 - Inflater: IntelInflater; 09:49:05.902 INFO Mutect2 - GCS max retries/reopens: 20; 09:49:05.902 INFO Mutect2 - Requester pays: disabled; 09:49:05.902 INFO Mutect2 - Initializing engine; 09:49:06.887 INFO Mutect2 - Done initializing engine; 09:49:06.935 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/Tools/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 09:49:06.937 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; 09:49:06.937 WARN PairHMM - ***WARNING: Machine does not have the AVX instruction set support needed for the accelerated AVX PairHmm. Falling back to the MUCH slower LOGLESS_CACHING implementation!; 09:49:07.007 INFO ProgressMeter - Starting traversal; 09:49:07.007 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 09:49:17.023 INFO ProgressMeter - 1:139173 0.2 480 2875.7; 09:49:27.704 INFO ProgressMeter - 1:763661 0.3 2590 7508.3; 09:49:38.001 INFO ProgressMeter - 1:958723 0.5 3290 6369.0; 09:49:49.182 INFO ProgressMeter - 1:981050 0.7 3380 4808.5; 09:50:02.383 INFO ProgressMeter - 1:988991 0.9 3440 3727.3; 09:50:13.586 INFO ProgressMeter - 1:1227096 1.1 4290 3866.1; 09:50:23.594 INFO ProgressMeter - 1:1460850 1.3 5240 4105.1; 09:50:34.165 INFO ProgressMeter - 1:1960541 1.5 7060 4860.1; 09:50:46.537 INFO ProgressMeter - 1:2489135 1.7 8930 5383.3; 09:50:56.541 INFO ProgressMeter - 1:3195743 1.8 11330 6206,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5543#issuecomment-470579844:1985,multi-thread,multi-threaded,1985,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5543#issuecomment-470579844,1,['multi-thread'],['multi-threaded']
Performance,"-42bf-ba5f-fde762934a59/Mutect2/fe3623c8-0eaf-4cd4-9f81-d1fda4073f2e/call-FilterAlignmentArtifacts/attempt-3/script -> /cromwell_root/script; 2020/07/25 01:38:45 Localization script execution complete.; 2020/07/25 01:38:58 Done localization.; 2020/07/25 01:38:59 Running user action: docker run -v /mnt/local-disk:/cromwell_root --entrypoint= us.gcr.io/broad-gatk/gatk@sha256:8051adab0ff725e7e9c2af5997680346f3c3799b2df3785dd51d4abdd3da747b /bin/bash /cromwell_root/script; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp.6c58e0ba; 01:39:02.909 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/gatk/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; 01:39:02.925 INFO NativeLibraryLoader - Loading libgkl_smithwaterman.so from jar:file:/gatk/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_smithwaterman.so; 01:39:02.927 INFO SmithWatermanAligner - Using AVX accelerated SmithWaterman implementation; 01:39:03.142 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 01:39:03.361 INFO FilterAlignmentArtifacts - ------------------------------------------------------------; 01:39:03.361 INFO FilterAlignmentArtifacts - The Genome Analysis Toolkit (GATK) v4.1.8.1; 01:39:03.361 INFO FilterAlignmentArtifacts - For support and documentation go to https://software.broadinstitute.org/gatk/; 01:39:03.362 INFO FilterAlignmentArtifacts - Executing as root@3f245e278eba on Linux v4.19.112+ amd64; 01:39:03.362 INFO FilterAlignmentArtifacts - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 01:39:03.362 INFO FilterAlignmentArtifacts - Start Date/Time: July 25, 2020 1:39:03 AM GMT; 01:39:03.362 INFO FilterAlignmentArtifacts - ------------------------------------------------------------; 01:39:03.362 INFO FilterAlignmentArtifacts - ------------------------------------------------------------; 01:39:03.363 INFO",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-664539860:2358,Load,Loading,2358,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-664539860,1,['Load'],['Loading']
Performance,"-65e6-451f-a04e-6a3f5e7fe5c8/call-CHMSampleHeadToHead/BenchmarkComparison/a332776f-175a-4595-bdeb-ab62e7f89921/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""CHM controlindelF1Score"": ""0.8724"",; ""CHM controlindelPrecision"": ""0.8814"",; ""CHM controlsnpF1Score"": ""0.9784"",; ""CHM controlsnpPrecision"": ""0.9706"",; ""CHM controlsnpRecall"": ""0.9863"",; ""CHM controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/0e5c32ab-65e6-451f-a04e-6a3f5e7fe5c8/call-CHMSampleHeadToHead/BenchmarkComparison/a332776f-175a-4595-bdeb-ab62e7f89921/call-BenchmarkVCFControlSample/Benchmark/06cbfab4-17a7-4415-9118-d0ebbe156bfd/call-CombineSummaries/summary.csv"",; ""CHM evalHCprocesshours"": ""84.26158888888888"",; ""CHM evalHCsystemhours"": ""0.19243055555555555"",; ""CHM evalHCwallclockhours"": ""60.242008333333345"",; ""CHM evalHCwallclockmax"": ""3.176513888888889"",; ""CHM evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/0e5c32ab-65e6-451f-a04e-6a3f5e7fe5c8/call-CHMSampleHeadToHead/BenchmarkComparison/a332776f-175a-4595-bdeb-ab62e7f89921/call-EVALRuntimeTask/cacheCopy/monitoring.pdf"",; ""CHM evalindelF1Score"": ""0.8724"",; ""CHM evalindelPrecision"": ""0.8814"",; ""CHM evalsnpF1Score"": ""0.9784"",; ""CHM evalsnpPrecision"": ""0.9706"",; ""CHM evalsnpRecall"": ""0.9863"",; ""CHM evalsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/0e5c32ab-65e6-451f-a04e-6a3f5e7fe5c8/call-CHMSampleHeadToHead/BenchmarkComparison/a332776f-175a-4595-bdeb-ab62e7f89921/call-BenchmarkVCFTestSample/Benchmark/362a3e75-6a39-4bde-bb79-e6562dc66dd9/call-CombineSummaries/summary.csv"",; ""EXOME1 controlindelF1Score"": ""0.727"",; ""EXOME1 controlindelPrecision"": ""0.632"",; ""EXOME1 controlsnpF1Score"": ""0.9878"",; ""EXOME1 controlsnpPrecision"": ""0.9815"",; ""EXOME1 controlsnpRecall"": ""0.9941"",; ""EXOME1 controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/0e5c32ab-65e6-451f-a04e-6a3f5e7fe5c8/call-EXOME1Sampl",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1535104202:18366,cache,cacheCopy,18366,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1535104202,1,['cache'],['cacheCopy']
Performance,"-coded values in master and doing the same via the exposed parameters in this branch have the same effect on a few existing test cases. However, while I'm doing the last three, I wonder if we could run whatever canonical evaluations/optimizations we have to see whether it's worth consolidating some of the parameter sets at this stage? I think there's an argument for having at least two sets (haplotype-to-reference + read-to-haplotype), but I'm not sure how to justify having a separate set for dangling heads/tails. But also not sure which set the latter should be consolidated with---@jamesemery thoughts? Again, let me reiterate that it seems that many of these parameter values were chosen arbitrarily (or, if not, that the procedure for choosing them has been lost). As a start, you can see the results of some optimizations I did on the CHM mix on slide 15 at https://docs.google.com/presentation/d/1zGuquAZWSUQ-wNxp8D6HhGNjIaFcV0_X9WAS4LODbEo/edit?usp=sharing Here, I optimized over haplotype-to-reference + read-to-haplotype SW parameters on various metrics after variant normalization using vcfeval. These optimizations were done using the Bayesian optimization framework I prototyped long ago (see https://github.com/broadinstitute/gatk-evaluation/tree/master/pipeline-optimizer and https://docs.google.com/presentation/d/1t5WOAEOMp0xAzJgpKbP68BUnclNYfIVRrDSL9wl1-3A/edit?usp=sharing); this entailed running parameter scans using a local Cromwell on my desktop. Probably this optimization work could be redone relatively easily using the Neptune framework put together by @dalessioluca, which was still in development at the time I did this work. Happy to share the resources and scripts I used if we go down this route; they are pretty lightweight. See more discussion starting here: https://github.com/broadinstitute/gatk/issues/5564#issuecomment-710107566. Alternatively, we could merge this branch to expose the parameters now and punt on consolidating/optimizing them. I'm not compl",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6885#issuecomment-891907471:1679,optimiz,optimized,1679,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6885#issuecomment-891907471,1,['optimiz'],['optimized']
Performance,"-g00a40ea-SNAPSHOT-spark.jar CountReadsSpark -I hdfs://arlab174:54310/GATK4TEST/BroadData/CEUTrio.HiSeq.WEx.b37.NA12892.bam -O hdfs://arlab174:54310/GATK4TEST/Output/Test_CEU_ReadsCount --sparkMaster yarn; 14:56:20.999 INFO IntelGKLUtils - Trying to load Intel GKL library from:; 	jar:file:/mnt/raid5/frankliu/code/GATK/gatk/build/libs/gatk-package-4.alpha.2-120-g00a40ea-SNAPSHOT-spark.jar!/com/intel/gkl/native/libgkl_compression.so; Exception in thread ""main"" java.lang.UnsatisfiedLinkError: /tmp/frankliu/libgkl_compression8271576113600851848.so: /tmp/frankliu/libgkl_compression8271576113600851848.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64-bit platform); 	at java.lang.ClassLoader$NativeLibrary.load(Native Method); 	at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941); 	at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824); 	at java.lang.Runtime.load0(Runtime.java:809); 	at java.lang.System.load(System.java:1086); 	at com.intel.gkl.IntelGKLUtils.load(IntelGKLUtils.java:133); 	at com.intel.gkl.compression.IntelDeflater.load(IntelDeflater.java:58); 	at com.intel.gkl.compression.IntelDeflater.load(IntelDeflater.java:53); 	at com.intel.gkl.compression.IntelDeflaterFactory.<init>(IntelDeflaterFactory.java:16); 	at com.intel.gkl.compression.IntelDeflaterFactory.<init>(IntelDeflaterFactory.java:20); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:143); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116); 	at org.broadinstitute.hellbender.Main.main(Main.java:158); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(Na",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2302#issuecomment-265854885:1865,load,load,1865,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2302#issuecomment-265854885,1,['load'],['load']
Performance,"-output-prefix {params.prefix} -imr OVERLAPPING_ONLY -O {output.ploidy_calls}`. When solved, {params.files} creates something like ""-I sample1.hdf5 -I sample2.hdf5"" etc... Involved **software versions**:; **gcnvkernel** = 0.7; **gatk** = 4.2.0.0; **Python** = 3.6.10. **Complete log**: . ```; Using GATK jar /software/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx8G -jar /software/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar DetermineGermlineContigPloidy -L results/cnv/targets.preprocessed.interval_list -I results/cnv/hdf5/MGM20-0848_S4.hdf5 -I results/cnv/hdf5/MGM20-0872_S2.hdf5 -I results/cnv/hdf5/MGM20-1121_S4.hdf5 -I results/cnv/hdf5/MGM20-1543_S10.hdf5 --contig-ploidy-priors resources/contig_ploidy_priors.tsv --output-prefix ploidy -imr OVERLAPPING_ONLY -O results/cnv/ploidy; 15:09:27.326 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/software/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Oct 18, 2021 3:09:27 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 15:09:27.686 INFO DetermineGermlineContigPloidy - ------------------------------------------------------------; 15:09:27.686 INFO DetermineGermlineContigPloidy - The Genome Analysis Toolkit (GATK) v4.2.0.0; 15:09:27.687 INFO DetermineGermlineContigPloidy - For support and documentation go to https://software.broadinstitute.org/gatk/; 15:09:27.687 INFO DetermineGermlineContigPloidy - Executing as n.liorni@hpc001 on Linux v3.10.0-1127.el7.x86_64 amd64; 15:09:27.687 INFO DetermineGermlineContigPloidy - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_302-b08; 15:09:27.687 INFO DetermineGermlineContigPloidy - Start Date/Time: 18 ottobre 2021 15.09.27 CEST; 15:09:27.68",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7444#issuecomment-945753905:1303,Load,Loading,1303,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7444#issuecomment-945753905,1,['Load'],['Loading']
Performance,. Error was: Failure while waiting for FeatureReader to initialize with exception: org.broadinstitute.hellbender.exceptions.UserException: Failed to create reader from gs://fc-c64a0fcd-fb30-4a7e-bdc6-3c09a9286941/f6aeb0ce-044a-4b36-a5f0-d3fda62252a5/ReblockGVCF/66c24439-cfeb-4b85-bc02-aefe693177b6/call-GenotypeGVCF/NWD197223.vcf.gz; 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lambda$getFeatureReadersInParallel$601(GenomicsDBImport.java:605); 	at java.util.LinkedHashMap.forEach(LinkedHashMap.java:684); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getFeatureReadersInParallel(GenomicsDBImport.java:600); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.createSampleToReaderMap(GenomicsDBImport.java:491); 	at com.intel.genomicsdb.importer.GenomicsDBImporter.lambda$null$2(GenomicsDBImporter.java:602); 	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590); 	... 3 more; Caused by: java.util.concurrent.ExecutionException: org.broadinstitute.hellbender.exceptions.UserException: Failed to create reader from gs://fc-c64a0fcd-fb30-4a7e-bdc6-3c09a9286941/f6aeb0ce-044a-4b36-a5f0-d3fda62252a5/ReblockGVCF/66c24439-cfeb-4b85-bc02-aefe693177b6/call-GenotypeGVCF/NWD197223.vcf.gz; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lambda$getFeatureReadersInParallel$601(GenomicsDBImport.java:602); 	... 8 more; Caused by: org.broadinstitute.hellbender.exceptions.UserException: Failed to create reader from gs://fc-c64a0fcd-fb30-4a7e-bdc6-3c09a9286941/f6aeb0ce-044a-4b36-a5f0-d3fda62252a5/ReblockGVCF/66c24439-cfeb-4b85-bc02-aefe693177b6/call-GenotypeGVCF/NWD197223.vcf.gz; 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getReaderFromPath(GenomicsDBImport.java:640); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lam,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-411503423:2260,concurren,concurrent,2260,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-411503423,1,['concurren'],['concurrent']
Performance,../coveragemodel/cachemanager/DuplicableNDArray.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9EdXBsaWNhYmxlTkRBcnJheS5qYXZh) | `81.818% <100%> (+38.068%)` | `6 <2> (+2)` | :arrow_up: |; | [...s/coveragemodel/cachemanager/DuplicableNumber.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9EdXBsaWNhYmxlTnVtYmVyLmphdmE=) | `80% <100%> (+80%)` | `5 <2> (+5)` | :arrow_up: |; | [...coveragemodel/cachemanager/PrimitiveCacheNode.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9QcmltaXRpdmVDYWNoZU5vZGUuamF2YQ==) | `83.333% <71.429%> (+30.702%)` | `10 <7> (+3)` | :arrow_up: |; | [...er/tools/coveragemodel/cachemanager/CacheNode.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9DYWNoZU5vZGUuamF2YQ==) | `80.645% <76.923%> (+30.645%)` | `9 <8> (+4)` | :arrow_up: |; | [...overagemodel/cachemanager/ComputableCacheNode.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9Db21wdXRhYmxlQ2FjaGVOb2RlLmphdmE=) | `89.189% <80%> (+32.779%)` | `18 <17> (+2)` | :arrow_up: |; | [...ols/coveragemodel/CoverageModelEMComputeBlock.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL0NvdmVyYWdlTW9kZWxFTUNvbXB1dGVCbG9jay5qYXZh) | `77.617% <82.558%> (-1.61%)` | `49 <2> (-1)` | |; | [...dinstitute/hellbender/utils/MathObjectAsserts.java](https://c,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3035#issuecomment-306412418:2910,cache,cachemanager,2910,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3035#issuecomment-306412418,2,"['Cache', 'cache']","['CacheNode', 'cachemanager']"
Performance,.018 INFO FeatureManager - Using codec VCFCodec to read file file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/dbsnp/hg38/hg38_All_20170710.vcf.gz; > 12:28:19.213 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/CancerGeneCensus_Table_1_full_2012-03-15.txt -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/cancer_gene_census/hg38/CancerGeneCensus_Table_1_full_2012-03-15.txt; > 12:28:19.227 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/Cosmic.db -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/cosmic/hg38/Cosmic.db; > 12:28:19.401 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/cosmic_tissue.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/cosmic_tissue/hg38/cosmic_tissue.tsv; > 12:28:19.487 INFO DataSourceUtils - Setting lookahead cache for data source: chr1_a_bed : 100000; > 12:28:19.495 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/chr1_a_bed.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/chr1_a_bed/hg38/chr1_a_bed.tsv; > 12:28:19.500 INFO FeatureManager - Using codec XsvLocatableTableCodec to read file file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/chr1_a_bed/hg38/chr1_a_bed.config; > 12:28:19.505 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/chr1_a_bed.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/chr1_a_bed/hg38/chr1_a_bed.tsv; > 12:28:19.507 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/chr1_a_bed.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/chr1_a_bed/hg38/chr1_a_bed.tsv; > WARNING 2020-07-21 12:28:19 AsciiLineReader Creating an index,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6708#issuecomment-661776975:13006,cache,cache,13006,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6708#issuecomment-661776975,1,['cache'],['cache']
Performance,".1.5 or early version as far as we know) when running `FilterAlignmentArtifacts` in one of our cluster but not the other. We narrowed down the issue, using the CPU differences (the working one does not support AVX2), to `libgkl_smithwaterman.so`. Paths are shortened for clarity in the following commands. ```; bash faa.sh ; Using GATK jar /app/gatk-package-4.1.8.0-local.jar; Running:; /bin/java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /app/gatk-package-4.1.8.0-local.jar FilterAlignmentArtifacts -V /output/sample.FilterMutectCalls.vcf.gz -R /db/hs37d5.fa --bwa-mem-index-image /db/hg38.fa.img -I /output/sample.Mutect2.bam -O sample.somatic_filter.test.vcf.gz --use-jdk-inflater true; 19:11:56.929 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/app/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 19:11:56.943 INFO NativeLibraryLoader - Loading libgkl_smithwaterman.so from jar:file:/app/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_smithwaterman.so; 19:11:56.944 INFO SmithWatermanAligner - Using AVX accelerated SmithWaterman implementation; 19:11:57.168 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/app/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jul 19, 2020 7:11:57 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 19:11:57.324 INFO FilterAlignmentArtifacts - ------------------------------------------------------------; 19:11:57.324 INFO FilterAlignmentArtifacts - The Genome Analysis Toolkit (GATK) v4.1.8.0; 19:11:57.325 INFO FilterAlignmentArtifacts - For support and documentation go to https://software.broadinstitute.org/gatk/; 19:11:57.325 INFO FilterAlignmentArtifacts - Executing as foo@bar.local on Linux v2.6.32-696.6.3.el6.x86_",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-660645356:1086,Load,Loading,1086,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-660645356,1,['Load'],['Loading']
Performance,".679 INFO HaplotypeCaller - Requester pays: disabled 13:39:56.680 INFO HaplotypeCaller - Initializing engine 13:39:56.968 INFO HaplotypeCaller - Done initializing engine 13:39:56.971 INFO HaplotypeCallerEngine - Tool is in reference confidence mode and the annotation, the following changes will be made to any specified annotations: 'StrandBiasBySample' will be enabled. 'ChromosomeCounts', 'FisherStrand', 'StrandOddsRatio' and 'QualByDepth' annotations have been disabled 13:39:57.000 INFO HaplotypeCallerEngine - Standard Emitting and Calling confidence set to -0.0 for reference-model confidence output 13:39:57.000 INFO HaplotypeCallerEngine - All sites annotated with PLs forced to true for reference-model confidence output 13:39:57.020 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/omics/groups/OE0540/internal/software/jvm/gatk/4.4.0.0/gatk-package-4.4.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so 13:39:57.026 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported 13:39:57.026 WARN PairHMM - ***WARNING: Machine does not have the AVX instruction set support needed for the accelerated AVX PairHmm. Falling back to the MUCH slower LOGLESS_CACHING implementation! 13:39:57.108 INFO ProgressMeter - Starting traversal 13:39:57.110 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute 13:40:07.119 INFO ProgressMeter - chr19:8969701 0.2 29900 179382.1 13:40:17.116 INFO ProgressMeter - chr19:20264701 0.3 67550 202609.5 13:40:27.115 INFO ProgressMeter - chr19:31874701 0.5 106250 212471.7 13:40:37.116 INFO ProgressMeter - chr19:44792701 0.7 149310 223937.0 13:40:49.251 WARN InbreedingCoeff - InbreedingCoeff will not be calculated at position chr19:55910646 and possibly subsequent; at least 10 samples must have called genotypes 13:40:49.413 INFO ProgressMeter - chr19:55910600 0.9 186370 213817.0; 13:40:55.466 INFO HaplotypeCaller - 0 read(s) filtered by: MappingQualityReadFilter; 0 r",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5727#issuecomment-1781017195:13703,multi-thread,multi-threaded,13703,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5727#issuecomment-1781017195,1,['multi-thread'],['multi-threaded']
Performance,.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.ContigAlignmentsModifier.computeNewRefSpanAndCigar(ContigAlignmentsModifier.java:163); at org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.ContigAlignmentsModifier.clipAlignmentInterval(ContigAlignmentsModifier.java:42); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.removeOverlap(CpxVariantInterpreter.java:179); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.deOverlapAlignments(CpxVariantInterpreter.java:122); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.furtherPreprocess(CpxVariantInterpreter.java:79); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.lambda$inferCpxVariant$bdd686a3$1(CpxVariantInterpreter.java:51); at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4648#issuecomment-380510575:2173,concurren,concurrent,2173,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4648#issuecomment-380510575,2,['concurren'],['concurrent']
Performance,".apply(PairRDDFunctions.scala:979); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61); at org.apache.spark.scheduler.Task.run(Task.scala:64); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 15/07/14 13:14:53 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 1.0 (TID 1, localhost): java.lang.IncompatibleClassChangeError: Found class org.apache.hadoop.mapreduce.TaskAttemptContext, but interface was expected; at com.cloudera.dataflow.spark.TemplatedTextOutputFormat.getOutputFile(TemplatedTextOutputFormat.java:50); at com.cloudera.dataflow.spark.TemplatedTextOutputFormat.getDefaultWorkFile(TemplatedTextOutputFormat.java:46); at org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:125); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$12.apply(PairRDDFunctions.scala:995); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$12.apply(PairRDDFunctions.scala:979); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61); at org.apache.spark.scheduler.Task.run(Task.scala:64); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 15/07/14 13:14:53 ERROR scheduler.TaskSetManager: Task 0 in stage 1.0 failed 1 times; aborting job; 15/07/14 13:14:53 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool; 15/07/14 13:14:53 INFO scheduler.TaskSchedulerImpl: Cancelling stage 1; 15/07/14 13:14:53 INFO scheduler.DAGScheduler: Stage 1 (saveAsNewAPIHadoopFile at TransformTranslator.java:432) failed in 0.155 s; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/639#issuecomment-121313713:31707,concurren,concurrent,31707,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/639#issuecomment-121313713,2,['concurren'],['concurrent']
Performance,".csv"",; ""NIST controlHCprocesshours"": ""90.94291388888888"",; ""NIST controlHCsystemhours"": ""0.182125"",; ""NIST controlHCwallclockhours"": ""63.56370277777778"",; ""NIST controlHCwallclockmax"": ""3.701625"",; ""NIST controlMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/e372bd14-cd1f-4563-8d8a-abf6b6ca7883/call-NISTSampleHeadToHead/BenchmarkComparison/103cd89c-b177-4a0b-84fc-9553a1f8161f/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""NIST controlindelF1Score"": ""0.9843"",; ""NIST controlindelPrecision"": ""0.9895"",; ""NIST controlsnpF1Score"": ""0.9908"",; ""NIST controlsnpPrecision"": ""0.992"",; ""NIST controlsnpRecall"": ""0.9896"",; ""NIST controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/e372bd14-cd1f-4563-8d8a-abf6b6ca7883/call-NISTSampleHeadToHead/BenchmarkComparison/103cd89c-b177-4a0b-84fc-9553a1f8161f/call-BenchmarkVCFControlSample/Benchmark/eaf4d582-e197-4e13-8122-5e1ec22591ae/call-CombineSummaries/summary.csv"",; ""NIST evalHCprocesshours"": ""73.06777222222223"",; ""NIST evalHCsystemhours"": ""0.1622555555555555"",; ""NIST evalHCwallclockhours"": ""46.65241388888888"",; ""NIST evalHCwallclockmax"": ""2.7461055555555554"",; ""NIST evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/e372bd14-cd1f-4563-8d8a-abf6b6ca7883/call-NISTSampleHeadToHead/BenchmarkComparison/103cd89c-b177-4a0b-84fc-9553a1f8161f/call-EVALRuntimeTask/cacheCopy/monitoring.pdf"",; ""NIST evalindelF1Score"": ""0.9843"",; ""NIST evalindelPrecision"": ""0.9895"",; ""NIST evalsnpF1Score"": ""0.9908"",; ""NIST evalsnpPrecision"": ""0.992"",; ""NIST evalsnpRecall"": ""0.9896"",; ""NIST evalsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/e372bd14-cd1f-4563-8d8a-abf6b6ca7883/call-NISTSampleHeadToHead/BenchmarkComparison/103cd89c-b177-4a0b-84fc-9553a1f8161f/call-BenchmarkVCFTestSample/Benchmark/87985440-93fa-4a33-ac09-e4cbead32bfb/call-CombineSummaries/summary.csv""; }; } ; </pre> </details>",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7723#issuecomment-1069381494:14468,cache,cacheCopy,14468,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7723#issuecomment-1069381494,1,['cache'],['cacheCopy']
Performance,".driver.maxResultSize=0,spark.driver.userClassPathFirst=true,spark.io.compression.codec=lzf,spark.yarn.executor.memoryOverhead=600,spark.driver.extraJavaOptions=-Dsamjdk.compression_level=1 -DGATK_STACKTRACE_ON_USER_EXCEPTION=true ,spark.executor.extraJavaOptions=-Dsamjdk.compression_level=1 -DGATK_STACKTRACE_ON_USER_EXCEPTION=true --jar /Users/markw/IdeaProjects/gatk/build/libs/gatk-package-4.alpha.2-157-g7d7c5ec-SNAPSHOT-spark.jar -- PrintReadsSpark -I gs://mw-pathseq-test/hs37d5cs.reads.sorted.bam -O hs37d5cs.reads.txt --apiKey XXXXXXXXXXXXXXXXXXXXX --verbosity DEBUG --sparkMaster yarn; Copying file:///Users/markw/IdeaProjects/gatk/build/libs/gatk-package-4.alpha.2-157-g7d7c5ec-SNAPSHOT-spark.jar [Content-Type=application/java-archive]...; - [1 files][ 95.3 MiB/ 95.3 MiB] 9.0 MiB/s; Operation completed over 1 objects/95.3 MiB.; Job [5b3d4225-0547-4aa9-8a83-ab26460aa2d2] submitted.; Waiting for job output...; 21:42:45.768 INFO IntelGKLUtils - Trying to load Intel GKL library from:; 	jar:file:/tmp/5b3d4225-0547-4aa9-8a83-ab26460aa2d2/gatk-package-4.alpha.2-157-g7d7c5ec-SNAPSHOT-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 21:42:45.791 DEBUG IntelGKLUtils - Extracted Intel GKL to /tmp/root/libgkl_compression6493251482684327282.so. 21:42:45.792 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [February 6, 2017 9:42:45 PM UTC] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark --output hs37d5cs.reads.txt --input gs://mw-pathseq-test/hs37d5cs.reads.sorted.bam --apiKey XXXXXXXXXXXXXXXX --sparkMaster yarn --verbosity DEBUG --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --help false --version false --QUIET false --use_jdk_deflater false --disableAllReadFilters false; [February 6, 2017 9:42:45 PM UTC] Executing as root@mw-test-m on Linux 3.16.0-4-amd64 amd64; OpenJDK ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2394#issuecomment-277823929:2301,load,load,2301,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2394#issuecomment-277823929,1,['load'],['load']
Performance,".gz --tmp-dir=/tmp --sample-ploidy 24 -L chrom2; ```. It failed at the same region it was failing before, with this error message:. ```; 01:15:27.623 INFO GenotypeGVCFs - Shutting down engine; GENOMICSDB_TIMER,GenomicsDB iterator next() timer,Wall-clock time(s),8476.664214527651,Cpu time(s),8391.206707930733; [January 14, 2020 1:15:30 AM BRT] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 279.78 minutes.; Runtime.totalMemory()=16865820672; htsjdk.tribble.TribbleException: Invalid block size -122708061; at htsjdk.variant.bcf2.BCF2Decoder.readNextBlock(BCF2Decoder.java:66); at htsjdk.variant.bcf2.BCF2Codec.decode(BCF2Codec.java:134); at htsjdk.variant.bcf2.BCF2Codec.decode(BCF2Codec.java:58); at org.genomicsdb.reader.GenomicsDBFeatureIterator.next(GenomicsDBFeatureIterator.java:181); at org.genomicsdb.reader.GenomicsDBFeatureIterator.next(GenomicsDBFeatureIterator.java:49); at org.broadinstitute.hellbender.engine.FeatureIntervalIterator.loadNextFeature(FeatureIntervalIterator.java:98); at org.broadinstitute.hellbender.engine.FeatureIntervalIterator.loadNextNovelFeature(FeatureIntervalIterator.java:74); at org.broadinstitute.hellbender.engine.FeatureIntervalIterator.next(FeatureIntervalIterator.java:62); at org.broadinstitute.hellbender.engine.FeatureIntervalIterator.next(FeatureIntervalIterator.java:24); at java.util.Iterator.forEachRemaining(Iterator.java:116); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEachOrdered(ReferencePipeline.java:490); at org.broadinsti",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6275#issuecomment-574113941:1697,load,loadNextFeature,1697,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6275#issuecomment-574113941,1,['load'],['loadNextFeature']
Performance,.internal.execution.steps.ResolveInputChangesStep.execute(ResolveInputChangesStep.java:48); at org.gradle.internal.execution.steps.ResolveInputChangesStep.execute(ResolveInputChangesStep.java:33); at org.gradle.internal.execution.steps.CancelExecutionStep.execute(CancelExecutionStep.java:39); at org.gradle.internal.execution.steps.TimeoutStep.executeWithoutTimeout(TimeoutStep.java:73); at org.gradle.internal.execution.steps.TimeoutStep.execute(TimeoutStep.java:54); at org.gradle.internal.execution.steps.CatchExceptionStep.execute(CatchExceptionStep.java:35); at org.gradle.internal.execution.steps.CreateOutputsStep.execute(CreateOutputsStep.java:51); at org.gradle.internal.execution.steps.SnapshotOutputsStep.execute(SnapshotOutputsStep.java:45); at org.gradle.internal.execution.steps.SnapshotOutputsStep.execute(SnapshotOutputsStep.java:31); at org.gradle.internal.execution.steps.CacheStep.executeWithoutCache(CacheStep.java:208); at org.gradle.internal.execution.steps.CacheStep.execute(CacheStep.java:70); at org.gradle.internal.execution.steps.CacheStep.execute(CacheStep.java:45); at org.gradle.internal.execution.steps.BroadcastChangingOutputsStep.execute(BroadcastChangingOutputsStep.java:49); at org.gradle.internal.execution.steps.StoreSnapshotsStep.execute(StoreSnapshotsStep.java:43); at org.gradle.internal.execution.steps.StoreSnapshotsStep.execute(StoreSnapshotsStep.java:32); at org.gradle.internal.execution.steps.RecordOutputsStep.execute(RecordOutputsStep.java:38); at org.gradle.internal.execution.steps.RecordOutputsStep.execute(RecordOutputsStep.java:24); at org.gradle.internal.execution.steps.SkipUpToDateStep.executeBecause(SkipUpToDateStep.java:96); at org.gradle.internal.execution.steps.SkipUpToDateStep.lambda$execute$0(SkipUpToDateStep.java:89); at org.gradle.internal.execution.steps.SkipUpToDateStep.execute(SkipUpToDateStep.java:54); at org.gradle.internal.execution.steps.SkipUpToDateStep.execute(SkipUpToDateStep.java:38); at org.gradle.internal.execution.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6466#issuecomment-590387973:9183,Cache,CacheStep,9183,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6466#issuecomment-590387973,2,['Cache'],['CacheStep']
Performance,.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.io.IOException: Existing mirrorFile and resourceId don't match isDirectory status! '/hadoop_gcs_connector_metadata_cache/hellbender/test/output/gatk4-spark/recalibrated.bam' (dir: 'false') vs 'gs://hellbender/test/output/gatk4-spark/recalibrated.bam/' (dir: 'true'); 	at com.google.cloud.hadoop.gcsio.FileSystemBackedDirectoryListCache.getCacheEntryInternal(FileSystemBackedDirectoryListCache.java:198); 	at com.google.cloud.hadoop.gcsio.FileSystemBackedDirectoryListCache.putResourceId(FileSystemBackedDirectoryListCache.java:363); 	at com.google.cloud.hadoop.gcsio.CacheSupplementedGoogleCloudStorage.createEmptyObjects(CacheSupplementedGoogleCloudStorage.java:150); 	at com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.mkdirs(GoogleCloudStorageFileSystem.java:578); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.mkdirs(GoogleHadoopFileSystemBase.java:1372); 	at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:1881); 	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:313); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1150); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1078); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1078); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); ,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2306#issuecomment-271419191:2473,Cache,CacheSupplementedGoogleCloudStorage,2473,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2306#issuecomment-271419191,1,['Cache'],['CacheSupplementedGoogleCloudStorage']
Performance,".reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:182); 	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:164); 	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:412); 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64); 	at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 5.0 failed 1 times, most recent failure: Lost task 1.0 in stage 5.0 (TID 12, localhost, executor driver): java.util.ConcurrentModificationException; 	at java.util.ArrayList.sort(ArrayList.java:1464); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.<init>(ReadThreadingAssembler.java:81); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerReadThreadingAssemblerArgumentCollection.makeReadThreadingAssembler(HaplotypeCallerReadThreadingAssemblerArgumentCollection.java:37); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerArgumentCollection.createReadThreadingAssembler(AssemblyBasedCallerArgumentCollection.java:36); 	at org.broadinstitute.hellbender.tools.walkers.haplo",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429:7699,concurren,concurrent,7699,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429,1,['concurren'],['concurrent']
Performance,".reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:182); 	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:164); 	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:412); 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64); 	at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 5.0 failed 1 times, most recent failure: Lost task 1.0 in stage 5.0 (TID 12, localhost, executor driver): java.util.ConcurrentModificationException; 	at java.util.ArrayList.sort(ArrayList.java:1464); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.<init>(ReadThreadingAssembler.java:81); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerReadThreadingAssemblerArgumentCollection.makeReadThreadingAssembler(HaplotypeCallerReadThreadingAssemblerArgumentCollection.java:37); 	at org.broadinstitute.hellbender.tools.walkers.hapl",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429:7519,concurren,concurrent,7519,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429,1,['concurren'],['concurrent']
Performance,.socketRead0; 1.7% 0 + 266 sun.nio.ch.EPollArrayWrapper.epollWait; 0.7% 0 + 99 sun.nio.ch.NativeThread.current; 0.6% 0 + 96 java.util.zip.Deflater.reset; 0.6% 0 + 88 java.util.zip.Inflater.reset; 0.4% 0 + 58 org.apache.hadoop.util.NativeCrc32.nativeComputeChunkedSumsByteArray; 0.3% 0 + 47 java.io.FileInputStream.readBytes; 0.3% 0 + 41 sun.nio.ch.FileDispatcherImpl.read0; 0.2% 0 + 26 java.lang.Throwable.fillInStackTrace; 0.1% 0 + 19 java.io.UnixFileSystem.getLength; 0.1% 0 + 13 java.lang.Object.getClass; 0.1% 0 + 12 java.lang.Object.hashCode; 0.1% 11 + 0 java.lang.ClassLoader.defineClass1; 0.1% 3 + 7 java.lang.Class.forName0; 0.1% 0 + 9 sun.nio.ch.FileDispatcherImpl.size0; 0.1% 0 + 9 java.util.zip.ZipFile.getEntry; 0.1% 0 + 8 java.lang.Class.isArray; 0.0% 0 + 7 java.io.FileOutputStream.open0; 0.0% 0 + 6 java.lang.Class.isPrimitive; 0.0% 0 + 6 java.io.FileOutputStream.close0; 73.9% 16 + 11236 Total stub (including elided). Thread-local ticks:; 60.2% 23027 Blocked (of total); 0.0% 1 Class loader; 0.0% 1 Unknown: thread_state; ```. and on igzip:. ```; Flat profile of 425.43 secs (38916 total ticks): Executor task launch worker-4. Interpreted + native Method ; 0.1% 0 + 23 java.net.Inet6AddressImpl.lookupAllHostAddr; 0.1% 0 + 16 java.io.UnixFileSystem.delete0; 0.1% 14 + 0 org.apache.spark.util.collection.TimSort.sort; 0.1% 10 + 0 org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1$$anonfun$12$$anonfun$apply$4.apply$mcV$sp; 0.1% 10 + 0 htsjdk.samtools.BAMRecordCodec.encode; 0.0% 0 + 5 java.net.SocketInputStream.socketRead0; 0.0% 5 + 0 org.apache.spark.util.collection.TimSort$SortState.mergeHi; 0.0% 0 + 3 java.net.Inet6AddressImpl.getHostByAddr; 0.0% 3 + 0 org.apache.spark.util.collection.ExternalSorter.insertAll; 0.0% 0 + 2 htsjdk.samtools.util.zip.IntelDeflater.deflateBytes; 0.0% 2 + 0 sun.misc.Unsafe.defineClass; 0.0% 2 + 0 sun.reflect.MethodAccessorGenerator.emitInvoke; 0.0% 2 + 0 org.apache.spark.deploy.SparkHadoopUtil$$anonfun$2$$anonfun$apply$mc,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1702#issuecomment-210127581:5424,load,loader,5424,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1702#issuecomment-210127581,1,['load'],['loader']
Performance,.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:512); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:128); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:125); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:92); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:47); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:125); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:109); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:131); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:104); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: javax.net.ssl.SSLException: Connection has been shutdown: javax.net.ssl.SSLException: java.net.SocketException: Connection reset; 	at sun.security.ssl.SSLSocketImpl.checkEOF(SSLSocketImpl.java:1541); 	at sun.security.ssl.AppInputStream.available(AppInputStream.java:60); 	at java.io.BufferedInputStream.available(BufferedInputStream.java:410); 	at sun.net.www.MeteredStream.available(MeteredStream.java:170); 	at sun.net.www.http.KeepAliveStream.close(KeepAliveStream.java:85); 	at java.io.FilterInputStream.close(FilterInputStream.java:181); 	at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.close(HttpURLConnection.java:3448); 	at java.io.FilterInputStream.close(FilterInputStream.java:181); 	at shaded.cloud_nio.com.google.api.client.util.IOUtils.copy(IOUtils.java:97); 	at shaded.cloud_nio.com.google.api.client.util.IOUtils.copy(IOUtils.java:63); 	at shaded.clou,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-301610931:6580,concurren,concurrent,6580,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-301610931,1,['concurren'],['concurrent']
Performance,.storage.contrib.nio.CloudStorageFileSystemProvider.newFileSystem(CloudStorageFileSystemProvider.java:192); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.newFileSystem(CloudStorageFileSystemProvider.java:83); 	at java.nio.file.FileSystems.newFileSystem(FileSystems.java:336); 	at org.seqdoop.hadoop_bam.util.NIOFileUtil.asPath(NIOFileUtil.java:40); 	at org.seqdoop.hadoop_bam.BAMRecordReader.initialize(BAMRecordReader.java:140); 	at org.seqdoop.hadoop_bam.BAMInputFormat.createRecordReader(BAMInputFormat.java:121); 	at org.seqdoop.hadoop_bam.AnySAMInputFormat.createRecordReader(AnySAMInputFormat.java:190); 	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.<init>(NewHadoopRDD.scala:170); 	at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:130); 	at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:67); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2450#issuecomment-285797337:6410,concurren,concurrent,6410,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2450#issuecomment-285797337,2,['concurren'],['concurrent']
Performance,/3146?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...ellbender/tools/exome/CalculateTargetCoverage.java](https://codecov.io/gh/broadinstitute/gatk/pull/3146?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9DYWxjdWxhdGVUYXJnZXRDb3ZlcmFnZS5qYXZh) | `92.265% <ø> (ø)` | `32 <0> (ø)` | :arrow_down: |; | [...bender/tools/exome/NormalizeSomaticReadCounts.java](https://codecov.io/gh/broadinstitute/gatk/pull/3146?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9Ob3JtYWxpemVTb21hdGljUmVhZENvdW50cy5qYXZh) | `77.143% <0%> (-2.024%)` | `10% <0%> (+4%)` | |; | [...bender/tools/exome/germlinehmm/xhmm/XHMMModel.java](https://codecov.io/gh/broadinstitute/gatk/pull/3146?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9nZXJtbGluZWhtbS94aG1tL1hITU1Nb2RlbC5qYXZh) | `100% <0%> (ø)` | `14% <0%> (+7%)` | :arrow_up: |; | [...te/hellbender/tools/exome/PerformSegmentation.java](https://codecov.io/gh/broadinstitute/gatk/pull/3146?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9QZXJmb3JtU2VnbWVudGF0aW9uLmphdmE=) | `100% <0%> (ø)` | `6% <0%> (+3%)` | :arrow_up: |; | [...ender/utils/hmm/segmentation/HMMPostProcessor.java](https://codecov.io/gh/broadinstitute/gatk/pull/3146?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9obW0vc2VnbWVudGF0aW9uL0hNTVBvc3RQcm9jZXNzb3IuamF2YQ==) | `85.439% <0%> (+1.215%)` | `92% <0%> (+15%)` | :arrow_up: |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/3146?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `73.649% <0%> (+2.027%)` | `34% <0%> (ø)` | :arrow_down: |; | [.../coverage/pca/HDF5PCACoveragePoNCreationUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/3146?src=pr&el=tree#diff-c3,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3146#issuecomment-311542216:1843,Perform,PerformSegmentation,1843,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3146#issuecomment-311542216,1,['Perform'],['PerformSegmentation']
Performance,"/FindBreakpointEvidenceSpark.java; - input coverage-scaled thresholds, convert to absolute internally. Allow thresholds to be double instead of int; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilter.java; - getter functions added to calculate properties for XGBoostEvidenceFilter. Also fromStringRep() and helper constructors added for testing; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointEvidence.java; - updates to tests reflecting changes to these interfaces; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilterTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/FindBreakpointEvidenceSparkUnitTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/integration/FindBreakpointEvidenceSparkIntegrationTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/integration/SVIntegrationTestDataProvider.java. 4. Added code; - factory to call appropriate BreakpointEvidence filter; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointFilterFactory.java; - simple helper class to hold feature vectors for classifier; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/EvidenceFeatures.java; - implement classifier-based BreakpointEvidence filter; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/XGBoostEvidenceFilter.java; - unit test for classifier filter; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/XGBoostEvidenceFilterUnitTest.java. 5. Added resources; - Genome tracts; src/main/resources/large/hg38_centromeres.txt.gz; src/main/resources/large/hg38_gaps.txt.gz; src/main/resources/large/hg38_umap_s100.txt.gz; - Classifier binary file; src/main/resources/large/sv_evidence_classifier.bin; - Data used for validation of performance in unit tests; src/test/resources/sv_classifier_test_data.json; src/test/resources/sv_features_test_data.json",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477:3093,perform,performance,3093,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477,1,['perform'],['performance']
Performance,"/arlab174:54310/GATK4TEST/Output/Test_CEU_ReadsCount --sparkMaster yarn; 14:56:20.999 INFO IntelGKLUtils - Trying to load Intel GKL library from:; 	jar:file:/mnt/raid5/frankliu/code/GATK/gatk/build/libs/gatk-package-4.alpha.2-120-g00a40ea-SNAPSHOT-spark.jar!/com/intel/gkl/native/libgkl_compression.so; Exception in thread ""main"" java.lang.UnsatisfiedLinkError: /tmp/frankliu/libgkl_compression8271576113600851848.so: /tmp/frankliu/libgkl_compression8271576113600851848.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64-bit platform); 	at java.lang.ClassLoader$NativeLibrary.load(Native Method); 	at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941); 	at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824); 	at java.lang.Runtime.load0(Runtime.java:809); 	at java.lang.System.load(System.java:1086); 	at com.intel.gkl.IntelGKLUtils.load(IntelGKLUtils.java:133); 	at com.intel.gkl.compression.IntelDeflater.load(IntelDeflater.java:58); 	at com.intel.gkl.compression.IntelDeflater.load(IntelDeflater.java:53); 	at com.intel.gkl.compression.IntelDeflaterFactory.<init>(IntelDeflaterFactory.java:16); 	at com.intel.gkl.compression.IntelDeflaterFactory.<init>(IntelDeflaterFactory.java:20); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:143); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116); 	at org.broadinstitute.hellbender.Main.main(Main.java:158); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.l",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2302#issuecomment-265854885:1995,load,load,1995,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2302#issuecomment-265854885,1,['load'],['load']
Performance,/codecov.io/gh/broadinstitute/gatk/pull/4902?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) | Coverage Δ | |; |---|---|---|; | [...e/hellbender/engine/FeatureDataSourceUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvRmVhdHVyZURhdGFTb3VyY2VVbml0VGVzdC5qYXZh) | `88.318% <ø> (ø)` | |; | [...institute/hellbender/engine/FeatureDataSource.java](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvRmVhdHVyZURhdGFTb3VyY2UuamF2YQ==) | `74.603% <57.143%> (+24.603%)` | :arrow_up: |; | [...ender/engine/cache/SideReadInputCacheStrategy.java](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvY2FjaGUvU2lkZVJlYWRJbnB1dENhY2hlU3RyYXRlZ3kuamF2YQ==) | `81.481% <81.481%> (ø)` | |; | [...adinstitute/hellbender/engine/ReadsDataSource.java](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUmVhZHNEYXRhU291cmNlLmphdmE=) | `91.667% <84.615%> (+33.333%)` | :arrow_up: |; | [...ellbender/engine/VariantWalkerIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3J,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4902#issuecomment-397744741:2336,cache,cache,2336,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4902#issuecomment-397744741,1,['cache'],['cache']
Performance,/funcotator/funcotator_dataSources.v1.6.20190124s/chr1_b_bed/hg38/chr1_b_bed.tsv; > 12:28:17.967 INFO FeatureManager - Using codec XsvLocatableTableCodec to read file file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/chr1_b_bed/hg38/chr1_b_bed.config; > 12:28:17.995 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/chr1_b_bed.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/chr1_b_bed/hg38/chr1_b_bed.tsv; > 12:28:17.997 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/chr1_b_bed.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/chr1_b_bed/hg38/chr1_b_bed.tsv; > WARNING 2020-07-21 12:28:17 AsciiLineReader Creating an indexable source for an AsciiFeatureCodec using a stream that is neither a PositionalBufferedStream nor a BlockCompressedInputStream; > 12:28:18.002 INFO DataSourceUtils - Setting lookahead cache for data source: Oreganno : 100000; > 12:28:18.009 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/oreganno.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/oreganno/hg38/oreganno.tsv; > 12:28:18.020 INFO FeatureManager - Using codec XsvLocatableTableCodec to read file file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/oreganno/hg38/oreganno.config; > 12:28:18.120 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/oreganno.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/oreganno/hg38/oreganno.tsv; > 12:28:18.121 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/oreganno.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/oreganno/hg38/oreganno.tsv; > WARNING 2020-07-21 12:28:18 AsciiLineReader Creating an indexable source for an Asci,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6708#issuecomment-661776975:9578,cache,cache,9578,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6708#issuecomment-661776975,1,['cache'],['cache']
Performance,/hg38/gencode_xhgnc_v90_38.hg38.tsv; > 12:28:17.925 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/achilles_lineage_results.import.txt -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/achilles/hg38/achilles_lineage_results.import.txt; > 12:28:17.932 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/gencode_xrefseq_v90_38.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/gencode_xrefseq/hg38/gencode_xrefseq_v90_38.tsv; > 12:28:17.939 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/hgnc_download_Nov302017.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/hgnc/hg38/hgnc_download_Nov302017.tsv; > 12:28:17.939 INFO Funcotator - Finalizing data sources (this step can be long if data sources are cloud-based)...; > 12:28:17.940 INFO DataSourceUtils - Setting lookahead cache for data source: chr1_b_bed : 100000; > 12:28:17.951 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/chr1_b_bed.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/chr1_b_bed/hg38/chr1_b_bed.tsv; > 12:28:17.967 INFO FeatureManager - Using codec XsvLocatableTableCodec to read file file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/chr1_b_bed/hg38/chr1_b_bed.config; > 12:28:17.995 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/chr1_b_bed.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/chr1_b_bed/hg38/chr1_b_bed.tsv; > 12:28:17.997 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/chr1_b_bed.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/chr1_b_bed/hg38/chr1_b_bed.tsv; > WARNING 2020-07-21 12:28:17 AsciiLineReader Creating an index,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6708#issuecomment-661776975:8388,cache,cache,8388,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6708#issuecomment-661776975,1,['cache'],['cache']
Performance,/reference/gatk_resource/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz -O PAAD11N.recal_data.test.table; Using GATK jar /data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx8G -Djava.io.tmpdir=/data/xieduo/Łuksza_2022_Nature -jar /data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar BaseRecalibrator -R /data/reference/gatk_resource/Homo_sapiens_assembly38.fasta -I /data/xieduo/Immun_genomics/data/Łuksza_2022_Nature/bam/PAAD11N.bam --known-sites /data/xieduo/WES_pipe/pipeline/gatk_resource/dbsnp_146.hg38.vcf.gz --known-sites /data/reference/gatk_resource/1000G_phase1.snps.high_confidence.hg38.vcf.gz --known-sites /data/reference/gatk_resource/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz -O PAAD11N.recal_data.test.table; 13:36:33.528 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 13:36:33.547 WARN NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (No such file or directory); 13:36:33.550 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 13:36:33.551 WARN NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (No such file or directory); 13:36:33.669 INFO BaseRecalibrator - ------------------------------------------------------------; 13:36:33.670 INFO BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.2.6.1; 13:36:33.670 INFO BaseRecalibrator - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:36:33.670 INFO BaseRecalibrator - Executin,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8005#issuecomment-1254561081:6490,Load,Loading,6490,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005#issuecomment-1254561081,1,['Load'],['Loading']
Performance,"/vidmap.json; 16:28:04.155 INFO GenomicsDBImport - Callset Map JSON file will be written to forkTest/callset.json; 16:28:04.156 INFO GenomicsDBImport - Complete VCF Header will be written to forkTest/vcfheader.vcf; 16:28:04.156 INFO GenomicsDBImport - Importing to array - forkTest/genomicsdb_array; 16:28:04.158 INFO ProgressMeter - Starting traversal; 16:28:04.158 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 16:28:05.198 INFO GenomicsDBImport - Starting batch input file preload; 16:29:23.571 INFO GenomicsDBImport - Finished batch preload; 16:48:46.140 INFO GenomicsDBImport - Shutting down engine; [May 4, 2018 4:48:46 PM EDT] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 20.96 minutes.; Runtime.totalMemory()=22281715712; java.util.concurrent.CompletionException: java.lang.OutOfMemoryError: Java heap space; at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273); at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1592); at java.util.concurrent.CompletableFuture$AsyncSupply.exec(CompletableFuture.java:1582); at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056); at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692); at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157); Caused by: java.lang.OutOfMemoryError: Java heap space; at com.intel.genomicsdb.importer.SilentByteBufferStream.<init>(SilentByteBufferStream.java:55); at com.intel.genomicsdb.importer.GenomicsDBImporterStreamWrapper.<init>(GenomicsDBImporterStreamWrapper.java:70); at com.intel.genomicsdb.importer.GenomicsDBImporter.addBufferStream(GenomicsDBImporter.java:397); at com.intel.genomicsdb.importer.GenomicsDBImporter.addSortedVariantContextIterator(Geno",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388079572:3603,concurren,concurrent,3603,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388079572,1,['concurren'],['concurrent']
Performance,0); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2114); at org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:78); ... 34 more; Caused by:; java.util.ConcurrentModificationException; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6513#issuecomment-601702690:8716,Concurren,ConcurrentModificationException,8716,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6513#issuecomment-601702690,1,['Concurren'],['ConcurrentModificationException']
Performance,0.3 35000 101621.1; 14:56:26.027 INFO ProgressMeter - 1:5856032 0.5 55000 105867.6; ...; 19:37:05.295 INFO ProgressMeter - GL000209.1:48811 281.2 30739000 109323.8; 19:37:15.543 INFO ProgressMeter - GL000224.1:65537 281.3 30758000 109324.9; 19:37:25.847 INFO ProgressMeter - GL000248.1:21736 281.5 30768000 109293.8; 19:37:25.906 INFO FilterMutectCalls - Finished pass 0 through the variants; 19:50:04.590 INFO FilterMutectCalls - Shutting down engine; [9 January 2020 7:50:04 PM] org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls done. Elapsed time: 294.19 minutes.; Runtime.totalMemory()=14966849536; java.lang.IllegalArgumentException: Values in probability array sum to a negative number NaN; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:731); 	at org.broadinstitute.hellbender.utils.MathUtils.normalizeSumToOne(MathUtils.java:731); 	at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.performEMIteration(SomaticClusteringModel.java:336); 	at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.learnAndClearAccumulatedData(SomaticClusteringModel.java:306); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.learnParameters(Mutect2FilteringEngine.java:158); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls.afterNthPass(FilterMutectCalls.java:159); 	at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.traverse(MultiplePassVariantWalker.java:44); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1048); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runC,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6202#issuecomment-572799341:4476,perform,performEMIteration,4476,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6202#issuecomment-572799341,1,['perform'],['performEMIteration']
Performance,"0/13 18:11:40 INFO yarn.Client: Application report for application_1507856833944_0003 (state: ACCEPTED); 17/10/13 18:11:41 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM); 17/10/13 18:11:41 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> mg, PROXY_URI_BASES -> http://mg:8088/proxy/application_1507856833944_0003), /proxy/application_1507856833944_0003; 17/10/13 18:11:41 INFO ui.JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter; 17/10/13 18:11:41 INFO yarn.Client: Application report for application_1507856833944_0003 (state: ACCEPTED); 17/10/13 18:11:42 INFO yarn.Client: Application report for application_1507856833944_0003 (state: RUNNING); 17/10/13 18:11:42 INFO yarn.Client: ; 	 client token: N/A; 	 diagnostics: N/A; 	 ApplicationMaster host: 10.131.101.145; 	 ApplicationMaster RPC port: 0; 	 queue: root.users.hdfs; 	 start time: 1507889497661; 	 final status: UNDEFINED; 	 tracking URL: http://mg:8088/proxy/application_1507856833944_0003/; 	 user: hdfs; 17/10/13 18:11:42 INFO cluster.YarnClientSchedulerBackend: Application application_1507856833944_0003 has started running.; 17/10/13 18:11:42 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44818.; 17/10/13 18:11:42 INFO netty.NettyBlockTransferService: Server created on 10.131.101.159:44818; 17/10/13 18:11:42 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy; 17/10/13 18:11:42 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.131.101.159, 44818, None); 17/10/13 18:11:42 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.131.101.159:44818 with 366.3 MB RAM, BlockManagerId(driver, 10.131.101.159, 44818, None); 17/10/13 18:11:42 INF",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-336412775:13009,queue,queue,13009,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-336412775,1,['queue'],['queue']
Performance,"00; >; > 16:17:04.377 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; >; > 16:17:04.397 **WARN** NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (/tmp/libgkl_compression3825249225068031371.so: /tmp/libgkl_compression3825249225068031371.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > 16:17:04.402 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; >; > 16:17:04.407 **WARN** NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (/tmp/libgkl_compression7506152962158874866.so: /tmp/libgkl_compression7506152962158874866.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > Sep 04, 2020 4:17:05 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; >; > INFO: Failed to detect whether we are running on Google Compute Engine.; >; > 16:17:05.842 INFO HaplotypeCaller - ------------------------------------------------------------; >; > 16:17:05.843 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.1.8.1; >; > 16:17:05.843 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; >; > 16:17:05.843 INFO HaplotypeCaller - Executing as robert@powerlinux on Linux v4.4.0-184-generic ppc64le; >; > 16:17:05.843 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_252-8u252-b09-1~16.04-b09; >; > 16:17:05.843 INFO HaplotypeCaller - Start Date/Time: September 4, 2020 4:17:04 PM UTC; >; > 16:17:05.843 INFO HaplotypeCaller - -------------------------------------------------",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600:2412,load,load,2412,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600,1,['load'],['load']
Performance,"01413230/document This method uses a low-rank approximation to the kernel to obtain an approximate segmentation in linear complexity in time and space. In practice, performance is actually quite impressive!. The implementation is relatively straightforward, clocking in at ~100 lines of python. Time complexity is O(log(maximum number of segments) * number of data points) and space complexity is O(number of data points * dimension of the kernel approximation), which makes use for WGS feasible. Segmentation of 10^6 simulated points with 100 segments takes about a minute and tends to recover segments accurately. Compare this with CBS, where segmentation of a WGS sample with ~700k points takes ~10 minutes---and note that these ~700k points are split up amongst ~20 chromosomes to start!. There are a small number of parameters that can affect the segmentation, but we can probably find good defaults in practice. What's also nice is that this method can find changepoints in moments of the distribution other than the mean, which means that it can straightforwardly be used for alternate-allele fraction segmentation. For example, all segments were recovered in the following simulated multimodal data, even though all of the segments have zero mean:. ![baf](https://user-images.githubusercontent.com/11076296/29100464-ad687946-7c79-11e7-99e4-962ab93709b4.png). Replacing the SNP segmentation in ACNV (which performs expensive maximum-likelihood estimation of the allele-fraction model) with this method would give a significant speedup there. Joint segmentation is straightforward and is simply given by addition of the kernels. However, complete data is still required. Given such a fast heuristic, I'm more amenable to augmenting this method with additional heuristics to clean up or improve the segmentation if necessary. We can also use it to initialize our more sophisticated HMM models, as well. @LeeTL1220 @mbabadi @davidbenjamin I'd be interested to hear your thoughts, if you have any.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-321121666:1522,perform,performs,1522,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-321121666,1,['perform'],['performs']
Performance,"015f-0a1b-f1bd-00002ce33928 ; on database directory /tmp/spark-98953d35-8594-4907-b4a5-0870f1d17b3e/metastore with class loader sun.misc.Launcher$AppClassLoader@5c647e05 ; Loaded from file:/opt/cloudera/parcels/CDH-5.12.1-1.cdh5.12.1.p0.3/jars/derby-10.11.1.1.jar; java.vendor=Oracle Corporation; java.runtime.version=1.8.0_91-b14; user.dir=/opt/Software/gatk; os.name=Linux; os.arch=amd64; os.version=3.10.0-514.el7.x86_64; derby.system.home=null; Database Class Loader started - derby.database.classpath=''; 17/10/11 14:25:33 WARN metastore.ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.1.0-cdh5.12.1; 17/10/11 14:25:33 WARN metastore.ObjectStore: Failed to get database default, returning NoSuchObjectException; SQL context available as sqlContext. **./gradlew bundle**; **[root@com1 gatk]# ./gradlew bundle; when I executed the command ”./gradlew bundle”， it appeared the error in the last ，did this matter？**. .......; [loading ZipFileIndexFileObject[/root/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.core/jackson-databind/2.6.5/d50be1723a09be903887099ff2014ea9020333/jackson-databind-2.6.5.jar(com/fasterxml/jackson/databind/annotation/JsonSerialize$Inclusion.class)]]; [loading ZipFileIndexFileObject[/root/.gradle/caches/modules-2/files-2.1/org.apache.logging.log4j/log4j-core/2.5/7ed845de1dfe070d43511fab1784e6c4118398/log4j-core-2.5.jar(org/apache/logging/log4j/core/config/plugins/PluginVisitorStrategy.class)]]; [done in 5759 ms]; 1 error; :gatkTabComplete FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':gatkTabComplete'.; > Javadoc generation failed. Generated Javadoc options file (useful for troubleshooting): '/opt/Software/gatk/build/tmp/gatkTabComplete/jadoc.options'. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 7.431 secs",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-335696240:2897,load,loading,2897,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-335696240,4,"['cache', 'load']","['caches', 'loading']"
Performance,"05f-a3fa-41fc-8d17-a2c55d875eab/call-CHMSampleHeadToHead/BenchmarkComparison/1fb97a8b-caee-4184-8e36-be21e6c43549/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""CHM controlindelF1Score"": ""0.8724"",; ""CHM controlindelPrecision"": ""0.8814"",; ""CHM controlsnpF1Score"": ""0.9784"",; ""CHM controlsnpPrecision"": ""0.9706"",; ""CHM controlsnpRecall"": ""0.9863"",; ""CHM controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/6ea2705f-a3fa-41fc-8d17-a2c55d875eab/call-CHMSampleHeadToHead/BenchmarkComparison/1fb97a8b-caee-4184-8e36-be21e6c43549/call-BenchmarkVCFControlSample/Benchmark/3b068fb2-7140-4c1e-8860-df8df21821ec/call-CombineSummaries/summary.csv"",; ""CHM evalHCprocesshours"": ""80.5165222222222"",; ""CHM evalHCsystemhours"": ""0.1713305555555555"",; ""CHM evalHCwallclockhours"": ""53.10978888888891"",; ""CHM evalHCwallclockmax"": ""2.7458416666666667"",; ""CHM evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/6ea2705f-a3fa-41fc-8d17-a2c55d875eab/call-CHMSampleHeadToHead/BenchmarkComparison/1fb97a8b-caee-4184-8e36-be21e6c43549/call-EVALRuntimeTask/cacheCopy/monitoring.pdf"",; ""CHM evalindelF1Score"": ""0.8724"",; ""CHM evalindelPrecision"": ""0.8814"",; ""CHM evalsnpF1Score"": ""0.9784"",; ""CHM evalsnpPrecision"": ""0.9706"",; ""CHM evalsnpRecall"": ""0.9863"",; ""CHM evalsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/6ea2705f-a3fa-41fc-8d17-a2c55d875eab/call-CHMSampleHeadToHead/BenchmarkComparison/1fb97a8b-caee-4184-8e36-be21e6c43549/call-BenchmarkVCFTestSample/Benchmark/7f7c4522-e293-4a03-ada8-9541a585250b/call-CombineSummaries/summary.csv"",; ""EXOME1 controlindelF1Score"": ""0.727"",; ""EXOME1 controlindelPrecision"": ""0.632"",; ""EXOME1 controlsnpF1Score"": ""0.9878"",; ""EXOME1 controlsnpPrecision"": ""0.9815"",; ""EXOME1 controlsnpRecall"": ""0.9941"",; ""EXOME1 controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/6ea2705f-a3fa-41fc-8d17-a2c55d875eab/call-EXOME1Sampl",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7651#issuecomment-1069766207:11445,cache,cacheCopy,11445,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7651#issuecomment-1069766207,1,['cache'],['cacheCopy']
Performance,07/25 01:37:55 Done container setup.; 2020/07/25 01:37:56 Starting localization.; 2020/07/25 01:38:02 Localization script execution started...; 2020/07/25 01:38:02 Localizing input gs://gatk-test-data/mutect2/Homo_sapiens_assembly38.index_bundle -> /cromwell_root/gatk-test-data/mutect2/Homo_sapiens_assembly38.index_bundle; 2020/07/25 01:38:40 Localizing input gs://fc-ac4624cb-a8fc-49a2-b071-d3a0ae799418/cb1feccb-0a69-42bf-ba5f-fde762934a59/Mutect2/fe3623c8-0eaf-4cd4-9f81-d1fda4073f2e/call-FilterAlignmentArtifacts/attempt-3/script -> /cromwell_root/script; 2020/07/25 01:38:45 Localization script execution complete.; 2020/07/25 01:38:58 Done localization.; 2020/07/25 01:38:59 Running user action: docker run -v /mnt/local-disk:/cromwell_root --entrypoint= us.gcr.io/broad-gatk/gatk@sha256:8051adab0ff725e7e9c2af5997680346f3c3799b2df3785dd51d4abdd3da747b /bin/bash /cromwell_root/script; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp.6c58e0ba; 01:39:02.909 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/gatk/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; 01:39:02.925 INFO NativeLibraryLoader - Loading libgkl_smithwaterman.so from jar:file:/gatk/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_smithwaterman.so; 01:39:02.927 INFO SmithWatermanAligner - Using AVX accelerated SmithWaterman implementation; 01:39:03.142 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 01:39:03.361 INFO FilterAlignmentArtifacts - ------------------------------------------------------------; 01:39:03.361 INFO FilterAlignmentArtifacts - The Genome Analysis Toolkit (GATK) v4.1.8.1; 01:39:03.361 INFO FilterAlignmentArtifacts - For support and documentation go to https://software.broadinstitute.org/gatk/; 01:39:03.362 INFO FilterAlignmentArtifacts - Executing as root@3f245e278eba on Linux v4.19.112+ amd64; 01:39:03.362 INFO FilterAlig,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-664539860:1941,Load,Loading,1941,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-664539860,1,['Load'],['Loading']
Performance,"08 INFO GermlineCNVCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 08:37:16.408 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 08:37:16.408 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 08:37:16.408 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 08:37:16.408 INFO GermlineCNVCaller - Deflater: IntelDeflater; 08:37:16.409 INFO GermlineCNVCaller - Inflater: IntelInflater; 08:37:16.409 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 08:37:16.409 INFO GermlineCNVCaller - Requester pays: disabled; 08:37:16.409 INFO GermlineCNVCaller - Initializing engine; 08:37:21.698 INFO GermlineCNVCaller - Done initializing engine; 08:37:22.015 INFO GermlineCNVCaller - Retrieving intervals from read-count file (results/200219_X008378.counts.tsv)...; 08:37:22.119 INFO GermlineCNVCaller - No annotated intervals were provided...; 08:37:22.120 INFO GermlineCNVCaller - No GC-content annotations for intervals found; explicit GC-bias correction will not be performed...; 08:37:22.194 INFO GermlineCNVCaller - Running the tool in the COHORT mode...; 08:37:22.195 INFO GermlineCNVCaller - Shutting down engine; [February 26, 2019 8:37:22 AM GMT] org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller done. Elapsed time: 0.29 minutes.; Runtime.totalMemory()=330301440; java.lang.IllegalArgumentException: Output directory results/190226.181217_K00178.CNVCaller does not exist.; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:724); at org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller.validateArguments(GermlineCNVCaller.java:361); at org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller.doWork(GermlineCNVCaller.java:281); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broa",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4825#issuecomment-467406398:15383,perform,performed,15383,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4825#issuecomment-467406398,1,['perform'],['performed']
Performance,"0; 15/07/14 13:14:53 INFO spark.ContextCleaner: Cleaned broadcast 0; 15/07/14 13:14:53 ERROR executor.Executor: Exception in task 0.0 in stage 1.0 (TID 1); java.lang.IncompatibleClassChangeError: Found class org.apache.hadoop.mapreduce.TaskAttemptContext, but interface was expected; at com.cloudera.dataflow.spark.TemplatedTextOutputFormat.getOutputFile(TemplatedTextOutputFormat.java:50); at com.cloudera.dataflow.spark.TemplatedTextOutputFormat.getDefaultWorkFile(TemplatedTextOutputFormat.java:46); at org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:125); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$12.apply(PairRDDFunctions.scala:995); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$12.apply(PairRDDFunctions.scala:979); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61); at org.apache.spark.scheduler.Task.run(Task.scala:64); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 15/07/14 13:14:53 ERROR util.SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker-0,5,main]; java.lang.IncompatibleClassChangeError: Found class org.apache.hadoop.mapreduce.TaskAttemptContext, but interface was expected; at com.cloudera.dataflow.spark.TemplatedTextOutputFormat.getOutputFile(TemplatedTextOutputFormat.java:50); at com.cloudera.dataflow.spark.TemplatedTextOutputFormat.getDefaultWorkFile(TemplatedTextOutputFormat.java:46); at org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:125); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$12.apply(PairRDDFunctions.scala:995); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$12.apply(PairRDDFunctions.scala:979); at org.apache.spark.scheduler.ResultTask.runTask",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/639#issuecomment-121313713:29404,concurren,concurrent,29404,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/639#issuecomment-121313713,1,['concurren'],['concurrent']
Performance,0b29scy9zcGFyay9waXBlbGluZXMvUmVhZHNQaXBlbGluZVNwYXJrLmphdmE=) | `0% <0%> (-89.583%)` | `0% <0%> (-12%)` | |; | [...adinstitute/hellbender/engine/ReadContextData.java](https://codecov.io/gh/broadinstitute/gatk/pull/5251/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUmVhZENvbnRleHREYXRhLmphdmE=) | `0% <0%> (-70.37%)` | `0% <0%> (-6%)` | |; | [...n/java/org/broadinstitute/hellbender/utils/KV.java](https://codecov.io/gh/broadinstitute/gatk/pull/5251/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9LVi5qYXZh) | `0% <0%> (-57.143%)` | `0% <0%> (-5%)` | |; | [...walkers/genotyper/afcalc/AFCalculatorProvider.java](https://codecov.io/gh/broadinstitute/gatk/pull/5251/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9hZmNhbGMvQUZDYWxjdWxhdG9yUHJvdmlkZXIuamF2YQ==) | `22.222% <0%> (-44.444%)` | `2% <0%> (-2%)` | |; | [...notyper/afcalc/ConcurrentAFCalculatorProvider.java](https://codecov.io/gh/broadinstitute/gatk/pull/5251/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9hZmNhbGMvQ29uY3VycmVudEFGQ2FsY3VsYXRvclByb3ZpZGVyLmphdmE=) | `50% <0%> (-33.333%)` | `1% <0%> (-1%)` | |; | [...ls/funcotator/metadata/VcfFuncotationMetadata.java](https://codecov.io/gh/broadinstitute/gatk/pull/5251/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9mdW5jb3RhdG9yL21ldGFkYXRhL1ZjZkZ1bmNvdGF0aW9uTWV0YWRhdGEuamF2YQ==) | `71.429% <0%> (-28.571%)` | `8% <0%> (+3%)` | |; | [...titute/hellbender/engine/TwoPassVariantWalker.java](https://codecov.io/gh/broadinstitute/gatk/pull/5251/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvVHdvUGFzc1ZhcmlhbnRXYWxrZXIuamF2YQ==) | `69.231% <0%> (-26.007%)` | `6% <0%> (+2%)` | |; | ... and [600 more](https://codecov.io/gh/broadinstitute/gatk/pull/5251/diff?src=pr&,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5251#issuecomment-426437671:3052,Concurren,ConcurrentAFCalculatorProvider,3052,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5251#issuecomment-426437671,1,['Concurren'],['ConcurrentAFCalculatorProvider']
Performance,"1	NM:i:7	MQ:i:60	AS:i:105	XS:i:20; EOF. bind 'set disable-completion off'. samtools view reads.sam -b > reads.bam; samtools index reads.bam. gatk HaplotypeCaller -R chr19.fa -I reads.bam -O output.g.vcf -ERC GVCF ; bcftools view output.g.vcf -c1 | bcftools annotate -x INFO,FORMAT/SB,FORMAT/PL | tail ; gatk GenotypeGVCFs -R chr19.fa -V output.g.vcf -O output.vcf ; bcftools view output.vcf -c1 | bcftools annotate -x INFO,FORMAT/SB,FORMAT/PL | tail. ```. output:; ```; Using GATK jar /omics/groups/OE0540/internal/software/jvm/gatk/4.4.0.0/gatk-package-4.4.0.0-local.jar Running: java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /omics/groups/OE0540/internal/software/jvm/gatk/4.4.0.0/gatk-package-4.4.0.0-local.jar HaplotypeCaller -R chr19.fa -I reads.bam -O output.g.vcf -ERC GVCF Picked up JAVA_TOOL_OPTIONS: -Djava.net.useSystemProxies=true 13:39:56.569 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/omics/groups/OE0540/internal/software/jvm/gatk/4.4.0.0/gatk-package-4.4.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so 13:39:56.647 INFO HaplotypeCaller - ------------------------------------------------------------ 13:39:56.660 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.4.0.0 13:39:56.666 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/ 13:39:56.666 INFO HaplotypeCaller - Executing as gleixner@odcf-worker02 on Linux v3.10.0-1160.76.1.el7.x86_64 amd64 13:39:56.666 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v17+35-2724 13:39:56.667 INFO HaplotypeCaller - Start Date/Time: October 26, 2023 at 1:39:56 PM CEST 13:39:56.667 INFO HaplotypeCaller - ------------------------------------------------------------ 13:39:56.667 INFO HaplotypeCaller - ------------------------------------------------------------ 13:39:56.669 INFO HaplotypeCaller - HTSJDK Ver",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5727#issuecomment-1781017195:11059,Load,Loading,11059,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5727#issuecomment-1781017195,1,['Load'],['Loading']
Performance,"1) we demonstrate that parameters don't have much of an effect and can be consolidated, or 2) we find more optimal sets of parameters. Potentially we could also show that 3) our parameters are already optimal (I'd say this would be by pure dumb luck), in which case we could at least demonstrate and document some justification for them. If the parameters don't have much of an impact on NA12878, I'm curious to see whether this holds for low coverage or messier data---and ultimately, in malaria. Just starting with NA12878 because of the availability of truth and the potential impact for the primary use case of calling in human data. Some preliminary results: I ran the aforementioned comparison on chr22 with 1) 4.1.8.1 master and 2) 4.1.8.1 with haplotype-to-reference SW parameters changed from `NEW_SW_PARAMETERS` to `STANDARD_NGS` on two replicates of NA12878 (O1D1 and O2D2 from the 2018 NovaSeq snapshot experiment). On each replicate, 2) demonstrated slightly lower performance, but it was well within the sample-to-sample variation between these two replicates. Here are the corresponding vcfeval summaries:. ```; ::::::::::::::; NA12878/O1D1/4.1.8.1/summary.txt; ::::::::::::::; Threshold True-pos-baseline True-pos-call False-pos False-neg Precision Sensitivity F-measure; ----------------------------------------------------------------------------------------------------; 84.000 40778 40780 35116 1412 0.5373 0.9665 0.6907; None 41994 41994 43760 196 0.4897 0.9954 0.6564; ::::::::::::::; NA12878/O1D1/STANDARD_NGS/summary.txt; ::::::::::::::; Threshold True-pos-baseline True-pos-call False-pos False-neg Precision Sensitivity F-measure; ----------------------------------------------------------------------------------------------------; 84.000 40743 40745 35255 1447 0.5361 0.9657 0.6895; None 41955 41954 43903 235 0.4886 0.9944 0.6553; ::::::::::::::; NA12878/O1D2/4.1.8.1/summary.txt; ::::::::::::::; Threshold True-pos-baseline True-pos-call False-pos False-neg Precision Sen",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-710107566:1711,perform,performance,1711,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-710107566,1,['perform'],['performance']
Performance,"1-07 11:33:52 INFO YarnClientSchedulerBackend:54 - Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> scc-hsn1.scc.bu.edu, PROXY_URI_BASES -> https://scc-hsn1.scc.bu.edu:8090/proxy/application_1542127286896_0153), /proxy/application_1542127286896_0153; 2019-01-07 11:33:52 INFO JettyUtils:54 - Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter; 2019-01-07 11:33:52 INFO Client:54 - Application report for application_1542127286896_0153 (state: ACCEPTED); 2019-01-07 11:33:53 INFO YarnSchedulerBackend$YarnSchedulerEndpoint:54 - ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM); 2019-01-07 11:33:53 INFO Client:54 - Application report for application_1542127286896_0153 (state: RUNNING); 2019-01-07 11:33:53 INFO Client:54 -; client token: Token { kind: YARN_CLIENT_TOKEN, service: }; diagnostics: N/A; ApplicationMaster host: 192.168.18.193; ApplicationMaster RPC port: 0; queue: default; start time: 1546878818531; final status: UNDEFINED; tracking URL: https://scc-hsn1.scc.bu.edu:8090/proxy/application_1542127286896_0153/; user: farrell; 2019-01-07 11:33:53 INFO YarnClientSchedulerBackend:54 - Application application_1542127286896_0153 has started running.; 2019-01-07 11:33:53 INFO Utils:54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45270.; 2019-01-07 11:33:53 INFO NettyBlockTransferService:54 - Server created on scc-hadoop.bu.edu:45270; 2019-01-07 11:33:53 INFO BlockManager:54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy; 2019-01-07 11:33:53 INFO BlockManagerMaster:54 - Registering BlockManager BlockManagerId(driver, scc-hadoop.bu.edu, 45270, None); 2019-01-07 11:33:53 INFO BlockManagerMasterEndpoint:54 - Registering block manager scc-hadoop.bu.edu:45270 with 408.6 MB RAM, BlockManagerId(driver, scc-hadoop.bu.edu, 45270, None); 2019-01-07 11:33:53 INFO BlockManagerMaster:54 - Register",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:15896,queue,queue,15896,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,1,['queue'],['queue']
Performance,"1-09 13:35:32 INFO Client:54 - Application report for application_1542127286896_0166 (state: ACCEPTED); 2019-01-09 13:35:33 INFO YarnClientSchedulerBackend:54 - Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> scc-hsn1.scc.bu.edu, PROXY_URI_BASES -> https://scc-hsn1.scc.bu.edu:8090/proxy/application_1542127286896_0166), /proxy/application_1542127286896_0166; 2019-01-09 13:35:33 INFO JettyUtils:54 - Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter; 2019-01-09 13:35:33 INFO YarnSchedulerBackend$YarnSchedulerEndpoint:54 - ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM); 2019-01-09 13:35:33 INFO Client:54 - Application report for application_1542127286896_0166 (state: RUNNING); 2019-01-09 13:35:33 INFO Client:54 -; client token: Token { kind: YARN_CLIENT_TOKEN, service: }; diagnostics: N/A; ApplicationMaster host: 192.168.18.195; ApplicationMaster RPC port: 0; queue: default; start time: 1547058922320; final status: UNDEFINED; tracking URL: https://scc-hsn1.scc.bu.edu:8090/proxy/application_1542127286896_0166/; user: farrell; 2019-01-09 13:35:33 INFO YarnClientSchedulerBackend:54 - Application application_1542127286896_0166 has started running.; 2019-01-09 13:35:33 INFO Utils:54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43627.; 2019-01-09 13:35:33 INFO NettyBlockTransferService:54 - Server created on scc-hadoop.bu.edu:43627; 2019-01-09 13:35:33 INFO BlockManager:54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy; 2019-01-09 13:35:33 INFO BlockManagerMaster:54 - Registering BlockManager BlockManagerId(driver, scc-hadoop.bu.edu, 43627, None); 2019-01-09 13:35:33 INFO BlockManagerMasterEndpoint:54 - Registering block manager scc-hadoop.bu.edu:43627 with 372.6 MB RAM, BlockManagerId(driver, scc-hadoop.bu.edu, 43627, None); 2019-01-09 13:35:33 INFO BlockManagerMaster:54 - Register",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:15195,queue,queue,15195,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,1,['queue'],['queue']
Performance,"1. Why can't standard tools operate on the distributed workspaces? You could run `GenotypeGVCFs` on these workspaces in a distributed fashion and then concatenate the results together if you want. I think you were initially considering this route - and still think it would be more performant. 2. Again, you can process/query a whatever set of intervals you want -- in a distributed fashion, right?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6620#issuecomment-640791782:282,perform,performant,282,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6620#issuecomment-640791782,1,['perform'],['performant']
Performance,"113 / 643 of them were CNV test files that I deleted in #3907, which is a shade bit more than 1%... only a small fraction of these are loaded dynamically or are index/dict files, which tests will catch (and they have already, now that I check---6 / 113). I will concede that it is likely that most of the remaining files are index files, etc., but I do see a few more bams, etc. that could stand deletion. I'm curious as to what the best way to do this sort of thing actually is!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3905#issuecomment-348598077:135,load,loaded,135,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3905#issuecomment-348598077,1,['load'],['loaded']
Performance,"119579965 4.4 5479000 1242549.2; 15:39:35.700 INFO ProgressMeter - 11:118752077 4.6 5530000 1207397.2; 15:39:46.028 INFO ProgressMeter - 12:58875536 4.8 5592000 1176709.9; 15:39:56.079 INFO ProgressMeter - 13:37022394 4.9 5628000 1143956.7; 15:40:06.117 INFO ProgressMeter - 16:14727212 5.1 5728000 1125992.7; 15:40:16.383 INFO SplitNCigarReads - Shutting down engine; [March 2, 2023 3:40:16 PM EST] org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads done. Elapsed time: 5.27 minutes.; Runtime.totalMemory()=3432513536; java.lang.ClassCastException: htsjdk.samtools.BAMRecord cannot be cast to java.lang.Comparable; 	at java.util.Arrays$NaturalOrder.compare(Arrays.java:102); 	at java.util.TimSort.countRunAndMakeAscending(TimSort.java:355); 	at java.util.TimSort.sort(TimSort.java:234); 	at java.util.ArraysParallelSortHelpers$FJObject$Sorter.compute(ArraysParallelSortHelpers.java:145); 	at java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:731); 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); 	at java.util.concurrent.ForkJoinTask.doInvoke(ForkJoinTask.java:401); 	at java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:734); 	at java.util.Arrays.parallelSort(Arrays.java:1180); 	at htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:247); 	at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:182); 	at htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:202); 	at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:36); 	at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:16); 	at htsjdk.samtools.util.AbstractAsyncWriter$WriterRunnable.run(AbstractAsyncWriter.java:123); 	at java.lang.Thread.run(Thread.java:750); 	Suppressed: htsjdk.samtools.util.RuntimeIOException: Attempt to add record to closed writer.; 		at htsjdk.samtools.util.AbstractAsyncWriter.write(AbstractAsyncWriter.java:57); 		at htsjdk.samtools.AsyncSAM",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452525485:6178,concurren,concurrent,6178,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452525485,1,['concurren'],['concurrent']
Performance,"11:57.862 INFO IntelPairHmm - Requested threads: 4; 19:11:57.862 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 19:11:57.862 INFO ProgressMeter - Starting traversal; 19:11:57.862 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; *** glibc detected *** /for/bar/bin/java: double free or corruption (out): 0x00007f450af58700 ***; ======= Backtrace: =========; /lib64/libc.so.6(+0x3d01675dee)[0x7f45058afdee]; /lib64/libc.so.6(+0x3d01678c80)[0x7f45058b2c80]; /tmp/libgkl_smithwaterman410767516409374085.so(_Z19runSWOnePairBT_avx2iiiiPhS_iiaPcPs+0x338)[0x7f4499f4cfa8]; /tmp/libgkl_smithwaterman410767516409374085.so(Java_com_intel_gkl_smithwaterman_IntelSmithWaterman_alignNative+0xd8)[0x7f4499f4cbf8]; [0x7f44f58be6a2]; ======= Memory map: ========; ```. Then we **disabled** AVX2 in the newer cluster using Intels [sde64](https://software.intel.com/en-us/articles/intel-software-development-emulator) with `-ivb`, which directed GATK to use the Java implementation, and the filter worked without core dump. ```; sde64 -ivb -- faa.sh; Using GATK jar /app/gatk-package-4.1.8.0-local.jar; Running:; /bin/java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /app/gatk-package-4.1.8.0-local.jar FilterAlignmentArtifacts -V /output/sample.FilterMutectCalls.vcf.gz -R /db/hs37d5.fa --bwa-mem-index-image /ref/hg38.fa.img -I /output/sample.Mutect2.bam -O sample.somatic_filter2.test.vcf.gz --use-jdk-inflater true --use-jdk-deflater true; 19:41:38.956 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/app/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 19:41:39.332 INFO SmithWatermanAligner - AVX accelerated SmithWaterman implementation is not supported, falling back to the Java implementation; ```; Hope this helps and we're looking forward the GKL fix. Cheers,; Richard",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-660645356:5815,Load,Loading,5815,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-660645356,1,['Load'],['Loading']
Performance,133GB file on NFS; 1490.96 seconds (95.4 MBytes per second). 30 GB file on NFS; real 5m18.298s (116.6 Mbytes per second); user 5m5.198s. same 30GB file on gcloud:; real 6m46.984s (91.3 Mbytes per second); user 6m6.124s; sys 0m35.352s. 1.6GB on NFS ; real 0m18.243s (91.9 Mbytes per second); user 0m17.073s; sys 0m0.595s. From this I conclude for now that it's not a huge deal - it will become a bottleneck at some point but we're a while away from that point.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1690#issuecomment-211945863:395,bottleneck,bottleneck,395,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1690#issuecomment-211945863,1,['bottleneck'],['bottleneck']
Performance,"14-cd1f-4563-8d8a-abf6b6ca7883/call-CHMSampleHeadToHead/BenchmarkComparison/7ff0db7c-0871-4cda-95f3-fa75436cbb21/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""CHM controlindelF1Score"": ""0.8778"",; ""CHM controlindelPrecision"": ""0.8968"",; ""CHM controlsnpF1Score"": ""0.9813"",; ""CHM controlsnpPrecision"": ""0.9774"",; ""CHM controlsnpRecall"": ""0.9852"",; ""CHM controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/e372bd14-cd1f-4563-8d8a-abf6b6ca7883/call-CHMSampleHeadToHead/BenchmarkComparison/7ff0db7c-0871-4cda-95f3-fa75436cbb21/call-BenchmarkVCFControlSample/Benchmark/16cd1efe-5cea-403e-8e85-aec15e71bd1d/call-CombineSummaries/summary.csv"",; ""CHM evalHCprocesshours"": ""67.35536666666667"",; ""CHM evalHCsystemhours"": ""0.1557166666666667"",; ""CHM evalHCwallclockhours"": ""42.53388888888889"",; ""CHM evalHCwallclockmax"": ""2.7197444444444443"",; ""CHM evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/e372bd14-cd1f-4563-8d8a-abf6b6ca7883/call-CHMSampleHeadToHead/BenchmarkComparison/7ff0db7c-0871-4cda-95f3-fa75436cbb21/call-EVALRuntimeTask/cacheCopy/monitoring.pdf"",; ""CHM evalindelF1Score"": ""0.8778"",; ""CHM evalindelPrecision"": ""0.8968"",; ""CHM evalsnpF1Score"": ""0.9813"",; ""CHM evalsnpPrecision"": ""0.9774"",; ""CHM evalsnpRecall"": ""0.9852"",; ""CHM evalsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/e372bd14-cd1f-4563-8d8a-abf6b6ca7883/call-CHMSampleHeadToHead/BenchmarkComparison/7ff0db7c-0871-4cda-95f3-fa75436cbb21/call-BenchmarkVCFTestSample/Benchmark/2071078a-158e-4c3e-9b2f-907bd501821b/call-CombineSummaries/summary.csv"",; ""EXOME1 controlindelF1Score"": ""0.7573"",; ""EXOME1 controlindelPrecision"": ""0.6882"",; ""EXOME1 controlsnpF1Score"": ""0.9896"",; ""EXOME1 controlsnpPrecision"": ""0.9852"",; ""EXOME1 controlsnpRecall"": ""0.9941"",; ""EXOME1 controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/e372bd14-cd1f-4563-8d8a-abf6b6ca7883/call-EXOME1Sam",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7723#issuecomment-1069381494:11446,cache,cacheCopy,11446,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7723#issuecomment-1069381494,1,['cache'],['cacheCopy']
Performance,"14:56:20.999 INFO IntelGKLUtils - Trying to load Intel GKL library from:; 	jar:file:/mnt/raid5/frankliu/code/GATK/gatk/build/libs/gatk-package-4.alpha.2-120-g00a40ea-SNAPSHOT-spark.jar!/com/intel/gkl/native/libgkl_compression.so; Exception in thread ""main"" java.lang.UnsatisfiedLinkError: /tmp/frankliu/libgkl_compression8271576113600851848.so: /tmp/frankliu/libgkl_compression8271576113600851848.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64-bit platform); 	at java.lang.ClassLoader$NativeLibrary.load(Native Method); 	at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941); 	at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824); 	at java.lang.Runtime.load0(Runtime.java:809); 	at java.lang.System.load(System.java:1086); 	at com.intel.gkl.IntelGKLUtils.load(IntelGKLUtils.java:133); 	at com.intel.gkl.compression.IntelDeflater.load(IntelDeflater.java:58); 	at com.intel.gkl.compression.IntelDeflater.load(IntelDeflater.java:53); 	at com.intel.gkl.compression.IntelDeflaterFactory.<init>(IntelDeflaterFactory.java:16); 	at com.intel.gkl.compression.IntelDeflaterFactory.<init>(IntelDeflaterFactory.java:20); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:143); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116); 	at org.broadinstitute.hellbender.Main.main(Main.java:158); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.S",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2302#issuecomment-265854885:2068,load,load,2068,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2302#issuecomment-265854885,1,['load'],['load']
Performance,"1625"",; ""NIST controlMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/2a8ce326-baa5-4052-bff9-bd684393ff6c/call-NISTSampleHeadToHead/BenchmarkComparison/d1047505-b7bc-455d-851f-fbed8d81e895/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""NIST controlindelF1Score"": ""0.9902"",; ""NIST controlindelPrecision"": ""0.9903"",; ""NIST controlsnpF1Score"": ""0.9899"",; ""NIST controlsnpPrecision"": ""0.9887"",; ""NIST controlsnpRecall"": ""0.9911"",; ""NIST controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/2a8ce326-baa5-4052-bff9-bd684393ff6c/call-NISTSampleHeadToHead/BenchmarkComparison/d1047505-b7bc-455d-851f-fbed8d81e895/call-BenchmarkVCFControlSample/Benchmark/5388d7b6-6bcd-451d-9a4e-925b386ecd0c/call-CombineSummaries/summary.csv"",; ""NIST evalHCprocesshours"": ""95.03499722222222"",; ""NIST evalHCsystemhours"": ""0.17304166666666665"",; ""NIST evalHCwallclockhours"": ""67.81165555555557"",; ""NIST evalHCwallclockmax"": ""3.691061111111111"",; ""NIST evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/2a8ce326-baa5-4052-bff9-bd684393ff6c/call-NISTSampleHeadToHead/BenchmarkComparison/d1047505-b7bc-455d-851f-fbed8d81e895/call-EVALRuntimeTask/monitoring.pdf"",; ""NIST evalindelF1Score"": ""0.9902"",; ""NIST evalindelPrecision"": ""0.9903"",; ""NIST evalsnpF1Score"": ""0.9899"",; ""NIST evalsnpPrecision"": ""0.9887"",; ""NIST evalsnpRecall"": ""0.9911"",; ""NIST evalsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/2a8ce326-baa5-4052-bff9-bd684393ff6c/call-NISTSampleHeadToHead/BenchmarkComparison/d1047505-b7bc-455d-851f-fbed8d81e895/call-BenchmarkVCFTestSample/Benchmark/faae76f3-8378-4271-9822-5d2587113415/call-CombineSummaries/summary.csv"",; ""ROC_Plots_Reported"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/2a8ce326-baa5-4052-bff9-bd684393ff6c/call-CreateHTMLReport/cacheCopy/report.html""; },; ""errors"": null; } ; </pre> </details>",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7876#issuecomment-1194801748:21372,cache,cacheCopy,21372,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7876#issuecomment-1194801748,1,['cache'],['cacheCopy']
Performance,1713); 	at shaded.cloud_nio.com.google.api.gax.core.ApiFutures.addCallback(ApiFutures.java:47); 	at shaded.cloud_nio.com.google.api.gax.retrying.RetryingFutureImpl.setAttemptFuture(RetryingFutureImpl.java:107); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:100); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:47); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:125); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:109); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:131); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:104); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: java.net.SocketTimeoutException: Read timed out; 	at java.net.SocketInputStream.socketRead0(Native Method); 	at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); 	at java.net.SocketInputStream.read(SocketInputStream.java:171); 	at java.net.SocketInputStream.read(SocketInputStream.java:141); 	at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); 	at sun.security.ssl.InputRecord.read(InputRecord.java:503); 	at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973); 	at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:930); 	at sun.security.ssl.AppInputStream.read(AppInputStream.java:105); 	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286); 	at java.io.BufferedInputStream.read(BufferedInputStream.java:345); 	at sun.net.www.http.HttpCl,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300298180:6077,concurren,concurrent,6077,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300298180,1,['concurren'],['concurrent']
Performance,"20-g00a40ea-SNAPSHOT-spark.jar; Running:; /mnt/raid5/frankliu/code/SPARK/spark-2.0.2//bin/spark-submit --master yarn --conf spark.kryoserializer.buffer.max=512m --conf spark.driver.maxResultSize=0 --conf spark.driver.userClassPathFirst=true --conf spark.io.compression.codec=lzf --conf spark.yarn.executor.memoryOverhead=600 --conf spark.driver.extraJavaOptions=-Dsamjdk.compression_level=1 -DGATK_STACKTRACE_ON_USER_EXCEPTION=true --conf spark.executor.extraJavaOptions=-Dsamjdk.compression_level=1 -DGATK_STACKTRACE_ON_USER_EXCEPTION=true --deploy-mode client --num-executors 59 --executor-cores 4 --executor-memory 24180M --driver-memory 10G /mnt/raid5/frankliu/code/GATK/gatk/build/libs/gatk-package-4.alpha.2-120-g00a40ea-SNAPSHOT-spark.jar CountReadsSpark -I hdfs://arlab174:54310/GATK4TEST/BroadData/CEUTrio.HiSeq.WEx.b37.NA12892.bam -O hdfs://arlab174:54310/GATK4TEST/Output/Test_CEU_ReadsCount --sparkMaster yarn; 14:56:20.999 INFO IntelGKLUtils - Trying to load Intel GKL library from:; 	jar:file:/mnt/raid5/frankliu/code/GATK/gatk/build/libs/gatk-package-4.alpha.2-120-g00a40ea-SNAPSHOT-spark.jar!/com/intel/gkl/native/libgkl_compression.so; Exception in thread ""main"" java.lang.UnsatisfiedLinkError: /tmp/frankliu/libgkl_compression8271576113600851848.so: /tmp/frankliu/libgkl_compression8271576113600851848.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64-bit platform); 	at java.lang.ClassLoader$NativeLibrary.load(Native Method); 	at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941); 	at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824); 	at java.lang.Runtime.load0(Runtime.java:809); 	at java.lang.System.load(System.java:1086); 	at com.intel.gkl.IntelGKLUtils.load(IntelGKLUtils.java:133); 	at com.intel.gkl.compression.IntelDeflater.load(IntelDeflater.java:58); 	at com.intel.gkl.compression.IntelDeflater.load(IntelDeflater.java:53); 	at com.intel.gkl.compression.IntelDeflaterFactory.<init>",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2302#issuecomment-265854885:1121,load,load,1121,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2302#issuecomment-265854885,1,['load'],['load']
Performance,"2; > 15:39:25.463 INFO ProgressMeter - 10:119579965 4.4 5479000 1242549.2; > 15:39:35.700 INFO ProgressMeter - 11:118752077 4.6 5530000 1207397.2; > 15:39:46.028 INFO ProgressMeter - 12:58875536 4.8 5592000 1176709.9; > 15:39:56.079 INFO ProgressMeter - 13:37022394 4.9 5628000 1143956.7; > 15:40:06.117 INFO ProgressMeter - 16:14727212 5.1 5728000 1125992.7; > 15:40:16.383 INFO SplitNCigarReads - Shutting down engine; > [March 2, 2023 3:40:16 PM EST]; > org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads done.; > Elapsed time: 5.27 minutes.; > Runtime.totalMemory()=3432513536; > java.lang.ClassCastException: htsjdk.samtools.BAMRecord cannot be cast to; > java.lang.Comparable; > at java.util.Arrays$NaturalOrder.compare(Arrays.java:102); > at java.util.TimSort.countRunAndMakeAscending(TimSort.java:355); > at java.util.TimSort.sort(TimSort.java:234); > at; > java.util.ArraysParallelSortHelpers$FJObject$Sorter.compute(ArraysParallelSortHelpers.java:145); > at java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:731); > at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); > at java.util.concurrent.ForkJoinTask.doInvoke(ForkJoinTask.java:401); > at java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:734); > at java.util.Arrays.parallelSort(Arrays.java:1180); > at; > htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:247); > at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:182); > at; > htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:202); > at; > htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:36); > at; > htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:16); > at; > htsjdk.samtools.util.AbstractAsyncWriter$WriterRunnable.run(AbstractAsyncWriter.java:123); > at java.lang.Thread.run(Thread.java:750); > Suppressed: htsjdk.samtools.util.RuntimeIOException: Attempt to add record; > to closed writer.; > at; ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452528344:6543,concurren,concurrent,6543,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452528344,1,['concurren'],['concurrent']
Performance,"2_BM.microbe_aligned.paired.bam:33554432+33554432; 20/07/17 09:38:46 ERROR Executor: Exception in task 0.0 in stage 2.0 (TID 5); java.util.NoSuchElementException: next on empty iterator; 	at scala.collection.Iterator$$anon$2.next(Iterator.scala:39); 	at scala.collection.Iterator$$anon$2.next(Iterator.scala:37); 	at scala.collection.Iterator$$anon$13.next(Iterator.scala:469); 	at scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:31); 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.Iterators$PeekingImpl.next(Iterators.java:1155); 	at org.broadinstitute.hellbender.utils.spark.SparkUtils.lambda$putReadsWithTheSameNameInTheSamePartition$7bd206b0$1(SparkUtils.java:190); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:153); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:153); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:123); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748)`. Looking at the aligned bams that go into the scoring task, they don't appear to be empty or different to the rest of the cohort. Any thoughts?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6319#issuecomment-660292360:2064,concurren,concurrent,2064,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6319#issuecomment-660292360,2,['concurren'],['concurrent']
Performance,38.vcf.gz --known-sites /data/reference/gatk_resource/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz -O PAAD11N.recal_data.test.table; Using GATK jar /data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx8G -Djava.io.tmpdir=./ -jar /data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar BaseRecalibrator -R /data/reference/gatk_resource/Homo_sapiens_assembly38.fasta -I /data/xieduo/Immun_genomics/data/Łuksza_2022_Nature/bam/PAAD11N.bam --known-sites /data/xieduo/WES_pipe/pipeline/gatk_resource/dbsnp_146.hg38.vcf.gz --known-sites /data/reference/gatk_resource/1000G_phase1.snps.high_confidence.hg38.vcf.gz --known-sites /data/reference/gatk_resource/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz -O PAAD11N.recal_data.test.table; 13:46:24.742 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 13:46:24.761 WARN NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (No such file or directory); 13:46:24.764 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 13:46:24.764 WARN NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (No such file or directory); 13:46:24.884 INFO BaseRecalibrator - ------------------------------------------------------------; 13:46:24.884 INFO BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.2.6.1; 13:46:24.885 INFO BaseRecalibrator - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:46:24.885 INFO BaseRecalibrator - Executin,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8005#issuecomment-1254561081:12798,Load,Loading,12798,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005#issuecomment-1254561081,1,['Load'],['Loading']
Performance,38:02 Localizing input gs://gatk-test-data/mutect2/Homo_sapiens_assembly38.index_bundle -> /cromwell_root/gatk-test-data/mutect2/Homo_sapiens_assembly38.index_bundle; 2020/07/25 01:38:40 Localizing input gs://fc-ac4624cb-a8fc-49a2-b071-d3a0ae799418/cb1feccb-0a69-42bf-ba5f-fde762934a59/Mutect2/fe3623c8-0eaf-4cd4-9f81-d1fda4073f2e/call-FilterAlignmentArtifacts/attempt-3/script -> /cromwell_root/script; 2020/07/25 01:38:45 Localization script execution complete.; 2020/07/25 01:38:58 Done localization.; 2020/07/25 01:38:59 Running user action: docker run -v /mnt/local-disk:/cromwell_root --entrypoint= us.gcr.io/broad-gatk/gatk@sha256:8051adab0ff725e7e9c2af5997680346f3c3799b2df3785dd51d4abdd3da747b /bin/bash /cromwell_root/script; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp.6c58e0ba; 01:39:02.909 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/gatk/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; 01:39:02.925 INFO NativeLibraryLoader - Loading libgkl_smithwaterman.so from jar:file:/gatk/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_smithwaterman.so; 01:39:02.927 INFO SmithWatermanAligner - Using AVX accelerated SmithWaterman implementation; 01:39:03.142 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 01:39:03.361 INFO FilterAlignmentArtifacts - ------------------------------------------------------------; 01:39:03.361 INFO FilterAlignmentArtifacts - The Genome Analysis Toolkit (GATK) v4.1.8.1; 01:39:03.361 INFO FilterAlignmentArtifacts - For support and documentation go to https://software.broadinstitute.org/gatk/; 01:39:03.362 INFO FilterAlignmentArtifacts - Executing as root@3f245e278eba on Linux v4.19.112+ amd64; 01:39:03.362 INFO FilterAlignmentArtifacts - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 01:39:03.362 INFO FilterAlignmentArtifacts - Start Date/Time:,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-664539860:2095,Load,Loading,2095,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-664539860,1,['Load'],['Loading']
Performance,3YWxrZXJzL3Zxc3Ivc2NhbGFibGUvRXh0cmFjdFZhcmlhbnRBbm5vdGF0aW9ucy5qYXZh) | `89.062% <ø> (-3.125%)` | :arrow_down: |; | [.../scalable/data/LabeledVariantAnnotationsDatum.java](https://codecov.io/gh/broadinstitute/gatk/pull/8132?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvZGF0YS9MYWJlbGVkVmFyaWFudEFubm90YXRpb25zRGF0dW0uamF2YQ==) | `63.158% <ø> (-5.263%)` | :arrow_down: |; | [...lable/modeling/VariantAnnotationsModelBackend.java](https://codecov.io/gh/broadinstitute/gatk/pull/8132?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvbW9kZWxpbmcvVmFyaWFudEFubm90YXRpb25zTW9kZWxCYWNrZW5kLmphdmE=) | `100.000% <ø> (ø)` | |; | [...sr/scalable/modeling/VariantAnnotationsScorer.java](https://codecov.io/gh/broadinstitute/gatk/pull/8132?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvbW9kZWxpbmcvVmFyaWFudEFubm90YXRpb25zU2NvcmVyLmphdmE=) | `64.706% <ø> (-13.072%)` | :arrow_down: |; | [...able/ExtractVariantAnnotationsIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/8132?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvRXh0cmFjdFZhcmlhbnRBbm5vdGF0aW9uc0ludGVncmF0aW9uVGVzdC5qYXZh) | `98.214% <ø> (+1.548%)` | :arrow_up: |; | [...rs/vqsr/scalable/TrainVariantAnnotationsModel.java](https://codecov.io/gh/broadinstitute/gatk/pull/8132?src=pr&el=tree&utm_medium=referral&utm_source=gith,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8132#issuecomment-1370265333:2711,scalab,scalable,2711,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8132#issuecomment-1370265333,1,['scalab'],['scalable']
Performance,"4 [addAssemblyQNames -> getKmerIntervals] mapPartitionsToPair, then reduceByKey, then mapPartitions; * Job 5 [getAssemblyQNames] mapPartitions twice and a collect; * Job 6 [generateFastqs] mapPartitions and combineByKey, then write FASTQ to files. A few observations:; * Jobs 0,1,3 are simple map jobs - very quick 1-2 mins each.; * Job 2 is a simple MR, with a tiny shuffle to sum by key (<1MB of shuffle data); * Job 4 takes a bit longer longer (3min), and shuffles ~3GB. This is a lot faster than when I ran it before with less memory, when it took 9 min. Is it creating a lot of garbage? If you wanted to speed things up you might look at what this is doing on a local machine and see if there are any opportunities to improve CPU efficiency.; * Job 5 takes ~8 mins, and has no shuffle. CPU intensive processing again?; * Job 6 take a little over 3 mins, shuffling ~3GB. Overall, it looks like it’s performing pretty well. There is very little data being shuffled relative to the size of the input (~6GB to 133GB input), so it’s not worth looking into optimizing the data structures there. The input data is being read multiple times, so it _might_ be worth seeing if it can be cached by Spark to avoid reading from disk over and over again. This is only worth it if you have sufficient memory available across the cluster to hold the input (which will be bigger than the on-disk size) _plus_ enough memory for the processing, which as we saw is quite memory hungry anyway. There might be some CPU efficiencies to pursue, especially if some code paths are creating a lot of objects that need garbage collecting (as Jobs 4 and 5 seem to be). Jobs 4 and 5 seem to have some skew (judging from the task time distribution in the UI). You might investigate this by logging the amount of data that each task processes (or rather than logging, generating another output that is some description of the task data - or use a Spark accumulator), and then seeing if there's some way to make it more uniform.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2458#issuecomment-292171884:2454,optimiz,optimizing,2454,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2458#issuecomment-292171884,2,"['cache', 'optimiz']","['cached', 'optimizing']"
Performance,"4180M --driver-memory 10G /mnt/raid5/frankliu/code/GATK/gatk/build/libs/gatk-package-4.alpha.2-120-g00a40ea-SNAPSHOT-spark.jar CountReadsSpark -I hdfs://arlab174:54310/GATK4TEST/BroadData/CEUTrio.HiSeq.WEx.b37.NA12892.bam -O hdfs://arlab174:54310/GATK4TEST/Output/Test_CEU_ReadsCount --sparkMaster yarn; 14:56:20.999 INFO IntelGKLUtils - Trying to load Intel GKL library from:; 	jar:file:/mnt/raid5/frankliu/code/GATK/gatk/build/libs/gatk-package-4.alpha.2-120-g00a40ea-SNAPSHOT-spark.jar!/com/intel/gkl/native/libgkl_compression.so; Exception in thread ""main"" java.lang.UnsatisfiedLinkError: /tmp/frankliu/libgkl_compression8271576113600851848.so: /tmp/frankliu/libgkl_compression8271576113600851848.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64-bit platform); 	at java.lang.ClassLoader$NativeLibrary.load(Native Method); 	at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941); 	at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824); 	at java.lang.Runtime.load0(Runtime.java:809); 	at java.lang.System.load(System.java:1086); 	at com.intel.gkl.IntelGKLUtils.load(IntelGKLUtils.java:133); 	at com.intel.gkl.compression.IntelDeflater.load(IntelDeflater.java:58); 	at com.intel.gkl.compression.IntelDeflater.load(IntelDeflater.java:53); 	at com.intel.gkl.compression.IntelDeflaterFactory.<init>(IntelDeflaterFactory.java:16); 	at com.intel.gkl.compression.IntelDeflaterFactory.<init>(IntelDeflaterFactory.java:20); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:143); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116); 	at org.broadinstitute.hellbender.Main.main(Main.java:158); 	at sun.reflect.N",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2302#issuecomment-265854885:1761,load,loadLibrary,1761,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2302#issuecomment-265854885,1,['load'],['loadLibrary']
Performance,45816 +473 ; Branches 16090 16107 +17 ; ===============================================; + Hits 126517 126891 +374 ; - Misses 12966 13048 +82 ; - Partials 5860 5877 +17; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/5601?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...kers/variantutils/CalculateGenotypePosteriors.java](https://codecov.io/gh/broadinstitute/gatk/pull/5601/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhcmlhbnR1dGlscy9DYWxjdWxhdGVHZW5vdHlwZVBvc3RlcmlvcnMuamF2YQ==) | `92.857% <100%> (ø)` | `17 <0> (ø)` | :arrow_down: |; | [...walkers/genotyper/afcalc/AFCalculatorProvider.java](https://codecov.io/gh/broadinstitute/gatk/pull/5601/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9hZmNhbGMvQUZDYWxjdWxhdG9yUHJvdmlkZXIuamF2YQ==) | `22.222% <0%> (-44.444%)` | `2% <0%> (-2%)` | |; | [...notyper/afcalc/ConcurrentAFCalculatorProvider.java](https://codecov.io/gh/broadinstitute/gatk/pull/5601/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9hZmNhbGMvQ29uY3VycmVudEFGQ2FsY3VsYXRvclByb3ZpZGVyLmphdmE=) | `50% <0%> (-33.333%)` | `1% <0%> (-1%)` | |; | [...nder/utils/downsampling/PositionalDownsampler.java](https://codecov.io/gh/broadinstitute/gatk/pull/5601/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9kb3duc2FtcGxpbmcvUG9zaXRpb25hbERvd25zYW1wbGVyLmphdmE=) | `88.462% <0%> (-11.538%)` | `22% <0%> (+1%)` | |; | [...er/engine/spark/datasources/VariantsSparkSink.java](https://codecov.io/gh/broadinstitute/gatk/pull/5601/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvZGF0YXNvdXJjZXMvVmFyaWFudHNTcGFya1NpbmsuamF2YQ==) | `78.125% <0%> (-11.53%)` | `8% <0%> (-1%)` | |; | [...broadinstitute/hellbender/engine/FeatureInput.java](https://codecov.io/,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5601#issuecomment-456985343:1611,Concurren,ConcurrentAFCalculatorProvider,1611,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5601#issuecomment-456985343,1,['Concurren'],['ConcurrentAFCalculatorProvider']
Performance,5); at htsjdk.samtools.MemoryMappedFileBuffer.readBytes(MemoryMappedFileBuffer.java:34); at htsjdk.samtools.AbstractBAMFileIndex.readBytes(AbstractBAMFileIndex.java:439); at htsjdk.samtools.AbstractBAMFileIndex.verifyIndexMagicNumber(AbstractBAMFileIndex.java:376); at htsjdk.samtools.AbstractBAMFileIndex.<init>(AbstractBAMFileIndex.java:70); at htsjdk.samtools.AbstractBAMFileIndex.<init>(AbstractBAMFileIndex.java:64); at htsjdk.samtools.CachingBAMFileIndex.<init>(CachingBAMFileIndex.java:56); at htsjdk.samtools.BAMFileReader.getIndex(BAMFileReader.java:418); at htsjdk.samtools.BAMFileReader.createIndexIterator(BAMFileReader.java:952); at htsjdk.samtools.BAMFileReader.query(BAMFileReader.java:612); at htsjdk.samtools.SamReader$PrimitiveSamReaderToSamReaderAdapter.query(SamReader.java:533); at htsjdk.samtools.SamReader$PrimitiveSamReaderToSamReaderAdapter.queryOverlapping(SamReader.java:405); at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.loadNextIterator(SamReaderQueryingIterator.java:125); at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.<init>(SamReaderQueryingIterator.java:66); at org.broadinstitute.hellbender.engine.ReadsDataSource.prepareIteratorsForTraversal(ReadsDataSource.java:416); at org.broadinstitute.hellbender.engine.ReadsDataSource.iterator(ReadsDataSource.java:342); at org.broadinstitute.hellbender.engine.MultiIntervalLocalReadShard.iterator(MultiIntervalLocalReadShard.java:134); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.<init>(AssemblyRegionIterator.java:86); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:188); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:173); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1048); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4578#issuecomment-681608709:1367,load,loadNextIterator,1367,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4578#issuecomment-681608709,1,['load'],['loadNextIterator']
Performance,"53 ERROR executor.Executor: Exception in task 0.0 in stage 1.0 (TID 1); java.lang.IncompatibleClassChangeError: Found class org.apache.hadoop.mapreduce.TaskAttemptContext, but interface was expected; at com.cloudera.dataflow.spark.TemplatedTextOutputFormat.getOutputFile(TemplatedTextOutputFormat.java:50); at com.cloudera.dataflow.spark.TemplatedTextOutputFormat.getDefaultWorkFile(TemplatedTextOutputFormat.java:46); at org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:125); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$12.apply(PairRDDFunctions.scala:995); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$12.apply(PairRDDFunctions.scala:979); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61); at org.apache.spark.scheduler.Task.run(Task.scala:64); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 15/07/14 13:14:53 ERROR util.SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker-0,5,main]; java.lang.IncompatibleClassChangeError: Found class org.apache.hadoop.mapreduce.TaskAttemptContext, but interface was expected; at com.cloudera.dataflow.spark.TemplatedTextOutputFormat.getOutputFile(TemplatedTextOutputFormat.java:50); at com.cloudera.dataflow.spark.TemplatedTextOutputFormat.getDefaultWorkFile(TemplatedTextOutputFormat.java:46); at org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:125); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$12.apply(PairRDDFunctions.scala:995); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$12.apply(PairRDDFunctions.scala:979); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61); at org.apache.spark.scheduler.Task.run(Task.scala:64); at org",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/639#issuecomment-121313713:29488,concurren,concurrent,29488,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/639#issuecomment-121313713,1,['concurren'],['concurrent']
Performance,"58875536 4.8 5592000 1176709.9; 15:39:56.079 INFO ProgressMeter - 13:37022394 4.9 5628000 1143956.7; 15:40:06.117 INFO ProgressMeter - 16:14727212 5.1 5728000 1125992.7; 15:40:16.383 INFO SplitNCigarReads - Shutting down engine; [March 2, 2023 3:40:16 PM EST] org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads done. Elapsed time: 5.27 minutes.; Runtime.totalMemory()=3432513536; java.lang.ClassCastException: htsjdk.samtools.BAMRecord cannot be cast to java.lang.Comparable; 	at java.util.Arrays$NaturalOrder.compare(Arrays.java:102); 	at java.util.TimSort.countRunAndMakeAscending(TimSort.java:355); 	at java.util.TimSort.sort(TimSort.java:234); 	at java.util.ArraysParallelSortHelpers$FJObject$Sorter.compute(ArraysParallelSortHelpers.java:145); 	at java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:731); 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); 	at java.util.concurrent.ForkJoinTask.doInvoke(ForkJoinTask.java:401); 	at java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:734); 	at java.util.Arrays.parallelSort(Arrays.java:1180); 	at htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:247); 	at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:182); 	at htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:202); 	at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:36); 	at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:16); 	at htsjdk.samtools.util.AbstractAsyncWriter$WriterRunnable.run(AbstractAsyncWriter.java:123); 	at java.lang.Thread.run(Thread.java:750); 	Suppressed: htsjdk.samtools.util.RuntimeIOException: Attempt to add record to closed writer.; 		at htsjdk.samtools.util.AbstractAsyncWriter.write(AbstractAsyncWriter.java:57); 		at htsjdk.samtools.AsyncSAMFileWriter.addAlignment(AsyncSAMFileWriter.java:58); 		at org.broadinstitute.hellbender.utils.read.SAMFileGATKReadWriter.addRead(SAMFileGATK",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452525485:6318,concurren,concurrent,6318,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452525485,1,['concurren'],['concurrent']
Performance,6/call-GenotypeGVCF/NWD197223.vcf.gz; 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lambda$getFeatureReadersInParallel$601(GenomicsDBImport.java:605); 	at java.util.LinkedHashMap.forEach(LinkedHashMap.java:684); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getFeatureReadersInParallel(GenomicsDBImport.java:600); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.createSampleToReaderMap(GenomicsDBImport.java:491); 	at com.intel.genomicsdb.importer.GenomicsDBImporter.lambda$null$2(GenomicsDBImporter.java:602); 	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590); 	... 3 more; Caused by: java.util.concurrent.ExecutionException: org.broadinstitute.hellbender.exceptions.UserException: Failed to create reader from gs://fc-c64a0fcd-fb30-4a7e-bdc6-3c09a9286941/f6aeb0ce-044a-4b36-a5f0-d3fda62252a5/ReblockGVCF/66c24439-cfeb-4b85-bc02-aefe693177b6/call-GenotypeGVCF/NWD197223.vcf.gz; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lambda$getFeatureReadersInParallel$601(GenomicsDBImport.java:602); 	... 8 more; Caused by: org.broadinstitute.hellbender.exceptions.UserException: Failed to create reader from gs://fc-c64a0fcd-fb30-4a7e-bdc6-3c09a9286941/f6aeb0ce-044a-4b36-a5f0-d3fda62252a5/ReblockGVCF/66c24439-cfeb-4b85-bc02-aefe693177b6/call-GenotypeGVCF/NWD197223.vcf.gz; 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getReaderFromPath(GenomicsDBImport.java:640); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lambda$getFeatureReadersInParallel$600(GenomicsDBImport.java:593); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	... 3 more; Caused by: htsjdk.tribble.TribbleException$MalformedFeatureFile: Unable to parse header with error: ComputeEngineCredentials cannot find the metadata server. ,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-411503423:2557,concurren,concurrent,2557,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-411503423,1,['concurren'],['concurrent']
Performance,"625"",; ""NIST controlMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/9bc521dc-3c4c-4274-972c-9d1e4be850d5/call-NISTSampleHeadToHead/BenchmarkComparison/4ffa2353-b1bc-4960-a5a4-96291208a7eb/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""NIST controlindelF1Score"": ""0.9902"",; ""NIST controlindelPrecision"": ""0.9903"",; ""NIST controlsnpF1Score"": ""0.9899"",; ""NIST controlsnpPrecision"": ""0.9887"",; ""NIST controlsnpRecall"": ""0.9911"",; ""NIST controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/9bc521dc-3c4c-4274-972c-9d1e4be850d5/call-NISTSampleHeadToHead/BenchmarkComparison/4ffa2353-b1bc-4960-a5a4-96291208a7eb/call-BenchmarkVCFControlSample/Benchmark/8cf95ec9-48a7-4e20-a8fe-816dc3e652ae/call-CombineSummaries/summary.csv"",; ""NIST evalHCprocesshours"": ""100.56416111111112"",; ""NIST evalHCsystemhours"": ""0.19999166666666665"",; ""NIST evalHCwallclockhours"": ""74.00048055555555"",; ""NIST evalHCwallclockmax"": ""4.007605555555555"",; ""NIST evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/9bc521dc-3c4c-4274-972c-9d1e4be850d5/call-NISTSampleHeadToHead/BenchmarkComparison/4ffa2353-b1bc-4960-a5a4-96291208a7eb/call-EVALRuntimeTask/monitoring.pdf"",; ""NIST evalindelF1Score"": ""0.9902"",; ""NIST evalindelPrecision"": ""0.9903"",; ""NIST evalsnpF1Score"": ""0.9899"",; ""NIST evalsnpPrecision"": ""0.9887"",; ""NIST evalsnpRecall"": ""0.9911"",; ""NIST evalsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/9bc521dc-3c4c-4274-972c-9d1e4be850d5/call-NISTSampleHeadToHead/BenchmarkComparison/4ffa2353-b1bc-4960-a5a4-96291208a7eb/call-BenchmarkVCFTestSample/Benchmark/6b79227b-3ca8-4f5b-96b6-60d57760cc5b/call-CombineSummaries/summary.csv"",; ""ROC_Plots_Reported"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/9bc521dc-3c4c-4274-972c-9d1e4be850d5/call-CreateHTMLReport/cacheCopy/report.html""; },; ""errors"": null; } ; </pre> </details>",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1533946590:22032,cache,cacheCopy,22032,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1533946590,1,['cache'],['cacheCopy']
Performance,"625"",; ""NIST controlMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/f7eac327-c59c-43f7-a850-21bc3e0ccf52/call-NISTSampleHeadToHead/BenchmarkComparison/d1a60d2b-8100-459a-9b05-72a22afccb4a/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""NIST controlindelF1Score"": ""0.9902"",; ""NIST controlindelPrecision"": ""0.9903"",; ""NIST controlsnpF1Score"": ""0.9899"",; ""NIST controlsnpPrecision"": ""0.9887"",; ""NIST controlsnpRecall"": ""0.9911"",; ""NIST controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/f7eac327-c59c-43f7-a850-21bc3e0ccf52/call-NISTSampleHeadToHead/BenchmarkComparison/d1a60d2b-8100-459a-9b05-72a22afccb4a/call-BenchmarkVCFControlSample/Benchmark/9f6d4e85-981d-4607-8ff6-97495034807f/call-CombineSummaries/summary.csv"",; ""NIST evalHCprocesshours"": ""96.65376666666666"",; ""NIST evalHCsystemhours"": ""0.17881944444444442"",; ""NIST evalHCwallclockhours"": ""68.38394444444445"",; ""NIST evalHCwallclockmax"": ""3.8226138888888888"",; ""NIST evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/f7eac327-c59c-43f7-a850-21bc3e0ccf52/call-NISTSampleHeadToHead/BenchmarkComparison/d1a60d2b-8100-459a-9b05-72a22afccb4a/call-EVALRuntimeTask/monitoring.pdf"",; ""NIST evalindelF1Score"": ""0.9902"",; ""NIST evalindelPrecision"": ""0.9903"",; ""NIST evalsnpF1Score"": ""0.9899"",; ""NIST evalsnpPrecision"": ""0.9887"",; ""NIST evalsnpRecall"": ""0.9911"",; ""NIST evalsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/f7eac327-c59c-43f7-a850-21bc3e0ccf52/call-NISTSampleHeadToHead/BenchmarkComparison/d1a60d2b-8100-459a-9b05-72a22afccb4a/call-BenchmarkVCFTestSample/Benchmark/e62b142c-c39c-4c1f-9a08-c41a96647879/call-CombineSummaries/summary.csv"",; ""ROC_Plots_Reported"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/f7eac327-c59c-43f7-a850-21bc3e0ccf52/call-CreateHTMLReport/cacheCopy/report.html""; },; ""errors"": null; } ; </pre> </details>",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7876#issuecomment-1182703672:21372,cache,cacheCopy,21372,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7876#issuecomment-1182703672,1,['cache'],['cacheCopy']
Performance,"64le system. When I use; > HaplotypeCaller, I see the following messages on the screen:; >; > Running:; > java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx50G -jar /home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar HaplotypeCaller -R ref.fa -I mybam.bam -O mycalls.vcf.gz -L snps.vcf -ip 100; >; > 16:17:04.377 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; >; > 16:17:04.397 **WARN** NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (/tmp/libgkl_compression3825249225068031371.so: /tmp/libgkl_compression3825249225068031371.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > 16:17:04.402 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; >; > 16:17:04.407 **WARN** NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (/tmp/libgkl_compression7506152962158874866.so: /tmp/libgkl_compression7506152962158874866.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > Sep 04, 2020 4:17:05 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; >; > INFO: Failed to detect whether we are running on Google Compute Engine.; >; > 16:17:05.842 INFO HaplotypeCaller - ------------------------------------------------------------; >; > 16:17:05.843 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.1.8.1; >; > 16:17:05.843 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/ga",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600:1970,Load,Loading,1970,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600,1,['Load'],['Loading']
Performance,7.246 INFO Mutect2 - HTSJDK Version: 2.20.3; 15:47:37.246 INFO Mutect2 - Picard Version: 2.21.1; 15:47:37.247 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 15:47:37.247 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 15:47:37.247 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 15:47:37.247 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 15:47:37.247 INFO Mutect2 - Deflater: IntelDeflater; 15:47:37.247 INFO Mutect2 - Inflater: IntelInflater; 15:47:37.247 INFO Mutect2 - GCS max retries/reopens: 20; 15:47:37.247 INFO Mutect2 - Requester pays: disabled; 15:47:37.247 INFO Mutect2 - Initializing engine; 15:47:41.204 INFO Mutect2 - Done initializing engine; 15:47:42.352 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/genouest/uni_limoges_fr/jpollet/.conda/envs/myd88/share/gatk4-4.1.4.0-1/gatk-package-4.1.4.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 15:47:42.423 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/genouest/uni_limoges_fr/jpollet/.conda/envs/myd88/share/gatk4-4.1.4.0-1/gatk-package-4.1.4.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 15:47:42.482 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 15:47:42.483 INFO IntelPairHmm - Available threads: 8; 15:47:42.483 INFO IntelPairHmm - Requested threads: 4; 15:47:42.483 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 15:47:42.936 INFO ProgressMeter - Starting traversal; 15:47:42.936 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 15:47:53.565 INFO ProgressMeter - ENA|LVXK01000001|LVXK01000001.1:19555 0.2 90 508.0; 15:48:05.962 INFO ProgressMeter - ENA|LVXK01000001|LVXK01000001.1:136820 0.4 600 1563.5; 15:48:16.023 INFO ProgressMeter - ENA|LVXK01000001|LVXK01000001.1:360783 0.6 1560 2828.9; 15:48:19.342 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 0.01,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6271#issuecomment-559553558:2278,Load,Loading,2278,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6271#issuecomment-559553558,1,['Load'],['Loading']
Performance,8); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:125); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:92); 	at shaded.cloud_nio.com.google.api.gax.retrying.RetryingFutureImpl.executeAttempt(RetryingFutureImpl.java:141); 	at shaded.cloud_nio.com.google.api.gax.retrying.RetryingFutureImpl.access$500(RetryingFutureImpl.java:59); 	at shaded.cloud_nio.com.google.api.gax.retrying.RetryingFutureImpl$AttemptFutureCallback.onFailure(RetryingFutureImpl.java:177); 	at shaded.cloud_nio.com.google.api.gax.core.ApiFutures$1.onFailure(ApiFutures.java:52); 	at shaded.cloud_nio.com.google.common.util.concurrent.Futures$6.run(Futures.java:1764); 	at shaded.cloud_nio.com.google.common.util.concurrent.MoreExecutors$DirectExecutor.execute(MoreExecutors.java:456); 	at shaded.cloud_nio.com.google.common.util.concurrent.Futures$ImmediateFuture.addListener(Futures.java:153); 	at shaded.cloud_nio.com.google.common.util.concurrent.ForwardingListenableFuture.addListener(ForwardingListenableFuture.java:47); 	at shaded.cloud_nio.com.google.api.gax.core.internal.ApiFutureToListenableFuture.addListener(ApiFutureToListenableFuture.java:53); 	at shaded.cloud_nio.com.google.common.util.concurrent.Futures.addCallback(Futures.java:1776); 	at shaded.cloud_nio.com.google.common.util.concurrent.Futures.addCallback(Futures.java:1713); 	at shaded.cloud_nio.com.google.api.gax.core.ApiFutures.addCallback(ApiFutures.java:47); 	at shaded.cloud_nio.com.google.api.gax.retrying.RetryingFutureImpl.setAttemptFuture(RetryingFutureImpl.java:107); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:100); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:47); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:125); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:109); 	at org.broadinstitute.hellb,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300298180:4680,concurren,concurrent,4680,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300298180,1,['concurren'],['concurrent']
Performance,"8.2; 23:10:12.683 INFO CountReadsSpark - Picard Version: 2.18.25; 23:10:12.683 INFO CountReadsSpark - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 23:10:12.683 INFO CountReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 23:10:12.683 INFO CountReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 23:10:12.683 INFO CountReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:10:12.684 INFO CountReadsSpark - Deflater: IntelDeflater; 23:10:12.684 INFO CountReadsSpark - Inflater: IntelInflater; 23:10:12.684 INFO CountReadsSpark - GCS max retries/reopens: 20; 23:10:12.684 INFO CountReadsSpark - Requester pays: disabled; 23:10:12.684 WARN CountReadsSpark -. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: CountReadsSpark is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 23:10:12.685 INFO CountReadsSpark - Initializing engine; 23:10:12.685 INFO CountReadsSpark - Done initializing engine; 19/02/05 23:10:13 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 19/02/05 23:10:15 WARN shortcircuit.DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.; 19/02/05 23:10:18 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.; 806177853; 19/02/05 23:11:51 ERROR scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(6,WrappedArray()); 19/02/05 23:11:51 ERROR scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(13,WrappedArray()); 23:11:51.429 INFO CountReadsSpark - Shutting down engine; [February 5, 2019 11:11:51 PM EST] org.broadinstitute.hellbender.tools.spark.pipelines.CountReadsSpark done. Elapsed time: 1.67 minutes.; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-460895912:5473,load,load,5473,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-460895912,2,['load'],"['load', 'loaded']"
Performance,"9.0/gatk-package-4.1.9.0-local.jar VariantRecalibrator -V temp/vatiant_germline/sites.only.vcf.gz -O temp/vatiant_germline/recaliberation.indel.vcf --tranches-file temp/vatiant_germline/tranches.indel.txt --trust-all-polymorphic -tranche 100.0 -tranche 99.95 -tranche 99.9 -tranche 99.5 -tranche 99.0 -tranche 97.0 -tranche 96.0 -tranche 95.0 -tranche 94.0 -tranche 93.5 -tranche 93.0 -tranche 92.0 -tranche 91.0 -tranche 90.0 -an DP -an FS -an MQRankSum -an QD -an ReadPosRankSum -an SOR -mode INDEL --max-gaussians 4 -resource:mills,known=false,training=true,truth=true,prior=12 ~/db/mutect2_support/b37/Mills_and_1000G_gold_standard.indels.b37.sites.vcf.gz -resource:dbsnp,known=true,training=false,truth=false,prior=2 ~/db/mutect2_support/b37/hg19_v0_dbsnp_138.b37.vcf.gz --use-allele-specific-annotations -resource:axiomPoly,known=false,training=true,truth=false,prior=10 ~/db/mutect2_support/b37/Axiom_Exome_Plus.genotypes.all_populations.poly.b37.vcf.gz. 14:58:10.389 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:~/bin/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Nov 12, 2020 2:58:10 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 14:58:10.555 INFO VariantRecalibrator - ------------------------------------------------------------; 14:58:10.555 INFO VariantRecalibrator - The Genome Analysis Toolkit (GATK) v4.1.9.0; 14:58:10.555 INFO VariantRecalibrator - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:58:10.555 INFO VariantRecalibrator - Executing as y@c001 on Linux v3.10.0-957.el7.x86_64 amd64; 14:58:10.555 INFO VariantRecalibrator - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_152-release-1056-b12; 14:58:10.556 INFO VariantRecalibrator - Start Date/Time: November 12, 2020 2:58:10 PM CST; 14:58:10.556 INFO VariantRecalibrator - -----------------------------",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6701#issuecomment-726406532:1622,Load,Loading,1622,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6701#issuecomment-726406532,1,['Load'],['Loading']
Performance,98); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.io.IOException: Existing mirrorFile and resourceId don't match isDirectory status! '/hadoop_gcs_connector_metadata_cache/hellbender/test/output/gatk4-spark/recalibrated.bam' (dir: 'false') vs 'gs://hellbender/test/output/gatk4-spark/recalibrated.bam/' (dir: 'true'); 	at com.google.cloud.hadoop.gcsio.FileSystemBackedDirectoryListCache.getCacheEntryInternal(FileSystemBackedDirectoryListCache.java:198); 	at com.google.cloud.hadoop.gcsio.FileSystemBackedDirectoryListCache.putResourceId(FileSystemBackedDirectoryListCache.java:363); 	at com.google.cloud.hadoop.gcsio.CacheSupplementedGoogleCloudStorage.createEmptyObjects(CacheSupplementedGoogleCloudStorage.java:150); 	at com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.mkdirs(GoogleCloudStorageFileSystem.java:578); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.mkdirs(GoogleHadoopFileSystemBase.java:1372); 	at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:1881); 	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:313); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1150); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1078); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1078); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.a,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2306#issuecomment-271419191:2418,Cache,CacheSupplementedGoogleCloudStorage,2418,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2306#issuecomment-271419191,1,['Cache'],['CacheSupplementedGoogleCloudStorage']
Performance,"99027448764 2.1547132944470522E-5; CSCC_0007-M1 0.00135679065051943 2.0692791516445317E-5; CSCC_0008-M1 0.004394182081805844 3.748225078434626E-5; CSCC_0009-M1 0.0019614948575730397 2.4555494660711082E-5; CSCC_0010-M1 0.004122282756273677 3.6210336627748355E-5; CSCC_0010-P1 0.001888852796306713 3.040210329291157E-5; CSCC_0011-M1 0.004616869166852859 3.9987828482322895E-5; CSCC_0012-M1 0.0013866025034395032 2.1835070542119526E-5; CSCC_0012-P1 0.9856060967650699 0.0023006992152694522; CSCC_0013-M1 0.014792148767770472 9.498068474793835E-5; CSCC_0014-M1 0.0028227703351458118 4.122117743000552E-5; CSCC_0015-M1 0.01467099675552882 9.531218938517413E-5; CSCC_0016-P1 0.0014411085088999514 2.716291906758587E-5; CSCC_0017-P1 0.0015213899870480127 2.712650453920576E-5; CSCC_0018-P1 0.001694677662867099 2.913615483470931E-5; CSCC_0019-P1 0.0016654868517623346 2.8602851235266697E-5; CSCC_0020-P1 0.0015496166402163914 2.7824601469663656E-5; ```. The procedure done is. > Base quality score recalibration was performed with GATK 4.1.2.0 (Van der Auwera et al. 2013). GATK SplitIntervals was used to define 32 evenly-sized genomic intervals over which GATK BaseRecalibrator was run for each sample. The 32 recalibration tables per sample were merged into one table per sample with GATK GatherReports.; > ; > Base-recalibrated BAM files were produced with GATK ApplyBQSR in parallel over each of the 3,366 contigs in the hg 38 + alt reference genome, plus the unmapped reads. These were merged into a final BAM per sample with GATK GatherBamFiles.; > ; > SplitIntervals was used to define 3,200 evenly sized genomic intervals across hg38 + alt contigs, excluding telomeres, centromeres, unplaced contigs, unlocalized contigs, decoy and the Epstein-Barr viral sequence (Supplementary data 1). These intervals were used for scattering tasks in the Germline short variant calling and Somatic short variant calling workflows outlined below.; > ; > Germline short variant calling; > ; > HaplotypeCaller was ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6674#issuecomment-656499472:1563,perform,performed,1563,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6674#issuecomment-656499472,1,['perform'],['performed']
Performance,9scy9leG9tZS9Bbm5vdGF0ZVRhcmdldHMuamF2YQ==) | `78.049% <ø> (ø)` | `7 <0> (ø)` | :arrow_down: |; | [...llbender/tools/exome/plotting/PlotACNVResults.java](https://codecov.io/gh/broadinstitute/gatk/pull/3135?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9wbG90dGluZy9QbG90QUNOVlJlc3VsdHMuamF2YQ==) | `84.615% <ø> (ø)` | `22 <0> (ø)` | :arrow_down: |; | [...adinstitute/hellbender/tools/exome/PadTargets.java](https://codecov.io/gh/broadinstitute/gatk/pull/3135?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9QYWRUYXJnZXRzLmphdmE=) | `100% <ø> (ø)` | `3 <0> (ø)` | :arrow_down: |; | [...hellbender/tools/genome/SparkGenomeReadCounts.java](https://codecov.io/gh/broadinstitute/gatk/pull/3135?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9nZW5vbWUvU3BhcmtHZW5vbWVSZWFkQ291bnRzLmphdmE=) | `91.089% <ø> (ø)` | `18 <0> (ø)` | :arrow_down: |; | [...egmentation/PerformAlleleFractionSegmentation.java](https://codecov.io/gh/broadinstitute/gatk/pull/3135?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9zZWdtZW50YXRpb24vUGVyZm9ybUFsbGVsZUZyYWN0aW9uU2VnbWVudGF0aW9uLmphdmE=) | `88.889% <ø> (ø)` | `2 <0> (ø)` | :arrow_down: |; | [...llbender/tools/walkers/validation/Concordance.java](https://codecov.io/gh/broadinstitute/gatk/pull/3135?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vQ29uY29yZGFuY2UuamF2YQ==) | `88.542% <ø> (ø)` | `28 <0> (ø)` | :arrow_down: |; | [...ute/hellbender/tools/exome/ConvertACNVResults.java](https://codecov.io/gh/broadinstitute/gatk/pull/3135?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9Db252ZXJ0QUNOVlJlc3VsdHMuamF2YQ==) | `87.805% <ø> (ø)` | `4 <0> (ø)` | :arrow_down: |; | [...idation/AnnotateVcfWithExpectedAlleleFraction.java](https://codecov.io/gh/broadinstitute/gatk/pull/313,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3135#issuecomment-309876624:2345,Perform,PerformAlleleFractionSegmentation,2345,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3135#issuecomment-309876624,1,['Perform'],['PerformAlleleFractionSegmentation']
Performance,: Failure while waiting for FeatureReader to initialize with exception: com.google.cloud.storage.StorageException: All 20 reopens failed. Waited a total of 1918000 ms between attempts; at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lambda$getFeatureReadersInParallel$614(GenomicsDBImport.java:605); at java.util.LinkedHashMap.forEach(LinkedHashMap.java:684); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getFeatureReadersInParallel(GenomicsDBImport.java:600); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.createSampleToReaderMap(GenomicsDBImport.java:491); at com.intel.genomicsdb.importer.GenomicsDBImporter.lambda$null$2(GenomicsDBImporter.java:602); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590); ... 3 more; Caused by: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: All 20 reopens failed. Waited a total of 1918000 ms between attempts; at java.util.concurrent.FutureTask.report(FutureTask.java:122); at java.util.concurrent.FutureTask.get(FutureTask.java:192); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lambda$getFeatureReadersInParallel$614(GenomicsDBImport.java:602); ... 8 more; Caused by: com.google.cloud.storage.StorageException: All 20 reopens failed. Waited a total of 1918000 ms between attempts; at com.google.cloud.storage.contrib.nio.CloudStorageRetryHandler.handleReopenForStorageException(CloudStorageRetryHandler.java:124); at com.google.cloud.storage.contrib.nio.CloudStorageRetryHandler.handleStorageException(CloudStorageRetryHandler.java:94); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:621); at java.nio.file.Files.exists(Files.java:2385); at htsjdk.tribble.util.ParsingUtils.resourceExists(ParsingUtils.java:419); at htsjdk.tribble.AbstractFeatureReader.isTabix(AbstractFeatureReader.java:222); at htsjdk.tribble.AbstractFeatureReader,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-412904420:2329,concurren,concurrent,2329,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-412904420,1,['concurren'],['concurrent']
Performance,": false; 11:19:40.101 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 11:19:40.101 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 11:19:40.101 INFO GenomicsDBImport - Deflater: IntelDeflater; 11:19:40.101 INFO GenomicsDBImport - Inflater: IntelInflater; 11:19:40.101 INFO GenomicsDBImport - GCS max retries/reopens: 20; 11:19:40.102 INFO GenomicsDBImport - Requester pays: disabled; 11:19:40.102 INFO GenomicsDBImport - Initializing engine; 11:19:40.385 INFO FeatureManager - Using codec BEDCodec to read file file:///mnt/data/project/reseq/KPSNY042021067K/result/03.bwa_dup_gvcf/geno/chr33.bed; 11:19:40.390 INFO IntervalArgumentCollection - Processing 10664 bp from intervals; 11:19:40.391 WARN GenomicsDBImport - A large number of intervals were specified. Using more than 100 intervals in a single import is not recommended and can cause performance to suffer. If GVCF data only exists within those intervals, performance can be improved by aggregating intervals with the merge-input-intervals argument.; 11:19:40.429 INFO GenomicsDBImport - Done initializing engine; 11:19:40.624 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.3.0-e701905; 11:19:40.625 INFO GenomicsDBImport - Vid Map JSON file will be written to /mnt/data/chr33.db/vidmap.json; 11:19:40.625 INFO GenomicsDBImport - Callset Map JSON file will be written to /mnt/data/chr33.db/callset.json; 11:19:40.625 INFO GenomicsDBImport - Complete VCF Header will be written to /mnt/data/chr33.db/vcfheader.vcf; 11:19:40.625 INFO GenomicsDBImport - Importing to workspace - /mnt/data/chr33.db; 11:19:40.625 INFO ProgressMeter - Starting traversal; 11:19:40.625 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 11:19:49.073 INFO GenomicsDBImport - Importing batch 1 with 1115 samples; 11:20:12.073 INFO GenomicsDBImport - Importing batch 1 with 1115 samples; 11:20:32.582 INFO GenomicsDBImport - Importing batch 1 with 1115",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7952#issuecomment-1196217460:3383,perform,performance,3383,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7952#issuecomment-1196217460,1,['perform'],['performance']
Performance,": note: 'template<class _Tp> typename __gnu_cxx::__enable_if<std::__is_arithmetic<_Tp>::__value, int>::__type std::isinf(_Tp)' declared here, later in the translation unit; isinf(_Tp __f); ^; In file included from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/utils.h:4:0,; from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/LoadTimeInitializer.cc:1:; /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/common_data_structure.h:94:38: error: 'isinf' was not declared in this scope, and no declarations were found by argument-dependent lookup at the point of instantiation [-fpermissive]; if (isinf(small) == -1 || isinf(big) == -1); ^; In file included from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/headers.h:27:0,; from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/common_data_structure.h:4,; from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/utils.h:4,; from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/LoadTimeInitializer.cc:1:; /usr/local/Cellar/gcc/5.3.0/include/c++/5.3.0/cmath:853:5: note: 'template<class _Tp> typename __gnu_cxx::__enable_if<std::__is_arithmetic<_Tp>::__value, int>::__type std::isinf(_Tp)' declared here, later in the translation unit; isinf(_Tp __f); ^. /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/utils.cc: In function 'void get_time(timespec*)':; /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/utils.cc:214:17: error: 'CLOCK_REALTIME' was not declared in this scope; clock_gettime(CLOCK_REALTIME, store_struct);; ^; /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/utils.cc:214:45: error: 'clock_gettime' was not declared in this scope; clock_gettime(CLOCK_REALTIME, store_struct);; ^; /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/utils.cc: In function 'uint64_t diff_time(timespec&)':; /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/utils.cc:220:17: error: 'CLOCK_R",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1504#issuecomment-187727343:14780,Load,LoadTimeInitializer,14780,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1504#issuecomment-187727343,1,['Load'],['LoadTimeInitializer']
Performance,": note: 'template<class _Tp> typename __gnu_cxx::__enable_if<std::__is_arithmetic<_Tp>::__value, int>::__type std::isinf(_Tp)' declared here, later in the translation unit; isinf(_Tp __f); ^; In file included from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/utils.h:4:0,; from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/LoadTimeInitializer.cc:1:; /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/common_data_structure.h:94:38: error: 'isinf' was not declared in this scope, and no declarations were found by argument-dependent lookup at the point of instantiation [-fpermissive]; if (isinf(small) == -1 || isinf(big) == -1); ^; In file included from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/headers.h:27:0,; from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/common_data_structure.h:4,; from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/utils.h:4,; from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/LoadTimeInitializer.cc:1:; /usr/local/Cellar/gcc/5.3.0/include/c++/5.3.0/cmath:853:5: note: 'template<class _Tp> typename __gnu_cxx::__enable_if<std::__is_arithmetic<_Tp>::__value, int>::__type std::isinf(_Tp)' declared here, later in the translation unit; isinf(_Tp __f); ^; In file included from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/utils.h:4:0,; from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/LoadTimeInitializer.cc:1:; /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/common_data_structure.h: In instantiation of 'static NUMBER ContextBase<NUMBER>::approximateLog10SumLog10(NUMBER, NUMBER) [with NUMBER = double]':; /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/common_data_structure.h:75:53: required from 'static void ContextBase<NUMBER>::initializeMatchToMatchProb() [with NUMBER = double]'; /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/common_data_structure.h:47:35: re",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1504#issuecomment-187727343:11845,Load,LoadTimeInitializer,11845,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1504#issuecomment-187727343,1,['Load'],['LoadTimeInitializer']
Performance,:+1: Could you add a note explaining that it's a short circuit for performance reasons? Merge when ready.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1624#issuecomment-200977770:67,perform,performance,67,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1624#issuecomment-200977770,1,['perform'],['performance']
Performance,:+1: I don't see any problems although I'm skeptical about some of the optimization making a difference.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1623#issuecomment-200996224:71,optimiz,optimization,71,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1623#issuecomment-200996224,1,['optimiz'],['optimization']
Performance,":+1: merge after addressing comments. If the test I requested is too slow to be part of our test suite, should create a ticket ""Fix performance issues in `BAMInputFormat.setIntervals()` with large interval lists""",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1516#issuecomment-188935087:132,perform,performance,132,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1516#issuecomment-188935087,1,['perform'],['performance']
Performance,":///home/proj/stage/cancer/reference/target_capture_bed/production/balsamic/gicfdna_3.1_hg19_design.bed; 11:47:51.465 INFO IntervalArgumentCollection - Processing 74592 bp from intervals; 11:47:51.474 INFO Mutect2 - Done initializing engine; 11:47:51.487 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/proj/bin/conda/envs/D_UMI_APJ/share/gatk4-4.1.8.0-0/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 11:47:51.489 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/proj/bin/conda/envs/D_UMI_APJ/share/gatk4-4.1.8.0-0/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 11:47:51.534 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; 11:47:51.534 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 11:47:51.534 INFO IntelPairHmm - Available threads: 16; 11:47:51.534 INFO IntelPairHmm - Requested threads: 4; 11:47:51.534 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 11:47:51.557 INFO ProgressMeter - Starting traversal; 11:47:51.557 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 11:47:52.683 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 0.0; 11:47:52.683 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 0.0; 11:47:52.683 INFO SmithWatermanAligner - Total compute time in java Smith-Waterman : 0.24 sec; 11:47:52.684 INFO Mutect2 - Shutting down engine; [July 2, 2020 11:47:52 AM CEST] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.03 minutes.; Runtime.totalMemory()=2511863808; java.lang.IllegalArgumentException: Read bases and read quality arrays aren't the same size: Bases: 38 vs Base Q's: 38 vs Insert Q's: 146 vs Delete Q's: 146.; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:734); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEng",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-652912482:4348,multi-thread,multi-threaded,4348,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-652912482,1,['multi-thread'],['multi-threaded']
Performance,":118752077 4.6 5530000 1207397.2; 15:39:46.028 INFO ProgressMeter - 12:58875536 4.8 5592000 1176709.9; 15:39:56.079 INFO ProgressMeter - 13:37022394 4.9 5628000 1143956.7; 15:40:06.117 INFO ProgressMeter - 16:14727212 5.1 5728000 1125992.7; 15:40:16.383 INFO SplitNCigarReads - Shutting down engine; [March 2, 2023 3:40:16 PM EST] org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads done. Elapsed time: 5.27 minutes.; Runtime.totalMemory()=3432513536; java.lang.ClassCastException: htsjdk.samtools.BAMRecord cannot be cast to java.lang.Comparable; 	at java.util.Arrays$NaturalOrder.compare(Arrays.java:102); 	at java.util.TimSort.countRunAndMakeAscending(TimSort.java:355); 	at java.util.TimSort.sort(TimSort.java:234); 	at java.util.ArraysParallelSortHelpers$FJObject$Sorter.compute(ArraysParallelSortHelpers.java:145); 	at java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:731); 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); 	at java.util.concurrent.ForkJoinTask.doInvoke(ForkJoinTask.java:401); 	at java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:734); 	at java.util.Arrays.parallelSort(Arrays.java:1180); 	at htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:247); 	at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:182); 	at htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:202); 	at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:36); 	at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:16); 	at htsjdk.samtools.util.AbstractAsyncWriter$WriterRunnable.run(AbstractAsyncWriter.java:123); 	at java.lang.Thread.run(Thread.java:750); 	Suppressed: htsjdk.samtools.util.RuntimeIOException: Attempt to add record to closed writer.; 		at htsjdk.samtools.util.AbstractAsyncWriter.write(AbstractAsyncWriter.java:57); 		at htsjdk.samtools.AsyncSAMFileWriter.addAlignment(AsyncSAMFileWriter.java:58); 		at org.broadin",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452525485:6247,concurren,concurrent,6247,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452525485,1,['concurren'],['concurrent']
Performance,":22.413 INFO HaplotypeCaller - Deflater: IntelDeflater; 22:42:22.413 INFO HaplotypeCaller - Inflater: IntelInflater; 22:42:22.413 INFO HaplotypeCaller - GCS max retries/reopens: 20; 22:42:22.413 INFO HaplotypeCaller - Requester pays: disabled; 22:42:22.413 INFO HaplotypeCaller - Initializing engine; 22:42:22.705 INFO IntervalArgumentCollection - Processing 2001 bp from intervals; 22:42:22.710 INFO HaplotypeCaller - Done initializing engine; 22:42:22.712 INFO HaplotypeCallerEngine - Tool is in reference confidence mode and the annotation, the following changes will be made to any specified annotations: 'StrandBiasBySample' will be enabled. 'ChromosomeCounts', 'FisherStrand', 'StrandOddsRatio' and 'QualByDepth' annotations have been disabled; 22:42:22.719 INFO NativeLibraryLoader - Loading libgkl_utils.dylib from jar:file:/Users/nhomer/miniconda3/envs/bfx/share/gatk4-4.1.8.1-0/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.dylib; 22:42:22.720 INFO NativeLibraryLoader - Loading libgkl_smithwaterman.dylib from jar:file:/Users/nhomer/miniconda3/envs/bfx/share/gatk4-4.1.8.1-0/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_smithwaterman.dylib; 22:42:22.722 INFO SmithWatermanAligner - Using AVX accelerated SmithWaterman implementation; 22:42:22.724 INFO HaplotypeCallerEngine - Standard Emitting and Calling confidence set to 0.0 for reference-model confidence output; 22:42:22.724 INFO HaplotypeCallerEngine - All sites annotated with PLs forced to true for reference-model confidence output; 22:42:22.734 WARN NativeLibraryLoader - Unable to find native library: native/libgkl_pairhmm_omp.dylib; 22:42:22.734 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; 22:42:22.734 INFO NativeLibraryLoader - Loading libgkl_pairhmm.dylib from jar:file:/Users/nhomer/miniconda3/envs/bfx/share/gatk4-4.1.8.1-0/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_pairhmm.dylib; 22:42:22.748 INFO IntelPairHmm",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6733#issuecomment-667631737:2851,Load,Loading,2851,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6733#issuecomment-667631737,1,['Load'],['Loading']
Performance,":49.226 INFO GenomicsDBImport - Done importing batch 2/5; 23 Feb 2022 18:26:19,107 DEBUG: 	18:26:19.105 INFO GenomicsDBImport - Done importing batch 3/5; 23 Feb 2022 19:20:18,500 DEBUG: 	19:20:18.478 INFO GenomicsDBImport - Done importing batch 4/5; 24 Feb 2022 16:51:19,017 DEBUG: 	[TileDB::utils] Error: (gzip_handle_error) Cannot decompress with GZIP: inflateInit error: Z_MEM_ERROR; 24 Feb 2022 16:51:19,048 DEBUG: 	[TileDB::Codec] Error: Could not decompress with GZIP.; 24 Feb 2022 16:51:19,056 DEBUG: 	[TileDB::ReadState] Error: Cannot decompress tile for /home/exacloud/gscratch/prime-seq/workDir/0950f56b-7565-103a-a738-f8f3fc8675d2/Job2.work/WGS_1852_consolidated.gdb/2$1$196197964/__e7217c9e-767d-4295-b75a-9162c22c6996139785909643008_1613563029631/END.tdb.; 24 Feb 2022 16:51:51,388 DEBUG: 	16:51:51.388 erro NativeGenomicsDB - pid=225263 tid=225739 VariantStorageManagerException exception : Error while consolidating TileDB array 2$1$196197964; 24 Feb 2022 16:51:51,405 DEBUG: 	TileDB error message : ; 24 Feb 2022 16:51:51,412 DEBUG: 	terminate called after throwing an instance of 'VariantStorageManagerException'; 24 Feb 2022 16:51:51,419 DEBUG: 	 what(): VariantStorageManagerException exception : Error while consolidating TileDB array 2$1$196197964; 24 Feb 2022 16:51:51,427 DEBUG: 	TileDB error message : ; 24 Feb 2022 16:52:27,478 WARN : 	process exited with non-zero value: 134; ```. Does that give anything to suggest troubleshooting steps?. The full command is:; ```; /home/exacloud/gscratch/prime-seq/java/java8/bin/java \; 	Xmx497g -Xms497g -Xss2m \; 	-jar /home/exacloud/gscratch/prime-seq/bin/GenomeAnalysisTK4.jar \; 	GenomicsDBImport \; 	-V 25780.g.vcf.gz \; 	-V <total of 92 gVCFs> \; 	--genomicsdb-update-workspace-path WGS_1852_consolidated.gdb \; 	--batch-size 10 \; 	--reader-threads 12 \; 	--consolidate \; 	--genomicsdb-shared-posixfs-optimizations \; 	--bypass-feature-reader \; 	-R 128_Mmul_10.fasta; ```. this is GATK v4.2.5.0. Thanks i advance for any ideas.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1050434022:2931,optimiz,optimizations,2931,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1050434022,1,['optimiz'],['optimizations']
Performance,":50:16.818 INFO HaplotypeCaller - Deflater: IntelDeflater; 14:50:16.818 INFO HaplotypeCaller - Inflater: IntelInflater; 14:50:16.818 INFO HaplotypeCaller - GCS max retries/reopens: 20; 14:50:16.818 INFO HaplotypeCaller - Using google-cloud-java patch c035098b5e62cb4fe9155eff07ce88449a361f5d from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 14:50:16.819 INFO HaplotypeCaller - Initializing engine; 14:50:18.950 INFO IntervalArgumentCollection - Processing 83257441 bp from intervals; 14:50:18.965 INFO HaplotypeCaller - Done initializing engine; 14:50:19.021 INFO HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output; 14:50:19.280 WARN PossibleDeNovo - Annotation will not be calculated, must provide a valid PED file (-ped) from the command line.; 14:50:19.481 WARN PossibleDeNovo - Annotation will not be calculated, must provide a valid PED file (-ped) from the command line.; 14:50:19.776 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/humgen/gsa-hpprojects/GATK/gatk4/gatk-4.beta.5/gatk-package-4.beta.5-local.jar!/com/intel/gkl/native/libgkl_utils.so; 14:50:19.795 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/humgen/gsa-hpprojects/GATK/gatk4/gatk-4.beta.5/gatk-package-4.beta.5-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 14:50:19.847 WARN IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 14:50:19.848 INFO IntelPairHmm - Available threads: 48; 14:50:19.848 INFO IntelPairHmm - Requested threads: 4; 14:50:19.848 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 14:50:19.926 INFO ProgressMeter - Starting traversal; 14:50:19.926 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 14:50:30.309 INFO ProgressMeter - chr17:740224 0.2 3010 17395.5; 14:50:41.016 INFO ProgressMeter - chr17:1675683 0.4 7020 19973.4; 14:50:51.041 INFO ProgressMeter - chr17:24",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3154#issuecomment-334840678:6625,Load,Loading,6625,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3154#issuecomment-334840678,1,['Load'],['Loading']
Performance,; 	at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.traverseVariants(MultiplePassVariantWalker.java:75); 	at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.traverse(MultiplePassVariantWalker.java:40); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1048); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:163); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:206); 	at org.broadinstitute.hellbender.Main.main(Main.java:292); Caused by: java.util.concurrent.ExecutionException: org.broadinstitute.hellbender.exceptions.GATKException: Expected message of length 3 but only found 0 bytes; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at org.broadinstitute.hellbender.utils.runtime.StreamingProcessController.waitForAck(StreamingProcessController.java:228); 	... 26 more; Caused by: org.broadinstitute.hellbender.exceptions.GATKException: Expected message of length 3 but only found 0 bytes; 	at org.broadinstitute.hellbender.utils.runtime.StreamingProcessController.getBytesFromStream(StreamingProcessController.java:261); 	at org.broadinstitute.hellbender.utils.runtime.StreamingProcessController.lambda$waitForAck$0(StreamingProcessController.java:208); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). but if I change memory as below: it works. ; qlogin -l s_vmem=20G -l mem_req=20G,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7397#issuecomment-895854147:3601,concurren,concurrent,3601,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7397#issuecomment-895854147,5,['concurren'],['concurrent']
Performance,; Caused by: com.google.cloud.storage.StorageException: Read timed out; 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:186); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:512); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:128); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:125); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:92); 	at shaded.cloud_nio.com.google.api.gax.retrying.RetryingFutureImpl.executeAttempt(RetryingFutureImpl.java:141); 	at shaded.cloud_nio.com.google.api.gax.retrying.RetryingFutureImpl.access$500(RetryingFutureImpl.java:59); 	at shaded.cloud_nio.com.google.api.gax.retrying.RetryingFutureImpl$AttemptFutureCallback.onFailure(RetryingFutureImpl.java:177); 	at shaded.cloud_nio.com.google.api.gax.core.ApiFutures$1.onFailure(ApiFutures.java:52); 	at shaded.cloud_nio.com.google.common.util.concurrent.Futures$6.run(Futures.java:1764); 	at shaded.cloud_nio.com.google.common.util.concurrent.MoreExecutors$DirectExecutor.execute(MoreExecutors.java:456); 	at shaded.cloud_nio.com.google.common.util.concurrent.Futures$ImmediateFuture.addListener(Futures.java:153); 	at shaded.cloud_nio.com.google.common.util.concurrent.ForwardingListenableFuture.addListener(ForwardingListenableFuture.java:47); 	at shaded.cloud_nio.com.google.api.gax.core.internal.ApiFutureToListenableFuture.addListener(ApiFutureToListenableFuture.java:53); 	at shaded.cloud_nio.com.google.common.util.concurrent.Futures.addCallback(Futures.java:1776); 	at shaded.cloud_nio.com.google.common.util.concurrent.Futures.addCallback(Futures.java:1713); 	at shaded.cloud_nio.com.google.api.gax.core.ApiFutures.addCallback(ApiFutures.java:47); 	at shaded.cloud_nio.com.google.api.gax.retrying.RetryingFutureImpl.setAttemptFuture(RetryingFutureImpl.java:107); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submi,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300298180:4364,concurren,concurrent,4364,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300298180,1,['concurren'],['concurrent']
Performance,; Caused by: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: Failure while waiting for FeatureReader to initialize with exception: org.broadinstitute.hellbender.exceptions.UserException: Failed to create reader from gs://fc-c64a0fcd-fb30-4a7e-bdc6-3c09a9286941/f6aeb0ce-044a-4b36-a5f0-d3fda62252a5/ReblockGVCF/66c24439-cfeb-4b85-bc02-aefe693177b6/call-GenotypeGVCF/NWD197223.vcf.gz; 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lambda$getFeatureReadersInParallel$601(GenomicsDBImport.java:605); 	at java.util.LinkedHashMap.forEach(LinkedHashMap.java:684); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getFeatureReadersInParallel(GenomicsDBImport.java:600); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.createSampleToReaderMap(GenomicsDBImport.java:491); 	at com.intel.genomicsdb.importer.GenomicsDBImporter.lambda$null$2(GenomicsDBImporter.java:602); 	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590); 	... 3 more; Caused by: java.util.concurrent.ExecutionException: org.broadinstitute.hellbender.exceptions.UserException: Failed to create reader from gs://fc-c64a0fcd-fb30-4a7e-bdc6-3c09a9286941/f6aeb0ce-044a-4b36-a5f0-d3fda62252a5/ReblockGVCF/66c24439-cfeb-4b85-bc02-aefe693177b6/call-GenotypeGVCF/NWD197223.vcf.gz; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lambda$getFeatureReadersInParallel$601(GenomicsDBImport.java:602); 	... 8 more; Caused by: org.broadinstitute.hellbender.exceptions.UserException: Failed to create reader from gs://fc-c64a0fcd-fb30-4a7e-bdc6-3c09a9286941/f6aeb0ce-044a-4b36-a5f0-d3fda62252a5/ReblockGVCF/66c24439-cfeb-4b85-bc02-aefe693177b6/call-GenotypeGVCF/NWD197223.vcf.gz; 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getReade,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-411503423:2151,concurren,concurrent,2151,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-411503423,1,['concurren'],['concurrent']
Performance,"; ```. example usage:. ```; @Test; public void testApplyToString(){; Pipeline p = GATKTestPipeline.create();; PCollection<Integer> pints = p.apply(Create.of(Arrays.asList(1, 2, 3)));. PCollection<String> presults = DataflowUtils.apply( pints, Object::toString).setCoder(StringUtf8Coder.of());. DataflowAssert.that(presults).containsInAnyOrder(""1"",""2"",""3"");; p.run();; }; ```. note the `setCoder` call, if you don't include that you get. ```; SLF4J: Class path contains multiple SLF4J bindings.; SLF4J: Found binding in [jar:file:/Users/louisb/.gradle/caches/modules-2/files-2.1/org.slf4j/slf4j-jdk14/1.7.7/25d160723ea37a6cb84e87cd70773ff02997e857/slf4j-jdk14-1.7.7.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: Found binding in [jar:file:/Users/louisb/.gradle/caches/modules-2/files-2.1/org.slf4j/slf4j-log4j12/1.7.10/b3eeae7d1765f988a1f45ea81517191315c69c9e/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: Found binding in [jar:file:/Users/louisb/.gradle/caches/modules-2/files-2.1/org.slf4j/slf4j-log4j12/1.7.5/6edffc576ce104ec769d954618764f39f0f0f10d/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.; SLF4J: Actual binding is of type [org.slf4j.impl.JDK14LoggerFactory]; java.lang.IllegalStateException: Unable to infer a default Coder for AnonymousParDo.out [PCollection]; either correct the root cause below or use setCoder() to specify one explicitly. ; at com.google.cloud.dataflow.sdk.values.TypedPValue.getCoder(TypedPValue.java:48); at com.google.cloud.dataflow.sdk.values.PCollection.getCoder(PCollection.java:137); at com.google.cloud.dataflow.sdk.transforms.windowing.Window$Bound.getDefaultOutputCoder(Window.java:286); at com.google.cloud.dataflow.sdk.transforms.windowing.Window$Bound.getDefaultOutputCoder(Window.java:221); at com.google.cloud.dataflow.sdk.transforms.PTransform.getDefaultOutputCoder(PTransform.java:334); at com.google.cloud.datafl",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/658#issuecomment-122314248:1545,cache,caches,1545,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/658#issuecomment-122314248,1,['cache'],['caches']
Performance,; callable 1.0; ```; results in FilterMutectCalls exception; ```; java.lang.IllegalArgumentException: logValues must be non-infinite and non-NAN; at org.broadinstitute.hellbender.utils.NaturalLogUtils.logSumExp(NaturalLogUtils.java:84); at org.broadinstitute.hellbender.utils.NaturalLogUtils.normalizeLog(NaturalLogUtils.java:51); at org.broadinstitute.hellbender.utils.NaturalLogUtils.normalizeFromLogToLinearSpace(NaturalLogUtils.java:27); at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.posteriorProbabilityOfError(Mutect2FilteringEngine.java:93); at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.probabilityOfSequencingError(SomaticClusteringModel.java:140); at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.probabilityOfSomaticVariant(SomaticClusteringModel.java:146); at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.performEMIteration(SomaticClusteringModel.java:345); at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.learnAndClearAccumulatedData(SomaticClusteringModel.java:330); at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.learnParameters(Mutect2FilteringEngine.java:153); at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls.afterNthPass(FilterMutectCalls.java:165); at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.traverse(MultiplePassVariantWalker.java:44); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1095); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLin,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7276#issuecomment-1293969047:1769,perform,performEMIteration,1769,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7276#issuecomment-1293969047,1,['perform'],['performEMIteration']
Performance,; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx8G -Djava.io.tmpdir=/data/xieduo/Łuksza_2022_Nature -jar /data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar BaseRecalibrator -R /data/reference/gatk_resource/Homo_sapiens_assembly38.fasta -I /data/xieduo/Immun_genomics/data/Łuksza_2022_Nature/bam/PAAD11N.bam --known-sites /data/xieduo/WES_pipe/pipeline/gatk_resource/dbsnp_146.hg38.vcf.gz --known-sites /data/reference/gatk_resource/1000G_phase1.snps.high_confidence.hg38.vcf.gz --known-sites /data/reference/gatk_resource/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz -O PAAD11N.recal_data.test.table; 13:36:33.528 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 13:36:33.547 WARN NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (No such file or directory); 13:36:33.550 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 13:36:33.551 WARN NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (No such file or directory); 13:36:33.669 INFO BaseRecalibrator - ------------------------------------------------------------; 13:36:33.670 INFO BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.2.6.1; 13:36:33.670 INFO BaseRecalibrator - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:36:33.670 INFO BaseRecalibrator - Executing as xieduo@pbs-master on Linux v3.10.0-1160.41.1.el7.x86_64 amd64; 13:36:33.670 INFO BaseRecalibrator - Java runtime: Java HotSpot(TM) 64-Bit Server VM v18+36-2087; 13:36:33.671 INFO BaseRecalibrator - Start Dat,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8005#issuecomment-1254561081:6708,load,load,6708,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005#issuecomment-1254561081,1,['load'],['load']
Performance,===========; Files 1139 1146 +7 ; Lines 60902 62029 +1127 ; Branches 9437 9684 +247 ; ===============================================; + Hits 48705 49752 +1047 ; - Misses 8401 8435 +34 ; - Partials 3796 3842 +46; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/3031?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...ellbender/tools/exome/FilterByOrientationBias.java](https://codecov.io/gh/broadinstitute/gatk/pull/3031?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9GaWx0ZXJCeU9yaWVudGF0aW9uQmlhcy5qYXZh) | `83.019% <ø> (ø)` | `14 <0> (ø)` | :arrow_down: |; | [...ls/walkers/mutect/CreateSomaticPanelOfNormals.java](https://codecov.io/gh/broadinstitute/gatk/pull/3031?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL211dGVjdC9DcmVhdGVTb21hdGljUGFuZWxPZk5vcm1hbHMuamF2YQ==) | `100% <0%> (ø)` | `10% <0%> (+3%)` | :arrow_up: |; | [...egmentation/PerformAlleleFractionSegmentation.java](https://codecov.io/gh/broadinstitute/gatk/pull/3031?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9zZWdtZW50YXRpb24vUGVyZm9ybUFsbGVsZUZyYWN0aW9uU2VnbWVudGF0aW9uLmphdmE=) | `88.889% <0%> (ø)` | `4% <0%> (+2%)` | :arrow_up: |; | [...itute/hellbender/tools/walkers/mutect/Mutect2.java](https://codecov.io/gh/broadinstitute/gatk/pull/3031?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL211dGVjdC9NdXRlY3QyLmphdmE=) | `92.593% <0%> (ø)` | `32% <0%> (+16%)` | :arrow_up: |; | [...s/spark/pathseq/PSBuildReferenceTaxonomyUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/3031?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9wYXRoc2VxL1BTQnVpbGRSZWZlcmVuY2VUYXhvbm9teVV0aWxzLmphdmE=) | `88.961% <0%> (ø)` | `39% <0%> (?)` | |; | [.../hellbender/tools/spark/utils/LongBloomFilter.java](https://codecov.io/gh/broadinstitute/gat,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3031#issuecomment-306370974:1560,Perform,PerformAlleleFractionSegmentation,1560,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3031#issuecomment-306370974,1,['Perform'],['PerformAlleleFractionSegmentation']
Performance,====================; Files 1138 1142 +4 ; Lines 62637 62823 +186 ; Branches 9521 9548 +27 ; ===============================================; + Hits 49731 49871 +140 ; - Misses 9110 9142 +32 ; - Partials 3796 3810 +14; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/3690?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...ber/coverage/readcount/ReadCountFileHeaderKey.java](https://codecov.io/gh/broadinstitute/gatk/pull/3690?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL2NvdmVyYWdlL3JlYWRjb3VudC9SZWFkQ291bnRGaWxlSGVhZGVyS2V5LmphdmE=) | `0% <0%> (ø)` | `0 <0> (?)` | |; | [...der/utils/test/IntegerReadCountFileComparator.java](https://codecov.io/gh/broadinstitute/gatk/pull/3690?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0ludGVnZXJSZWFkQ291bnRGaWxlQ29tcGFyYXRvci5qYXZh) | `70.732% <70.732%> (ø)` | `6 <6> (?)` | |; | [...pynumber/utils/CachedBinarySearchIntervalList.java](https://codecov.io/gh/broadinstitute/gatk/pull/3690?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL3V0aWxzL0NhY2hlZEJpbmFyeVNlYXJjaEludGVydmFsTGlzdC5qYXZh) | `74.603% <74.603%> (ø)` | `18 <18> (?)` | |; | [...bender/tools/copynumber/CollectFragmentCounts.java](https://codecov.io/gh/broadinstitute/gatk/pull/3690?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL0NvbGxlY3RGcmFnbWVudENvdW50cy5qYXZh) | `88.406% <88.406%> (ø)` | `11 <11> (?)` | |; | [...er/tools/spark/sv/discovery/AlignmentInterval.java](https://codecov.io/gh/broadinstitute/gatk/pull/3690?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvQWxpZ25tZW50SW50ZXJ2YWwuamF2YQ==) | `90.517% <0%> (-0.431%)` | `63% <0%> (-1%)` | |; | [...ute/hellbender/utils/read/ArtificialReadUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/3690?sr,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3690#issuecomment-335961649:1557,Cache,CachedBinarySearchIntervalList,1557,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3690#issuecomment-335961649,1,['Cache'],['CachedBinarySearchIntervalList']
Performance,========================; + Coverage 86.362% 86.626% +0.264% ; + Complexity 39551 38919 -632 ; ===============================================; Files 2362 2336 -26 ; Lines 186121 182603 -3518 ; Branches 20305 20062 -243 ; ===============================================; - Hits 160738 158181 -2557 ; + Misses 18236 17379 -857 ; + Partials 7147 7043 -104 ; ```. | [Files Changed](https://app.codecov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) | Coverage |; |---|---|; | [...lkers/vqsr/scalable/ExtractVariantAnnotations.java](https://app.codecov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvRXh0cmFjdFZhcmlhbnRBbm5vdGF0aW9ucy5qYXZh) | `ø` |; | [...walkers/vqsr/scalable/ScoreVariantAnnotations.java](https://app.codecov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvU2NvcmVWYXJpYW50QW5ub3RhdGlvbnMuamF2YQ==) | `0.000%` |; | [.../scalable/data/LabeledVariantAnnotationsDatum.java](https://app.codecov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvZGF0YS9MYWJlbGVkVmFyaWFudEFubm90YXRpb25zRGF0dW0uamF2YQ==) | `ø` |; | [...scalable/modeling/BGMMVariantAnnotationsModel.java](https://app.codecov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstit,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8131#issuecomment-1352314153:1950,scalab,scalable,1950,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8131#issuecomment-1352314153,1,['scalab'],['scalable']
Performance,=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvY2FjaGUvU2lkZVJlYWRJbnB1dENhY2hlU3RyYXRlZ3kuamF2YQ==) | `81.481% <81.481%> (ø)` | |; | [...adinstitute/hellbender/engine/ReadsDataSource.java](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUmVhZHNEYXRhU291cmNlLmphdmE=) | `91.667% <84.615%> (+33.333%)` | :arrow_up: |; | [...ellbender/engine/VariantWalkerIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvVmFyaWFudFdhbGtlckludGVncmF0aW9uVGVzdC5qYXZh) | `87.288% <86.667%> (ø)` | |; | [...engine/cache/DrivingFeatureInputCacheStrategy.java](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvY2FjaGUvRHJpdmluZ0ZlYXR1cmVJbnB1dENhY2hlU3RyYXRlZ3kuamF2YQ==) | `88.000% <88.000%> (ø)` | |; | [...ellbender/engine/cache/LocatableCacheUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvY2FjaGUvTG9jYXRhYmxlQ2FjaGVVbml0VGVzdC5qYXZh) | `96.471% <96.471%> (ø)` | |; | [...gumentcollections/ReadInputArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#di,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4902#issuecomment-397744741:3501,cache,cache,3501,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4902#issuecomment-397744741,1,['cache'],['cache']
Performance,"> 15:39:56.079 INFO ProgressMeter - 13:37022394 4.9 5628000 1143956.7; > 15:40:06.117 INFO ProgressMeter - 16:14727212 5.1 5728000 1125992.7; > 15:40:16.383 INFO SplitNCigarReads - Shutting down engine; > [March 2, 2023 3:40:16 PM EST]; > org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads done.; > Elapsed time: 5.27 minutes.; > Runtime.totalMemory()=3432513536; > java.lang.ClassCastException: htsjdk.samtools.BAMRecord cannot be cast to; > java.lang.Comparable; > at java.util.Arrays$NaturalOrder.compare(Arrays.java:102); > at java.util.TimSort.countRunAndMakeAscending(TimSort.java:355); > at java.util.TimSort.sort(TimSort.java:234); > at; > java.util.ArraysParallelSortHelpers$FJObject$Sorter.compute(ArraysParallelSortHelpers.java:145); > at java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:731); > at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); > at java.util.concurrent.ForkJoinTask.doInvoke(ForkJoinTask.java:401); > at java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:734); > at java.util.Arrays.parallelSort(Arrays.java:1180); > at; > htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:247); > at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:182); > at; > htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:202); > at; > htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:36); > at; > htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:16); > at; > htsjdk.samtools.util.AbstractAsyncWriter$WriterRunnable.run(AbstractAsyncWriter.java:123); > at java.lang.Thread.run(Thread.java:750); > Suppressed: htsjdk.samtools.util.RuntimeIOException: Attempt to add record; > to closed writer.; > at; > htsjdk.samtools.util.AbstractAsyncWriter.write(AbstractAsyncWriter.java:57); > at; > htsjdk.samtools.AsyncSAMFileWriter.addAlignment(AsyncSAMFileWriter.java:58); > at; > org.broadinstitute.hellbender.utils.read.SAMFi",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452528344:6761,concurren,concurrent,6761,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452528344,1,['concurren'],['concurrent']
Performance,"> > Hi, Meng,; > > I also meet this issue, have you solve it?; > > Thank you; > ; > Hello,; > Actually, I didn't solve it completely.; > When I change to another server, it can run 2.14GB's data well.; > I think it's because the data is too large, and the server can't perform it normally.; > If it's not necessary, you can choose other tools.; > ; > Best wish; > Meng. Hi @MengZhang2019 Meng, Thank you; I also didn't solve this problem.; Do you have any tools recommend? I have tried other methods, but the running time was too long, they do not suit large sample data.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6293#issuecomment-625218636:269,perform,perform,269,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6293#issuecomment-625218636,1,['perform'],['perform']
Performance,"> > In the latest filtering paradigm, how would somebody who only wanted variants with really high quality bases change the default parameters?; > ; > You could decrease `f-score-beta` (default 1.0) to bias the threshold optimization in favor of precision versus sensitivity. Okay. Rip it out.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5827#issuecomment-475756509:221,optimiz,optimization,221,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5827#issuecomment-475756509,1,['optimiz'],['optimization']
Performance,"> > Not to mention, in theory one could have some job trying to read the original workspace, which might get hosed if some other job is trying to edit that workspace in place.; > ; > This is not possible as new GenomicsDB workspace fragments are created for the incremental updates. During the actual finalization of the fragments, the array in the workspace is locked using Posix file locks for concurrency. As @nalinigans says, individual arrays/intervals in a GenomicsDB workspace will be consistent during incremental import. One caveat though, since each array is independently updated, different arrays/intervals will finish adding samples at different times while incremental import is in progress. So, querying a workspace that is being incrementally imported into isn't recommended.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6558#issuecomment-617405801:396,concurren,concurrency,396,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6558#issuecomment-617405801,1,['concurren'],['concurrency']
Performance,> @Bowen1992 Could you please try running with the latest GATK release (`4.2.6.1`) and reporting whether the issue persists?. Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx50G -Djava.io.tmpdir=./tmp -jar /public/home/gaoshibin/software/GATK/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar GenotypeGVCFs -R /public/home/gaoshibin/B73_REF/Zea_mays.AGPv4.dna.toplevel.fa -V gendb://./CHR7_gvcf_database -G StandardAnnotation --genomicsdb-shared-posixfs-optimizations true -O new_ALL_MATERIALS_chr7.g.vcf.gz; 17:49:50.404 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 17:49:50.653 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/public/home/gaoshibin/software/GATK/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 17:49:51.271 INFO GenotypeGVCFs - ------------------------------------------------------------; 17:49:51.273 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.6.1; 17:49:51.273 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:49:51.273 INFO GenotypeGVCFs - Executing as gaoshibin@comput6 on Linux v3.10.0-693.el7.x86_64 amd64; 17:49:51.274 INFO GenotypeGVCFs - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_211-b12; 17:49:51.274 INFO GenotypeGVCFs - Start Date/Time: 2022年5月22日 下午05时49分50秒; 17:49:51.274 INFO GenotypeGVCFs - ------------------------------------------------------------; 17:49:51.275 INFO GenotypeGVCFs - ------------------------------------------------------------; 17:49:51.276 INFO GenotypeGVCFs - HTSJDK Version: 2.24.1; 17:49:51.276 INFO GenotypeGVCFs - Picard Version: 2.27.1; 17:49:51.276 INFO GenotypeGVCFs - Built for Spark Version: 2.4.5; 17:49:51.277 INFO GenotypeGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 17:49:,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7866#issuecomment-1135302097:574,optimiz,optimizations,574,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7866#issuecomment-1135302097,2,"['Load', 'optimiz']","['Loading', 'optimizations']"
Performance,"> At the very least we should add a unit test that generates the evidenceIndexBySampleIndex cache, then calls marginalize() (both types) and asserts that we have emptied the cache. I would do the same for appendEvidence() and addMissingAlleles(). It's simpler than this because allele operations such as `marginalize()` and `addMissingAlleles` don't modify the evidence list. While they require care with the likelihoods arrays they don't require anything at all from the evidence-to-index caches. As I mentioned above, I left the cache updating in `appendEvidence` as it was because it was so simple. I will try to write the test for removing evidence tomorrow. Tempting to try tonight, but I'm trying to accept the reality that working until 2 am is a bad idea.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6593#issuecomment-633180869:92,cache,cache,92,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6593#issuecomment-633180869,4,['cache'],"['cache', 'caches']"
Performance,"> Do you have numbers for the performance here? How big is this code in the profiler before or after? I'm curious. The unit tests are about 5% faster. In practice, this won't affect HaplotyeCaller because the overwhelming majority of the CPU cost of those tests comes from the ploidy = 20, allele count = 6 cases. For anything else the genotyping likelihoods calculation is not only not a bottleneck, it's completely negligible.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-580337113:30,perform,performance,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-580337113,2,"['bottleneck', 'perform']","['bottleneck', 'performance']"
Performance,"> Hi, I am trying to generate vcf using GATK pipeline from bam file, but everytime, I am getting the following exception: 01:13:15.801 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data1/ngs/programs/gatk-4.0.0.0/gatk-package-4.0.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so 01:13:16.075 INFO HaplotypeCaller - ------------------------------------------------------------ 01:13:16.075 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.0.0.0 01:13:16.075 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/ 01:13:16.076 INFO HaplotypeCaller - Executing as shashank@grande on Linux v3.13.0-79-generic amd64 01:13:16.076 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_72-internal-b15 01:13:16.076 INFO HaplotypeCaller - Start Date/Time: January 18, 2020 1:13:15 AM IST 01:13:16.076 INFO HaplotypeCaller - ------------------------------------------------------------ 01:13:16.076 INFO HaplotypeCaller - ------------------------------------------------------------ 01:13:16.077 INFO HaplotypeCaller - HTSJDK Version: 2.13.2 01:13:16.077 INFO HaplotypeCaller - Picard Version: 2.17.2 01:13:16.077 INFO HaplotypeCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 1 01:13:16.078 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false 01:13:16.078 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true 01:13:16.078 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false 01:13:16.078 INFO HaplotypeCaller - Deflater: IntelDeflater 01:13:16.078 INFO HaplotypeCaller - Inflater: IntelInflater 01:13:16.078 INFO HaplotypeCaller - GCS max retries/reopens: 20 01:13:16.078 INFO HaplotypeCaller - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes 01:13:16.078 INFO HaplotypeCaller - Initializing engine 01:13:17.087 INFO HaplotypeCaller - ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5947#issuecomment-1605272955:162,Load,Loading,162,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5947#issuecomment-1605272955,1,['Load'],['Loading']
Performance,"> Hi, Meng,; > I also meet this issue, have you solve it?; > Thank you. Hello,; Actually, I didn't solve it completely.; When I change to another server, it can run 2.14GB's data well.; I think it's because the data is too large, and the server can't perform it normally. ; If it's not necessary, you can choose other tools. Best wish; Meng",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6293#issuecomment-623968600:251,perform,perform,251,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6293#issuecomment-623968600,1,['perform'],['perform']
Performance,> I also encounter this error when most samples have been imported. I ran importing in batches '--batch-size 50 --consolidate '. The error occured at the last batch. Can I reuse some of the imported data files or have to rerun the whole importing again?. ...; 13:13:26.069 INFO GenomicsDBImport - Done importing batch 21/22; 13:13:26.069 INFO GenomicsDBImport - Starting batch input file preload; 13:13:27.440 INFO GenomicsDBImport - Finished batch preload; 13:13:27.440 INFO GenomicsDBImport - Importing batch 22 with 22 samples; [TileDB::Buffer] Error: Cannot read from buffer; End of buffer reached.; [TileDB::BookKeeping] Error: Cannot load book-keeping; Reading tile offsets failed.; terminate called after throwing an instance of 'VariantStorageManagerException'; what(): VariantStorageManagerException exception : Error while consolidating TileDB array chrY$1$57227415; TileDB error message : [TileDB::BookKeeping] Error: Cannot load book-keeping; Reading tile offsets failed,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6519#issuecomment-641091886:640,load,load,640,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6519#issuecomment-641091886,2,['load'],['load']
Performance,"> I think we've seen similar issues before. Libgomp needs to be installed and findable. I think it typically is installed when you install gcc. See #6012 for more discussion. Thank you, I checked my system and found gcc module not loaded. I reloaded my gcc by typing; `module load gcc/5.4.0`; and it works now. Thank you!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8194#issuecomment-1425155969:231,load,loaded,231,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8194#issuecomment-1425155969,2,['load'],"['load', 'loaded']"
Performance,"> If one thinks about a genomicsDB workspace more like a database than single file, are there defrag/shrink-like tasks that need to be performed on the workspace for efficiency?. Did you use the `--consolidate` option with GenomicsDBImport? This option consolidates fragments to help with query performance later. Usually not needed for very small batch sizes.; Also, available from 4.1.7.0 is a `--genomicsdb-shared-posixfs-optimizations` option. Can you try GenotypeGVCFs with this knob turned on if your workspace is on NFS/Lustre and let us know?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6688#issuecomment-669344356:135,perform,performed,135,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6688#issuecomment-669344356,3,"['optimiz', 'perform']","['optimizations', 'performance', 'performed']"
Performance,"> It would appear this does what it says on the tin, though I'm curious what the scenario was where the absence of the timestamp caused a call caching issue. I reuse a dataset for development (rsa_gvs_quickstart_dev). One past run was did not use compressed references, so that is always used when call caching is turned on, even though the dataset has reingested compressed references since then. This is the exact scenario that `GetBQTableLastModifiedDatetime` was created for — database-based tasks that we want to be able to call cache accurately.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8667#issuecomment-1917045421:534,cache,cache,534,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8667#issuecomment-1917045421,1,['cache'],['cache']
Performance,"> Just curious, why no last modified checks? Was it to keep the code simpler?. Mostly because I couldn't readily think of a scenario where I would actually want this to call cache, but I could easily imagine call caching leading to undesired clobbering of previously generated results. We can certainly revisit this decision if it turns out we're using the script in ways where we really would want call caching.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8002#issuecomment-1227739990:174,cache,cache,174,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8002#issuecomment-1227739990,1,['cache'],['cache']
Performance,"> One final thing: i'm happy to try to debug this, and was going to write a test case based on the existing GenomicsDB integration tests. However, when I try to run any integration test involving genomicsdb, I get an exception like the following. I am on windows, so perhaps this is the issue?; > ; > 09:03:37.460 FATAL GenomicsDBLibLoader -; > java.io.FileNotFoundException: File /tiledbgenomicsdb.dll was not found inside JAR.; > at org.genomicsdb.GenomicsDBLibLoader.loadLibraryFromJar(GenomicsDBLibLoader.java:118) ~[genomicsdb-1.3.2.jar:?]; > at org.genomicsdb.GenomicsDBLibLoader.loadLibrary(GenomicsDBLibLoader.java:55) [genomicsdb-1.3.2.jar:?]; > at org.genomicsdb.GenomicsDBUtilsJni.(GenomicsDBUtilsJni.java:30) [genomicsdb-1.3.2.jar:?]; > at org.genomicsdb.GenomicsDBUtils.createTileDBWorkspace(GenomicsDBUtils.java:46) [genomicsdb-1.3.2.jar:?]; > at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.overwriteCreateOrCheckWorkspace(GenomicsDBImport.java:1005) [classes/:?]; > at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.onTraversalStart(GenomicsDBImport.java:661) [classes/:?]; > at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1056) [classes/:?]. Yes, Windows is not supported by GenomicsDB. This is mentioned obliquely in the requirements for gatk too -; ```; Operating system. The GATK runs natively on most if not all flavors of UNIX, which includes MacOSX, Linux and BSD. It is possible to get it ; running on some recent versions of Windows, but we don't provide any support nor instructions for that. If you need to run on; a Windows machine, consider using Docker.; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7005#issuecomment-754106359:470,load,loadLibraryFromJar,470,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7005#issuecomment-754106359,2,['load'],"['loadLibrary', 'loadLibraryFromJar']"
Performance,"> Running with default arguments locally the runtime (for a WGS full chr15) drops from ~8.9 minutes to ~4.7 minutes after this patch. If I had to peg something else to optimize it would be replacing CSVWriter which seems to be somewhat slow but I can be contented that this tool is reasonably fast when nothing pathological is being triggered. Hello, you mentioned that running DepthOfCoverage for a WGS full chr15 only takes ~4.7 minutes. Would you mind letting me know what is the coverage for the BAM you use? It took days for me to run DepthOfCoverage on a 80X WGS. And, will there be a Spark implementation for DepthOfCoverage in the near future? Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6740#issuecomment-723391687:168,optimiz,optimize,168,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6740#issuecomment-723391687,1,['optimiz'],['optimize']
Performance,"> Therefore we don't actually have to take the log of the probability and normalize it, we can just take the probability straight from HypergeometricDistribution. The problem is mostly inside `HypergeometricDistribution`. A better implementation of this class should cache the last value, such that computing `hypergeo(i)` and then `hypergeo(i+1)` consecutively does not unnecessarily trigger full computation, which is quite expensive. Anyway, the current implementation is fine given that exact test is unlikely to be a bottleneck in its current use.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266857573:267,cache,cache,267,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266857573,2,"['bottleneck', 'cache']","['bottleneck', 'cache']"
Performance,"> What happen to the bundling performance improvement changes by the way?. The large 2D file array can be handled by the latest Cromwell versions, so we do not need to bundle. It is much more elegant and readable this way and should actually improve performance.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6607#issuecomment-672068385:30,perform,performance,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6607#issuecomment-672068385,2,['perform'],['performance']
Performance,">In the latest filtering paradigm, how would somebody who only wanted variants with really high quality bases change the default parameters?. You could decrease `f-score-beta` (default 1.0) to bias the threshold optimization in favor of precision versus sensitivity.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5827#issuecomment-475728493:212,optimiz,optimization,212,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5827#issuecomment-475728493,1,['optimiz'],['optimization']
Performance,"@Aqoolare Hello. There are a a few things going on here. The `unrecognized runtime attribute keys` warning is coming from cromwell. It's telling you that the cromwell **local** backend doesn't understand those keys, which is true. That means it's just ignoring them. I think the actual problem is different though. You're running the spark tool in spark local mode, which in this case isn't configured to use the correct amount of cores or memory. I think the intent of this wdl script was that it would be run in a container on a cluster and the container would restrict the cores and memory options. In any case, it's not configured correctly for what you need. I would skip running cromwell and just invoke gatk directly since this wdl only executes a single job. Since this is going to run spark in local mode you need to specify the number of cores using the `--spark-master` argument, and set the memory using the `--java-options ""-Xmx""` arguments. For example:. ```; gatk ReadsPipelineSpark ; --java-options ""-Xmx16G"" ; --spark-master 'local[8]'; --I yourbam.; ... etc; ```. The above command is specifying to use 8 (that's what the local[**8**] means) cores for spark and give it 16G of memory. Your job was accidentally using 200 cores so it doesn't surprise me that it would run into memory issues. Using spark with more than 16ish cores in a single process is going to bog down a lot. I think 8 is a good starting place to try. If you want to go wider you should really look into running a proper cluster (or using dataproc), but there's pretty heavy diminishing returns. Try 8 or 16 and tune the memory from there.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7796#issuecomment-1108913771:1599,tune,tune,1599,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7796#issuecomment-1108913771,1,['tune'],['tune']
Performance,"@Aqoolare You might, but there are scaling issues when running within a single java process which is what you're doing. There are issues with lock contention and garbage collection which cause more cores to not be utilized very well. (Lots of cores waiting while garbage collection stops the world, that sort of thing.). . You could definitely test it. We found that 8-16 cores was optimal for our use cases for running in spark local mode, but spark performance is extremely finicky and it's very possible your system might do better than what I'm used to. If you wantt to go very parallel it works better to run an actual spark cluster. You should be able to utilize cores more efficiently that way, but it's more complicated to set up and operatte and there are still bottlenecks that keep it from being infinitely scaleable. (IO bandwidth and network traffic being important ones). . There are lots of articles on the internet about how to set up a local yarn cluster that can help walk you through it if you want to try. . I'm going to close this ticket since it seems like the problem is solved. Feel free to reopen or open a new one if you have other issues.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7796#issuecomment-1111473992:451,perform,performance,451,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7796#issuecomment-1111473992,2,"['bottleneck', 'perform']","['bottlenecks', 'performance']"
Performance,"@Bowen1992 **I got the same error, do you have a solution now?**. Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx76800m -jar /home/zwc1988/miniconda3/envs/gatk/share/gatk4-4.2.6.1-0/gatk-package-4.2.6.1-local.jar GenotypeGVCFs -R data/ref/CL200105941_L02.fa -V gendb://results/genotype/genodb/group2 -O results/genotype/vcfs/group2.vcf.gz --tmp-dir ./tmp; 18:24:10.205 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/zwc1988/miniconda3/envs/gatk/share/gatk4-4.2.6.1-0/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 18:24:10.451 INFO GenotypeGVCFs - ------------------------------------------------------------; 18:24:10.452 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.6.1; 18:24:10.452 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 18:24:10.452 INFO GenotypeGVCFs - Executing as zwc1988@fat01 on Linux v3.10.0-957.el7.x86_64 amd64; 18:24:10.453 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_312-b07; 18:24:10.453 INFO GenotypeGVCFs - Start Date/Time: June 19, 2022 6:24:10 PM CST; 18:24:10.453 INFO GenotypeGVCFs - ------------------------------------------------------------; 18:24:10.453 INFO GenotypeGVCFs - ------------------------------------------------------------; 18:24:10.454 INFO GenotypeGVCFs - HTSJDK Version: 2.24.1; 18:24:10.454 INFO GenotypeGVCFs - Picard Version: 2.27.1; 18:24:10.454 INFO GenotypeGVCFs - Built for Spark Version: 2.4.5; 18:24:10.454 INFO GenotypeGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 18:24:10.455 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 18:24:10.455 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 18:24:10.455 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 18:24:10.455 INFO Genotype",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7866#issuecomment-1159695894:523,Load,Loading,523,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7866#issuecomment-1159695894,1,['Load'],['Loading']
Performance,"@Cashalow Can you include the full command line? 50 human genomes (ploidy 2) would need less than 7GB memory in my experience, even for highly multi-allelic indel sites. I would expect ploidy 1 calls (as most users run microbial genomes) would require even less, but we have some diploid-specific optimizations. Are you using the `new-qual` argument? That QUAL calculation algorithm is less computationally intensive and I believe it is applicable to all ploidies.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4467#issuecomment-370462977:297,optimiz,optimizations,297,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4467#issuecomment-370462977,1,['optimiz'],['optimizations']
Performance,"@DarioS I wonder if you're looking at the JVM garbage collector threads -- by default, Java uses a multi-threaded garbage collector. You can control the number of threads it uses via the `-XX:ParallelGCThreads=N` argument, where N is the number of garbage collector threads. To pass this option into GATK, use the `--java-options` argument. Eg., `./gatk --java-options '-XX:ParallelGCThreads=4'`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7156#issuecomment-804282075:99,multi-thread,multi-threaded,99,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7156#issuecomment-804282075,1,['multi-thread'],['multi-threaded']
Performance,"@DonFreed, I agree with @magicDGS's assessment about it. This feels like a fix that was applied to Gatk3 but doesn't translate to 4? Of course, there could be implementations of GATKRead that don't obey the given contract about copying, but it's worth fixing those since we were more careful to think about copy/no copy when we wrote the new interface. . Of note: if you haven't seen it, `GATKRead` provides a set of unsafe `getBaseQualitiesNoCopy()` methods for times when the copy is a performance bottleneck and you can guarantee safe use of the underlying array. . I'm going to close this. Feel free to reopen if you disagree / can provide a unit test that demonstrates the issue still exists.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4926#issuecomment-399219562:488,perform,performance,488,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4926#issuecomment-399219562,2,"['bottleneck', 'perform']","['bottleneck', 'performance']"
Performance,"@EdwardDixon I did not know that! In that case master does already require AVX. If it only impacts this tool and we provide sufficient warning and instructions, I think the single intel-optimized conda environment will be so much easier to test and maintain. Users who don't have AVX can simply install an older tensorflow in their environment, but GATK doesn't need to worry about it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-429142837:186,optimiz,optimized,186,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-429142837,1,['optimiz'],['optimized']
Performance,"@EdwardDixon Thanks for this! I think the AVX check in CNNScoreVariants is good. As @droazen points out, we still want the split environments, though now with the check in place I think we can make intel optimized the default. Other thoughts @droazen, @cmnbroad ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-428272123:204,optimiz,optimized,204,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-428272123,1,['optimiz'],['optimized']
Performance,"@EdwardDixon Well, you'd be surprised at some of the hardware we have to deal with. Even some machines here at the Broad don't have AVX. In general, our policy with hardware-dependent optimizations in GATK has been to insist on having a transparent fallback mechanism when the required hardware isn't present -- I'd really prefer not to start making exceptions to that rule. Could the Intel-optimized Tensorflow be patched to fall back to vanilla tensorflow when AVX is not present? Is that an option? Or could it at least be patched to not actually crash in that case?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5142#issuecomment-417073151:184,optimiz,optimizations,184,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5142#issuecomment-417073151,2,['optimiz'],"['optimizations', 'optimized']"
Performance,"@EvanTheB crai index is supported. This ticket is quite old, and quite a few changes have been made to CRAM in the interim (especially to index queries, some of which affected both .crai and .bai). The TL;DR version is that while there are still open tickets in htsjdk relating to CRAM performance, and some to index queries, I don't see any that would have an obvious connection to the HC discrepancies reported in the forum post.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2850#issuecomment-405930781:286,perform,performance,286,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2850#issuecomment-405930781,1,['perform'],['performance']
Performance,@Fruit-loops this happens for many-allelic sites. SelectVariants historically assumed every input was a germline site with PL values (genotype likelihoods) for every possible combination of alleles and ploidy and generated an expensive cache of these indices. For somatic sites this concept is meaningless and for many-allelic sites it's combinatorially expensive. The PR mentioned above skips the expensive calculation for somatic sites that lack PLs.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6254#issuecomment-583801876:236,cache,cache,236,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6254#issuecomment-583801876,1,['cache'],['cache']
Performance,"@Horneth: what @lbergelson said. Java will use all of the memory you give it, regardless of how much the program asks for. The only difference is how long it takes until the memory's used up and the garbage collector needs to kick in. Thus lowering the memory requirements of a Java program tends to increase its performance, up to the point where we cut memory that it needed. . I would suggest either measuring the memory immediately after a garbage collection or, more directly, measure the performance of the program as you vary buffer sizes. Make sure to control for cache effects since the program's doing I/O.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2640#issuecomment-298973266:313,perform,performance,313,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2640#issuecomment-298973266,3,"['cache', 'perform']","['cache', 'performance']"
Performance,"@Horneth:. - With 100 samples, is the tool always running out of memory, or is it only running out of memory with certain buffer sizes?. - If I'm interpreting your document correctly, you found that with a buffer size of 0 or 1, performance degrades by 10%, is that right?. - Can you post the file sizes involved here? Both the size of the original GVCF inputs, *and* the size of the data from those inputs that overlaps your interval (you can find out the latter by running GATK4 SelectVariants on the GVCF using the same interval, and recording the resulting file size). I'd also like to know the sizes of the index files. - It would be good if you could post your profiling results directly in this ticket, rather than in a Google doc, so that @jean-philippe-martin (who has implemented the GCS support in GATK) can easily see them. @jean-philippe-martin Could you chime in here to remind us of your profiling results with NIO buffer sizes and the use case of a single query interval? Didn't you find that there was a very large performance difference between running with and without buffering?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2640#issuecomment-298927579:229,perform,performance,229,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2640#issuecomment-298927579,2,['perform'],['performance']
Performance,"@LeeTL1220 - I addressed in the next commit the documentation issues; let me know if you have more suggestions on that. If you have any suggestion for an already implemented tool in GATK for the performance and some data for profiling, I can do a couple of runs in my local computer for look at them. If not, I can implement an small example tool for profiling when splitting by sample is needed (I can also modify `ExampleLocusWalker` for that in a different PR to being able to profile this after this PR too).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2321#issuecomment-332227961:195,perform,performance,195,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2321#issuecomment-332227961,1,['perform'],['performance']
Performance,"@LeeTL1220 A few minor remaining comments. Do what you will. How much of a performance impact does the change have? You said it slows it down, is it significant? It might be faster if you make it a long instead of an atomic long which should be safe it it's single threaded and you don't use parallel streams anywhere.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3305#issuecomment-316490330:75,perform,performance,75,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3305#issuecomment-316490330,1,['perform'],['performance']
Performance,"@MartonKN I've labeled the update of the caller as a ""reach"", so I'm not expecting that it gets done before release. However, I expect that the tutorial data should be updated well before release. The tutorial data runs quickly (~1 hr for coverage collection, which is mostly limited by the slowest samples or cloud preemptions, and then ~minutes once collection has been call cached), so we should have plenty of time. Whether or not the actual tutorial itself will be ready depends on whether @sooheelee has available bandwidth and if it is a high priority for comms.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3826#issuecomment-353730988:377,cache,cached,377,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3826#issuecomment-353730988,1,['cache'],['cached']
Performance,"@Neato-Nick ~~No, it's very same.~~ Sorry, typing too fast. I tried it again and it crashes:. Using GATK jar /home/vojta/bin/gatk/gatk-package-4.0.5.2-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/vojta/bin/gatk/gatk-package-4.0.5.2-local.jar GenotypeGVCFs -O rad34test.comb2.raw.g.vcf.gz -R ../../../jic_reference/alygenomes.fasta -V rad34test.comb2.raw.vcf.gz -new-qual; 21:10:39.975 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/vojta/bin/gatk/gatk-package-4.0.5.2-local.jar!/com/intel/gkl/native/libgkl_compression.so; 21:10:43.152 INFO GenotypeGVCFs - ------------------------------------------------------------; 21:10:43.153 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.0.5.2; 21:10:43.153 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 21:10:44.334 INFO GenotypeGVCFs - Initializing engine; 21:10:44.849 INFO FeatureManager - Using codec VCFCodec to read file file:///home/vojta/dokumenty/fakulta/botanika/arabidopsis/samples/lib_2018_06/4_joined/rad34test.comb2.raw.vcf.gz; 21:10:44.979 INFO GenotypeGVCFs - Done initializing engine; 21:10:45.057 INFO ProgressMeter - Starting traversal; 21:10:45.057 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 21:10:45.344 WARN InbreedingCoeff - Annotation will not be calculated, must provide at least 10 samples; 21:10:47.780 INFO GenotypeGVCFs - Shutting down engine; [2. července 2018 21:10:47 CEST] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 0.13 minutes.; Runtime.totalMemory()=501219328; java.lang.IllegalArgumentException: log10LikelihoodsOfAC are bad 6.911788849595091E-17,NaN; at org.broadinstitute.hellbender.tools.walkers.genotyper.afcalc.AFCalculationResult.<init>(AFCalculationResult.java:72); at org.broadinstitute.hellb",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4544#issuecomment-401904928:559,Load,Loading,559,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4544#issuecomment-401904928,1,['Load'],['Loading']
Performance,"@RWilton The `HaplotypeCaller` performs two separate filtering passes on the read mapping qualities:. The first is in the `MappingQualityReadFilter`, and occurs before the `HaplotypeCaller` even sees the reads. This filtering pass can be controlled via the `--minimum-mapping-quality` argument you're using (and I can confirm that this argument is hooked up and does work as intended). The second pass is in `HaplotypeCallerEngine.filterNonPassingReads()`. This happens later (just before genotyping), and is hardcoded to filter out reads with mapping quality < 20. There is no way to control the mapping quality threshold in this second pass, as allowing reads with low mapping qualities into the genotyper would result in large numbers of false positive variant calls. You can confirm that the `--minimum-mapping-quality` argument for `MappingQualityReadFilter` works as intended by setting it to a value higher than 20, and inspecting the differences in the output.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6854#issuecomment-701446008:31,perform,performs,31,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6854#issuecomment-701446008,1,['perform'],['performs']
Performance,"@SHuang-Broad . Nice hack using the cluster name. I don't see any other way to pass an arg to an initialization action. I have one suggestion to consider, but if you think it's too much work or not worth it feel free to skip: what if we separated out the reference bundle to copy from the data by specifying them both in the cluster name? That way we could, say, load either the hg19 or hg38 reference depending on the data we might be working with. So you could say ""cluster-hg38"" or ""cluster-hg19"" or ""cluster-hg19-na12878"". . Carrying it further, if we had a special convention for specifying data, like ""data-$SAMPLE"", we could just map $SAMPLE to a subdirectory on the bucket. That would provide a ton of flexibility. One minor note while you are messing with these scripts: the createCluster.sh script comment says ""This script deletes a Google Dataproc cluster used for running the GATK-SV pipeline."" Could you change to say it creates rather than deletes a cluster?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2456#issuecomment-286876038:363,load,load,363,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2456#issuecomment-286876038,1,['load'],['load']
Performance,@SHuang-Broad where does this optimization around soft clipping come from? How does it get turned on or off?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2595#issuecomment-294157699:30,optimiz,optimization,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2595#issuecomment-294157699,1,['optimiz'],['optimization']
Performance,"@SaintBacchus Yes, this work is in progress. The first step was to add a ""strict"" mode to `HaplotypeCallerSpark` that causes it to match the output of the regular `HaplotypeCaller` very closely (this was done in https://github.com/broadinstitute/gatk/pull/5416). The next step will be to improve the performance of the Spark version.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5265#issuecomment-451206406:300,perform,performance,300,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5265#issuecomment-451206406,1,['perform'],['performance']
Performance,"@SusieX . Marking duplicates: I do recommend removing duplicates (we run MarkDuplicates from Picard). . BQSR: The pipeline we're developing is for Whole Genome data, so our bams have gone through BQSR in the whole genome pipeline. We're using those recalibrated base qualities. I haven't tested running BQSR only on the mitochondria so I don't know how well that would work. . If you do need to run BQSR only on the mitochondria I'd start by using the phylotree sites as `--known-sites`, but you'd need to have those sites in vcf format. Again, I haven't tested this so I don't know how well it will perform. If you end up using BQSR I think you're pipeline (BAM -> remove dup -> BQ recalibrate -> Mutect2 call -> FilterMutectCalls) is correct. Good luck!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5193#issuecomment-431852996:600,perform,perform,600,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5193#issuecomment-431852996,1,['perform'],['perform']
Performance,"@akiezun ; I agree with separation of a native code repo from gatk. But can we have a single source tree for the native code? It will be useful for code maintenance to keep the common files in a single repository since 75% of the code are common between AVX and PPC. The source tree originally had the file set for the 128-bit vector ISA in GATK3, and the PPC porting adds only one file to the file set. We can split the files into the common and cpu-specific file sets. You propose two repos for the native library. Can you create a single source tree without code duplication (below) by using the two repos?. ```; common/ (~1500 lines); baseline.cc; common_data_structure.h; headers.h; jni_common.h; LoadTimeInitializer.{h,cc}; org_broadinstitute_hellbender_utils_pairhmm_VectorLoglessPairHMM.{h,cc}; pairhmm-template-kernel.cc; template.h; utils.{h,cc}; avx/ (~450 lines, including the file set for the 256-bit vector ISA; enabled for AVX); define-double.h; define-float.h; function_instantiations.cc (old name: avx_function_instantiations.cc); headers_md.h; shift_template.cc; template_md.h; utils_md.{h,cc}; power8/ (~490 lines, including the file set for the 128-bit vector ISA; enabled for POWER8 (optional: SSE)); define-double.h (old name: define-sse-double.h); define-float.h (old name: define-sse-float.h); function_instantiations.cc (old name: sse_function_instantiations.cc); headers_md.h; power8.h; shift_template.cc; template_md.h; utils_md.{h,cc}; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1748#issuecomment-215007739:702,Load,LoadTimeInitializer,702,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1748#issuecomment-215007739,1,['Load'],['LoadTimeInitializer']
Performance,"@akiezun @droazen @lbergelson Found the issue with FTZ being cleared. The FTZ setting only applies to the thread where FTZ is set. When running tests in gradle/testng, each test is run in a new thread. However, the pairHMM native library is only loaded for the first HaplotypeCaller test, since code in `VectorLoglessPairHMM.java` prevents the library from being loaded more than once in the same JVM. This means only the first test uses FTZ. A code change that loads the pairHMM native library for each test resolves the issue, and all `HaplotypeCallerIntegrationTest` tests pass. Another option to explore is setting FTZ in `main`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1771#issuecomment-216259701:246,load,loaded,246,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1771#issuecomment-216259701,3,['load'],"['loaded', 'loads']"
Performance,@akiezun @droazen Here are my performance results for the latest commit using `-ERC GVCF` and `--activeRegionsOnly`. **Original Code**. ```; Elapsed time: 2.54 minutes.; Time in Smith-Waterman search = 0.0037741310000000004 sec; Time in Smith-Waterman full = 19.315050559 sec; Total Smith-Waterman calls = 84948 (0 substring matches = 0.00 %); ```. **Code with lastIndexOf for array**. ```; Elapsed time: 2.43 minutes.; Time in Smith-Waterman search = 0.020596938000000002 sec; Time in Smith-Waterman full = 12.461222691000001 sec; Total Smith-Waterman calls = 84948 (69338 substring matches = 81.62 %); ```. ~5% improvement for this test case. The Smith-Waterman profiling is currently enabled by default. There is also code to dump every Smith-Waterman call and result (currently disabled by default).,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1677#issuecomment-204507562:30,perform,performance,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1677#issuecomment-204507562,1,['perform'],['performance']
Performance,"@akiezun @lindenb I can help. It's probably going to be a slightly complicated process though, especially actually building the cross platform jar. I assume we're targeting OSX and x86-64 to start with, and then hopefully expanding to POWER8 in the future? . The general idea is to prebuild the c code for whatever platforms we want to support. Then package that in a structured way into a jar, and write some java code which will detect the platform at runtime and extract the correct executable into a temporary location. Then we can publish that jar as a maven artifact. We have an example of how to do the extraction in the `VectorLoglessPairHMM` constructor. It's not perfect and probably needs a bit of refactoring to make it more general but it's the right idea. Other libraries that package native code have similar examples. I.e. Snappy-java https://github.com/xerial/snappy-java. We're going to be performing similar packaging for other native dependencies that we have, so standardizing it is a good idea.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1750#issuecomment-215820135:908,perform,performing,908,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1750#issuecomment-215820135,1,['perform'],['performing']
Performance,@akiezun A few documentation issues. I think the default methods should be have the contract that they must match the behavior of the default implementations but may be implemented differently for performance reasons. . Either remove `getBaseCounts` or replace `getLength` with it.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1477#issuecomment-182546660:197,perform,performance,197,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1477#issuecomment-182546660,1,['perform'],['performance']
Performance,"@akiezun Can you determine whether you're using the HDFS -> GCS adapter in your test case? The adapter historically did have performance problems of this magnitude. As @jean-philippe-martin mentioned, we should benchmark the new NIO -> GCS support as well.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1755#issuecomment-213097025:125,perform,performance,125,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1755#issuecomment-213097025,1,['perform'],['performance']
Performance,"@akiezun Can you double-check that assumption? I am concerned because caching is disabled on the second `FeatureDataSource` for the driving variants source (the one we add to the `FeatureManager` -- see `VariantWalker.initializeDrivingVariants()`), and I know that `VariantFiltration` does queries on that cacheless data source.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1092#issuecomment-155563487:306,cache,cacheless,306,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1092#issuecomment-155563487,1,['cache'],['cacheless']
Performance,"@akiezun Change the cloud tests to broadcast a 2bit reference stored in our test GCS bucket for now? Should work fine. We need to benchmark the performance of broadcasting a 2bit reference in the cloud vs. reading it directly from a bucket from every worker, but this will require some code changes before we can do the comparison.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1756#issuecomment-213150052:144,perform,performance,144,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1756#issuecomment-213150052,1,['perform'],['performance']
Performance,"@akiezun Description of the steps to be performed:; 1. Create a ~100 GB slice of `/humgen/gsa-hpprojects/GATK/bundle/current/b37/CEUTrio.HiSeq.WGS.b37.NA12878.bam` using `PrintReads` (will need to experiment with intervals a bit to get the right size -- for reference, the first 130 megabases of chromosome 1 produces a ~14 GB bam).; 2. Create an index on the bam slice above using `samtools index`; 3. Create a slice of `/humgen/gsa-hpprojects/GATK/bundle/current/b37/dbsnp_138.b37.vcf` using `SelectVariants` and the same interval used to create the bam above.; 4. Create an index on the vcf slice above using `IndexFeatureFile`; 5. For the reference, use the existing complete `.2bit` reference `/local/dev/droazen/spark_inputs/human_g1k_v37.2bit` on `dataflow01`; 6. Put all inputs into both hdfs (for the BROADCAST implementation) and a GCS bucket (for the SHARDED implementation).; 7. Create an up-to-date hellbender spark jar from the latest master. Run the BROADCAST implementation on `dataflow01` using the unix `time` command and a script like the one below:. ```; spark-submit \; --master yarn-client \; --driver-memory 8G \; --num-executors 16 \; --executor-cores 4 \; --executor-memory 10G \; --conf spark.driver.maxResultSize=0 \; --conf spark.driver.userClassPathFirst=true \; --conf spark.executor.userClassPathFirst=true \; --conf spark.io.compression.codec=lzf \; --conf spark.yarn.executor.memoryOverhead=600 \; $JAR BaseRecalibratorSpark \; --input hdfs:///path/to/your.bam \; --output bqsr_out_broadcast.bam \; -R hdfs:///user/droazen/bqsr/human_g1k_v37.2bit \; --knownSites hdfs:///path/to/your.vcf \; --joinStrategy BROADCAST \; --apiKey $API_KEY \; --sparkMaster yarn-client; ```. I recommend running BROADCAST with 4 cores per executor and 10 GB/memory per executor (though this may need to be increased if you see any swapping). 16 executors (ie., 64 cores total) seems like a good size for our cluster as it would allow you to increase memory per core if necessary. All inpu",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/995#issuecomment-160722098:40,perform,performed,40,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/995#issuecomment-160722098,1,['perform'],['performed']
Performance,"@akiezun Hate to interrupt your performance work, but could we get this and https://github.com/broadinstitute/gatk/pull/1424 into a mergeable state this week? They are blocking one of @cmnbroad's tickets.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1425#issuecomment-203968000:32,perform,performance,32,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1425#issuecomment-203968000,1,['perform'],['performance']
Performance,@akiezun Have we captured all of the performance suggestions from Intel in github tickets? (I know this was one of them),MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1872#issuecomment-223036925:37,perform,performance,37,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1872#issuecomment-223036925,1,['perform'],['performance']
Performance,"@akiezun Here is a case study for your reference:; **Scalable Genomics Data Processing Pipeline with Alluxio, Mesos, and Minio**; https://alluxio.com/blog/scalable-genomics-data-processing-pipeline-with-alluxio-mesos-and-minio",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1508#issuecomment-288377411:53,Scalab,Scalable,53,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1508#issuecomment-288377411,2,"['Scalab', 'scalab']","['Scalable', 'scalable-genomics-data-processing-pipeline-with-alluxio-mesos-and-minio']"
Performance,"@akiezun Instead of adding these overloads would we see the same speedup if we cached the result of isUnmapped and isPaired in the adapter? That would have the downside of complicating the adapter but it might avoid adding these strange methods to the interface. . If caching seems like a bad alternative, I think maybe these methods should have names that make it clear that they're some sort of performance hack and you should generally prefer the standard ones. 'getContigUnsafe` for instance.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235101307:79,cache,cached,79,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235101307,2,"['cache', 'perform']","['cached', 'performance']"
Performance,"@akiezun Keep it open as an alpha-3 ticket, but un-assign yourself. We'll continue to work on improving performance in the next quarter.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1608#issuecomment-234342209:104,perform,performance,104,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1608#issuecomment-234342209,1,['perform'],['performance']
Performance,"@akiezun My only concern now is that someone takes log10 of a very large number triggering a massive and slow cache expansion. This caching scheme is good for clustered queries of small values, but terrible for sparse large queries. Is that a case we need to consider?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1957#issuecomment-230863916:110,cache,cache,110,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1957#issuecomment-230863916,1,['cache'],['cache']
Performance,"@akiezun That's why I was suggesting invalidating all cached values on every call to any setter -- that way we don't have to think about the nuances of when it's necessary to recalculate, and greatly reduce the risks that normally come with caching while still getting most of the performance benefit in typical usage.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235136126:54,cache,cached,54,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235136126,2,"['cache', 'perform']","['cached', 'performance']"
Performance,"@akiezun it's really important to make sure that the results match between the original and newly optimized versions (and if they don't, to make sure we understand why). Some of the ""indel stuff"" should probably stay, e.g. the masking of sites using known indels. Since we aren't running Indel Realigner anymore, there may be some ""errors"" that we do want to mask because they are alignment artifacts and not sequencing errors.; Pinging @yfarjoun for his thoughts. We can discuss this at a future methods meeting if you like.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1056#issuecomment-152598082:98,optimiz,optimized,98,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1056#issuecomment-152598082,1,['optimiz'],['optimized']
Performance,"@akiezun sorry, I apologize for didn't describe it properly. Basically, I need to obtain the base/deletion counts (and insertions after the position) from the reads that overlaps each position in the reference genome. Previously I did that with a `LocusWalker` like this:. ``` java; public Integer map(RefMetaDataTracker refMetaDataTracker, ReferenceContext referenceContext, AlignmentContext alignmentContext) {; ReadBackedPileup pileup = alignmentContext.getBasePileup();; int[] baseCounts = pileup.getBaseCounts();; int delCounts = pileup.getNumberOfDeletions();; int insCounts = pileup.getNumberOfInsertionsAfterThisElement();; // other operations ...; }; ```. I don't know exactly how could be done this using the `ReadWalker` (my first idea is to accumulate the reads in a queue with positions, but it does not seem very efficient). On the other hand, with the `IntervalWalker` I imagine that could be done iterating over each position in the interval (in the apply implementation), but for an efficient position by position analysis will be interesting to pass them in a base per base basis. What is the best approach? How are the GATK team planning to do it for their tools?. Thank you very much.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1463#issuecomment-178659953:779,queue,queue,779,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1463#issuecomment-178659953,1,['queue'],['queue']
Performance,"@akiezun thanks. I think this is ready for review now. It would be good to merge something that works, even if there are future performance and usability improvements we can do later.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1750#issuecomment-219460978:128,perform,performance,128,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1750#issuecomment-219460978,1,['perform'],['performance']
Performance,"@akiezun understanding how to optimize a Spark job can apply to multiple projects, so it doesn't seem right to put this in the readme or wiki of this project. Ideally we'd write an article for a blog or a technical presentation.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/284#issuecomment-147513499:30,optimiz,optimize,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/284#issuecomment-147513499,1,['optimiz'],['optimize']
Performance,@akiezun we still have the limitation that refs in CRAM are treated as a local file. We need a release of https://github.com/damiencarol/jsr203-hadoop so that HDFS works as a java.nio.file.Path implementation. Then we can include that JAR on the hellbender classpath and hdfs URIs will be loaded correctly.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1326#issuecomment-163879054:289,load,loaded,289,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1326#issuecomment-163879054,1,['load'],['loaded']
Performance,"@akiezun... because you are working in the performance of LIBS (#2032), could you have a look?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2041#issuecomment-235542188:43,perform,performance,43,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2041#issuecomment-235542188,1,['perform'],['performance']
Performance,"@ashwini06 . This bam appears to be malformed and it fails Picard ValidateSamFile. I think you'll need to examine the earlier stages of your pipeline that produce your bam to ensure you get a correctly formed bam. I'm going to close this ticket now since this doesn't appear to be an issue with Mutect2. (base) wm462-624:Downloads fleharty$ java -jar $PICARD ValidateSamFile I=concatenated_ACC5611A1_XXXXXX_consensusalign_ds.bam ; INFO	2020-07-14 11:25:52	ValidateSamFile	. ********** NOTE: Picard's command line syntax is changing.; **********; ********** For more information, please see:; ********** https://github.com/broadinstitute/picard/wiki/Command-Line-Syntax-Transition-For-Users-(Pre-Transition); **********; ********** The command line looks like this in the new syntax:; **********; ********** ValidateSamFile -I concatenated_ACC5611A1_XXXXXX_consensusalign_ds.bam; **********. 11:25:52.673 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/fleharty/resources/picard.jar!/com/intel/gkl/native/libgkl_compression.dylib; [Tue Jul 14 11:25:52 EDT 2020] ValidateSamFile INPUT=concatenated_ACC5611A1_XXXXXX_consensusalign_ds.bam MODE=VERBOSE MAX_OUTPUT=100 IGNORE_WARNINGS=false VALIDATE_INDEX=true INDEX_VALIDATION_STRINGENCY=EXHAUSTIVE IS_BISULFITE_SEQUENCED=false MAX_OPEN_TEMP_FILES=8000 SKIP_MATE_VALIDATION=false VERBOSITY=INFO QUIET=false VALIDATION_STRINGENCY=STRICT COMPRESSION_LEVEL=5 MAX_RECORDS_IN_RAM=500000 CREATE_INDEX=false CREATE_MD5_FILE=false GA4GH_CLIENT_SECRETS=client_secrets.json USE_JDK_DEFLATER=false USE_JDK_INFLATER=false; [Tue Jul 14 11:25:52 EDT 2020] Executing as fleharty@wm462-624 on Mac OS X 10.15.5 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_191-b12; Deflater: Intel; Inflater: Intel; Provider GCS is not available; Picard version: 2.20.4-SNAPSHOT; WARNING	2020-07-14 11:25:52	ValidateSamFile	NM validation cannot be performed without the reference. All other validations will still occur.; ERROR: Record 18321, Read name U",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6695#issuecomment-658247132:931,Load,Loading,931,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6695#issuecomment-658247132,1,['Load'],['Loading']
Performance,"@asmirnov239 I think that some of the optimizations that @vruano made to the postprocessing step concern the config JSONs, gCNV version, and interval list output added in #5176. These take a lot of time to localize when the number of shards is large but, aside from the interval list, aren't really used for anything, correct?. Were these just added for debugging purposes, or for reproducibility/provenance? Let's revisit whether it's necessary to pass these files on when we merge @vruano's changes into the canonical WDL.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4397#issuecomment-472862393:38,optimiz,optimizations,38,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4397#issuecomment-472862393,1,['optimiz'],['optimizations']
Performance,"@bbimber @mlathara Here is a pretty good article for optimizing the GenomicsDBImport [https://gatk.broadinstitute.org/hc/en-us/articles/360056138571-GDBI-usage-and-performance-guidelines] There is some advice about handling many small contigs that may be useful. . To troubleshoot the GenomicsDBImport high memory issue my script have, I reran the script on chr1 to narrow down the source of the high memory issue. These are running on reblocked gvcfs. . 1. Without --bypass-feature-reader and -consolidate; 2. With --bypass-feature-reader; 3. With --consolidate without --bypass-feature-reader (This ended up on a node with 384gb.) The other ran on 256GB nodes. . Test 2 ran the fastest with the lowest memory requirements (Wall clock 76 hours); Test 1 ran slower and required more memory 40-50% of 256GB (Wall Clock 94 hours); Test 3 ran initially faster with less memory than test 1 but by batch 65 it was using 75% of 384 GB. This job has not finished and appears stuck on importing batch 65. So the consolidate option appears to have a memory leak or using just requiring too much memory. The -consolidate option was the culprit. So rerunning chr1-3 with just the --bypass-feature-reader option (test2) ran fine without lots of memory being used. Below is the time output from chr1. The output shows the Maximum resident set size (kbytes): **2630440**. Using GATK jar /share/pkg.7/gatk/4.2.6.1/install/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar defined in environment variable GATK_LOCAL_JAR; ```; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx200g -Xms16g -jar /share/pkg.7/gatk/4.2.6.1/install/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar GenomicsDBImport --sample-name-map sample_map.chr1 --genomicsdb-workspace-path genomicsDB.rb.bypass.time.chr1 --genomicsdb-shared-posixfs-optimizations True --tmp-dir tmp --bypass-feature-reader --L chr1 --batch-size 50 --reade",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1252598687:53,optimiz,optimizing,53,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1252598687,2,"['optimiz', 'perform']","['optimizing', 'performance-guidelines']"
Performance,"@bbimber Let me see if I can dig up the old branches where I started this work. I'm not sure what state they're in, and I'm sure they're very out of date wrt/master, but they're probably at least worth looking at. I recall thinking that https://github.com/broadinstitute/gatk/issues/5441 would be quite a lot of work. Have you done any profiling to see where the performance issues actually are for your use case? I'd highly recommend doing that first before embarking on any big changes like this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5439#issuecomment-720514553:363,perform,performance,363,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5439#issuecomment-720514553,1,['perform'],['performance']
Performance,"@bbimber The -Xmx option controls only the maximum size of the *Java* heap. It does not limit the size of the C heap used by the GenomicsDB library. This is why we always advise leaving extra memory available for C when using GenomicsDB by selecting an -Xmx value that is comfortably under the amount of physical memory available. Even though your GenotypeGVCFs command is not importing into a GenomicsDB, it is reading out of a GenomicsDB and performing an on-the-fly merge, which will cause the native library to use nontrivial amounts of memory. Having said that, ~80 GB for the native heap does seem like a lot. Do you have lots of highly multi-allelic records in your callset? @nalinigans @mlathara Thoughts on this one?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7542#issuecomment-963504342:444,perform,performing,444,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7542#issuecomment-963504342,1,['perform'],['performing']
Performance,"@bbimber There are still a lot of unaddressed comments from my review. Eg.,. https://github.com/broadinstitute/gatk/pull/4495#discussion_r189690850; https://github.com/broadinstitute/gatk/pull/4495#discussion_r189693827; https://github.com/broadinstitute/gatk/pull/4495#discussion_r189694685; https://github.com/broadinstitute/gatk/pull/4495#discussion_r189697161; https://github.com/broadinstitute/gatk/pull/4495#discussion_r193207051; https://github.com/broadinstitute/gatk/pull/4495#discussion_r195236794; https://github.com/broadinstitute/gatk/pull/4495#discussion_r195242052; https://github.com/broadinstitute/gatk/pull/4495#discussion_r195242436; https://github.com/broadinstitute/gatk/pull/4495#discussion_r195243668; https://github.com/broadinstitute/gatk/pull/4495#discussion_r195243837. You also missed a bunch of @cmnbroad 's comments from his earlier review. I'm guessing what's going on here is that you might not be expanding the `N Hidden Conversations...Load More` box that github inserts into the middle of the reviews to hide comments when there are more than a certain number of them?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4495#issuecomment-397359678:970,Load,Load,970,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4495#issuecomment-397359678,1,['Load'],['Load']
Performance,"@bbimber There is a defrag operation that you can perform on the DB. My understanding is that it's only necessary if you have >100ish input batches. . When you say similar size combine gVcfs do you mean similar size of the input data, or similar size of the stored database compared to the merged gvcf? If you mean the later then that's expected because a genomicsdb will have much more data than an equivalent gvcf. I assume you mean the former though. I believe it is also expected that running from a genomicsdb should be slower than from a combined gvcf (up to a certain size of gvcf) because the combination operation is done at at read-time so it slows down the genotyping operation. The thing to look at is the sum of GenomicsDBImport + GenotypeGvcfs vs CombineGVCFS + GenotypeGvcfs.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6688#issuecomment-669343616:50,perform,perform,50,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6688#issuecomment-669343616,1,['perform'],['perform']
Performance,"@bbimber You may not want to restart at this point, but the latest release (https://github.com/broadinstitute/gatk/releases/tag/4.1.8.0) has some optimizations targeted towards shared posix filesystems -- a knob called `--genomicsdb-shared-posixfs-optimizations`. It also reduces the storage space requirements for the genomicsdb workspace substantially. You could also try to check on the size of the workspace where that scatter job is writing. The import should be writing to an ""invisible"" directory (i.e., one starting with a ""."") so it may not be visible under the contig directory. But if the process is making progress, the size should increase over time.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6688#issuecomment-655897920:146,optimiz,optimizations,146,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6688#issuecomment-655897920,2,['optimiz'],['optimizations']
Performance,"@bbimber question for you - what is the primary motivation for wanting to merge the scattered workspaces back into a single workspace here? I'm assuming the scatter is because you have a large enough dataset that you need multiple nodes to run the import in parallel. (side note: we're planning on enabling reading vcfs through native htslib in GenomicsDBImport which should drive down memory usage for cases that are able to take advantage of that route. This might make it more feasible to use `--max-intervals-to-import-in-parallel` for multiple threads on a single node ). If the large dataset is the primary reason, wouldn't you want the benefits of distributed processing on the query side as well? You mentioned in the previous thread that you saw the fact that a single workspace is a valid GenomicsDB workspace as a benefit...and that's certainly true - but if performance is the driving factor then it might be worth it to keep the workspace separate. @droazen Could you elaborate on what you envision we should do here? This approach should work as long as the same command line is used for each import with a different/unique interval list each time. Are you asking for a test case to be run just for sanity? Or add test cases to GATK? Or add a tool to do this...?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6620#issuecomment-635081015:870,perform,performance,870,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6620#issuecomment-635081015,1,['perform'],['performance']
Performance,"@bbimber, I have placed another version of consolidate_genomicsdb_array [here](https://github.com/GenomicsDB/GenomicsDB/releases/download/v1.4.3/consolidate_genomicsdb_array). This allows for batch-wise consolidation with the `--batch-size` or `-b` option. The tool also does better reuse of the consolidation buffers between reads, so might work a little better. Be aware that the total time to consolidate increases with the batch option and the final batches require almost as much memory as consolidating all the fragments at once. Please try consolidating any one GenomicsDB array to see how it functions as we are still working at making this scalable in a better way. This is just a draft version and if you can share some of the resulting logs, that will be very helpful. Please do let me know the total size of all the `__book_keeping.tdb.gz` files in your fragments. Just a back-of-envelope calculation, you probably need about 40 times that total size of memory to successfully consolidate.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1081029205:649,scalab,scalable,649,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1081029205,1,['scalab'],['scalable']
Performance,"@bbimber, did you use `GenomicsDBImport` from v4.2.5.0 as well? How large is your /home/exacloud/gscratch/prime-seq/cachedData/16b9ede7-6db8-103a-9262-f8f3fc86a851/WGS_Feb22_1852.gdb/1$1$223616942/__9b9a9e96-139c-4105-81ec-ab1455d1f01d140490873108224_1597099702436/__book_keeping.tdb.gz file? Anyway, you can share it with us?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1042028821:116,cache,cachedData,116,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1042028821,1,['cache'],['cachedData']
Performance,"@bbimber, sorry that the import with consolidate did not complete. If you are amenable to using a native tool, please download the tool from [here](https://github.com/GenomicsDB/GenomicsDB/releases/download/v1.4.3/consolidate_genomicsdb_array) for consolidation. This executable will consolidate a given array in a GenomicsDB workspace, it has been instrumented to output memory stats to help tune the segment size. Note that the executable is for Centos 7, if you find any unresolved shared library dependencies during usage, please let me know and I will work on getting another one to you. For usage from a bash shell:; ```; ~/GenomicsDB: ./consolidate_genomicsdb_array; Usage: consolidate_genomicsdb_array [options]; where options include:; 	 --help, -h Print a usage message summarizing options available and exit; 	 --workspace=<GenomicsDB workspace URI>, -w <GenomicsDB workspace URI>; 		 Specify path to GenomicsDB workspace; 	 --array-name=<Array Name>, -a <Array Name>; 		 Specify name of array that requires consolidation; 	 --segment-size=<Segment Size>, -z <Segment Size>; 		 Optional, default is 10M. Specify a buffer size for consolidation; 	 --shared-posixfs-optimizations, -p; 		 Optional, default is false. If specified, the array folder is not locked for read/write and file handles are kept open until a final close for write; 	 --version Print version and exit; ```. ```; ~/GenomicsDB.: ./consolidate_genomicsdb_array -w /Users/xxx/WGS.gdb/ -a ""1\$1\$249250621"" -z 1048576 -p; 21:09:47.100 info consolidate_genomicsdb_array - pid=30881 tid=30881 Starting consolidation of 1$1$249250621 in ws; Using buffer_size=1048576 for consolidation; 21:9:47 Memory stats(pages) beginning consolidation size=45821 resident=18998 share=1824 text=3530 lib=0 data=16810 dt=0; 21:9:47 Memory stats(pages) after alloc for attribute=END size=45821 resident=19009 share=1835 text=3530 lib=0 data=16810 dt=0; 21:9:48 Memory stats(pages) after alloc for attribute=REF size=46788 resident=19743 share=18",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1057680354:393,tune,tune,393,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1057680354,1,['tune'],['tune']
Performance,"@bbimber, thanks to @mlathara and @kgururaj, here is a suggestion. With `~36 attributes+offsets` and `~80 fragments` in GenomicsDB parlance getting stored, with `--genomicsdb-segment-size=1048576(1M default)`, we are looking at memory requirements in the range of 10G of memory for reading during consolidation. Just wondering, if you could try GenomicsDBImport with the following options to see if it helps.; ```; java heap options of say -Xmx100g -Xms 100g; --genomicsdb-update-workspace-path WGS_1852_consolidated.gdb \; --batch-size 10 \; --consolidate \; --genomicsdb-shared-posixfs-optimizations \; --bypass-feature-reader \; --genomicsdb-segment-size 32768 \; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1051157333:588,optimiz,optimizations,588,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1051157333,1,['optimiz'],['optimizations']
Performance,"@bbimber, there was an issue with `max-alternate-alleles` getting passed to the GenomicsDB layer from `GenotypeGVCFs`. That has been fixed in this [branch](https://github.com/broadinstitute/gatk/tree/ng_genomicsdb_args) if you would like to try. See this related [GenomicsDB Issue](https://github.com/GenomicsDB/GenomicsDB/commit/3c7d1ead0110fec26d56ff85a5871d8df673504d). This will hopefully help with memory usage. On another note, there is a [performance/bug fix ](https://github.com/broadinstitute/gatk/pull/7520) while reading/writing GenomicDB bookkeeping artifacts in the latest release and in this branch as well. This may help with memory while incrementally adding new batches and using the `--consolidate` option with import.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7542#issuecomment-974487707:446,perform,performance,446,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7542#issuecomment-974487707,1,['perform'],['performance']
Performance,"@bbimber, we are investigating some scalable solutions for you. Meanwhile, can you provide the following information?; 1. What is the total and free amount of memory available to say consolidate_genomicsdb_array on your individual nodes?; 2. Sizes of all files under any one fragment say in 9$1$134124166?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1062349294:36,scalab,scalable,36,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1062349294,1,['scalab'],['scalable']
Performance,"@bbimber, your approach should mostly work, this is exactly what I am going to allow with the standalone tool, a new arg for `batch-wise consolidation`. The tool is also better optimized with memory allocations and you can specify the batch size to the tool for batch-wise consolidation which should clamp down the memory use. Still testing out the tool, hopefully I can get some version to you over the weekend or on Monday.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1073040553:177,optimiz,optimized,177,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1073040553,1,['optimiz'],['optimized']
Performance,"@bhanugandham As a side note, you shouldn't be running GATK4 using `java -jar` directly. You should use the included `gatk` launcher script, which sets a lot of important configuration settings, some of which have a major effect on tool performance.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5900#issuecomment-485873403:237,perform,performance,237,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5900#issuecomment-485873403,1,['perform'],['performance']
Performance,"@bhanugandham It looks like the user is not using our provided resource file for `GetPileupSummaries`. It seems that the user selected only biallelic sites from gnomAD by hand, without removing all the extraneous info fields and restricting the exonic variants as we do in our resource. This means that the tool needs to load a huge amount of gnomAD into memory at any given time and is probably causing the crash. . If the user follows our best practices I expect the problem to go away.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5918#issuecomment-495288994:321,load,load,321,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5918#issuecomment-495288994,1,['load'],['load']
Performance,"@ccastane9, looks like a memory issue. Some questions -. 1. What are the sizes of the book-keeping files in your GenomicsDB workspace? Try running `find /ECA3_GenomicsDB_260 -name __book_keeping.tdb.gz -ls`.; 2. Is /ECA3_GenomicsDB_260 on NFS or another shared Posix FS? Can you try running GenotypeGVCFs with `--genomicsdb-shared-posixfs-optimizations` turned on?; 3. What does your hardware configuration look like, memory wise?; 3. What are your `-Xmx` and `-Xms` java options?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-754244432:339,optimiz,optimizations,339,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-754244432,1,['optimiz'],['optimizations']
Performance,"@chandrans Can you please ask the user to test with the `4.0.0.0` (or `4.0.1.0`) release, as there were major performance improvements to the `HaplotypeCaller` just before the release.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3631#issuecomment-361988531:110,perform,performance,110,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3631#issuecomment-361988531,1,['perform'],['performance']
Performance,"@chandrans Hmn, I don't really know what's happening. We wouldn't expect gatk4 haplotype caller to be that much slower. . It looks like they're running beta2 which is kind of old as well. Can you ask them what exact version they're using?. Can you ask if they have the log (stdout + stderr) for the gatk4 non-spark run? I can't tell what pairhmm they're actually running with and the logs would help with that. . Can you also find out what sort of hardware they're running on? Specifically, is it an intel machine with support for AVX?. A good setting for` --nativePairHmmThreads` is probably 4-8, you won't see any improvement after that. I also noticed that they're setting -XX:+UseParallelGC -XX:ParallelGCThreads=32 for the gatk3. They would be better off setting it to 2-4 threads. Performance gets worse beyond that typically from what I've seen. They can set the same thing for gatk4 using`--javaOptions ' -XX:+UseParallelGC -XX:ParallelGCThreads=4'`. Their spark configuration looks wrong in a number of ways which is probably a big part of why they're not seeing any improvement. In general you want executors with ~4-8 cores and at least 4g of memory per core. I don't know how much memory their nodes have, and I don't know if they're running with autoscaling turned on, but I suspect they're only allocating 1 executor on 1 node and then it's thrashing memory because it's trying to run 32 threads at once. Spark tuning for haplotype caller is going to be complicated though and I don't know how to do it will yet, we will be revisiting it in the next quarter probably. They're also running withs spark 2.1.0, we currently require spark 2.0.2 which is an unfortunately specific version, we're planning on upgrading to spark 2.2.+ in the next quarter. . You should make it clear to them that the results will not be the same between 3, 4, and 4-spark yet and that 4 is in rapid state of flux and has known performance issues that we're planning on working soon. Even so though, that slowdow",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3631#issuecomment-332879964:787,Perform,Performance,787,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3631#issuecomment-332879964,1,['Perform'],['Performance']
Performance,"@chandrans Please ask the user to repeat the test, with the following adjustments:. * Run both GATK 3.7 and GATK 4.0.1.1 (latest release) on the same machine, one right after another, on chromosome 20 only (using `-L` in both cases), and ensure that there are no other expensive processes running on this machine during the tests. Run each version 3 times, and take the average of the results.; * Add `-pairHMM AVX_LOGLESS_CACHING` to both the GATK3 and GATK4 command lines, to guarantee that the native PairHMM will be used in both cases.; * Get rid of the `--native-pair-hmm-threads 32` in the GATK 4 command line. Too many threads can sometimes make performance worse by introducing too much contention.; * Check both the GATK3 and GATK4 output to ensure that the Intel inflater and deflater were used in both cases.; * Check both the GATK3 and GATK4 command lines to be sure they are equivalent (eg., if one is running with -ERC GVCF, the other one should as well).; * Compute wall-clock time by running the Unix `time` command, if the user is not already doing so (eg., `time ./gatk HaplotypeCaller....`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4361#issuecomment-363583464:653,perform,performance,653,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4361#issuecomment-363583464,1,['perform'],['performance']
Performance,"@chapmanb We were able to reproduce a failure with your command line. This looks like an issue related to JNI and garbage collection that is exposed by setting `-Xmx46965m` and `-XX:+UseSerialGC`, but it needs further debugging. To confirm, can you please try running without specifying these javaOptions? Something like this:; ```; ./gatk-launch --javaOptions '-Djava.io.tmpdir=$TEMP_DIR' \; ApplyBQSRSpark \; --sparkMaster local[16] \; --input $BAM_IN \; --output $BAM_OUT \; --bqsr_recal_file $BQSR_RECAL \; -- \; --conf spark.local.dir=$SPARK_LOCAL_DIR; ```. FYI, we see better performance from Spark when using an SSD for spark.local.dir. The `--conf ` option above shows how to set the spark.local.dir.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3605#issuecomment-332370070:582,perform,performance,582,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3605#issuecomment-332370070,1,['perform'],['performance']
Performance,"@cmnbroad . The non-spark version of CountReads runs fine...... ```; gatk-4.0.12.0/gatk CountReads --input /restricted/projectnb/adsp/wgs.hg38/adsp.cc/cram/A-ADC-AD010072-BL-NCR-11AD44210.hg38.realign.bqsr.cram --reference /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa; Using GATK jar /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/gatk-4.0.12.0/gatk-package-4.0.12.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/gatk-4.0.12.0/gatk-package-4.0.12.0-local.jar CountReads --input /restricted/projectnb/adsp/wgs.hg38/adsp.cc/cram/A-ADC-AD010072-BL-NCR-11AD44210.hg38.realign.bqsr.cram --reference /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa; 15:18:43.541 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/gatk-4.0.12.0/gatk-package-4.0.12.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 15:18:45.267 INFO CountReads - ------------------------------------------------------------; 15:18:45.267 INFO CountReads - The Genome Analysis Toolkit (GATK) v4.0.12.0; 15:18:45.268 INFO CountReads - For support and documentation go to https://software.broadinstitute.org/gatk/; 15:18:45.268 INFO CountReads - Executing as farrell@scc-hadoop.bu.edu on Linux v2.6.32-754.6.3.el6.x86_64 amd64; 15:18:45.268 INFO CountReads - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 15:18:45.269 INFO CountReads - Start Date/Time: January 2, 2019 3:18:43 PM EST; 15:18:45.269 INFO CountReads - ------------------------------------------------------------; 15:18:45.269 INFO CountReads - ------------------------------------------------------------; 15:18:45.270 INFO CountReads - HTSJDK Version: 2.18.1; 15:18:45.270 INFO CountReads - Picard Version: 2.18.16; 15:18:45.270 INFO CountReads ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-450983210:925,Load,Loading,925,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-450983210,1,['Load'],['Loading']
Performance,"@cmnbroad @lbergelson Looks like `SparkContextFactory.DEFAULT_TEST_PROPERTIES` is currently initialized statically at class-loading time, resulting in a call to `getGcsHadoopAdapterTestProperties()` even when we're not running the test suite.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3591#issuecomment-330902481:124,load,loading,124,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3591#issuecomment-330902481,1,['load'],['loading']
Performance,"@cmnbroad Thanks for pointing to the conda environment file. I tried to install load it but the first error it gave me was ; ```; NoPackagesFoundError: Package missing in current linux-64 channels:; - intel-openmp 2018.0.0*; ```; so I installed this package by hand with ´conda install -c anaconda intel-openmp´ and afterwards tried to install the gatkcondaenv.yml again with ´conda env create -n gatk -f gatkcondaenv.yml´ . Unfortunately I run into the next error which says:. ```; root@k-hg-srv1:/BioinfSoftware# conda env create -n gatk -f gatkcondaenv.yml; Using Anaconda API: https://api.anaconda.org; Solving environment: done. Downloading and Extracting Packages; mkl-service 1.1.2: ################################################################################################################################################################################################################## | 100%; libgcc-ng 7.2.0: #################################################################################################################################################################################################################### | 100%; mkl 2018.0.1: ####################################################################################################################################################################################################################### | 100%; intel-openmp 2018.0.0: ############################################################################################################################################################################################################## | 100%; Preparing transaction: done; Verifying transaction: done; Executing transaction: done; Requirement 'build/gatkPythonPackageArchive.zip' looks like a filename, but the file does not exist; Processing ./build/gatkPythonPackageArchive.zip; Exception:; Traceback (most recent call last):; File ""/BioinfSoftware/Anaconda/envs/gatk/lib/python3.6/site-packages/pip/basecommand.py"", lin",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4125#issuecomment-357188460:80,load,load,80,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4125#issuecomment-357188460,1,['load'],['load']
Performance,"@cmnbroad The results look good to me. Ideally though, shouldn't we be downloading and baking these images into our bundle instead of fetching them at page load? It seems bad to rely on an external webservice for documentation to render.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6606#issuecomment-631708551:156,load,load,156,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6606#issuecomment-631708551,1,['load'],['load']
Performance,"@cmnbroad This one is unblocked now, and should be a very quick change. Since changing `copy()` into a deep copy method might hurt the performance of things like the read clipper, recommend adding a separate method `deepCopy()` to `GATKRead`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/623#issuecomment-157452737:135,perform,performance,135,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/623#issuecomment-157452737,1,['perform'],['performance']
Performance,"@cmnbroad on a related note -- it might be worthwhile to setup the Docker to include a dynamic BLAS library and pass it to theano. I will test how it affects the performance. NumPy is usually either linked against MKL or OpenBLAS. If theano has no dynamic BLAS lib available to link the compiled graph against, it will fall back to NumPy for linalg ops. It is not too bad since the only cost is the c++/python communication overhead.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3912#issuecomment-350264808:162,perform,performance,162,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3912#issuecomment-350264808,1,['perform'],['performance']
Performance,"@cmnbroad right, at the time when we impemented the pedigree annotations we made the decision that we didn't really expect the `PossibleDeNovo` to be used in the same way as the other pedigree annotations, as it seemed to be a specific case. It would be possible to add the ability for `PossibleDeNovo` to perform generate a SampleDB from its pedigree file at construction so it will be compatible. We still couldn't use the variant annotator for `CalculateGenotypePosteriors` though as we still can't have the same argument name on the path in multiple places",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4987#issuecomment-403538495:306,perform,perform,306,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4987#issuecomment-403538495,1,['perform'],['perform']
Performance,"@cwhelan . The soft-clip -> hard-clip optimization is in lines 47-63 in `BwaMemAlignmentUtils.applyAlignment()`, which was called by `AlignedAssemblyOrExcuse.writeSAMFile()` in our discovery pipeline (BwaSparkEngine calls it as well but not affecting what we are talking about here).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2595#issuecomment-294160235:38,optimiz,optimization,38,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2595#issuecomment-294160235,1,['optimiz'],['optimization']
Performance,@cwhelan Can you please provide some of these calls? I am considering filtering the mapping/alignments and want to evaluate the filter's performance. Thanks!,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3225#issuecomment-313876090:137,perform,performance,137,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3225#issuecomment-313876090,1,['perform'],['performance']
Performance,"@cwhelan I've having problems with the non-Spark JAR though:. ``` bash; $ gradle clean installDist; $ java -jar build/libs/gatk-4.pre-alpha-*-SNAPSHOT.jar; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/logging/log4j/LogManager; at org.broadinstitute.hellbender.cmdline.ClassFinder.<clinit>(ClassFinder.java:29); at org.broadinstitute.hellbender.Main.extractCommandLineProgram(Main.java:108); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.main(Main.java:86); Caused by: java.lang.ClassNotFoundException: org.apache.logging.log4j.LogManager; at java.net.URLClassLoader$1.run(URLClassLoader.java:372); at java.net.URLClassLoader$1.run(URLClassLoader.java:361); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:360); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); ... 4 more; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1213#issuecomment-162013287:891,load,loadClass,891,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1213#issuecomment-162013287,3,['load'],['loadClass']
Performance,"@cwhelan is seeing performance issues now with large interval lists + reads + spark. This might be a result of the way intervals are currently being sent to Hadoop-BAM in `ReadsSparkSource` (marshaled into a giant String, set as a Java system property, then re-parsed on the other end). @akiezun we might want to bump up the priority of this ticket.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1532#issuecomment-208969908:19,perform,performance,19,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1532#issuecomment-208969908,1,['perform'],['performance']
Performance,"@david-wb Interesting... what version of spark / spark submit are you using? I was pretty sure that at the point where setMaster() was called by gatk the spark context was already created by yarn and setup with the appropriate master. I wonder if it changed from a previous version of spark... Alternatively, I may be misremembering and relying on the fact that our wrapper script supplies `--sparkMaster yarn` as a gatk argument which would probably override what's being set. Could you try:; ```; spark-submit \; --deploy-mode client \; --class org.broadinstitute.hellbender.Main \; --master yarn \; /home/hadoop/gatk-package-4.alpha.2-269-gdce8abc-SNAPSHOT-spark.jar BwaSpark \; --bwamemIndexImage /var/tmp/hs38DH-V.fasta.img \; -I hdfs:///unaligned.bam \; -O hdfs:///aligned.bam \; -R hdfs:///hg38/hs38DH-V.fasta \; --disableSequenceDictionaryValidation true \; --sparkMaster yarn; ```. or with the wrapper: ; ```; GATK_SPARK_JAR_ENV_VARIABLE=/home/hadoop/gatk-package-4.alpha.2-269-gdce8abc-SNAPSHOT-spark.jar. ./gatk-launch \; --bwamemIndexImage /var/tmp/hs38DH-V.fasta.img \; -I hdfs:///unaligned.bam \; -O hdfs:///aligned.bam \; -R hdfs:///hg38/hs38DH-V.fasta \; --disableSequenceDictionaryValidation true \; -- \; --sparkRunner SPARK \; --sparkMaster yarn; ```. Our wrapper script sets a number of properties which we think are important for running our spark tools. If running directly on spark you might want to set them explicitly. Things like `-Dsamjdk.compression_level=1` have MAJOR performance implications. . Also, be aware the BwaSpark is not in the best health at the moment may have issues. If you're running it you may want to run the version that's in this branch which is currently under review https://github.com/broadinstitute/gatk/pull/2494.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2718#issuecomment-301925138:1498,perform,performance,1498,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2718#issuecomment-301925138,1,['perform'],['performance']
Performance,"@davidangb Based on your experience, would you be able to give an upper limit for magnitude of number of datapoints/dimensions beyond which performance is a blocking issue, on eg a reasonable personal laptop?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/248#issuecomment-77250988:140,perform,performance,140,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/248#issuecomment-77250988,1,['perform'],['performance']
Performance,"@davidbenjamin . - Downsampling is definitely a concern for us, but as you have suggested, I have utilized this parameter to not limit ourselves to 50 reads per alignment start. After running a range of values, I landed on 500 as not being too much of a burden computationally, but still allowing us to fully digest alignments at each start site in a given region. For us, I have estimated a value of 500 would cover us to a depth of about 2000 or so, since we expect to see a bias at the projected amplicon start site.; - The read filters you list don't have a huge effect on the majority of our regions. Generally, I would not expect to see more than a 5% loss based on the mapping quality filter. The read size filter should never be triggered, as we input only reads larger than 30 bases to M2. We perform duplicate removal, as we are working with UMIs, so this also should not be an issue. ; - I wouldn't expect the realignment stage to cause the type of effect I see, and as you say, it really just corrects the data anyways. **Example:**; The following was called:; ```; 1	12919623	.	C	T	.	.	DP=741;ECNT=4;POP_AF=1.000e-03;P_GERMLINE=-2.169e-04;TLOD=743.86; 	GT:AD:AF:F1R2:F2R1:MBQ:MFRL:MMQ:MPOS:PGT:PID:SA_MAP_AF:SA_POST_PROB	0/1:520,210:0.291:0,0:520,210:36:153,132:57:47:0; |1:12919623_C_T:0.232,0.283,0.288:0.835,2.995e-04,0.165; ```; Also called in a different variant caller:; ```; 1	12919623	.	CC	TG	6989.54	.	AB=0.308968;ABP=423.639;AC=1;AF=0.5;AN=2;AO=410;CIGAR=2X;DP=1327;DPB=1327;DPRA=0;EPP=55.973;EPPR=139.678;GTI=0;LEN=2;MEANALT=8;MQM=50.4732;MQMR=56.3933;NS=1;NUMALT=1;ODDS=1609.4;PAIRED=1;PAIREDR=1;PAO=0;PQA=0;PQR=0;PRO=0;QA=13156;QR=30163;RO=900;RPL=282;RPP=128.617;RPPR=256.291;RPR=128;RUN=1;SAF=177;SAP=19.6194;SAR=233;SRF=377;SRP=54.4404;SRR=523;TYPE=mnp;technology.ILLUMINA=1	GT:DP:AD:RO:QR:AO:QA:GL	0/1:1327:900,410:900:30163:410:13156:-725.008,0,-2308.15; ```; Yes, M2 also calls the neighboring C>G substitution, these are just being represented differently between the ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3808#issuecomment-344740595:802,perform,perform,802,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3808#issuecomment-344740595,1,['perform'],['perform']
Performance,"@davidbenjamin @droazen unfortunately the new PON does not make up for the precision loss introduced in v4.1.9.0.; In v4.4.0.0 we get just 2 fewer FP SNVs in our performance evaluation, compared to the old PON.; Benchmarking results in WES tumor-normal mode, HCC1395 benchmark, and:. - v4.1.8.1 (last release with high SNV precision), v4.1.9.0 (first release affected by precision drop), v4.4.0.0 (current release); - oldPON: 1000g_pon.hg38.vcf.gz, newPON: mutect2-hg38-pon.vcf.gz; ![FD_TN_4181_FD_TN_4181_oldPON_FD_TN_4181_newPON_FD_TN_4190_FD_TN_4190_oldPON_FD_TN_4190_newPON_FD_TN_4400_FD_TN_4400_oldPON_FD_TN_4400_newPON](https://user-images.githubusercontent.com/15612230/236126940-9fc26627-260a-43c2-b409-69fbcec6ad47.png). Any chance to get this issue fixed? With Mutect3 not being available and v4.1.8.1 being affected by the log4j vulnerability, it is quite regrettable to be stuck with inferior precision. Extended methods, code, and data to reproduce the issue are here: ; [https://github.com/ddrichel/Mutect2_calling_performance_bug](https://github.com/ddrichel/Mutect2_calling_performance_bug)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1534177043:162,perform,performance,162,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1534177043,1,['perform'],['performance']
Performance,"@davidbenjamin I feel like we need to put good, continuous *performance* regression tests in place for the `HaplotypeCaller` so that we can make changes of this nature without fear. Testing for a performance regression in the `HaplotypeCaller` is currently very non-trivial -- you have to run with and without intervals, with and without -ERC GVCF, on both exome and genome to be confident that you haven't killed performance in a certain mode.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3184#issuecomment-358777173:60,perform,performance,60,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3184#issuecomment-358777173,3,['perform'],['performance']
Performance,"@davidbenjamin I tried and this time its a different error. ; ```; 14:55:53.232 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/shollizeck/clustering.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 09, 2020 2:55:53 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 14:55:53.432 INFO FilterMutectCalls - ------------------------------------------------------------; 14:55:53.433 INFO FilterMutectCalls - The Genome Analysis Toolkit (GATK) v4.1.4.1-6-g6bb31a7-SNAPSHOT; 14:55:53.433 INFO FilterMutectCalls - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:55:53.433 INFO FilterMutectCalls - Executing as shollizeck@stpr-res-compute02.unix.petermac.org.au on Linux v3.10.0-1062.4.3.el7.x86_64 amd64; 14:55:53.433 INFO FilterMutectCalls - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_232-b09; 14:55:53.434 INFO FilterMutectCalls - Start Date/Time: 9 January 2020 2:55:53 PM; 14:55:53.434 INFO FilterMutectCalls - ------------------------------------------------------------; 14:55:53.434 INFO FilterMutectCalls - ------------------------------------------------------------; 14:55:53.434 INFO FilterMutectCalls - HTSJDK Version: 2.21.0; 14:55:53.435 INFO FilterMutectCalls - Picard Version: 2.21.2; 14:55:53.435 INFO FilterMutectCalls - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 14:55:53.435 INFO FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 14:55:53.435 INFO FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 14:55:53.435 INFO FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 14:55:53.435 INFO FilterMutectCalls - Deflater: IntelDeflater; 14:55:53.435 INFO FilterMutectCalls - Inflater: IntelInflater; 14:55:53.435 INFO FilterMutectCalls - GCS max retries/reopens: 20; 14:55:53.435 INFO FilterMutectCalls - Requester pays: disabled; 14:55:",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6202#issuecomment-572799341:107,Load,Loading,107,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6202#issuecomment-572799341,1,['Load'],['Loading']
Performance,"@davidbenjamin Let me do a bit of profiling/optimization on the new code path to see if I can narrow the gap. The patch was intended to address memory use rather than runtime, and could use a profiling pass.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3238#issuecomment-330324547:44,optimiz,optimization,44,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3238#issuecomment-330324547,1,['optimiz'],['optimization']
Performance,"@davidbenjamin The bug here is that we misinterpreted what a TreeSet in java does. The actual behavior for this method was that it only took the FRIST variant at each start position that it saw from the ordering of the haplotypes it saw. This meant if a SNP and INDEL started at the same position then there was a chance that site only looks like a SNP to the subsequent trimming code and we trim incorrectly. . See the TreeSet docs:; ```; <p>Note that the ordering maintained by a set (whether or not an explicit; * comparator is provided) must be <i>consistent with equals</i> if it is to; * correctly implement the {@code Set} interface. (See {@code Comparable}; * or {@code Comparator} for a precise definition of <i>consistent with; * equals</i>.) This is so because the {@code Set} interface is defined in; * terms of the {@code equals} operation, but a {@code TreeSet} instance; * performs all element comparisons using its {@code compareTo} (or; * {@code compare}) method, so two elements that are deemed equal by this method; * are, from the standpoint of the set, equal. The behavior of a set; * <i>is</i> well-defined even if its ordering is inconsistent with equals; it; * just fails to obey the general contract of the {@code Set} interface.; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6661#issuecomment-645528965:888,perform,performs,888,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6661#issuecomment-645528965,1,['perform'],['performs']
Performance,@davidbenjamin You like minor optimizations...,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6439#issuecomment-581622363:30,optimiz,optimizations,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6439#issuecomment-581622363,1,['optimiz'],['optimizations']
Performance,"@davidbenjamin looks that it works for CombineVariants now (not on the full set of data). However, I am getting a lot of random errors for HaplotypeCallerSpark (4.1.8.1):. > 20/09/15 21:46:45 ERROR Executor: Exception in task 14.0 in stage 5.0 (TID 464); > java.util.ConcurrentModificationException; > 	at java.util.ArrayList.sort(ArrayList.java:1456). after rerunning with the same parameters for some runs problems disappeared, for some doesn't, and I must rerun them once again. There was no this kind of issue when I was using 4.1.3.0 HaplotypeCallerSpark O_o. I am confused O_o for what version tools works and for what doesn't. [H1_1.2.gatk.spark.HaplotypeCaller.gvcf.log](https://github.com/broadinstitute/gatk/files/5229907/H1_1.2.gatk.spark.HaplotypeCaller.gvcf.log); [H1_2.5.gatk.spark.HaplotypeCaller.gvcf.log](https://github.com/broadinstitute/gatk/files/5229908/H1_2.5.gatk.spark.HaplotypeCaller.gvcf.log). in contrast to the working processes; [H1_2.3.gatk.spark.HaplotypeCaller.gvcf.log](https://github.com/broadinstitute/gatk/files/5229912/H1_2.3.gatk.spark.HaplotypeCaller.gvcf.log). exactly same HPC infrastructure O_o. ________________; I think I have stuck once again somewhere in #5680 and #6730",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6473#issuecomment-693185541:267,Concurren,ConcurrentModificationException,267,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6473#issuecomment-693185541,1,['Concurren'],['ConcurrentModificationException']
Performance,"@davidbenjamin ready for re-review. I'm going to do a little performance benchmarking in the meantime. It takes ~40min to call the whole contig for my 4000X bams, which isn't terrible, but it isn't great. Anecdotally it seems like the AF thresholding slowed things down, but I'll collect some numbers.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5312#issuecomment-449458286:61,perform,performance,61,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5312#issuecomment-449458286,1,['perform'],['performance']
Performance,"@davidbenjamin take note: the changes to `AssemblyRegionWalker` here have a major effect on `HaplotypeCaller` performance when run with a large (eg., whole-exome) interval list. I would expect there to also be a dramatic effect on `Mutect2` performance when run with such an interval list -- would you have time to do a quick check on this?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4031#issuecomment-354902432:110,perform,performance,110,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4031#issuecomment-354902432,2,['perform'],['performance']
Performance,"@davidbenjamin the benchmark data is still not ours, it is the current somatic ""gold standard"" HCC1395 from [https://www.nature.com/articles/s41587-021-00993-6](https://www.nature.com/articles/s41587-021-00993-6), based on data from multiple WGS sequencing runs with combined 1,500x coverage, etc.... Apart from that, we are on the same page here. Now that the change leading to the apparent differences in performance is found, and holds up to scrutiny so far (thanks for looking into it), it is a more real possibility than ever that this is not a bug but a case of Mutect2 outperforming the ""gold standard"". Let's keep the issue open for now, I am still investigating and would like to have a place to report my progress.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1543422454:407,perform,performance,407,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1543422454,1,['perform'],['performance']
Performance,"@davidbenjamin, @lbergelson reminded me about this bug. Unlikely, but any chance the sign error in the digamma implementation (which kicks in for x >= 49) affects M2 or anything else that uses the Dirichlet class? Looks like there are also a few calls in VQSR. I'm still primarily interested in any possible performance/runtime improvements that could be gained by updating to a more recent package, perhaps one more actively developed than Apache Commons Math (e.g. Apache Commons Numbers). Seems to be some complications regarding release policies across those projects that preclude them from being updated frequently.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6133#issuecomment-547988573:308,perform,performance,308,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6133#issuecomment-547988573,1,['perform'],['performance']
Performance,"@davidbenjamin, et al. I have two recommendations:. 1) Though I prefer to work symbolically and through proofs, it might be nice to first expand on the validation by proof in the JavaDoc - including for the specific function's header - and anywhere else where necessary across the GATK code, just for sanity's sake, and for tying things together neatly and properly. This process of always going through the mathematical steps alerts me when I code that I have not missed anything. . 2) When dealing with multiple levels of transformations, it probably would be good to formulate a collection of complete set of simple tests. Since like you mentioned {phased} is a subset (⊂) of {unphased}, then the paths of phased genotypes one works with would also be ideal to test on. Does this function have any validation tests confirming the correct likelihoods, which would be performed for both phased and unphased genotypes? These can be generated tests, if original files do not exists. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2019#issuecomment-236759221:869,perform,performed,869,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2019#issuecomment-236759221,1,['perform'],['performed']
Performance,"@davidbernick Thanks for looking into this. ; 1. It looks like we have a big 200G called /app disk but we're only writing to the root for some reason. Can we write there instead? ; 2. 👍 to removing most of the old jobs. It would be nice if we could save the job status logs but delete the artifacts and temp files. If that's hard to do though it's fine to just rotate out the old ones. Can we keep two months of jobs? That would be enough buffer so we can look back and see when something went wrong. For the performance tests you wrote, if there's any way we can keep their historical run times that would be best since they're mostly useful to see trends against the repositories history.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2180#issuecomment-248319308:509,perform,performance,509,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2180#issuecomment-248319308,1,['perform'],['performance']
Performance,"@ddrichel It is regrettable that the tradeoff between precision and recall wasn't beneficial in your data, but in other data it has been. I can't call it a bug because mathematically speaking it is an improvement. At worst it can be considered an arbitrary movement along the ROC curve. Regardless, we have always developed Mutect2 and its successor under the principle that theoretical soundness is the best long-term policy. That is not to deny or disregard that some changes harm performance on some data. There is a chance that adjusting the `-f-score-beta`, which controls the relative contribution of recall and precision to the weighted F score that FilterMutectCalls seeks to optimize, will be able to reverse this shift and give up the gained recall in exchange for the lost precision. I can't promise that this will help, but fortunately you have the patched 4.1.8.1 that @droazen has kindly produced.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1538858085:483,perform,performance,483,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1538858085,2,"['optimiz', 'perform']","['optimize', 'performance']"
Performance,"@droazen +1 for being affected by this issue in production. As this is in production (same as @schelhorn , with big pharma which have very strict security requirements), and as 4.1.8.0 contains critical security vulnerabilities that were mitigated in subsequent releases, we are in a serious pickle here. @jhl667 how did you conclude that 4.1.8.0 performs better than newer versions? What we see is that it emits more variants, but after filtering and intersecting with other callers (i.e. Strelka), we get more variants and a ""better"" result (we can't really define ""better"" - it's merely an observation) with 4.2.4.1.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1407439000:347,perform,performs,347,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1407439000,1,['perform'],['performs']
Performance,"@droazen - That won't be solved by the current #3447, because there is no way of fine-tune the codecs: I require to being able to add/remove concrete classes, and exclude codecs from a concrete package. An example is a custom codec implementation for some feature, to provide extra-validation for the downstream toolkit. This would be even more useful if HTSJDK is moving to an interface-based library...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2139#issuecomment-337622596:86,tune,tune,86,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2139#issuecomment-337622596,1,['tune'],['tune']
Performance,"@droazen - sorry for the compilation error, it was just an early optimization. Can you have a look to it? Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4682#issuecomment-384300799:65,optimiz,optimization,65,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4682#issuecomment-384300799,1,['optimiz'],['optimization']
Performance,"@droazen @cmnbroad @mbabadi I generally agree with the sentiments expressed in #4127, except that I think it's OK to require a conda environment (or even use of the Docker) for these particular tools. How we should validate this requirement is another question. We can discuss more with @vdauwera. @stefandiederich Hopefully once you get the conda environment set up you will be able to run the tools. We would definitely appreciate any feedback you might be able to provide. Note that the gCNV model is relatively sophisticated, so there may be some parameters (which control the priors for the model as well as how inference is performed) that you will need to adjust for your data. Depending on the number of intervals/bins you are using and your memory constraints, you may also need to scatter across multiple GermlineCNVCaller runs; see how things are done in the WDLs here: https://github.com/broadinstitute/gatk/tree/master/scripts/cnv_wdl/germline. As you noted, this pipeline is still in beta. We are currently running several evaluations and hope to soon release some Best Practices recommendations for the aforementioned parameter values that should work well for various data types generated at the Broad. We will also have some blog or forum posts that explain the new CNV pipelines in more detail coming soon---stay tuned!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4125#issuecomment-357034364:630,perform,performed,630,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4125#issuecomment-357034364,2,"['perform', 'tune']","['performed', 'tuned']"
Performance,"@droazen I didn't really look at runtime scientifically, so I can't comment on that. I pasted this table and explanation into the picard issue, but I think it had already been closed at that point, so I'm not sure how many people saw it:. > I have some _small_ test BAMs that are constructed by extracting reads overlapping a few hundred kb of genome from WGS samples. I made the table below using such a BAM made from the 1KG PCR-free WGS data from NA19625. Not ideal, but I would _think_ would perform fairly similar to a full WGS bam for compression purposes. What I see is that at compression level 1 the intel deflator produces a significantly larger BAM that the JDK deflator at level=1. . Compression Level | Intel Deflater File Size | Intel Deflater % of JDK l=5 | JDK Deflater File Size | JDK Delfater % of JDK l=5; ---|------------|----------|---------------|---------; 1 | 54,840,445 | 175.23% | 38,543,684 | 123.16%; 2 | 35,782,642 | 114.33% | 36,745,494 | 117.41%; 3 | 34,989,899 | 111.80% | 35,262,326 | 112.67%; 4 | 31,815,698 | 101.66% | 32,549,560 | 104.00%; 5 | 31,240,892 | 99.82% | 31,296,433 | 100.00%; 6 | 30,675,174 | 98.01% | 30,577,906 | 97.70%; 7 | 30,379,699 | 97.07% | 30,380,325 | 97.07%; 8 | 30,124,200 | 96.25% | 30,124,375 | 96.25%; 9 | 30,064,322 | 96.06% | 30,064,325 | 96.06%. That does seem to suggest that the intel deflator at `level=2` produces a BAM that is smaller than the JDK deflator at either level 2 or 1, and if it is also faster in your testing, that sounds pretty good for intermediate files. It's still ~15% bigger than a `level=5` BAM though, so unless the vast majority of users are switching over to CRAM for storage, I'd hesitate to change the default compression level in any of the toolkits.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3413#issuecomment-323840661:496,perform,perform,496,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3413#issuecomment-323840661,1,['perform'],['perform']
Performance,"@droazen I hacked one of the TrainVariantAnnotationsModelIntegrationTest cases to run in your Docker (only necessary because it seems like `gradlew test --tests *TrainVariantAnnotationsModelIntegrationTest` doesn't recognize tests that use a `DataProvider`, but perhaps I did something wrong). Here are the differences:. ```; (gatk) root@a87e0994889e:/repo# h5diff -v /repo/src/test/resources/large/org/broadinstitute/hellbender/tools/walkers/vqsr/scalable/train/expected/extract.nonAS.snpIndel.posUn.train.snpIndel.posOnly.IF.snp.trainingScores.hdf5 /repo/extract.nonAS.snpIndel.posUn.train.snpIndel.posOnly.IF.snp.trainingScores.hdf5. file1 file2; ---------------------------------------; x x / ; x x /data ; x x /data/scores . group : </> and </>; 0 differences found; group : </data> and </data>; 0 differences found; dataset: </data/scores> and </data/scores>; size: [445] [445]; position scores scores difference ; ------------------------------------------------------------; [ 60 ] -0.419202 -0.419202 5.55112e-17 ; 1 differences found; ```. Looks pretty negligible to me! :stuck_out_tongue_closed_eyes: Probably a result of the native code being called by the python/ML packages used in these tools; even minor changes in the compilers across Ubuntu versions might introduce differences like these. A quick fix might be to replace all system calls to `h5diff` in these tests with `h5diff --use-system-epsilon`; seems to do the trick here. But if that doesn't fix all test cases, then perhaps you can relax things with `h5diff -p EPSILON`, where `EPSILON` is a relative threshold. Probably OK to pick something like `1E-6`. OK if I leave it to you to try this or otherwise check the rest of the cases?. Sorry for the inconvenience! I think the exact-match test worked as intended here, but I probably could've put in better messaging originally. Unfortunately, it's a bit awkward to grab the output of system commands. And thanks for dealing with conda again (a necessary evil, unless we want ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8610#issuecomment-1848796931:448,scalab,scalable,448,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8610#issuecomment-1848796931,1,['scalab'],['scalable']
Performance,@droazen I have pushed the cache removal step down to a more testable point in the code and added the assertion to the existing testing infrastructure. Can you take a quick look at this branch so it can go in at some point?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5911#issuecomment-491914930:27,cache,cache,27,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5911#issuecomment-491914930,1,['cache'],['cache']
Performance,"@droazen I posted the complete command line I used (the version is above). I posted a test.sam that reproducibly fails on my machine (OSX). And below is the log from my machine:. ```; 22:42:22.298 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/nhomer/miniconda3/envs/bfx/share/gatk4-4.1.8.1-0/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.dylib; Aug 01, 2020 10:42:22 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 22:42:22.412 INFO HaplotypeCaller - ------------------------------------------------------------; 22:42:22.412 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.1.8.1; 22:42:22.412 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 22:42:22.412 INFO HaplotypeCaller - Executing as nhomer@ip-192-168-7-102.ec2.internal on Mac OS X v10.14.6 x86_64; 22:42:22.412 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_192-b01; 22:42:22.412 INFO HaplotypeCaller - Start Date/Time: August 1, 2020 10:42:22 PM MST; 22:42:22.412 INFO HaplotypeCaller - ------------------------------------------------------------; 22:42:22.412 INFO HaplotypeCaller - ------------------------------------------------------------; 22:42:22.413 INFO HaplotypeCaller - HTSJDK Version: 2.23.0; 22:42:22.413 INFO HaplotypeCaller - Picard Version: 2.22.8; 22:42:22.413 INFO HaplotypeCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 22:42:22.413 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 22:42:22.413 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 22:42:22.413 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 22:42:22.413 INFO HaplotypeCaller - Deflater: IntelDeflater; 22:42:22.413 INFO HaplotypeCaller - Inflater: IntelInflater; 22:42:22.413 INFO HaplotypeCaller ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6733#issuecomment-667631737:224,Load,Loading,224,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6733#issuecomment-667631737,1,['Load'],['Loading']
Performance,"@droazen I ran #4314 and it did not solve the problem. When I reverted the ADAM patch from the #4314 branch I got the normal performance. @fnothaft I wish I knew. @lbergelson said he saw more logs being produced. Another (untested) theory is that the Kryo registrations changed somehow. GATK only uses the 2bit code from ADAM, so it is surprising that it is having such an effect. I'm not sure how to track down the problem at this point.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4376#issuecomment-366294101:125,perform,performance,125,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4376#issuecomment-366294101,1,['perform'],['performance']
Performance,"@droazen I ran the latest version but the message about google is still there!. 14:08:05.607 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/cm/shared/unil/software/8.3/GATK/4.1; .9.0-GCCcore-8.3.0-Java-8/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Oct 14, 2020 2:08:06 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6875#issuecomment-708360241:120,Load,Loading,120,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6875#issuecomment-708360241,1,['Load'],['Loading']
Performance,@droazen I think the BCF code is broken here too. The problem is fundamental to htsjdk. CombineVariants almost certainly has the same or similar problems because it's fundamental to combining vcfs and the fact that htsjdk doesn't handle partially empty lists. Bcftools likely has similar issues. Or loading the correct output from bcftools will recreate the issuue. What about fixing the combine operation so it can substitute default missing values with a per attribute configuration for what value to substitute?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6744#issuecomment-677814646:299,load,loading,299,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6744#issuecomment-677814646,1,['load'],['loading']
Performance,"@droazen I thought you might say that. We might be able to come up with an intermediate caching solution where not the entire index is cached. Although the index isn't really that big, so I'm not sure if it's a big deal.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2366#issuecomment-275425244:135,cache,cached,135,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2366#issuecomment-275425244,1,['cache'],['cached']
Performance,"@droazen I'm not sure this is an improvement. We want the fundamental unit of spark tool to be the transform, not the cli wrapper around it. If we do this then we're pushing more of the contract of the transform outside of itself, i.e. see the newly duplicated bqsr code. I think that it was a deliberate decision to lift all reads into the initial rdd and then filter them in the transforms to what was needed by that transform. This is paying some performance cost in multi-stage pipelines which will potentially apply the same filters over and over again, but it simplifies the code because the filters can be baked into the transform and the pipeline writer doesn't have to think about them. It would be nice if we had a mechanism for adding metadata to an RDD so we can say ""this is a sorted RDD filtered with X,Y,and Z filters"", so we could intelligently avoid re-filtering.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1159#issuecomment-158168856:450,perform,performance,450,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1159#issuecomment-158168856,1,['perform'],['performance']
Performance,"@droazen It's just a bit strange that it goes from 20s on local machines to 5 minutes on travis. Must be memory. Or some critical spark performance bug we haven't seen before ""_""",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1342#issuecomment-163781324:136,perform,performance,136,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1342#issuecomment-163781324,1,['perform'],['performance']
Performance,"@droazen Looks good. I'm so glad to see that repetitive loading code gone. Just a few minor comments. Feel free to merge when tests pass. (providing that they do pass... I think you've changed the behavior of a few tools to now pull in unmapped reads, which is an improvement, but it's possible the tests are assuming that unmapped reads are ignored)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/955#issuecomment-146009870:56,load,loading,56,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/955#issuecomment-146009870,1,['load'],['loading']
Performance,"@droazen Off the top of my head, we. * cache `log10(n)` and `log10(n!)` up to some large value.; * have a fast version of `log10SumLog10(double a, double a)` that works as follows: we want to compute `log10(10^a + 10^b)`. WLOG `a < b`, so this comes out to `a + log10(1 + 10^(a - b))`. I believe we cache the values of `log10(1 + 10^(x)` over a finely-spaced grid and round `a-b` to the nearest cached `x`. . There might not be anything else. There's a lot of stuff to keep calculations in log space for numerical stability but those don't avoid `log10()` and `Math.pow()`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2577#issuecomment-292584322:39,cache,cache,39,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2577#issuecomment-292584322,3,['cache'],"['cache', 'cached']"
Performance,"@droazen Responded to comments, note that i added a further escape condition where getMatchingPriors is avoided altogether if the VCpriors list is empty. Remember that this method is in a performance sensitive part of the code so every little bit of speed counts.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5616#issuecomment-461597381:188,perform,performance,188,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5616#issuecomment-461597381,1,['perform'],['performance']
Performance,"@droazen Thank you for the confirmation that HaplotypeCaller performs separate filtering passes on the read mapping qualities, and that the code on line 729 of HaplotypeCallerEngine.java (method ```filterNonPassingReads()``` ) is indeed executing subsequent to the ```MappingQualityReadFilter```. May I suggest, however, that MAPQ values less than 20 might not necessarily lead to an increase in FP variant calls? My understanding is that HaplotypeCaller uses MAPQ values only in a nonparametric rank sum test, in which case MAPQ is treated as an ordinal. This seems appropriate since the magnitude of a MAPQ value depends both on the data and on the computational model the read aligner uses to calculate it. With this in mind, a set of mappings with MAPQ in a lower range (e.g., ```--minimum-mapping-quality 10``` and a correspondingly lower ```--maximum-mapping-quality``` as well) might very well be appropriate for variant calling. So changing the semantics of ```MappingQualityReadFilter``` or parameterizing the currently-hardwired MAPQ range would enable additional control without affecting performance. @jamesemery I will watch for the HaplotypeCaller update that implements that functionality. And if you have a moment, could you please point me to the code that might be adversely affected by decreasing the low-end MAPQ threshold? I might have some ideas about that (or not!)... Thanks again!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6854#issuecomment-701512278:61,perform,performs,61,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6854#issuecomment-701512278,2,['perform'],"['performance', 'performs']"
Performance,"@droazen The thought was that we would fail at the point we try to load bases which is usually very close to the start. It moves the error back a bit which isn't great, but would allow for crams with embedded references.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6665#issuecomment-645474983:67,load,load,67,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6665#issuecomment-645474983,1,['load'],['load']
Performance,"@droazen Yes, it's tied for next in my queue.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2067#issuecomment-256740267:39,queue,queue,39,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2067#issuecomment-256740267,1,['queue'],['queue']
Performance,"@droazen Yes, this is because the native PairHMM is using single precision floating point and Flush To Zero (FTZ), while the Java PairHMM is using double precision and not using FTZ. I planned to address this when we integrate native PairHMM into HaplotypeCaller. It looks like the time is here. For now, you can configure native PairHMM to use double precision and not use FTZ. With the diff below, the VCFs from native and Java PairHMM are exactly the same. In the future, we need to enable FTZ in the Java PairHMM and provide the option to use single precision or double precision in native PairHMM. ``` diff; --- i/src/main/cpp/VectorLoglessPairHMM/LoadTimeInitializer.cc; +++ w/src/main/cpp/VectorLoglessPairHMM/LoadTimeInitializer.cc; @@ -23,7 +23,7 @@ LoadTimeInitializer::LoadTimeInitializer() //will be called when library is loa; //Very important to get good performance on Intel processors; //Function: enabling FTZ converts denormals to 0 in hardware; //Denormals cause microcode to insert uops into the core causing big slowdown; - _MM_SET_FLUSH_ZERO_MODE(_MM_FLUSH_ZERO_ON);; + // _MM_SET_FLUSH_ZERO_MODE(_MM_FLUSH_ZERO_ON);. //Profiling: times for compute and transfer (either bytes copied or pointers copied); m_compute_time = 0;; diff --git i/src/main/cpp/VectorLoglessPairHMM/org_broadinstitute_hellbender_utils_pairhmm_VectorLoglessPairHMM.cc w/src/main/cpp/VectorLoglessPairH; index f45153e..70cf54f 100644; --- i/src/main/cpp/VectorLoglessPairHMM/org_broadinstitute_hellbender_utils_pairhmm_VectorLoglessPairHMM.cc; +++ w/src/main/cpp/VectorLoglessPairHMM/org_broadinstitute_hellbender_utils_pairhmm_VectorLoglessPairHMM.cc; @@ -6,7 +6,7 @@. using namespace std;. -bool use_double = false;; +bool use_double = true;. //Should be called only once for the whole Java process - initializes field ids for the classes JNIReadDataHolderClass; //and JNIHaplotypeDataHolderClass; diff --git i/src/main/java/org/broadinstitute/hellbender/utils/pairhmm/VectorLoglessPairHMM.java w/src/main/",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1572#issuecomment-195496083:653,Load,LoadTimeInitializer,653,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1572#issuecomment-195496083,5,"['Load', 'perform']","['LoadTimeInitializer', 'performance']"
Performance,"@droazen correct. ; Generally, one issue is that this slows down the docker image creation in a somewhat substantial way. Around 10 minutes currently. Half of this is unzipping the bundled jar, and another piece is some redundant gradle downloading that can be alleviated with cache shenanigans.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4955#issuecomment-400797666:277,cache,cache,277,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4955#issuecomment-400797666,1,['cache'],['cache']
Performance,"@droazen here are the error messages with gatk4.1.8.1 and gatk4.1.4.1:. 15:01:44.424 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/cm/shared/unil/software/8.3/GATK/4.1.4.1-GCCcore-8.3.0-Java-8/gatk-package-4.1.4.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Oct 09, 2020 3:01:45 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine. 14:28:22.786 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/cm/shared/unil/software/8.3/GATK/4.1.8.1-GCCcore-8.3.0-Java-8/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Oct 09, 2020 2:28:23 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6875#issuecomment-707085229:112,Load,Loading,112,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6875#issuecomment-707085229,2,['Load'],['Loading']
Performance,"@droazen ideally, the executor must be able to fire up one (or more) python kernels and keep it (them) alive as long as the user decides to keep it (or the GATK session terminates). Here's an example why this is desirable: the compilation of a complicated theano computational graph can take a significant portion of the total computation time. The compiled graph is a function of data dimensions, which in my use case, varies from loci to loci. My current solution to this is to pre-compile and cache a number of theano computational graphs with different sizes, pad the data to fit it to the closest matching computational graph, and re-use the same compiled graph(s) as required. To this end, one needs to keep the python kernel w/ the compiled graphs alive.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3501#issuecomment-325040324:496,cache,cache,496,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3501#issuecomment-325040324,1,['cache'],['cache']
Performance,"@droazen thanks for noticing that PR. Looks like there’s quite a bit more going on there than is covered by this issue, although I didn’t take a close look. We should certainly make sure that any assumptions on SW parameters, etc. there are checked as well, if it does end up going in—seems quite stale, no?. Also, hope you don’t mind if I unassign myself from this—what’s the point of punting on something if it just gets reassigned to you? ;) Happy to review a PR if someone else is convinced that the original optimization is worth restoring, though!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7441#issuecomment-908616477:513,optimiz,optimization,513,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7441#issuecomment-908616477,1,['optimiz'],['optimization']
Performance,"@droazen this behavior hasn't changed in the most recent GenomicsDB release. . Short recap: this happens because bcf codec doesn't support the 64 bit values that GenomicsDB is returning. Running with `--genomicsdb-use-vcf-codec` will resolve it. From our discussions in the office hours, I thought we had decided to change the behavior in htsjdk so that it doesn't try to decode the type if it doesn't recognize it. (maybe I should have filed https://github.com/broadinstitute/gatk/issues/6548 in htsjdk instead? I thought I was told to do in GATK, but its been long enough that I can't remember). Another possibility is to make `--genomicsdb-use-vcf-codec` the default - though I recall you had some potential performance concerns about that. Lastly, we could change GenomicsDB to throw out a warning if a 64 bit value is needed and we're using bcf codec. Of course, this would still require the user to (re)run with `--genomicsdb-use-vcf-codec` to avoid hitting the NPE (or whatever other failure would be hit if the NPE was changed to something a bit more meaningful).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6667#issuecomment-646167430:711,perform,performance,711,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6667#issuecomment-646167430,1,['perform'],['performance']
Performance,"@droazen this branch wasn't STRICTLY dependent on #5607, so I removed it from this branch to make reviewing easier. Its worth noting that the performance numbers and observed speedup were seen when this branch did hang off of #5607.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5616#issuecomment-460423597:142,perform,performance,142,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5616#issuecomment-460423597,1,['perform'],['performance']
Performance,@droazen what do you want to do here? We're matching the performance but not beating it at this point. We can either keep this open and continue to investigate or close as 'matching not beating',MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1608#issuecomment-234341641:57,perform,performance,57,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1608#issuecomment-234341641,1,['perform'],['performance']
Performance,"@droazen would you mind reviewing/assigning? Or perhaps @jamesemery can take a look, since he expressed interest in doing similar Bayesian optimizations for an HMM? Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6885#issuecomment-904536278:139,optimiz,optimizations,139,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6885#issuecomment-904536278,1,['optimiz'],['optimizations']
Performance,"@droazen you were asking for a check of the performance. I ran the following two commands:. ```; $ ./gatk-launch PrintReads -I gs://$MYBUCKET/CEUTrio.HiSeq.WGS.b37.ch20.1m-2m.NA12878.bam -O gs://$MYBUCKET/pr.bam; $ ./gatk-launch PrintReads -I gs://$MYBUCKET/CEUTrio.HiSeq.WGS.b37.ch20.1m-2m.NA12878.bam -O /tmp/pr.bam; ```. output to: | local disk | GCS; --|--|--; run 1 | 0.12 min | 0.68 min; run 2 | 0.12 min | 0.29 min; run 3 | 0.12 min | 0.28 min; **median** |**0.12 min** | **0.29 min**. So it looks like there's a significant performance difference. For what it's worth, copying the output file to GCS from my desktop takes 3.5s. The log when running PrintReads indicates:. ```; 11:06:13.011 INFO PrintReads - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 11:06:13.011 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 11:06:13.011 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 11:06:13.011 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2558#issuecomment-336978915:44,perform,performance,44,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2558#issuecomment-336978915,2,['perform'],['performance']
Performance,"@droazen's [measurements in #995](https://github.com/broadinstitute/gatk/issues/995#issuecomment-152310985) show that for the 7GB input the optimized version (""sharding"") is **9x faster** than the unoptimized version (""shuffle""). The code is fast as intended, I'm closing this bug.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1006#issuecomment-155863239:140,optimiz,optimized,140,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1006#issuecomment-155863239,1,['optimiz'],['optimized']
Performance,"@droazen, I will address this in #2041. As you suggested this when I implemented `LocusWalker`, I would like to have some idea about why `DownsamplingMethod` is used as a parameter in the constructor. I think that this is misleading, because independently of the method for downsampling the one that is used by `SamplePartitioner` is a `ReservoirDownsampler` (if downsampling is performed), so API users could think that they are performing a different downsampling in LIBS that the actual one. I will keep the constructor in the PR, but I would like some feedback for this either here or in #2041.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1879#issuecomment-235530006:379,perform,performed,379,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1879#issuecomment-235530006,2,['perform'],"['performed', 'performing']"
Performance,"@droazen, the SAMRecord interface exposes getReferenceIndex and getMateReferenceIndex, so there is no question that the index has to be there. . What I was talking about what the optimization where when serializing, the [BAMRecordCodec only saves the index](https://github.com/samtools/htsjdk/blob/master/src/java/htsjdk/samtools/BAMRecordCodec.java#L131), and not the name. That is because if we have the header then we can go from the index to the name. If there is no header, then we can't do that optimization anymore and instead have to save those two fields for every read.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141251365:179,optimiz,optimization,179,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141251365,2,['optimiz'],['optimization']
Performance,"@droazen, yes for luster, we should pass the `--genomicsdb-shared-posixfs-optimizations` especially with GenomicsDBImport.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7646#issuecomment-1032004698:74,optimiz,optimizations,74,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7646#issuecomment-1032004698,1,['optimiz'],['optimizations']
Performance,"@dwuab, we are making some performance improvements with GenomicsDB and still are in the testing stage. Just wondering if you could try gatk from this branch `https://github.com/broadinstitute/gatk/tree/genomicsdb_142` to import large intervals(the ones that were problematic before) and let us know.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-930343647:27,perform,performance,27,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-930343647,1,['perform'],['performance']
Performance,"@erniebrau That's the performance of the pairhmm, but not the entire haplotype caller, there's definitely diminishing returns at some point. I could be a bit off on the core count before it starts leveling out.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3631#issuecomment-332917726:22,perform,performance,22,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3631#issuecomment-332917726,1,['perform'],['performance']
Performance,"@fleharty sorry, each curve is a different set of parameters run on the same sample, and I'm plotting all curves generated over the course of optimizing those parameters.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-712333444:142,optimiz,optimizing,142,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-712333444,1,['optimiz'],['optimizing']
Performance,@fnothaft I tried reverting 1eed8e8 and the performance was back to normal! So it would be worth reverting in ADAM for the next release if possible.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4376#issuecomment-368564267:44,perform,performance,44,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4376#issuecomment-368564267,1,['perform'],['performance']
Performance,"@gspowley I'm having trouble building on mac still. I changed from clang to gcc and I'm getting different errors now. ```; :compileVectorLoglessPairHMMSharedLibraryVectorLoglessPairHMMCpp; In file included from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/LoadTimeInitializer.h:4:0,; from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/baseline.cc:4:; /Library/Java/JavaVirtualMachines/jdk1.8.0_45.jdk/Contents/Home/jre/../include/jni.h:45:20: fatal error: jni_md.h: No such file or directory; compilation terminated. In file included from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/LoadTimeInitializer.h:4:0,; from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/LoadTimeInitializer.cc:2:; /Library/Java/JavaVirtualMachines/jdk1.8.0_45.jdk/Contents/Home/jre/../include/jni.h:45:20: fatal error: jni_md.h: No such file or directory; compilation terminated. In file included from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/jni_common.h:4:0,; from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/org_broadinstitute_hellbender_utils_pairhmm_VectorLoglessPairHMM.cc:2:; /Library/Java/JavaVirtualMachines/jdk1.8.0_45.jdk/Contents/Home/jre/../include/jni.h:45:20: fatal error: jni_md.h: No such file or directory; compilation terminated. In file included from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/LoadTimeInitializer.h:4:0,; from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/utils.cc:3:; /Library/Java/JavaVirtualMachines/jdk1.8.0_45.jdk/Contents/Home/jre/../include/jni.h:45:20: fatal error: jni_md.h: No such file or directory; compilation terminated. In file included from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/template.h:86:0,; from /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/avx_function_instantiations.cc:3:; /Users/louisb/Workspace/gatk/src/main/cpp/VectorLoglessPairHMM/common_data_structure.h: In instanti",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1504#issuecomment-187417081:274,Load,LoadTimeInitializer,274,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1504#issuecomment-187417081,3,['Load'],['LoadTimeInitializer']
Performance,"@gspowley Yes, defining the env. var. avoided crash. It just says unable to load GKL. thx!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2302#issuecomment-265880530:76,load,load,76,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2302#issuecomment-265880530,1,['load'],['load']
Performance,"@gudeqing I think you are referrring to the calls to `GetPileupSummaries`, where we have both `-L` and `-V` arguments with the same variable. This is actually not redundant, though I admit it is clumsy. This is a consequence of `GetPileupSummaries` being written as a GATK `LocusWalker`, which is necessary for optimal performance.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7731#issuecomment-1154211085:319,perform,performance,319,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7731#issuecomment-1154211085,1,['perform'],['performance']
Performance,"@jamesemery Great, thanks for checking. Could you do a review pass on this when you get a chance? It's not clear that the approach taken here of sending the owner config file around is what we want....it seems like instead we need a way to load the owner config from the launcher script itself.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4653#issuecomment-420055188:240,load,load,240,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4653#issuecomment-420055188,1,['load'],['load']
Performance,"@jamesemery I agree - all access (read and write) to `GenotypeLikelihoodCalculators` instance variables needs to be synchronized to make it safe. I think it would be sufficient to make `getInstance()` and `calculateGenotypeCountUsingTables()` synchronized. @droazen, are you concerned about performance for the Spark case? For the walker version, presumably the access is single-threaded, and hence [uncontended, which is very cheap](https://books.google.co.uk/books?id=mzgFCAAAQBAJ&pg=PA230&lpg=PA230&dq=java+uncontended+synchronization+goetz&source=bl&ots=7W4J807faW&sig=YALE1qdWoAUELPqLRhIedz-bZ20&hl=en&sa=X&ved=2ahUKEwj4jJeko8zdAhXVFsAKHazkBrcQ6AEwB3oECAIQAQ#v=onepage&q=java%20uncontended%20synchronization%20goetz&f=false). Another option would be to maintain a separate instance of `GenotypeLikelihoodCalculators` per genotyping engine. The size of the table is ploidy * alleles, so not too large?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5071#issuecomment-423546586:291,perform,performance,291,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5071#issuecomment-423546586,1,['perform'],['performance']
Performance,"@jamesemery While you're in the `HaplotypeCallerEngine` doing optimizations, you should profile peak memory usage as well and see if we can get it down to < 3 GB. This would reduce costs by allowing us to use cheaper instances on the cloud.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2591#issuecomment-460407699:62,optimiz,optimizations,62,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2591#issuecomment-460407699,1,['optimiz'],['optimizations']
Performance,@jberghout If you post your actual output we might be able to track down your variant of the problem (the error message in the original post looks to me somewhat like a corrupt gradle cache: error reading /vsc-hard-mounts/leuven-user/304/vsc30484/.gradle/caches/modules-2/files-2.1/org.spire-math/spire_2.11/0.11.0/998b1c1d841baf4fc5d1b119ea55f165f6684ef5/spire_2.11-0.11.0.jar; error in opening zip file). Is it the `gatkTabComplete` task that is failing for you as well ?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4155#issuecomment-566793345:184,cache,cache,184,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4155#issuecomment-566793345,2,['cache'],"['cache', 'caches']"
Performance,@jean-philippe-martin Any thoughts on the cause of the failure to load class `org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos` in travis? Do tests pass for you if you run them locally using `./gradlew test`?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1994#issuecomment-232406786:66,load,load,66,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1994#issuecomment-232406786,1,['load'],['load']
Performance,"@jean-philippe-martin Given the issues identified above with the zero-shuffle implementation, should this PR be closed while you re-think your approach to optimizing this tool?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/863#issuecomment-139650193:155,optimiz,optimizing,155,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/863#issuecomment-139650193,1,['optimiz'],['optimizing']
Performance,"@jean-philippe-martin I disagree that just because there are getters for the index, the index has to be there. It is already allowed to be absent for various reasons (via the special value `NO_ALIGNMENT_REFERENCE_INDEX`). Since use of the index instead of the name is mostly a performance optimization, I think we can get away with allowing records that have the name filled in but not the index. Relying on contig indices in general is a bad idea, as they are quite brittle, particularly when querying data from multiple sources. We purposefully moved away from using the indices in hellbender in favor of names when we migrated from `GenomeLoc` to `SimpleInterval`, and are, I think, willing to pay the extra cost of string parsing to avoid the subtle bugs that historically resulted from relying on the indices. This is a micro-optimization that probably isn't worth pursuing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141264817:277,perform,performance,277,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141264817,3,"['optimiz', 'perform']","['optimization', 'performance']"
Performance,"@jean-philippe-martin I think we should do the comparison in https://github.com/broadinstitute/gatk/issues/995 before porting the ApplyBQSR optimizations, actually. If it turns out that we decide to go with the simpler broadcast approach we'd then need to figure out how the dataflow ApplyBQSR changes fit in. So it probably makes sense to spin out a separate ticket for the ApplyBQSR changes and close this one.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/970#issuecomment-148758446:140,optimiz,optimizations,140,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/970#issuecomment-148758446,1,['optimiz'],['optimizations']
Performance,"@jean-philippe-martin In our initial tests with the latest gatk, we're still getting errors like this at a rate of ~2%:. ```; com.google.cloud.storage.StorageException: 503 Service Unavailable; java.lang.IllegalArgumentException: A project ID is required for this service but could not be determined from the builder or the environment. Please set a project ID using the builder.; com.google.cloud.storage.StorageException: 503 Service Unavailable; com.google.cloud.storage.StorageException: 503 Service Unavailable; java.lang.RuntimeException: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: Connection has been shutdown: javax.net.ssl.SSLException: java.net.SocketException: Connection reset; com.google.cloud.storage.StorageException: 503 Service Unavailable; com.google.cloud.storage.StorageException: 503 Service Unavailable; com.google.cloud.storage.StorageException: 503 Service Unavailable; com.google.cloud.storage.StorageException: 503 Service Unavailable; com.google.cloud.storage.StorageException: 503 Service Unavailable; java.lang.RuntimeException: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: Connection has been shutdown: javax.net.ssl.SSLException: java.net.SocketException: Connection reset; htsjdk.tribble.TribbleException$MalformedFeatureFile: Unable to parse header with error: Remote host closed connection during handshake, for input source:; ```. Now, I know that you put in an explicit retry for 503's, so I'm wondering what could be going on. I've asked the person running the tests to check that they're using an up-to-date GATK jar, but I'm wondering if we're setting all the right retry options on the GATK side. Eg., your PR https://github.com/GoogleCloudPlatform/google-cloud-java/pull/2083 says ""but only when OptionMaxChannelReopens is set"" -- are we setting this properly? Any other thoughts on things we could try?. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2749#issuecomment-306944607:555,concurren,concurrent,555,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2749#issuecomment-306944607,2,['concurren'],['concurrent']
Performance,"@jean-philippe-martin Thank you for putting this together, but performing authentication and reading the first byte might be too small a test for running on the Cloud. Could you run a profile test using `gcloud-java-nio` with 100 GB, 500 GB, 1 TB, 10 TB of data and process it?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2013#issuecomment-233417619:63,perform,performing,63,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2013#issuecomment-233417619,1,['perform'],['performing']
Performance,"@jean-philippe-martin Yup, reading hundreds of different files in the same tool. We're trying to optimize the buffer size for that case, that's what we're looking at in that other thread you were commenting on. It seems like we may have some memory leak somewhere else, you seem to need much more memory than we would expect if everything is working as expected. . This branch should fix the thread leak we were seeing. 👍 Merge when tests pass.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2643#issuecomment-299038867:97,optimiz,optimize,97,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2643#issuecomment-299038867,1,['optimiz'],['optimize']
Performance,"@jean-philippe-martin do you think it would be possible to fix this? The workaround is to set `GOOGLE_APPLICATION_CREDENTIALS`, but it would be preferable to have gcloud-java-nio fail when a `gs://` path is specified and the credentials are not set, rather than when the filesystem providers are loaded.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2110#issuecomment-241670337:296,load,loaded,296,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2110#issuecomment-241670337,1,['load'],['loaded']
Performance,"@jean-philippe-martin, this change changes `testBQSRLocal` to be truly local, so that it reads the reference from the local filesystem (which was added in https://github.com/broadinstitute/hellbender/pull/827). So, for that test no API key is needed. . The `testBQSRRefCloud` test still loads the reference from the cloud, so we're still testing that case too.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/862#issuecomment-136760832:287,load,loads,287,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/862#issuecomment-136760832,1,['load'],['loads']
Performance,"@jean-philippe-martin, we want to expose a walker-like interface, but we also care about the general ease of writing programs using the tools we and the user wrote (in native Dataflow/Spark). This is a point that @droazen, has emphasized to me several times. I'll let him add more detail on this if needed. I agree that the static approach won't work for Dataflow when workers are added, but I think I have a solution for Spark that works even when workers are added.; We'd create a new class (like I suggested above), but this would have a `Broadcast<SAMFileHeader>`, which is basically a lazy-loader for headers. We'd only load the header when needed.; I think this may be the best of all solution as it could also support several headers.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141177047:595,load,loader,595,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141177047,2,['load'],"['load', 'loader']"
Performance,"@jhl667 I'm looking into this. It looks like I neglected to set the sqlite connection to read only mode when connecting to the db file. I'm going to update it to do so. I'm not sure this applies when a read-only connection is created, but it looks like sqlite has some issues with NFS / distributed file systems:; - https://stackoverflow.com/questions/9907429/locking-sqlite-file-on-nfs-filesystem-possible ; - https://github.com/CGATOxford/CGATPipelines/issues/. One post in the github thread above mentions using `-o flock` when mounting Lustre partitions so that they all have concurrent locks. This _may_ be a workaround in the meantime. . I'll try to look at it on our NFS mounts - I don't have access to a Lustre fs.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4413#issuecomment-366009015:580,concurren,concurrent,580,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4413#issuecomment-366009015,1,['concurren'],['concurrent']
Performance,"@jjfarrell . After talking with @cmnbroad this afternoon, we'd like to ask you to perform an experiment to limit the scope where hunt down the issue. Is it possible for you to run `PrintReadsSpark` on the same cluster? That is, something similar to . ```bash; gatk --java-options ""-Djava.io.tmpdir=tmp"" \; PrintReadsSpark \; -R $REF \; -I $CRAM_DIR/$SAMPLE.cram \; -O hdfs:///project/casa/gcad/$CENTER/sv/$SAMPLE.cram \; -- \; --spark-runner SPARK \; --spark-master yarn \; --deploy-mode client \; --executor-memory 85G \; --driver-memory 30g \; --num-executors 40 \; --executor-cores 4 \; --conf spark.yarn.submit.waitAppCompletion=false \; --name ""$SAMPLE"" \; --conf spark.yarn.executor.memoryOverhead=5000 \; --conf spark.network.timeout=600 \; --conf spark.executor.heartbeatInterval=120; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5942#issuecomment-495357208:82,perform,perform,82,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5942#issuecomment-495357208,1,['perform'],['perform']
Performance,"@jjfarrell Glad you found that article useful!. In general, `--consolidate` will be memory and time intensive. It's not intuitive, but as you already figured out if `--consolidate` is enabled, we do it on the very last batch. If you only have on the order of a few hundred batches total, not having specified consolidate shouldn't affect read performance much. The only other thing that would help scale here would be to break up your intervals so that larger contigs are split up into multiple regions. Less memory required and you can throw more cores at it (if you have them). What sort of performance did you see on `GenotypeGVCFs` or `SelectVariants`? That could be the other issue with these large intervals.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1252834003:343,perform,performance,343,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1252834003,2,['perform'],['performance']
Performance,"@jjfarrell You don't need splitting index for cram. The index works around a bam specific problem which makes it hard to find good split points in the file. Cram is designed in a way that makes it easier to find the split points so the index is unnecessary. . I don't have good numbers for how long it takes to find the split points for bam. It depends on your filesystem. If you have a low latency file system like a local disk or hdfs setup than finding split points takes very little time (~seconds), but if you have a high latency file system like something backed by google object store then finding split points may take a long time (on the order of minutes to tens of minutes depending on latency and file size).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4506#issuecomment-371224015:391,latency,latency,391,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4506#issuecomment-371224015,3,['latency'],['latency']
Performance,"@jjfarrell and @mlathara: thanks for running these tests and sorry I havent been able to do anything yet with our data. I'm under some grant deadlines until into Oct, but I do hope to add to this. A couple comments:. 1) that broadly matches my experience. 2) My sense is that we were caught between and rock and a hard place with GenomicsDb and GenotypeGVCFs. Our workflow until this summer involved creating a workspace (running per-contig), which involved importing >1500 animals at first. This would execute OK when using a reasonable --batch-size on GenomicsDbImport. However, when we had large workspaces that were imported in lots of batches, GenotypeGVCFs (which we execute scattered, where each job works on a small interval) tended to perform badly and was a bottleneck (i.e. would effectively stall). Therefore we began to --consolidate the workspaces using GenomicsDBImport during the append process. Initially --consolidate worked; however, as @jjfarrell noted, that's memory intensive and once our workspace was a certain size, this basically died again. Therefore we even worked with @nalinigans to their the standalone GenomicsDB consolidate tool. This was a viable way to consolidate the workspaces and we successfully aggregated and consolidated all our data (which took a while). However, these massive, consolidated workspaces seem to choke GenotypeGVCFs. Therefore this process is still basically dead. 3) As I noted above, I'm currently giving up on trying to maintain permanent data in genomicsDB. There's so many advantages to not doing so, and letting the gVCFs exist as the permanent store. Notably, there are many reasons we would want/need to remake a gVCF (like the introduction of reblocking). Whenever any one of the source gVCFs changes, the workspace is basically worthless anyway (which is a massive waste of computation time). We've had great success running each GenotypeGVCFs scattered, where each job runs GenomicsDbImport on-the-fly, to make a transient workspace",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1256246852:744,perform,perform,744,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1256246852,2,"['bottleneck', 'perform']","['bottleneck', 'perform']"
Performance,"@kdatta @kgururaj We have some questions about this pull request. It looks like maybe you forgot some commits, (and included some extra ones that you didn't intend to). . 1. The tests are a direct copy and paste of `PrintReadsIntegrationTests` and couldn't possibly run. Did you forget to push the commit with the actual test code? We really need some tests especially because it's not totally obvious how to use this tool and what needs to be in the jsons. . 2. The tool currently takes a partition index and a json with stream ids. The way we had envisioned this working was that it would take a `-V` argument with a list of vcfs and `-L` argument specifying what chunk of the genome to load. Can you explain why it is designed this way and if it is possible to change to use the more idiomatic input style? . 3. Can you let us know when 0.4.0 is available on maven? . We have additional review comments, but it seems premature to get into those details until the above are answered.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-277310677:689,load,load,689,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-277310677,1,['load'],['load']
Performance,"@kdatta I think the reason its failing is because the dylib has an unresolved transitive dependency on openssl (and possibly other things). When I debug locally, I can see that it has streamed the dylib out to a local folder and its trying to load it, but then I get this (see the highlighted text):. /private/var/folders/cr/16ghvyfj5lvfwxx01rt1k4tdl04sy3/T/libtiledbgenomicsdb2535884808429708562.dylib: dlopen(/private/var/folders/cr/16ghvyfj5lvfwxx01rt1k4tdl04sy3/T/libtiledbgenomicsdb2535884808429708562.dylib, 1): Library not loaded: **/usr/local/opt/openssl/lib/libssl.1.0.0.dylib**; Referenced from: /private/var/folders/cr/16ghvyfj5lvfwxx01rt1k4tdl04sy3/T/libtiledbgenomicsdb2535884808429708562.dylib; Reason: image not found. It looks TileDB does have such a dependency ([here](https://github.com/Intel-HLS/TileDB/blob/master/CMakeLists.txt#L79)). Do you know if thats new ? I think we need to figure out how many of these there are and resolve them somehow.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-294230131:243,load,load,243,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-294230131,2,['load'],"['load', 'loaded']"
Performance,"@kdatta Looks like the integration tests passed on travis after clearing the cache! Once you address comments, squash, and rebase onto the latest gatk master the unit tests should pass as well, since you just need the TestNG fix that got merged into master. This means we can merge this today in all likelihood!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-296301829:77,cache,cache,77,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-296301829,1,['cache'],['cache']
Performance,"@kdatta Note that a requirement of this feature is that the batch size should limit the number of simultaneous `FeatureReaders` open at any given time. Each `FeatureReader` has an NIO buffer around its byte stream to make queries over GCS performant, and maintaining too many of such buffers at once would likely exceed any reasonable memory limits.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2613#issuecomment-296683686:239,perform,performant,239,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2613#issuecomment-296683686,1,['perform'],['performant']
Performance,"@kdatta The .so doesn't seem to load on travis, any thoughts?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-296267896:32,load,load,32,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-296267896,1,['load'],['load']
Performance,"@kdatta Won't using a `VCFCodec` instead of a `BCF2Codec` hurt performance? Can we try to understand why the problem arises when using a `BCF2Codec` by having a look in a debugger?. Also, can you explain why those particular INFO fields are not supported? Many of them are default annotations of the `HaplotypeCaller`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-293593226:63,perform,performance,63,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-293593226,1,['perform'],['performance']
Performance,"@kgururaj As I start to think about upgrading exome joint calling to use GenomicsDBImport the 100 interval threshold seems like it might be problematic. I've been working with WGS data, so I don't have much intuition for benchmarking with missing data. Is there any performance downside to running over larger intervals that include missing data? For example, if we want to scatter the exome 50 ways, each subset of the exome interval list will have ~4000 intervals, but the GVCFs won't have data outside those intervals. Does it make sense to pass to GenomicsDBImport a single interval encompassing all of those?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5066#issuecomment-409956462:266,perform,performance,266,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5066#issuecomment-409956462,1,['perform'],['performance']
Performance,"@kgururaj I ran the commands you suggested. . > [user@cedar5 bin]$ bash -x TestGenomicsDBJar/run_checks.sh; > + [[ hB != hxB ]]; > + XTRACE_STATE=-x; > + [[ hxB != hxB ]]; > + VERBOSE_STATE=+v; > + set +xv; > + unset XTRACE_STATE VERBOSE_STATE; > ++ uname -s; > + osname=Linux; > + jar xf genomicsdb--jar-with-dependencies.jar libtiledbgenomicsdb.so; > java.io.FileNotFoundException: genomicsdb--jar-with-dependencies.jar (No such file or directory); > at java.util.zip.ZipFile.open(Native Method); > at java.util.zip.ZipFile.<init>(ZipFile.java:219); > at java.util.zip.ZipFile.<init>(ZipFile.java:149); > at java.util.zip.ZipFile.<init>(ZipFile.java:120); > at sun.tools.jar.Main.extract(Main.java:1004); > at sun.tools.jar.Main.run(Main.java:305); > at sun.tools.jar.Main.main(Main.java:1288); > + jar xf genomicsdb--jar-with-dependencies.jar libtiledbgenomicsdb.dylib; > java.io.FileNotFoundException: genomicsdb--jar-with-dependencies.jar (No such file or directory); > at java.util.zip.ZipFile.open(Native Method); > at java.util.zip.ZipFile.<init>(ZipFile.java:219); > at java.util.zip.ZipFile.<init>(ZipFile.java:149); > at java.util.zip.ZipFile.<init>(ZipFile.java:120); > at sun.tools.jar.Main.extract(Main.java:1004); > at sun.tools.jar.Main.run(Main.java:305); > at sun.tools.jar.Main.main(Main.java:1288); > + '[' Linux == Darwin ']'; > + LIBRARY_SUFFIX=so; > + ldd libtiledbgenomicsdb.so; > ldd: ./libtiledbgenomicsdb.so: No such file or directory; > + md5sum libtiledbgenomicsdb.so; > md5sum: libtiledbgenomicsdb.so: No such file or directory. I'm using a compute canada server, so I don't have root access. The version of gatk4 I'm using was installed by their support team, and I load it using 'module load gatk'. I had that module loaded when I ran this test.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4124#issuecomment-357005071:1697,load,load,1697,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4124#issuecomment-357005071,3,['load'],"['load', 'loaded']"
Performance,"@kgururaj I ran with batch size 50, using a sample map, 5 reader threads:. `java -jar GATK_GDBfork.jar GenomicsDBImport --genomicsdb-workspace-path forkTest --batch-size 50 -L chr20:45840744-45870555 --sample-name-map gnarly_reblocked_all.sample_map --reader-threads 5`. That sample map has 80K genomes because that's the project I'm working on now. Log was as follows:; ```; 16:27:48.831 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/humgen/gsa-hpprojects/dev/gauthier/reblockGVCF/GATK_GDBfork.jar!/com/intel/gkl/nati; ve/libgkl_compression.so; 16:27:48.947 INFO GenomicsDBImport - ------------------------------------------------------------; 16:27:48.948 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.0.3.0-24-g8804e16-SNAPSHOT; 16:27:48.948 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 16:27:48.949 INFO GenomicsDBImport - Executing as gauthier@gsa5.broadinstitute.org on Linux v2.6.32-642.15.1.el6.x86_64 amd64; 16:27:48.949 INFO GenomicsDBImport - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 16:27:48.950 INFO GenomicsDBImport - Start Date/Time: May 4, 2018 4:27:48 PM EDT; 16:27:48.950 INFO GenomicsDBImport - ------------------------------------------------------------; 16:27:48.950 INFO GenomicsDBImport - ------------------------------------------------------------; 16:27:48.950 INFO GenomicsDBImport - HTSJDK Version: 2.14.3; 16:27:48.951 INFO GenomicsDBImport - Picard Version: 2.18.1; 16:27:48.951 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 16:27:48.951 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:27:48.951 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:27:48.951 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:27:48.951 INFO GenomicsDBImport - Deflater: IntelDeflater; 16:27:48.951 INFO GenomicsDBImport - Inflater: IntelInflater; 16",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388079572:416,Load,Loading,416,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388079572,1,['Load'],['Loading']
Performance,"@kgururaj Right, but we want the extra performance provided by using the BCF2Codec, if at all possible..",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2632#issuecomment-297985092:39,perform,performance,39,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2632#issuecomment-297985092,1,['perform'],['performance']
Performance,"@kgururaj Thanks for adding the test. Running it locally on my laptop (without your fix) succeeds though - I have to bump it up from 1000 intervals to 9000 to reproduce the stack overflow. But if I do that, it takes a long time to run, since it appears to be creating lots of small partitions. Is there any way to get it to use fewer partitions in a case like this where there are lots of intervals ?. Somewhat more concerning is that when with 8000 intervals, I see a different failure mode. First I see lots (thousands) of these messages:. `[GenomicsDB::VariantStorageManager] INFO: ignore message ""[TileDB::StorageManager] Error: Cannot list TileDB directory; Directory buffer overflow."" in the previous line`. followed by a failure that ends like this:. `[TileDB::StorageManager] Error: Cannot store schema; Too many open files in system.; libc++abi.dylib: terminating with uncaught exception of type LoadOperatorException: LoadOperatorException : Could not define TileDB array; TileDB error message : [TileDB::StorageManager] Error: Cannot store schema; Too many open files in system`. Can you reproduce that ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407214031:905,Load,LoadOperatorException,905,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407214031,2,['Load'],['LoadOperatorException']
Performance,"@kvn95ss Can you clarify whether the performance was **faster** in the newer version (4.2.5) compared to the older version? We did do some performance work in `SelectVariants` between 4.1.8 and 4.2.5. Also, there are reports that `SelectVariants` is **much** slower if the sample names in the header are not sorted, because the tool has to reorder the genotypes on output if they're not sorted. Can you check whether your sample names are sorted?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7671#issuecomment-1074271066:37,perform,performance,37,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7671#issuecomment-1074271066,2,['perform'],['performance']
Performance,"@kvn95ss It looks like you're using `--exclude-non-variants` with SelectVariants, which can negatively impact performance. It would be interesting to know how much leaving that out changes things, if thats an option for you.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7671#issuecomment-1075132292:110,perform,performance,110,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7671#issuecomment-1075132292,1,['perform'],['performance']
Performance,"@laserson One use case would be that after running your Spark pipeline you wanted to run a file-based tool on the resulting reads. The vast majority of our tools are still file-based, and for tasks like performing QC on the output of a spark pipeline it's easiest to materialize a single file and run existing tools on it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1015#issuecomment-153081201:203,perform,performing,203,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1015#issuecomment-153081201,1,['perform'],['performing']
Performance,"@laserson When you get a chance, would you mind adding an update here with the status of this, and whether you think it'll be possible to get any additional optimizations for `MarkDuplicatesSpark` in by alpha (Dec 4)? Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1100#issuecomment-158539873:157,optimiz,optimizations,157,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1100#issuecomment-158539873,1,['optimiz'],['optimizations']
Performance,"@laserson the `SAMRecord` vs. Google `Read` is a loooooong story.; The super-short version:; We had a bunch of utilities written for `SAMRecord` that @droazen refactored over months to take the GATKRead interface. As it happens, the SAM spec and the GA4GH spec are not 100% compatible. So, it's not possible to losslessly convert from A -> B -> A (where A is `SAMRecord` or Google `Read`). The cases where it doesn't work are edge cases, but they exist. Second, @jean-philippe-martin found that converting to Google `Read` was fairly expensive. Between those two points, I think we're probably better off with SAM-backed reads. (Also, right now the Google `Read` is serialized via JSON, so it's not that small anyway.). @tomwhite and @jean-philippe-martin, I think adding the header back will be fine for us engineers working on the engine, but it will make for a poorer user experience for newcomers and Comp Bios to burden them with having to care about what happens with shuffles (when they just want to prototype something). . That said, I think this is probably the best approach we have at our disposal. If we do, we need to do an excellent job of throwing errors if users try to perform actions that would require the header. The error message should explain what really happened and ideally point to some documentation we write explaining the stripping of the header and how to fix it. If this error occurs, it needs to be simple for anyone to fix it. @droazen @lbergelson, what do you two think? (also @laserson, do you have any ideas or thoughts on the header since we're probably stuck with `SAMRecord`?)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141086025:1186,perform,perform,1186,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141086025,1,['perform'],['perform']
Performance,"@lbergelson , as we discussed, I made the mistake of not using the returned RDD when calling `cache()`, after I fixed it, [here](http://dataflow01.broadinstitute.org:18088/history/application_1464285223085_0460/jobs/) is the runtime, running the following code:. ```; 148 final JavaPairRDD<Long, SGAAssemblyResult> cachedResults = results.cache(); // cache because Spark doesn't have an efficient RDD.split(predicate) yet; 149 // results.count(); // ugly hack to make the actual computation happen, so later filtering step will be based on what has been actually computed; 150 ; 151 // save fasta file contents or failure message; 152 final JavaPairRDD<Long, SGAAssemblyResult> success = cachedResults.filter(entry -> entry._2().assembledContigs!=null);; 153 final JavaPairRDD<Long, SGAAssemblyResult> failure = cachedResults.filter(entry -> entry._2().assembledContigs==null);; 154 ; 155 if(!success.isEmpty()){; 156 success.map(entry -> entry._1().toString() + ""\n"" + entry._2().assembledContigs.toString()); 157 .saveAsTextFile(outputDir+""_0"");; 158 }; 159 ; 160 if(!failure.isEmpty()){; 161 failure.map(entry -> entry._1().toString() + ""\n"" + entry._2().collectiveRuntimeInfo.toString()); 162 .saveAsTextFile(outputDir+""_1"");; 163 }; ```. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1877#issuecomment-225641452:94,cache,cache,94,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1877#issuecomment-225641452,6,['cache'],"['cache', 'cachedResults']"
Performance,"@lbergelson Are you interested in exploring this proposal as part of your performance work on `MarkDuplicatesSpark` this quarter (https://github.com/broadinstitute/gatk/issues/3706)? If not, feel free to remove from the 4.0 milestone.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1873#issuecomment-337351994:74,perform,performance,74,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1873#issuecomment-337351994,1,['perform'],['performance']
Performance,"@lbergelson Can you add a summary of the performance improvements (eg., runtime % speedup) introduced in this branch as a comment to the master Funcotator performance ticket (https://github.com/broadinstitute/gatk/issues/4586)?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4740#issuecomment-387475240:41,perform,performance,41,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4740#issuecomment-387475240,2,['perform'],['performance']
Performance,"@lbergelson Do you have an opinion on the best way to pip install the gcnvkernel python package and dependencies for Travis testing? I've verified that the pip install works within a basic conda environment with python=3.6. We'll need to load this environment both for unit/integration tests as well as WDL tests. As long as this is the only python environment we need, I think we can simply use the base environment in the Docker. If more environments are required (e.g., for @lucidtronix), then maybe we'll need to be more clever for unit/integration tests, but we can still load them manually in the scripts that kick off the WDL tests.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3838#issuecomment-348073948:238,load,load,238,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3838#issuecomment-348073948,2,['load'],['load']
Performance,"@lbergelson I am trying latest release, I use following command:. ```; ./gatk-4.1.3.0/gatk --java-options ""-Xmx4g"" FilterMutectCalls -O Filtered.vcf -V Try.vcf.gz -R ~/human.fa/ucsc.hg19.fasta; ```. and got following Info:. ```; Using GATK jar /mnt/md0/DataProcess/Ranshi/Mutect2/gatk-4.1.3.0/gatk-package-4.1.3.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx4g -jar /mnt/md0/DataProcess/Ranshi/Mutect2/gatk-4.1.3.0/gatk-package-4.1.3.0-local.jar FilterMutectCalls -O Filtered.vcf -V Try.vcf.gz -R /home/imp/human.fa/ucsc.hg19.fasta; 09:44:27.763 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/mnt/md0/DataProcess/Ranshi/Mutect2/gatk-4.1.3.0/gatk-package-4.1.3.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Aug 21, 2019 9:44:29 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 09:44:29.499 INFO FilterMutectCalls - ------------------------------------------------------------; 09:44:29.500 INFO FilterMutectCalls - The Genome Analysis Toolkit (GATK) v4.1.3.0; 09:44:29.500 INFO FilterMutectCalls - For support and documentation go to https://software.broadinstitute.org/gatk/; 09:44:29.500 INFO FilterMutectCalls - Executing as imp@imp-WorkStation on Linux v4.15.0-55-generic amd64; 09:44:29.500 INFO FilterMutectCalls - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_222-8u222-b10-1ubuntu1~16.04.1-b10; 09:44:29.501 INFO FilterMutectCalls - Start Date/Time: 2019年8月21日 上午09时44分27秒; 09:44:29.501 INFO FilterMutectCalls - ------------------------------------------------------------; 09:44:29.501 INFO FilterMutectCalls - ------------------------------------------------------------; 09:44:29.502 INFO FilterMutectCalls - HTSJDK Version: 2.20.1; 09:44:29.502 INFO FilterMutectCalls - Picard Version: 2.20.5; 09:",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6102#issuecomment-523262338:714,Load,Loading,714,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6102#issuecomment-523262338,1,['Load'],['Loading']
Performance,"@lbergelson I completely agree. GenomicsDB has been a big step up from CombineGVCFs, but it's been a struggle. Here are some notable characteristics of our data, in case anything seems relevant:. - The core data is ~2K WGS (not WXS) rhesus macaque datasets. WXS tends to perform better.; - We imported them into a workspace using GenomicsDB. ; - We regularly append new batches. As the data has grown, we found we need to lower the batch size (maybe 50-100) to make GenomicsDbImport practically work. It's possible a --consolidate type step might help us; however, our earlier efforts to run this resulted in hung jobs.; - We execute these jobs scattered across a cluster that is using a lustre filesystem. We use the non-posix optimization flag.; - We include the unplaced contigs, so our genome is ~2,900 contigs.; - We currently ask GenotypeGVCFs to call sites genome-wide. I've been pondering whether we should mask repetitive regions. This might also have the effect of removing sites with incredibly high numbers of distinct alleles . I'd appreciate any ideas you or the GATK team has.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7542#issuecomment-964343707:271,perform,perform,271,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7542#issuecomment-964343707,2,"['optimiz', 'perform']","['optimization', 'perform']"
Performance,"@lbergelson I think it doesn't actually need to implement max and min at all because those are only used in unit tests. Furthermore, the `kmerCounts` in `AssemblyResultSet` themselves are only used in such methods i.e. they don't need to be members at all. You could delete `CountSet` entirely without replacing it. And even if you keep it there it is definitely not crucial for performance and could easily be any old `SortedSet`. So, the options are 1) replace with `TreeSet` or whatever; 2) delete entirely. Which would you like?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2890#issuecomment-397398169:379,perform,performance,379,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2890#issuecomment-397398169,1,['perform'],['performance']
Performance,@lbergelson I updated this branch with the new key representation. After some performance runs it appears that these lead to a slightly faster mapping operation and approximately 15% less serialization for the step where they are used. Can you take a look?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4878#issuecomment-396730222:78,perform,performance,78,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4878#issuecomment-396730222,1,['perform'],['performance']
Performance,"@lbergelson I was referring more to the middle part of the StackOverflow by Daniel Chapman - specifically the [4.1 The ObjectStreamClass Class](http://docs.oracle.com/javase/8/docs/platform/serialization/spec/class.html#a5082) and [4.6 Stream Unique Identifiers](http://docs.oracle.com/javase/8/docs/platform/serialization/spec/class.html#a4100):. _If not specified by the class, the value returned is a hash computed from the class's name, interfaces, methods, and fields using the Secure Hash Algorithm (SHA) as defined by the National Institute of Standards._. Now when I look at the `java.io.ObjectStreamClass.java` file for 64-bit JDK7 and JDK8 - from src.zip - both have the same code for the following parts after performing a `diff` - I didn't list all of the lines of code since they are quite long:. ```; public long getSerialVersionUID() {; // REMIND: synchronize instead of relying on volatile?; if (suid == null) {; suid = AccessController.doPrivileged(; new PrivilegedAction<Long>() {; public Long run() {; return computeDefaultSUID(cl);; }; }; );; }; return suid.longValue();; }; ... private static long computeDefaultSUID(Class<?> cl) {; ...very long code which can be inspected via the src.zip file...; }; ```. So looking at the code portions of `computeDefaultSUID()` and I notice in our instance `ReadFilter` is a interface, which gets defined later via [ReadFilterLibrary.java](https://github.com/broadinstitute/hellbender/blob/62ef76ba60951c562a0d4c39189aa3f01f27f8d3/src/main/java/org/broadinstitute/hellbender/engine/filters/ReadFilterLibrary.java) or via `new ReadsFilter(readFilter, header)`, in either of these instances the fields would be different, based on this portion of `computeDefaultSUID` when looking at declared fields:. ```; Field[] fields = cl.getDeclaredFields();; MemberSignature[] fieldSigs = new MemberSignature[fields.length];; for (int i = 0; i < fields.length; i++) {; fieldSigs[i] = new MemberSignature(fields[i]);; }; ```. Therefore the `SUID` would be ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499:721,perform,performing,721,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499,1,['perform'],['performing']
Performance,"@lbergelson I'm not convinced we want to pre-calculate it at construction time, though. The null check is no big deal, and only needed in one place for each field (the method that retrieves or recalulates the cached value).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235127503:209,cache,cached,209,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235127503,1,['cache'],['cached']
Performance,"@lbergelson If you query for output after you've terminated the process, the query will fail immediately because the Futures will have been completed with a CancellationException when the pipes were broken by the termination. But I think even that might be subject to a race condition. Previously we were dependent on stdout/stderr for synchronization and error detection, but with the ack fifo and the python exception handler installed, we really aren't anymore. We do need to fix https://github.com/broadinstitute/gatk/issues/5100, and have a better logging integration strategy, but in general I think we should seek to eliminate all use of stdout/stderr except for advisory purposes. On a separate tangent, what I'd really like to do is unify the two PythonExecutors into a single one. All of these features I'm adding like profiling, version checking, logging integration etc., will have to be done in both of them otherwise.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5097#issuecomment-413575698:270,race condition,race condition,270,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5097#issuecomment-413575698,1,['race condition'],['race condition']
Performance,"@lbergelson In your opinion, how likely is this feature to cause problems? We do still call `QueryInterval.optimizeIntervals()` to merge intervals in `ReadsDataSource` before starting an iteration, and I think that's the main example of an HTSJDK query interface that can't handle overlapping intervals.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5887#issuecomment-567634113:107,optimiz,optimizeIntervals,107,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5887#issuecomment-567634113,1,['optimiz'],['optimizeIntervals']
Performance,@lbergelson Performance hit is not enough to worry about.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3305#issuecomment-316495018:12,Perform,Performance,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3305#issuecomment-316495018,1,['Perform'],['Performance']
Performance,"@lbergelson The purpose of `HomogeneousPloidyModel` is so that `getPloidy(int sample)` could return `ploidy` instead of `ploidies[sample]`. The speed difference ought to be negligible, I think. I suppose there's an O(# samples) memory cost, but that can't conceivably matter, can it?. Do you think it's okay to have a single `PloidyModel`, implemented as `HeterogeneousPloidyModel` is currently? That is, do you see any point in the ""optimizations"" in `HomogeneousPloidyModel`?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6082#issuecomment-519354634:434,optimiz,optimizations,434,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6082#issuecomment-519354634,1,['optimiz'],['optimizations']
Performance,"@lbergelson These changes are dependent on a Barclay [PR](https://github.com/broadinstitute/barclay/pull/17) that is not merged yet, so we should at least wait for that snapshot. Ideally, the other two (tiny) Barclay PRs in the queue should also be reviewed/merged, then we could do a release and upgrade to that.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2327#issuecomment-271991891:228,queue,queue,228,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2327#issuecomment-271991891,1,['queue'],['queue']
Performance,"@lbergelson did you get to the bottom of why the `hdfs` provider is loaded, but the `gs` one isn't?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2312#issuecomment-267992182:68,load,loaded,68,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2312#issuecomment-267992182,1,['load'],['loaded']
Performance,"@lbergelson everything I know I learned from there:; http://stackoverflow.com/questions/28939166/error-submitting-a-cloud-dataflow-job. Mine was also in the 4MB range, I switched to loading that file at the worker instead of the client and it worked. So the size limit is probably somewhere between 3 and 4MB.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/595#issuecomment-114594863:182,load,loading,182,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/595#issuecomment-114594863,1,['load'],['loading']
Performance,"@lbergelson sorry for my late response. I'm currently on vacations but I will try to respond (with some delay) any question.; So, what I'm seeing here (using Github web without having a proper dev env) is that for each interval, it's going to call (in parallel) sample reader function from the configuration =>; ```; final Map<String, FeatureReader<VariantContext>> sampleToReaderMap =; this.config.sampleToReaderMapCreator().apply(; this.config.getSampleNameToVcfPath(), updatedBatchSize, index); ; ```; That's is the first difference from previous implementation. If whatever you have in that function consume lots of memory, that's an issue.; Regarding the thread pool, I'm not seeing it's being starved by chromosome parallel import but it might use extra memory to execute since there is a high load of threads use due to the number of parallel imports.; Worker threads can execute only one task at the time, but the ForkJoinPool doesn’t create a separate thread for every single subtask. Instead, each thread in the pool has its own double-ended queue (or deque, pronounced deck) **which stores tasks**. Those are the two things I'm seeing right now without having the chance to debug :(.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-387810542:800,load,load,800,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-387810542,2,"['load', 'queue']","['load', 'queue']"
Performance,@lbergelson thank you for the comment and sorry for my bit late response. I excluded the dependency to the jsr203-s3a and tested that both local- and spark-gatk can access s3a files by dynamically loading it. I also added a new directory `scripts/s3a` for documentation and simple tests for s3a demonstration.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6698#issuecomment-665484597:197,load,loading,197,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6698#issuecomment-665484597,1,['load'],['loading']
Performance,"@lbergelson yes, i meant 2 concurrent tools making indices for the same **input** file. They will both try to write the same index file, with bad consequences.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/365#issuecomment-93539172:27,concurren,concurrent,27,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/365#issuecomment-93539172,1,['concurren'],['concurrent']
Performance,"@lbergelson yes, i meant the same number of WES samples as input (either merged into one gVCF or imported into one workspace), not size of data on the disk. I did not use --consolidate because your docs seemed to recommend against it. For WES, we performed the GenomicsDB import with default options, which I assume means no batching. We are doing batching on our WGS data, but that is not complete yet and we havent yet tried this for GenotypeGVCFs. I didnt realize --genomicsdb-shared-posixfs-optimizations was an option for GenotypeGVCFs, but will try this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6688#issuecomment-669369551:247,perform,performed,247,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6688#issuecomment-669369551,2,"['optimiz', 'perform']","['optimizations', 'performed']"
Performance,"@lbergelson, I don't think that this solution will help in this case, because another error when trying to use `CommandLineProgramTest`is that it extends `BaseTest`, which loads directly a `GenomeLocParser` for a reference that is not present and it blows up in every test. Regarding the `Main` class, because you point it out here, I would like to have some control over `Main` and how it manages things like errors or logging header. Basically all the things that I'm facing at the moment are, apart of this error using the testing framework, is that the framework have tons of mentions to the GATK itself (error messages pointing to the GATK manual page or bundle tools), and little control over which of them should be expose to the final user. Only as an example, I would like to output a line with the name and version of my software and a short notice about the usage of the GATK framework and which version I'm using (for easier maintenance, and contribution if a bug is found).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2122#issuecomment-242802278:172,load,loads,172,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2122#issuecomment-242802278,1,['load'],['loads']
Performance,"@ldgauthier ; My thinking on not doing the larger tests was that the native code hasn't changed for this support, so performance and functionality shouldn't see anything unexpected. Additionally, we technically do ""incremental import"" whenever the import is batched currently. We're just extending that same paradigm to extend beyond the case where the initial GenomicsDBImport command is used. Of course, all of this is not to say I don't want to do the larger tests...just wondering if we could capture that in a separate issue? @droazen mentioned that there's a tentative plan for a new GATK release this week and we would like to have this feature in there, if you agree. We'll work in parallel on the performance testing you requested. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5970#issuecomment-518327043:117,perform,performance,117,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5970#issuecomment-518327043,2,['perform'],['performance']
Performance,"@ldgauthier ; Performance for single import is the same, and we have CI tests on the genomicsdb side as well. Admittedly we haven't done a lot of large scale performance testing using GenomicsDBImport. Can you elaborate on what you mean by accuracy for large scale imports?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5970#issuecomment-517850985:14,Perform,Performance,14,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5970#issuecomment-517850985,2,"['Perform', 'perform']","['Performance', 'performance']"
Performance,"@ldgauthier @mbabadi Revisiting this in detail in preparation for a Methods meeting presentation, I discovered that much of the initial poor model performance and the issue with the strange GQs on chr1 was actually fixed in #4335 via a seemingly innocuous change. . Prior to the change, the ploidy model used *lexicographical* sorting to determine contig order (see #4374), but the contig order of the per-contig counts matrix was determined by the *sequence dictionary*. This understandably leads to shenanigans, since the model needs to know things like the number of intervals on each contig. After the change, the sequence-dictionary order is properly used everywhere and most of the bad behavior seems to be resolved (not all of the unusual sex genotypes are resolved, but this may be partly due to mosaicism in those samples) . (@mbabadi, not sure if we actually realized this before? I don't recall nor do I see it discussed elsewhere, but if so, then consider this a note of it!). So the problems with this model are not as severe as we initially thought, and thus we can keep this issue at relatively low priority. Nevertheless, the improved model should still yield additional benefits, such as better depth estimates and ploidy qualities, as well as more robustness to unusual sex genotypes.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-414852311:147,perform,performance,147,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-414852311,1,['perform'],['performance']
Performance,"@ldgauthier I think it's probably best to try and get to the bottom of why this variant's qual isn't being adjusted as it's reduced from 7 alleles in the gVCF to 2 alleles in the called VCF. This is, as you guessed, all single-sample. I've attached a reduced gVCF that just includes the variant in question, and the resulting genotyped VCF from running the command line below (and their indices) ; [here](https://github.com/broadinstitute/gatk/files/3059414/gatk-5793-testcase.tar.gz). ```; gatk GenotypeGVCFs \; -R /Work/refseq/hg19/hg19.fasta \; -V HG02568.g.vcf \; -O HG02568.vcf \; -L chr11:6637700-6637800 \; -stand-call-conf 30; ```. A few observations from running the above command but varying the `-stand-call-conf` at that locus, all performed with GATK 4.1.1.0:; - Running with `-stand-call-conf 30` results in a reduction from 7->2 alleles but no change at all in QUAL; - Running with `-stand-call-conf 0` results in the same genotype and QUAL, but another allele squeaks through even though it's not referenced in the genotype; - Running with `-stand-call-conf 100` results in no variants being emitted. Circling back to one of my original statements, I believe the least confusing way for this to work would be to think of it this way:; - If you run with `-stand-call-conf 0` you should see all variants; - If you run with `-stand-call-conf n` you should only lose variants that were previously emitted with `-stand-call-conf 0` that had QUAL < n. That said, it sounds like maybe the problem is less with the filtering on QUAL and more to do with the calculation of the final QUAL that ends up in the VCF?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5793#issuecomment-481273026:744,perform,performed,744,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5793#issuecomment-481273026,1,['perform'],['performed']
Performance,"@ldgauthier I'm in complete agreement, I don't think we should optimize outside the high confidence regions.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-714549510:63,optimiz,optimize,63,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-714549510,1,['optimiz'],['optimize']
Performance,"@ldgauthier Indexing inputs on the fly is prone to race conditions, so we decided early on to only index outputs on-the-fly.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3837#issuecomment-345064459:51,race condition,race conditions,51,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3837#issuecomment-345064459,1,['race condition'],['race conditions']
Performance,"@ldgauthier It actually looks like the PairHMM one might be a bug. It should be skipping the subset of ones that aren't loadable, but it seems like it think's it's succesfully loading the library but then failing when it actually tries to compute it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5339#issuecomment-592720481:120,load,loadable,120,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5339#issuecomment-592720481,2,['load'],"['loadable', 'loading']"
Performance,"@ldgauthier It does, but you should always run via the launch script and `--java-options`, since the script sets a number of important system properties, some of which affect performance. Do you know whether the GATK3 HC is able to run on the same bams without running out of memory in 4G/2G/1G?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4272#issuecomment-385489993:175,perform,performance,175,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4272#issuecomment-385489993,1,['perform'],['performance']
Performance,"@ldgauthier This is why the `--disable-sequence-dictionary-validation` argument exists in `GATKTool`. If you're confident in the compatibility of your inputs, and the checks are too expensive, you can run with that option and (optionally) perform some less strict validation of your own in your `onTraversalStart()` method.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6589#issuecomment-625366653:239,perform,perform,239,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6589#issuecomment-625366653,1,['perform'],['perform']
Performance,@ldgauthier This optimization is in the old `DiploidExactAFCalculator`. Can we close the issue?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1437#issuecomment-397359485:17,optimiz,optimization,17,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1437#issuecomment-397359485,1,['optimiz'],['optimization']
Performance,@ldgauthier gradle caches things in ~/.gradle by default. This is tiny on our systems so it's likely you're exceeding your quota. . set GRADLE_USER_HOME in your bashrc to somewhere with sufficient space. ```; export GRADLE_USER_HOME=<somewherewithspace>; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1364#issuecomment-164529488:19,cache,caches,19,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364#issuecomment-164529488,1,['cache'],['caches']
Performance,"@ldgauthier so I think this is a different issue with overlapping deletions. The original bug was with queries, this is manifesting on loading. This seems to be an issue specific to dealing with the * allele and the fact that the deletion spans the interval specified. We're working on a fix.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5449#issuecomment-484951791:135,load,loading,135,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5449#issuecomment-484951791,1,['load'],['loading']
Performance,"@ldgauthier's points are well-taken. I'm not quite ready to close this issue, but I believe there are other optimizations that don't have the downsides of this one.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2945#issuecomment-310475504:108,optimiz,optimizations,108,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2945#issuecomment-310475504,1,['optimiz'],['optimizations']
Performance,"@ldgauthier, I've performed a pass. I think the _Caveat_ section of the AS_StrandOddsRatio doc could use some attention. Here it is currently (I did not touch it):. ![screenshot 2019-03-07 15 37 39](https://user-images.githubusercontent.com/11543866/53987567-60fc0000-40ef-11e9-8415-d52403f01f83.png). Here is what the rendered javadocs look like now. Let me know what you think.; ![screenshot 2019-03-07 15 37 21](https://user-images.githubusercontent.com/11543866/53987525-475ab880-40ef-11e9-8f0e-24a57a84586a.png). ![screenshot 2019-03-07 15 37 32](https://user-images.githubusercontent.com/11543866/53987535-4d509980-40ef-11e9-835b-65ceee2cdc06.png)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5703#issuecomment-470685949:18,perform,performed,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5703#issuecomment-470685949,1,['perform'],['performed']
Performance,"@lucidtronix @mbabadi @samuelklee I think the best solution would be to establish a single, common Python environment, with a single set of dependencies, that all GATK Python tools depend on. We would establish a single docker image that has all of these dependencies pip installed, and could also include a conda env for the GATK environment for users who don't want to use the docker image. If we could do that, it would eliminate the need load per-tool conda environments. From what I've seen so far based on existing branches, the two environments we need (gCNV and CNN-VQSR) don't look that far apart in terms of dependencies. gCNV is using Theano, and CNN Tensorflow, but the rest looks [pretty close](https://docs.google.com/a/broadinstitute.org/spreadsheets/d/1RV7--uBQ0ctlXzMH09cmr0VimpZYIU68DdxJzE60y-c/edit?usp=sharing). So a strawman proposal for the main components for a common environment would be:. Python 3.6; Numpy >= 1.13.1; Scipy 1.0.0; Theano .0.9.0; Tensorflow 1.4.0; Pymc3 3.1; Keras 2.1.1. Can you all chime on on whether you think we can converge in a single environment ? If so, it would greatly simplify things, and we can start with getting a docker image built for running travis tests.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3692#issuecomment-348188451:442,load,load,442,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3692#issuecomment-348188451,1,['load'],['load']
Performance,"@lucidtronix Any suggestions on this? I don't recall ever seeing this before, but we've hit it about 5 times in tests in the last week or so. It looks like its coming from this [jit assembler](https://github.com/intel/caffe/blob/a3d5b022fe026e9092fc7abc7654b1162ab9940d/xbyak/xbyak.h#L229) code used by the intel-optimized tensorflow ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6307#issuecomment-566108578:313,optimiz,optimized,313,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6307#issuecomment-566108578,1,['optimiz'],['optimized']
Performance,"@magicDGS . I am afraid this is not easy. I didn't write the binding (@tedsharpe did), but I would asseme the limitation comes from bwa mem itself, not the binding, as the binding is a thin wrapper that delegates the loading of the index files (or the image that combines all 5 index files in this case) to bwa. . The SV team here have a script (`scripts/sv/default_init.sh`) that when the Spark cluster is created and initialized, the image file is distributed to all walker nodes. Spark clusters other than Google's Dataproc would probably allow you to provide scripts as initialization actions as well. On the other hand, there seem to be a `--files` argument that you can append to your cmd line arguments which yarn will parse and distribute the provided local file to all nodes, though in this case it will be very inefficient considering the image file's size.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312362074:217,load,loading,217,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312362074,1,['load'],['loading']
Performance,"@magicDGS @heuermh It sounds like there are definite advantages to switching to SLF4j. Our fear is that the switch will end up resulting in confusing class loading issues in different spark environments. We're just beginning a round of spark performance testing and evaluation under a pretty tight deadline, and we don't want to introduce any surprises. . We'd be happy to look at a pull request, but we might delay until the end of quarter to fully test / merge it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2176#issuecomment-261547892:156,load,loading,156,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2176#issuecomment-261547892,2,"['load', 'perform']","['loading', 'performance']"
Performance,@magicDGS Doesn't the static block run when a subclass of Main is loaded as well?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3483#issuecomment-324388854:66,load,loaded,66,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3483#issuecomment-324388854,1,['load'],['loaded']
Performance,@magicDGS I provide some example data for a tutorial at <https://gatkforums.broadinstitute.org/gatk/discussion/7156/howto-perform-local-realignment-around-indels>. Search the page for ` tutorial_7156.tar.gz`. I showcase illustrative sites within the tutorial and also in <https://software.broadinstitute.org/gatk/blog?id=7847>. I'm actually new to test data so what cases are you hoping to test with the data? The snippet in the tutorial data is much larger than you need so it would be good narrow down the test case.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3104#issuecomment-314886000:122,perform,perform-local-realignment-around-indels,122,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3104#issuecomment-314886000,1,['perform'],['perform-local-realignment-around-indels']
Performance,"@magicDGS I'm still missing why this wouldn't work for downstream projects (as long as they load Main or some Main-derived class). I think the owner config issue is different; for locale, we need to always force US. Can you verify this, or maybe provide more details about what case doesn't work ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3483#issuecomment-324622945:92,load,load,92,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3483#issuecomment-324622945,1,['load'],['load']
Performance,"@magicDGS Overall, the documentation changes are good. A few more minor comments: ; - If these are classes that are really pegged to the usage of LIBS, then you may want to mention that in the javadocs for the class. A comment along the lines of ""these classes are fairly low-level, developers should probably confirm that their changes do not belong in a higher-level calss such as LIBS"". ; - I am okay with an informal test of the speed, even if you just look at some logs. From the review, it looks like behavior of higher-level API calls will be unchanged. >I think that because the LocusIteratorByState is already splitting by sample, that can improve even more performance, because it will come directly in the state where it can be used by-sample in an efficient way. And maybe, if the tool does not require to split by sample at all, we can add an option to disable that behavior while creating the tracker. Agreed. I do not think you need to worry about the flag, for now. If you'd like, file an issue, but I think it is low priority for us. I tend to be worried about new developers or contributors getting lost in the codebase. And many do not have experience w/ GATK3. Hence, the documentation about when to use the class and why it exists. A couple additional things:; - I found a typo (see my comment); - Can you document the `presorted` parameter? Make sure to mention that if a developer specifies false, and sorting is needed, it will be done under-the-hood.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2321#issuecomment-332215187:667,perform,performance,667,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2321#issuecomment-332215187,1,['perform'],['performance']
Performance,"@magicDGS Sorry for the delayed reply, I had to see what direction the `HaplotypeCaller` branch would take before I could answer your post above. In order to get the `HaplotypeCaller` performance up to acceptable levels we've had to make some changes to the traversal that have caused it to diverge quite a bit from the idea of a `SlidingWindowWalker` in this branch. Also, the way `SlidingWindowWalker` handles the `intervalsForTraversal` (using them to select fixed-size windows) is not compatible with what the `HaplotypeCaller` currently requires. As a result, I recommend that we merge your `SlidingWindowWalker` in as a separate traversal rather than trying to reconcile it with the `HaplotypeCaller` branch and mutate it into something that might not be as useful to you. Fortunately, walkers in GATK4 are simple enough that it's perfectly fine to have several similar-but-subtly-different walker types, provided they all serve actual use cases. I'll",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1528#issuecomment-204134447:184,perform,performance,184,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1528#issuecomment-204134447,1,['perform'],['performance']
Performance,"@magicDGS The HaplotypeCaller traversal has undergone some changes in the past few weeks to improve performance and bring the output of the tool closer to GATK3. There is now an `AssemblyRegionWalker` that divides the intervals into active and inactive regions, in a greatly simplified version of the GATK3 traversal. Initially, I did plan on having `AssemblyRegionWalker` extend the former `ReadWindowWalker`, or an adapted version of your `SlidingWindowWalker`, and I did implement it like this at first, but ultimately I collapsed it into a single class for several reasons:; - Inheriting from a more generic traversal type caused usability issues and confusion with respect to the command-line arguments. The `ReadWindow` was the unit of processing for the superclass, but for `AssemblyRegionWalker` it was the unit of I/O and `AssemblyRegion` was the unit of processing, and I couldn't update the docs for `ReadWindowWalker` to clear up the confusion without mentioning `AssemblyRegion`-specific concepts.; - The `ReadShard` / `ReadWindow` was/is **only** there to prove that we can shard the data without introducing calling artifacts, and to provide a unit of parallelism for the upcoming Spark implementation. It's not something we really want to expose to users as a prominent knob, and we may hide it completely in the future once the shard size is tuned for performance.; - Inheriting from a more abstract walker type caused a number of other problems as well: methods that should have been final in the supertype could no longer be made final, with the result that tool implementations could inappropriately override engine initialization/shutdown routines. Also, there were issues with the progress meter, since both the supertype traversal and subtype traversal needed their own progress meter for their different units of processing. Ultimately it was just too awkward and forced, and the read shard is something that we eventually want to make an internal/encapsulated implementation d",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210806513:100,perform,performance,100,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210806513,1,['perform'],['performance']
Performance,"@mbabadi Ah, well file-based I/O would be the simplest option of all, of course, and should definitely be considered as a candidate solution to this ticket, particularly if you've already tried it and found the performance penalty to be minimal for your use case (@cmnbroad take note). The division of labor you describe between Java and Python sounds great, by the way -- exactly the sort of approach I was hoping you'd implement.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3698#issuecomment-337319853:211,perform,performance,211,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3698#issuecomment-337319853,1,['perform'],['performance']
Performance,"@mbabadi I've updated my PR to use miniconda3. @mbabadi @lucidtronix @samuelklee I think we should aim for tools that at least run out-of-box, without depending on any out-of-band configuration other than the conda env. On top of that we can provide guidance/configs for users on how to enable further optimizations, like g++. Does that sound like an achievable goal ?. As for the docker, we're going to have strike the right balance between image bloat and performance(including test performance). I think we're around 4+ gig now, and counting. Before the Python integration we were at 1.9G, and trying to find ways to reduce it. So lets see where we wind up but keep that in mind. Finally, we need to find a way to install the (GATK) python package(s) without depending on access to the GATK repo. Right now I think the gCNV branch has a ""pip install from source"" added to the conda env .yml. That will work on the docker at the moment (and thus on travis), but that won't work for non-docker users how don't have source/repo access. Also, one of the proposals to reduce the size of the docker is to remove the repo clone that is currently there. My proposal is that we change the gradle build to create an archive/zip of the python source (this would include the VQSR-CNN package code as well as gCNV kernel). We can then copy that on to the docker image, and pip-install it from the copy. That would retain the ability to always run travis tests based on the code in the repo, and also keep the nightly docker image in sync. We'll also have deliver the archive as an artifact somehow (perhaps including PyPi) for non-docker users.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3912#issuecomment-350303277:302,optimiz,optimizations,302,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3912#issuecomment-350303277,3,"['optimiz', 'perform']","['optimizations', 'performance']"
Performance,@meganshand - FYI a small bug fix and there is another one (in t0 parsing) to follow. This one has a very tiny positive effect on indel performance,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8171#issuecomment-1403552050:136,perform,performance,136,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8171#issuecomment-1403552050,1,['perform'],['performance']
Performance,"@meganshand Here's a quick example:. ![image](https://user-images.githubusercontent.com/11076296/158385742-20a3303b-d8ce-4335-b42f-622da9bfa8d3.png); ![image](https://user-images.githubusercontent.com/11076296/158385777-6174f8b8-7abb-4b31-92d1-11cc8064854b.png). Note that the malaria data used was pretty small: chr1-2 training (~20k positive training/truth variants, ~50k negative training variants; note also that the threshold for determining negative training was not tuned---a threshold corresponding to a 98% truth sensitivity was arbitrarily chosen), chr3 validation (~50k variants), and chr4-6 test (~150k variants). The LL score is calculated from a validation set held out from the training/truth positives used to train the model, while the F1 score is calculated using ""orthogonal truth"" positives/negatives determined using 3 families of ~30 trios each. However, there's some arbitrariness in how we define the boundary for the latter positives/negatives, and hence some arbitrariness in the F1 score itself. But I'd expect using gold-standard GIAB truth would be more straightforward. Not sure how much we can conclude, but that the validation and test F1s are similar and that the validation LL score isn't *too* far off are encouraging. That said, there is a pretty big drop in recall when optimizing LL. But we should also expect some discrepancy between LL and F1, according to one of the papers linked above. I would hope that with more variants or reliable training/truth (as in your data), things might stabilize or line up better. I'll try running with more malaria data, as well. The following trios x sites heatmap (top plot) for the validation set might better illustrate the arbitrariness in F1 (click to enlarge):. ![image](https://user-images.githubusercontent.com/11076296/158385585-1a0dfe8e-d4b7-4770-aed0-19ad81162c92.png). Here, yellow = het errors (since these are supposed to be clonal malaria samples), red = Mendelian errors, grey = no calls, green = Mendelian con",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1067396431:473,tune,tuned---a,473,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1067396431,1,['tune'],['tuned---a']
Performance,@mishaploid and @spikebike. 4.1.8.0 has a new option - `--genomicsdb-shared-posixfs-optimizations` for GenomicsDBImport that disable file locking and minimize writes to NFS. We are interested to know if this option helps your use case even as we continue making performance improvements. Thanks.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6487#issuecomment-651343677:84,optimiz,optimizations,84,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6487#issuecomment-651343677,2,"['optimiz', 'perform']","['optimizations', 'performance']"
Performance,"@mishaploid, I am assuming the 295 to be single sample vcfs. What do the `data/processed/scattered_intervals/0004-scattered.intervals` look like? Are the intervals very small? Have you tried larger intervals?. @spikebike, thanks for the systemtap output. We do have large internal buffers to help with this type of usage, but will revisit the code to figure out the behavior you are seeing. We do have some experimental optimizations not rolled out yet for writing minimally to shared filesystems. Would you be able to run your tests if we create a gatk branch next week with those changes?. @ldgauthier, I think the Hail team used multi sample vcfs as well. We do have some optimizations(work-in-progress) for importing multi sample vcfs that will get rolled out in the next GenomicsDB release.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6487#issuecomment-595405471:420,optimiz,optimizations,420,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6487#issuecomment-595405471,2,['optimiz'],['optimizations']
Performance,"@mlathara As suggested, I removed all MNPs from normal vcf files and changed the bed file format and it worked. But it is not going beyond chromosome 1. Here are the stack trace:. 15:44:02.495 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/akansha/vivekruhela/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 16, 2021 3:44:02 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 15:44:02.750 INFO GenomicsDBImport - ------------------------------------------------------------; 15:44:02.750 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.1.9.0; 15:44:02.750 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 15:44:02.750 INFO GenomicsDBImport - Executing as akansha@sbilab on Linux v4.4.0-169-generic amd64; 15:44:02.751 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_265-8u265-b01-0ubuntu2~16.04-b01; 15:44:02.751 INFO GenomicsDBImport - Start Date/Time: January 16, 2021 3:44:02 PM IST; 15:44:02.751 INFO GenomicsDBImport - ------------------------------------------------------------; 15:44:02.751 INFO GenomicsDBImport - ------------------------------------------------------------; 15:44:02.751 INFO GenomicsDBImport - HTSJDK Version: 2.23.0; 15:44:02.751 INFO GenomicsDBImport - Picard Version: 2.23.3; 15:44:02.752 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 2 ; 15:44:02.752 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 15:44:02.752 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 15:44:02.752 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 15:44:02.752 INFO GenomicsDBImport - Deflater: IntelDeflater; 15:44:02.752 INFO GenomicsDBImport - Inflater: IntelInflater; 15:44:02.752 INFO GenomicsDBImport ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7037#issuecomment-761558811:220,Load,Loading,220,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7037#issuecomment-761558811,1,['Load'],['Loading']
Performance,"@mlathara I think we're talking a bit in circles. The main use case I foresee for a generic split/merge tool would be to allow parallelized processing. I cant say there wouldnt be other uses I'm not seeing now (in the VCF world, SelectVariants is an extremely useful tool), but i dont have a specific use-case for GenomicsDB subsetting today beyond this. . I would point out this rapidly gets into specifics and quirks of any one user's infrastructure. I dont actually mind copying the GenomicsDB workspace prior to appending to it, because processing occurs on shared lustre space, while our permanent data lives on other disk space. Therefore we would probably do a copy no matter what. I agree you dont want to develop our one person's infrastructure. . The only aspect that gives me pause on your plan regarding split jobs is that GATK doesnt provide the scheduler. Sure there used to be queue and I gather GATK pushes WIDL/Cromwell (unless this changed), but we never used these. If GATK is not trying to provide the scheduler (which is better), does this really just look like: . 1) kick off X independent jobs for GenomicsDB/append; 2) each job specifies the interval(s) on which to operate; 3) Each job has no knowledge of the other jobs; 4) each job writes it's output to the same workspace; 5) Presumably there is something in place so jobs can run concurrently. This must be the new feature?. I imagine this could work. It does obligate one to have/use some kind of shared disk space, which we can handle, but could be a negative for some.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6620#issuecomment-641469405:892,queue,queue,892,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6620#issuecomment-641469405,2,"['concurren', 'queue']","['concurrently', 'queue']"
Performance,"@mlathara Let's discuss our options at our next meeting...if the problem is common enough, and a proper fix is not coming in the short term, we might want to consider making the VCF codec the default despite the performance hit.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6667#issuecomment-646232210:212,perform,performance,212,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6667#issuecomment-646232210,1,['perform'],['performance']
Performance,"@mlathara The nodes have considerably more (256 or so). is there any rule or thumb or guidance on expected memory needs based on number of gVCFs and/or type of input (WES vs WGS)?. I do think you might be onto something though. Out default cluster submission code takes our slurm job memory request, subtracts only a few GB and passes the remainder to -Xmx/Xms. I will update to leave more buffer as you suggest. Our cluster happens to be undergoing maintenance this week, so this particular job was killed. I'll update the GATK version, add --genomicsdb-shared-posixfs-optimizations, and adjust the memory. One other thing: i noticed GenomicsDBImport is not nearly as verbose in logging as typical GATK tools. Is that expected, or a symptom of whatever problem we're having?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6688#issuecomment-656259475:570,optimiz,optimizations,570,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6688#issuecomment-656259475,1,['optimiz'],['optimizations']
Performance,"@nalinigans Mixed results so far. I'm running the new consolidate tool, per chromosome as before. I ran it with defaults (no custom arguments). It is running longer than previously, but chr 1, the largest, died after consolidating 2 attributes. This job had 248G of RAM allocated. Are there optimizations you'd suggest?. ```; 02 Apr 2022 16:34:31,433 DEBUG: 	[April 2, 2022 4:34:31 PM PDT] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 5,993.06 minutes.; 02 Apr 2022 16:34:31,438 DEBUG: 	Runtime.totalMemory()=178017796096; 02 Apr 2022 16:34:31,443 DEBUG: 	Tool returned:; 02 Apr 2022 16:34:31,448 DEBUG: 	true; 02 Apr 2022 16:34:34,663 INFO : Will consolidate the workspace using consolidate_genomicsdb_array; 02 Apr 2022 16:34:34,723 INFO : Consolidating contig folder: /home/exacloud/gscratch/prime-seq/workDir/344c6137-8a85-103a-821a-f8f3fc86deba/Job1.work/WGS_v2_713_2ndMerge.gdb/1$1$223616942; 02 Apr 2022 16:34:34,748 INFO : 	/home/exacloud/gscratch/prime-seq/bin/consolidate_genomicsdb_array -w /home/exacloud/gscratch/prime-seq/workDir/344c6137-8a85-103a-821a-f8f3fc86deba/Job1.work/WGS_v2_713_2ndMerge.gdb --shared-posixfs-optimizations -a 1$1$223616942; 02 Apr 2022 16:34:34,754 DEBUG: using path: /home/exacloud/gscratch/prime-seq/bin:/home/exacloud/gscratch/prime-seq/bin/:/home/exacloud/gscratch/prime-seq/java/current/bin/:/home/exacloud/gscratch/prime-seq/bin/:/usr/local/bin:/usr/bin; 02 Apr 2022 16:34:35,059 DEBUG: 	16:34:35.059 info consolidate_genomicsdb_array - pid=34848 tid=34848 Starting consolidation of 1$1$223616942 in /home/exacloud/gscratch/prime-seq/workDir/344c6137-8a85-103a-821a-f8f3fc86deba/Job1.work/WGS_v2_713_2ndMerge.gdb; 02 Apr 2022 16:34:36,091 DEBUG: 	Using buffer_size=10485760 for consolidation; 02 Apr 2022 16:34:36,097 DEBUG: 	Number of fragments to consolidate=26; 02 Apr 2022 16:34:36,101 DEBUG: 	Sat Apr 2 16:34:36 2022 Memory stats beginning consolidation size=483MB resident=379MB share=6MB text=13MB lib=0 data=",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1087750975:291,optimiz,optimizations,291,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1087750975,1,['optimiz'],['optimizations']
Performance,"@nalinigans OK, so most of these jobs are still going (we run per contig); however, one just died as follows:; ```; 03 Mar 2022 12:49:28,390 INFO : 	/home/exacloud/gscratch/prime-seq/bin/consolidate_genomicsdb_array -w /home/exacloud/gscratch/prime-seq/workDir/0950f56c-7565-103a-a738-f8f3fc8675d2/Job3.work/WGS_1852_consolidated.gdb --shared-posixfs-optimizations --segment-size 32768 -a 3$1$185288947; 03 Mar 2022 12:49:28,444 DEBUG: 	12:49:28.444 info consolidate_genomicsdb_array - pid=147768 tid=147768 Starting consolidation of 3$1$185288947 in /home/exacloud/gscratch/prime-seq/workDir/0950f56c-7565-103a-a738-f8f3fc8675d2/Job3.work/WGS_1852_consolidated.gdb; 03 Mar 2022 12:58:29,209 DEBUG: 	Using buffer_size=32768 for consolidation; 03 Mar 2022 12:58:29,222 DEBUG: 	12:58:29 Memory stats(pages) beginning consolidation size=16404102 resident=16377455 share=1814 text=3530 lib=0 data=16375282 dt=0; 03 Mar 2022 12:58:29,228 DEBUG: 	12:58:29 Memory stats(pages) after alloc for attribute=END size=16404138 resident=16377465 share=1821 text=3530 lib=0 data=16375318 dt=0; 04 Mar 2022 16:03:57,025 WARN : 	process exited with non-zero value: 137; ```. Is there any information from this, or information i could gather, that's helpful here?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1059616021:351,optimiz,optimizations,351,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1059616021,1,['optimiz'],['optimizations']
Performance,"@nalinigans Tests passed, but it's worth pointing out that the new `testWriteToAndQueryFromGCS()` test took 9 minutes to complete, which seems very slow. Possible performance issue?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5197#issuecomment-430760216:163,perform,performance,163,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5197#issuecomment-430760216,1,['perform'],['performance']
Performance,"@nalinigans This iteration is on a smaller input (~600 WGS samples). Based on the info below, do you suggest changing --buffer-size or --batch-size?. To your questions:. In chromosome 1's folder (1$1$223616942), there are 26 fragments (the GUID-named folders). The sizes of book_keeping files are:. 168M; 131M; 155M; 149M; 136M; 142M; 216M; 147M; 150M; 134M; 127M; 75M; 172M; 122M; 207M; 122M; 581M; 149M; 150M; 141M; 149M; 143M; 143M; 165M; 160M; 163M. The last job failed an OOM error (the job requested 256GB and the slurm controller killed it). This is the command and output (with timestamps):. ```; 10 Apr 2022 01:56:21,275 INFO : 	/home/exacloud/gscratch/prime-seq/bin/consolidate_genomicsdb_array -w /home/exacloud/gscratch/prime-seq/workDir/344c6137-8a85-103a-821a-f8f3fc86deba/Job1.work/WGS_v2_713_2ndMerge.gdb --shared-posixfs-optimizations -a 1$1$223616942. 10 Apr 2022 01:56:21,556 DEBUG: 	01:56:21.556 info consolidate_genomicsdb_array - pid=146087 tid=146087 Starting consolidation of 1$1$223616942 in /home/exacloud/gscratch/prime-seq/workDir/344c6137-8a85-103a-821a-f8f3fc86deba/Job1.work/WGS_v2_713_2ndMerge.gdb; 10 Apr 2022 01:56:22,385 DEBUG: 	Using buffer_size=10485760 for consolidation; 10 Apr 2022 01:56:22,391 DEBUG: 	Number of fragments to consolidate=26; 10 Apr 2022 01:56:22,396 DEBUG: 	Sun Apr 10 01:56:22 2022 Memory stats beginning consolidation size=483MB resident=379MB share=6MB text=13MB lib=0 data=371MB dt=0; 10 Apr 2022 01:56:22,400 DEBUG: 	Sun Apr 10 01:56:22 2022 Memory stats Start: batch 1/1 size=503MB resident=379MB share=6MB text=13MB lib=0 data=391MB dt=0; 10 Apr 2022 01:59:22,970 DEBUG: 	Sun Apr 10 01:59:22 2022 Memory stats after alloc for attribute=END size=25GB resident=25GB share=7MB text=13MB lib=0 data=25GB dt=0; 11 Apr 2022 03:36:27,065 WARN : 	process exited with non-zero value: 137; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1095228878:838,optimiz,optimizations,838,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1095228878,1,['optimiz'],['optimizations']
Performance,"@nalinigans We have a very large WGS dataset (<2K subjects). We incrementally add to it over time. After each new batch of data is added, we typically run GenotypesGVCFs. We have run GenotypesGVCFs on prior iterations of this GenomicsDB workspace; however, we have never run it on this particular workspace, after the addition of new samples. You can get those files here: . https://prime-seq.ohsu.edu/_webdav/Labs/Bimber/Collaborations/GATK/%40files/Issue7674/. I think I was mistaken above. According to the job logs, we did run GATK GenomicsDBImport v4.2.5.0 when we did our last append. However, prior append operations would have used earlier GATK versions. I believe we have 79 fragments. We rarely do --consolidate, primarily because those jobs essentially never finish. We've had a lot of issues getting GATK/GenomicsDB to run effectively on this sample set. We have settled on doing the GenomicsDBImport/append operation with a moderate batch size. I realize newer GATK/GenomicsDB versions have been addressing performance, and it is possible we should re-evaluate this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1042101804:1020,perform,performance,1020,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1042101804,1,['perform'],['performance']
Performance,"@nalinigans, to answer your questions as best as I can (sorry, I'm a bit of a novice); 1. the size of the book keeping file is: 20,779,823bytes (~21Mb); 2. I believe it is a shared Posix FS; Running this option created a similar error except this time there was: ""cannot load book-keeping: Reading MBR failed"" (output below) ; 3. available memory ~89Gb; 4. I am running -Xmx16g java option. Newest output:; Using GATK jar /data1/_software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx16g -jar /data1/_software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar GenotypeGVCFs --genomicsdb-shared-posixfs-optimizations --reference /data1/EquCab/_ECA30/Equus_caballus.EquCab3.0.dna_sm.toplevel.fa/ -V gendb://ECA3_GenomicsDB_260/3 -O ECA3_GenomicsDB_260.3.g.vcf.gz; 16:26:34.912 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data1/_software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 06, 2021 4:26:35 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 16:26:35.417 INFO GenotypeGVCFs - ------------------------------------------------------------; 16:26:35.418 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.1.8.1; 16:26:35.418 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 16:26:35.420 INFO GenotypeGVCFs - Executing as ccastane9@andersserver-01.cvm.tamu.edu on Linux v3.10.0-1127.19.1.el7.x86_64 amd64; 16:26:35.421 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_275-b01; 16:26:35.421 INFO GenotypeGVCFs - Start Date/Time: January 6, 2021 4:26:34 PM CST; 16:26:35.421 INFO GenotypeGVCFs - ------------------------------------------------------------; 16:26:35",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-755760402:271,load,load,271,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-755760402,3,"['Load', 'load', 'optimiz']","['Loading', 'load', 'optimizations']"
Performance,@nh13 I wrote a test for your branch (its very simple it just reruns the gvcf mode tests with --disable-optimizations enabled) that should work for your branch. Its in the branch je_addTestForDisableOptimizations. Since you submitted this PR from your own clone of the GATK I cannot push this onto the branch as it stands. Would you be able to copy it into this branch?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7125#issuecomment-793077846:104,optimiz,optimizations,104,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7125#issuecomment-793077846,1,['optimiz'],['optimizations']
Performance,"@nvnieuwk, have you tried the following to circumvent the inode limit issue?. - use of the `merge-contigs-into-num-partitions` option with GenomicsDBImport; ```; --merge-contigs-into-num-partitions <Integer>; Number of GenomicsDB arrays to merge input intervals into. Defaults to 0, which disables; this merging. This option can only be used if entire contigs are specified as intervals.; The tool will not split up a contig into multiple arrays, which means the actual number of; partitions may be less than what is specified for this argument. This can improve; performance in the case where the user is trying to import a very large number of contigs; ```; Thanks.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8297#issuecomment-1557405072:564,perform,performance,564,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8297#issuecomment-1557405072,1,['perform'],['performance']
Performance,"@owensgl Sorry you're running into problems. We typically speed up the process by running multiple GenotypeGVCF processes in parallel, subsetting by genomic intervals. GenotypeGVCFs isn't really multicore, you'll probably be best off giving each process 1 or 2 cores. (You'll see better performance with 2 since java has parallel garbage collection, but it might be more cost effective to run twice as many slower processes...) If you do that you'll want to run with `--only-output-calls-starting-in-intervals` enabled in order to avoid problems on the edges of intervals. . Things tend to bog down with many highly multi-allelic sites. If you have a population with very high diversity you may be hitting lots of sites like that. I'm not sure why it's as slow as you say it is though. It should be faster than 800bp / 30 minutes even with old qual. If you could provide a subset of your data we might be able to profile and see if there's some pathological case we're not handling well. I believe new-qual handles multi-allelic sites more efficiently which I suspect is why it's going faster.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4161#issuecomment-358054994:287,perform,performance,287,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4161#issuecomment-358054994,1,['perform'],['performance']
Performance,"@pgrosu Here's how I did it, although this was using the old mapred Hadoop 1.0 API so would be deprecated now. This class adds a bunch of BWA index files to the distributed cache:. https://github.com/cwhelan/cloudbreak/blob/master/src/main/java/edu/ohsu/sonmezsysbio/cloudbreak/command/CommandBWAPairedEnds.java. By calling into the addDistributedCacheFile method here:. https://github.com/cwhelan/cloudbreak/blob/master/src/main/java/edu/ohsu/sonmezsysbio/cloudbreak/command/BaseCloudbreakCommand.java. This mapper class can then just reference them as local files on the executing node:. https://github.com/cwhelan/cloudbreak/blob/master/src/main/java/edu/ohsu/sonmezsysbio/cloudbreak/mapper/BWAPairedEndMapper.java. Feel free to poke around that repo if you find it interesting.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/567#issuecomment-112864194:173,cache,cache,173,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/567#issuecomment-112864194,1,['cache'],['cache']
Performance,"@pgrosu I'm not sure that actually does explain what's happening. If I read it correctly it's saying that some objects were serialized, then the class was changed, and the old saved objects were no longer loadable. . Our current situation is that we serialized some objects, and deserializing them with the exact same class failed. The first situation is expected, the second one should not happen. . Is it possible that we are using different jvms on our local machine vs on the google cluster? So classes are serialized locally and then a jvm dependent hashcode is different at the other end?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107699331:205,load,loadable,205,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107699331,1,['load'],['loadable']
Performance,"@rhysnewell The default settings for the GATK downsampler are optimized for germline calling on typical (ie., Illumina) short reads data. The goal of the downsampler is to control memory usage at dubious sites where the aligner happens to place huge numbers of low-mapping-quality reads, without harming the quality of the variant calls in any meaningful way. By default, if there are more than 50 reads starting at the exact same alignment start position, GATK will start to randomly eliminate reads -- this gives a maximum total coverage for each locus of `50 * read_length`, which is more than sufficient for typical whole-exome or genome germline calling. For other applications, such as amplicon sequencing (where the reads inherently all tend to start at the same positions) or microbial calling, these defaults will not work well, and you are likely better off disabling the built-in downsampler completely, and using a different downsampling approach if memory use is an issue.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7873#issuecomment-1139176006:62,optimiz,optimized,62,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7873#issuecomment-1139176006,1,['optimiz'],['optimized']
Performance,"@rsasch my thought was to keep it around for now, but in the future we could add a step that removes it once we know the samples are safely loaded. It could also be that we get rid of the ""is_loaded' flag in sample_info instead since this data is more detailed…",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7573#issuecomment-983938451:140,load,loaded,140,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7573#issuecomment-983938451,1,['load'],['loaded']
Performance,"@samuelklee DRAGEN STRE model doesn't actually make any alterations to the smith waterman parameters or how they work, it just works by adjusting the indel gap penalties that are used for the PairHMM. At one point we were concerned about SW parameters being different with dragen but as it turns out the biggest visible effect of the SW parameters on the output (the alignment we perform after haplotypes discovery) is irrelevant since they don't realign their reads internally. We kept the default gatk alignment behavior and thus the SW parameters that are used (for dangling head recovery which I believe are the old arguments) still match. As far as unifying the parameters I suspect it could be done though one wonders if there aren't risks where the different contexts in which we use the parameters will not perform as well with a unified set. Speculation on my part though. I agree with David that we should be cautious about making changes that will affect the HaplotypeCaller before November. . I support including an argument in any case (possibly multiple) to include the SW parameters. I would actually advocate we read these files in as tables of parameters where you simply point to on the command line to configure new parameters.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6863#issuecomment-705611993:380,perform,perform,380,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6863#issuecomment-705611993,2,['perform'],['perform']
Performance,"@samuelklee I wrote some benchmarks for the exact combinatorics and you were right, my optimization was pointless. Although the `CombinatoricsUtils` method does explicitly multiply out instead of using cached factorials 1) the number of multiplications is only min(ploidy, (allele count - 1)), and 2) it actually takes quite a while (much larger than reasonable ploidy and allele count) for multiplication to take longer than the memory access of stored factorials. . I have removed this error in judgment.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1066873168:87,optimiz,optimization,87,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1066873168,2,"['cache', 'optimiz']","['cached', 'optimization']"
Performance,"@samuelklee I'd say lets leave 2.1 base image up there for now, and yes on the cache clearing. Once tests pass with the cache cleared it should be good to merge. Feel free to squash and rebase if you like.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5040#issuecomment-408453109:79,cache,cache,79,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5040#issuecomment-408453109,2,['cache'],['cache']
Performance,"@samuelklee It wasn't just a rebase, it was a complete rewrite because the old code had since become completely entangled with DRAGEN code. But I did it! Everything is passing, the code is dramatically simpler, and it's even a bit faster. I have done my best to make a coherent commit history. I would recommend reviewing one commit at a time in side-by-side diff mode. Note that some commits rip out old code and replace it with pseudocode, deferring the new code to a later commit. Other commits tell a story of what all the different caches meant in order to motivate the simpification of later commits. The baroqueness of the old code was motivated by three considerations:; * cache-friendliness -- traversing all arrays by incrementing the innermost index, reads. This is absolutely essential.; * flattening 3D arrays into 1D arrays. This was a premature optimization.; * Precomputing addition operations -- this was misguided. The DRAGEN code relied on these caches in a rather complex way, which fortunately turned out not to be necessary and which could be dramatically simplified. My notes on tracking all the variables from the parent genotype calculator down to the DRAGEN calculator are in this google doc: https://docs.google.com/document/d/1v6s57mUAwfj38nL3VdktjA059kYBkJfokq18IDy79E8/edit?usp=sharing. Good luck and don't hesitate to ask me to explain anything.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1023647476:537,cache,caches,537,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1023647476,4,"['cache', 'optimiz']","['cache-friendliness', 'caches', 'optimization']"
Performance,"@samuelklee My plan for the deprecation was to do it concurrently with (or just before) the merge of this PR. As this gets closer to a final merge, we can coordinate on that aspect.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8561#issuecomment-1939712690:53,concurren,concurrently,53,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8561#issuecomment-1939712690,1,['concurren'],['concurrently']
Performance,"@samuelklee Now it's back to you. I agreed with and implemented all of your suggestions. `GenotypeIndexCalculator`, `GenotypeAlleleCounts`, `GenotypeLikelihoodsCalculator` and `GenotypeLikelihoodsCalculator` (renamed to `GenotypesCache`) now have clearly-defined roles. A lot of premature optimization is gone.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1068695779:289,optimiz,optimization,289,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1068695779,1,['optimiz'],['optimization']
Performance,"@samuelklee Performance was comparable on one exome bam using the 5M sites file. Each took ~15 minutes. Sample size of 1, though.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3203#issuecomment-313156153:12,Perform,Performance,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3203#issuecomment-313156153,1,['Perform'],['Performance']
Performance,"@samuelklee The module can now save and load everything, including the state of the optimizer. This allows to making interesting inference pipelines. Here's a decent strategy for obtaining the global optimum (it works flawlessly on simulated data every time):. - In the first pass, one disables annealing and obtains the variational parameters in a thermal state. The temperature needs to be _high enough_ to allow most/all local minima to merge, though, not too high to allow copy numbers to travel too far away from baseline copy numbers. If this occurs, one must anneal very slowly in the next stage (see below). The results are checkpointed once converged. - In the second pass, one makes another call to the CLI tool, this time w/ annealing enabled (starting from the same temperature) and starting from the checkpointed thermal results (model params, posteriors, adam(ax) state). The annealing rate must be slow enough to prevent thermal fluctuations from getting quenched (i.e. the evolution must be quasi-isothermal). One must look for a steady and linear rise of ELBO, such that when the annealing protocol ends, SNR quickly drops to values below 1. In both runs, the learning rate must be very small (in the rate 0.01-0.05) such that we wouldn't have to worry about controlling stochastic noise. Adam(ax) quickly adjusts its moment estimates and compensates for the small learning rate, so this doesn't increase the training time significantly.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3838#issuecomment-347369020:40,load,load,40,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3838#issuecomment-347369020,2,"['load', 'optimiz']","['load', 'optimizer']"
Performance,"@samuelklee The performance issues were a mirage. It turned out that there was a misconfiguration in the Carrot tests so it was pegging the ""control"" version to an older GATK release from a year ago. There was a slight regression in the past year the we haven't caught but that is a separate discussion.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1551819210:16,perform,performance,16,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1551819210,1,['perform'],['performance']
Performance,"@samuelklee This is a very dangerous/misleading check! Most of the files in your list are actually in use in the test suite, and their presence is inferred from the path to the file(s) they are associated with. For example, vcf index files are loading indirectly by our readers, using the path to the vcf to infer the path to the index.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3905#issuecomment-348591768:244,load,loading,244,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3905#issuecomment-348591768,1,['load'],['loading']
Performance,"@samuelklee Yeah, you did [see this issue there](https://github.com/broadinstitute/gatk/issues/6513#issuecomment-602077546) (as well as the concurrent modification exception).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6649#issuecomment-640866423:140,concurren,concurrent,140,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6649#issuecomment-640866423,1,['concurren'],['concurrent']
Performance,"@schelhorn Sorry for the long wait on this issue, which we do take seriously! Unfortunately as mentioned above we have extremely limited resources for Mutect2 maintenance at the moment due to our focus on Mutect3, which we see as the long-term solution to this and other issues. You should definitely continue to run 4.1.8.0 until this is resolved, if at all possible. Having said that, we are generating a new M2 PON now, and should know soon whether it resolves this issue as @davidbenjamin hopes. Stay tuned for an update on the outcome of this evaluation within the next week or two.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1405524738:505,tune,tuned,505,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1405524738,1,['tune'],['tuned']
Performance,"@schelhorn We have a large clinical ""truth"" set we utilize during workflow validations. We also utilize spike-in samples from SeraCare and perform dilutions using a couple of the common Coriell cell lines. We noticed the Mutect2 calling inconsistency while validating a small targeted panel.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1171672027:139,perform,perform,139,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1171672027,1,['perform'],['perform']
Performance,"@shengqh ; Looking at it, it seems that `test.pbs` is basically a shell script. So at the top of that add:. ```; source activate gatk; ```. Or... whatever it is you need to do to activate the environment. What the `exec` command is going to do is launch whatever is passed in. So... something like:. ```; singularity exec gatk.simg cat /etc/os-release; ```; Would show you the `/etc/os-release` file from within the container. `exec` isn't going to load a shell environment, but launch the application directly. In a `shell` look at: `/.singularity.d/actions/exec`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-433549054:449,load,load,449,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-433549054,1,['load'],['load']
Performance,"@shisheng-1 Hello, thank you for your contribution! . I tried running with and with-out your change enabled and I didn't see any performance improvement when compiling. I didn't do a very scientific test, but running the following a few times on master vs your branch didn't show any performance advantage for using fork = true.; ```; ./gradlew clean compileTestJava; ```. | fork = | false | true |; | --- | ----- | ---- |; | run 1 | 77 | 93 |; | run 2 | 67 | 70 |. From my quick tests it looked like the forking version might actually be a bit slower but I suspect that's just noise. . Do you see different results?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7561#issuecomment-967263911:129,perform,performance,129,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7561#issuecomment-967263911,2,['perform'],['performance']
Performance,"@sooheelee ""Flush to zero"" or FTZ is a CPU flag that causes extremely small floating-point values to be treated as 0. Enabling this flag improves performance and consistency across PairHMM implementations, without causing precision loss of any significance.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1572#issuecomment-422175512:146,perform,performance,146,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1572#issuecomment-422175512,1,['perform'],['performance']
Performance,"@sooheelee Are you sure that smoothing doesn't always converge before we can see different behavior for your test data? How many iterations of smoothing do you get when you set this parameter to 0, and how many do you get when you set it to 1? (It might be helpful to post a condensed version of the stdout.). This parameter is not meant to be a boolean. If it is set to N > 0, then at most N smoothing iterations will be attempted before the next MCMC fit. However, if convergence is reached before N iterations, then the final MCMC fit is performed. If the parameter is set to 0, then no MCMC refits are performed. In both cases, the total number of allowed smoothing iterations is set by a separate parameter, `maximum-number-of-smoothing-iterations`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382781387:541,perform,performed,541,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382781387,2,['perform'],['performed']
Performance,"@sooheelee I found that on FC, it was only necessary to make the File -> String change at the task level (i.e., in CollectCounts). Not sure if this works due to the particular version of Cromwell on FC. In any case, I don't think you should stress too much about getting NIO working on your VM. Again, I'd expect the current non-NIO gCNV WDL to only take a few hours on FC, and then less than that once coverage collection is call cached. No use spending more time getting NIO working than it would save you, after all!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4806#issuecomment-437362263:431,cache,cached,431,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4806#issuecomment-437362263,1,['cache'],['cached']
Performance,"@sooheelee We've decided to delete the old tools. I will issue a PR soon. Hopefully this lightens the load on everyone a bit!. Incidentally, I would be fine with adding the tools `CollectFragmentCounts` and `CollectAllelicCounts` to the `CoverageAnalysis` category, as they are performing relatively generic tasks that fall in that category. (with the caveat that the output formats contain column headers that are specific to the new CNV workflows).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-349145382:102,load,load,102,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-349145382,2,"['load', 'perform']","['load', 'performing']"
Performance,@spatel-gfb Can you try running `GenomicsDBImport` with the `--genomicsdb-shared-posixfs-optimizations` argument and see if that helps? . @nalinigans @mlathara any other suggestions?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7646#issuecomment-1031830101:89,optimiz,optimizations,89,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7646#issuecomment-1031830101,1,['optimiz'],['optimizations']
Performance,"@spatel-gfb another option that should improve performance is to run the GATK GVCF postprocessing tool ReblockGVCF on all of your input GVCFs. That tool merges reference blocks (which BAF calculation doesn't need) and throws out uncalled alleles (which BAF calculation doesn't need), both of which should speed up your import. For the purposes of GATK-SV you can use the default GQ bands ([0,20], and (20, 99]).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7646#issuecomment-1088703926:47,perform,performance,47,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7646#issuecomment-1088703926,1,['perform'],['performance']
Performance,"@spikebike, just started looking at this issue again. We are benchmarking operations with NFS and will put out an optimized library soon. But, GenomicsDB does use filesystem locking to allow for simultaneous reads/writes. - Did you try `export TILEDB_DISABLE_FILE_LOCKING=1` ?; - Are you using the `--consolidate true` option with GenomicsDBImport explicitly? If yes, would it be possible to set that option to false which is also the default.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6487#issuecomment-616657603:114,optimiz,optimized,114,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6487#issuecomment-616657603,1,['optimiz'],['optimized']
Performance,"@stefandiederich We are finalizing some hyperparameter optimizations and will have some results and recommendations to share within ~1 month. You may find some preliminary recommendations for WES in this forum thread: https://gatkforums.broadinstitute.org/gatk/discussion/11711/germlinecnvcaller-interval-merging-rule-error. Once our optimizations are finalized, @vdauwera and her team will put together a tutorial and other workshop materials---stay tuned!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4679#issuecomment-382777741:55,optimiz,optimizations,55,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4679#issuecomment-382777741,3,"['optimiz', 'tune']","['optimizations', 'tuned']"
Performance,"@takeshi-yoshimura One more thing to note. You should be able to use this library with an existing version of gatk by including this in your classpath since filesystem providers should be dynamically loaded. ( something along the lines of`--java-options ""-cp <pathtothisjar>' should work.)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6698#issuecomment-662035091:200,load,loaded,200,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6698#issuecomment-662035091,1,['load'],['loaded']
Performance,"@takutosato had a good suggestion: to stratify to low-complexity regions in the high-confidence regions. Not sure how many variants are there, but will take a look. EDIT: looks like it's ~5k / ~54k on chr22 in CHM. More generally, I think that defining the appropriate loss function for optimization to set ""default"" parameter values obviously has no unique answer. The problem is also made a little more complicated by our current strategy of sensitive calling + non-trivial filtering. But it would be great to come up with some hard constraints (e.g., we never want runtime/cost to exceed X, we always want to maintain Y metrics in these regions on these samples) and general procedures, then apply them as equitably as possible across all method/parameter changes. Also generally, I'm a bit wary of focusing too hard on the high-confidence regions, as this might lead to overfitting or could understate the potential of method/parameter changes in more difficult regions. But probably we'll have to downweight the loss or do more manual checks in low-confidence regions until we improve truth resources there. One naive question, just want to double check: is it correct that the overall scaling of each set of SW parameters is inconsequential? E.g., if I multiply each by a constant, should I expect the same results? I would expect this to be the case (unless my hazy recollection of the details of SW scoring is off) and simple experiments bear this out, but I'm not sure if there are some edge cases or idiosyncrasies in our implementation or use of the scores that I might be missing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-714570055:287,optimiz,optimization,287,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-714570055,1,['optimiz'],['optimization']
Performance,@tedsharpe How's the performance of this new binding? I'm assuming it's better and less broken than the old one. Do you have any numbers or even just anecdotes?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2367#issuecomment-275511690:21,perform,performance,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2367#issuecomment-275511690,1,['perform'],['performance']
Performance,"@tedsharpe I've addressed some of your comments here -- all the simpler stuff plus:. - I now only make distal targets for split reads with one supplementary alignment. We can make a ticket to handle more complex cases at some point.; - I renamed the concept of strand in the `EvidenceTargetLink` and related classes -- I'm now calling it `evidenceUpstreamOfBreakpoint`.; - I canonicalize `EvidenceTargetLinks` and only create them when the source is upstream of the target. This allowed me to get rid of the de-duplication code, so thanks for the suggestion. It seemed tricky to me to try to cluster these links during the initial pass over the reads while at the same time keeping track of coherent evidence. In my testing it doesn't seem like it is slow to run over the `EvidenceRDD` again to do this, but we could think about trying to change this sometime if we're looking for optimizations. . Want to take another look?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3469#issuecomment-328300806:881,optimiz,optimizations,881,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3469#issuecomment-328300806,1,['optimiz'],['optimizations']
Performance,"@tedsharpe So we've addressed a bunch of the issues you've mentioned in the new htsjdk.beta reader apis. In particular we now optimize the decoding checks so that we only scan the beginning of the files exactly once and the give the necessary bytes to all available codecs. That should work in combination with your optimization to push down the filtering which makes a lot of sense as well. Lazily loading the indexes when you need to query for them is a good idea. I think the thought behind aggressively loading them was probably to handle potential errors up front instead of waiting until later, and it's confounded by the bad choice of automatically discovering indexes by default so the engine can't tell if there SHOULD be an index until it tries and fails to load it. We've also addressed that in the new APIs to give the client more control over how indexes are discovered which should allow for cleaner lazy loading and things like that.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8031#issuecomment-1364325181:126,optimiz,optimize,126,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8031#issuecomment-1364325181,6,"['load', 'optimiz']","['load', 'loading', 'optimization', 'optimize']"
Performance,"@tedsharpe Thanks for checking. In general I've seen the CPB tends to help a lot when reading through long contiguous stretches of BAM file and less when doing anything on smaller or fragmented data. I'm surprised it didn't make any difference here, but seems like it doesn't so that's fine. I've seen catastrophic interactions between insufficiently buffered index inputs with it disabled where it ended up performing an http request for every byte, but hopefully that's avoided just by using a buffered reader for it. . I have a plan to someday enable the more intelligent -L aware prefetcher that will use the list of actual positions of interest to buffer more intelligently, but that's not happening on any specific schedule.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8031#issuecomment-1358245320:408,perform,performing,408,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8031#issuecomment-1358245320,1,['perform'],['performing']
Performance,"@tedsharpe This looks good to me. In general partition sizes can be much larger than 100kb without problems, so I suspect it's is something funny to do with the task serialization of ctx.paralellize(). . If this is a performance critical tool it would probably be better to rewrite it in a way that it can load the reference in parallel. Since I assume this is something you run essentially once per reference it may not be worth it. . If you're worried about small machines running out of memory, I would expose the parameter that lets you configure how much memory each partition gets. I would expect in any spark configuration each core will have no less than ~1gb to work with and likely 2 -4 on any machines used for biology work.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1505#issuecomment-187387150:217,perform,performance,217,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1505#issuecomment-187387150,2,"['load', 'perform']","['load', 'performance']"
Performance,"@tedsharpe, I have changed the implementation to use a lazy non-cached recreation of the read mapping information. . I have not added the additional mapping information in your branch like the MQ value. I would do that in a separate commit. . Please take a look and perhaps run your benchmark.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3133#issuecomment-311404369:64,cache,cached,64,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3133#issuecomment-311404369,1,['cache'],['cached']
Performance,@tfenne The gatk3 behavior when using intervals was that it only loaded data that started within the interval. We change that in gatk4 so queries get all overlapping data. This means that for GenoytpeGVCFs at least you should be getting all upstream deletions that overlap. If that's *NOT* the case then it's a bug we should fix. The side effect was getting duplicate calls in adjacent shards which is why we added `--only-output-calls-starting-in-interval`. . The HaplotypeCaller behavior is probably different and I don't know for sure what it is. ![HaplotypeCaller leading deletion behavior](https://user-images.githubusercontent.com/4700332/72105530-b569a080-32fb-11ea-9fc5-1adaee2ac039.png). It seems like there are 3 possible outcomes for a deletion that starts outside the interval in HaplotypeCaller. @davidbenjamin Do you know which we do? James said you were re-writing related code recently. A toggle to change between behavior 1 and 2 would be idea like you're suggesting @tfenne. I'm a bit afraid that we do 3 and emit nothing in these cases though.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6339#issuecomment-572762496:65,load,loaded,65,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6339#issuecomment-572762496,1,['load'],['loaded']
Performance,"@tomwhite #1847 is in, and there's now a `JBWAIntegrationTest` showing how to load and use jbwa in a way that will work on both Linux and Mac. You should be all set -- let me know if there are problems.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1750#issuecomment-220759191:78,load,load,78,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1750#issuecomment-220759191,1,['load'],['load']
Performance,"@tomwhite , #1701 is not a JNI binding (I'm working on providing one), but a really naive way of calling bwa from cmdline. Right now if you want to bring in the ""correct"" branch of BWA, I think messing with @lindenb 's MAKEFILE (bottom lines) and clone only the Apache2 branch would be enough. Per discussion in #1517 , the only difference seem to be `bwa index` performance, which isn't a concern here.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1750#issuecomment-212961920:363,perform,performance,363,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1750#issuecomment-212961920,1,['perform'],['performance']
Performance,@tomwhite From what I saw the hdfs provider wasn't loaded (at least on my local cluster). Only the default filesystem and the jar file system.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2312#issuecomment-268009239:51,load,loaded,51,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2312#issuecomment-268009239,1,['load'],['loaded']
Performance,"@tomwhite I think the general goal of unifying the Spark/non-Spark tool hierarchies is worthwhile, but I don't like the idea of all tools having `if (sparkArgs.useSpark) {} else {}` boilerplate. If we do this, we should have separate abstract methods that get called automatically in the Spark/non-Spark cases. I also think we should wait to perform this refactoring until later in the quarter, after the Spark evaluation, and after we've standardized more on `Path` for inputs/outputs, as it will break a lot of downstream code in gatk-protected -- and we don't want to break all the tools more than once if we can help it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2217#issuecomment-254228946:342,perform,perform,342,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2217#issuecomment-254228946,1,['perform'],['perform']
Performance,"@tomwhite I'm looking into the performance issues now with the new code path -- it brings the output much closer to GATK3, but clearly needs some profiling work. Can you tell me what kind of difference you saw in the runtime on Spark? Eg., was it on the order of 20-30%, or was it worse than that?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3533#issuecomment-330905564:31,perform,performance,31,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3533#issuecomment-330905564,1,['perform'],['performance']
Performance,"@tomwhite If there are a million reads mapped to the same position, my hope was that we could avoid loading them into an RDD at all, if possible (ie., eliminate as they are first read in), rather than load them into an RDD and then downsample. This is why I proposed doing this at the Hadoop-BAM layer. Do you think this is possible?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1644#issuecomment-205861300:100,load,loading,100,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1644#issuecomment-205861300,2,['load'],"['load', 'loading']"
Performance,"@tomwhite Ignore my previous message, sorry -- I see that you commented above that you tested https://github.com/broadinstitute/gatk/pull/4314 and it had no effect. I've opened https://github.com/broadinstitute/gatk/pull/4428 to revert the ADAM upgrade for now until we understand the underlying cause of the performance regression. @fnothaft It seems to me that the serializer registrations in ADAM could in theory affect the GATK, since we both register serializers for core classes in htsjdk. It seems worth investigating as a possibility, at least, as it's the only candidate mentioned so far that seems to have the potential to cause such a massive performance difference.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4376#issuecomment-367069808:309,perform,performance,309,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4376#issuecomment-367069808,2,['perform'],['performance']
Performance,"@tomwhite In your `--files` branch, you might try enabling feature caching on the `FeatureDataSource` (assuming your sequential queries are forward-only, increasing). Without that it will execute a separate index query for each `query(interval)` to find the overlaps. If you pass a non-zero `queryLookaheadBases` value (we usually use 100000) to the FeatureDataSource constructor in `BroadcastJoinReadsWithVariants`, it will use lookahead and the feature cache to satisfy queries - I would expect it to speed it up a lot.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5103#issuecomment-412917472:455,cache,cache,455,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5103#issuecomment-412917472,1,['cache'],['cache']
Performance,"@tomwhite Like we discussed this morning, we can and should get rid of the broadcast code but we should ideally first get some sort of plot we can point to in order to justify the change. This will also be useful for future presentations of our performance improvements. The plot would ideally be a comparison between the new distribution strategy and broadcasting compared across a variable number of cores, so the performance improvement can be better understood.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5127#issuecomment-416327226:245,perform,performance,245,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5127#issuecomment-416327226,2,['perform'],['performance']
Performance,"@tomwhite OOC, do you have any comment as to why bumping the ADAM version caused a performance regression? I'm not aware of any changes we've made in ADAM that would have impacted either BQSR or HC.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4376#issuecomment-366072491:83,perform,performance,83,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4376#issuecomment-366072491,1,['perform'],['performance']
Performance,"@tomwhite On the GATK side, I think the steps are:; 1. Rev our Hadoop-BAM dependency once https://github.com/HadoopGenomics/Hadoop-BAM/pull/49 is in.; 2. Take one of our existing test inputs and create a copy of it sorted by query name using the hellbender tool `SortSam`. I think a good file to use would be `src/test/resources/org/broadinstitute/hellbender/tools/BQSR/CEUTrio.HiSeq.WGS.b37.ch20.1m-1m1k.NA12878.noMD.noBQSR.bam`, as that file hasn't had either MD or BQSR run on it, and it's the primary input in the `ReadsPipelineSparkIntegrationTest`.; 3. Before making any actual changes to MarkDuplicatesSpark, write a new integration test proving that MarkDuplicatesSpark can read in the queryname-sorted bam above, and produces the same result as when run on the coordinate-sorted version of the bam. The output in both cases when running with `--shardedOutput false` should be a coordinate-sorted bam.; 4. Now the potentially tricky part: in `MarkDuplicatesSpark`, we want to detect queryname-sorted reads using the `SAMFileHeader` SO attribute (`SAMFileHeader.getSortOrder()`), and if we have them, avoid performing the first `groupByKey()` operation in `MarkDuplicatesSparkUtils.transformReads()`, instead relying on the natural ordering of the reads to create the `PairedEnds` objects.; 5. Make sure that the test you wrote in step 3 doesn't break after doing step 4 :)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1411#issuecomment-170043162:1114,perform,performing,1114,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1411#issuecomment-170043162,1,['perform'],['performing']
Performance,"@tomwhite Please review, and confirm that this resolves the performance issues you encountered.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4428#issuecomment-367067878:60,perform,performance,60,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4428#issuecomment-367067878,1,['perform'],['performance']
Performance,"@tomwhite That's interesting -- @lbergelson 's profiling on `HaplotypeCallerSpark` in isolation did not show any noticeable slowdown as a result of the switch to the new code path (in addition to the output being much more correct than before), and the walker version which uses the same codepath is as fast or faster than GATK3 at this point in all modes. We should try to see whether we can replicate your results on our end. Can you provide more details on how you ran the tools?. The new `HaplotypeCaller` code path does result in the tool doing more work in some cases as compared to the early betas, particularly for genomic locations not covered by any reads -- but this work is necessary in order to produce correct output without boundary effects, and the fact that the early betas didn't do it was an oversight. I wonder if that could explain what you're seeing -- see the full discussion in https://github.com/broadinstitute/gatk/issues/4169. There are also several ways in which `HaplotypeCallerSpark` currently makes suboptimal use of the new code path introduced in https://github.com/broadinstitute/gatk/pull/4278:. 1. It uses only a single interval per shard, instead of many intervals as the walker version does (https://github.com/broadinstitute/gatk/issues/4299). 2. It materializes all of the assembly regions in a shard at once (https://github.com/broadinstitute/gatk/issues/4301), as opposed to the walker version which materializes the regions in a shard as lazily as possible. 3. The default read shard size may need adjusting (https://github.com/broadinstitute/gatk/issues/4298). The walker version creates one shard per contig, and lazily creates the assembly regions within that shard. That's obviously not possible for the Spark version, but I don't think that the effect of the shard size on performance has been measured yet for the Spark version.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4376#issuecomment-364179033:1821,perform,performance,1821,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4376#issuecomment-364179033,1,['perform'],['performance']
Performance,@tomwhite This seems like an easy way to get a big performance boost in `HaplotypeCallerSpark` -- what do you think?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4296#issuecomment-363201577:51,perform,performance,51,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4296#issuecomment-363201577,1,['perform'],['performance']
Performance,"@tomwhite To clarify, I think that the caller of `ensureCapacity()`, namely `GenotypeLikelihoodCalculators.calculateGenotypeCountUsingTables()`, also needs to be synchronized in order to avoid some unlikely but still-possible races. Given this, I think that we should consider whether `ThreadLocal` might be a better option here. It's not 100% clear to me whether a `ThreadLocal` `get()` call is cheaper than a synchronized method call, but some casual googling suggests that it might be. If we're going to end up entering a synchronized method on every single call to `GenotypeLikelihoodCalculators.getInstance()`, we might want to do some research into whether `ThreadLocal` + no synchronization would be faster, since I believe that this is a performance-sensitive section of code.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5071#issuecomment-422171244:746,perform,performance-sensitive,746,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5071#issuecomment-422171244,1,['perform'],['performance-sensitive']
Performance,"@tomwhite Well, the idea is to downsample **before** we pay the cost of holding all the reads in memory -- with `sample` I think you'd be downsampling after we've paid that cost. The `ReservoirDownsampler` in GATK4 is able to perform a fair elimination on a stream of items as they come in, without having to store all the items in memory at once, and guaranteeing that every item has an equal probability of ending up in the final set. Also, I think we might want to downsample on a per-alignment-start basis, rather than per-spark-partition. We want protection against the situation where mapping artifacts have caused an insane number of reads to be aligned to the same position, without eliminating coverage at positions that do not pose a problem for us.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1644#issuecomment-203986193:226,perform,perform,226,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1644#issuecomment-203986193,1,['perform'],['perform']
Performance,"@tomwhite When you get a chance, could you please test this branch to check whether the performance regression you reported earlier is resolved?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4584#issuecomment-376283397:88,perform,performance,88,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4584#issuecomment-376283397,1,['perform'],['performance']
Performance,@tomwhite and/or @laserson Could we please get your expert opinions as to whether it's safe to remove the protective copying operations from the spark BQSR like this? (these were necessary in dataflow due to sibling fusion optimization).,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/925#issuecomment-143236364:223,optimiz,optimization,223,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/925#issuecomment-143236364,1,['optimiz'],['optimization']
Performance,@tomwhite and/or @laserson should have a look at this and give JP high-level feedback on his approach to optimization.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/987#issuecomment-146905889:105,optimiz,optimization,105,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/987#issuecomment-146905889,1,['optimiz'],['optimization']
Performance,"@tomwhite before we close, how does this improved performance compare to non-spark writing of the same file (just use PrintReads vs PrintReadsSpark or something)?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1015#issuecomment-158954274:50,perform,performance,50,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1015#issuecomment-158954274,1,['perform'],['performance']
Performance,@tomwhite is this ticket related [Get Hadoop reference loading code into htsjdk](https://github.com/broadinstitute/gatk/issues/831) ?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1326#issuecomment-165812238:55,load,loading,55,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1326#issuecomment-165812238,1,['load'],['loading']
Performance,"@ury the performance evaluation is based on the HCC1395 benchmark, as described in my original report (see above). More variants in newer versions is in line with our analysis, but the new variants are likely to be mostly false positives.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1407452410:9,perform,performance,9,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1407452410,1,['perform'],['performance']
Performance,"@vdauwera JS performance is highly dependent, as you can imagine, on client hardware, browser version, what else is running on the machine, etc. It's also dependent on the type of plot - if the plot involves animation, shadows, etc. All that said, I'd put the start-to-worry line in the thousands, not hundreds. YMMV.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/248#issuecomment-77364832:13,perform,performance,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/248#issuecomment-77364832,1,['perform'],['performance']
Performance,"@vdauwera The tools are categorized and listed in the Google Spreadsheet above. It is waiting for you to assign tech leads to tools for documentation. One thing that @chandrans brought to my attention is that for BaseRecalibrator one of the parameters (`-bqsr`) actually causes an error. One can no longer generate the 2nd recalibration table with correction on the fly and instead must use the recalibrated BAM through BaseRecalibrator to generate the 2nd recal table for plotting. **This type of information is missing from the tool docs.** Furthermore, updates I made to the BQSR slidedeck (that showcase this `-bqsr` parameter) are based on information from a developer and this information turns out to be incorrect now (perhaps correct at some point in development?). Soooo, I think it may be prudent that those responsible for tool docs test the commands on data. - [4] Make sure the doc content enables Best Practices, e.g. plotting BQSR recalibration, and ; - [5] Test example commands to ensure they work. If they do not, make corrections and notate the change in application in the documentation.; - [6] Remember @vdauwera's plan to change the representation of parameters from camel to KEBAB case. Issue is <https://github.com/broadinstitute/gatk/issues/2596>. Geraldine would like your help to do this for the tools you are responsible for. Remember to change the integration tests too. . ### What the gatkDocs look like as of commit of `Mon Nov 20 17:30:46 2017 -0500` where we upgraded htsjdk to 2.13.1: . # [gatkdoc.zip](https://github.com/broadinstitute/gatk/files/1492593/gatkdoc.zip). Download and load the `index.html` into a web browser to click through the docs.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-346060583:1617,load,load,1617,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-346060583,1,['load'],['load']
Performance,"@vruano @mwalker174 Any estimates on cost that could help determine the priority of issue 1? Specifically, is the disk cost required to localize the entire count file for all samples a determining factor? If we can drastically reduce this cost, then we can dedicate more to increasing resolution, etc. Here is a minimal set of fixes that could enable the querying of intervals for GermlineCNVCaller (and also for DetermineGermlineContigPloidy without too much extra work, since we also subset intervals there) *only in the gCNV WGS pipeline*, without disrupting other interfaces:. 1) Write a Tribble SimpleCountCodec for the `counts.tsv` extension. I've already done this in a branch.; 2) Change GermlineCNVCaller and DetermineGermlineContigPloidy tools to accept paths.; 3) If an index is present for each count path, create a FeatureDataSource, merge the requested -L/-XL intervals, and query to perform the subset. We will also need to stream the SAM header metadata. It should not require much code to extract all this to a temporary IndexedSimpleCountCollection class. (Caveat: for now, this will work with the current gCNV convention of providing bins via -L/-XL. Technically, it will also work with the more conventional use of -L/-XL to denote contiguous regions, but we may have to perform checks that bins are not duplicated in adjacent shards if they overlap both, since querying a FeatureDataSource will return any bins that overlap the interval---rather than only those that are completely contained within it.); 4) Index read-count TSVs in the gCNV WGS pipeline after collection and modify the DetermineGermlineContigPloidy and GermlineCNVCaller tasks to take read-count paths and indices, if necessary. These changes could be confined in the gCNV WGS WDL for now. I think that should do the trick. If this is high priority, I can implement now. In the future, we might be able to promote all Locatable CNV Records to Features and write code to automatically pass the columns/encoders/de",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5716#issuecomment-468360082:898,perform,perform,898,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5716#issuecomment-468360082,1,['perform'],['perform']
Performance,"@vruano The HGDP crams also trigger this error. . ftp:/­/­ftp.­1000genomes.­ebi.­ac.­uk/­vol1/­ftp/­data_collections/­HGDP/­data/­Brahui/­HGDP00001/­alignment/­HGDP00001.­alt_bwamem_GRCh38DH.­20181023.­Brahui.­cram. ```; 14:32:34.745 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Apr 17, 2021 2:32:34 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 14:32:34.877 INFO CalibrateDragstrModel - ------------------------------------------------------------; 14:32:34.877 INFO CalibrateDragstrModel - The Genome Analysis Toolkit (GATK) v4.2.0.0; 14:32:34.877 INFO CalibrateDragstrModel - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:32:34.877 INFO CalibrateDragstrModel - Executing as farrell@scc-gh3.scc.bu.edu on Linux v3.10.0-1160.15.2.el7.x86_64 amd64; 14:32:34.877 INFO CalibrateDragstrModel - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_172-b11; 14:32:34.877 INFO CalibrateDragstrModel - Start Date/Time: April 17, 2021 2:32:34 PM EDT; 14:32:34.878 INFO CalibrateDragstrModel - ------------------------------------------------------------; 14:32:34.878 INFO CalibrateDragstrModel - ------------------------------------------------------------; 14:32:34.878 INFO CalibrateDragstrModel - HTSJDK Version: 2.24.0; 14:32:34.878 INFO CalibrateDragstrModel - Picard Version: 2.25.0; 14:32:34.878 INFO CalibrateDragstrModel - Built for Spark Version: 2.4.5; 14:32:34.878 INFO CalibrateDragstrModel - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 14:32:34.878 INFO CalibrateDragstrModel - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 14:32:34.878 INFO CalibrateDragstrModel - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 14:32:34.878 INFO CalibrateDragstrModel - HTSJDK Defaults.USE_ASYNC_IO",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7182#issuecomment-821876394:261,Load,Loading,261,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7182#issuecomment-821876394,1,['Load'],['Loading']
Performance,@wujh2017 Great! Let us know if you have any more feedback. Please be aware that both DetermineGermlineContigPloidy and GermlineCNVCaller are still in beta. There are some parameters that may need to be tuned appropriately for your data. We are currently running evaluations and will release some recommendations that we find suitable for data generated at the Broad.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4457#issuecomment-369254401:203,tune,tuned,203,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4457#issuecomment-369254401,1,['tune'],['tuned']
Performance,"@wujh2017 thank you for trying out this beta tool! As @samuelklee mentioned above, your issue stems from not having the proper python conda env. I guess you have a python 2.x interpreter in your system environment which fails to parse GATK's python code (which requires python 3.6+). Please either follow the instructions in the README.md file, or use the official Docker image instead. Also, I would like to add that the default parameters for running the Germline CNV calling pipeline are currently being fine-tuned separately for WES and WGS data. The default parameters shipped with GATK 4.0.0.0 are preliminary and are not expected to yield optimal results.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4389#issuecomment-365117170:512,tune,tuned,512,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4389#issuecomment-365117170,1,['tune'],['tuned']
Performance,"@yfarjoun I've built docs from your branch to see what the changes look like. Here's the PicardDoc bundle I've zipped for you: [picarddoc_yossi_test.zip](https://github.com/broadinstitute/gatk/files/1535680/picarddoc_yossi_test.zip). View the rest by loading `index.html` into a browser. Here is one of the tool docs where you can see your changes:; <img width=""1099"" alt=""screenshot 2017-12-06 10 18 34"" src=""https://user-images.githubusercontent.com/11543866/33668948-1c286b90-da6f-11e7-91d6-3b368375cbab.png"">. ---; # Steps to build docs to see what changes look like:; [1] Download and switch to branch with changes, e.g. ; ```; git branch yf_documentation_update origin/yf_documentation_update; git checkout yf_documentation_update; ```; [2] Generate the docs. GATK and Picard do this differently:; ```; ./gradlew clean gatkDoc; ```; or; ```; ./gradlew clean picardDoc; ```. [3] Open `index.html` from a browser. Again, GATK and Picard have different locations for their respective docs:; > build/docs/gatkdoc; > build/docs/picarddoc",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-349675765:251,load,loading,251,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-349675765,1,['load'],['loading']
Performance,"@yfarjoun for small arrays, `FastMath.log` is pretty much as fast as it gets. It is highly optimized. If you are dealing with giant arrays, however, an alternative is to cast the Java arrays into [NDArray](www.nd4j.org) and execute Nd4j native ops. Here's how the timings look:; ```N = 1, Apache = 0.001625 ms, Nd4j = 0.113038 ms; N = 10, Apache = 0.002112 ms, Nd4j = 0.029833 ms; N = 100, Apache = 0.011660 ms, Nd4j = 0.028464 ms; N = 1000, Apache = 0.049915 ms, Nd4j = 0.052455 ms; N = 10000, Apache = 0.348786 ms, Nd4j = 0.430606 ms; N = 100000, Apache = 3.572483 ms, Nd4j = 1.810641 ms; N = 10000000, Apache = 323.021844 ms, Nd4j = 175.421305 ms; ```; The nd4j times include the overhead of creating NDArrays and pulling back the results to JVM heap. The break even point is around N ~ 1000. If accuracy is not a concern, (1) a native implementation of log using half-precision floats or (2) caching, tabulation, and linear interpolation could help. ps> I just realized that the Nd4j call was using 4 threads. if you replace for loops in Java with parallel streams in Java with the same number of threads, Apache always beats Nd4j in this specific test:; ```N = 1, Apache = 0.073910 ms, Nd4j = 0.122488 ms; N = 10, Apache = 0.086508 ms, Nd4j = 0.056924 ms; N = 100, Apache = 0.067022 ms, Nd4j = 0.051674 ms; N = 1000, Apache = 0.081751 ms, Nd4j = 0.075098 ms; N = 10000, Apache = 0.202142 ms, Nd4j = 0.514030 ms; N = 100000, Apache = 1.190085 ms, Nd4j = 1.990945 ms; N = 10000000, Apache = 96.536308 ms, Nd4j = 210.331251 ms; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2577#issuecomment-292602609:91,optimiz,optimized,91,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2577#issuecomment-292602609,1,['optimiz'],['optimized']
Performance,@yfarjoun this is a memory optimization but I have confirmed it uses less memory and runs at the same speed,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3445#issuecomment-323204475:27,optimiz,optimization,27,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3445#issuecomment-323204475,1,['optimiz'],['optimization']
Performance,A couple more minor comments. I will take your word that your changes improve performance!,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4846#issuecomment-414776060:78,perform,performance,78,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4846#issuecomment-414776060,1,['perform'],['performance']
Performance,"A couple of thoughts after doing a little more reading on this. Depending on the source it would appear that each arena will allocate either 64MB or 128MB of virtual memory (i.e. address space). So it would probably also be fine to set this limit a bit higher. Secondly, while there's lots of discussion online that setting this doesn't negatively impact Java code running on the JVM, it is possible that native code invoked using JNI could see a modest reduction in performance _if_ a) it's highly multithreaded and b) it's doing lots of heap allocations. I don't know enough about the native pair-HMM and other native code, but it would be helpful if someone who knows more about that could weigh in.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5849#issuecomment-478574401:467,perform,performance,467,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5849#issuecomment-478574401,1,['perform'],['performance']
Performance,"A few minor issues:. - [x] Change `--resource <blah>` to `--resource:<blah>` in tool-level documentation. EDIT: Added to the sl_lite_overlap branch mentioned below.; - [x] The VCF writer in VariantRecalibrator has a few conditionals to allow for VCF headers without contig lines, we could do the same for the writer in LabeledVariantAnnotationsWalker. EDIT: Added to the sl_lite_overlap branch mentioned below.; - [ ] Double check whether we should worry about any differences in extraction on test data (provided via email) from https://gatk.broadinstitute.org/hc/en-us/community/posts/7974912707099-VariantRecalibrator-IndexOutOfBoundsException. Probably nothing to worry about, and at least the error messaging in the new tools is more informative.; - [x] We could change the strategy for checking for resource overlaps to require allele-level matching (rather than only matching on start position, as was inherited from VQSR). A quick test on malaria shows that this can reduce the number of overlaps by O(10%), but performance doesn't really change too much. Branch is already open at https://github.com/broadinstitute/gatk/tree/sl_lite_overlap; - [ ] Expand the exact-match tests to cover some of these strategies, which were added separately in #8049 and merged to make a release deadline.; - [x] Catch the exception in https://github.com/broadinstitute/gatk/blob/fd782504d18b56dbc266c2b3bb4eb32f21916776/src/main/java/org/broadinstitute/hellbender/tools/walkers/vqsr/scalable/LabeledVariantAnnotationsWalker.java#L389 and throw the same message that is thrown in AS mode. Added in #8074.; - [x] Add message to the score tool that the scores HDF5 file will not be out when the input VCF is empty (such a message is already emitted about the annotations HDF5 file). Added in #8074.; - [ ] Megan suggested in the review of #8074 that dynamic disk sizing could be added to the WDL.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7724#issuecomment-1222787946:1020,perform,performance,1020,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7724#issuecomment-1222787946,2,"['perform', 'scalab']","['performance', 'scalable']"
Performance,"A happy side effect is that it makes it possible for me to run it locally -; ReadsSparkSink currently only seems to work when run on a cluster (I am; sure this can be fixed though). That said, I have not had the luxury of time to optimize this whole thing,; so someone will get all the fun of comparing the two approaches,; identifying the bottlenecks, and melding the two together in the best way; possible. My goal is merely to make this algorithm available quickly so we; can see if it can be as helpful here as it has been on the Dataflow side. On Mon, Oct 12, 2015 at 7:37 AM, Tom White notifications@github.com wrote:. > At a high-level, the approach here seems to be to create shards (of size 1; > million bps) and load the reads, variants and references for each shard in; > memory. Each shard then has a recalibration table created for it, then the; > tables are merged into one.; > ; > The existing version finds the variants for each read, and has two; > shuffles (this is for the reference broadcast approach, there's an extra; > one for the reference shuffle approach). The first is to join the variants; > and reads together, and a second to aggregate the variants for each read.; > The optimization in this PR has no shuffles, since it loads all variants; > into memory rather than doing distributed joins. Is that a valid; > assumption? If so, it would be possible to load the variants into memory in; > the driver and broadcast to all workers to remove the shuffles (in the; > existing implementation).; > ; > I noticed that AddContextDataToReadSpark#subdivideAndFillReads opens the; > BAM itself, rather than using ReadsSparkSink. This won't work well in the; > Hadoop case since you lose locality - i.e. the work won't be scheduled on; > the node where the BAM files are stored. It would be much better to find a; > way of using ReadsSparkSink.; > ; > —; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/gatk/pull/987#issuecomment-147415253.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/987#issuecomment-147476006:2494,optimiz,optimization,2494,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/987#issuecomment-147476006,3,"['load', 'optimiz']","['load', 'loads', 'optimization']"
Performance,"A quick look at the code results in the following finding:. The error message : `""SA-BWT inconsistency: seq_len is not the same.""` is generated in function `bwt_restore_sa()` defined in `bwt.c`, and resulted in an `abort()` call.; `bwt_restore_sa()` itself was called in `bwa_idx_load_bwt()` defined in `bwa.c`, when trying to restore the suffix array from a file with extension "".sa"". This function call is issued when bwa mem (in its main) tries to load the reference information. This happens before input are read. Because of the `abort()` call, letting the `BwaMem` class handle the error is difficult, so I would propose two possible solutions:; 1. changing the behavior of `BwaIndex` class, so that it eagerly loads the reference, and throws an `Java.lang.IOException` when this error is encountered. Of course this must lead to code duplication, i.e. copying a large part of index loading code from bwa itself and walk around`abort()`ing.; 2. ask @lh3 to change the behavior so that it will not `abort()` any more, but return a null pointer. But the null pointer return error covers so many error cases that this might not be a good idea.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2123#issuecomment-243181189:451,load,load,451,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2123#issuecomment-243181189,3,['load'],"['load', 'loading', 'loads']"
Performance,"ACKTRACE_ON_USER_EXCEPTION=true --jar /Users/markw/IdeaProjects/gatk/build/libs/gatk-package-4.alpha.2-157-g7d7c5ec-SNAPSHOT-spark.jar -- PrintReadsSpark -I gs://mw-pathseq-test/hs37d5cs.reads.sorted.bam -O hs37d5cs.reads.txt --apiKey XXXXXXXXXXXXXXXXXXXXX --verbosity DEBUG --sparkMaster yarn; Copying file:///Users/markw/IdeaProjects/gatk/build/libs/gatk-package-4.alpha.2-157-g7d7c5ec-SNAPSHOT-spark.jar [Content-Type=application/java-archive]...; - [1 files][ 95.3 MiB/ 95.3 MiB] 9.0 MiB/s; Operation completed over 1 objects/95.3 MiB.; Job [5b3d4225-0547-4aa9-8a83-ab26460aa2d2] submitted.; Waiting for job output...; 21:42:45.768 INFO IntelGKLUtils - Trying to load Intel GKL library from:; 	jar:file:/tmp/5b3d4225-0547-4aa9-8a83-ab26460aa2d2/gatk-package-4.alpha.2-157-g7d7c5ec-SNAPSHOT-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 21:42:45.791 DEBUG IntelGKLUtils - Extracted Intel GKL to /tmp/root/libgkl_compression6493251482684327282.so. 21:42:45.792 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [February 6, 2017 9:42:45 PM UTC] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark --output hs37d5cs.reads.txt --input gs://mw-pathseq-test/hs37d5cs.reads.sorted.bam --apiKey XXXXXXXXXXXXXXXX --sparkMaster yarn --verbosity DEBUG --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --help false --version false --QUIET false --use_jdk_deflater false --disableAllReadFilters false; [February 6, 2017 9:42:45 PM UTC] Executing as root@mw-test-m on Linux 3.16.0-4-amd64 amd64; OpenJDK 64-Bit Server VM 1.8.0_111-8u111-b14-2~bpo8+1-b14; Version: Version:4.alpha.2-157-g7d7c5ec-SNAPSHOT; 21:42:45.795 INFO PrintReadsSpark - Defaults.BUFFER_SIZE : 131072; 21:42:45.795 INFO PrintReadsSpark - Defaults.COMPRESSION_LEVEL : 1; 21:42:45.795 INFO PrintReadsSpark - Defaults.CREATE_INDEX : false; ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2394#issuecomment-277823929:2645,load,loaded,2645,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2394#issuecomment-277823929,1,['load'],['loaded']
Performance,"AD_FOR_SAMTOOLS : false; 15:44:02.752 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 15:44:02.752 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 15:44:02.752 INFO GenomicsDBImport - Deflater: IntelDeflater; 15:44:02.752 INFO GenomicsDBImport - Inflater: IntelInflater; 15:44:02.752 INFO GenomicsDBImport - GCS max retries/reopens: 20; 15:44:02.752 INFO GenomicsDBImport - Requester pays: disabled; 15:44:02.752 INFO GenomicsDBImport - Initializing engine; 15:44:07.262 INFO FeatureManager - Using codec BEDCodec to read file file:///home/akansha/vivekruhela/gatk_bundle/hglift_genome1.bed; 15:44:07.274 INFO IntervalArgumentCollection - Processing 2759468497 bp from intervals; 15:44:07.276 WARN GenomicsDBImport - A large number of intervals were specified. Using more than 100 intervals in a single import is not recommended and can cause performance to suffer. If GVCF data only exists within those intervals, performance can be improved by aggregating intervals with the merge-input-intervals argument.; 15:44:07.307 INFO GenomicsDBImport - Done initializing engine; 15:44:07.591 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.3.2-e18fa63; 15:44:07.592 INFO GenomicsDBImport - Vid Map JSON file will be written to /home/akansha/vivekruhela/pon_db/vidmap.json; 15:44:07.592 INFO GenomicsDBImport - Callset Map JSON file will be written to /home/akansha/vivekruhela/pon_db/callset.json; 15:44:07.592 INFO GenomicsDBImport - Complete VCF Header will be written to /home/akansha/vivekruhela/pon_db/vcfheader.vcf; 15:44:07.592 INFO GenomicsDBImport - Importing to workspace - /home/akansha/vivekruhela/pon_db; 15:44:07.592 WARN GenomicsDBImport - GenomicsDBImport cannot use multiple VCF reader threads for initialization when the number of intervals is greater than 1. Falling back to serial VCF reader initialization.; 15:44:07.592 INFO ProgressMeter - Starting traversal; 15:44:07.593 INFO ProgressMeter - Current L",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7037#issuecomment-761558811:2610,perform,performance,2610,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7037#issuecomment-761558811,1,['perform'],['performance']
Performance,"AD_FOR_SAMTOOLS : false; 22:42:22.413 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 22:42:22.413 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 22:42:22.413 INFO HaplotypeCaller - Deflater: IntelDeflater; 22:42:22.413 INFO HaplotypeCaller - Inflater: IntelInflater; 22:42:22.413 INFO HaplotypeCaller - GCS max retries/reopens: 20; 22:42:22.413 INFO HaplotypeCaller - Requester pays: disabled; 22:42:22.413 INFO HaplotypeCaller - Initializing engine; 22:42:22.705 INFO IntervalArgumentCollection - Processing 2001 bp from intervals; 22:42:22.710 INFO HaplotypeCaller - Done initializing engine; 22:42:22.712 INFO HaplotypeCallerEngine - Tool is in reference confidence mode and the annotation, the following changes will be made to any specified annotations: 'StrandBiasBySample' will be enabled. 'ChromosomeCounts', 'FisherStrand', 'StrandOddsRatio' and 'QualByDepth' annotations have been disabled; 22:42:22.719 INFO NativeLibraryLoader - Loading libgkl_utils.dylib from jar:file:/Users/nhomer/miniconda3/envs/bfx/share/gatk4-4.1.8.1-0/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.dylib; 22:42:22.720 INFO NativeLibraryLoader - Loading libgkl_smithwaterman.dylib from jar:file:/Users/nhomer/miniconda3/envs/bfx/share/gatk4-4.1.8.1-0/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_smithwaterman.dylib; 22:42:22.722 INFO SmithWatermanAligner - Using AVX accelerated SmithWaterman implementation; 22:42:22.724 INFO HaplotypeCallerEngine - Standard Emitting and Calling confidence set to 0.0 for reference-model confidence output; 22:42:22.724 INFO HaplotypeCallerEngine - All sites annotated with PLs forced to true for reference-model confidence output; 22:42:22.734 WARN NativeLibraryLoader - Unable to find native library: native/libgkl_pairhmm_omp.dylib; 22:42:22.734 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; 22:42:22.734 INFO NativeLibrar",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6733#issuecomment-667631737:2641,Load,Loading,2641,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6733#issuecomment-667631737,1,['Load'],['Loading']
Performance,"According to this paper https://www.nature.com/articles/s41467-018-03590-5. it is: ""Each resulting qualified captured library with the SureSelect Human; All Exon kit (Aglient) was then loaded on *BGISEQ-5000 *sequencing; platforms, and we performed high-throughput sequencing for each captured; library. High-quality reads were aligned to the human reference genome; (GRCh37) using the Burrows-Wheeler Aligner (BWA v0.7.15) software. All; genomic variations, including single-nucleotide polymorphisms and InDels; were detected by *HaplotypeCaller of GATK *(v3.0.0).; "". On Wed, Dec 12, 2018 at 3:18 PM Louis Bergelson <notifications@github.com>; wrote:. > @yfarjoun <https://github.com/yfarjoun> Do you know if BGI's sequencing; > is compatible with our tools without any special treatment?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/5517#issuecomment-446729153>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACnk0mujY7YzxUJ-6IPU8B7jPiZWQuzMks5u4WR3gaJpZM4ZQNxZ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5517#issuecomment-446769514:185,load,loaded,185,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5517#issuecomment-446769514,3,"['load', 'perform', 'throughput']","['loaded', 'performed', 'throughput']"
Performance,"Actually, if you have the time, an even more valuable test would be to repeat your comparison with latest master vs. a rebased copy of this branch onto latest master. That would tell us whether the performance difference you saw is due to the downsampling, or due to the differences in the traversal code.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3238#issuecomment-330921307:198,perform,performance,198,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3238#issuecomment-330921307,1,['perform'],['performance']
Performance,"Adding the following helps debug where snappy is being loaded from. ```; --driver-java-options -verbose:class; ```. And gives. ```; [Loaded org.xerial.snappy.SnappyNativeLoader from __JVM_DefineClass__]; [Loaded org.xerial.snappy.SnappyNativeAPI from __JVM_DefineClass__]; [Loaded org.xerial.snappy.SnappyNative from __JVM_DefineClass__]; [Loaded org.xerial.snappy.SnappyErrorCode from __JVM_DefineClass__]; [Loaded org.xerial.snappy.OSInfo from file:/opt/cloudera/parcels/CDH-5.7.0-1.cdh5.7.0.p0.45/jars/snappy-java-1.0.4.1.jar]; ...; [Loaded org.xerial.snappy.SnappyNativeAPI from file:/opt/cloudera/parcels/CDH-5.7.0-1.cdh5.7.0.p0.45/jars/snappy-java-1.0.4.1.jar]; [Loaded org.xerial.snappy.SnappyNative from file:/opt/cloudera/parcels/CDH-5.7.0-1.cdh5.7.0.p0.45/jars/snappy-java-1.0.4.1.jar] ; ```. If I add a later version of snappy as an extra JAR on the command line using. ```; --conf spark.driver.extraClassPath=/home/tom/workspace/gatk/build/install/gatk/lib/snappy-java-1.1.2.jar; ```. then it works. And I only get one `Loaded org.xerial.snappy.SnappyNative` and the one from `__JVM_DefineClass__` doesn't appear:. ```; [Loaded org.xerial.snappy.SnappyNative from file:/home/tom/workspace/gatk/build/install/gatk/lib/snappy-java-1.1.2.jar]; ```. If I try to add snappy-java-1.1.2.jar as a dependency and include it in the spark jar then it doesn't work and it's loaded from snappy-java-1.0.4.1.jar, despite `spark.driver.userClassPathFirst` being set to `true`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1873#issuecomment-229319855:55,load,loaded,55,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1873#issuecomment-229319855,11,"['Load', 'load']","['Loaded', 'loaded']"
Performance,"Additionally. You may wish to compare your variant depths and allelic balances to those captured in gnomAD 4.1 ; [Variant page here](https://gnomad.broadinstitute.org/variant/15-93002203-G-GA?dataset=gnomad_r4); Looking at the balances of heterozygous calls your sites don't seem to display any inconsistencies compared to those available in gnomAD sample set. . Artificial Haplotypes are produced by HaplotypeCaller are available only for debugging purposes. Bamout also includes those informative reads used to produce these haplotypes to provide additional debugging support. Soft clipped bases, realignment, overlapping pairs, duplicates and reads that are available to HaplotypeCaller due to interval restrictions all affect the way you observe these bamouts. . HaplotypeCaller by design is not 100% deterministic in performing its calls. But this does not mean it is inconsistent in making calls when provided all the available reads and reference for its function. That is why we don't recommend restricting HaplotypeCaller when making variant calls with short intervals.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8959#issuecomment-2304607950:822,perform,performing,822,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8959#issuecomment-2304607950,1,['perform'],['performing']
Performance,"After more experimentation, one issue I was running into with the ApproxKernSeg method was failure on small and ""epidemic"" events. This is because 1) the segment cost function used in that paper is extensive (growing with the number of points in a segment), and 2) binary segmentation is a global, greedy algorithm. These both cause long events to be preferred over short events, and thus the first changepoints found (and retained after applying the penalty) may not include those for small, obvious events. For example, see performance on this simulated data, which includes events of size 10, 20, 30, and 40 within 100,000 points at S/N ratio 3:1 in addition to sine waves of various frequency at S/N ratio 1:2 (to roughly simulate GC waves). Changepoints arising from the sine waves will be found first, since these give rise to longer segments:. ![wave-kern-no-local](https://user-images.githubusercontent.com/11076296/29322673-4dd9a1ac-81ac-11e7-94f5-5c5494e44ac5.png). CBS similarly finds many false positive breakpoints:. ![wave-cbs](https://user-images.githubusercontent.com/11076296/29322677-5576e4ba-81ac-11e7-888b-07ed5bff27e3.png). However, when we tune down the sine waves to 1:10, ApproxKernSeg still gets tripped up, but CBS looks better:. ![wave-kern-no-local-small-waves](https://user-images.githubusercontent.com/11076296/29322732-815df58c-81ac-11e7-8305-6e1798616336.png); ![wave-cbs-small-waves](https://user-images.githubusercontent.com/11076296/29322737-836b78fe-81ac-11e7-93be-753a40011203.png). To improve ApproxKernSeg, we can 1) make the cost function intensive, by simply dividing by the number of points in a segment, and 2) add to the cost function a local term, given by the cost of making each point a changepoint within a local window of a determined size. This local term was inspired by methods such as SaRa (http://c2s2.yale.edu/software/sara/). The reasoning is that with events at higher S/N ratio, we typically don't need to perform a global test to see whether ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-322502045:526,perform,performance,526,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-322502045,1,['perform'],['performance']
Performance,Agreed @samuelklee this is likely due to changes in default parameters but still concerning to see differences like this. The defaults were changed in accordance with the gCNV Nature paper but are designed for exomes. @Stikus you may want to look at the [GATK-SV default settings](https://github.com/broadinstitute/gatk-sv/blob/dc92e9f4bc46a1ab19459b515c24c9747a73a967/wdl/GermlineCNVCohort.wdl#L495) which are tuned for genome calling.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8628#issuecomment-1856621583:411,tune,tuned,411,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8628#issuecomment-1856621583,1,['tune'],['tuned']
Performance,"Ah, I thought that might be the case, to be honest. . My vote is for ripping out the old. If for whatever reason you find an edge case where the new doesn't perform as well, then in my view that just means the new can be improved. . Ultimately it seems to me that @ldgauthier is the one with the most experience in this space and should make the call.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2255#issuecomment-258170795:157,perform,perform,157,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2255#issuecomment-258170795,1,['perform'],['perform']
Performance,"Aha, after clearing the travis cache for the PR build it passed! Merging",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5194#issuecomment-422499733:31,cache,cache,31,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5194#issuecomment-422499733,1,['cache'],['cache']
Performance,"Aha, good -- that's consistent with @skwalker's results showing a ~30%-40% performance regression vs. GATK3 when using a large interval list + the latest GATK4 HC. I think this is something we can resolve through profiling -- I'll update you in a few days with my progress. In the mean time, it would be valuable to me to know whether you see the same performance difference in Mutect2 when running *without* an interval list (latest master vs. this downsampling branch). Perhaps you could create a large-but-not-too-large bam snippet to test that out?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3238#issuecomment-330910227:75,perform,performance,75,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3238#issuecomment-330910227,2,['perform'],['performance']
Performance,Almost looks like there is a buffer overrun somewhere. Most of our testing has been on `nfs` and have not encountered a tcache(thread local cache) issue. Is `gpfs` available as open source?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8683#issuecomment-1935180685:140,cache,cache,140,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8683#issuecomment-1935180685,1,['cache'],['cache']
Performance,"Alright, I'll try to get it published to the Broad artifactory by end of the week. Once it's there, this branch will need to be modified to depend on the version in artifactory, locate the library on the classpath, and extract and load it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1750#issuecomment-220158188:231,load,load,231,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1750#issuecomment-220158188,1,['load'],['load']
Performance,"Also as part of the mock-up, we should actually package the mock config files inside of our jar, load them off the classpath, and test file-based override ability.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3126#issuecomment-309543353:97,load,load,97,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3126#issuecomment-309543353,1,['load'],['load']
Performance,Also fill in some cost optimizations for some tasks.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3940#issuecomment-351478645:23,optimiz,optimizations,23,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3940#issuecomment-351478645,1,['optimiz'],['optimizations']
Performance,"Also, FeatureDataSource.getGenomicsDBFeatureReader requires the reference, but it's never used, apparently:. https://github.com/broadinstitute/gatk/blob/bc0994c180312cdca7afbe45b410b2c6fc312043/src/main/java/org/broadinstitute/hellbender/engine/FeatureDataSource.java#L387. Should something related to GenomicsDBFeatureReader be getting the reference? FeatureData source has an error if it's not provided: ""You must provide a reference if you want to load from GenomicsDB"".",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7005#issuecomment-747448285:451,load,load,451,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7005#issuecomment-747448285,1,['load'],['load']
Performance,"Also, and again not part of this PR, does the concept of trying to run VariantEvalEngine over an interval, serializing its state to disk somehow (which would involve doing something with StratificationManager), and then writing a tool that loads/aggregates those per-interval objects seem reasonable as a way to support scatter/gather?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-754946232:240,load,loads,240,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-754946232,1,['load'],['loads']
Performance,"Also, if we can't figure this out, then I really think it's worth kicking it up to Cromwell to see if call caching can be made more scalable.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6607#issuecomment-632309675:132,scalab,scalable,132,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6607#issuecomment-632309675,1,['scalab'],['scalable']
Performance,"Also, interestingly, if you look at the pipeline we run nightly in jenkins, there hasn't been any performance regression there. https://gatk-jenkins.broadinstitute.org/view/Performance/job/gatk-perf-test-spark-readpipeline/buildTimeTrend",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4376#issuecomment-364188535:98,perform,performance,98,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4376#issuecomment-364188535,2,"['Perform', 'perform']","['Performance', 'performance']"
Performance,"Also, it seems that if the input VCF and the dbSNP are not sorted in the same way, for example:; Input VCF; >chr22; >chrX; >chrY; >chrM. dbSNP; >chr22; >chrM; >chrX; >chrY. again, the comparison analysis will fail with the error; > java.lang.IllegalStateException: The elements of the input Iterators are not sorted according to the comparator htsjdk.variant.variantcontext.VariantContextComparator. where the workaround is again to sort both the Input VCF and the dbSNP according to a specific dict, in order to make sure they are matching (if, for example, you don't know how the input VCF and dbSNP were handled upstream), thus leading to always having to perform sorting before VariantEval, which can be 'expensive' if using the recent dbSNP databases (which are around 80GB in size). (Reposting this, originally posted from a wrong account)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6855#issuecomment-710908612:659,perform,perform,659,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6855#issuecomment-710908612,1,['perform'],['perform']
Performance,"Also, we should check that the maven dependencies are being cached properly per-branch (they don't seem to be at the moment)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1769#issuecomment-214503329:60,cache,cached,60,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1769#issuecomment-214503329,1,['cache'],['cached']
Performance,"An update: apparently asking the user to regenerate their index and dictionary files for their references has resolved the issue. We should soften the blow of null pointers in the future. Apparently the dictionary was present (as it wouldn't work otherwise) but was corrupt somehow, a proposal from @droazen would be to add to the sequnce dictionary check we already perform a check that the file is readable.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6142#issuecomment-529558237:367,perform,perform,367,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6142#issuecomment-529558237,1,['perform'],['perform']
Performance,"Another development, a new inference method claiming improved performance over ADVI:. https://arxiv.org/abs/1706.02375. We can think of such inference methods (along with the three with implementations available in Stan, namely MAP, ADVI, and NUTS) as interchangeable black boxes that take the optimization target specified by the modeling language and the corresponding automatically generated derivative as inputs. Whatever JNI layer we implement would ideally allow us to build such boxes in pure Java that call out to the JNI for each target/derivative evaluation, as the amount and complexity of code for such boxes should be relatively manageable (in comparison to that required for the autodiff/autotransformation/modeling infrastructure). However, it remains to be seen whether the overhead of such calls will be acceptable.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2746#issuecomment-311091138:62,perform,performance,62,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2746#issuecomment-311091138,2,"['optimiz', 'perform']","['optimization', 'performance']"
Performance,"Another library that just popped up on my radar---and this one is actually in Java!. http://www.amidsttoolbox.com/; https://arxiv.org/abs/1704.01427; https://arxiv.org/abs/1604.07990. The approach is quite different from the other libraries we have been considering; here, the focus seems to be on streaming/parallel data and inference via message passing. I think our models can be expressed in their framework (although, at a glance, the modeling language is not as nice as Stan---it looks like you have to build DAGs explicitly), but I have to admit that I am not familiar with message passing and how performance compares to MCMC, ADVI, etc. @mbabadi @davidbenjamin any thoughts?. I still think it's worth playing around with this library. We could investigate how quick it would be to implement a streaming/minibatch version of VQSR, for example, which might be useful for @eitanbanks @ldgauthier.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2746#issuecomment-318674946:605,perform,performance,605,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2746#issuecomment-318674946,1,['perform'],['performance']
Performance,"Any update on this? I just ran into a form of this problem in the context of some pipeline unit tests. I have a task that runs the following:. ```; gatk GenomicsDBImport \; --sample-name-map ${sample_map} \; --genomicsdb-workspace-path ${cohort_name}_gdb \; --genomicsdb-shared-posixfs-optimizations \; -L ${interval_list}. gatk GenotypeGVCFs \; -R ${ref_fasta} \; -V gendb://${cohort_name}_gdb \; -O ${cohort_name}.joint.vcf \; -L ${interval_list} ; ```. Which runs fine, but if I re-run the test suite the system complains it can't delete the gdb workspace. I have to manually `sudo rm` which is gross. I can work around this by adding either `chmod 777 -R ${cohort_name}_gdb` or `rm -r ${cohort_name}_gdb` as a cleanup step, but that seems gross too. . My use case is just a toy example for training purposes, but I worry about what this could mean for a production environment. Am I missing something?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8233#issuecomment-2078126120:286,optimiz,optimizations,286,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8233#issuecomment-2078126120,1,['optimiz'],['optimizations']
Performance,"Apart from using htsjdk asyncIO, we should maybe look into running the prefetching (or reads and variants) asynchronously - maybe prefetching+filtering could in be done in a different thread and fed to the walker via a blocking queue.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1606#issuecomment-202534392:228,queue,queue,228,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1606#issuecomment-202534392,1,['queue'],['queue']
Performance,"ApplyBQSR (aka PrintReads) is done. Performance stats look very good. user time, gatk4 is 37% better; GATK3 57555.36 seconds; GATK4 35718.76 seconds . and that's using less CPU: ""Percent of CPU this job got""; GATK3 121%; GATK4 106%. Maximum resident set size (kbytes) - gatk4 uses 82% less memory (?); GATK3 84675872 kbytes; GATK4 14657904 kbytes",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1413#issuecomment-188319033:36,Perform,Performance,36,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1413#issuecomment-188319033,1,['Perform'],['Performance']
Performance,"Are gradle dependencies cached anywhere for the Travis build? Pretty sure they aren't, but I'm out of ideas why Travis is failing for this repo.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-296293145:24,cache,cached,24,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-296293145,1,['cache'],['cached']
Performance,"Are there any updates on this feature? The use of soft-clipping is not only confusing, but can negatively affect the performance of other tools that use this sort of information. Ignoring soft-clipped reads altogether, if possible at all, is not a good solution. We are forced to use GATK3 because the output of the GATK4 version does not work well with others tools we need for the detection of certain variants in RNA-seq.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7356#issuecomment-1846884589:117,perform,performance,117,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7356#issuecomment-1846884589,1,['perform'],['performance']
Performance,Are we interested in writing some definitive guide on how to tune the `af-of-alleles-not-in-resource` parameter for different contexts?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4745#issuecomment-387218068:61,tune,tune,61,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4745#issuecomment-387218068,1,['tune'],['tune']
Performance,"Are you sure that there is enough space? Could you run something like:. ```; df -h /storage/home/data/gendb/; ```; And check that available space is in line with your expectations? Also, is there any user specific quota being enforced on your system?. Not completely sure, but another possibility is that you have too many (fairly small) intervals in your `chr13.bed` file. Unless you are specifically trying to exclude intervals not in the bed file, you should get a lot better performance/efficiency by specifying fewer intervals (think <100 intervals, and in your case since you are doing a single chromosome, maybe much less than that). . If you don't want to manually specify fewer intervals, you could try setting `--merge-input-intervals` to `true` which will return one interval for the contig spanning all the intervals you've specified. Having many smaller intervals results in many small files created which will definitely be bad for performance, and may also cause quota issues.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6950#issuecomment-726416409:479,perform,performance,479,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6950#issuecomment-726416409,2,['perform'],['performance']
Performance,"As @nalinigans suggested, the `--genomicsdb-shared-posixfs-optimizations` should help, though probably mostly for import. Similarly, I would highly recommend `--bypass-feature-reader` [link](https://gatk.broadinstitute.org/hc/en-us/articles/13832686645787-GenomicsDBImport#--bypass-feature-reader) for the import as well. As I mentioned before, reblocking will help import and query - mainly because it reduces the input GVCF size by 5x-8x. Shouldn't be necessary for the number of samples you indicate, but will become more important as number of samples scales up (and does help at any number of samples, I should add). That doesn't seem to the crux of your problem though...you note that running serially does better than trying to parallelize across many cores. I don't have a lot of insight into Lustre specifically, but do you have any metrics on how the IOPS looks for the Lustre FS in each case? Also, the bit about the the first set of variants taking a while - does that time look different when running serially versus in parallel?. One experiment to consider - maybe try to copy the workspace to the `$PBS_JOBFS` folder on the compute node before running `GenotypeGVCFs`. Not sure it is feasible in terms of amount of storage, etc but it would at least rule out possible Lustre issues.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8637#issuecomment-1879964821:59,optimiz,optimizations,59,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8637#issuecomment-1879964821,1,['optimiz'],['optimizations']
Performance,"As discussed in https://github.com/broadinstitute/gatk/issues/1203, we are keeping SHUFFLE so that we have a baseline during performance testing. Closing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2231#issuecomment-288534664:125,perform,performance,125,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2231#issuecomment-288534664,1,['perform'],['performance']
Performance,"As far as I can tell, getting that error message means that BaseTest is being loaded at runtime, and running it's static initializer block which calls `SparkContextFactory.enableTestSparkContext();`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3591#issuecomment-330658299:78,load,loaded,78,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3591#issuecomment-330658299,1,['load'],['loaded']
Performance,"As for TableReader.... we could make it a bit more efficient by reusing DataLine instances. Currently it creates one per each input line, but same instance could be reused loading each new line data onto it before calling ```createRecord```. . We are delegating to a external library to parse the lines into String[] arrays (one element per cell) .... we could save on that by implementing it ourselves more efficiently but of course that would be take one of our some of his/her precious development time... In any case I don't know what the gain would be considering that these operations are done close to I/O that typically should be dominating the time-cost. . The DataLine reuse may save some memory churning and wouldn't take long to code.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316489131:172,load,loading,172,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316489131,1,['load'],['loading']
Performance,"As we discussed on Slack, this will fix the NaNs, but I'm not convinced that we should allow the single-contig use case without at least a warning. The ploidy step will essentially perform no inference, since I think the per-contig bias and ploidy factors will cancel out with the way the likelihood is written---it will simply return the prior, and all samples will be guaranteed to have ploidy = 2. @asmirnov239 is going to do some more testing to make sure we understand this right and perhaps add a warning/documentation. The current likelihood is a bit confusing (I tried to address some of these issues in the unmerged ploidy-model update), but in any case, the problem is degenerate and it's hard to define appropriate behavior without additional priors and model structure.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6613#issuecomment-631693589:181,perform,perform,181,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6613#issuecomment-631693589,1,['perform'],['perform']
Performance,Assigning to @akiezun to gather his past performance results into one place.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2012#issuecomment-233390678:41,perform,performance,41,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2012#issuecomment-233390678,1,['perform'],['performance']
Performance,"At a high-level, the approach here seems to be to create shards (of size 1 million bps) and load the reads, variants and references for each shard in memory. Each shard then has a recalibration table created for it, then the tables are merged into one. The existing version finds the variants for each read, and has two shuffles (this is for the reference broadcast approach, there's an extra one for the reference shuffle approach). The first is to join the variants and reads together, and a second to aggregate the variants for each read. The optimization in this PR has no shuffles, since it loads all variants into memory rather than doing distributed joins. Is that a valid assumption? If so, it would be possible to load the variants into memory in the driver and broadcast to all workers to remove the shuffles (in the existing implementation). I noticed that `AddContextDataToReadSpark#subdivideAndFillReads` opens the BAM itself, rather than using `ReadsSparkSink`. This won't work well in the Hadoop case since you lose locality - i.e. the work won't be scheduled on the node where the BAM files are stored. It would be much better to find a way of using `ReadsSparkSink`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/987#issuecomment-147415253:92,load,load,92,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/987#issuecomment-147415253,4,"['load', 'optimiz']","['load', 'loads', 'optimization']"
Performance,"BI_REFERENCE_SERVICE_URL_MASK : http://www.ebi.ac.uk/ena/cram/md5/%s; 18:01:51.712 INFO SortSam - Defaults.NON_ZERO_BUFFER_SIZE : 131072; 18:01:51.712 INFO SortSam - Defaults.REFERENCE_FASTA : null; 18:01:51.712 INFO SortSam - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 18:01:51.713 INFO SortSam - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 18:01:51.713 INFO SortSam - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 18:01:51.713 INFO SortSam - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 18:01:51.713 INFO SortSam - Defaults.USE_CRAM_REF_DOWNLOAD : false; 18:01:51.713 INFO SortSam - Deflater IntelDeflater; 18:01:51.713 INFO SortSam - Initializing engine; 18:01:51.713 INFO SortSam - Done initializing engine; 18:02:01.512 INFO SortSam - Shutting down engine; [December 7, 2016 6:02:01 PM AST] org.broadinstitute.hellbender.tools.picard.sam.SortSam done. Elapsed time: 0.16 minutes.; Runtime.totalMemory()=1911029760; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/xerial/snappy/LoadSnappy; 	at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:86); 	at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:52); 	at htsjdk.samtools.util.TempStreamFactory.getSnappyLoader(TempStreamFactory.java:42); 	at htsjdk.samtools.util.TempStreamFactory.wrapTempOutputStream(TempStreamFactory.java:74); 	at htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:223); 	at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:166); 	at htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:192); 	at org.broadinstitute.hellbender.tools.picard.sam.SortSam.doWork(SortSam.java:52); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:112); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgram.instanceMain(PicardCommandLineProgram.java:62); 	at org.br",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2299#issuecomment-265469924:1812,Load,LoadSnappy,1812,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2299#issuecomment-265469924,1,['Load'],['LoadSnappy']
Performance,"BTW, we could actually get a more accurate DP estimate by using the INFO field instead of the FORMAT DP (so we wouldn't have to touch the genotypes at all), but I'm still thinking through the implementation details. It would introduce batch effects, but it would be an improvement in results and rescue your performance optimization.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3688#issuecomment-377568302:308,perform,performance,308,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3688#issuecomment-377568302,2,"['optimiz', 'perform']","['optimization', 'performance']"
Performance,"Based on testing performed on google cloud, this problem seems to have resolved itself after we updated to the newest googleCloudJava package in this #5135. Something fixed by switching off of our ancient fork must have been responsible for the file being misread by spark. I suggest we close this issue as it appears to be resolved unless @droazen you want to do a post-mortem to figure out what the relevant change must have been?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5133#issuecomment-419539317:17,perform,performed,17,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5133#issuecomment-419539317,1,['perform'],['performed']
Performance,"Btw - it is interesting (as in, unexpected, at least to me) that appending new data is slowing down as the workspace grows. Adding new data is sorta fire-and-forget in that it shouldn't care much about what already exists...offhand, I'm not sure why you're seeing a slowdown. You mentioned needing to use smaller batch sizes...were you otherwise seeing larger memory overheads than before? Or just general slower performance?. There is a new `--bypass-feature-reader` option in the latest release that should help with dramatically lowering memory usage, and potentially offering a slight speedup for imports. Might be worth a shot. edit: you mentioned consolidate, so I should also add that consolidate is expected to help with read performance (shouldn't affect import) after we've had a fair number of batches imported (left intentionally vague. As a guess, ~100 or so?)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7542#issuecomment-964432955:413,perform,performance,413,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7542#issuecomment-964432955,2,['perform'],['performance']
Performance,"Build is failing because of a warning. It's odd that we didn't see this before, it seems unrelated to my change:. ```; :compileJava/home/travis/.gradle/caches/modules-2/files-2.1/com.google.cloud/gcloud-java-nio/0.2.8/57e30d28f80ab3a320e560e7b4aaac07baf98c4/; gcloud-java-nio-0.2.8-shaded.jar(com/google/cloud/storage/contrib/nio/CloudStorageFileSystemProvider.class): warning:; Cannot find annotation method 'value()' in type 'AutoService': class file for shaded.cloud-nio.com.google.auto.service.AutoService not found; error: warnings found and -Werror specified; 1 error; 1 warning; FAILED; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2224#issuecomment-257033597:152,cache,caches,152,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2224#issuecomment-257033597,1,['cache'],['caches']
Performance,But I agree that it would be good to quantify the performance difference.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/115#issuecomment-70113718:50,perform,performance,50,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/115#issuecomment-70113718,1,['perform'],['performance']
Performance,"By default we only support OSX and modernish linux on amd64. We don't have a plan to test or officially support aarch64. However, it's mostly java so it might mostly work? The things I can think of off the bat that won't work are the optimizations for intel hardware, i.e. the fast pairhmm and deflate/inflate optimizations will definitely not work. They **should** degrade to java implementations automatically though. . IBM has a fork that works on power8 which reimplements some of the optimizations for PairHMM, so it's definitely possible to implement the various native optimizations for other architectures although not a small project. . GenomicsDB won't work since we only bundle libs for amd64/osx. You could in theory compile it yourself and pass the library on the library path. Other things that will need special attention would any of the library wrappers, for bwa-mem, fermlite, and hdf5. Those will similarly need custom builds supplied to the gatk. . I suspect that you could get the reads pipeline working, but newer tools with weirder native dependencies will be tricky.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6118#issuecomment-524959548:234,optimiz,optimizations,234,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6118#issuecomment-524959548,4,['optimiz'],['optimizations']
Performance,"By using 5 executors, 24 cores on each when no one else seem to be using the local cluster and having the number of partitions approximately equal to the number of FASTQ files, [this](http://dataflow01.broadinstitute.org:18088/history/application_1464285223085_0420/jobs/) is the time it takes to run on the 44188 small FASTQ files, without capturing the stdout and stderr of SGA processes. Note that this time the RDD caching is done differently than the way it is done in code living in master:. ```; results.cache(); // cache because Spark doesn't have an efficient RDD.split(predicate) yet; results.count(); // ugly hack to make the actual computation happen, so later filtering step will be based on what has been actually computed. // save fasta file contents or failure message; final JavaPairRDD<Long, SGAAssemblyResult> success = results.filter(entry -> entry._2().assembledContigs!=null);; final JavaPairRDD<Long, SGAAssemblyResult> failure = results.filter(entry -> entry._2().assembledContigs==null);. if(!success.isEmpty()){; success.map(entry -> entry._1().toString() + ""\n"" + entry._2().assembledContigs.toString()); .saveAsTextFile(outputDir+""_0"");; }. if(!failure.isEmpty()){; failure.map(entry -> entry._1().toString() + ""\n"" + entry._2().collectiveRuntimeInfo.toString()); .saveAsTextFile(outputDir+""_1"");; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1877#issuecomment-225187611:511,cache,cache,511,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1877#issuecomment-225187611,2,['cache'],['cache']
Performance,"CEUTrio.HiSeq.WEx.b37.NA12892.chr10.bam (1.2Gb on local drive); Mac OS X 10.9.5 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_25-b17;. initial optimized run on 1.2Gb file after ripping out some of the indel code:; 4.pre-alpha-71-g50b094b. ```; real 4m58.795s; user 5m3.401s; ```. For reference, here are times for pre-optimization GATK3 and GATK4 (some data from #1033). GATK4 master branch, with indels. ```; real 9m7.691s; user 9m2.250s; ```. GATK4 master branch, the `-DIQ` option (ie skip indel quals). ```; real 5m19.914s; user 5m14.710s; ```. (which is already faster than GATK3.4.46 - numbers listed below). GATK3.4.46 with indels . ```; real 11m21.538s; user 17m24.320s; ```. GATK3.4.46 with the `-DIQ` option (ie skip indel quals). ```; real 6m3.662s; user 10m34.859s; ```. For reference, the best possible bottom line (for bqsr optimizations, not reading/writing itself) is established by PrintReads on the same data:. ```; real 2m48.873s; user 2m27.752s; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1056#issuecomment-152595055:145,optimiz,optimized,145,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1056#issuecomment-152595055,3,['optimiz'],"['optimization', 'optimizations', 'optimized']"
Performance,"CRAM + NIO looks to be ~3 cents per sample. This essentially includes disk optimizations, since the disk size is determined by the CRAM size; this is not too large, so this results in disk costs of ~0.3 cents per sample. Note that I ran on the CRAMs in gs://broad-sv-dev-data/TCGA_blood_normals.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5715#issuecomment-467608484:75,optimiz,optimizations,75,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5715#issuecomment-467608484,1,['optimiz'],['optimizations']
Performance,"CRAM w/o NIO is also ~3 cents per sample (it was marginally more expensive than CRAM w/ NIO, but within the noise). CRAM w/o NIO w/ SSD is ~5 cents. So I'd say CRAM w/ or w/o NIO is fine. Strictly speaking, we can't directly compare the BAM and CRAM costs, since they were done on different sets of TCGA samples. But both are well under the goal of ~15 cents per sample, so I think it's safe to say that we can turn our attention to optimizing inference costs.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5715#issuecomment-467612453:433,optimiz,optimizing,433,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5715#issuecomment-467612453,1,['optimiz'],['optimizing']
Performance,"Can someone explain this pseudoinverse business to me? We standardize the counts to give a matrix C, calculate the SVD to get the left-singular matrix U and the pseudoinverse of C, but then perform *another* SVD on U to get the pseudoinverse of U. Why do we need these pseudoinverses? @LeeTL1220 @davidbenjamin?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316267546:190,perform,perform,190,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316267546,1,['perform'],['perform']
Performance,"Can someone modify the underlying read without calling a setter?. On Monday, July 25, 2016, droazen notifications@github.com wrote:. > @akiezun https://github.com/akiezun That's why I was suggesting; > invalidating all cached values on every call to any setter -- that way we; > don't have to think about the nuances of when it's necessary to; > recalculate, and greatly reduce the risks that normally come with caching; > while still getting most of the performance benefit in typical usage.; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235136126,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/AB5rL8K8hmn3JygZbx39Covn8lc14S5sks5qZWIxgaJpZM4JR8AP; > . ## . Sent from Gmail Mobile",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235139023:219,cache,cached,219,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235139023,2,"['cache', 'perform']","['cached', 'performance']"
Performance,"Can you please test this change with the `HaplotypeCaller` in protected and make sure nothing changes? In particular, can you run `HaplotypeCallerIntegrationTest` and `HaplotypeCallerEngineUnitTest` and make sure they pass with this change? This PR makes me a little nervous given the centrality of the classes touched, even though the optimization itself is simple enough...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1795#issuecomment-216595202:336,optimiz,optimization,336,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1795#issuecomment-216595202,1,['optimiz'],['optimization']
Performance,Can you provide a summary of the optimizations included here @pnvaidya ?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4379#issuecomment-364256682:33,optimiz,optimizations,33,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4379#issuecomment-364256682,1,['optimiz'],['optimizations']
Performance,"Can you try a few more cache sizes like 200000 and 500000? Also, when you do the PR, could you create `VariantWalker.FEATURE_CACHE_SIZE = 100000` (or whatever the final value is) and pass it into the `FeatureManager` in `initializeFeatures()`? `FeatureDataSource.DEFAULT_QUERY_LOOKAHEAD_BASES` should stay at 1000 (for ReadWalkers, which have a lot more overlap in their query access patterns)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1092#issuecomment-155505129:23,cache,cache,23,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1092#issuecomment-155505129,1,['cache'],['cache']
Performance,Cf. performance results in https://github.com/broadinstitute/gatk/pull/1695,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1696#issuecomment-207488641:4,perform,performance,4,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1696#issuecomment-207488641,1,['perform'],['performance']
Performance,Chaning the holding collection to a Vector which is synchronized. There is no performance impact as this add and single collection iteractiion should happen just a finite number of times.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7403#issuecomment-899839756:78,perform,performance,78,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7403#issuecomment-899839756,1,['perform'],['performance']
Performance,"Chiming in here -- the default output for GenotypeGVCFs should be a VCF that follows the spec. An important consideration here is that many users/pipelines usually only perform _site level_ filtering. Genotype level filtering is rarely performed. This causes problems in cases where most genotypes are of high quality at a site and a small number are missing but called as ""0/0"". Those 0/0 calls would then slip by. Coding missing data as 0/0 also breaks common site level filters such as vcftools' --max-missing, which relies on missing sites being coded per the spec.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8328#issuecomment-1932170780:169,perform,perform,169,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8328#issuecomment-1932170780,2,['perform'],"['perform', 'performed']"
Performance,Closing -- we now have a `--strict` mode that matches the regular `HaplotypeCaller` at the expense of speed. Next step is to work on performance (https://github.com/broadinstitute/gatk/issues/5263),MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5265#issuecomment-460398930:133,perform,performance,133,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5265#issuecomment-460398930,1,['perform'],['performance']
Performance,Closing because of bad travis cache for builds. Will submit identical PR with cloned branch.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5119#issuecomment-414426377:30,cache,cache,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5119#issuecomment-414426377,1,['cache'],['cache']
Performance,"Closing this issue, since the performance issues that the `HaplotypeCaller` had in the early betas are believed to be resolved in the 4.0 release. @chandrans please open a new ticket if the user can replicate the performance issue in the 4.0 release.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3631#issuecomment-361989230:30,perform,performance,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3631#issuecomment-361989230,2,['perform'],['performance']
Performance,"Consolidated with #2498. Now that #6885 is done, I'm going to kick off a Bayesian optimization (using the pipeline-optimizer from https://github.com/broadinstitute/gatk-evaluation/tree/master/pipeline-optimizer I presented on long ago) over all 3 sets of SW parameters using unfiltered HaplotypeCaller -> vcfeval F1 on NA12878 chr22 (with F1 optimized on GQ threshold and enabling decomposition of variants) as the target. This is probably not what we want to ultimately do in practice---we may want to more heavily weight sensitivity after calling, hook up variant filtering, stratify on high/low confidence regions or variant characteristics, etc.---but I'm just curious to see what happens. I see two potentially useful outcomes: 1) we demonstrate that parameters don't have much of an effect and can be consolidated, or 2) we find more optimal sets of parameters. Potentially we could also show that 3) our parameters are already optimal (I'd say this would be by pure dumb luck), in which case we could at least demonstrate and document some justification for them. If the parameters don't have much of an impact on NA12878, I'm curious to see whether this holds for low coverage or messier data---and ultimately, in malaria. Just starting with NA12878 because of the availability of truth and the potential impact for the primary use case of calling in human data. Some preliminary results: I ran the aforementioned comparison on chr22 with 1) 4.1.8.1 master and 2) 4.1.8.1 with haplotype-to-reference SW parameters changed from `NEW_SW_PARAMETERS` to `STANDARD_NGS` on two replicates of NA12878 (O1D1 and O2D2 from the 2018 NovaSeq snapshot experiment). On each replicate, 2) demonstrated slightly lower performance, but it was well within the sample-to-sample variation between these two replicates. Here are the corresponding vcfeval summaries:. ```; ::::::::::::::; NA12878/O1D1/4.1.8.1/summary.txt; ::::::::::::::; Threshold True-pos-baseline True-pos-call False-pos False-neg Precision Sen",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-710107566:82,optimiz,optimization,82,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-710107566,4,['optimiz'],"['optimization', 'optimized', 'optimizer']"
Performance,"CopyGCSDirectoryIntoHDFSSpark - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Defaults.USE_CRAM_REF_DOWNLOAD : false; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Deflater IntelDeflater; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Inflater IntelInflater; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Initializing engine; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4769b07b] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@5ef60048].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4769b07b] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@5ef60048].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.or",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2618#issuecomment-296871363:1899,load,loaded,1899,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2618#issuecomment-296871363,1,['load'],['loaded']
Performance,Could I have some feedback about the efficiency of the Fisher's Exact Test implemented here? I'm planning to use it in other context where the performance could be reduced and I think that this is a good opportunity to have some information about it. Thanks in advance!,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2307#issuecomment-267036486:143,perform,performance,143,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2307#issuecomment-267036486,1,['perform'],['performance']
Performance,"Could this be related to having sliced objects in the gsutils buckets but not using a code path that goes through a native CRC implementation? I ask because I noticed that when I try to download the file. ```; gs://hellbender/test/resources/benchmark/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam; ```. with gsutil, I get this error:. ```; CommandException: ; Downloading this composite object requires integrity checking with CRC32c,; but your crcmod installation isn't using the module's C extension, so the; hash computation will likely throttle download performance. For help; installing the extension, please see:. $ gsutil help crcmod. To download regardless of crcmod performance or to skip slow integrity; checks, see the ""check_hashes"" option in your boto config file.; ```. Could the GATK command path be computing all of the CRC hashes in Java code, slowing it down?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1755#issuecomment-223982882:541,throttle,throttle,541,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1755#issuecomment-223982882,3,"['perform', 'throttle']","['performance', 'throttle']"
Performance,"Couldn't open hdf5 files.; ![Screenshot_2020-10-29_17-20-17](https://user-images.githubusercontent.com/29140765/97586406-459fe000-1a0b-11eb-86cf-d70a28c55637.png); Running without the optional --sequence-dictionary argument also causes an error.; `17:00:00.556 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/lmbs02/bio/biosoft/gatk/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Oct 29, 2020 5:00:00 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 17:00:00.683 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------; 17:00:00.684 INFO PostprocessGermlineCNVCalls - The Genome Analysis Toolkit (GATK) v4.1.9.0; 17:00:00.684 INFO PostprocessGermlineCNVCalls - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:00:00.684 INFO PostprocessGermlineCNVCalls - Executing as lmbs02@Lmbs01 on Linux v5.4.0-48-generic amd64; 17:00:00.684 INFO PostprocessGermlineCNVCalls - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_265-8u265-b01-0ubuntu2~18.04-b01; 17:00:00.684 INFO PostprocessGermlineCNVCalls - Start Date/Time: October 29, 2020 5:00:00 PM MSK; 17:00:00.684 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------; 17:00:00.684 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------; 17:00:00.684 INFO PostprocessGermlineCNVCalls - HTSJDK Version: 2.23.0; 17:00:00.684 INFO PostprocessGermlineCNVCalls - Picard Version: 2.23.3; 17:00:00.684 INFO PostprocessGermlineCNVCalls - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 17:00:00.685 INFO PostprocessGermlineCNVCalls - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:00:00.685 INFO PostprocessGermlineCNVCalls - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 17:00:00.685 INFO PostprocessGermlineCNV",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6924#issuecomment-718787427:288,Load,Loading,288,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6924#issuecomment-718787427,1,['Load'],['Loading']
Performance,"Current status of this: The tool can physically run on 11k samples, but with a 1-5% failure rate, depending on the combination of arguments used. The failures are almost all due to https://github.com/broadinstitute/gatk/issues/2685 (see the stack trace in https://github.com/broadinstitute/gatk/issues/2685#issuecomment-308541727 for a representative example error). . One possibility is that we are being throttled in a way that GATK itself can't recover from. GATK is retrying in the face of these SSL errors 20 times, with increasing wait times between each attempt, and still running out of retries. See @jean-philippe-martin 's latest hypothesis in https://github.com/broadinstitute/gatk/issues/2685#issuecomment-308586876. @kcibul @Horneth take note.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2633#issuecomment-308755736:406,throttle,throttled,406,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2633#issuecomment-308755736,1,['throttle'],['throttled']
Performance,D.scala:823); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:123); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); 	at,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429:10381,concurren,concurrent,10381,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429,1,['concurren'],['concurrent']
Performance,"DBImport, I randomly select 50 samples from our history samples(using the same probe set) along with the current batch.; time ${gatk} --java-options ""-Xmx8g -Xms2g"" GenomicsDBImport \; --tmp-dir /paedyl01/disk1/yangyxt/test_tmp \; --genomicsdb-update-workspace-path ${vcf_dir}/genomicdbimport_chr${1} \; -R ${ref_gen}/ucsc.hg19.fasta \; --batch-size 0 \; --sample-name-map ${gvcf}/batch_cohort.sample_map \; --reader-threads 5; check_return_code. # For GenotypeGVCFs; time ${gatk} --java-options ""-Xmx8g -Xms2g -DGATK_STACKTRACE_ON_USER_EXCEPTION=true"" GenotypeGVCFs \; -R ${ref_gen}/ucsc.hg19.fasta \; -V gendb://${vcf_dir}/genomicdbimport_chr${1} \; -G StandardAnnotation \; -G AS_StandardAnnotation \; -L chr${1} \; -O ${bgvcf}/all_${seq_type}_samples_plus_${sample_batch}.chr${1}.HC.vcf. # These are log records:; 02:07:51.286 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 02:07:51.321 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/yangyxt/software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl; Nov 06, 2020 2:07:56 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 02:07:56.529 INFO GenotypeGVCFs - ------------------------------------------------------------; 02:07:56.529 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.1.8.1; 02:07:56.530 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 02:08:01.543 INFO GenotypeGVCFs - Executing as yangyxt@paedyl01 on Linux v3.10.0-1062.18.1.el7.x86_64 amd64; 02:08:01.543 INFO GenotypeGVCFs - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_161-b12; 02:08:01.543 INFO GenotypeGVCFs - Start Date/Time: November 6, 2020 2:07:51 AM HKT; 02:08:01.543 INFO GenotypeGVCFs - ----------------------------------------------------------",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3429#issuecomment-722764059:1257,Load,Loading,1257,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3429#issuecomment-722764059,1,['Load'],['Loading']
Performance,Date/Time: 28 novembre 2019 15:47:36 CET; 15:47:37.246 INFO Mutect2 - ------------------------------------------------------------; 15:47:37.246 INFO Mutect2 - ------------------------------------------------------------; 15:47:37.246 INFO Mutect2 - HTSJDK Version: 2.20.3; 15:47:37.246 INFO Mutect2 - Picard Version: 2.21.1; 15:47:37.247 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 15:47:37.247 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 15:47:37.247 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 15:47:37.247 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 15:47:37.247 INFO Mutect2 - Deflater: IntelDeflater; 15:47:37.247 INFO Mutect2 - Inflater: IntelInflater; 15:47:37.247 INFO Mutect2 - GCS max retries/reopens: 20; 15:47:37.247 INFO Mutect2 - Requester pays: disabled; 15:47:37.247 INFO Mutect2 - Initializing engine; 15:47:41.204 INFO Mutect2 - Done initializing engine; 15:47:42.352 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/genouest/uni_limoges_fr/jpollet/.conda/envs/myd88/share/gatk4-4.1.4.0-1/gatk-package-4.1.4.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 15:47:42.423 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/genouest/uni_limoges_fr/jpollet/.conda/envs/myd88/share/gatk4-4.1.4.0-1/gatk-package-4.1.4.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 15:47:42.482 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 15:47:42.483 INFO IntelPairHmm - Available threads: 8; 15:47:42.483 INFO IntelPairHmm - Requested threads: 4; 15:47:42.483 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 15:47:42.936 INFO ProgressMeter - Starting traversal; 15:47:42.936 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 15:47:53.565 INFO ProgressMeter - ENA|LVXK01000001|LVXK01000001.1:19555 0.2 90 508.0; 15:48:05.962 INFO ProgressMete,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6271#issuecomment-559553558:2052,Load,Loading,2052,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6271#issuecomment-559553558,1,['Load'],['Loading']
Performance,"Date/Time: March 7, 2019 9:49:03 AM EST; 09:49:05.901 INFO Mutect2 - ------------------------------------------------------------; 09:49:05.901 INFO Mutect2 - ------------------------------------------------------------; 09:49:05.901 INFO Mutect2 - HTSJDK Version: 2.18.2; 09:49:05.901 INFO Mutect2 - Picard Version: 2.18.25; 09:49:05.902 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 09:49:05.902 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 09:49:05.902 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 09:49:05.902 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 09:49:05.902 INFO Mutect2 - Deflater: IntelDeflater; 09:49:05.902 INFO Mutect2 - Inflater: IntelInflater; 09:49:05.902 INFO Mutect2 - GCS max retries/reopens: 20; 09:49:05.902 INFO Mutect2 - Requester pays: disabled; 09:49:05.902 INFO Mutect2 - Initializing engine; 09:49:06.887 INFO Mutect2 - Done initializing engine; 09:49:06.935 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/Tools/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 09:49:06.937 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; 09:49:06.937 WARN PairHMM - ***WARNING: Machine does not have the AVX instruction set support needed for the accelerated AVX PairHmm. Falling back to the MUCH slower LOGLESS_CACHING implementation!; 09:49:07.007 INFO ProgressMeter - Starting traversal; 09:49:07.007 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 09:49:17.023 INFO ProgressMeter - 1:139173 0.2 480 2875.7; 09:49:27.704 INFO ProgressMeter - 1:763661 0.3 2590 7508.3; 09:49:38.001 INFO ProgressMeter - 1:958723 0.5 3290 6369.0; 09:49:49.182 INFO ProgressMeter - 1:981050 0.7 3380 4808.5; 09:50:02.383 INFO ProgressMeter - 1:988991 0.9 3440 3727.3; 09:50:13.586 INFO ProgressMeter - 1:1227096 1.1 4290 3866.1; 09:50:23.594 INFO ProgressMeter - 1:1",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5543#issuecomment-470579844:1817,Load,Loading,1817,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5543#issuecomment-470579844,1,['Load'],['Loading']
Performance,"Dear @droazen I might now have an idea.; Since we use the container in a restricted are, we pull the container from docker hub, save it as tar.gz, transfer it to the server and load it to the docker local image library. On this system we had the latest version of docker installed that was available from the ubuntu repository (20.something). This might lead to the error.; Best,; Daniel",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8402#issuecomment-1710415398:177,load,load,177,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8402#issuecomment-1710415398,1,['load'],['load']
Performance,"Deflater; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - Inflater: IntelInflater; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - Initializing engine; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.varia.NullAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.varia.NullAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""NullAppender"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not inst",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312229998:5156,load,loaded,5156,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312229998,1,['load'],['loaded']
Performance,"Did we change the name of the files since the initial bug was filed? Because my earlier comment talks about downloading to my desktop in 2.5min, but retrying now it takes about an hour! The file is 34.56 GiB. I guess that's what I get for moving to a different office. Assuming this time is correct, I see that NIO (when multithreaded) matches gsutil performance. Output below. gsutil: . ```; $ time gsutil cp gs://hellbender/test/resources/benchmark/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam .; Copying gs://hellbender/test/resources/benchmark/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam...; Downloading ..../CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam: 8.64 GiB/8.64 GiB ; Downloading ..../CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam: 8.64 GiB/8.64 GiB ; Downloading ..../CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam: 8.64 GiB/8.64 GiB ; Downloading ..../CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam: 8.64 GiB/8.64 GiB . real 55m11.517s; user 11m27.584s; sys 13m44.240s; ```. NIO (via ParallelCountBytes). ```; Reading the whole file using 4 threads...; Read all 37107966478 bytes in 3354s. ; The MD5 is: 0x73B5B38156DF9E2FF25FC57DD858DA0F; ```. (3354s is 55min54s). Next step: try on a Google Compute Engine computer, to get datacenter speeds.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1755#issuecomment-227605962:351,perform,performance,351,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1755#issuecomment-227605962,1,['perform'],['performance']
Performance,Did you reproduce the issue after using `--genomicsdb-shared-posixfs-optimizations` with GenomicsDBImport?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-761265484:69,optimiz,optimizations,69,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-761265484,1,['optimiz'],['optimizations']
Performance,"Discussed with @LeeTL1220 just now, and we decided that an approach based on the vanilla `LocusWalker` with calls to `splitContextBySampleName()` would result in a cleaner and more easily-parallelizable implementation, without the need for maintaining state across calls. So this branch is getting put on hold for now until we see whether the overhead of having to merge bams and then pull apart the pileups in the single-pass implementation is a significant performance issue. @LeeTL1220 Let me know the outcome of that evaluation, and we'll decide whether to resurrect or kill this PR.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2374#issuecomment-275748013:459,perform,performance,459,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2374#issuecomment-275748013,1,['perform'],['performance']
Performance,Dispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); 	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); 	at com.sun.proxy.$Proxy2.stop(Unknown Source); 	at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:377); 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.testng.TestNGException:An error occurred while instantiating class org.broadinstitute.hellbender.engine.spark.ReadsPreprocessingPipelineSparkTestData. Check to make sure it can be instantiated; 	at org.testng.internal.InstanceCreator.createInstanceUsingObjectFactory(InstanceCreator.java:134); 	at org.testng.internal.InstanceCreator.createInstance(InstanceCreator.java:79); 	at org.testng.internal.ClassImpl.getDefaultInstance(ClassImpl.java:110); 	at org.testng.internal.ClassImpl.getInstances(ClassImpl.java:195); 	at org.testng.TestClass.getInstances(TestClass.java:102); 	at org.testng.TestClas,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5787#issuecomment-472107858:1938,concurren,concurrent,1938,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5787#issuecomment-472107858,1,['concurren'],['concurrent']
Performance,"Do you manually download the files in src/main/resources/large from git-lfs, you need those in order to perform the build. I'm not sure a good way to get those without using git. You could of course pull them and then tar them though.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6395#issuecomment-584454437:104,perform,perform,104,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6395#issuecomment-584454437,1,['perform'],['perform']
Performance,"Do you mean in production? In the ~2.92Gbp of the WGS calling regions, I see all ~2.44Gbp of the GIAB HCR, as well as ~485Mbp / ~778Mbp of the GIAB ""LCR"" (naively, just the complement of the HCR). Or did you mean in another context?. I'm kicking off runs with the CHM now. Hopefully, optimizations of sensitivity outside the CHM HCR will be more meaningful, since I see a decent amount of calls outside of it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-713785389:284,optimiz,optimizations,284,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-713785389,1,['optimiz'],['optimizations']
Performance,Doing so comes at a significant performance cost that we have to consider.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/312#issuecomment-82518368:32,perform,performance,32,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/312#issuecomment-82518368,1,['perform'],['performance']
Performance,"Don't mean to derail the conversation, but I just wanted to chime in about this:; > A good setting for `--nativePairHmmThreads` is probably 4-8, you won't see any improvement after that. This doesn't sound right. We have informally tested how the performance of PHMM changes with this parameter, and we've observed roughly linear performance improvement as you increase the value of `nativePairHmmThreads`, up to the number of physical cores in the system. These performance tests were done a few months ago, mind you, but I can't think of anything that would have altered this behavior since then, at least in the GKL.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3631#issuecomment-332916917:247,perform,performance,247,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3631#issuecomment-332916917,3,['perform'],['performance']
Performance,"Done, the problematic commit was https://github.com/broadinstitute/gatk/commit/a304725a60f5000ec6381040137043a557fc3dc1. Below are the results with versions suggested by git bisect.; Calling performance was evaluated with chr22 in the query vcf and full truth vcf, therefore the calculation of the recall is invalid (but irrelevant for the purpose):; ![FD_TN_4181_chr22_FD_TN_-4 1 8 1-28-g6d8cdfc_chr22_FD_TN-4 1 8 1-42-g851c840_chr22_FD_TN-4 1 8 1-49-g4e8b73e_chr22_FD_TN-4 1 8 1-50-ga304725_chr22_FD_TN-4 1 8 1-51-g66570bf_chr22_FD_TN-4 1 8 1-52-g1238565_chr22_FD_TN_4190_chr22](https://user-images.githubusercontent.com/15612230/236503899-e922abb0-5d9f-44d3-bf44-a801652744ea.png). As stated in the pull request https://github.com/broadinstitute/gatk/pull/6821, the change was evaluated with the DREAM3 benchmark and made sense at the time. To be fair, in the HCC1395 benchmark there is still a gain in recall (<2%, see the first figure in my original report above), which I assume to be mostly due to this commit, but the recall/precision tradeoff looks different now with the better benchmark callset. I have not figured out what exactly the code in the commit does, maybe we can fine-tune the changes @davidbenjamin ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1536484624:191,perform,performance,191,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1536484624,2,"['perform', 'tune']","['performance', 'tune']"
Performance,"Done. Full results are on the internal presentation slides. The summary table follows:. | speedup vs async on a 1-core machine | slice | whole | intervals |; |-------------|----------|---------|----------|; | vcf | 0.91| 1.40| 0.57|; | bam (exome)| 40.42| 1.06| 1.02|; | bam (wgs)| 111.18| 1.21| 0.99|. This table compares the execution time of a single machine running PrintReads or SelectVariants, getting its input either directly from the Google bucket (NIO), or by first copying it with gsutil and then running off the local disk (with the async option turned on, allowing eager decompression of the stream - a feature the NIO code does not have). Each experiment is run four times, and each number here represents the ratio of two such experiments. Numbers larger than 1 indicate that NIO was faster. Each row is a different input file: vcf, small bam (exome), large bam (whole genome). Each column is a selection of what to read from the file (via the `-L` argument): a megabase slice, the whole file, or a long list of intervals. The NIO code relies heavily on prefetching, so it doesn't perform well with the many disjoint accesses of the right column. When processing only a small part of the (already small) vcf file, NIO loses out to copy + local processing. Everywhere else the direct-to-bucket ""NIO"" code performs quite well, up to 111x faster than the ""copy then process"" approach. I also ran the full set with async disabled. It makes a difference but NIO still wins and loses at the same places by similar margins (in particular the 111x win remains).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2424#issuecomment-284056956:1096,perform,perform,1096,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2424#issuecomment-284056956,2,['perform'],"['perform', 'performs']"
Performance,"Dsamjdk.compression_level=1 -Dsnappy.disable=true --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.di$able=true --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 --conf spark.cores.max=720 --executor-cores 20 --executor-memory 50g --conf spark.driver.memory=50g /home/myname/gatk4/gatk$build/libs/gatk-package-4.alpha.2-1125-g27b5190-SNAPSHOT-spark.jar BwaAndMarkDuplicatesPipelineSpark -I hdfs://ln16/user/myname/NA12878/wes/NA12878-NGv3-LAB1360-A.unaligned.bam -O hdfs://ln16/user/myname/gatk4test/B$aAndMarkDuplicatesPipelineSpark_out.bam -R hdfs://ln16/user/myname/genomes/Hsapiens/GRCh37/seq/GRCh37.2bit --bwamemIndexImage /hadoop/myname/GRCh37.fa.img --disableSequenceDictionaryValidation --sparkMaster spark://$n16:7077; 16:55:20.195 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/myname/gatk4/gatk/build/libs/gatk-package-4.alpha.2-1125-g27b5190-SNAPSHOT-spark.jar!/com/intel/gkl/native/libgkl_compression.so; [June 29, 2017 4:55:20 PM CST] BwaAndMarkDuplicatesPipelineSpark --bwamemIndexImage /hadoop/myname/GRCh37.fa.img --output hdfs://ln16/user/myname/gatk4test/BwaAndMarkDuplicatesPipelineSpark_out.bam --reference hdfs$//ln16/user/myname/genomes/Hsapiens/GRCh37/seq/GRCh37.2bit --input hdfs://ln16/user/myname/NA12878/wes/NA12878-NGv3-LAB1360-A.unaligned.bam --disableSequenceDictionaryValidation true --sparkMaster spark://ln16:7077 --duplicates_scoring_strategy SUM_OF_BASE_QUALITIES --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --shardedOutput false --numReducers 0 --help fal$e --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --disableToolDefaultReadFilters fals",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312229998:2468,Load,Loading,2468,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312229998,1,['Load'],['Loading']
Performance,"Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx50G -jar /home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar HaplotypeCaller -R ref.fa -I mybam.bam -O mycalls.vcf.gz -L snps.vcf -ip 100; >; > 16:17:04.377 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; >; > 16:17:04.397 **WARN** NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (/tmp/libgkl_compression3825249225068031371.so: /tmp/libgkl_compression3825249225068031371.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > 16:17:04.402 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; >; > 16:17:04.407 **WARN** NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (/tmp/libgkl_compression7506152962158874866.so: /tmp/libgkl_compression7506152962158874866.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > Sep 04, 2020 4:17:05 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; >; > INFO: Failed to detect whether we are running on Google Compute Engine.; >; > 16:17:05.842 INFO HaplotypeCaller - ------------------------------------------------------------; >; > 16:17:05.843 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.1.8.1; >; > 16:17:05.843 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; >; > 16:17:05.843 INFO HaplotypeCaller - Executing as robert@powerlinux on Linux v4.4.0-184-generic ppc64le; >; > 16:17:05.843 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v1.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600:2175,load,load,2175,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600,1,['load'],['load']
Performance,"EF ALT QUAL FILTER INFO FORMAT CGAAGAGGTAGGTGCGAG-1; chr19 55910646 . AC A,<NON_REF> 352.6 . . GT:AD:DP:GQ:PGT:PID:PS 0|1:6,9,0:15:99:0|1:55910646_AC_A:55910646; chr19 55910648 . AAATCCCCC A,<NON_REF> 352.6 . . GT:AD:DP:GQ:PGT:PID:PS 0|1:6,9,0:15:99:0|1:55910646_AC_A:55910646; chr19 55910653 . CCCCAT *,C,<NON_REF> 227.84 . . GT:AD:DP:GQ:PGT:PID:PS 2|1:0,9,6,0:15:99:1|0:55910646_AC_A:55910646; chr19 55910675 . T C,<NON_REF> 30.64 . . GT:AD:DP:GQ 0/1:13,2,0:15:38; ```. ```; Using GATK jar /omics/groups/OE0540/internal/software/jvm/gatk/4.4.0.0/gatk-package-4.4.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /omics/groups/OE0540/internal/software/jvm/gatk/4.4.0.0/gatk-package-4.4.0.0-local.jar GenotypeGVCFs -R chr19.fa -V output.g.vcf -O output.vcf; Picked up JAVA_TOOL_OPTIONS: -Djava.net.useSystemProxies=true; 13:40:59.573 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/omics/groups/OE0540/internal/software/jvm/gatk/4.4.0.0/gatk-package-4.4.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 13:40:59.636 INFO GenotypeGVCFs - ------------------------------------------------------------; 13:40:59.653 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.4.0.0; 13:40:59.653 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:40:59.654 INFO GenotypeGVCFs - Executing as gleixner@odcf-worker02 on Linux v3.10.0-1160.76.1.el7.x86_64 amd64; 13:40:59.654 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v17+35-2724; 13:40:59.655 INFO GenotypeGVCFs - Start Date/Time: October 26, 2023 at 1:40:59 PM CEST; 13:40:59.655 INFO GenotypeGVCFs - ------------------------------------------------------------; 13:40:59.655 INFO GenotypeGVCFs - ------------------------------------------------------------; 13:40:59.658 INFO GenotypeGVCFs - HTSJDK Version: 3.0",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5727#issuecomment-1781017195:17114,Load,Loading,17114,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5727#issuecomment-1781017195,1,['Load'],['Loading']
Performance,"EVEL : 2; > 17 15:07:52.441 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; > 18 15:07:52.441 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; > 19 15:07:52.442 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; > 20 15:07:52.442 INFO Mutect2 - Deflater: IntelDeflater; > 21 15:07:52.442 INFO Mutect2 - Inflater: IntelInflater; > 22 15:07:52.442 INFO Mutect2 - GCS max retries/reopens: 20; > 23 15:07:52.443 INFO Mutect2 - Requester pays: disabled; > 24 15:07:52.443 INFO Mutect2 - Initializing engine; > 25 15:07:52.848 INFO FeatureManager - Using codec VCFCodec to read file file://ref_nobackup/af-only-gnomad.hg38.vcf.gz; > 26 15:07:53.126 INFO Mutect2 - Done initializing engine; > 27 15:07:53.196 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/scicore/soft/apps/GATK/4.4.0.0-GCCcore-10.3.0-Java-17/gatk-package-4.4.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; > 28 15:07:53.201 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/scicore/soft/apps/GATK/4.4.0.0-GCCcore-10.3.0-Java-17/gatk-package-4.4.0.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; > 29 15:07:53.223 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; > 30 15:07:53.223 INFO IntelPairHmm - Available threads: 2; > 31 15:07:53.224 INFO IntelPairHmm - Requested threads: 4; > 32 15:07:53.224 WARN IntelPairHmm - Using 2 available threads, but 4 were requested; > 33 15:07:53.224 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; > 34 15:07:53.231 WARN Mutect2 - Note that the Mutect2 reference confidence mode is in BETA -- the likelihoods model and output format are subject to change in subsequent versions.; > 35 15:07:53.314 INFO ProgressMeter - Starting traversal; > 36 15:07:53.314 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; > 37 15:07:54.410 INFO VectorLoglessPairHMM - Time spent in setup for",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7849#issuecomment-1597198632:3105,Load,Loading,3105,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7849#issuecomment-1597198632,1,['Load'],['Loading']
Performance,E_FOR_TRIBBLE : false; 11:47:51.057 INFO Mutect2 - Deflater: IntelDeflater; 11:47:51.057 INFO Mutect2 - Inflater: IntelInflater; 11:47:51.057 INFO Mutect2 - GCS max retries/reopens: 20; 11:47:51.057 INFO Mutect2 - Requester pays: disabled; 11:47:51.057 INFO Mutect2 - Initializing engine; 11:47:51.372 INFO FeatureManager - Using codec VCFCodec to read file file:///home/proj/stage/cancer/reference/GRCh37/variants/dbsnp_grch37_b138.vcf.gz; 11:47:51.457 INFO FeatureManager - Using codec BEDCodec to read file file:///home/proj/stage/cancer/reference/target_capture_bed/production/balsamic/gicfdna_3.1_hg19_design.bed; 11:47:51.465 INFO IntervalArgumentCollection - Processing 74592 bp from intervals; 11:47:51.474 INFO Mutect2 - Done initializing engine; 11:47:51.487 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/proj/bin/conda/envs/D_UMI_APJ/share/gatk4-4.1.8.0-0/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 11:47:51.489 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/proj/bin/conda/envs/D_UMI_APJ/share/gatk4-4.1.8.0-0/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 11:47:51.534 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; 11:47:51.534 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 11:47:51.534 INFO IntelPairHmm - Available threads: 16; 11:47:51.534 INFO IntelPairHmm - Requested threads: 4; 11:47:51.534 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 11:47:51.557 INFO ProgressMeter - Starting traversal; 11:47:51.557 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 11:47:52.683 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 0.0; 11:47:52.683 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 0.0; 11:47:52.683 INFO SmithWatermanAligner - Total compute time in java Smith-Waterman : 0.24 sec; 11:47:52.684 INFO Mutect,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-652912482:3853,Load,Loading,3853,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-652912482,1,['Load'],['Loading']
Performance,"Each scatter of ModelSegments will run as before, aside from skipping the segmentation step in favor of using the joint segmentation. We will repeat the het-genotyping step, but this is cheap and it's probably better to repeat it to make sure filtering is applied consistently. It would also require more changes to the command line to specify where to output the hets for each sample during multisample segmentation and to skip genotyping in each scatter, if we were to go that route. There are many possible combinations of inputs that need to be tested, but the same is already true of the current ModelSegments. Furthermore, there are slight wrinkles when running in tumor-only mode (i.e., when `--normal-allelic-counts` are not available). Because each sample is genotyped indiviudally, each may yield a different set of hets (in contrast to genotyping in matched-normal mode, in which the normal determines the set of hets used in all samples). We will thus have to take the intersection of these hets before performing multisample segmentation. Unfortunately, we will not be able to re-perform this intersection in each scatter, since we will no longer have access to the hets from the other samples. However, we *will* ultimately intersect the hets from each sample with the joint segmentation before modeling, which may be a rough proxy for the intersection of hets from all samples. As always, tumor-only mode may yield suboptimal results in certain scenarios, e.g., high purity CNLOH. I think I'm OK with just documenting these wrinkles, rather than working too hard to iron them out. I think this structure sets us up nicely to accommodate germline tagging/filtering in the near future. We can still pass the Picard interval list containing the joint segmentation to the scatter for the normal, but can instead subsequently pass the *.modelBegin.seg result from the normal to the tumors. This modeled-segment file will have breakpoints identical to those from the joint segmentation (as op",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6499#issuecomment-607313549:3026,perform,performing,3026,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6499#issuecomment-607313549,1,['perform'],['performing']
Performance,"Essentially using preclustering of samples (e.g., by performing clustering on a subset of the coverage bins) to automate building of multiple PoNs when appropriate. Proof-of-principle was implemented for germline (#5633) in https://github.com/broadinstitute/gatk-evaluation/tree/sl_mega_wdl, but it hasn't been merged yet nor do we have plans to put it in production or heavy use anytime soon---sticking with manual preclustering when needed, for now.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5634#issuecomment-525798707:53,perform,performing,53,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5634#issuecomment-525798707,1,['perform'],['performing']
Performance,"Evaluation of THCA/STAD/LUAD TCGA WGS/WES CR concordance with SNP arrays was implemented on FC last summer and showed good performance. For WES, comparisons against GATK CNV and CODEX showed comparable to highly improved performance, respectively, with minimal parameter tuning. WGS comparisons were unavailable due to limitations of competing tools. This evaluation will be expanded to include CR/MAF concordance against PanCanAtlas ABSOLUTE results. Some curation of the samples could be performed; some batch effects were observed in some LC WGS LUAD samples. Comparisons to other tools will probably be removed for ease of maintenance. Will be adapted to fit into whatever framework arises from #4630; same goes for HCC1143 and CRSP validations.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4122#issuecomment-459833697:123,perform,performance,123,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4122#issuecomment-459833697,3,['perform'],"['performance', 'performed']"
Performance,"Even if we had default methods, `Locatable` should be simple (like `Comparable`) and shouldn't be polluted with every possible operation you might want to perform on an interval.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/305#issuecomment-79198026:155,perform,perform,155,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/305#issuecomment-79198026,1,['perform'],['perform']
Performance,"Even without any `-L` intervals, running `CountReads` on the cram is 3x slower than running it on the bam, so the performance problems are not confined to the interval-parsing code.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1787#issuecomment-215521825:114,perform,performance,114,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1787#issuecomment-215521825,1,['perform'],['performance']
Performance,"Excellent, thank you @davidbenjamin and @ddrichel!. 1. Since it has been three weeks, are there any news on whether the regenerated PoN works, and where from it is available?; 2. Also, since there are likely many people (some of which have spoken up in the thread) who have been using the affected releases throughout the years, possibly affecting patients' diagnoses, does the Broad have a communication plan to notify Mutect2 users of this serious performance degradation in the specific cases you outlined?; 3. Last, are there plans to incorporate fully functional benchmark regression tests such as @ddrichel's into the Mutect2 (and later Mutect3) CI pipeline, or at least into the build process for each version that is released to the public?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1225309315:450,perform,performance,450,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1225309315,1,['perform'],['performance']
Performance,"Executive summary: . My main concern is that the amount of unsupported code is continuing to grow. Adding this PR would bring the total to about ~5k lines of WDL, Java, and test code. In comparison, the amount of corresponding supported CNV code clocks in around ~33k---this includes all of gCNV, as well! Development time has also been non-negligible and dates back to pre-4.0 release. Another concern is that the number of users of this unsupported code is also growing. In fact, it seems like we are actively pointing users to it. This seems unsustainable going forward. Finally, I don't think we have satisfactorily demonstrated which of the functions accomplished by this code (format conversion, post-hoc blacklisting, germline/""CNLOH"" tagging and imputation) are necessary or cannot be performed by existing code or more streamlined and principled methods. (Some of these functions, such as IGV conversion, are already performed by existing code.) Of those functions, I think format conversion is the only one we should retain from this code in an unsupported fashion. So if this PR introduces a useful GISTIC conversion, no harm in merging that. This all sounds like a decision for the new tech lead! @mwalker174 any thoughts? . More detailed responses follow:. > Users are already using this branch and giving me positive feedback (definitely more positive than adjusting num_changepoints_penalty_factor). I suggest merging mostly for practical reasons. It buys us more time to put in a principled solution. And this workflow is clearly marked as an unsupported prototype anyway (as are the GATK CLIs). I want to emphasize that this whole workflow is not a long-term solution. In other words, I would like to get this in and then focus on a supported solution. While it's great that users are giving positive feedback, I refer you to CellBender team's manifesto at https://github.com/broadinstitute/CellBender/commit/28f02f8dbd716aff922bb8da1e56da29347b245b. Can these users help us definitiv",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5450#issuecomment-461431199:793,perform,performed,793,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5450#issuecomment-461431199,2,['perform'],['performed']
Performance,Executor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: Failure while waiting for FeatureReader to initialize with exception: com.google.cloud.storage.StorageException: All 20 reopens failed. Waited a total of 1918000 ms between attempts; at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lambda$getFeatureReadersInParallel$614(GenomicsDBImport.java:605); at java.util.LinkedHashMap.forEach(LinkedHashMap.java:684); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getFeatureReadersInParallel(GenomicsDBImport.java:600); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.createSampleToReaderMap(GenomicsDBImport.java:491); at com.intel.genomicsdb.importer.GenomicsDBImporter.lambda$null$2(GenomicsDBImporter.java:602); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590); ... 3 more; Caused by: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: All 20 reopens failed. Waited a total of 1918000 ms between attempts; at java.util.concurrent.FutureTask.report(FutureTask.java:122); at java.util.concurrent.FutureTask.get(FutureTask.java:192); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lambda$getFeatureReadersInParallel$614(GenomicsDBImport.java:602); ... 8 more; Caused by: com.google.cloud.storage.StorageException: All 20 reopens failed. Waited a total of 1918000 ms between attempts; at com.google.cloud.storage.contrib.nio.CloudStorageRetryHandler.handleReopenForStorageException(CloudStorageRetryHandler.java:124); at com.google.cloud.storage.contrib.nio.CloudStorageRetryHandler.handleStorageException(CloudStorageRetryHandler.java:94); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSyst,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-412904420:2064,concurren,concurrent,2064,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-412904420,1,['concurren'],['concurrent']
Performance,FO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 11:57:50.897 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 11:57:50.897 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 11:57:50.897 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 11:57:50.897 INFO GenomicsDBImport - Deflater: IntelDeflater; 11:57:50.897 INFO GenomicsDBImport - Inflater: IntelInflater; 11:57:50.897 INFO GenomicsDBImport - GCS max retries/reopens: 20; 11:57:50.898 INFO GenomicsDBImport - Requester pays: disabled; 11:57:50.898 INFO GenomicsDBImport - Initializing engine; 11:57:52.588 INFO FeatureManager - Using codec BEDCodec to read file file:///mnt/isilon/experiment/resources/final_mm10_exon.bed; 11:57:53.791 INFO IntervalArgumentCollection - Processing 199895151 bp from intervals; 11:57:53.825 WARN GenomicsDBImport - A large number of intervals were specified. Using more than 100 intervals in a single import is not recommended and can cause performance to suffer. It is recommended that intervals be aggregated together.; 11:57:53.837 INFO GenomicsDBImport - Done initializing engine; 11:57:54.037 INFO GenomicsDBImport - Vid Map JSON file will be written to /mnt/isilon/experiment/18075-01/aligned/variant_calling/genomics_db/vidmap.json; 11:57:54.037 INFO GenomicsDBImport - Callset Map JSON file will be written to /mnt/isilon/experiment/18075-01/aligned/variant_calling/genomics_db/callset.json; 11:57:54.037 INFO GenomicsDBImport - Complete VCF Header will be written to /mnt/isilon/experiment/18075-01/aligned/variant_calling/genomics_db/vcfheader.vcf; 11:57:54.037 INFO GenomicsDBImport - Importing to array - /mnt/isilon/experiment/18075-01/aligned/variant_calling/genomics_db/genomicsdb_array; 11:57:54.038 INFO ProgressMeter - Starting traversal; 11:57:54.038 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 12:05:27.926 INFO GenomicsDBImport - Starting batch inpu,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5342#issuecomment-453590820:3182,perform,performance,3182,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5342#issuecomment-453590820,1,['perform'],['performance']
Performance,"FO HaplotypeCaller - Using google-cloud-java patch c035098b5e62cb4fe9155eff07ce88449a361f5d from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 14:50:16.819 INFO HaplotypeCaller - Initializing engine; 14:50:18.950 INFO IntervalArgumentCollection - Processing 83257441 bp from intervals; 14:50:18.965 INFO HaplotypeCaller - Done initializing engine; 14:50:19.021 INFO HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output; 14:50:19.280 WARN PossibleDeNovo - Annotation will not be calculated, must provide a valid PED file (-ped) from the command line.; 14:50:19.481 WARN PossibleDeNovo - Annotation will not be calculated, must provide a valid PED file (-ped) from the command line.; 14:50:19.776 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/humgen/gsa-hpprojects/GATK/gatk4/gatk-4.beta.5/gatk-package-4.beta.5-local.jar!/com/intel/gkl/native/libgkl_utils.so; 14:50:19.795 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/humgen/gsa-hpprojects/GATK/gatk4/gatk-4.beta.5/gatk-package-4.beta.5-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 14:50:19.847 WARN IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 14:50:19.848 INFO IntelPairHmm - Available threads: 48; 14:50:19.848 INFO IntelPairHmm - Requested threads: 4; 14:50:19.848 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 14:50:19.926 INFO ProgressMeter - Starting traversal; 14:50:19.926 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 14:50:30.309 INFO ProgressMeter - chr17:740224 0.2 3010 17395.5; 14:50:41.016 INFO ProgressMeter - chr17:1675683 0.4 7020 19973.4; 14:50:51.041 INFO ProgressMeter - chr17:2415218 0.5 10100 19477.4; 14:51:01.041 INFO ProgressMeter - chr17:3591332 0.7 14920 21773.6; 14:51:11.059 INFO ProgressMeter - chr17:4574538 0.9 19100 22412.6; 14:51:21.089 INFO ProgressMeter - chr17:5",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3154#issuecomment-334840678:6822,Load,Loading,6822,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3154#issuecomment-334840678,1,['Load'],['Loading']
Performance,"FYI @sooheelee I pointed you at the docs recently, but realized they're slightly out of date. CreatePanelOfNormals currently expects proportional coverage, upon which it initially performs a series of filtering steps. The docs state that these steps are performed on integer counts, which is incorrect. The fact that filtering yields different post-filter coverage for the two types of input ultimately results in slightly different denoising. Not a big deal, but we should decide what the actual method should be doing and clarify in the code/docs.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-314529956:180,perform,performs,180,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-314529956,2,['perform'],"['performed', 'performs']"
Performance,"Fair points. I agree that legacy tools/versions that are part of a canonical or relatively widely used pipeline should have good documentation. However, there are many of the CNV tools that are basically prototypes---they have never been part of a pipeline, have no tutorial materials, and the chances that any external users have actually used them are probably extremely low. The sooner they are deprecated, the less the overall burden on both comms and methods---I don't think comms should need to feel protective of code or tools that developers are willing to scrap wholesale!. I'd like to cordon off or hide such tools so the program group doesn't get too cluttered---if we can do this in a way that doesn't require @cmnbroad to add more categories, that would be great. For example, we will have 5 tools that one might reasonably try to use for segmentation (PerformSegmentation, ModelSegments, PerformAlleleFractionSegmentation, PerformCopyRatioSegmentation, and PerformJointSegmentation). The first two are part of the legacy and new pipelines, respectively, but the last 3 were experimental prototypes. I think it's definitely confusing to have these 3 presented in the program group, and treating them the same as the other tools in terms of documentation is just extra work for everyone. In any case, I definitely think an additional program group to separate the legacy and new tools is warranted, since many of the updated tools in the new pipeline have very similar names to the legacy tools. If this is OK with everyone, I'll just add a ""LegacyCopyNumber"" program group, which I don't think should require extra work on anyone else's part.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-346187604:866,Perform,PerformSegmentation,866,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-346187604,4,['Perform'],"['PerformAlleleFractionSegmentation', 'PerformCopyRatioSegmentation', 'PerformJointSegmentation', 'PerformSegmentation']"
Performance,FeatureDataSource.java:326); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:282); 	at org.broadinstitute.hellbender.engine.VariantLocusWalker.initializeDrivingVariants(VariantLocusWalker.java:76); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.initializeFeatures(VariantWalkerBase.java:67); 	at org.broadinstitute.hellbender.engine.GATKTool.onStartup(GATKTool.java:709); 	at org.broadinstitute.hellbender.engine.VariantLocusWalker.onStartup(VariantLocusWalker.java:63); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: java.io.IOException: GenomicsDB JNI Error: VariantQueryProcessorException : Could not open array 3$1$121351753 at workspace: /data1/EquCab/GenomicsDB/ECA3_GenomicsDB_260/3; TileDB error message : [TileDB::BookKeeping] Error: Cannot load book-keeping; Reading MBR failed; 	at org.genomicsdb.reader.GenomicsDBQueryStream.jniGenomicsDBInit(Native Method); 	at org.genomicsdb.reader.GenomicsDBQueryStream.<init>(GenomicsDBQueryStream.java:209); 	at org.genomicsdb.reader.GenomicsDBQueryStream.<init>(GenomicsDBQueryStream.java:182); 	at org.genomicsdb.reader.GenomicsDBQueryStream.<init>(GenomicsDBQueryStream.java:91); 	at org.genomicsdb.reader.GenomicsDBFeatureReader.generateHeadersForQuery(GenomicsDBFeatureReader.java:200); 	at org.genomicsdb.reader.GenomicsDBFeatureReader.<init>(GenomicsDBFeatureReader.java:85); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getGenomicsDBFeatureReader(FeatureDataSource.java:407); 	... 12 more,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-755760402:8446,load,load,8446,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-755760402,2,['load'],['load']
Performance,Filed #2331 to add prefetching. As expected this has a big positive impact on performance.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2224#issuecomment-270988561:78,perform,performance,78,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2224#issuecomment-270988561,1,['perform'],['performance']
Performance,"Finally ready to close for two further reasons:. * After David R taught me how to use the profiler I was surprised to see that assembly in M2 is twice as expensive as pairHMM, so post-assembly optimizations aren't so valuable.; * Any issue of spurious or wasteful haplotypes could be addressed more elegantly using a string graph for assembly.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2945#issuecomment-381082611:193,optimiz,optimizations,193,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2945#issuecomment-381082611,1,['optimiz'],['optimizations']
Performance,"Fine but this is clearly premature optimization. How about a class called Intervals or intervalutils for this sort of random; utility ?. On Thursday, February 18, 2016, droazen notifications@github.com wrote:. > @akiezun https://github.com/akiezun What will actually happen is that; > someone will need that functionality months from now, forget that it; > already exists (embedded in some random tool), and re-implement it. It; > should be moved back now before this is allowed to happen.; > ; > —; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/gatk/pull/1497#issuecomment-185886728. ## . Sent from Gmail Mobile",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1497#issuecomment-185888065:35,optimiz,optimization,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1497#issuecomment-185888065,1,['optimiz'],['optimization']
Performance,"Finished, [2D CNN inference](https://github.com/broadinstitute/gatk/blob/master/src/main/java/org/broadinstitute/hellbender/tools/walkers/vqsr/CNNScoreVariants.java) and training merged, many new issues spawned.:; - [X] A cool(?) name CNNScoreVariants; - [x] Model training script (in Python, eventually in Java); - [x] Pretrained model for WGS; - [x] Pretrained model for WEx (still being validated and was only trained on NA12878); - [x] Model inference and VCF annotation (in Java); - [x] Solution for applying filters based on CNN score cutoff (tranches.py script); - [x] Currently just re-filtering. Still no joint calling solution...; - [x] Hyperparameters optimized for small 2d model similar performance but .25 params.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4225#issuecomment-377623706:663,optimiz,optimized,663,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4225#issuecomment-377623706,2,"['optimiz', 'perform']","['optimized', 'performance']"
Performance,"Finishing up a tieout using old MalariaGEN Pf7 CNV genotyping runs, and I think things look good from this perspective, at least. This tieout uses a subset of the Pf7 samples containing 300 cohort and 1683 case samples (which were indeed treated as a cohort-case cluster in the original Pf7 CNV genotyping analysis). ~4k genomic bins are covered. We compare this branch against 4.5.0.0, as well as this branch against itself (checking for reproducibility). Costs for this branch ($10.92) and 4.5.0.0 ($10.96) were quite comparable. Note that a small portion of these costs derives from Pf7-specific genotyping steps, which I did not bother to remove from the workflow. Runtime for the ploidy modeling and postprocessing steps were comparable. Interestingly, **runtime for the gCNV was ~20-25% longer with this branch than with 4.5.0.0, but memory usage fell by a factor of ~3 (~6GB to ~2GB)!** I am not sure if we could recoup the runtime with some more tweaking of the environment (perhaps double checking that optimized BLAS/MKL/etc. packages are properly used, changing environment variables/flags, etc.), but I think the decrease in memory usage is quite nice. Concordance was checked for the following quantities (4.5.0.0 is on the x-axis and this branch is on the y-axis in all plots below):. 1) Variational posterior means (`mu_*`) and standard deviations (`std_*`) for all analogous variables in the ploidy and gCNV models. There were some slight changes to the gCNV model in this branch (e.g., the functional form of the ARD prior was changed), which means some variables are no longer directly comparable. Furthermore, some variables (such as the bias factors W) are degenerate and cannot be immediately compared. Otherwise, there is good concordance between the remaining variables, e.g.:. ![image](https://github.com/broadinstitute/gatk/assets/11076296/614cf501-ca31-4199-badb-3194b7f78154); ![image](https://github.com/broadinstitute/gatk/assets/11076296/f615084d-d0bf-44e9-bcf5-98abd26ce",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8561#issuecomment-2079695268:1012,optimiz,optimized,1012,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8561#issuecomment-2079695268,1,['optimiz'],['optimized']
Performance,"First comment, I would like to see what happens when there are 2 concurrent failing commits being run by travis.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6247#issuecomment-550468443:65,concurren,concurrent,65,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6247#issuecomment-550468443,1,['concurren'],['concurrent']
Performance,"For @davidadamsphd to review, please. If @jean-philippe-martin could also review the modifications to the dataflow BQSR, it would be much appreciated (keeping in mind that we still need to rebase on top of the optimized BQSR branch).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/911#issuecomment-142118507:210,optimiz,optimized,210,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/911#issuecomment-142118507,1,['optimiz'],['optimized']
Performance,For @lbergelson. The general plan for HaplotypeCaller work in Q2 is:. -I will work on validation/testing of the walker version; -Louis will take the Spark version; -Adam will work on further HC performance optimizations (potentially for both walker and spark),MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1639#issuecomment-202447009:194,perform,performance,194,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1639#issuecomment-202447009,2,"['optimiz', 'perform']","['optimizations', 'performance']"
Performance,"For Mark Duplicates, the timings after the latest Spark improvements (like https://github.com/broadinstitute/gatk/pull/1190) are; - Walker: 18s; - Spark: 36s (sharded BAM, all cores), 55s (sharded BAM, 1 core), 93s (single BAM, all cores), 105s (single BAM, 1 core). I'm going to close this now since the runner itself is OK (and as I said before is not going to match hand tuned implementations typically). We can make further improvements to the different algorithms, e.g. in https://github.com/broadinstitute/gatk/issues/1100.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1047#issuecomment-159577490:374,tune,tuned,374,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1047#issuecomment-159577490,1,['tune'],['tuned']
Performance,"For evaluations, we don't look at calls outside the high confidence regions. For production we deliver calls over the whole genome (except for Y in WGS, which is another story). Even for CHM, I wouldn't optimize outside the high confidence regions. For example, seg dupes are excluded from the high confidence regions. There likely will be calls there, but there's no guarantee that even a real variant should map to that copy of the segment. Of course if either @davidbenjamin or @fleharty has a differing opinion I'm open to discussion.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-714493769:203,optimiz,optimize,203,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-714493769,1,['optimiz'],['optimize']
Performance,For improving performance by rapidly joining local sites we could do something like:. PCollection<Event> -> key event by which 10kb window of the genome it falls into ; keyed events -> Combine.PerKey into interval trees; combined keyed events -> drop keys; Combine.globally the interval trees into a single interval tree,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/469#issuecomment-97176875:14,perform,performance,14,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/469#issuecomment-97176875,1,['perform'],['performance']
Performance,"For production we use a different execution engine (Zamboni, not Queue). The number of scatters is a static part of the workflow and Zamboni has the responsibility of splitting the interval list per scatter.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2326#issuecomment-270436261:65,Queue,Queue,65,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2326#issuecomment-270436261,1,['Queue'],['Queue']
Performance,"For record keeping, as the comments and replies may be buried in the many commits. ------------; ### On the problem of too many splits of RDD and performance concerns. Initial comment by @cwhelan :; > I'm starting to really not like this approach of splitting up the RDD into lots of smaller RDDs for later processing. It seems inefficient to me: it launches tons of different Spark stages each of which has a bunch of overhead. Perhaps not in this PR, but I think it would be better to classify the contigs on the fly and dispatch them to the right processing methods in a single pass over the RDD. Reply by @SHuang-Broad. > I tried to fix it in this PR, but that seems to be a big task,; and probably is impossible to achieve in a single pass,; because currently each class of contig ends up producing a different type of object; (3 general classes: simple -> SimpleNovelAdjacency, complex -> ComplexVariantCanonicalRepresentation, and unknown -> SAM records of the contigs); and a groupBy() operation is necessary in the middle using these objects as keys; due to the fact that different contigs may produce the same variant; So what I'm thinking about, is two pass:; one pass for splitting them up into the 3 classes,; then another pass on each of those 3 RDD's to turn them into VariantContext's.; Any better idea?. Reply by @cwhelan ; > That would be better, and yeah you don't have to do it in this PR.; In theory you could make the keys for the groupByKey() (ie NovelAdjacencyAndAltHaplotype, CpxVariantCanonicalRepresentation, right?) all inherit from the same superclass and do a single group by, couldn't you? Then you could do everything in a single pass. Reply by @SHuang-Broad; > Yes, that is what I'm planning but I'm not sure yet about how to approach that (I actually tried it, before putting in the above comment, and quickly ran into the problem of mixing Java serialization and Kryo serialization, so a larger re-structuring might be needed, and not just a inheritance structure). ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4663#issuecomment-387899030:146,perform,performance,146,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4663#issuecomment-387899030,1,['perform'],['performance']
Performance,"For reference, plots of memory usage for a 50x10000 shard:. ![gcnv-10k-mem](https://user-images.githubusercontent.com/11076296/53906618-6be46100-4019-11e9-8d70-5971fd5d2e35.png). It appears that the 0.9.0 leak only occurs during denoising + sampling. Not sure if differences in runtime are indicative of OpenBLAS vs. MKL performance or within the noise.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5764#issuecomment-470236552:321,perform,performance,321,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5764#issuecomment-470236552,1,['perform'],['performance']
Performance,"For some performance comparisons with a small dataset:. Smith-Waterman runtime without AVX: `256.88`s; Smith-Waterman runtime with AVX: `34.09`s (`7.5`x faster 😱). It's not surprising, but it is a huge difference.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8485#issuecomment-1684147845:9,perform,performance,9,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8485#issuecomment-1684147845,1,['perform'],['performance']
Performance,"For the following variants: . `chr1 819955 . TTCACGAATT T . PASS .`; `chr1 819979 . CATGAGCAT C . PASS .`. _VEP_ seems to produce **incorrect** data: ; ```; ##fileformat=VCFv4.1; ##VEP=""v94"" time=""2018-10-30 19:13:50"" cache=""/nfs/public/release/ensweb-data/latest/tools/grch37/e94/vep/cache/homo_sapiens/94_GRCh37"" db=""homo_sapiens_core_94_37@hh-mysql-ens-grch37-web"" 1000genomes=""phase3"" COSMIC=""81"" ClinVar=""201706"" ESP=""20141103"" HGMD-PUBLIC=""20164"" assembly=""GRCh37.p13"" dbSNP=""150"" gencode=""GENCODE 19"" genebuild=""2011-04"" gnomAD=""170228"" polyphen=""2.2.2"" regbuild=""1.0"" sift=""sift5.2.2""; ##INFO=<ID=CSQ,Number=.,Type=String,Description=""Consequence annotations from Ensembl VEP. Format: Allele|Consequence|IMPACT|SYMBOL|Gene|Feature_type|Feature|BIOTYPE|EXON|INTRON|HGVSc|HGVSp|cDNA_position|CDS_position|Protein_position|Amino_acids|Codons|Existing_variation|DISTANCE|STRAND|FLAGS|SYMBOL_SOURCE|HGNC_ID|TSL|APPRIS|ENSP|SIFT|PolyPhen|AF|AFR_AF|AMR_AF|EAS_AF|EUR_AF|SAS_AF|AA_AF|EA_AF|gnomAD_AF|gnomAD_AFR_AF|gnomAD_AMR_AF|gnomAD_ASJ_AF|gnomAD_EAS_AF|gnomAD_FIN_AF|gnomAD_NFE_AF|gnomAD_OTH_AF|gnomAD_SAS_AF|CLIN_SIG|SOMATIC|PHENO|PUBMED|MOTIF_NAME|MOTIF_POS|HIGH_INF_POS|MOTIF_SCORE_CHANGE"">; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO; chr1	819955	.	TTCACGAATT	T	.	PASS	CSQ=-|splice_acceptor_variant&coding_sequence_variant&intron_variant|HIGH|AL645608.2|ENSG00000269308|Transcript|ENST00000594233|protein_coding|3/3|2/2|||?-38|?-38|?-13|||||1||Clone_based_ensembl_gene||||ENSP00000470877|||||||||||||||||||||||||||; chr1	819979	.	CATGAGCAT	C	.	PASS	CSQ=-|coding_sequence_variant&3_prime_UTR_variant|MODIFIER|AL645608.2|ENSG00000269308|Transcript|ENST00000594233|protein_coding|3/3||||54-?|54-?|18-?|||||1||Clone_based_ensembl_gene||||ENSP00000470877|||||||||||||||||||||||||||; ```. There is no cDNA string / amino acid change and the positions have `?` characters in them (not 100% sure the question marks are wrong - there is no real spec for these fields). _Oncotator_ seems to produce **incorre",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4307#issuecomment-434440224:218,cache,cache,218,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4307#issuecomment-434440224,2,['cache'],['cache']
Performance,For the new GATK framework the multi-thread support is through Spark (see https://github.com/broadinstitute/gatk/issues/2345 for more details).,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4448#issuecomment-367984760:31,multi-thread,multi-thread,31,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4448#issuecomment-367984760,1,['multi-thread'],['multi-thread']
Performance,"For this ticket, we need to modify `ReferenceUtils.loadFastaDictionary()` to throw a `UserException` if the header returned from its `SAMTextHeaderCodec` contains no sequence dictionary",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2609#issuecomment-305017587:51,load,loadFastaDictionary,51,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2609#issuecomment-305017587,1,['load'],['loadFastaDictionary']
Performance,"For those who might be interested, I finally found a good set up to run gcnvkernel outside a conda environment. Modules loaded: GATK/4.2.4.0 and python/3.8.2. In my python virtualenv:. pip install --no-index --upgrade pip; pip install --no-index --ignore-installed numpy==1.21.0; pip install --no-index scipy==1.2.0; pip install pymc3==3.1; pip install Theano==1.0.4; pip install --no-index tqdm==4.19.5; pip install --no-index PyVCF==0.6.8. git clone https://github.com/broadinstitute/gatk.git; cd gatk/src/main/python/org/broadinstitute/hellbender; python setup_gcnvkernel.py install; python setup.py install",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8387#issuecomment-1613468762:120,load,loaded,120,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8387#issuecomment-1613468762,1,['load'],['loaded']
Performance,For various reasons that backport would be too awful to contemplate even if we wanted to backport. The question remains for GATK4 whether we want to preserve the old qual in case there's some edge case where it performs better. I consider this highly unlikely but others might disagree.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2255#issuecomment-258167008:211,perform,performs,211,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2255#issuecomment-258167008,1,['perform'],['performs']
Performance,"For what it's worth, if we go forward with NIO then we'll need the splitting index and then we should be able to offer this functionality for little additional effort (since we'd be writing a partitioner anyways to work with the splitting index). ; The naive (least-work) approach would read each partition independently so we wouldn't save on reads (but we'd still save from not having to go through a shuffle).; An optimized approach would be to read & parse the overlapping reads only once (since parsing is currently a non-trivial expense), but that'd be more work.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1988#issuecomment-250231487:417,optimiz,optimized,417,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1988#issuecomment-250231487,1,['optimiz'],['optimized']
Performance,From the stack trace I had assumed the error was likely related to loading the dbsnp vcf. From: gs://broad-references/hg38/v0/. I have attached a 1Mbase subset of my gvcf and a script that reliably reproduces the error for me. [script.txt](https://github.com/broadinstitute/gatk/files/2217511/script.txt); [NA12878.hg38.g.vcf.gz](https://github.com/broadinstitute/gatk/files/2217512/NA12878.hg38.g.vcf.gz),MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5024#issuecomment-406906206:67,load,loading,67,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5024#issuecomment-406906206,1,['load'],['loading']
Performance,"From what I have heard from Dmitriy, this loss in Mutect2 precision between Mutect2 v.4.1.8.1. and v.4.1.9.0 seems to be perfectly reproducible using default parameters on the HCC1395 somatic benchmark gold-standard from the [Sequencing Quality Control Phase II Consortium](https://pubmed.ncbi.nlm.nih.gov/?term=Somatic+Mutation+Working+Group+of+Sequencing+Quality+Control+Phase+II+Consortium%5BCorporate+Author%5D). The drop in performance seems to still persist in the current Mutect2 version. If true, then this could have dramatic affects on Mutect2 clinical variant calling quality since v.4.1.9.0 until now. It would appear that isolating the root cause of this drop in performance has high importance for maintaining the clinical validity of Mutect2. It seems quite a number of Mutect2 code details have [changed between v.4.1.8.1. and v.4.1.9.0](https://github.com/broadinstitute/gatk/compare/4.1.8.1...4.1.9.0), as for instance [this commit concerning quality filtering](https://github.com/broadinstitute/gatk/commit/a304725a60f5000ec6381040137043a557fc3dc1), or [this one concerning soft-clipped bases](https://github.com/broadinstitute/gatk/commit/4982c2fa60e89f699a81150116d058aeac2f7573). Perhaps the circumstance of the drop in precision affecting WES but not WGS data may point to the culprit?. Also, may I ask whether the GATK team is doing real-world regression test on gold-standard variant callsets between version releases, and have these tests passed between v.4.1.8.1. and v.4.1.9.0?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1169735509:429,perform,performance,429,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1169735509,2,['perform'],['performance']
Performance,"GCS max retries/reopens: 20; >; > 16:17:05.844 INFO HaplotypeCaller - Requester pays: disabled; >; > 16:17:05.845 INFO HaplotypeCaller - Initializing engine; >; > 16:17:05.928 WARN IntelDeflaterFactory - IntelInflater is not supported, using Java.util.zip.Inflater; >; > 16:17:05.932 WARN IntelDeflaterFactory - IntelInflater is not supported, using Java.util.zip.Inflater; >; > 16:17:06.503 INFO FeatureManager - Using codec VCFCodec to read file file:///home/robert/test/snps.vcf; >; > 16:17:06.539 INFO IntervalArgumentCollection - Processing 61464 bp from intervals; >; > 16:17:06.551 INFO HaplotypeCaller - Done initializing engine; >; > 16:17:06.573 INFO HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output; >; > 16:17:06.588 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; >; > 16:17:06.589 **WARN** NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils347167544598047196.so: /tmp/libgkl_utils347167544598047196.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > 16:17:06.589 **WARN** IntelPairHmm - Intel GKL Utils not loaded; >; > 16:17:06.589 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; >; > 16:17:06.589 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; >; > 16:17:06.590 **WARN** NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils6186849302609329058.so: /tmp/libgkl_utils6186849302609329058.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > 16:17:06.590 **WARN",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600:5178,load,load,5178,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600,1,['load'],['load']
Performance,GJlbmRlci91dGlscy9HQVRLUHJvdGVjdGVkTWF0aFV0aWxzLmphdmE=) | `69.531% <ø> (-13.802%)` | `51 <0> (-8)` | |; | [...stitute/hellbender/utils/icg/DuplicableNumber.java](https://codecov.io/gh/broadinstitute/gatk/pull/3903/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9pY2cvRHVwbGljYWJsZU51bWJlci5qYXZh) | `80% <ø> (ø)` | `5 <0> (?)` | |; | [...bender/tools/exome/NormalizeSomaticReadCounts.java](https://codecov.io/gh/broadinstitute/gatk/pull/3903/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9Ob3JtYWxpemVTb21hdGljUmVhZENvdW50cy5qYXZh) | `79.167% <ø> (ø)` | `6 <0> (ø)` | :arrow_down: |; | [...institute/hellbender/tools/exome/CallSegments.java](https://codecov.io/gh/broadinstitute/gatk/pull/3903/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9DYWxsU2VnbWVudHMuamF2YQ==) | `83.333% <ø> (ø)` | `3 <0> (ø)` | :arrow_down: |; | [...broadinstitute/hellbender/utils/icg/CacheNode.java](https://codecov.io/gh/broadinstitute/gatk/pull/3903/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9pY2cvQ2FjaGVOb2RlLmphdmE=) | `80.645% <ø> (ø)` | `9 <0> (?)` | |; | [...ntationbiasvariantfilter/OrientationBiasUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/3903/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9vcmllbnRhdGlvbmJpYXN2YXJpYW50ZmlsdGVyL09yaWVudGF0aW9uQmlhc1V0aWxzLmphdmE=) | `84.956% <ø> (ø)` | `56 <0> (ø)` | :arrow_down: |; | [...rg/broadinstitute/hellbender/utils/io/IOUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/3903/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9pby9JT1V0aWxzLmphdmE=) | `59.686% <ø> (ø)` | `49 <0> (ø)` | :arrow_down: |; | [...tute/hellbender/tools/exome/TargetTableReader.java](https://codecov.io/gh/broadinstitute/gatk/pull/3903/diff?src=pr&el=tree#diff-c3JjL21haW,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3903#issuecomment-348528911:2713,Cache,CacheNode,2713,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3903#issuecomment-348528911,1,['Cache'],['CacheNode']
Performance,"GenomicsDB loads now, and I can run the tests. The MIN_DP issue is happening because the BCF codec creates the VariantContext with an Integer value for the attribute since it has the type descriptor at hand, whereas the VCF codec just creates it as a String (it doesn't consult the header, which I might call a bug). The comparator doesn't compare these correctly, and ironically, its the string that gets converted to a double. There also seem to be other problematic type differences between the VCs created from BCF vs. VCF. I need to do think a bit about how to properly fix this; I'm not sure just making the comparator more tolerant of these differences is a good idea, since it could wind up masking real problems.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-294962429:11,load,loads,11,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-294962429,1,['load'],['loads']
Performance,"Getting closer; the GenomicsDB tests all pass locally for me now, but we seem to have a few other issues. All of the GenomicsDB tests are failing on travis when trying to load GenomicsDB, and the failure looks somewhat similar to the stack we got when we had the UnsatisfiedLinkException (manifests as java.lang.NoClassDefFoundError: Could not initialize class com.intel.genomicsdb.GenomicsDBImporter). Not sure if its the same thing or not; as I said I can run this branch fine. Maybe the dylib works, but not the so ?. There is a seemingly unrelated unit test failure in AlleleFrequencyCalculatorUnitTest, which I'll look at. Also, we probably shouldn't have the getGCPTestInputPath() calls in static initializers in GenomicsDBImportIntegrationTest, since if it throws it can fail badly (I don't think thats affecting travis though).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-296190917:171,load,load,171,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-296190917,1,['load'],['load']
Performance,"Glad to see we're not the only one's noticing this. We have been using 4.0.11.x for quite a while with success. During a recent revalidation effort, we have been working with 4.2.x versions and missing quite a few calls we consider ""truth"", while making these calls with other variant callers. We have since verified v4.1.8.0 is performing acceptably, so our data supports the issue occurring during the 4.1.9.0 update.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1171550473:329,perform,performing,329,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1171550473,1,['perform'],['performing']
Performance,"Good news. User says this is no longer an issue:. `We found the reason why HaplotypeCaller 4.0.0.0 performed worse than 4.beta.2. We are scattering tools using intervals from a custom BED file. Before, each instance of HaplotypeCaller received a BAM file from ApplyBQSR that was produced using a specific interval, but not the interval itself. This worked for 4.beta.2, but was causing poor performance for 4.0.0.0. We now pass both the BAM and the interval to HaplotypeCaller 4.0.0.0 and it performs just as well as 4.beta.2 with the same amount of memory.`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4169#issuecomment-359944183:99,perform,performed,99,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4169#issuecomment-359944183,3,['perform'],"['performance', 'performed', 'performs']"
Performance,"Good point @akiezun on the array allocation in KMP. Here's a clean room brute force `lastIndexOf`. Initial testing show identical output from HaplotypeCaller and better performance than KMP. I'll push this later today and include the Smith-Waterman profiling code. ``` java; private int lastIndexOf(final byte[] reference, final byte[] query) {; int N = reference.length;; int M = query.length;. for (int i = N - M; i >= 0; i--) {; int j = 0;; while (j < M && reference[i+j] == query[j]) {; j++;; }; if (j == M) {; return i;; }; }. return -1;; }. ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1677#issuecomment-204418438:169,perform,performance,169,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1677#issuecomment-204418438,1,['perform'],['performance']
Performance,"Great! And yes, LL will be optimized separately for SNPs and INDELs. How about this for a first workflow to target?. 1) Run ExtractVariantAnnotations on a training set of chromosomes. You can keep training/truth labels as in Best Practices, for now.; 2) Run TrainVariantAnnotationsModel on that. We'll use the truth scores generated here for any sensitivity conversions---i.e., we'll be calibrating scores only to the truth sites that are contained in the training set of chromosomes.; 3) Use the trained model to run a single shard of ScoreVariantAnnotations on a validation set of chromosomes.; 4) Run some variation of the above script on the resulting outputs to determine SNP and INDEL score thresholds for optimizing the corresponding LL scores. We can also add some code to the script to use the truth scores from step 2 to convert these score thresholds into truth-sensitivity thresholds.; 5) Provide these truth-sensitivity thresholds to ScoreVariantAnnotations and use them to hard filter. Evaluate on a test set of chromosomes. If all looks good, we can later move steps 3-4 into the train tool and automate the passing of sensitivities in 5 via outputs in the model directory. This will let us keep the basic interface of ScoreVariantAnnotations the same, but we'll have to add a few basic parameters to TrainVariantAnnotationsModel to control the train/validation split. So I think all this branch is missing is step 5---we'll simply need to add command-line parameters for the SNP/INDEL sensitivity thresholds and then do the hard filtering in the VCF writing method highlighted above. Do you think you can handle implementing that in this branch, and then the rest at the WDL level? I can help with the python script for the LL stuff (or anything else), if needed. Not sure if you got a chance to check out what your collaborators are doing in the methods you're looking to compare against, but it would be good to understand if this basic scheme for train/validation/test splitting can",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1068345084:27,optimiz,optimized,27,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1068345084,2,['optimiz'],"['optimized', 'optimizing']"
Performance,"HI @sooheelee I did try using BWA to index the 2-bit reference, but it doesn't work as well. @lbergelson Do you think the reference loading issue can be fixed soon? like this month?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2171#issuecomment-251738251:132,load,loading,132,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2171#issuecomment-251738251,1,['load'],['loading']
Performance,HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 19:11:57.326 INFO FilterAlignmentArtifacts - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 19:11:57.326 INFO FilterAlignmentArtifacts - Deflater: IntelDeflater; 19:11:57.326 INFO FilterAlignmentArtifacts - Inflater: JdkInflater; 19:11:57.326 INFO FilterAlignmentArtifacts - GCS max retries/reopens: 20; 19:11:57.326 INFO FilterAlignmentArtifacts - Requester pays: disabled; 19:11:57.326 WARN FilterAlignmentArtifacts - . !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: FilterAlignmentArtifacts is an EXPERIMENTAL tool and should not be used for production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 19:11:57.326 INFO FilterAlignmentArtifacts - Initializing engine; 19:11:57.666 INFO FeatureManager - Using codec VCFCodec to read file file:///output/sample.FilterMutectCalls.vcf.gz; 19:11:57.757 INFO FilterAlignmentArtifacts - Done initializing engine; 19:11:57.827 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/app/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 19:11:57.861 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 19:11:57.862 INFO IntelPairHmm - Available threads: 4; 19:11:57.862 INFO IntelPairHmm - Requested threads: 4; 19:11:57.862 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 19:11:57.862 INFO ProgressMeter - Starting traversal; 19:11:57.862 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; *** glibc detected *** /for/bar/bin/java: double free or corruption (out): 0x00007f450af58700 ***; ======= Backtrace: =========; /lib64/libc.so.6(+0x3d01675dee)[0x7f45058afdee]; /lib64/libc.so.6(+0x3d01678c80)[0x7f45058b2c80]; /tmp/libgkl_smithwaterman410767516409374085.so(_Z19runSWOnePairBT_avx2iiiiPhS_iiaPcPs+0x338)[0x7f4499f4cfa8]; /tmp/libgkl_smithwaterman410767516409374085.so(Java_com_intel_gkl_smithwaterman,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-660645356:3880,Load,Loading,3880,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-660645356,1,['Load'],['Loading']
Performance,HaplotypeCaller does not resume from where it stopped. If you need to perform the same task again restart the whole task using the very same commandline.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7454#issuecomment-918232900:70,perform,perform,70,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7454#issuecomment-918232900,1,['perform'],['perform']
Performance,"Hello @bharathramh thank you for your question. We just recently merged the bulk of the code for DRAGEN-GATK (#6634) which means that we are slated to release the DRAGEN-GATK for the next release of GATK. When that happens we will also be releasing an official wdl workflow to use all the new features together. . As for STR variants, one of the code improvements in the new genotyping engine is to better model STR errors by performing a per-sample pre-processing step where a table of empirically generated STR priors is generated by the tools `ComposeSTRTableFile` and `CallibrateDragstrModel`. These work by modifying the priors for genotying to be more in line with the sample PCR/Sequencing errors and can't really be thought of as a feature to ""find STRs"" (except `ComposeSTRTableFile` which is used to find STRs in the reference). These will be featured in the published wdls with appropriate commands. . I would recommend in the future that you direct questions like this one to our forums since this ticket tracker is used to track bugs/ongoing development on the GATK and our Comms Team is better equipped to answer questions there: https://gatk.broadinstitute.org/hc/en-us/community/topics",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6912#issuecomment-716747266:426,perform,performing,426,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6912#issuecomment-716747266,1,['perform'],['performing']
Performance,"Hello @droazen and @nalinigans , we tried out the parameter you suggested and did improve the performance but not as great as compared to writing the data locally and then moving to FSx for luster. Using the parameter, the job completed in ~2 hrs 52 mins, whereas when we run to write on EBS volumes it completes within 1 hr.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7646#issuecomment-1039658049:94,perform,performance,94,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7646#issuecomment-1039658049,1,['perform'],['performance']
Performance,"Hello @nalinigans,. As part of gatk-sv pipeline we are using GATK : v4.1.8.1 which doesn't have bypass-feature-reader option. Also, we didn’t capture strace for the run with just ""--genomicsdb-shared-posixfs-optimizations"" so wont be able to share the FUTEX process counts. So after using v4.2.4.1 we get below results. 	- Using ""--genomicsdb-shared-posixfs-optimizations"" & ""--bypass-feature-reader"" the process took 118 mins.; ""FUTEX_WAIT_PRIVATE, 0, NULL"" : 1266. 	- Using ""--genomicsdb-shared-posixfs-optimizations"" & ""--bypass-feature-reader"" and ; TILEDB_UPLOAD_BUFFER_SIZE=5242880 as env variable the process took 113 mins.; 	""FUTEX_WAIT_PRIVATE, 0, NULL"" : 3. 	- Even using 10 MB as buffer size resulted in same execution time of 113 mins.; 	- Using a buffer size bigger i.e. 50 MBs caused the process to run slower so we aborted it. Please let us know if we can improve it further.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7646#issuecomment-1040947845:208,optimiz,optimizations,208,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7646#issuecomment-1040947845,3,['optimiz'],['optimizations']
Performance,"Hello,. I did encounter the same behaviour that I reported one year ago with the official Docker GATK 4.1.0.0 container converted into a singularity image. ## Version of softwares:. Singularity : 2.5.1, GATK : 4.1.0.0. ### Command. Singularity : ; singularity build gatk-4.0.4.0.img docker://broadinstitute/gatk:4.0.4.0. ### Actual behavior; ```; 14:39:21.762 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 14:39:24.079 INFO DetermineGermlineContigPloidy - ------------------------------------------------------------; 14:39:24.079 INFO DetermineGermlineContigPloidy - The Genome Analysis Toolkit (GATK) v4.1.0.0; 14:39:24.080 INFO DetermineGermlineContigPloidy - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:39:24.081 INFO DetermineGermlineContigPloidy - Executing as tintest@dahu38 on Linux v4.9.0-8-amd64 amd64; 14:39:24.081 INFO DetermineGermlineContigPloidy - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_191-8u191-b12-0ubuntu0.16.04.1-b12; 14:39:24.081 INFO DetermineGermlineContigPloidy - Start Date/Time: May 26, 2019 2:39:21 PM UTC; 14:39:24.081 INFO DetermineGermlineContigPloidy - ------------------------------------------------------------; 14:39:24.081 INFO DetermineGermlineContigPloidy - ------------------------------------------------------------; 14:39:24.082 INFO DetermineGermlineContigPloidy - HTSJDK Version: 2.18.2; 14:39:24.082 INFO DetermineGermlineContigPloidy - Picard Version: 2.18.25; 14:39:24.083 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 14:39:24.083 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 14:39:24.083 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 14:39:24.083 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 14:39:24.083 INFO DetermineGermline",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-496007081:387,Load,Loading,387,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-496007081,1,['Load'],['Loading']
Performance,"Hello,. I think we're pretty close on porting this. However, i wanted to ask again if I could get access to the test data used in GATK3 VariantEvalWalkerUnitTest and VariantEvalIntegrationTest. We dont need to port those files to GATK4 unless some of them already exist; however, having these files would help confirm the ported tool is behaving exactly as GATK3. Second, I am hoping to confirm a behavior for FeaureContext: many GATK3 stratifiers follow roughly this pattern (VariantEval / knownCNVsFile being an example). The pattern is: 1) some VCF/BED file provided as an argument, 2) in initialize(), the walker reads this file in memory into a List<GenomeLoc> or IntervalTree<GenomeLoc>. This is so each locus can quickly find overlapping intervals. In GATK4, there is no longer a point in loading the whole file into memory, right? If I define an argument as FeatureInput<Feature>, it will automatically be initialized. FeatureContext.getValues() seems to give me overlapping intervals (including partial overlaps). Therefore there is no reason to try to replicate that GATK3 pattern of reading intervals into memory. Can someone confirm this is correct?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/616#issuecomment-343563884:796,load,loading,796,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/616#issuecomment-343563884,1,['load'],['loading']
Performance,"Here are some initial thoughts, after working on my local machine. It appears that writing a single BAM file is slowed significantly on the sort when the tasks have to spill to disk. So:; - Lots of memory is good. Decreases the chance of spilling.; - The sort itself should use multiple partitions. This should decrease the amount of data going to each reducer/sort task.; - When we end up writing to a single partition, we actually want to induce an additional shuffle. If we perform `.coalesce(1)` without the shuffle, then the `sortByKey` gets pushed into a single reducer task, ensuring lots of spilling and making things slower. It might make sense to modify the API to look more like this:. `ReadsSparkSink.writeReads(..., boolean sortReads, int outputPartitions, int sortPartitions)`. This would enable a flag that decides whether to to a total sort of the data, and if so, let you set the number of sort tasks (perhaps we leave this one off). Finally, the `outputPartitions` parameter lets us set how many partitions to perform on reshuffling. If either are set to 0, they can just use default values. If the output is set to 1 partition, it would write a single file.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1015#issuecomment-152355638:477,perform,perform,477,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1015#issuecomment-152355638,2,['perform'],['perform']
Performance,"Here is a recap of what we discussed today during the CNV meeting:. For the first round of evaluations we decided to run Germline CNV pipeline on TCGA exomes using a range of key hyperparameters (namely psi-t-scale and p-alt) and establish the base level performance metrics using output of GenomeSTRiP on matched WGS samples as ground truth. . @mbabadi could you come up with a good range of hyperparameter values that you think should be cross-validated?. In particular we need to:. - Dockerize tools we will be evaluating against (XHMM, CODEX2, CLAMMS, GenomeSTRiP); - Write a WDL that runs Germline CNV that scatters across range of key hyperparameters and outputs array of VCFs; - Write VCF processors for output of CLAMMS and CODEX2 ; - Write WDLs for running XHMM, CODEX2, CLAMMS and GenomeSTRiP that output VCFs; - Write WDL that takes results of the above and uses @mbabadi 's gCNV evaluation python modules(located here /dsde/working/mehrtash/gCNV_theano_eval) to output performance metrics; - Decide on an automatic evaluation framework. For the next round of evaluations we need to:. - Decide on appropriate metrics for evaluating performance on trios and write scripts that implement them; - Expand the range of hyperparameters in search space (possibly include different bin sizes, GC vs no GC correction, and fragment mid point coverage collection vs largest fragment overlap coverage collection); - Use gnomAD subset of matched WES/WGS pairs for validation",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4123#issuecomment-362075071:255,perform,performance,255,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4123#issuecomment-362075071,3,['perform'],['performance']
Performance,"Here is a script I ran to run the import on 3500 unblocked gvcfs. The script imports one chromosome per workspace.  As the chromosomes get larger --more and more memory is needed.  chr4 through 22 ran fine. The chr3 (see log below) ends without an error BUT with the callset.json NOT being written out.   I could split the chr1-3 at the centromere to try it again. Any other suggestions? Would increasing -Xmx150g to 240g help? . For chromosome 1, which is still running, top indicates is using about  240g (after importing the 65 batches).; ```; PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM  TIME+ COMMAND; 21698 farrell   20   0      443.7g 240.3g   1416 S  86.7 95.5   7398:14 java; ```. ```; #!/bin/bash -l; #$ -l mem_total=251; #$ -P casa; #$ -pe omp 32; #$ -l h_rt=240:00:00; module load miniconda/4.9.2; module load gatk/[4.2.6.1](http://4.2.6.1/); conda activate /share/pkg.7/gatk/[4.2.6.1/install/gatk-4.2.6.1](http://4.2.6.1/install/gatk-4.2.6.1). CHR=$1; DB=""genomicsDB.rb.chr$CHR""; rm -rf $DB; # mkdir -p $DB; # mkdir tmp; echo ""Processing chr$CHR""; echo ""NSLOTS: $NSLOTS""; # head sample_map.chr$CHR.reblock.list; head sample_map.chr$CHR; wc   sample_map.chr$CHR; gatk --java-options ""-Xmx150g -Xms16g"" \;        GenomicsDBImport \;        --sample-name-map sample_map.chr$CHR \;        --genomicsdb-workspace-path $DB \;        --genomicsdb-shared-posixfs-optimizations True\;        --tmp-dir tmp \;        --L chr$CHR\;        --batch-size 50 \;        --bypass-feature-reader\;        --reader-threads 5\;        --merge-input-intervals \;        --overwrite-existing-genomicsdb-workspace\;        --consolidate. ```; End of log on chr3. ```; 07:19:44.855 INFO  GenomicsDBImport - Done importing batch 38/65; 08:05:11.651 INFO  GenomicsDBImport - Done importing batch 39/65; 08:49:12.112 INFO  GenomicsDBImport - Done importing batch 40/65; 09:32:39.526 INFO  GenomicsDBImport - Done importing batch 41/65; 10:23:36.849 INFO  GenomicsDBImport - Done importing batch 42/65; 1",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1246785232:800,load,load,800,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1246785232,2,['load'],['load']
Performance,Here is another one; ```; java.lang.RuntimeException: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: Read timed out; 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:309); 	at htsjdk.samtools.seekablestream.SeekablePathStream.read(SeekablePathStream.java:86); 	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286); 	at java.io.BufferedInputStream.read(BufferedInputStream.java:345); 	at htsjdk.samtools.seekablestream.SeekableBufferedStream.read(SeekableBufferedStream.java:100); 	at htsjdk.samtools.util.BlockCompressedInputStream.readBytes(BlockCompressedInputStream.java:550); 	at htsjdk.samtools.util.BlockCompressedInputStream.readBytes(BlockCompressedInputStream.java:539); 	at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:493); 	at htsjdk.samtools.util.BlockCompressedInputStream.nextBlock(BlockCompressedInputStream.java:451); 	at htsjdk.samtools.util.BlockCompressedInputStream.readBlock(BlockCompressedInputStream.java:441); 	at htsjdk.samtools.util.BlockCompressedInputStream.available(BlockCompressedInputStream.java:194); 	at htsjdk.samtools.util.BlockCompressedInputStream.read(BlockCompressedInputStream.java:322); 	at htsjdk.tribble.readers.TabixReader.readIndex(TabixReader.java:215); 	at htsjdk.tribble.readers.TabixReader.readIndex(TabixReader.java:269); 	at htsjdk.tribble.readers.TabixReader.<init>(TabixReader.java:161); 	at htsjdk.tribble.readers.TabixReader.<init>(TabixReader.java:125); 	at htsjdk.tribble.TabixFeatureReader.<init>(TabixFeatureReader.java:84); 	at htsjdk.tribble.AbstractFeatureReader.getFeatureReader(AbstractFeatureReader.java:106); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getReaderFromVCFUri(GenomicsDBImport.java:437); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getF,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300298180:64,concurren,concurrent,64,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300298180,1,['concurren'],['concurrent']
Performance,"Here is the command line. I'm using funcotator_dataSources.v1.2.20180329.tar.gz.; /omics/chatchawit/gatk/gatk Funcotator -R /omics/chatchawit/bundle/hsa38.fasta -V /omics/chatchawit/sm/out/sample21.vcf -O /omics/chatchawit/sm/anno/sample21.vcf --data-sources-path /omics/chatchawit/bundle/dsrc/ --ref-version hg38. Using GATK jar /omics/chatchawit/gatk/gatk-package-4.0.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -jar /omics/chatchawit/gatk/gatk-package-4.0.0.0-local.jar Funcotator -R /omics/chatchawit/bundle/hsa38.fasta -V /omics/chatchawit/sm/out/sample21.vcf -O /omics/chatchawit/sm/anno/sample21.vcf --data-sources-path /omics/chatchawit/bundle/dsrc/ --ref-version hg38; 22:56:54.836 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/omics/chatchawit/gatk/gatk-package-4.0.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 22:56:55.064 INFO Funcotator - ------------------------------------------------------------; 22:56:55.065 INFO Funcotator - The Genome Analysis Toolkit (GATK) v4.0.0.0; 22:56:55.065 INFO Funcotator - For support and documentation go to https://software.broadinstitute.org/gatk/; 22:56:55.065 INFO Funcotator - Executing as chatchawit@omics on Linux v3.13.0-133-generic amd64; 22:56:55.066 INFO Funcotator - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_161-b12; 22:56:55.066 INFO Funcotator - Start Date/Time: April 27, 2018 10:56:54 PM ICT; 22:56:55.066 INFO Funcotator - ------------------------------------------------------------; 22:56:55.066 INFO Funcotator - ------------------------------------------------------------; 22:56:55.067 INFO Funcotator - HTSJDK Version: 2.13.2; 22:56:55.067 INFO Funcotator - Picard Version: 2.17.2; 22:56:55.067 INFO Funcotator - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 22:56:55.067 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : fa",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385021157:859,Load,Loading,859,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385021157,1,['Load'],['Loading']
Performance,"Here is the command line. ```; ./gatk Funcotator --variant /home/deepak/software_library/gatk-4.1.7.0/SAMPL3_VARIANTFIL.vcf --reference /media/deepak/EXTRA/Genomedir/hg38/hg38.fasta --ref-version hg38 --data-sources-path /media/deepak/EXTRA/FUNCOTATOR_DATA/DATA_SOURCES --output variants.funcotated.vcf --output-file-format VCF; Using GATK jar /home/deepak/software_library/gatk-4.1.7.0/gatk-package-4.1.7.0-local.jar. java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/deepak/software_library/gatk-4.1.7.0/gatk-package-4.1.7.0-local.jar Funcotator --variant /home/deepak/software_library/gatk-4.1.7.0/SAMPL3_VARIANTFIL.vcf --reference /media/deepak/EXTRA/Genomedir/hg38/hg38.fasta --ref-version hg38 --data-sources-path /media/deepak/EXTRA/FUNCOTATOR_DATA/DATA_SOURCES --output variants.funcotated.vcf --output-file-format VCF; 16:01:36.165 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/deepak/software_library/gatk-4.1.7.0/gatk-package-4.1.7.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; May 12, 2020 4:01:36 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 16:01:36.870 INFO Funcotator - ------------------------------------------------------------; 16:01:36.871 INFO Funcotator - The Genome Analysis Toolkit (GATK) v4.1.7.0; 16:01:36.871 INFO Funcotator - For support and documentation go to https://software.broadinstitute.org/gatk/; 16:01:36.871 INFO Funcotator - Executing as deepak@ngs on Linux v5.3.0-26-generic amd64; 16:01:36.871 INFO Funcotator - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_241-b07; 16:01:36.871 INFO Funcotator - Start Date/Time: 12 May, 2020 4:01:35 PM IST; 16:01:36.871 INFO Funcotator - ------------------------------------------------------------; 16:01:36.871 INFO Funcotator -",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6598#issuecomment-664565036:989,Load,Loading,989,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6598#issuecomment-664565036,1,['Load'],['Loading']
Performance,"Here is the equivalent output for the master branch:. ```; 2022-08-16T22:45:53.6066834Z Gradle suite > Gradle test > org.broadinstitute.hellbender.utils.help.DocumentationGenerationIntegrationTest > documentationSmokeTest2 [31mFAILED[39m[0K; 2022-08-16T22:45:53.6067689Z java.lang.AssertionError: Loading source files for package javadoc...[0K; 2022-08-16T22:45:53.6068266Z programName: warning - No source files for package javadoc; 2022-08-16T22:45:53.6068997Z Loading source files for package org.broadinstitute.hellbender.tools.walkers.variantutils...; 2022-08-16T22:45:53.6079174Z Constructing Javadoc information...; 2022-08-16T22:45:53.6079564Z [search path for source files: src/main/java]; 2022-08-16T22:45:53.6082788Z [search path for class files: /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/resources.jar,/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar,/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/sunrsasign.jar,/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/jsse.jar,/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/jce.jar,/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/charsets.jar,/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/jfr.jar,/usr/lib/jvm/java-8-openjdk-amd64/jre/classes,/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/zipfs.jar,/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/dnsns.jar,/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/sunjce_provider.jar,/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/icedtea-sound.jar,/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/nashorn.jar,/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/java-atk-wrapper.jar,/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/sunec.jar,/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/sunpkcs11.jar,/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/cldrdata.jar,/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/jaccess.jar,/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/localedata.jar,/gatk/gatk-package-unspecified-SNAPSHOT-local.jar,/jars/gatk-package-4.2.6.1-50-g40182c7-SNAPSHOT-testDependencies.jar,/jars/gatk-package-4.2.6.1-50-g40182c7-SNAPSH",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217253370:300,Load,Loading,300,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217253370,2,['Load'],['Loading']
Performance,"Here the error log ; Using GATK jar /share/scientific_bin/gatk/4.1.4.1/gatk-package-4.1.4.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /share/scientific_bin/gatk/4.1.4.1/gatk-package-4.1.4.1-local.jar IndexFeatureFile -I output/called/final/allsites.filtered.vcf; 00:57:08.257 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/scientific_bin/gatk/4.1.4.1/gatk-package-4.1.4.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Sep 11, 2023 12:57:08 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 00:57:08.789 INFO IndexFeatureFile - ------------------------------------------------------------; 00:57:08.789 INFO IndexFeatureFile - The Genome Analysis Toolkit (GATK) v4.1.4.1; 00:57:08.789 INFO IndexFeatureFile - For support and documentation go to https://software.broadinstitute.org/gatk/; 00:57:08.790 INFO IndexFeatureFile - Executing as [ychrysostomakis@compute-0-3.local](mailto:ychrysostomakis@compute-0-3.local) on Linux v3.10.0-1160.53.1.el7.x86_64 amd64; 00:57:08.790 INFO IndexFeatureFile - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_231-b11; 00:57:08.790 INFO IndexFeatureFile - Start Date/Time: 11. September 2023 00:57:07 MESZ; 00:57:08.790 INFO IndexFeatureFile - ------------------------------------------------------------; 00:57:08.790 INFO IndexFeatureFile - ------------------------------------------------------------; 00:57:08.790 INFO IndexFeatureFile - HTSJDK Version: 2.21.0; 00:57:08.790 INFO IndexFeatureFile - Picard Version: 2.21.2; 00:57:08.790 INFO IndexFeatureFile - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 00:57:08.790 INFO IndexFeatureFile - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 00:57:08.790 INFO IndexFeatureFile - HTSJDK Defaults.USE_ASYNC_I",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8372#issuecomment-1733069316:447,Load,Loading,447,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8372#issuecomment-1733069316,1,['Load'],['Loading']
Performance,"Here's a suggested set of things to look at as part of this ticket:. -See if we can avoid fetching all headers on startup by passing in the needed info via alternate args (https://github.com/broadinstitute/gatk/issues/2639). -Do profiling to find an appropriate value for the --batchSize argument,; once it's merged (https://github.com/broadinstitute/gatk/issues/2641). -Shrink NIO buffers (--cloudPrefetchBuffer and --cloudIndexPrefetchBuffer) down to the smallest values that still produce acceptable performance (https://github.com/broadinstitute/gatk/issues/2640). Thibault of red team aka @Horneth has agreed to take this on.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2633#issuecomment-298338616:503,perform,performance,503,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2633#issuecomment-298338616,1,['perform'],['performance']
Performance,"Here's some code you can add to the test to check that jbwa actually works. ```; @Test; public void testBwaMemAlignSingleRead() throws Exception {; final String libraryPath = NativeUtils.runningOnLinux() ? ""/lib/libbwajni.so"" : ""/lib/libbwajni_mac.jnilib"";; Assert.assertTrue(NativeUtils.loadLibraryFromClasspath(libraryPath), ""jbwa library was not loaded. "" +; ""This could be due to a configuration error, or your system might not support it."");. BwaIndex index= new BwaIndex(new File(b37_reference_20_21));; final BwaMem bwaMem = new BwaMem(index);; //real read taken from src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam; final String name=""20FUKAAXX100202:3:46:9213:168594"";; final byte[] seqs= ""GTTTTGTTTACTACAGCTTTGTAGTAAATTTTGAACTCTAAAGTGTTAGTTCTCTAACTTTGTTTGTTTTTCAAGAGTGTTTTGACTCTTCTTACTGCATC"".getBytes(); ;; final byte[] quals= ""DGFDGFDHFFFFGFEFHEGFFFGGHEHFHGHHGEGGGGGFGFHGHGHEHGGGFGAEFGDACAHHDHGCGFGGGFGDHGHFFHDDCGGDGEE"".getBytes();; final ShortRead read = new ShortRead(name, seqs, quals);; final AlnRgn[] align = bwaMem.align(read);; Assert.assertEquals(align.length, 1);; Assert.assertEquals(align[0].getChrom(), ""20"");; Assert.assertEquals(align[0].getCigar(), ""101M"");; Assert.assertEquals(align[0].getMQual(), 60);; Assert.assertEquals(align[0].getPos(), 9999997-1); #note difference from the bam file (9999997 in bam, 9999996 here); Assert.assertEquals(align[0].getNm(), 0);; bwaMem.dispose();; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1847#issuecomment-220755636:288,load,loadLibraryFromClasspath,288,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1847#issuecomment-220755636,2,['load'],"['loadLibraryFromClasspath', 'loaded']"
Performance,Here's the performance benchmarking; sen_del sen_dup ppv_del ppv_dup sen_del_hist sen_dup_hist ppv_del_hist ppv_dup_hist; 1 0.2000000 0.4121864 0.9387755 0.8521127 345 279 98 142; 2 0.4598540 0.6086957 0.9647059 0.8602941 137 184 85 136; 3 0.6352941 0.7181208 0.9565217 0.8650794 85 149 69 126; 4 0.7796610 0.7744361 0.9636364 0.8606557 59 133 55 122; 5 0.8913043 0.8715596 0.9583333 0.8495575 46 109 48 113; 6 0.9024390 0.9019608 0.9750000 0.8545455 41 102 40 110; 7 0.9428571 0.9263158 0.9722222 0.8666667 35 95 36 105; 8 0.9310345 0.9534884 0.9666667 0.8736842 29 86 30 95; 9 0.9285714 0.9518072 1.0000000 0.9213483 28 83 26 89; 10 0.9565217 0.9487179 1.0000000 0.9259259 23 78 23 81. ### NEW gCNV docker; sen_del sen_dup ppv_del ppv_dup sen_del_hist sen_dup_hist ppv_del_hist ppv_dup_hist; 1 0.2034884 0.4107143 0.9387755 0.8581560 344 280 98 141; 2 0.4705882 0.6086957 0.9647059 0.8676471 136 184 85 136; 3 0.6547619 0.7248322 0.9565217 0.8730159 84 149 69 126; 4 0.7931034 0.7819549 0.9636364 0.8677686 58 133 55 121; 5 0.9111111 0.8807339 0.9583333 0.8558559 45 109 48 111; 6 0.9250000 0.9019608 0.9750000 0.8584906 40 102 40 106; 7 0.9705882 0.9263158 0.9722222 0.8712871 34 95 36 101; 8 0.9642857 0.9534884 0.9655172 0.8791209 28 86 29 91; 9 0.9629630 0.9518072 1.0000000 0.9186047 27 83 25 86; 10 1.0000000 0.9487179 1.0000000 0.9113924 22 78 22 79,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8561#issuecomment-2187034329:11,perform,performance,11,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8561#issuecomment-2187034329,1,['perform'],['performance']
Performance,"Hey @mwalker174 ,. Do you have any suggestions about how to perform step 1? I naively tried to use picard's `MergeBamAlignment` using the PathSeq output BAM as the aligned bam and the PathSeq input BAM as the unmapped BAM but I get the following error message. `IllegalArgumentException: Do not use this function to merge dictionaries with different sequences in them. Sequences must be in the same order as well. Found [NZ_DS990135.1, NZ_AJSY01000035.1, ... `. I tried sorting both BAM files by queryname and removing the alignment for the input BAM using `RevertSam` but neither of these worked. I suspect that it's because of the PathSeq output BAM given the references to the microbial sequences. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6655#issuecomment-644426370:60,perform,perform,60,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6655#issuecomment-644426370,1,['perform'],['perform']
Performance,"Hey @mwalker174,. Setting `--host-min-identity 20` lowered the `READS_AFTER_PREALIGNED_HOST_FILTER` to 131467, which is closer to what I expected (albeit still 12% of the reads) so thanks for the suggestion. I'm still unsure why the preferred approach is to use this fairly arbitrary metric that doesn't incorporate both mates instead of leveraging the known alignments from STAR, which does. Given what I've learned today, I think the better approach (for my use case) would be to filter the unique mappers, the multi-mappers and the chimeric reads (which in my data set represent 97.5% of the reads) and then apply the QUALITY_AND_COMPLEXITY_FILTER and the DUPLICATE_READS_FILTER. Would you agree?. To put myself in your shoes, I would guess that PathSeq is designed for general purpose use cases (which is probably `--is-host-aligned false`) and that performing such a filtering would require specific handling for each supported aligner, which would be a lot of work. Moreover, my use case with short paired-end reads is also probably not common. So I understand why you use the existing approach but are there any reasons that I'm missing as to why you'd suggest to stick with the basic approach for my use case instead of the one that I proposed above?. Best, Welles",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6687#issuecomment-652524772:854,perform,performing,854,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6687#issuecomment-652524772,1,['perform'],['performing']
Performance,"Hi @Hemantcnaik ,. This is a much better question for the GATK forum. We try to limit github issues to bugs and feature requests, while questions about tool usage should go to the forum where they're triaged by another team. We haven't done much RNA development in a long time, though hopefully that's about to change. In the meantime, a lot of other places in the GATK code we use 20 as a minimum base quality. If you're really concerned about optimizing this value, you could probably come up with an in silico experiment similar to what we do for BQSR. If you assume that every site in your sample that's in dbSNP is a real variant and everything that's not is a false positive (not true, but to a good approximation), then you can look at the base quality distributions for true positives and false positives and try to decide what's a good tradeoff between sensitivity and precision.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6753#issuecomment-678345871:445,optimiz,optimizing,445,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6753#issuecomment-678345871,1,['optimiz'],['optimizing']
Performance,"Hi @cccnrc, glad you found this discussion interesting and apologies for the very late reply. Unfortunately, there hasn't been much movement on this front, as I think we decided that the current model sufficed. I think we are moving in a direction so that we can obviate the need for a separate step to fit global (e.g., depth) and per-contig (e.g., contig ploidy) parameters for each sample. Recall that this is only necessary because each gCNV genomic shard needs these quantities to perform inference, but cannot infer them from the data they are responsible for fitting (which typically covers less than a contig). We are looking to reimplement gCNV in a more modern inference framework that could allow us to do away with such sharding entirely. We could thus fit global/per-contig quantities of interest jointly with the rest of the gCNV model. The timeframe for this work may be relatively long (~year), but I think it'll be worth it. That said, I think a key takeaway from this work is that genotype priors can be more powerful for breaking degeneracies than contig-ploidy priors, so we will probably try to incorporate that insight in future work. To answer your questions:; 1) There are no additional results much further beyond what is shown above.; 2) You can see snippets/comparisons of the genotype and contig-ploidy prior file above. If you're just looking for information about the contig-ploidy prior table used in the current pipeline, see an example at https://gatk.broadinstitute.org/hc/en-us/articles/360051304711-DetermineGermlineContigPloidy and feel free to modify it to be more/less strict as desired.; 3) Unfortunately I believe the samples discussed above are not publicly available. If you are having difficulties with the current model, it would probably be useful to hear about them!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-894432129:486,perform,perform,486,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-894432129,1,['perform'],['perform']
Performance,"Hi @cmnbroad, we can do better than crash - you'll notice that in my pull request, I've added a check to `CNNScoreVariants.java` to test whether AVX is present (the method was checking if command-line arguments are valid, so I stretched a point and used it to test the environment). . ~. You could definitely offer your own build of TensorFlow, but do you _want_ to? Apart from anything else, it'll be 10X slower than the default. My understanding is that inference using CNNScoreVariants was taking ~ 50 hours on a typical real-world file before optimization. Wouldn't it be more cost-effective just to use instances that at least have AVX?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-429323785:547,optimiz,optimization,547,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-429323785,1,['optimiz'],['optimization']
Performance,"Hi @davidbenjamin ,. this ticket is already open for a while, but I haven't found a sensible multi-sample wdl yet, so I've spent some time putting a multi-sample calling workflow together and tested it extensively: https://github.com/phylyc/gatk4-somatic-snvs-indels. It's somewhat optimized for resource needs, at least much more so than the published gatk mutect2 wdl.; One option that I added is to run the realignment filter only on a subset of called variants, which is especially helpful for tumor-only calling to reduce costs if we are hard filtering variants afterwards based on some thresholds anyways (who doesn't?). One todo is still to choose the best normal sample for the contamination model based on which has the highest sequencing depth, as you had mentioned in some old gatk forum post. Happy to have some contributions there :); I also brushed up the PoN wdl, which is also tested. How important is cram input support? I took that part out, but it's easy to plug it back in, I suppose.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6022#issuecomment-1113847113:282,optimiz,optimized,282,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6022#issuecomment-1113847113,1,['optimiz'],['optimized']
Performance,"Hi @davidbenjamin and @ldgauthier,. I'm running into this same problem with version 4.4.0.0 in tumor only mode (actually creating a panel of normal). As far as I understand ""--emit-ref-confidence GVCF"" is required for compatibility with GenomicsDBImport? Is there some workaround?. > 1 Using GATK jar /scicore/soft/apps/GATK/4.4.0.0-GCCcore-10.3.0-Java-17/gatk-package-4.4.0.0-local.jar; > 2 Running:; > 3 java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /scicore/soft/apps/GATK/4.4.0.0-GCCcore-10.3.0-Java-17/gatk-package-4.4.0.0-local.jar Mutect2 -R /ref_nobackup/Homo_sapiens_assembly38.fasta -I /BQSR/BSSE_QGF_229563_bqsr.bam --emit-ref-confidence GVCF --germline-resource /ref_nobackup/af-only-gnomad.hg38.vcf.gz -max-mnp-distance 0 -O /output/BSSE_QGF_229563_pon.g.vcf.gz --tmp-dir /scratch; > 4 15:07:52.399 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/scicore/soft/apps/GATK/4.4.0.0-GCCcore-10.3.0-Java-17/gatk-package-4.4.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; > 5 15:07:52.435 INFO Mutect2 - ------------------------------------------------------------; > 6 15:07:52.437 INFO Mutect2 - The Genome Analysis Toolkit (GATK) v4.4.0.0; > 7 15:07:52.438 INFO Mutect2 - For support and documentation go to https://software.broadinstitute.org/gatk/; > 8 15:07:52.438 INFO Mutect2 - Executing as X on Linux v3.10.0-1160.66.1.el7.x86_64 amd64; > 9 15:07:52.438 INFO Mutect2 - Java runtime: OpenJDK 64-Bit Server VM v17.0.6+10; > 10 15:07:52.438 INFO Mutect2 - Start Date/Time: June 19, 2023 at 3:07:52 PM CEST; > 11 15:07:52.438 INFO Mutect2 - ------------------------------------------------------------; > 12 15:07:52.438 INFO Mutect2 - ------------------------------------------------------------; > 13 15:07:52.439 INFO Mutect2 - HTSJDK Version: 3.0.5; > 14 15:07:52.440 INFO Mutect2 - Picard Version: 3.0.0; > 15 15:07:52.440 INFO Mu",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7849#issuecomment-1597198632:958,Load,Loading,958,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7849#issuecomment-1597198632,1,['Load'],['Loading']
Performance,"Hi @droazen ,; I am sorry for the late response. I used the docker container from dockerhub broadinstitute/gatk:4.4.0.0 .; The which python command produces the correct path. I re-loaded the container and tried it again without modification. Again the pipeline crashed with BaseRecalibrator and the above mentioned error.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8402#issuecomment-1691458235:180,load,loaded,180,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8402#issuecomment-1691458235,1,['load'],['loaded']
Performance,"Hi @droazen! Again, I'm skeptical that it would be a serialization change as we have only made one change to serializer registration for htsjdk classes since 0.20.0:. ```; $ git diff adam-parent_2.10-0.20.0 adam-core/src/main/scala/org/bdgenomics/adam/serialization/ADAMKryoRegistrator.scala | grep htsjdk; kryo.register(classOf[scala.Array[htsjdk.variant.vcf.VCFHeader]]); ```. Additionally, the [Kryo version used in ADAM has been unchanged since 0.20.0](https://github.com/bigdatagenomics/adam/commit/295fb5cf7bf1f089e82a1624e2056053e817b5f3). Since we do not shuffle `SAMRecord`s, we do not register a serializer for `SAMRecord`, which would presumably be the main record that HaplotypeCaller is serializing. The GATK's implementation of BQSR doesn't perform any shuffles other than when reducing the recalibration tables, correct? If so, any performance regression in GATK's BQSR would be exceedingly unlikely to be caused by serializer registrations in ADAM.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4376#issuecomment-367112827:755,perform,perform,755,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4376#issuecomment-367112827,2,['perform'],"['perform', 'performance']"
Performance,"Hi @fpbarthel,. You have a few options, but I think they would all require a bit of manual processing on your part. For example, you could use CollectReadCounts with your initial list of bins, but then you'd have to do the averaging step manually. Hopefully, it should not be too trouble to put together your own script to do this; however, since it is somewhat of a custom operation, it's unlikely we'll support it as a feature. In any case, note that the rest of the downstream tools in both our germline and somatic pipelines are only set up to work with integer counts. I don't know your reasons for wanting to average counts over multiple bins, but since 1) averages contain less information than the full resolution bins, 2) the BAM I/O to perform count collection is expensive (so we don't want to discard information during the CollectReadCounts step that we might need later), and 3) we want to build generative models of the counts, our philosophy is to always retain the integer counts.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5432#issuecomment-439892644:746,perform,perform,746,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5432#issuecomment-439892644,1,['perform'],['perform']
Performance,"Hi @jamesemery Thanks for your email.; Actually, I was successful with calculation of ""DepthOfCoverage"" with following procedure:; 1. I found the exact reference fasta file which is used for mapping procedures in my pipeline + .dict and .fa.fai; 2. I made a bed file out of the reference fasta file, using the following command:; faidx -i bed genome.fa > out.bed; 3. Finally I did this for performing the ""DepthOfCoverage"" tool:; gatk DepthOfCoverage -R reference.fa -O result -I s269825.haplotagged.bam -L out.bed. So everything works fine but there is another problem: the size of output file is very huge (GBs)! Should it be like that?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7453#issuecomment-971402891:390,perform,performing,390,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7453#issuecomment-971402891,1,['perform'],['performing']
Performance,"Hi @lbergelson ; Thanks for the response!; I tried building the current gatk from the master branch, and now the error message says something a little different. java.lang.ClassCastException: class htsjdk.samtools.BAMRecord cannot be cast to class java.lang.Comparable (htsjdk.samtools.BAMRecord is in unnamed module of loader 'app'; java.lang.Comparable is in module java.base of loader 'bootstrap'); 	at java.base/java.util.Arrays$NaturalOrder.compare(Arrays.java:105); 	at java.base/java.util.TimSort.countRunAndMakeAscending(TimSort.java:355); 	at java.base/java.util.TimSort.sort(TimSort.java:234); 	at java.base/java.util.ArraysParallelSortHelpers$FJObject$Sorter.compute(ArraysParallelSortHelpers.java:145); 	at java.base/java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:746); 	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290); 	at java.base/java.util.concurrent.ForkJoinTask.doInvoke(ForkJoinTask.java:408); 	at java.base/java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:736); 	at java.base/java.util.Arrays.parallelSort(Arrays.java:1183); 	at htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:247); 	at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:182); 	at htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:202); 	at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:36); 	at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:16); 	at htsjdk.samtools.util.AbstractAsyncWriter$WriterRunnable.run(AbstractAsyncWriter.java:123); 	at java.base/java.lang.Thread.run(Thread.java:829); 	Suppressed: htsjdk.samtools.util.RuntimeIOException: Attempt to add record to closed writer.; 		at htsjdk.samtools.util.AbstractAsyncWriter.write(AbstractAsyncWriter.java:57); 		at htsjdk.samtools.AsyncSAMFileWriter.addAlignment(AsyncSAMFileWriter.java:58); 		at org.broadinstitute.hellbender.utils.read.SAMFileGATKReadWriter.addRead(SA",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452463087:320,load,loader,320,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452463087,6,"['concurren', 'load']","['concurrent', 'loader']"
Performance,"Hi @lbergelson I added **--disableAllReadFilters**, the log still says ""null"". Using GATK wrapper script /home/kh3/Softwares/gatk/build/install/gatk/bin/gatk; Running:; /home/kh3/Softwares/gatk/build/install/gatk/bin/gatk BwaSpark -I /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.bam -R /home/kh3/Resources/genome_b37/genome.fa --disableAllReadFilters --disableSequenceDictionaryValidation true -t 16 -O /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.aligned.bam; 12:08:44.856 INFO IntelGKLUtils - Trying to load Intel GKL library from:; jar:file:/home/kh3/Softwares/gatk/build/install/gatk/lib/gkl-0.1.2.jar!/com/intel/gkl/native/libIntelGKL.so; 12:08:44.905 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [September 17, 2016 12:08:44 PM EDT] org.broadinstitute.hellbender.tools.spark.bwa.BwaSpark --output /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.aligned.bam --threads 16 --reference /home/kh3/Resources/genome_b37/genome.fa --input /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.bam --disableSequenceDictionaryValidation true --disableAllReadFilters true --fixedChunkSize 100000 --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false; [September 17, 2016 12:08:44 PM EDT] Executing as kh3@rgcaahauva08091.rgc.aws.com on Linux 3.13.0-91-generic amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_101-b13; Version: Version:4.alpha.2-45-ga30af5a-SNAPSHOT; 12:08:44.930 INFO BwaSpark - Defaults.BUFFER_SIZE : 131072; 12:08:44.930 INFO BwaSpark - Defaults.COMPRESSION_LEVEL : 1; 12:08:44.930 INFO BwaSpark - Defaults.CREATE_INDEX : false; 12:08:44.930 INFO BwaSpark - Defaults.CREATE_MD5 : false; 12:08:44.930 INFO BwaSpark - Defaults.CUSTOM_READER_FACTORY : ; 12:08:44.930 INFO BwaSpark - Defaults.EBI_REFERENCE_SERVICE_URL_MAS",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2171#issuecomment-247785408:525,load,load,525,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2171#issuecomment-247785408,2,['load'],"['load', 'loaded']"
Performance,"Hi @lbergelson Thanks for the quick response! I just used the fasta as reference, and it still doesn't work. The log only says ""null"". Using GATK wrapper script /home/kh3/Softwares/gatk/build/install/gatk/bin/gatk; Running:; /home/kh3/Softwares/gatk/build/install/gatk/bin/gatk BwaSpark -I /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.bam -R /home/kh3/Resources/genome_b37/genome.fa --disableSequenceDictionaryValidation true -t 16 -O /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.aligned.bam; 16:55:32.261 INFO IntelGKLUtils - Trying to load Intel GKL library from:; jar:file:/home/kh3/Softwares/gatk/build/install/gatk/lib/gkl-0.1.2.jar!/com/intel/gkl/native/libIntelGKL.so; 16:55:32.310 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [September 16, 2016 4:55:32 PM EDT] org.broadinstitute.hellbender.tools.spark.bwa.BwaSpark --output /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.aligned.bam --threads 16 --reference /home/kh3/Resources/genome_b37/genome.fa --input /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.bam --disableSequenceDictionaryValidation true --fixedChunkSize 100000 --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false --disableAllReadFilters false; [September 16, 2016 4:55:32 PM EDT] Executing as kh3@rgcaahauva08091.rgc.aws.com on Linux 3.13.0-91-generic amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_101-b13; Version: Version:4.alpha.2-45-ga30af5a-SNAPSHOT; 16:55:32.335 INFO BwaSpark - Defaults.BUFFER_SIZE : 131072; 16:55:32.335 INFO BwaSpark - Defaults.COMPRESSION_LEVEL : 1; 16:55:32.335 INFO BwaSpark - Defaults.CREATE_INDEX : false; 16:55:32.335 INFO BwaSpark - Defaults.CREATE_MD5 : false; 16:55:32.335 INFO BwaSpark - Defaults.CUSTOM_READER_FACTORY : ; 16:55:32.335 INFO BwaSpark - Default",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2171#issuecomment-247709200:557,load,load,557,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2171#issuecomment-247709200,2,['load'],"['load', 'loaded']"
Performance,"Hi @lbergelson and @AJDCiarla ,. Thank you for your working!. I am the one who reported this bug and I had given it a try as @lbergelson suggested. I performed tests in two different scenarios:. 1. Using full path without any non-ascii characters as tmp path and it succeeded:; ```; /data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk --java-options ""-Xmx8G -Djava.io.tmpdir=/data/xieduo/gatktest"" BaseRecalibrator -R /data/reference/gatk_resource/Homo_sapiens_assembly38.fasta -I /data/xieduo/Immun_genomics/data/Łuksza_2022_Nature/bam/PAAD11N.bam --known-sites /data/xieduo/WES_pipe/pipeline/gatk_resource/dbsnp_146.hg38.vcf.gz --known-sites /data/reference/gatk_resource/1000G_phase1.snps.high_confidence.hg38.vcf.gz --known-sites /data/reference/gatk_resource/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz -O PAAD11N.recal_data.test.table; Using GATK jar /data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx8G -Djava.io.tmpdir=/data/xieduo/gatktest -jar /data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar BaseRecalibrator -R /data/reference/gatk_resource/Homo_sapiens_assembly38.fasta -I /data/xieduo/Immun_genomics/data/Łuksza_2022_Nature/bam/PAAD11N.bam --known-sites /data/xieduo/WES_pipe/pipeline/gatk_resource/dbsnp_146.hg38.vcf.gz --known-sites /data/reference/gatk_resource/1000G_phase1.snps.high_confidence.hg38.vcf.gz --known-sites /data/reference/gatk_resource/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz -O PAAD11N.recal_data.test.table; 13:35:32.710 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 13:35:32.890 INFO BaseRecalibrator - ------------------------------------------------------------; 13",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8005#issuecomment-1254561081:150,perform,performed,150,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005#issuecomment-1254561081,1,['perform'],['performed']
Performance,"Hi @lbergelson,. We experienced the related issue in GATK 4.1.8 (it persisted since 4.1.5 or early version as far as we know) when running `FilterAlignmentArtifacts` in one of our cluster but not the other. We narrowed down the issue, using the CPU differences (the working one does not support AVX2), to `libgkl_smithwaterman.so`. Paths are shortened for clarity in the following commands. ```; bash faa.sh ; Using GATK jar /app/gatk-package-4.1.8.0-local.jar; Running:; /bin/java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /app/gatk-package-4.1.8.0-local.jar FilterAlignmentArtifacts -V /output/sample.FilterMutectCalls.vcf.gz -R /db/hs37d5.fa --bwa-mem-index-image /db/hg38.fa.img -I /output/sample.Mutect2.bam -O sample.somatic_filter.test.vcf.gz --use-jdk-inflater true; 19:11:56.929 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/app/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 19:11:56.943 INFO NativeLibraryLoader - Loading libgkl_smithwaterman.so from jar:file:/app/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_smithwaterman.so; 19:11:56.944 INFO SmithWatermanAligner - Using AVX accelerated SmithWaterman implementation; 19:11:57.168 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/app/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jul 19, 2020 7:11:57 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 19:11:57.324 INFO FilterAlignmentArtifacts - ------------------------------------------------------------; 19:11:57.324 INFO FilterAlignmentArtifacts - The Genome Analysis Toolkit (GATK) v4.1.8.0; 19:11:57.325 INFO FilterAlignmentArtifacts - For support and documentation go to https://software.broadinstitute.org/gatk/; 19:11:57.325 INFO Fil",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-660645356:933,Load,Loading,933,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-660645356,1,['Load'],['Loading']
Performance,"Hi @ruslan-abasov,. I believe your GermlineCNVCaller results should have inherited the correct dictionary from the count files. The issue is you created some GermlineCNVCaller shards (e.g., shard 4) with inappropriately ordered intervals (since these were instead ordered w.r.t. to the idiosyncratic dictionary you attached). However, I think if you just reshard and rerun GermlineCNVCaller for any such shards, you may be able to reuse most of your results. For example, you could take your shard 4 interval list, which contains intervals from chr18, chr19, and chr1, and reshard these intervals into two shards: 4a containing chr18-19 intervals, and 4b containing chr1 intervals. After rerunning 4a and 4b through GermlineCNVCaller, you should be able to use PostprocessGermlineCNVCalls to stitch together shards 4b, 1, 2, 3, and 4a, since these will be ordered w.r.t. the correct dictionary from the count files (i.e, they will contain intervals in the order chr1, chr10-19). Of course, you will want not want to perform this exact procedure; you'll want to generalize it to whatever will yield the correct order for all 10 of your shards across all contigs. Again, this may be error prone and I can't guarantee that it will be successful, since I haven't tried it myself. I would personally just rerun the pipeline. You might be able to cut down on runtime by using smaller shards (I believe we typically shard the entire genome into far more than 10 shards, which we usually run in parallel) and making sure you set parameters appropriately for WGS. @mwalker174 has the most experience running on WGS and should be able to provide you the latest recommendations, or you might be able to find them by searching GitHub or the GATK Forums. Your point is well taken about failing earlier, and I think I've outlined the best strategy above. It is impossible to catch all possible errors early, but for some we can certainly fail before the GermlineCNVCaller step.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6924#issuecomment-720091802:1016,perform,perform,1016,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6924#issuecomment-720091802,1,['perform'],['perform']
Performance,"Hi @shengqh, tools such as you describe are still under development. We are also still working to tune hyperparameters in the main gCNV pipeline, so thanks for trying it out! Please let us know if you encounter any issues.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5373#issuecomment-434275953:98,tune,tune,98,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5373#issuecomment-434275953,1,['tune'],['tune']
Performance,"Hi @tedsharpe, . A quick follow-up question on this – how does the function currently handle the Q-scores from overlapping portions of paired-end reads? @BenjaminWehnert1008 noticed that read pre-merging and dual Q-score integration can help improve our performance for 1nt variants. Best wishes,; Max",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8995#issuecomment-2416095456:254,perform,performance,254,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8995#issuecomment-2416095456,1,['perform'],['performance']
Performance,"Hi @tomwhite! The serializer registrations in ADAM shouldn't effect any GATK serialization, unless you're serializing classes from ADAM (such as `org.bdgenomics.format.avro.AlignmentRecord`); additionally, we haven't seen any performance issues with serialization in ADAM 0.23.0 outside of the GATK. We actually made a number of patches to eliminate logging in 0.23.0 (relative to 0.22.0), so I'd doubt that is the culprit. The one exception to this is logging when writing Parquet out to disk, which greatly increased sometime between ADAM 0.21.0 and 0.23.0, due to a change in Parquet versions upstream in Spark. However, this would only impact you if you were writing Parquet, and additionally this issue was resolved with the release of Spark 2.2.0, so you should see the logging go away with #4314. If I had to hypothesize anything, I'd suggest that the 2bit file change is the one thing that could be biting you. That said, we've been running with this 2bit file code quite frequently on AWS and Azure for at least the last 6 months using GATK HC on WES and WGS data and haven't seen any performance issues. I show that the changes in the 2bit file code between ADAM 0.20.0 and 0.23.0 are API compatible, so if you check out ADAM 0.23.0 and then `git checkout adam-parent-spark2_2.11-0.20.0 adam-core/src/main/scala/org/bdgenomics/adam/util/TwoBitFile.scala` and build the ADAM JAR (and then package that jar into GATK), you should be able to test that hypothesis.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4376#issuecomment-366308897:226,perform,performance,226,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4376#issuecomment-366308897,2,['perform'],['performance']
Performance,"Hi @zaneChou1,. Thanks for filing the issue---I'm actually working on this at the moment. The gCNV python code requires some updating, since the APIs for inference changed after the pymc3 version we use (which is the primary reason we stuck with it). Because the changes required go beyond those in the PR you opened, I'll go ahead and close it, but thanks for making the effort to help us keep things updated! We certainly appreciate it. I would be surprised if updating numpy led to drastic performance improvements (1.17.5 was only released in 1/2020), but it's possible there have been improvements in the pymc3 python code. However, we are planning to move much of the python inference code over to pyro, for which the numpy upgrade is also necessary.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6978#issuecomment-733001543:493,perform,performance,493,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6978#issuecomment-733001543,1,['perform'],['performance']
Performance,"Hi Adam,. Maybe I'm thinking naively here - and I don't have access to a complete and proper Spark cluster for rigorous testing - but just as a simple test of loading a VCF via Spark, I took [PrintReadsSpark.java](https://github.com/broadinstitute/gatk/blob/030858bc08328200b9df287db2571b907189ec66/src/main/java/org/broadinstitute/hellbender/tools/spark/pipelines/PrintReadsSpark.java) and performed following updates:; - Copied `./src/test/resources/org/broadinstitute/hellbender/utils/SequenceDictionaryUtils/test.vcf` into the local directory.; - Renamed the copy of `PrintReadsSpark.java` as `PrintVCFSpark.java`; - Added `import org.broadinstitute.hellbender.utils.variant.Variant;`; - Added `import org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSource;`; - As a test, I changed to the `runTool` method with the following to print the information in the first element in the RDD:. ``` Java; JavaRDD<Variant> rddParallelVariants =; variantsSparkSource.getParallelVariants(output);. System.out.println( rddParallelVariants.first().toString() );; ```. And after re-compiling GATK and running `PrintVCFSpark`, I got the following to print the first element of the RDD:. ``` Bash; $ ./gatk-launch PrintVCFSpark --input test.vcf --output test.vcf. Running:; /home/pgrosu/me/hellbender_broad_institute/gatk/build/install/gatk/bin/gatk PrintVCFSpark --input test.vcf --output test.vcf; [February 14, 2016 7:04:16 PM EST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintVCFSpark --output test.vcf --input test.vcf --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false; [February 14, 2016 7:04:16 PM EST] Executing as pgrosu on Linux 2.6.32-358.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_05-b13; Version: Version:4.alpha-86-g154d0a8-SNAPSHOT JdkD",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857:159,load,loading,159,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857,2,"['load', 'perform']","['loading', 'performed']"
Performance,"Hi Adam,. Thank you for this amazing work, and would like to suggest something small without disturbing the test-flow. So unless I'm not mistaken, the class that is consuming the memory is [NestedIntegerArray.java](https://github.com/broadgsa/gatk/blob/master/public/gatk-utils/src/main/java/org/broadinstitute/gatk/utils/collections/NestedIntegerArray.java). Why not try an experiment where you implement something similar to [Google's SSTable](https://www.igvita.com/2012/02/06/sstable-and-log-structured-storage-leveldb/) that got release as [leveldb](https://github.com/google/leveldb). For instance the key-index lookup for you can be hash or a hash+offset. Basically these structures borrow from the concept of a [Log-Structured Merge-Tree (LSM-Tree) - link is a PDF paper](http://paperhub.s3.amazonaws.com/18e91eb4db2114a06ea614f0384f2784.pdf). You can push most of these to disk instead of memory, and they can be compressed as well. The indices can remain in memory for fast-lookup with the data staying on disk. So for instance, you can generate an key-index digest of specific values via `hash_function(numReadGroups.id, qualDimension.index, eventDimension.type)`, whose result you then use to quickly lookup the value for that key - which would reside on disk. That can be further improved with a priority queue cache for the most accessed values. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1460#issuecomment-180626838:1318,queue,queue,1318,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1460#issuecomment-180626838,2,"['cache', 'queue']","['cache', 'queue']"
Performance,"Hi Adam,. That is the verbosity level that is performed by `bwa`. So since [BWASpark](https://github.com/broadinstitute/gatk/blob/c9807a1a94a60b69e8184c1fcf3083516bb0a78b/src/main/java/org/broadinstitute/hellbender/tools/spark/bwa/BwaSpark.java#L47) launches `BwaSparkEngine` as follows:. ```; final BwaSparkEngine engine = new BwaSparkEngine(bwaArgs.numThreads, bwaArgs.fixedChunkSize, referenceFileName);; ```. and launches the alignment as follows:. ```; final JavaRDD<GATKRead> reads = engine.alignWithBWA(ctx, unalignedReads, readsHeader);; ```. That in turn calls the `align()` method within [BwaSparkEngine.java](https://github.com/broadinstitute/gatk/blob/9fb4d756afbbad58a7e709e2e9fd308983ad255b/src/main/java/org/broadinstitute/hellbender/tools/spark/bwa/BwaSparkEngine.java#L72):. ```; final JavaRDD<String> samLines = align(shortReadPairs);; ```. which then instantiates a new [BwaMem](https://github.com/broadinstitute/gatk/blob/9fb4d756afbbad58a7e709e2e9fd308983ad255b/src/main/java/org/broadinstitute/hellbender/tools/spark/bwa/BwaSparkEngine.java#L101) object:. ```; final BwaMem mem = new BwaMem(index);; ```. Since the BWA implementation at [lindenb/jbwa](https://github.com/lindenb/jbwa) is basically a direct call to Heng's BWA as a library, the BWA option for verbosity is set by the `-v` argument as noted [here](http://bio-bwa.sourceforge.net/bwa.shtml#3):. ```; -v INT Control the verbose level of the output. This option has not been fully supported ; throughout BWA. Ideally, a value 0 for disabling all the output to stderr; 1 for ; outputting errors only; 2 for warnings and errors; 3 for all normal messages; 4 or ; higher for debugging. When this option takes value 4, the output is not SAM. [3] ; ```. This is used in Heng's [bwamem.c](https://github.com/lh3/bwa/blob/5961611c358e480110793bbf241523a3cfac049b/bwamem.c#L1224-L1226) file - and several other places - which generates the printout you see, as follows:. ```; if (bwa_verbose >= 3); fprintf(stderr, ""[M::%s] P",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2054#issuecomment-235675398:46,perform,performed,46,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2054#issuecomment-235675398,1,['perform'],['performed']
Performance,"Hi David,. 1. Here, we must distinguish between the WES minimal example (default parameters, command lines in my original bug report) and our ""production pipeline"". In the minimal example, we do not use PON. In the production pipeline, we use the exome PON file from: gs://gatk-best-practices/somatic-hg38/1000g_pon.hg38.vcf.gz as recommended here: https://gatk.broadinstitute.org/hc/en-us/articles/360035890631-Panel-of-Normals-PON-; In the production pipeline, we also use gnomAD 3.1.2, filtered for AF>0 and FILTER=PASS as ""germline resource"". Some more data from the production pipeline, just in case it could help. ; Since the PON is exome-only, we were not sure whether including the PON would improve the performance of our WGS workflow in the ""production pipeline"" with gatk version 4.1.7.0, so we ran experiments:; - calling without germline resource and without PON; - calling with germline resource only, without PON; - calling with both germline resource and PON. The results:. ![WGS_FD_tumor-normal_reference_workflow_v04_WGS_FD_tumor-normal_reference_workflow_v04_gnomAD_only_WGS_FD_tumor-normal_reference_workflow_v04_no_gnomAD_no_PON](https://user-images.githubusercontent.com/15612230/182359001-a173e711-748b-49ec-b03f-71e5a8293c51.png). We also conducted experiments with a subset of the WES FD sample (FD_1, 1/3 of the full ~100x FD dataset):. - calling without germline resource and without PON; - calling with PON only, without germline resource; - calling with germline resource only, without PON; - calling with both germline resource and PON. The results (please note that the labeling conventions are now different compared to WGS experiments, apologies for the inconvenience):. ![FD_1_T_tumor-normal_WES_muTect2_FD_1_tumor-normal_muTect2_PON_FD_1_tumor-normal_muTect2_gnomAD_FD_1_tumor-normal_muTect2_PON_gnomAD](https://user-images.githubusercontent.com/15612230/182358963-97f04d12-94c6-4f77-acef-c3ebcd78a98f.png). 2. The reference is GRCh38.primary_assembly.genome.fa; The",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1202344705:712,perform,performance,712,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1202344705,1,['perform'],['performance']
Performance,"Hi again,; I tried installing java8 and switching to this version prior to running gatk. It runs and looks to be running the right Java, but spits out roughly the same error:. Thoughts?. /cold/drichard/gatk/./gatk --java-options ""-Xmx25g"" SplitNCigarReads \; -R /cold/drichard/VARIANTS/Homo_sapiens.GRCh38.dna.primary_assembly.fa -I subset_TINY_rehead.bam \; --tmp-dir /thing -O thing.bam; Using GATK jar /cold/drichard/gatk/build/libs/gatk-package-4.3.0.0-44-g227bbca-SNAPSHOT-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx25g -jar /cold/drichard/gatk/build/libs/gatk-package-4.3.0.0-44-g227bbca-SNAPSHOT-local.jar SplitNCigarReads -R /cold/drichard/VARIANTS/Homo_sapiens.GRCh38.dna.primary_assembly.fa -I subset_TINY_rehead.bam --tmp-dir /thing -O thing.bam; 15:34:59.974 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/cold/drichard/gatk/build/libs/gatk-package-4.3.0.0-44-g227bbca-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_compression.so; 15:35:00.220 INFO SplitNCigarReads - ------------------------------------------------------------; 15:35:00.226 INFO SplitNCigarReads - The Genome Analysis Toolkit (GATK) v4.3.0.0-44-g227bbca-SNAPSHOT; 15:35:00.226 INFO SplitNCigarReads - For support and documentation go to https://software.broadinstitute.org/gatk/; 15:35:00.226 INFO SplitNCigarReads - Executing as drichard@illuvatar on Linux v5.19.0-32-generic amd64; 15:35:00.226 INFO SplitNCigarReads - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_362-8u362-ga-0ubuntu1~22.04-b09; 15:35:00.226 INFO SplitNCigarReads - Start Date/Time: March 2, 2023 3:34:59 PM EST; 15:35:00.226 INFO SplitNCigarReads - ------------------------------------------------------------; 15:35:00.226 INFO SplitNCigarReads - ------------------------------------------------------------; 15:35:00.227 INFO SplitNCigarReads - HTSJDK Version: 3.0.1; 15:35:00.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452525485:940,Load,Loading,940,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452525485,1,['Load'],['Loading']
Performance,"Hi everyone. I try to run gatk 4.2.5.0 VariantAnnotator using gnomAD data. However I get this error message java.lang.IllegalStateException: Allele in genotype C not in the variant context [C*, CT] can you maybe advise whats going on? . java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx30G -jar /run/media/riadh/One Touch1/Analysis/gatk-4.2.4.1/gatk-package-4.2.5.0-local.jar VariantAnnotator -V PE69_chr3.vcf -R /run/media/riadh/One Touch/Reference_data_b38/resources_broad_hg38_v0_Homo_sapiens_assembly38.fasta --resource:gnomad /run/media/riadh/One Touch/Reference_data_b38/gnomad.genomes.v3.1.2.sites.chr3.vcf.bgz -E gnomad.nhomalt -E gnomad.ALT -E gnomad.AF -O PE69_ch3_vep_cadd_gnomad.vcf --resource-allele-concordance; 10:58:19.715 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/run/media/riadh/One%20Touch1/Analysis/gatk-4.2.4.1/gatk-package-4.2.5.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Mar 17, 2022 10:58:19 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 10:58:19.796 INFO VariantAnnotator - ------------------------------------------------------------; 10:58:19.796 INFO VariantAnnotator - The Genome Analysis Toolkit (GATK) v4.2.5.0; 10:58:19.796 INFO VariantAnnotator - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:58:19.797 INFO VariantAnnotator - Executing as riadh@ikm-unix-1012.uio.no on Linux v5.16.12-200.fc35.x86_64 amd64; 10:58:19.797 INFO VariantAnnotator - Java runtime: OpenJDK 64-Bit Server VM v11.0.14.1+1; 10:58:19.797 INFO VariantAnnotator - Start Date/Time: March 17, 2022 at 10:58:19 AM CET; 10:58:19.797 INFO VariantAnnotator - ------------------------------------------------------------; 10:58:19.797 INFO VariantAnnotator - -------------------------------",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6689#issuecomment-1070784053:881,Load,Loading,881,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6689#issuecomment-1070784053,1,['Load'],['Loading']
Performance,"Hi! I have the same issue as @chandrans.; When I run Mutect2 this is the error:; `(gatk) root@d387db9e4351:/Desktop# gatk Mutect2 -R /Desktop/UCSC_hg19_genome.fasta -I /Desktop/HP0049.bam -O /Desktop/HP0049.vcf.g; Using GATK jar /gatk/gatk-package-4.1.1.0-local.ja; Running:. java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.1.1.0-local.jar Mutect2 -R /Desktop/UCSC_hg19_genome.fasta -I /Desktop/HP0049.bam -O /Desktop/HP0049.vcf.g; 08:27:06.032 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.1.0-local.jar!/com/intel/gkl/native/libgkl_compression.s; Apr 23, 2019 8:27:10 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngin; INFO: Failed to detect whether we are running on Google Compute Engine. 08:27:10.882 INFO Mutect2 - -----------------------------------------------------------; 08:27:10.883 INFO Mutect2 - The Genome Analysis Toolkit (GATK) v4.1.1.; 08:27:10.883 INFO Mutect2 - For support and documentation go to https://software.broadinstitute.org/gatk/. 08:27:10.884 INFO Mutect2 - Executing as root@d387db9e4351 on Linux v4.9.125-linuxkit amd64. 08:27:10.884 INFO Mutect2 - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_191-8u191-b12-0ubuntu0.16.04.1-b12. 08:27:10.885 INFO Mutect2 - Start Date/Time: April 23, 2019 8:27:05 AM UT; 08:27:10.885 INFO Mutect2 - -----------------------------------------------------------; 08:27:10.886 INFO Mutect2 - -----------------------------------------------------------; 08:27:10.887 INFO Mutect2 - HTSJDK Version: 2.19.; 08:27:10.887 INFO Mutect2 - Picard Version: 2.19.; 08:27:10.887 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 2. 08:27:10.888 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : fals; 08:27:10.888 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : tru; 08:27:10.888 INFO Mutec",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4665#issuecomment-485729136:610,Load,Loading,610,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4665#issuecomment-485729136,1,['Load'],['Loading']
Performance,"Hi, . I am experiencing exactly the same issue. I also run gatk on a cluster using singularity. When using --include-non-variant-sites the same error message is reported for all but the first chromosome. When exluding non variant sites it works fine. . ```; 05:04:16.405 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.5.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 05:04:16.556 INFO GenotypeGVCFs - ------------------------------------------------------------; 05:04:16.559 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.5.0.0; 05:04:16.559 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 05:04:16.563 INFO GenotypeGVCFs - Initializing engine; 05:04:16.929 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.5.1-84e800e; 16:04:16.979 INFO NativeGenomicsDB - pid=680685 tid=680686 No valid combination operation found for INFO field InbreedingCoeff - the field will NOT be part of INFO fields in the generated VCF records; 16:04:16.979 INFO NativeGenomicsDB - pid=680685 tid=680686 No valid combination operation found for INFO field MLEAC - the field will NOT be part of INFO fields in the generated VCF records; 16:04:16.979 INFO NativeGenomicsDB - pid=680685 tid=680686 No valid combination operation found for INFO field MLEAF - the field will NOT be part of INFO fields in the generated VCF records; 05:04:17.059 INFO GenotypeGVCFs - Done initializing engine; 05:04:17.104 INFO ProgressMeter - Starting traversal; 05:04:17.105 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 05:04:17.124 INFO GenotypeGVCFs - Shutting down engine; [June 26, 2024 at 5:04:17 AM GMT] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=1126170624; java.lang.IllegalStateException: There are no sources based on those query parameters; at org.genomicsdb.reader.GenomicsDBFeature",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8415#issuecomment-2191214079:298,Load,Loading,298,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8415#issuecomment-2191214079,1,['Load'],['Loading']
Performance,"Hi, @jamesemery .I downloaded the `GTF` file for `basic gene annotation` in the CHR regions from gencode. The version I obtained was `Release 43 (GRCh38.p13)`. After making some modifications as follow, I managed to run SVAnnotate without encountering any errors. ; ```; sed 's/; tag ""Ensembl_canonical""//g' gencode.v43.basic.annotation.gtf|sed 's/; tag ""overlaps_pseudogene""//g'|sed 's/; tag ""readthrough_gene""//g'|sed 's/; tag ""artifactual_duplication""//g' > gencode.v43.basic.modified_annotation.gtf; ```; However, the tool still fails to provide meaningful annotation information. The output file remains unchanged compared to the original file. Based on the SVAnnotate output as follow, I suspect that the issue might be caused by 'Current Locus unmapped.' ; ```; 16:11:10.044 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/Division/1user/2_Exome/Tools/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 16:11:10.072 INFO SVAnnotate - ------------------------------------------------------------; 16:11:10.074 INFO SVAnnotate - The Genome Analysis Toolkit (GATK) v4.4.0.0; 16:11:10.074 INFO SVAnnotate - For support and documentation go to https://software.broadinstitute.org/gatk/; 16:11:10.074 INFO SVAnnotate - Executing as user@localhost.localdomain on Linux v3.10.0-1160.90.1.el7.x86_64 amd64; 16:11:10.074 INFO SVAnnotate - Java runtime: Java HotSpot(TM) 64-Bit Server VM v17.0.7+8-LTS-224; 16:11:10.075 INFO SVAnnotate - Start Date/Time: 2023年7月5日 CST 下午4:11:10; 16:11:10.075 INFO SVAnnotate - ------------------------------------------------------------; 16:11:10.075 INFO SVAnnotate - ------------------------------------------------------------; 16:11:10.075 INFO SVAnnotate - HTSJDK Version: 3.0.5; 16:11:10.075 INFO SVAnnotate - Picard Version: 3.0.0; 16:11:10.076 INFO SVAnnotate - Built for Spark Version: 3.3.1; 16:11:10.076 INFO SVAnnotate - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 16:11:10.076 INFO SVAnnotate - ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8394#issuecomment-1621377138:809,Load,Loading,809,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8394#issuecomment-1621377138,1,['Load'],['Loading']
Performance,"Hi, I am encountering a similar error attempting to run `GenotypeGVCFs` in `gatk v4.1.2.0`. It runs very briefly and writes a handful of variants from a single scaffold to the output file but then exits with `java.lang.ArrayIndexOutOfBoundsException` (see below). I have also tried adding the `-L` flag and an interval list, which performs similarly but outputs variants from a different scaffold. Any idea why this is happening or what I can do to overcome this problem? I have run `GenomicsDBImport` and `GenotypeGVCFs` successfully in the past (same version, same computer) on a different dataset, so I'm not sure what about this data is causing the problem. Any guidance is much appreciated!. Thanks,; Jessie. ```; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/jsalt/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar GenotypeGVCFs -R /nfs/data1/jsalt/3RAD/colinus_virginianus_13May2017_V3Fw6_newchrom.fasta -V gendb://odont_cyr_8_snp_db -O odont_cyr_8_snp_db.vcf; 14:59:47.866 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/jsalt/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 03, 2020 2:59:59 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 14:59:59.674 INFO GenotypeGVCFs - ------------------------------------------------------------; 14:59:59.675 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.1.2.0; 14:59:59.675 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 15:00:09.686 INFO GenotypeGVCFs - Executing as jsalt@mustard on Linux v3.10.0-957.1.3.el7.x86_64 amd64; 15:00:09.686 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_192-b01; 15:00:09.687 INFO GenotypeGVCFs - Start Date/Time: F",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6357#issuecomment-581619640:331,perform,performs,331,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6357#issuecomment-581619640,1,['perform'],['performs']
Performance,"Hi, I encountered a similar error attempting to run GenotypeGVCFs in gatk v4.1.4.1. ; It ran very briefly and writed few variants to the output file but then exited with java.lang.ArrayIndexOutOfBoundsException. . I solved the problem (I created a merged vcf) by performing bgzip (and tabix) of each gvcf file before running GenomicsDBImport and GenotypeGVCFs.; I don't know if it could be a solution also for your issues. command:; for P in my_gvcf.list; do; bgzip -c $P.g.vcf > $P\.g.vcf.gz. tabix -p vcf $P.g.vcf.gz; done. gatk --java-options ""-Xmx4g -Xms4g"" GenomicsDBImport --genomicsdb-workspace-path my_database --sample-name-map sample_map --batch-size 50 --consolidate true --tmp-dir=tmp/ --reader-threads 5 -L file.bed. gatk --java-options ""-Xmx12g -Xms12g"" GenotypeGVCFs -R hg19.fa -V gendb://my_database -O global.vcf --new-qual --tmp-dir=tmp/",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6357#issuecomment-583260728:263,perform,performing,263,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6357#issuecomment-583260728,1,['perform'],['performing']
Performance,"Hi, you have probably deleted out the previous comment with the `No space left on device` error, but was wondering if you could check if you have enough space, import to a new workspace and turn on `--genomicsdb-shared-posixfs-optimizations` with GenomicsDBImport?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-759831424:227,optimiz,optimizations,227,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-759831424,1,['optimiz'],['optimizations']
Performance,"Hi,; I am trying to generate vcf using GATK pipeline from bam file, but everytime, I am getting the following exception:; 01:13:15.801 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data1/ngs/programs/gatk-4.0.0.0/gatk-package-4.0.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 01:13:16.075 INFO HaplotypeCaller - ------------------------------------------------------------; 01:13:16.075 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.0.0.0; 01:13:16.075 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 01:13:16.076 INFO HaplotypeCaller - Executing as shashank@grande on Linux v3.13.0-79-generic amd64; 01:13:16.076 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_72-internal-b15; 01:13:16.076 INFO HaplotypeCaller - Start Date/Time: January 18, 2020 1:13:15 AM IST; 01:13:16.076 INFO HaplotypeCaller - ------------------------------------------------------------; 01:13:16.076 INFO HaplotypeCaller - ------------------------------------------------------------; 01:13:16.077 INFO HaplotypeCaller - HTSJDK Version: 2.13.2; 01:13:16.077 INFO HaplotypeCaller - Picard Version: 2.17.2; 01:13:16.077 INFO HaplotypeCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 01:13:16.078 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 01:13:16.078 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 01:13:16.078 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 01:13:16.078 INFO HaplotypeCaller - Deflater: IntelDeflater; 01:13:16.078 INFO HaplotypeCaller - Inflater: IntelInflater; 01:13:16.078 INFO HaplotypeCaller - GCS max retries/reopens: 20; 01:13:16.078 INFO HaplotypeCaller - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 01:13:16.078 INFO HaplotypeCaller - Initializing engine; 01:13:17.087 INF",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5947#issuecomment-575601220:162,Load,Loading,162,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5947#issuecomment-575601220,1,['Load'],['Loading']
Performance,"Hi. Same Error on 4.0.3.0; ```; java -jar /usr/hpc-bio/gatk/gatk-package-4.0.3.0-local.jar Mutect2 --verbosity WARNING -R /usr/bio-ref/GRCh38.p0.dnaref/dnaref.fa --germline-resource /usr/bio-ref/GRCh38.p0.dnaref/common.vcf --max-reads-per-alignment-start 100 -L X -I /biowrk/BaseSpace/bam.bwa/HiSeqX-PCR-free-v2.5-NA12878/md.bam -tumor HiSeqX-PCR-free-v2.5-NA12878 -O mutect2.tumor-only.vcf; 23:50:01.301 WARN IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; [March 28, 2018 11:50:04 PM CST] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.17 minutes.; Runtime.totalMemory()=2354577408; java.lang.IndexOutOfBoundsException: Index: 0, Size: 0; at java.util.ArrayList.rangeCheck(ArrayList.java:657); at java.util.ArrayList.get(ArrayList.java:433); at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2Engine.isActive(Mutect2Engine.java:316); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.loadNextAssemblyRegion(AssemblyRegionIterator.java:159); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.next(AssemblyRegionIterator.java:135); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.next(AssemblyRegionIterator.java:34); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:290); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:271); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:893); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4578#issuecomment-376937140:963,load,loadNextAssemblyRegion,963,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4578#issuecomment-376937140,1,['load'],['loadNextAssemblyRegion']
Performance,"Hm. Even though I totally expected to be able to reproduce this, I can't. I tried quite a few different combinations. I think this one captures the steps that were reported, but it works:. ```; (base) /tmp/test a /Users/cnorman/projects/gatk/gatk MergeVcfs -I data/calling/my.vcf.gz -I data/calling/b.vcf.gz -O out.vcf.gz; Using GATK jar /Users/cnorman/projects/gatk/build/libs/gatk-package-4.1.7.0-41-g79586b8-SNAPSHOT-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /Users/cnorman/projects/gatk/build/libs/gatk-package-4.1.7.0-41-g79586b8-SNAPSHOT-local.jar MergeVcfs -I data/calling/my.vcf.gz -I data/calling/b.vcf.gz -O out.vcf.gz; 16:00:13.443 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/cnorman/projects/gatk/build/libs/gatk-package-4.1.7.0-41-g79586b8-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_compression.dylib; [Mon Jun 22 16:00:13 EDT 2020] MergeVcfs --INPUT data/calling/my.vcf.gz --INPUT data/calling/b.vcf.gz --OUTPUT out.vcf.gz --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX true --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false; [Mon Jun 22 16:00:13 EDT 2020] Executing as cnorman@WMCEA-78B on Mac OS X 10.13.6 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_111-b14; Deflater: Intel; Inflater: Intel; Provider GCS is available; Picard version: Version:4.1.7.0-41-g79586b8-SNAPSHOT; [Mon Jun 22 16:00:13 EDT 2020] picard.vcf.MergeVcfs done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=373293056; Tool returned:; 0; ```; The only way I can reproduce it is to delete one of the files so it *really* doesn't exist at the specified location:; ```; (base) /tmp/test a /Users/cnorman/projects/gatk/gatk Merge",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6664#issuecomment-647743321:814,Load,Loading,814,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6664#issuecomment-647743321,1,['Load'],['Loading']
Performance,"Hm. Just yesterday we updated from TF 1.4 to 1.9. Although this makes it more compelling to switch the default to Intel-optimized, we may still have an issue for the reasons outlined in the previous PR (academic users, not all GCS zones guaranty AVX hardware, and its still unclear to me if Travis, which uses both GCS and EC2, makes such a guaranty). It [sounds like](https://github.com/tensorflow/tensorflow/issues/18689) the failure mode is to crash. For running inference at least (training may be a different story), we may need something better. Another option is that it sounds like its possible to build our own distribution without AVX dependencies to use as a fallback.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-429316465:120,optimiz,optimized,120,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-429316465,1,['optimiz'],['optimized']
Performance,"Hmm, actually, could this be a problem due to the way the native libraries are loaded in the test code? Note that we first cycle through all implementations in the DataProvider, loading the respective library for each implementation via the `synchronized boolean load` method in the `NativeLibraryLoader`. I'm not really that familiar with concurrency in Java (nor loading native libraries, for that matter), but it seems that the intermittent failure goes away when I refactor the test to remove the DataProvider (by just looping through the implementations in the test method). Perhaps related to https://github.com/broadinstitute/gatk/issues/5339?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607596205:79,load,loaded,79,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607596205,5,"['concurren', 'load']","['concurrency', 'load', 'loaded', 'loading']"
Performance,"Hmm, guess I didn’t realize how important it was to subset to high-confidence regions. Most of the false positives above are coming from the low-confidence regions, so lower precision may actually indicate higher sensitivity to any real variants that may be there and missing in the truth set. When optimizing parameters in the past, have we always just focused on the high confidence regions and called it a day?. Also, do we typically run HC/M2 identically in low/high confidence regions? I’m noticing that it’s taking much longer to get through the low confidence regions.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-713168743:299,optimiz,optimizing,299,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-713168743,1,['optimiz'],['optimizing']
Performance,"Hmm, it looks like we could perform more checks upstream in GermlineCNVCaller as well. I think it might be also possible to introduce inconsistencies there by using read counts and sharded interval lists with different dictionaries or contig orders (which might have happened here). The canonical dictionary is taken from the first read count file, but the intervals pass through the `-L` machinery and I think any dictionary information is not checked (probably because it will not be present if Picard interval lists were not used as input); intervals are also just (re)sorted by the read-count dictionary to use in subsetting counts. EDIT: Seems like checking intervals for the dictionary may be something we should properly add at the engine level. See e.g. the following `TODO`:. ````; /**; * Returns the ""best available"" sequence dictionary or {@code null} if there is no single best dictionary.; *; * The algorithm for selecting the best dictionary is as follows:; * 1) If a master sequence dictionary was specified, use that dictionary; * 2) if there is a reference, then the best dictionary is the reference sequence dictionary; * 3) Otherwise, if there are reads, then the best dictionary is the sequence dictionary constructed from the reads.; * 4) Otherwise, if there are features and the feature data source has only one dictionary, then that one is the best dictionary.; * 5) Otherwise, the result is {@code null}.; *; * TODO: check interval file(s) as well for a sequence dictionary; * ...; * @return best available sequence dictionary given our inputs or {@code null} if no one dictionary is the best one.; */; public SAMSequenceDictionary getBestAvailableSequenceDictionary() {; ````. We could also just require a sequence dictionary for the relevant tools and/or somehow try to reconcile with dictionaries from other inputs, but I think it's better to check and fail.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6924#issuecomment-718792218:28,perform,perform,28,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6924#issuecomment-718792218,1,['perform'],['perform']
Performance,"Hmm, looks like we lose events 1 and 3 with CollectReadCounts at 250bp using analogous ModelSegments parameters. However, I experimented with tweaking the segmentation to work on the copy ratios (rather than the log2 copy ratios), which seems to recover them. Although one of the goals of having evaluations backed by SV truth sets is to tune such parameters/methods, I'm beginning to think that SV integration might benefit from using the CNV tools in a more customized pipeline---especially if maximizing sensitivity at resolutions of ~100bp jointly with breakpoint evidence is the goal. For example, you might imagine a tool that directly uses CNV backend code to collect coverage over regions specified by `-L`, builds a PoN, denoises, and segments on the fly. Or we can put together a custom WDL optimized for sensitivity. Let's discuss in person?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4519#issuecomment-372875222:338,tune,tune,338,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4519#issuecomment-372875222,2,"['optimiz', 'tune']","['optimized', 'tune']"
Performance,"Hmm, running `./gradlew --info ...` yields the following snippet:. ```; Gradle suite > Gradle test > org.broadinstitute.hellbender.utils.downsampling.ReservoirDownsamplerUnitTest > testReservoirDownsampler[29](TestDataProvider(ReservoirDownsamplerTest: reservoirSize=10000 totalReads=10000 expectedNumReadsAfterDownsampling=10000 expectedNumDiscardedItems=0)) STANDARD_ERROR; 01:40:10.641 WARN gatk - Running test: TestDataProvider(ReservoirDownsamplerTest: reservoirSize=10000 totalReads=10000 expectedNumReadsAfterDownsampling=10000 expectedNumDiscardedItems=0); Finished 130000 tests; Finished 140000 tests. Gradle suite > Gradle test > org.broadinstitute.hellbender.utils.pairhmm.VectorPairHMMUnitTest STANDARD_ERROR; 01:40:14.522 WARN NativeLibraryLoader - Unable to load libgkl_pairhmm_fpga.so from native/libgkl_pairhmm_fpga.so (/tmp/libgkl_pairhmm_fpga17703278887667828152.so: libgkl_pairhmm_shacc.so: cannot open shared object file: No such file or directory); #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fe1a5cd00f2, pid=6969, tid=6997; #; # JRE version: OpenJDK Runtime Environment (11.0.2+9) (build 11.0.2+9); # Java VM: OpenJDK 64-Bit Server VM (11.0.2+9, mixed mode, tiered, compressed oops, g1 gc, linux-amd64); # Problematic frame:; # V [libjvm.so+0x8fd0f2] jni_GetByteArrayElements+0x72; #; # Core dump will be written. Default location: Core dumps may be processed with ""/usr/share/apport/apport %p %s %c %d %P"" (or dumping to /home/travis/build/broadinstitute/gatk/core.6969); #; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid6969.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; #; Starting process 'Gradle Test Executor 2'. Working directory: /home/travis/build/broadinstitute/gatk Command: /usr/local/lib/jvm/openjdk11/bin/java -Dgatk.spark.debug -Dorg.gradle.native=false -Dsamjdk.compres",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607332088:772,load,load,772,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607332088,1,['load'],['load']
Performance,"Hmm, thanks for suggesting the addition of a regression test @fleharty. This caused me to realize that I actually missed another gap in the previous filtering logic that might have yielded NaNs (resulting from division by zero interval medians) in this particular edge case, which actually takes effect before the rounding error I originally fixed. However, because of how HDF5 writes NaN values as 0, this apparently doesn't lead to any catastrophic failures. We should definitely check that behavior is reasonable in this case (i.e., when interval medians are zero); I've filed #6878. In the end, I added a regression test that only passes with the changes to address the rounding error. This was a bit of a pain because we use simulated data in the tests that cover this code, and the filters are applied in sequential order only on those elements that passed the previous filter. Note that there are many other possible filtering combinations that would be impractical to test. I think that all of this filtering logic was ported over from GATK CNV (I only rewrote the code to perform the filtering in-place to improve memory usage), and I'm not sure that all edge-case behavior was well defined by the original logic (which probably implicitly assumed typical, well formed data, i.e., using more than one sample, without too many uncovered intervals). Fortunately, these edge-case usages (i.e., using a single sample to build the PoN, mistakenly including too many uncovered intervals, and/or disabling various filters) are probably not too common.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6624#issuecomment-705843882:1081,perform,perform,1081,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6624#issuecomment-705843882,1,['perform'],['perform']
Performance,"Hmmm....as an alternate proposal, what if we implemented a custom serializer for `SimpleInterval` that does the contig name -> index and index -> contig name conversion transparently at serialization/deserialization time? Would that address the performance issue with shuffles and allow us all to share the same interval class?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5154#issuecomment-418509374:245,perform,performance,245,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5154#issuecomment-418509374,1,['perform'],['performance']
Performance,"Hmn, we could be seeing quota issues. We just dramatically increased the size of our build matrix and there's a ton of work going on today. It's hard to tell because there's no way to view the queue...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2733#issuecomment-305020745:193,queue,queue,193,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2733#issuecomment-305020745,1,['queue'],['queue']
Performance,How about I close the PR for now and open a ticket to investigate caching and its performance implications? I'll keep the branch so you can always compare etc.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235939855:82,perform,performance,82,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235939855,1,['perform'],['performance']
Performance,"How much does count collection cost at the desired bin size? How does this compare to bincov? Perhaps we could eliminate one of these steps if redundant. Note that the read counts are read once and stored in memory, so unless this takes a significant amount of time, then indexing is probably not the highest priority here (although I agree it would be nice to have in general). One related issue, as you mention, is file localization---since each shard only operates on a portion of the counts in each sample, it is a bit wasteful to localize the whole file. But how much does file localization cost? I can't imagine that it is the lowest hanging fruit. One of the more important issues, which you also mention, is optimizing parameters for inference. This includes not only the minimum number of epochs for training, but also things like the learning rate, annealing schedule, iterations per epoch, conditions for epoch convergence, etc. I'll be talking about how to tune these inference parameters---as well as other things in the pipeline---at the next BSV meeting. Let's brainstorm more things to try and prioritize them.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5288#issuecomment-427562932:716,optimiz,optimizing,716,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5288#issuecomment-427562932,2,"['optimiz', 'tune']","['optimizing', 'tune']"
Performance,"How to reproduce:. Modify the code in ```AlignmentIntervalUnitTest.testConstructionFromSAMRecord``` to perform a validation of the read returned by ```applyAlignment```:. ```; final SAMRecord samRecord = BwaMemAlignmentUtils.applyAlignment(""whatever"", SVDiscoveryTestDataProvider.makeDummySequence(expectedContigLength, (byte)'A'), null, null, bwaMemAlignment, refNames, hg19Header, false, false);; if (samRecord.isValid() != null) {; throw new IllegalStateException(samRecord.isValid().stream().map(s -> s.getMessage()).collect(Collectors.joining("", "")));; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3459#issuecomment-323218384:103,perform,perform,103,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3459#issuecomment-323218384,1,['perform'],['perform']
Performance,Huh... That's not good. I wonder if it's because we removed the jcenter repository resolver from the build. Maybe local caches are hiding the problem for us. . Could you try checking out the branch `lb_add_back_jcenter` and see if that fixes the problem?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7636#issuecomment-1012346184:120,cache,caches,120,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7636#issuecomment-1012346184,1,['cache'],['caches']
Performance,I added a few fairly minor comments. Curious to see the performance implications.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/928#issuecomment-143605225:56,perform,performance,56,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/928#issuecomment-143605225,1,['perform'],['performance']
Performance,"I added a small test demonstrating how to broadcast the reference sequence here: https://github.com/broadinstitute/hellbender/compare/master...tomwhite:hadoop-references#diff-957e843e027a45b3ac07d9a6c0515534R42. This will need to be run on a cluster to see how it performs, but I'd like to understand better how it will fit into the work in the dr_read_preprocessing_pipeline_skeleton branch. What is the integration point ther do you think @droazen?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/567#issuecomment-113185670:264,perform,performs,264,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/567#issuecomment-113185670,1,['perform'],['performs']
Performance,"I agree with @tedsharpe that `ctx.defaultParallelism()` is the recommended way to get the number of cores. If there's a race condition, then that is a bug that should be reported (stack trace if possible), so it can be fixed in Spark. Getting memory is harder. The `executorMemory()` method is probably the best bet (although it is not a public API any more), and for the local case could you use the JVM memory?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1947#issuecomment-233297988:120,race condition,race condition,120,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1947#issuecomment-233297988,1,['race condition'],['race condition']
Performance,"I also forgot to mention that this change needs an update for Hadoop-BAM due to a change in the way filesystems classes are loaded by Spark submit. @cmnbroad, would you be able to take a look at https://github.com/HadoopGenomics/Hadoop-BAM/pull/120, so I can merge it and do a new release?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2220#issuecomment-257583089:124,load,loaded,124,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2220#issuecomment-257583089,1,['load'],['loaded']
Performance,"I also just got this ConcurrentModificationException in HaplotypeCallerSparkIntegrationTest locally when running tests on one of my branches (this time in `testNonStrictVCFModeIsConsistentWithPastResults`, but the rest of the stack looks the same). I also vaguely recall seeing once before on travis on a branch where all I did was try to update miniconda to a newer version. My local branch doesn't have any dependency changes, so I suspect this is an existing issue that is just showing up intermittently.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6513#issuecomment-602804665:21,Concurren,ConcurrentModificationException,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6513#issuecomment-602804665,1,['Concurren'],['ConcurrentModificationException']
Performance,"I am also getting this error on chromosome 11:; ```; 11:49:44.992 INFO gcnvkernel.postprocess.viterbi_segmentation - Segmenting contig (5/12) (contig name: 9)...; 11:49:44.996 INFO gcnvkernel.postprocess.viterbi_segmentation - Segmenting contig (6/12) (contig name: 11)... Stderr: Traceback (most recent call last):; File ""/ngc/projects/gm/data/resources/envs/conda/ngs_gatk_cnv/4.1.6.0/lib/python3.6/site-packages/theano/compile/function_module.py"", line 903, in __call__; self.fn() if output_subset is None else\; File ""/ngc/projects/gm/data/resources/envs/conda/ngs_gatk_cnv/4.1.6.0/lib/python3.6/site-packages/theano/scan_module/scan_op.py"", line 963, in rval; r = p(n, [x[0] for x in i], o); File ""/ngc/projects/gm/data/resources/envs/conda/ngs_gatk_cnv/4.1.6.0/lib/python3.6/site-packages/theano/scan_module/scan_op.py"", line 952, in p; self, node); File ""scan_perform.pyx"", line 215, in theano.scan_module.scan_perform.perform; NotImplementedError: We didn't implemented yet the case where scan do 0 iteration. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/segment_gcnv_calls.6491270870870970325.py"", line 79, in <module>; viterbi_engine.write_copy_number_segments(); File ""/ngc/projects/gm/data/resources/envs/conda/ngs_gatk_cnv/4.1.6.0/lib/python3.6/site-packages/gcnvkernel/postprocess/viterbi_segmentation.py"", line 234, in write_copy_number_segments; for segment in self._viterbi_segments_generator():; File ""/ngc/projects/gm/data/resources/envs/conda/ngs_gatk_cnv/4.1.6.0/lib/python3.6/site-packages/gcnvkernel/postprocess/viterbi_segmentation.py"", line 160, in _viterbi_segments_generator; log_prior_c, log_trans_contig_tcc, copy_number_log_emission_contig_tc); File ""/ngc/projects/gm/data/resources/envs/conda/ngs_gatk_cnv/4.1.6.0/lib/python3.6/site-packages/gcnvkernel/models/theano_hmm.py"", line 88, in perform_forward_backward; prev_log_posterior_tc, admixing_rate, temperature))); File ""/ngc/projects/gm/data/res",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5852#issuecomment-613371282:926,perform,perform,926,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5852#issuecomment-613371282,1,['perform'],['perform']
Performance,"I am also seeing this warning 3x with 4.0.11.0 on a cluster but outside of docker (centos 6). . ```; 18:05:08.861 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg/gatk/4.0.11.0/install/bin/gatk-package-4.0.11.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Nov 24, 2018 6:05:09 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; WARNING: Failed to detect whether we are running on Google Compute Engine.; java.net.NoRouteToHostException: No route to host (Host unreachable); at java.net.PlainSocketImpl.socketConnect(Native Method); at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350); at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206); at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188); at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392); at java.net.Socket.connect(Socket.java:589); at sun.net.NetworkClient.doConnect(NetworkClient.java:175); at sun.net.www.http.HttpClient.openServer(HttpClient.java:463); at sun.net.www.http.HttpClient.openServer(HttpClient.java:558); at sun.net.www.http.HttpClient.<init>(HttpClient.java:242); at sun.net.www.http.HttpClient.New(HttpClient.java:339); at sun.net.www.http.HttpClient.New(HttpClient.java:357); at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1220); at sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1156); at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1050); at sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:984); at shaded.cloud_nio.com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:104); at shaded.cloud_nio.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:981); at shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials.runningOnComputeEngine(ComputeEngineCredential",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-441873417:141,Load,Loading,141,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-441873417,1,['Load'],['Loading']
Performance,"I am encountering a similar error: ; ```; Using GATK jar /nics/d/home/hchen3/bin/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /nics/d/home/hchen3/bin/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar GenotypeGVCFs -R /lustre/haven/proj/UTHSC0013/Tristan_GATK/reference/genome.fa -V gendb:///lustre/haven/proj/UTHSC0013/Tristan_GATK//DB/chr7 -G StandardAnnotation --use-new-qual-calculator -O /lustre/haven/proj/UTHSC0013/Tristan_GATK//gvcf//merged//joint_called_gvcfs_chr7.vcf; 23:15:47.053 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 23:15:47.249 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/nics/d/home/hchen3/bin/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 07, 2020 11:15:49 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 23:15:49.543 INFO GenotypeGVCFs - ------------------------------------------------------------; 23:15:49.545 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.1.2.0; 23:15:49.546 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 23:15:49.547 INFO GenotypeGVCFs - Executing as hchen3@acf-knl002 on Linux v3.10.0-514.26.1.el7.x86_64 amd64; 23:15:49.548 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_131-b12; 23:15:49.548 INFO GenotypeGVCFs - Start Date/Time: January 7, 2020 11:15:47 PM EST; 23:15:49.549 INFO GenotypeGVCFs - ------------------------------------------------------------; 23:15:49.549 INFO GenotypeGVCFs - ------------------------------------------------------------; 23:15:49.551 INFO GenotypeGVCFs - HTSJDK Version: 2.19.0; 23:15:",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6340#issuecomment-571886057:831,Load,Loading,831,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6340#issuecomment-571886057,1,['Load'],['Loading']
Performance,I am not sure if any performance gained by having Spark versions of these tools is worth the support cost. Can reopen if we feel otherwise.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4198#issuecomment-900613847:21,perform,performance,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4198#issuecomment-900613847,1,['perform'],['performance']
Performance,I ask that because for htsjdk defaults they must be system properties and they're final and set statically on load so mucking about resetting system properties after the JVM started already is going to be a bit of a fiddly ordering nightmare. . [Stack overflow](http://stackoverflow.com/questions/6736235/set-java-system-properties-with-a-configuration-file) doesn't seem to think that it's possible to initialize them from a file.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2316#issuecomment-267126704:110,load,load,110,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2316#issuecomment-267126704,1,['load'],['load']
Performance,"I brew installed openssl, and now there is a different unresolved dependency:. java.lang.UnsatisfiedLinkError: /private/var/folders/cr/16ghvyfj5lvfwxx01rt1k4tdl04sy3/T/libtiledbgenomicsdb6507285380909029818.dylib: dlopen(/private/var/folders/cr/16ghvyfj5lvfwxx01rt1k4tdl04sy3/T/libtiledbgenomicsdb6507285380909029818.dylib, 1): Library not loaded: **/usr/local/opt/libcsv/lib/libcsv.3.dylib**; Referenced from: /private/var/folders/cr/16ghvyfj5lvfwxx01rt1k4tdl04sy3/T/libtiledbgenomicsdb6507285380909029818.dylib; Reason: image not found",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-294232254:340,load,loaded,340,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-294232254,1,['load'],['loaded']
Performance,"I can actually replicate this with an interval list with two intervals: ; ```; chr1	11719	18516	+	.; chr2	38664	202755	+	.; ```; throws; ```; org.broadinstitute.hellbender.exceptions.GATKException: Cannot call query with different interval, expected:chr1:11719-18516 queried with: chr2:38664-202755; 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport$InitializedQueryWrapper.query(GenomicsDBImport.java:816); 	at com.intel.genomicsdb.importer.GenomicsDBImporter.<init>(GenomicsDBImporter.java:136); 	at com.intel.genomicsdb.importer.GenomicsDBImporter.lambda$null$2(GenomicsDBImporter.java:563); 	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ```. We typically run with `--reader-threads 5` in the pipeline, but if I change it to 1 I can get it to run. That's not a longterm solution, but hopefully it's a good hint and it's a good enough workaround for me for the moment.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5300#issuecomment-438818966:627,concurren,concurrent,627,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5300#issuecomment-438818966,3,['concurren'],['concurrent']
Performance,"I can confirm that in my own extensive runs of the tools I've seen it sometimes get hung when out of memory instead of exiting (resulting in bad data for that run). My own benchmarking was with PrintReads on a large file, using various cache buffer sizes. I remember seeing an increase in performance up to about 50MB buffer size and then it flattened out. I would expect a more CPU-intensive (or a more heavily loaded machine) would reduce the impact of the buffer size as I/O ceases to be the bottleneck. I also ran experiments on a 1-cpu machine with VCF and loading a single interval, which I think matches what you are asking about. In that experiment 10MB was enough, adding to the cache did not bring any improvement and of course too large a cache leads to running out of memory.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2640#issuecomment-298972380:236,cache,cache,236,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2640#issuecomment-298972380,7,"['bottleneck', 'cache', 'load', 'perform']","['bottleneck', 'cache', 'loaded', 'loading', 'performance']"
Performance,"I can confirm that, GATK 4.1.3.0 with CollectRawWgsMetrics; ```; Exception in thread ""main"" java.lang.IllegalArgumentException: The requested position is not covered by this StartEdgingRecordAndOffset object.; ```. **Cmdline:**; ```; picard -Xms4000m \; CollectRawWgsMetrics \; INPUT=example.bam; VALIDATION_STRINGENCY=SILENT \; REFERENCE_SEQUENCE=Homo_sapiens_assembly38.fasta \; INCLUDE_BQ_HISTOGRAM=true \; INTERVALS=wgs_coverage_regions.hg38.interval_list \; OUTPUT=example.raw_wgs_metrics \; USE_FAST_ALGORITHM=true \; READ_LENGTH=250; ```. **Output:**; ```; 18:48:46.330 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/usr/local/share/picard-2.20.7-0/picard.jar!/com/intel/gkl/native/libgkl_compression.so; [Fri Sep 20 18:48:46 GMT 2019] CollectRawWgsMetrics INPUT=example.bam; [Fri Sep 20 18:48:46 GMT 2019] Executing as root@98e13b9f4ef1 on Linux 4.19.44+ amd64; OpenJDK 64-Bit Server VM 1.8.0_152-release-1056-b12; Deflater: Intel; Inflater: Intel; Provider GCS is not available; Picard version: 2.20.7-SNAPSHOT; [Fri Sep 20 18:49:00 GMT 2019] picard.analysis.CollectRawWgsMetrics done. Elapsed time: 0.24 minutes.; Runtime.totalMemory()=4054515712; To get help, see http://broadinstitute.github.io/picard/index.html#GettingHelp; Exception in thread ""main"" java.lang.IllegalArgumentException: The requested position is not covered by this StartEdgingRecordAndOffset object.; at htsjdk.samtools.util.AbstractRecordAndOffset.validateOffset(AbstractRecordAndOffset.java:146); at htsjdk.samtools.util.EdgingRecordAndOffset$StartEdgingRecordAndOffset.getBaseQuality(EdgingRecordAndOffset.java:112); at picard.analysis.FastWgsMetricsCollector.excludeByQuality(FastWgsMetricsCollector.java:189); at picard.analysis.FastWgsMetricsCollector.processRecord(FastWgsMetricsCollector.java:144); at picard.analysis.FastWgsMetricsCollector.addInfo(FastWgsMetricsCollector.java:105); at picard.analysis.WgsMetricsProcessorImpl.processFile(WgsMetricsProcessorImpl.java:93); at picard.an",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6163#issuecomment-533770047:604,Load,Loading,604,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6163#issuecomment-533770047,1,['Load'],['Loading']
Performance,"I can reproduce this on a cluster running CDH 5.7.0. It's the same as this report:. https://community.cloudera.com/t5/Advanced-Analytics-Apache-Spark/Override-libraries-for-spark/m-p/32146. Despite the conversation being marked as ""solved"" I'm not sure what the resolution was. This error seems to occur if snappy is loaded multiple times:. http://mail-archives.apache.org/mod_mbox/incubator-kafka-users/201208.mbox/%3CCA+sHyy98F8JoQf3wDKKNCy8jcoPiddTBt4_PXr8J1zeN2Jq6AA@mail.gmail.com%3E. So its possible that the snappy jar (or jars) is on the classpath multiple times. It should be provided by CDH, and it's not in the GATK spark JAR - so I'm not sure where it's being duplicated.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1873#issuecomment-229098679:317,load,loaded,317,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1873#issuecomment-229098679,1,['load'],['loaded']
Performance,"I can't reproduce this yet. I tried downloading the jar, unzipping it, and running the example command you gave, but I can't reproduce what you're seeing. I modified it for my local files:; ```; java -jar gatk-package-4.2.5.0-local.jar \; GenotypeGVCFs \; -R /Users/louisb/Workspace/gatk/src/test/resources/large/Homo_sapiens_assembly19.fasta.gz \; --variant gendb:///Users/louisb/Workspace/gatk/output \; -O out.vcf \; --annotate-with-num-discovered-alleles \; -stand-call-conf 30 \; --max-alternate-alleles 6 \; --force-output-intervals 20 \; -L 20 \; --only-output-calls-starting-in-intervals \; --genomicsdb-shared-posixfs-optimizations; ```; It runs to completion on my machine. ; My md5sum matches yours so that's not the problem. It's not clear to me what's going on here. Are the previous releases working on your cluster still?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7675#issuecomment-1042010522:627,optimiz,optimizations,627,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7675#issuecomment-1042010522,1,['optimiz'],['optimizations']
Performance,"I can, this only happens on 10 of our 2000 samples (only in WES) none of our 600 WGS seems to have the same issue. It is always on some small contig (you can see here range is 544, but all cases are small ranges like this one). Everything is the default mutect2 pipeline and params (e.g. [gs://genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.fasta](https://console.cloud.google.com/storage/browser/genomics-public-data/resources/broad/hg38/v0?prefix=Homo_sapiens_assembly38.fasta&authuser=jkalfon%40broadinstitute.org)) : except the interval file: [gs://ccleparams/region_file_wgs.list](https://console.cloud.google.com/storage/browser/ccleparams?prefix=region_file_wgs.list&authuser=jkalfon%40broadinstitute.org); GATK 4.2.6.1. . Here is the VCF file to annotate `gs://ccleparams/test/CDS-2jucw0.hg38-filtered.vcf.gz`. Here is the stacktrace:. ```; ....; 10:53:39.044 INFO VcfFuncotationFactory - ClinVar_VCF 20180429_hg38 cache hits/total: 0/2145; 10:53:39.249 INFO VcfFuncotationFactory - dbSNP 9606_b151 cache hits/total: 0/1069225; 10:53:39.520 INFO Funcotator - Shutting down engine; [July 12, 2022 10:53:39 AM GMT] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 115.46 minutes.; Runtime.totalMemory()=2050490368; java.lang.StringIndexOutOfBoundsException: String index out of range: 544; at java.lang.String.substring(String.java:1963); at org.broadinstitute.hellbender.tools.funcotator.ProteinChangeInfo.initializeForInsertion(ProteinChangeInfo.java:293); at org.broadinstitute.hellbender.tools.funcotator.ProteinChangeInfo.<init>(ProteinChangeInfo.java:101); at org.broadinstitute.hellbender.tools.funcotator.ProteinChangeInfo.create(ProteinChangeInfo.java:399); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createSequenceComparison(GencodeFuncotationFactory.java:2054); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createCodingRegionFunc",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6651#issuecomment-1182102653:945,cache,cache,945,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6651#issuecomment-1182102653,1,['cache'],['cache']
Performance,"I checked in the spark JAR, and `META-INF/services/java.nio.file.spi.FileSystemProvider` contains . ```; com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider; hdfs.jsr203.HadoopFileSystemProvider; ```. So it looks like the service loader file is being created correctly. I tried to reproduce on GCS by running (essentially the same as @vdauwera's command.). ```; time ./gatk-launch ApplyBQSRSpark \; -I gs://hellbender/test/resources/benchmark/CEUTrio.HiSeq.WEx.b37.NA12892.bam \; -R gs://gatk-legacy-bundles/b37/human_g1k_v37.2bit \; -O gs://gatk-demo-tom/TEST/gatk4-spark/recalibrated.bam \; -bqsr gs://gatk-demo/TEST/gatk4-spark/recalibration.table \; -apiKey $GOOGLE_APPLICATION_CREDENTIALS \; -- \; --sparkRunner GCS \; --cluster cluster-tom \; --num-executors 40 \; --executor-cores 4 \; --executor-memory 10g; ```. But I got another error earlier on. Any ideas what this could be? (I can see the input bam with `gsutil cp`). ```; org.broadinstitute.hellbender.exceptions.UserException: A USER ERROR has occurred: Failed to read bam header from gs://hellbender/test/resources/benchmark/CEUTrio.HiSeq.WEx.b37.NA12892.bam; Caused by:Error reading null at position 0; 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:182); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReads(GATKSparkTool.java:376); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:357); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:347); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.insta",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2287#issuecomment-264909676:246,load,loader,246,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2287#issuecomment-264909676,1,['load'],['loader']
Performance,I cleaned up the mutect2 wdl and added multi-sample support. I also optimized resource usage and exposed the memory parameters: https://github.com/phylyc/gatk4-somatic-snvs-indels,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7532#issuecomment-1125321665:68,optimiz,optimized,68,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7532#issuecomment-1125321665,1,['optimiz'],['optimized']
Performance,"I copied 60× BAM file to an interactive Linux server with 768 GB physical RAM and eighty cores and used version 4.5.0.0. ```; %Cpu(s): 1.3 us, 0.0 sy, 0.1 ni, 98.7 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st; GiB Mem : 754.5 total, 52.1 free, 107.3 used, 600.3 buff/cache ; GiB Swap: 931.3 total, 924.9 free, 6.4 used. 647.3 avail Mem . PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND ; 171365 dario 20 0 35.0g 31.3g 23040 S 100.0 4.1 32:18.12 java ; ```. I removed `-Xmx` and using `top` to see the process is consistently at about 32 GB. So, `-Xmx` is irrelevant to the problem. ```; 12:15:04.531 INFO ProgressMeter - Current Locus Elapsed Minutes Loci Processed Loci/Minute; 12:57:32.208 INFO GetPileupSummaries - Shutting down engine; [January 13, 2024 at 12:57:32 PM AEDT] org.broadinstitute.hellbender.tools.walkers.contamination.GetPileupSummaries done. Elapsed time: 50.36 minutes.; Runtime.totalMemory()=20753416192; java.lang.OutOfMemoryError: Java heap space: failed reallocation of scalar replaced objects; ```. What does ""reallocation of scalar replaced objects"" mean? I don't think it could possibly have run out of memory.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8654#issuecomment-1890249060:257,cache,cache,257,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8654#issuecomment-1890249060,1,['cache'],['cache']
Performance,"I created a panel of normals from 90 WGS TCGA samples with 250bp (~11.5M) bins, which took **~57 minutes** total and produced an **11GB PoN** (this file includes all of the input read counts---which take up 20GB as a combined TSV file and a whopping 63GB as individual TSV files---as well as the eigenvectors, filtering results, etc.). The run can be found in /dsde/working/slee/wgs-pon-test/tieout/no-gc. It completed successfully with **-Xmx32G** (in comparison, CreatePanelOfNormals crashed after 40 minutes with -Xmx128G). The runtime breakdown was as follows:. - ~45 minutes simply from reading of the 90 TSV read-count files in serial. Hopefully #3349 should greatly speed this up. (In comparison, CombineReadCounts reading 10 files in parallel at a time took ~100 minutes to create the aforementioned 20GB combined TSV file, creating 25+GB of temp files along the way.). - ~5 minutes from the preprocessing and filtering steps. We could probably further optimize some of this code in terms of speed and heap usage. (I had to throw in a call to System.gc() to avoid an OOM with -Xmx32G, which I encountered in my first attempt at the run...). - ~5 minutes from performing the SVD of the post-filtering 8643028 x 86 matrix, maintaining 30 eigensamples. I could write a quick implementation of randomized SVD, which I think could bring this down a bit (the scikit-learn implementation takes <2 minutes on a 10M x 100 matrix), but this can probably wait. Clearly making I/O faster and more space efficient is the highest priority. Luckily it's also low hanging fruit. The 8643028 x 30 matrix of eigenvectors takes <2 minutes to read from HDF5 when the WGS PoN is used in DenoiseReadCounts, which gives us a rough idea of how long it should take to read in the original ~11.5M x 90 counts from HDF5. So once #3349 is in, then I think that a **~15 minute single-core WGS PoN could easily be viable**. I believe that a PoN on the order of this size will be all that is required for WGS denoising, if i",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-317614503:961,optimiz,optimize,961,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-317614503,1,['optimiz'],['optimize']
Performance,I definitely say kill the pretty streaming code if it's harming performance in these critical sections -- no shame in a straightforward loop!,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2241#issuecomment-257582318:64,perform,performance,64,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2241#issuecomment-257582318,1,['perform'],['performance']
Performance,"I did a couple of quick tests, with GATK and htsjdk, including tight loops that just reset the header 10M times, and I see virtually no performance difference between strict and not. i.e, iterating through a bam with 500k records and just calling set vs strict.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1541#issuecomment-191898375:136,perform,performance,136,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1541#issuecomment-191898375,1,['perform'],['performance']
Performance,"I did a quick scalability ministudy - this seems to scale well up to 12 cores and then diminishes due to Amdahl's law I think that's fine. There is no diminished runtime due to OMP overhead when on 1 core. Note that our cluster on which I wan these was not empty (a few of the 48 cores were in use) and do this is just a ballpark estimate of scalability, in particular 24 was worse than 12 probably due to interference. Based on this I think OMP is a good idea and it's going to work on 1 CPU too. limited to 1 OMP thread, using 10GB of RAM. ```; real 2m15.621s; user 3m17.269s; Total compute time in PairHMM computeLogLikelihoods() : 50.964700625000006; ```. ---. limited to 1 OMP thread, using 32GB of RAM . ```; real 1m46.597s; user 3m17.363s; Total compute time in PairHMM computeLogLikelihoods() : 45.797104454; ```. limited to 2 OMP threads, using 32GB of RAM. ```; real 1m26.310s; user 3m24.636s; Total compute time in PairHMM computeLogLikelihoods() : 23.790980359000002; ```. limited to 4 OMP threads, using 32GB of RAM. ```; real 1m15.298s; user 3m29.834s; Total compute time in PairHMM computeLogLikelihoods() : 11.332445694; ```. limited to 6 OMP threads, using 32GB of RAM. ```; real 1m14.015s; user 3m20.876s; Total compute time in PairHMM computeLogLikelihoods() : 7.862075811; ```. limited to 12 OMP threads, using 32GB of RAM. ```; real 1m6.370s ; user 3m42.340s; Total compute time in PairHMM computeLogLikelihoods() : 4.585800097; ```. limited to 24 OMP threads, using 32GB of RAM (clearly, OMP hits the limit here). ```; real 1m8.779s; user 4m15.489s; Total compute time in PairHMM computeLogLikelihoods() : 3.047581173; ```. limited to 48 OMP threads, using 32GB of RAM (worse than 12 threads). ```; real 1m11.535s; user 6m26.100s; Total compute time in PairHMM computeLogLikelihoods() : 4.112299148; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1800#issuecomment-218810496:14,scalab,scalability,14,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1800#issuecomment-218810496,2,['scalab'],['scalability']
Performance,"I did, but then I rethought it and decided that I think it's too fragile to do it that way: You have to guarantee that all classes that might want to be registered are loaded and initialized before you instantiate the SparkConf. The problem is that it varies among JVM implementations exactly when that (class loading and initialization) happens. Some JVMs do the whole mess, chasing all references down from main recursively at the beginning, others are as lazy as possible and initialize only when actually traversing a reference for the first time. We could use that technique to ""inject"" a set of registrations into the GATKRegistrator from each main class. But since it's unreliable to do it in arbitrary classes, it seemed more straightforward to just let the normal object oriented method of overrides handle the problem, since it will need to be done only by direct subclasses, anyway. But I'm certainly open to other solutions, so long as we provide the functionality.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1767#issuecomment-214473964:168,load,loaded,168,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1767#issuecomment-214473964,2,['load'],"['loaded', 'loading']"
Performance,"I disagree, I think any common operation that you would want to perform on a `Locatable` should be lifted up to it. This was bad in java n < 8, because every implementing class would have to provide it's own implementation. Now that we have default methods I think the preference should be to put utility methods directly in the interface.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/305#issuecomment-79205767:64,perform,perform,64,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/305#issuecomment-79205767,1,['perform'],['perform']
Performance,"I discovered that it actually is possible to load unmapped reads with the existing google code code. I had thought it didn't include the ability to do so, but after further digging, there's a way to do it that we just haven't exposed.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/560#issuecomment-114228839:45,load,load,45,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/560#issuecomment-114228839,1,['load'],['load']
Performance,"I don't think that hiding/disable arguments would work in every case: sometimes, an argument shouldn't be exposed but still available to set programmatically, or maybe just reduce visibility making it `@Hidden` and/or `@Advance`. What is the problem of making an interface for the top-level argument to the GATK? Changing the interface or the `CommadnLineProgram` has the same effect, but the API user can still behave the same as before. It is much more extensible and downstream-friendly. What's about making the `CLPConfigurationArgumentCollection` an interface always returning defaults to be able to change it in a proper way? The cycle of development of a new argument will be: 1) add a new method to the interface with a default returning what will be expected from the previous behaviour, 2) add and return by the argument in the GATK implementation, 3) use the getter in the CLP for perform the operation. This only adds the first point, and operating in 3 classes instead of 3. For API user it is really easy to maintain the previous behavior when upgrading the dependency by just using their own implementation of the class, or include the top-level new arguments by using the GATK implementation. It is much more flexible and extensible (I always think about GATK also as a library). In addition, I think that this approach is also important for evolving GATK. For example, if a new top-level argument is tagged as experimental (still not supported but requested in Barclay), removing it would allow to keep the interface (no version bump) the same and final users can still operate with the experimental argument. The same applies to the `GATKTool` base class (https://github.com/broadinstitute/gatk/issues/4341), and for downstream projects the aim should be to be able to extend safely the `CommandLineProgram` directly to implement their own toolkit using the powerful GATK framework.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3998#issuecomment-366185003:892,perform,perform,892,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3998#issuecomment-366185003,1,['perform'],['perform']
Performance,"I don't understand why you say ""Using a singleton should not be needed, since mocks are sufficient"". The existing code uses both a singleton and mocks. There are two issues at play here. (1) We want to have a way to make sure everyone gets the mock, without having to pass it as argument deep into the call hierarchy, and; (2) We want to cache the results of initializing the RefAPISource because it takes a long time. Let's start with point 1. When `BaseRecalibratorDataflow` calls `RefAPISource.getInstance()` or equivalent, how is `BaseRecalibratorDataflowIntegrationTest` going to ensure that `BaseRecalibratorDataflow` gets the mock instead of the internet-fetching version? ; (and by ""or equivalent"" I mean `new ReferenceDataflowSource(...)`). ; We still need a way to stash the mock somewhere for the user code to retrieve. Right now we do this by setting the ""singleton"" value. ReferenceDataflowSource is a wrapper instead, so we can add a method to stash the mock on a static member - but that isn't there right now so I'm surprised you seem to think this PR completely satisfies the uses cases that the singleton does.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/804#issuecomment-130831140:338,cache,cache,338,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/804#issuecomment-130831140,1,['cache'],['cache']
Performance,"I feel like this is going to be problematic in a different way than what @magicDGS is mentioning. We expect many versions of gatk to be compatible with the same python environment. Also for performance reasons we want to start avoid rebuilding the conda environment on every push and bake it into the base docker instead. This change means we definitely have to build it every time. . It feels like we need something more sophisticated. Instead of stamping the conda environment with the gatk version that matches it, maybe we should be stamping the gatk jar and the conda environment with some version based on the conda.env? Maybe we can do something like taking the md5 of the conda.yml and pushing that into both the jar manifest and the conda environment in some way? I'm guessing this scheme has an issue with the actual python code in the gatk since I think that's installed with conda as well? I'd really like to be able to preinstall the various dependencies though and then only update the code that's part of the gatk.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5081#issuecomment-411214721:190,perform,performance,190,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5081#issuecomment-411214721,1,['perform'],['performance']
Performance,"I found this online., this might be the reason why use include spark is different from system spark in this case. In spark, when i try to cast the variable to the same 'class' (with exact the same class name, in this case, you could try to cast any ""Variant"" to ""Variant"" in BroadcastJoinReadsWithVariants ), it always throw out the class exception error. It is because that ""The equality of two classes in Java depends on the fully qualified name and the class loader that loaded it.""; In system spark, it might split the JVM, which means the broadcast variable may cause problem to identify if it is the same class?. This is what i test in spark:; List<Variant> vs = variants.take(10);; MinimalVariant v = (MinimalVariant) vs.get(0);. this will throw ClassCastException error... http://stackoverflow.com/questions/826319/classcastexception-when-casting-to-the-same-class",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1386#issuecomment-166160181:462,load,loader,462,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1386#issuecomment-166160181,2,['load'],"['loaded', 'loader']"
Performance,"I guess I could, but what is GATKTool contract... is it supposed to only perform reference driven analysis? I thought that is what a Walker is.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6390#issuecomment-577247790:73,perform,perform,73,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6390#issuecomment-577247790,1,['perform'],['perform']
Performance,I have also stumbled over this. I am adding a detailed error log.; I think that the incompatibility of accelerated PairHMM with a tmp directory mounted noexec should be mentioned in ; the installation requirements. I found it well-documented in [the troubleshooting section](https://gatk.broadinstitute.org/hc/en-us/articles/18965297287067-How-to-setup-and-use-temporary-folder-for-GATK-local-execution). But everyone with this setup will experience falling back to the slow implementation for no other reason. . ```; INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/; miniconda2/envs/polyploidPhasing/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; WARN NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils9418239050694741169.so: /tmp/libgkl_utils9; 418239050694741169.so: failed to map segment from shared object: Operation not permitted); WARN IntelPairHmm - Intel GKL Utils not loaded; PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8453#issuecomment-1905717389:545,Load,Loading,545,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8453#issuecomment-1905717389,4,"['Load', 'load', 'multi-thread']","['Loading', 'load', 'loaded', 'multi-threaded']"
Performance,"I have found that 'hadoop distcp' scales linearly with the size of file. Its runtime seems not to change as I scale the number of machines from 10 to 100. Furthermore, I have found that taken in concert with performance findings from #1675 that for large file sizes it is actually more efficient to first load a bam file into HDFS first using 'hadoop distcp' in order to run the spark BQSR. . Breakdown:; - 150GB bam file takes 25:30 minutes to download into HDFS; - BQSRSpark takes 47:25 minutes to run on a 150GB bam file in HDFS; - Total runtime = 72:55 minutes ; - BQSRSpark run from GCS bucket = 77:15 minutes . A small improvement in runtime but it is worth keeping in mind for #2015 going forward when evaluating which approach is best to take on spark.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2014#issuecomment-234267283:208,perform,performance,208,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2014#issuecomment-234267283,2,"['load', 'perform']","['load', 'performance']"
Performance,"I have more examples of this now (90 and counting, ~1% of jobs) which seems to match with the above numbers:. `htsjdk.tribble.TribbleException$MalformedFeatureFile: Unable to parse header with error: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: Connection has been shutdown: javax.net.ssl.SSLException: java.net.SocketException: Connection reset, for input source: gs://fc-4c1c7765-2de2-4214-ac41-dc10bbcbb55b/1e300bb3-6990-4342-8959-118826efb3dd/PairedEndSingleSampleWorkflow/3b32519a-f910-49a6-a5fc-b7ec9700d281/call-GatherVCFs/S153-2.g.vcf.gz; 	at htsjdk.tribble.TabixFeatureReader.readHeader(TabixFeatureReader.java:102); 	at htsjdk.tribble.TabixFeatureReader.<init>(TabixFeatureReader.java:86); 	at htsjdk.tribble.AbstractFeatureReader.getFeatureReader(AbstractFeatureReader.java:106); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getReaderFromVCFUri(GenomicsDBImport.java:437); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.loadHeaderFromVCFUri(GenomicsDBImport.java:252); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.initializeHeaderAndSampleMappings(GenomicsDBImport.java:223); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.onStartup(GenomicsDBImport.java:202); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:114); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:220); Caused by: java.lang.RuntimeException: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: Connection has been shutdown: javax.net.ssl.SSLException:",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-301610931:210,concurren,concurrent,210,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-301610931,1,['concurren'],['concurrent']
Performance,"I have not had time to do any profiling, but I have looked at a lot of commits. I think it's likely that my recent changes cause some haplotypes with leading indels to be kept when previously they may have been dropped. It's hard to believe that this could cause a 10-20% slowdown via a commensurate increase in the number of haplotypes assembled. However, haplotypes with leading indels would have a disproportionate pair HMM cost since they would spoil caching of the read-haplotype pair HMM matrix at the very beginning of the matrix. That is, in addition to being particularly expensive haplotypes because they would diverge from the previous haplotype at the first position and therefore not benefit from caching at all, they would also completely destroy whatever caching the previous haplotype would have gotten. We ought to think about haplotypes that start or end with indels. It seems to me that they are bad news and very likely artifacts of assembly windows and/or reads that end in the middle of an STR. I would worry about discarding them outright, because what if all the real variation is attached to haplotypes like this. Therefore, I think the best thing to do is to choose assembly windows more carefully and increase or decrease padding to avoid ending in an STR. Avoiding assembly windows that end in STRs is a wise thing to do regardless, so how about I make a branch for that and we can see if the performance regression goes away?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6567#issuecomment-623054595:1421,perform,performance,1421,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6567#issuecomment-623054595,1,['perform'],['performance']
Performance,"I haven't been able to reproduce @vdauwera error, but there are issues with the https checkout at the moment. ; There's one annoying issue witwhere it will prompt for a password before every individual file download. This will be fixed in https://github.com/github/git-lfs/issues/755. It can be worked around by using `git config credential.helper cache` but the easiest thing to do at the moment is just using the ssh checkout. . I need to investigate what happens with ssh checkout if you don't have a key set up.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/952#issuecomment-150376513:348,cache,cache,348,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/952#issuecomment-150376513,1,['cache'],['cache']
Performance,"I haven't understood how multi-allele model exactly works in the old GATK, so can't comment on why it does not perform well. In general, I am supportive of making the new model the default going forward. However:. > when we remove the other models. I would suggest retaining the old model if possible. As I said on the method meeting, the old model takes the full power of population information (by full, I mean under the Wright-Fisher and HWE assumptions, you can't derive a more powerful model in theory). My understanding is that David's current model isn't. This is fine as long as the information from sequence data overwhelms the population information, which is usually true for highCov data. However, when data is thin, the population information will play a more important role. Without thorough evaluations in multiple scenarios, it is not clear when the loss of population information in the new model starts to matter. It would be good to keep the old model as a reference point, at least for biallelic SNPs, until we have more comparison.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2098#issuecomment-242810127:111,perform,perform,111,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2098#issuecomment-242810127,1,['perform'],['perform']
Performance,"I just got the same error:. ```; Using GATK jar /software/anaconda2/share/gatk4-4.0.12.0-0/gatk-package-4.0.12.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx16G -jar /software/anaconda2/share/gatk4-4.0.12.0-0/gatk-package-4.0.12.0-local.jar FilterMutectCalls -V tumor-vs-normal.mutect.temp1.vcf -O tumor-vs-normal.mutect.temp2.vcf; 22:58:25.052 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/software/anaconda2/share/gatk4-4.0.12.0-0/gatk-package-4.0.12.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 22:58:26.911 INFO FilterMutectCalls - ------------------------------------------------------------; 22:58:26.912 INFO FilterMutectCalls - The Genome Analysis Toolkit (GATK) v4.0.12.0; 22:58:26.912 INFO FilterMutectCalls - For support and documentation go to https://software.broadinstitute.org/gatk/; 22:58:26.912 INFO FilterMutectCalls - Executing as www-data@SpongeBob on Linux v4.15.0-39-generic amd64; 22:58:26.912 INFO FilterMutectCalls - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_152-release-1056-b12; 22:58:26.912 INFO FilterMutectCalls - Start Date/Time: January 6, 2019 10:58:24 PM SGT; 22:58:26.913 INFO FilterMutectCalls - ------------------------------------------------------------; 22:58:26.913 INFO FilterMutectCalls - ------------------------------------------------------------; 22:58:26.913 INFO FilterMutectCalls - HTSJDK Version: 2.18.1; 22:58:26.913 INFO FilterMutectCalls - Picard Version: 2.18.16; 22:58:26.913 INFO FilterMutectCalls - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 22:58:26.913 INFO FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 22:58:26.913 INFO FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 22:58:26.913 INFO FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 22:58:26.914 INFO FilterMutectCalls - De",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5553#issuecomment-451749085:513,Load,Loading,513,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5553#issuecomment-451749085,1,['Load'],['Loading']
Performance,"I know we've had to tune this parameter to get things working, and different sizes are better for different tools. If we increase it by much we start thrashing memory and spilling to disk, but the partition size range depends on the tool. MD works more slowly if you increase this by several times, others you can increase it an order of magnitude and it's ok.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1166#issuecomment-158528360:20,tune,tune,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1166#issuecomment-158528360,1,['tune'],['tune']
Performance,I like it and want to put it in but we can't unless the code compiles and runs on the Mac. I'd be fine with having Mac use just 1 thread if need be but the code must compile and run. With some hacking @lbergelson was able to compile and run it but it's not something we want users to do. The official XCode on the Mac does not include OpenMP and having our users go through a complex install of an alternative clang is a no-starter for us. Also non-starter is removal of native PairHMM from the Mac. . So I think we have 2 options:; a) change the code+build to disable OMP on the Mac (ie have Mac be single threaded). The benefit is that we can start running with improved performance right away on linux. That includes production (when we're ready with the HaplotypeCaller); b) wait until the new intel gatk native library is done and then move the problem of compiling etc to that repo. That would remove the burden of building this from us and shift to the other repo's owners who can then build their library however they want and we'd only pick up compiled versions. @gspowley @lbergelson @droazen votes?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1813#issuecomment-218849928:673,perform,performance,673,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1813#issuecomment-218849928,1,['perform'],['performance']
Performance,I loaded the docker repo GATK v4.1.4.0 and had the same (or similar) error result. ```; 2019-10-30T13:35:51.791637449Z java.lang.IllegalArgumentException: log10 p: Values must be non-infinite and non-NAN; 2019-10-30T13:35:51.792001654Z 	at org.broadinstitute.hellbender.utils.NaturalLogUtils.logSumExp(NaturalLogUtils.java:84); 2019-10-30T13:35:51.792175325Z 	at org.broadinstitute.hellbender.utils.NaturalLogUtils.normalizeLog(NaturalLogUtils.java:51); 2019-10-30T13:35:51.792358868Z 	at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.clusterProbabilities(SomaticClusteringModel.java:203); 2019-10-30T13:35:51.792559803Z 	at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.probabilityOfSequencingError(SomaticClusteringModel.java:96); 2019-10-30T13:35:51.792736667Z 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.TumorEvidenceFilter.calculateErrorProbability(TumorEvidenceFilter.java:27); 2019-10-30T13:35:51.792905235Z 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2VariantFilter.errorProbability(Mutect2VariantFilter.java:15); 2019-10-30T13:35:51.793072365Z 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.lambda$new$1(ErrorProbabilities.java:19); 2019-10-30T13:35:51.793261944Z 	at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1321); 2019-10-30T13:35:51.793456807Z 	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); 2019-10-30T13:35:51.793619935Z 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 2019-10-30T13:35:51.793810301Z 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482); 2019-10-30T13:35:51.794006885Z 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472); 2019-10-30T13:35:51.794191116Z 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 2019-10-30T13:35:51.794367593Z 	at java.util.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5821#issuecomment-547909227:2,load,loaded,2,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5821#issuecomment-547909227,1,['load'],['loaded']
Performance,"I may have a lead. The error occurs here (no insight so far, just looking up the line from the stack trace):; ```; final Object2IntMap<EVIDENCE> evidenceIndexes = evidenceIndexBySampleIndex(sampleIndex);; final int[] indexesToRemove = evidences.stream().mapToInt(e -> {; final int index = evidenceIndexes.getInt(e);; if (index == MISSING_INDEX) {; throw new IllegalArgumentException(""evidence provided is not in sample"");; }; ```; We get an error when `evidenceIndexBySampleIndex(sampleIndex)` yields a `Map` that for some reason doesn't contain a read that it should. So let's investigate `evidenceIndexBySampleIndex()`. This method returns the `evidenceIndexBySampleIndex.get(sampleIndex)` field if it is not `null` (ie uninitialized); otherwise it fills it and then returns it. The code for filling it seems fine, and it explicitly loops over every sample read, so it's hard to see that the error could come from there. It seems rather that the problem is in returning the cached value whenever it is not `null`. The cached value of `evidenceIndexBySampleIndex.get(sampleIndex)` becomes invalid whenever reads are added or removed. However, you can check all the accesses of `evidenceIndexBySampleIndex` (there are only six) and verify that the class never accounts for this. So, suppose that an `AlleleLikelihoods` object invokes `evidenceIndexBySampleIndex(sampleIndex)` more than once and adds or removes reads between these. The second call returns the cached map from the first call, which is bogus. Even if it doesn't explain this issue, it is a bug. Now let's think about which public methods `evidenceIndexBySampleIndex(sampleIndex)` is called in and where this occurs in HaplotypeCaller:. * `addEvidence` (in HC this happens only in the likelihoods for annotations, downstream of our issue, so this is not the culprit).; * `filterPoorlyModeledEvidence` (this happens after Pair-HMM to the haplotype likelihoods, so not the culprit either); * `contaminationDownsampling`; * `retainEvidence`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6586#issuecomment-625021336:976,cache,cached,976,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6586#issuecomment-625021336,1,['cache'],['cached']
Performance,"I might be missing something, but this looks like it iterates over assembly regions, rather than loading them all at once. `Utils.stream` converts an iterator to a stream, but even that doesn't load them all into memory as it basically creates a one-shot `Iterable` that just returns the passed in `Iterator` (this is what `() -> iterator` does).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4301#issuecomment-368830180:97,load,loading,97,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4301#issuecomment-368830180,2,['load'],"['load', 'loading']"
Performance,"I ran BWA-MEM on 1m read pairs. Here's a comparison for pure BWA-MEM versus the Spark version, using two threads in each case. (The Spark version had a single executor which was allocated two cores via the `--executor-cores` argument.) . ```; $BWA mem -K 100000 -v 3 -t 2 human_g1k_v37.fasta ERR000589_1.1m.fastq ERR000589_2.1m.fastq > ERR242035.1m.sam 2> foo.err; # Real time: 666.345 sec; CPU: 1267.746 sec. ./gatk-launch BwaSpark \; --threads 2 \; --ref hdfs://bottou01.sjc.cloudera.com/user/$USER/bwa/human_g1k_v37.fasta \; --input hdfs:///user/$USER/bwa/ERR000589.1m.bam \; --output hdfs:///user/$USER/bwa/ERR000589-aligned.1m.bam \; -- \; --sparkRunner SPARK \; --sparkMaster yarn-client \; --driver-memory 3G \; --num-executors 1 \; --executor-cores 2 \; --executor-memory 15G \; --archives jbwa-native.tar#jbwa-native \; --conf 'spark.executor.extraLibraryPath=jbwa-native'; BwaSpark - Total time to run tool: 932s; BwaSpark - Time to download reference: 32s ; BwaSpark - Time in bwa: 1575s; ```. So BwaSpark was 40% slower overall in terms of elapsed time. Of course, this was only using a single executor, so if we used multiple executors the elapsed time could be made proportionately shorter (and this a valid use case for some). In terms of CPU time in the BWA algorithm itself, it was about 24% slower (1575/1268). The latter could be improved by optimizing the JNI path (e.g. by passing several arrays of primitives or Strings corresponding to the fields in a `ShortRead`, rather than a single array of `ShortRead`s and then translating it). There are probably other improvements to make too, but I think the summary is that this approach is feasible.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1750#issuecomment-219080628:1361,optimiz,optimizing,1361,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1750#issuecomment-219080628,1,['optimiz'],['optimizing']
Performance,"I ran `FindBreakpointEvidenceSpark` and did some high-level checks to see if there are any opportunities for performance improvements. (cc @tedsharpe @cwhelan). This is the command line I ran. (Earlier I had run more executors with smaller memory settings, but the job didn't complete then.); ; ```bash; ./gatk-launch FindBreakpointEvidenceSpark \; -I hdfs:///user/$USER/broad-svdev-test-data/data/NA12878_PCR-_30X.bam \; -O hdfs:///user/$USER/broad-svdev-test-data/assembly \; --exclusionIntervals hdfs:///user/$USER/broad-svdev-test-data/reference/GRCh37.kill.intervals \; --kmersToIgnore hdfs:///user/$USER/broad-svdev-test-data/reference/Homo_sapiens_assembly38.dups \; -- \; --sparkRunner SPARK --sparkMaster yarn-client --sparkSubmitCommand spark2-submit\; --driver-memory 16G \; --num-executors 5 \; --executor-cores 7 \; --executor-memory 25G; ```. What does FindBreakpointEvidenceSpark do, from the perspective of Spark?. * [runTool] filter out secondary and supplementary alignments; * [getMappedQNamesSet] filter out duplicate reads, reads that failed vendor checks, unmapped reads; * Job 0 [ReadMetadata] mapPartitions to find partition stats; * Job 1 [getIntervals] filter and multiple map partitions to find breakpoint intervals ; * Job 2 [removeHighCoverageIntervals] mapPartitionsToPair to find coverage for each interval, then reduceByKey; * Job 3 [getQNames] mapPartitions; * Job 4 [addAssemblyQNames -> getKmerIntervals] mapPartitionsToPair, then reduceByKey, then mapPartitions; * Job 5 [getAssemblyQNames] mapPartitions twice and a collect; * Job 6 [generateFastqs] mapPartitions and combineByKey, then write FASTQ to files. A few observations:; * Jobs 0,1,3 are simple map jobs - very quick 1-2 mins each.; * Job 2 is a simple MR, with a tiny shuffle to sum by key (<1MB of shuffle data); * Job 4 takes a bit longer longer (3min), and shuffles ~3GB. This is a lot faster than when I ran it before with less memory, when it took 9 min. Is it creating a lot of garbage? If you want",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2458#issuecomment-292171884:109,perform,performance,109,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2458#issuecomment-292171884,1,['perform'],['performance']
Performance,"I ran a few Bayesian optimizations, mostly focusing on various subsets/ranges of the read-to-haplotype and haplotype-to-reference SW parameters. I also varied whether I used part/all of chr22 and chose either F1/sensitivity as the optimization target. These experiments show that varying the SW parameters can definitely move the needle in terms of performance, even after variant normalization. For example, here are the SNP precision/sensitivity curves for an optimization of F1 over about half of chr22:. ![snp_f1](https://user-images.githubusercontent.com/11076296/96470421-44490900-11fc-11eb-96f6-83a6833b2b1f.png). And the same for indels:. ![non_snp_f1](https://user-images.githubusercontent.com/11076296/96470468-5034cb00-11fc-11eb-86a7-bbf81bc10122.png). This all raises the question of what the best optimization strategy should be. It looks like the SW parameters aren't the limiting factor for sensitivity (I would guess that might be MQ or other read filters) but can probably help improve precision. So we may want to relax some filters or optimize them jointly. We may also want to look at optimizations that include filtering, as I mention above.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-712237919:21,optimiz,optimizations,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-712237919,7,"['optimiz', 'perform']","['optimization', 'optimizations', 'optimize', 'performance']"
Performance,"I rebased this, and responded to code review comments:. - updated comments; - reverted GenotypeGVCFs change; - reverted changing the default lookahead for VariantWalker side inputs. I think changing the default lookahead for VariantWalker side inputs to the new, smaller value will hurt performance for tools like VQSR. I did some crude timing tests using the FeatureDataSource default (1000 bases) proposed in this branch, and the current default (100,000 bases). The following are total times as reported by Gradle for serial runs of the VQSR integration tests:. With 1000 base lookahead:; 1m40s; 1m29s; 1m29s; 1m25s. With 100,000 base lookahead:; 1m29s; 1m17s; 1m16s; 1m18s. Back to 1000 base lookahead again:; 1m36s; 1m26s; 1m26s; 1m29s. It seems pretty consistently slower with the smaller lookahead.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3480#issuecomment-417360848:287,perform,performance,287,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3480#issuecomment-417360848,1,['perform'],['performance']
Performance,"I refactored some of the stream methods to address @akiezun's (well-founded, as running HaplotypeCaller showed) performance concerns. That, along with @lbergelson's suggestion to lazily evaluate error messages, leaves HaplotypeCaller's performance unaffected. Tests are passing locally but Travis is giving me a mysterious error:. > compileTestJavaerror: error reading /home/travis/.gradle/caches/modules-2/files-2.1/commons-httpclient/commons-httpclient/3.1/964cd74171f427720480efdec40a7c7f6e58426a/commons-httpclient-3.1.jar; error in opening zip file; > error: error reading /home/travis/.gradle/caches/modules-2/files-2.1/commons-httpclient/commons-httpclient/3.1/964cd74171f427720480efdec40a7c7f6e58426a/commons-httpclient-3.1.jar; cannot read zip file. I am fully rebased onto master, including @akiezun's PR from this afternoon. @droazen and @lbergelson do you have insights?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1979#issuecomment-232447710:112,perform,performance,112,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1979#issuecomment-232447710,4,"['cache', 'perform']","['caches', 'performance']"
Performance,"I reproduced various out of memory errors in a Linux VM with 4G of RAM, both with the `IntelInflaterDeflaterIntegrationTest` enabled and disabled. Most resulted in the kernel killing the Java process, like this one (from `dmesg`):; ```; [38425.759992] Out of memory: Kill process 10295 (java) score 747 or sacrifice child; [38425.759998] Killed process 10295 (java) total-vm:7885212kB, anon-rss:3250892kB, file-rss:0kB; ```. Some were caught by the JVM, like this one:; ```; #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 90177536 bytes for committing reserved memory.; # Possible reasons:; # The system is out of physical RAM or swap space; # In 32 bit mode, the process size limit was hit; # Possible solutions:; # Reduce memory load on the system; # Increase physical memory or swap space; # Check if swap backing store is full; # Use 64 bit Java on a 64 bit OS; # Decrease Java heap size (-Xmx/-Xms); # Decrease number of Java threads; # Decrease Java thread stack sizes (-Xss); # Set larger code cache with -XX:ReservedCodeCacheSize=; # This output file may be truncated or incomplete.; #; # Out of Memory Error (os_linux.cpp:2627), pid=20484, tid=139679452493568; #; # JRE version: Java(TM) SE Runtime Environment (8.0_72-b15) (build 1.8.0_72-b15); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.72-b15 mixed mode linux-amd64 compressed oops); # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; ```. Here's my theory of what's happening. The `maxHeapSize` for test JVMs is set to 4G in `build.gradle`:; ```; maxHeapSize = ""4G""; ```. A 4G max heap size is too high for systems with 4G of RAM, because the Java heap grows until the system runs out of memory. If we decrease `maxHeapSize`, the GC should prevent the Java heap from growing too large, with the trade-off of more GC calls. I changed the `maxHeapSize` to `2G` a",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2490#issuecomment-288423316:813,load,load,813,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2490#issuecomment-288423316,2,"['cache', 'load']","['cache', 'load']"
Performance,"I see same problem in `4.0.5.2`:. java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/vojta/bin/gatk/gatk-package-4.0.5.2-local.jar GenotypeGVCFs -O rad34test.comb2.raw.g.vcf.gz -R ../../../jic_reference/alygenomes.fasta -V rad34test.comb2.raw.vcf.gz --new-qual; 22:45:47.050 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/vojta/bin/gatk/gatk-package-4.0.5.2-local.jar!/com/intel/gkl/native/libgkl_compression.so; 22:45:47.648 INFO GenotypeGVCFs - ------------------------------------------------------------; 22:45:47.649 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.0.5.2; 22:45:47.649 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 22:45:47.800 INFO GenotypeGVCFs - Initializing engine; 22:45:48.331 INFO FeatureManager - Using codec VCFCodec to read file file:///home/vojta/dokumenty/fakulta/botanika/arabidopsis/samples/lib_2018_06/4_joined/rad34test.comb2.raw.vcf.gz; 22:45:48.467 INFO GenotypeGVCFs - Done initializing engine; 22:45:48.555 INFO ProgressMeter - Starting traversal; 22:45:48.556 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 22:45:51.038 WARN InbreedingCoeff - Annotation will not be calculated, must provide at least 10 samples; 22:45:51.045 INFO GenotypeGVCFs - Shutting down engine; [2. července 2018 22:45:51 CEST] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 0.07 minutes.; Runtime.totalMemory()=528482304; java.lang.IllegalArgumentException: log10LikelihoodsOfAC are bad 6.911788849595091E-17,NaN; at org.broadinstitute.hellbender.tools.walkers.genotyper.afcalc.AFCalculationResult.<init>(AFCalculationResult.java:72); at org.broadinstitute.hellbender.tools.walkers.genotyper.afcalc.AlleleFrequencyCalculator.getLog10PNonRef(AlleleFrequencyCalculator.java:143); at org.broadinstitute",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4975#issuecomment-401933028:422,Load,Loading,422,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4975#issuecomment-401933028,1,['Load'],['Loading']
Performance,"I see the exception on most chromosomes, here's the one for chr1:. java.lang.IllegalArgumentException: Invalid interval. Contig:chr1 start:79293873 end:79293872; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:49); at org.broadinstitute.hellbender.engine.AssemblyRegion.add(AssemblyRegion.java:335); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.fillNextAssemblyRegionWithReads(AssemblyRegionIterator.java:230); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.loadNextAssemblyRegion(AssemblyRegionIterator.java:194); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.next(AssemblyRegionIterator.java:135); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.next(AssemblyRegionIterator.java:34); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:290); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:271); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:893); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:136); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:152); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:195); at org.broadinstitute.hellbender.Main.main(Main.java:275)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4120#issuecomment-356718095:782,load,loadNextAssemblyRegion,782,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4120#issuecomment-356718095,1,['load'],['loadNextAssemblyRegion']
Performance,"I see. . Yes. That's what I'm planning on (except that `AssemblyContigAlignmentsConfigPicker` is upstream of this unit), and here's the thought for why:; * I'd try to place the alignment picking step in a single place as much as possible, this makes improvements to the alignment picking/filtering step easier; * the size-based filter can be tuned, even by an CLI argument, this would affect the number of segments in the CPX logic, and the alt_arrangment annotations, and the simple variants re-interpreted by `CpxVariantReInterpreterSpark`, but it won't affect the alt haplotype sequence, which IMO is what really is important. ; * I'm developing a downstream variant filter, which hopefully can cut down the false-positives. And for the question of ""why 2 instead of 1"", I think what you are suggesting is to change; ```java; public static final int MIN_READ_SPAN_AFTER_DEOVERLAP = 2;; if (one.getSizeOnRead() >= MIN_READ_SPAN_AFTER_DEOVERLAP) result.add(one);; ```; to; ```java; public static final int MIN_READ_SPAN_AFTER_DEOVERLAP = 1;; if (one.getSizeOnRead() > MIN_READ_SPAN_AFTER_DEOVERLAP) result.add(one);; ```; Am i right?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4962#issuecomment-405619353:342,tune,tuned,342,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4962#issuecomment-405619353,1,['tune'],['tuned']
Performance,"I started with Mark Duplicates and found that on the 76MB BAM file in resources, the walker took ~20 seconds, while local Spark took ~120s (with all cores). However, when I changed GATKRegistrator to use SAMRecordToGATKReadAdapterSerializer, local Spark took ~36s (all cores) or ~55s (1 core). Also, this is not writing a single file, since https://github.com/broadinstitute/gatk/issues/1015 has not been fixed yet. So we should definitely use the better serializer - is there any reason not to do that now?. Spark performance running locally with a single core on small files is probably not going to match the walker implementations since the local runner is not tuned for that use case.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1047#issuecomment-157095589:515,perform,performance,515,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1047#issuecomment-157095589,2,"['perform', 'tune']","['performance', 'tuned']"
Performance,"I still got the same error with version 4.1.9.0. . > Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/orange/reed/nhouse/Raw_seqs/SEQ9_samples/tmp; 11:30:50.248 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 11:30:50.478 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/apps/gatk/4.1.9.0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Oct 26, 2020 11:30:50 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 11:30:50.791 INFO CombineGVCFs - ------------------------------------------------------------; 11:30:50.791 INFO CombineGVCFs - The Genome Analysis Toolkit (GATK) v4.1.9.0; 11:30:50.792 INFO CombineGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:30:50.792 INFO CombineGVCFs - Executing as nwijewardena@c3a-s8.ufhpc on Linux v3.10.0-1062.18.1.el7.x86_64 amd64; 11:30:50.792 INFO CombineGVCFs - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_31-b13; 11:30:50.792 INFO CombineGVCFs - Start Date/Time: October 26, 2020 11:30:50 AM EDT; 11:30:50.793 INFO CombineGVCFs - ------------------------------------------------------------; 11:30:50.793 INFO CombineGVCFs - ------------------------------------------------------------; 11:30:50.794 INFO CombineGVCFs - HTSJDK Version: 2.23.0; 11:30:50.794 INFO CombineGVCFs - Picard Version: 2.23.3; 11:30:50.794 INFO CombineGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 11:30:50.794 INFO CombineGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 11:30:50.794 INFO CombineGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 11:30:50.794 INFO CombineGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 11:30:50.795 INFO CombineGVCFs - Deflater: IntelDeflater; 11:30:50.795 INFO CombineGVCFs - Inflater: IntelInflater; 11:30:50",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6766#issuecomment-716640444:326,Load,Loading,326,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6766#issuecomment-716640444,1,['Load'],['Loading']
Performance,"I suspect it will also make things substantially slower. It will end up doing essentially a full sort of the bam file I believe. Since we really only need local sorting to fix the problem this is a bit overkill. If this is a bottleneck we can likely improve it, but it would be more work than just flipping the single flag.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1865#issuecomment-224902018:225,bottleneck,bottleneck,225,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1865#issuecomment-224902018,1,['bottleneck'],['bottleneck']
Performance,"I suspect it's a different issue than #4062, but probably some similar library incompatibility issue. Unfortunately the stack trace doesn't have enough information in it. We should consider rewriting the error message for this so that we are sure to have the first cause reported, not just the `Could not load genomicsdb native library` message.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4124#issuecomment-356984584:305,load,load,305,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4124#issuecomment-356984584,1,['load'],['load']
Performance,I suspect this was caused by the race condition in #2050 ; will close now - reopen if seen after #2053 is merged,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2039#issuecomment-235660642:33,race condition,race condition,33,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2039#issuecomment-235660642,1,['race condition'],['race condition']
Performance,"I talked to comms and we agreed that a ""mitochondria-mode"" argument to Mutect2 was the right balance of clarity (you're really running Mutect2 not a wrapper) and simplicity (you don't need a laundry list of arguments to change which mode you're in if you just want to run with optimized defaults). . @ldgauthier @davidbenjamin @takutosato @rcmajovski Could you please take another look? Removing the wrapper tools has cleaned up the code so there are fewer changes now. I also changed TLOD to LOD in this version, but I'm happy to take that out and have that be future work if anyone is worried about it being a breaking change.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5193#issuecomment-428195077:277,optimiz,optimized,277,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5193#issuecomment-428195077,1,['optimiz'],['optimized']
Performance,"I terms of comparing implementations in this particular instance, the entire BAM file is also being scanned, so there is no additional limitation in using Parquet. In general, though, there are some additional complications for typical Hadoop data sets due to their distributed nature. There are multiple methods for accelerating lookups, depending on the particular application and what kind of latency you need. A few of them:; - Data set partitioning in the style of Hive, where you split your data set into a directory hierarchy that's based on the values of one of the columns. This is like building an index, and should be done on cols that feature in lot of predicates. In the quince repo that we're using for ingest, we are partitioning the data based on genome locus. This makes it easy to access the data only from the locus of interest.; - Parquet supports predicate pushdown and column stats on its row chunks, so the more you homogenize the data (e.g., by sorting), the more likely it is you can skip large blocks of data.; - The parquet file format supports the concept of indexing (though I don't think it is implemented yet in any of the packages that read/write it); - For particular applications, you can also use a different storage backend like HBase or Kudu that allow very rapid point/range queries at scale. I believe GEL is planning on trying out HBase for some of their applications; - The Hammerbacher lab and the ADAM folks are also working on tools for BAM file indexing and visualization that scales on Hadoop. I think the projects are cycledash/pileup.js and mango, respectively.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1067#issuecomment-152446541:396,latency,latency,396,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1067#issuecomment-152446541,1,['latency'],['latency']
Performance,"I think all of our dataflow / spark code is at least almost entirely using `GATKRead`. GATKRead is designed to not provide access to the header because it's not available from a google `Read` backed `GATKRead`. It sounds like there is some information that `Read` includes that is missing from a headerless `SAMRecord`. I think we could audit the `SAMRecordToGATKReadAdaptor` to find any places it touches the header and then cache that information in the adaptor before stripping the header. We don't need to add back in the headers at any point, because we provide library functions to perform any header related operation with a provided header.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141107659:426,cache,cache,426,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141107659,2,"['cache', 'perform']","['cache', 'perform']"
Performance,"I think issues with gCNV postprocessing initially stemmed from things like 1) taking large sample x shard transposes (which Cromwell may have had trouble with at some point), 2) localizing more files than necessary for each sample, 3) introduction of lexicographical globbing bugs, etc. I think various people have tried to address this via things like bundling, which we ended up reverting in favor of transposing. Unfortunately, I'm afraid I've lost the plot on this after so long; and not sure I ever had it---is there any documentation (e.g., of things like how bundling improved performance) elsewhere that might help us decide what's left to be done? Seems like I had some more coherent thoughts in https://github.com/broadinstitute/gatk/pull/6607#issuecomment-632303744. As before, if any remaining issues like the call caching mentioned above would best be addressed at the Cromwell level, I think it's at least worth an investigation of what might be involved.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4397#issuecomment-900624782:584,perform,performance,584,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4397#issuecomment-900624782,1,['perform'],['performance']
Performance,"I think it would be good to profile and compare a variety of approaches for dealing with the reference data. In the dr_read_preprocessing_pipeline_skeleton branch we are taking the approach of sharding the reference into fixed-size chunks, and using GA4GH API calls (rather than files) to populate each chunk. We currently have no idea how well this will perform, though we suspect it will be better than copying the entire reference to each worker. @tomwhite Would it be possible at this point to profile the two options you mentioned (""broadcast the entire reference sequence to all nodes in the cluster"" vs. ""streaming reads from HDFS"") with a real human reference like GRCh37 so that we can make an informed choice between them?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/567#issuecomment-112850702:355,perform,perform,355,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/567#issuecomment-112850702,1,['perform'],['perform']
Performance,"I think that this is part of wider need to for dependency-injection in tools; often the initialize() method might be loaded with instantiation of components that themselves require some user argument inputs.. can this be done in a more declarative fashion? . For example... HC, UG or GenotypeGVCFs the have annotationEngine or genotypingEngine components that are explicitly initialized in initialize() what if the engine is responsible to instantiate them and add the appropriate arguments to the command line which are declared in the corresponding classes rather than in the tool (or a synthetic argument collection class)?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/96#issuecomment-69810912:117,load,loaded,117,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/96#issuecomment-69810912,1,['load'],['loaded']
Performance,"I think the `Optimized` version deals with avoiding replicating reads in a more nuanced manner, but if I understand correctly, it doesn't seem to me to avoid the shuffle. It looks like it essentially ignores the concept of data locality entirely, and potentially transfers a lot of data over the network. (Equivalent to performing a shuffle on a sorted file in HDFS.). IMO, the current ""shuffle"" implementation is already a ""Spark-y"" way to do it, but with multiple inefficiencies:; - Reads/variants are keyed to their corresponding shards, replicating reads if they cross over shard boundaries. This necessitates an `aggregateByKey` operation that potentially reshuffles the entire data set at the end to deal with neighboring shards that could be hashed to different machines.; - The impl uses `cogroup` and `groupByKey`, which require materializing all values for a key in memory (which could be large). Best to avoid these if possible.; - And related to the previous issue, the join strategy for reads and variants is basically a cross-product-and-filter, which is not very efficient, especially considering that the data can be ordered. I think the best implementation here would steal JP's method of sharding the reads/variants, but make use of `repartitionAndSortWithinPartition`, which lets you specify what partition to use and also sorts all the values within a given partition. This means that we could employ a sort-merge on each partition, and only scan through the datasets once after shuffling them. Do you already have an impl for doing a sort-merge of `Locatable`s? These can be a bit tricky. I wrote one for the `ShuffleRegionJoin` impl in ADAM, but there are semantic differences that would make it less efficient to use. (Specifically, it would require the `aggregateByKey` operation and also creating `SimpleInterval`-style objects from a separate model.). Finally, I would also add the ability to specify which join strategy to use separately for the reference bases and the vari",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1007#issuecomment-151721602:13,Optimiz,Optimized,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1007#issuecomment-151721602,2,"['Optimiz', 'perform']","['Optimized', 'performing']"
Performance,"I think the root cause is the method ensureCapacity of GenotypesCache is not synchronized. So when multiple task threads run into this method, the new added cache is not fully initialized.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8961#issuecomment-2306051902:157,cache,cache,157,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8961#issuecomment-2306051902,1,['cache'],['cache']
Performance,"I think there are two points to this issue: 1. validation or no-validation and 2. need to pass GenomeLocParser around... . Is 1. about performance? otherwise we prefer to have validated locations, right? and If it is about performance I think it should be shown that it really makes a difference to remove those checks. About, 2., can be solved by being able to recover the sequence-dictonary/reference object (called Reference from this point on) from a genome loc and then you can ask it for a new genome-loc if the appropriate methods are added instead of depending on that annoying middle man called GenomeLocParser. I would say that is unlikely to be in the situation where you want to create a GenomeLoc out of the blur without having already a reference to another GenomeLoc or Reference object available. It is an issue if GenomeLoc holds on to a reference to the Reference object? (or the instantiating class is a inner class of the Reference?)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/100#issuecomment-69802695:135,perform,performance,135,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/100#issuecomment-69802695,2,['perform'],['performance']
Performance,"I think this is happening because were trying to serialize the class loader sun.misc.Launcher$AppClassLoader), which appears to be reached through the graph by way of via https://github.com/damiencarol/jsr203-hadoop/blob/master/src/main/java/hdfs/jsr203/HadoopFileSystem.java#L82. We probably need to short circuit that with a custom serializer for one of these:. Serialization trace:; classes (sun.misc.Launcher$AppClassLoader); classLoader (org.apache.hadoop.conf.Configuration); conf (org.apache.hadoop.hdfs.DistributedFileSystem); fs (hdfs.jsr203.HadoopFileSystem); hdfs (hdfs.jsr203.HadoopPath); path (htsjdk.samtools.seekablestream.SeekablePathStream); seekableStream (htsjdk.tribble.TribbleIndexedFeatureReader); featureReader (org.broadinstitute.hellbender.engine.FeatureDataSource); featureSources (org.broadinstitute.hellbender.engine.FeatureManager). See, for instance, https://github.com/dbpedia/distributed-extraction-framework/issues/9.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5680#issuecomment-668654169:69,load,loader,69,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680#issuecomment-668654169,1,['load'],['loader']
Performance,"I think this issue is very important to me. When I deal with the CAP data (high capture cfDNA data), GATK4.1.0.8's performance is much better than GATK4.2 ...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1174725963:115,perform,performance,115,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1174725963,1,['perform'],['performance']
Performance,"I think we are also waiting for FC to update, so that NIO can be call cached. Not sure what the status is on that.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5015#issuecomment-461938277:70,cache,cached,70,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5015#issuecomment-461938277,1,['cache'],['cached']
Performance,I think we discovered at some point that IntervalSkipList has worse performance than htsjdk `OverlapDetector`. We should probably be deprecating and removing it. #3608,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3541#issuecomment-331950525:68,perform,performance,68,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3541#issuecomment-331950525,1,['perform'],['performance']
Performance,"I think we should wait for the NIO-based fix rather than merging this, which would be useless for our performance work.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2333#issuecomment-271720196:102,perform,performance,102,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2333#issuecomment-271720196,1,['perform'],['performance']
Performance,I think we should write most readable code for now and optimize with a profiler later. The regexp machinery in Java libraries definitely does a better job optimizing the state machine than a person can.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/380#issuecomment-94477514:55,optimiz,optimize,55,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/380#issuecomment-94477514,2,['optimiz'],"['optimize', 'optimizing']"
Performance,"I thought I figured this out but I haven't. I'm looking at sample1.md.bam file header, but I can't figure it out. ```. for BAM in sample1.md.bam ; do samtools view -H $BAM; > header.sam; done; @HD	VN:1.5	SO:coordinate; @SQ	SN:1	LN:4918979; @SQ	SN:2	LN:4844472; @SQ	SN:3	LN:4079167; @SQ	SN:4	LN:3923705; @SQ	SN:5	LN:3948441; @SQ	SN:6	LN:3778736; @SQ	SN:7	LN:2058334; @SQ	SN:8	LN:1833124; @PG	ID:hisat2	PN:hisat2	VN:2.1.0	CL:""/usr/local/apps/eb/HISAT2/2.1.0-foss-2016b/bin/hisat2-align-s --wrapper basic-0 -p 8 --no-spliced-alignment -I 100 -x ./ref_tran.dir/ref_tran -S sample1.sam -1 e1_R1.fastq -2 e1_R2.fastq""; @PG	ID:MarkDuplicates	VN:2.16.0-SNAPSHOT	CL:MarkDuplicates INPUT=[sample1.bam] OUTPUT=sample1.md.bam METRICS_FILE=sample1.md.metrics.txt MAX_SEQUENCES_FOR_DISK_READ_ENDS_MAP=50000 MAX_FILE_HANDLES_FOR_READ_ENDS_MAP=8000 SORTING_COLLECTION_SIZE_RATIO=0.25 TAG_DUPLICATE_SET_MEMBERS=false REMOVE_SEQUENCING_DUPLICATES=false TAGGING_POLICY=DontTag CLEAR_DT=true ADD_PG_TAG_TO_READS=true REMOVE_DUPLICATES=false ASSUME_SORTED=false DUPLICATE_SCORING_STRATEGY=SUM_OF_BASE_QUALITIES PROGRAM_RECORD_ID=MarkDuplicates PROGRAM_GROUP_NAME=MarkDuplicates READ_NAME_REGEX=<optimized capture of last three ':' separated fields as numeric values> OPTICAL_DUPLICATE_PIXEL_DISTANCE=100 MAX_OPTICAL_DUPLICATE_SET_SIZE=300000 VERBOSITY=INFO QUIET=false VALIDATION_STRINGENCY=STRICT COMPRESSION_LEVEL=5 MAX_RECORDS_IN_RAM=500000 CREATE_INDEX=false CREATE_MD5_FILE=false GA4GH_CLIENT_SECRETS=client_secrets.json USE_JDK_DEFLATER=false USE_JDK_INFLATER=false	PN:MarkDuplicates. ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5372#issuecomment-434156611:1174,optimiz,optimized,1174,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5372#issuecomment-434156611,1,['optimiz'],['optimized']
Performance,"I tried clearing my caches and rebuilding, but I resolve everything. I noticed that our artifactory website looks much different today than it did yesterday. I wonder if it was down temporarily for an update. Maybe try again now? Unless they put it behind the firewall which would be a disaster... can you access https://artifactory.broadinstitute.org/?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2579#issuecomment-292587641:20,cache,caches,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2579#issuecomment-292587641,1,['cache'],['caches']
Performance,"I tried from a GCE instance and got **6m12s** for the gsutil copy, and **5m21s** for the NIO code. So we know that using the NIO code would match the gsutil performance (in this case, about 100MB/s). . As to the original question of how this translates to Spark performance, well, this test just fails to prove that GCS/NIO are too slow; more investigation is needed. . The cluster we're using has 10 machines so it may be able to run up to 10x faster than this single-machine test, ie. only about 30s to load the data via NIO. Of course, the program does more than that, and we still have to demonstrate that we're not going to bottleneck the GCS servers.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1755#issuecomment-227905621:157,perform,performance,157,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1755#issuecomment-227905621,4,"['bottleneck', 'load', 'perform']","['bottleneck', 'load', 'performance']"
Performance,"I tried the FilterSamReads command using the Picard jar file and the output bam file to stdout had no issues; Command:; `java -jar ~/picard/build/libs/picard.jar FilterSamReads -I subsampled.bam -O /dev/stdout --READ_LIST_FILE read_names.txt --FILTER excludeReadList --VALIDATION_STRINGENCY SILENT --QUIET > picard_stdoutbam.bam`; Log:; ```; 10:33:09.327 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/gbrandt/picard/build/libs/picard.jar!/com/intel/gkl/native/libgkl_compression.dylib; INFO	2021-02-23 10:33:09	FilterSamReads	Filtering [presorted=true] subsampled.bam -> OUTPUT=stdout [sortorder=coordinate]; INFO	2021-02-23 10:33:09	SAMFileWriterFactory	Unknown file extension, assuming BAM format when writing file: file:///dev/stdout; INFO	2021-02-23 10:33:09	FilterSamReads	6 SAMRecords written to stdout; ```; Checking the file:; `gunzip -c -d -f picard_stdoutbam.bam | head -n 5`; No issues:; ```; BAM?2@HD	VN:1.6	SO:coordinate; @SQ	SN:1	LN:249250621	AS:NCBI-Build-37	SP:Homo sapiens	UR:http://www.bcgsc.ca/downloads/genomes/9606/hg19/1000genomes/bwa_ind/genome/GRCh37-lite.fa; @SQ	SN:2	LN:243199373	AS:NCBI-Build-37	SP:Homo sapiens	UR:http://www.bcgsc.ca/downloads/genomes/9606/hg19/1000genomes/bwa_ind/genome/GRCh37-lite.fa; @SQ	SN:3	LN:198022430	AS:NCBI-Build-37	SP:Homo sapiens	UR:http://www.bcgsc.ca/downloads/genomes/9606/hg19/1000genomes/bwa_ind/genome/GRCh37-lite.fa; @SQ	SN:4	LN:191154276	AS:NCBI-Build-37	SP:Homo sapiens	UR:http://www.bcgsc.ca/downloads/genomes/9606/hg19/1000genomes/bwa_ind/genome/GRCh37-lite.fa; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7080#issuecomment-784427228:382,Load,Loading,382,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7080#issuecomment-784427228,1,['Load'],['Loading']
Performance,I tried the latest GATK release and also reported errors.; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx50G -Djava.io.tmpdir=./tmp -jar /public/home/gaoshibin/software/GATK/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar GenotypeGVCFs -R /public/home/gaoshibin/B73_REF/Zea_mays.AGPv4.dna.toplevel.fa -V gendb://./CHR7_gvcf_database -G StandardAnnotation --genomicsdb-shared-posixfs-optimizations true -O new_ALL_MATERIALS_chr7.g.vcf.gz; 17:49:50.404 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 17:49:50.653 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/public/home/gaoshibin/software/GATK/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 17:49:51.271 INFO GenotypeGVCFs - ------------------------------------------------------------; 17:49:51.273 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.6.1; 17:49:51.273 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:49:51.273 INFO GenotypeGVCFs - Executing as gaoshibin@comput6 on Linux v3.10.0-693.el7.x86_64 amd64; 17:49:51.274 INFO GenotypeGVCFs - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_211-b12; 17:49:51.274 INFO GenotypeGVCFs - Start Date/Time: 2022年5月22日 下午05时49分50秒; 17:49:51.274 INFO GenotypeGVCFs - ------------------------------------------------------------; 17:49:51.275 INFO GenotypeGVCFs - ------------------------------------------------------------; 17:49:51.276 INFO GenotypeGVCFs - HTSJDK Version: 2.24.1; 17:49:51.276 INFO GenotypeGVCFs - Picard Version: 2.27.1; 17:49:51.276 INFO GenotypeGVCFs - Built for Spark Version: 2.4.5; 17:49:51.277 INFO GenotypeGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 17:49:51.277 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_S,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7866#issuecomment-1135301848:507,optimiz,optimizations,507,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7866#issuecomment-1135301848,2,"['Load', 'optimiz']","['Loading', 'optimizations']"
Performance,"I used to have it in there, but once I got copy performance up I assumed it was just clutter. I need to make a minor update to the scripts anyway (option to run the debug program instead of the full pipeline) so I can put back the option to not copy fastq files as well.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4332#issuecomment-362655529:48,perform,performance,48,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4332#issuecomment-362655529,1,['perform'],['performance']
Performance,"I verified that the intel deflater was being loaded in the executors by checking their logs. Not all executors actually do any work on the cluster I was using, so check one that has output.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1785#issuecomment-215285682:45,load,loaded,45,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1785#issuecomment-215285682,1,['load'],['loaded']
Performance,"I vote no -- the danger (and need to worry about the danger!) is not worth the relatively small gain. `HaplotypeCaller` might very possibly create cyclical object references, so I don't think this is merely hypothetical. I recommend we stick to safe optimizations.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1734#issuecomment-212134553:250,optimiz,optimizations,250,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1734#issuecomment-212134553,1,['optimiz'],['optimizations']
Performance,"I vote to close. As per my earlier remark, performance reading from GCS buckets (at least in the case highlighted in this ticket) is just fine. The root cause was that the comparison was forgetting about the 6min it takes to place the GCS file onto the cluster.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1755#issuecomment-265000498:43,perform,performance,43,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1755#issuecomment-265000498,1,['perform'],['performance']
Performance,I was able to reproduce this problem by running 1000 concurrent read streams for a while. This allowed me to verify that my fix indeed resolves the problem (at least for my repro). I have sent a [PR](https://github.com/GoogleCloudPlatform/google-cloud-java/pull/2083) to gcloud for review.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-303211426:53,concurren,concurrent,53,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-303211426,1,['concurren'],['concurrent']
Performance,"I was assuming that this would be fixed by #1433, but it only fixed the inverse of this problem. It's possible now to load HDFS files from the local runner using the full namenode path . i.e. ; `hdfs://dataflow01.broadinstitute.org/user/louisb/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam`, . Loading files with the sparkRunner and yarn-client is still failing. . We're getting a new error now though. ```; java.lang.IllegalArgumentException: unknown SAM format, cannot create RecordReader: file:/local/dev/akiezun/bin/gatk/src/test/resources/org/broadinstitute/hellbender/tools/valid.bam; at org.seqdoop.hadoop_bam.AnySAMInputFormat.createRecordReader(AnySAMInputFormat.java:181); at org.apache.spark.rdd.NewHadoopRDD$$anon$1.<init>(NewHadoopRDD.scala:151); at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:124); at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:65); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:88); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(T",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1417#issuecomment-174655123:118,load,load,118,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1417#issuecomment-174655123,2,"['Load', 'load']","['Loading', 'load']"
Performance,"I was looking `ReadWindowWalker` and I found it very interesting for iterate over windows, but although it is similar I still think that is not solving the same problem as the `SlidingWindowWalker` for two reasons:; 1. `ReadWindowWalker` requires reads to construct the `ReadWindow`, and it is not general for both reads and variants. My main idea behind the `SlidingWindowWalker` was to perform operation over windows along the genome (or requested intervals) for any kind of source provided to the walker.; 2. On the other hand, the approach to generate the sliding windows in my implementation is different from the one in `ReadWindowWalker`: first, the overlap between windows is only in one direction; second, `SlidingWindowWalker` is more like a reference/interval walker, from the beginning of the reference (or interval) till the end, it walks in overlapping windows. One example is the following (window-size 10, window-step 5, the - represent the window):. ```; Reference: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _; Windows1: _ _ _ _ _ _ _ _ _ _; Windows2: _ _ _ _ _ _ _ _ _ _; Windows3: _ _ _ _ _ _ _ _ _ _; Windows4: _ _ _ _ _ _ _ _ _ _; Windows5: _ _ _ _ _ _ _ _ _ _; ```. Of course, after having a look to `ReadWindowWalker` I think that several things could be improved in my implementation for a general `SlidingWindowWalker`:; - Apply function similar to the `ReadWindowWalker`, with `ReadWindow` being empty if reads are not provided.; - Three window options: `windowSize` (the actual size of the window), `windowStep` (how much advance for the following window) and `windowPadding` (how much extend the window in both directions). Using this abstraction, `ReadWindowWalker` could be implemented setting `windowSize=windowStep`, and the problem that I need to solve could be implemented setting `windowPadding=0`. The simplest way to acomplish this is to use the current implementation of `ReadWindowWalker` to develop a `SlidingWindowWalker` adding three abstract ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1528#issuecomment-198438775:388,perform,perform,388,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1528#issuecomment-198438775,1,['perform'],['perform']
Performance,"I was looking into this because it is useful for me, and I have found that there is going to be redundancy between the `VariantAnnotatorEngine`code and the plugin. Here a couple of suggestions after trying to implement something in this regard time ago:. * Remove/deprecate the private class `AnnotationManager` in favor of the plugin. The current code is performing reflection operations by itself, and this can cause some problems.; * Refactor the `VariantAnnotatorEngine` constructors in favor of a constructor from the barclay plugin and a list of annotations to apply, to avoid the `AnnotationManager` implementation.; * Remove/deprecate static methods for creating an annotator engine (`ofAllMinusExcluded` and `ofSelectedMinusExcluded`) in favor of handling this in the plugin.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3287#issuecomment-316077922:356,perform,performing,356,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3287#issuecomment-316077922,1,['perform'],['performing']
Performance,"I was mistaken about this not being faster - I was using a counting function that Spark can optimise by pulling onto the map side so that the records don't go through the shuffle. I changed this to simply dump the processed reads so they have to go through the shuffle, and I got the following timings when processing a 121GB BAM file.; - With shuffle: 27 min; - No shuffle (two scans over input): 24.7 min (8% saving); - No shuffle (one scan over input): 17 min (37% saving). The version that does two scans is faster, but not hugely so. Removing a scan is possible, but requires the use of a sequence dictionary to find the end points of contigs. I've done this in the latest version of my branch (https://github.com/broadinstitute/gatk/compare/tw_overlap_partitioner), but there are more edge cases to test. Before I do this, however, it would be worth trying this approach with the Haplotype Caller to see if it works, and if it is appreciably faster. If the number of reads is filtered significantly so only a fraction go through the shuffle, then the performance gains will be smaller, and may not in fact be worth the increase in code complexity. @droazen, what do you think?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1988#issuecomment-249590040:1057,perform,performance,1057,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1988#issuecomment-249590040,1,['perform'],['performance']
Performance,"I was outputting to .vcf.gz. . I reran the command to output to just. vcf and it runs without error:; ```; /gatk-launch FilterByOrientationBias --artifactModes 'G/T' -V /Users/shlee/Documents/workshop_materials/mutect2_tutorial/mutect2_handson/8_mutect2.vcf.gz -P ~/Documents/workshop_materials/mutect2_tutorial/mutect2_handson/gatk_6_T_artifact.pre_adapter_detail_metrics -O test_filterbyorientationbias.vcf; Using GATK wrapper script /Users/shlee/Documents/branches/hellbender/build/install/gatk/bin/gatk; Running:; /Users/shlee/Documents/branches/hellbender/build/install/gatk/bin/gatk FilterByOrientationBias --artifactModes G/T -V /Users/shlee/Documents/workshop_materials/mutect2_tutorial/mutect2_handson/8_mutect2.vcf.gz -P /Users/shlee/Documents/workshop_materials/mutect2_tutorial/mutect2_handson/gatk_6_T_artifact.pre_adapter_detail_metrics -O test_filterbyorientationbias.vcf; 01:16:16.916 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/shlee/Documents/branches/hellbender/build/install/gatk/lib/gkl-0.4.3.jar!/com/intel/gkl/native/libgkl_compression.dylib; [June 6, 2017 1:16:16 AM EDT] FilterByOrientationBias --output test_filterbyorientationbias.vcf --preAdapterDetailFile /Users/shlee/Documents/workshop_materials/mutect2_tutorial/mutect2_handson/gatk_6_T_artifact.pre_adapter_detail_metrics --artifactModes G/T --variant /Users/shlee/Documents/workshop_materials/mutect2_tutorial/mutect2_handson/8_mutect2.vcf.gz --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --readValidationStringency SILENT --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation false --createOutputBamIndex true --createOutputBamMD5 false --createOutputVariantIndex true --createOutputVariantMD5 false --lenient false --addOutputSAMProgramRecord true --addOutputVCFCommandLine true --cloudPrefetchBuffer 40 --cloudIndexPrefetchBuffer -1 --disableBamIndexCaching false --help false --version false --showHidden false --verbosity ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3030#issuecomment-306384891:928,Load,Loading,928,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3030#issuecomment-306384891,1,['Load'],['Loading']
Performance,"I was thinking more carefully on this and another option is create methods in `ReadPileup` to fix the overlaps after construction and/or getBaseCounts without overlaps. This won't break the behaviour of LIBS and it is up to the user to change overlaps. But for performance issues, I would like to have a variable in `ReadPileup` for track if the overlaps are corrected/fixed, to avoid recomputation. I can implement this in a different PR or in this one if the basic idea behind this one is not accepted.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2041#issuecomment-235993555:261,perform,performance,261,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2041#issuecomment-235993555,1,['perform'],['performance']
Performance,"I will start working on this off of the work that currently resides in #6034. The proposal will be to perform KBestHaplotype finding for multiple source/sink vertexes and then perform smith waterman on the resulting ""dangling"" haplotypes that are created in order to recover the probable dangling sequence. Hopefully the number of haplotypes will have been brought down by enough that this operation will be tolerable in terms of cost.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5957#issuecomment-511024376:102,perform,perform,102,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5957#issuecomment-511024376,2,['perform'],['perform']
Performance,"I wondered how much of the time was due to parsing the VCF file. To test this, I used Kryo to serialize the `IntervalsSkipList` to a binary blob, then tried loading the binary blob directly. This reduced the load time from around 6 minutes to 4.7 minutes - so some speed improvement, but not a lot. See https://github.com/broadinstitute/gatk/tree/tw_known_sites_perf_kryo",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5103#issuecomment-412897602:157,load,loading,157,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5103#issuecomment-412897602,2,['load'],"['load', 'loading']"
Performance,"I would guess that we actually do get the requested r-backports 1.1.10, but since this is an R package rather than a python package, the python package version check performed by the failing test is not applicable (i.e., you cannot `import r-backports` in python).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6955#issuecomment-726977652:166,perform,performed,166,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6955#issuecomment-726977652,1,['perform'],['performed']
Performance,"I would like to eventually port the indel realignment pipeline, but I don't know if I will have time. @sooheelee, maybe you can tell me if people is interested on it? I think that it is still important for Pool-Seq data, where haplotype-based calling is not usually performed.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3084#issuecomment-307734736:266,perform,performed,266,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3084#issuecomment-307734736,1,['perform'],['performed']
Performance,"I wouldn't be surprised if I could optimize the implementation of the likelihoods model quite a bit. Just having a slightly looser threshold for convergence, for example.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5312#issuecomment-449499104:35,optimiz,optimize,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5312#issuecomment-449499104,1,['optimiz'],['optimize']
Performance,"I'd also note that in the example command that @tomwhite is running uses just 4GB of memory on the driver while using a broadcast join strategy, which I believe is the default strategy used by BQSR. The [GZIP'ed dbSnp build 138 VCF in the hg18 GATK bundle](ftp://ftp.broadinstitute.org/bundle/hg18/) is ~1.4GB, and the hg18 2bit file is probably going to be ballpark 1GB of data. That's a lot of data to collect onto a driver with just 4GB of memory, so I wouldn't be surprised if the driver is OOMing. @lbergelson you'd mentioned [above](https://github.com/broadinstitute/gatk/issues/4376#issuecomment-364187885) that you haven't seen said performance regression and that it isn't showing up in the regression tests kicked off by CI. Can you confirm what driver memory settings you are using?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4376#issuecomment-367150206:641,perform,performance,641,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4376#issuecomment-367150206,1,['perform'],['performance']
Performance,I'll add that we do already know that the existing caching strategy massively improves Funcotator performance (by about an order of magnitude!) due to an evaluation that @lbergelson did.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5143#issuecomment-416986210:98,perform,performance,98,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5143#issuecomment-416986210,1,['perform'],['performance']
Performance,I'll have a look at it - maybe i'll write a naive first cut for later optimization,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/104#issuecomment-77039616:70,optimiz,optimization,70,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/104#issuecomment-77039616,1,['optimiz'],['optimization']
Performance,I'll kick off a test run on FC soon (the queue is currently a bit backed up)...,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5490#issuecomment-450968381:41,queue,queue,41,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5490#issuecomment-450968381,1,['queue'],['queue']
Performance,"I'll open a ticket for ref conf performance optimization, but I'm keen to get a ""draft"" of the MT joint calling pipeline into 4.1.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5312#issuecomment-451466938:32,perform,performance,32,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5312#issuecomment-451466938,2,"['optimiz', 'perform']","['optimization', 'performance']"
Performance,"I'm getting a similar error. any solutions?; ```; 17:14:13.170 INFO FuncotateSegments - The following datasources support funcotation on segments:; 17:14:13.171 INFO FuncotateSegments - Gencode 34 CANONICAL; 17:14:13.209 INFO FuncotatorEngine - VCF sequence dictionary detected as B37 in HG19 annotation mode. Performing conversion.; 17:14:13.209 WARN FuncotatorEngine - WARNING: You are using B37 as a reference. Funcotator will convert your variants to GRCh37, and this will be fine in the vast majority of cases. There MAY be some errors (e.g. in the Y chromosome, but possibly in other places as well) due to changes between the two references.; 17:14:13.411 INFO ProgressMeter - Starting traversal; 17:14:13.412 INFO ProgressMeter - Current Locus Elapsed Minutes Features Processed Features/Minute; 17:14:15.391 INFO FuncotateSegments - Shutting down engine; [September 11, 2022 5:14:15 PM GMT] org.broadinstitute.hellbender.tools.funcotator.FuncotateSegments done. Elapsed time: 0.30 minutes.; Runtime.totalMemory()=1752170496; java.lang.IllegalArgumentException: Invalid interval. Contig:chr1 start:917445 end:911649; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:804); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:59); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:35); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.segment.SegmentExonUtils.findInclusiveExonIndex(SegmentExonUtils.java:95); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.segment.SegmentExonUtils.determineSegmentExonPosition(SegmentExonUtils.java:63); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createSegmentFuncotations(GencodeFuncotationFactory.java:2939); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createSegmentFuncotations(GencodeFuncotationFactory.java:2914); at ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6598#issuecomment-1243013314:310,Perform,Performing,310,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6598#issuecomment-1243013314,1,['Perform'],['Performing']
Performance,"I'm getting the same issue on GATK 4.1.9.0 FilterAlignmentArtifacts. This bug has been present for 1 year. Has this been fixed?; Note: There is no work-around because FilterAlignmentArtifacts does not have a --smith-waterman option. Here is my error:; ```; 20:12:42.724 WARN FilterAlignmentArtifacts - . [1m[31m !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: FilterAlignmentArtifacts is an EXPERIMENTAL tool and should not be used for production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!![0m. 20:12:42.725 INFO FilterAlignmentArtifacts - Initializing engine; 20:12:48.403 INFO FeatureManager - Using codec VCFCodec to read file gs://fc-secure-024a1aae-a4f9-4025-aa93-f759f93a8203/50383670-4607-4e59-9bfc-4db970980f0e/Mutect2/773a91ea-25be-4d49-b97c-16527076250c/call-Filter/cacheCopy/TN-20-36-filtered.vcf; 20:12:50.117 INFO FilterAlignmentArtifacts - Done initializing engine; 20:12:51.042 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk/gatk-package-4.1.9.0-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 20:12:51.099 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 20:12:51.100 INFO IntelPairHmm - Available threads: 14; 20:12:51.100 INFO IntelPairHmm - Requested threads: 4; 20:12:51.100 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 20:12:51.100 INFO ProgressMeter - Starting traversal; 20:12:51.100 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 20:20:25.766 INFO ProgressMeter - chr3:104142090 7.6 1000 132.0; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007efc9818177e, pid=24, tid=0x00007f13b3c76700; #; # JRE version: OpenJDK Runtime Environment (8.0_242-b08) (build 1.8.0_242-8u242-b08-0ubuntu3~18.04-b08); # Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 ); # Problematic frame:; # C [libgkl_smithwaterman18",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-781673098:820,cache,cacheCopy,820,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-781673098,2,"['Load', 'cache']","['Loading', 'cacheCopy']"
Performance,"I'm having a similar issue on gatk 4.2.6.1; ```; 17:14:13.170 INFO FuncotateSegments - The following datasources support funcotation on segments:; 17:14:13.171 INFO FuncotateSegments - Gencode 34 CANONICAL; 17:14:13.209 INFO FuncotatorEngine - VCF sequence dictionary detected as B37 in HG19 annotation mode. Performing conversion.; 17:14:13.209 WARN FuncotatorEngine - WARNING: You are using B37 as a reference. Funcotator will convert your variants to GRCh37, and this will be fine in the vast majority of cases. There MAY be some errors (e.g. in the Y chromosome, but possibly in other places as well) due to changes between the two references.; 17:14:13.411 INFO ProgressMeter - Starting traversal; 17:14:13.412 INFO ProgressMeter - Current Locus Elapsed Minutes Features Processed Features/Minute; 17:14:15.391 INFO FuncotateSegments - Shutting down engine; [September 11, 2022 5:14:15 PM GMT] org.broadinstitute.hellbender.tools.funcotator.FuncotateSegments done. Elapsed time: 0.30 minutes.; Runtime.totalMemory()=1752170496; java.lang.IllegalArgumentException: Invalid interval. Contig:chr1 start:917445 end:911649; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:804); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:59); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:35); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.segment.SegmentExonUtils.findInclusiveExonIndex(SegmentExonUtils.java:95); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.segment.SegmentExonUtils.determineSegmentExonPosition(SegmentExonUtils.java:63); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createSegmentFuncotations(GencodeFuncotationFactory.java:2939); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createSegmentFuncotations(GencodeFuncotationFactory.java:2914); at o",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7676#issuecomment-1252518062:309,Perform,Performing,309,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7676#issuecomment-1252518062,1,['Perform'],['Performing']
Performance,"I'm not sure what proportion of users leverage the incremental import functionality...it wasn't available when GenomicsDBImport was first made available, but has been around for ~3 years now. As for workspaces with whole chromosomes -- there is no requirement or performance benefits to using whole chromosomes. As you say, subsetting a chromosome to smaller regions will work and make the import and query parallelizable. (if you remember where the advice about whole chromosomes came from, let us know. That might be something that needs to be updated/clarified). Many small contigs does add overhead to import though and, till recently, multiple contigs couldn't be imported together (i.e., each contig would have it's own folder under the GenomicsDB workspace - which gets inefficient with many small contigs). For WGS, probably the best way to create the GenomicsDBImport interval list is to split based on where there are consecutive N's in the reference genome (maybe using [Picard](https://broadinstitute.github.io/picard/command-line-overview.html#ScatterIntervalsByNs)) and/or regions that you are blacklisting. I think you suggested that some of the blacklisted regions were especially gnarly - maybe ploidy or high alternate allele count? - depending on the frequency of those, we may save a bit on space/memory requirements. That may address your concern about overlap between variants and import intervals. In general, any variant that starts in a specified import interval will show up in a query to that workspace. I'm not sure if the blacklist regions contain any variants that start within but extend beyond the blacklist -- those may not show up if the regions are split up in this way.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1212486548:263,perform,performance,263,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1212486548,1,['perform'],['performance']
Performance,I'm posting an experimental version of this branch for performance purposes. This is no longer in a state where it should be merged.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4878#issuecomment-396645011:55,perform,performance,55,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4878#issuecomment-396645011,1,['perform'],['performance']
Performance,"I'm still not very familiar with the way people have used extensions of; Locatable, or how indexing can be used to boost performance. The stuff I've; worked on is a bit of a corner case, and I didn't write much of the; infrastructure, I've been tacking on features. For now I've just been keeping the gzipped text files from the UCSC; browser. They're tab delimited with two header lines, the first basically; giving info about context of the data (it's genome data for the , hg38) and; the second being a description of the columns (each being of form; tract_name.column_name). There's nothing at all sophisticated about this; format, but it's pretty generalizable and easy to parse (and create). An; example; >; > # hgIntegrator: database=hg38 region=genome Wed Apr 18 11:15:34 2018; >; > #gap.chrom gap.chromStart gap.chromEnd gap.type; >; > chr1 0 10000 telomere; >; > chr1 207666 257666 contig; >; > chr1 297968 347968 contig; >; > chr1 535988 585988 contig; >; > chr1 2702781 2746290 scaffold; >; >; For what it's worth, your description of your approach sounds like a; sensible one to me.; I am concerned about the size of the data and how we'd access it. I've; chosen the tracts I have because they are small enough to jam into; resources. On Tue, May 1, 2018 at 8:06 AM samuelklee <notifications@github.com> wrote:. > @TedBrookings <https://github.com/TedBrookings> which formats are you; > using, in particular?; >; > In the CNV package, I've taken pains to unify how tabular data are; > represented in Java, depending on whether each record is Locatable or; > whether the collection of records can be associated with a sample name or; > sequence dictionary. This allows us to represent records that extend; > Locatable with multidimensional numerical or non-numerical annotations; > along with some metadata (sample name and sequence dictionary) with a; > minimum of boilerplate. There are also base methods for producing interval; > trees, etc.; >; > However, this unification effort was a",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-385683551:121,perform,performance,121,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-385683551,1,['perform'],['performance']
Performance,"I'm worried that we're losing the point of caching these. Do we have any idea how important this is to performance/ how many of the calls fall in the 0-9,999 range?. Would it be better to consider replacing with a threadsafe cache that actually does caching rather than a selection of precomputed values? . We could potentially use one of the [guava caches](https://code.google.com/p/guava-libraries/wiki/CachesExplained) if we actually need large values cached.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1395#issuecomment-166951570:103,perform,performance,103,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1395#issuecomment-166951570,5,"['Cache', 'cache', 'perform']","['CachesExplained', 'cache', 'cached', 'caches', 'performance']"
Performance,"I've also seen UnknownHostException: metadata, which seems like it's probably related. My favorite part about that exception is `This is likely because code is not running on Google Compute Engine.`. ```; java.util.concurrent.CompletionException: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: Failure while waiting for FeatureReader to initialize with exception: org.broadinstitute.hellbender.exceptions.UserException: Failed to create reader from gs://fc-c64a0fcd-fb30-4a7e-bdc6-3c09a9286941/f6aeb0ce-044a-4b36-a5f0-d3fda62252a5/ReblockGVCF/66c24439-cfeb-4b85-bc02-aefe693177b6/call-GenotypeGVCF/NWD197223.vcf.gz; 	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273); 	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280); 	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1592); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: Failure while waiting for FeatureReader to initialize with exception: org.broadinstitute.hellbender.exceptions.UserException: Failed to create reader from gs://fc-c64a0fcd-fb30-4a7e-bdc6-3c09a9286941/f6aeb0ce-044a-4b36-a5f0-d3fda62252a5/ReblockGVCF/66c24439-cfeb-4b85-bc02-aefe693177b6/call-GenotypeGVCF/NWD197223.vcf.gz; 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lambda$getFeatureReadersInParallel$601(GenomicsDBImport.java:605); 	at java.util.LinkedHashMap.forEach(LinkedHashMap.java:684); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getFeatureReadersInParallel(GenomicsDBImport.java:600); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.createSampleT",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-411503423:215,concurren,concurrent,215,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-411503423,5,['concurren'],['concurrent']
Performance,"I've been looking at 2bit performance today, comparing ADAM release version 0.20.0 to release version 0.23.0 and to git HEAD (0.24.0-SNAPSHOT), in various use cases, and do not see any performance differences. @tomwhite `loadReferenceFile` in ADAM only supports local 2bit files, what happens in between the file in `gs://` cloud storage to when you load it via ADAM APIs?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4376#issuecomment-367159861:26,perform,performance,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4376#issuecomment-367159861,4,"['load', 'perform']","['load', 'loadReferenceFile', 'performance']"
Performance,"I've created a third party JAR that depends on GATK4 (4.beta.2). I believe I just hit a similar issue; however, what's odd is that it seems intermittent (though i have n=2 on this tool so far):. [October 2, 2017 6:10:01 AM PDT] com.github.discvrseq.walkers.BackportLiftedVcf done. Elapsed time: 6.27 minutes.; Runtime.totalMemory()=45813334016; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/xerial/snappy/LoadSnappy; 	at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:86); 	at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:52); 	at htsjdk.samtools.util.TempStreamFactory.getSnappyLoader(TempStreamFactory.java:42); 	at htsjdk.samtools.util.TempStreamFactory.wrapTempOutputStream(TempStreamFactory.java:74); 	at htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:223); 	at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:166); 	at com.github.discvrseq.walkers.BackportLiftedVcf.apply(BackportLiftedVcf.java:156); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.lambda$traverse$0(VariantWalkerBase.java:110); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.traverse(VariantWalkerBase.java:108); 	at org.broadinsti",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2300#issuecomment-333579182:422,Load,LoadSnappy,422,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2300#issuecomment-333579182,1,['Load'],['LoadSnappy']
Performance,I've created https://github.com/broadinstitute/gatk/issues/2625 to separately address a possible performance issue introduced by this branch.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2605#issuecomment-297483331:97,perform,performance,97,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2605#issuecomment-297483331,1,['perform'],['performance']
Performance,"I've found at least one optimization that may be invalid if SW parameters are changed (also noted by @yfarjoun https://github.com/broadinstitute/gatk/commit/4ccbaddc069539fa6e3ac2690469c1e0a4b03ccb#r31525284). I'm going to remove this for now, but will leave a TODO that it could be reintroduced if appropriate checks on the SW parameters are performed. Not sure if there are any other similar assumptions lurking...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6863#issuecomment-707870344:24,optimiz,optimization,24,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6863#issuecomment-707870344,2,"['optimiz', 'perform']","['optimization', 'performed']"
Performance,"I've had this issue on MacOS X and was able to install the environment successfully by removing the open-mp and mkl lines from the yaml:; ```; - intel-openmp=2018.0.0; - mkl=2018.0.1; - mkl-service=1.1.2; ``` ; Then you may need to remove the partially installed environment with:; ```; conda remove --name gatk --all; ```; Then you can run:; ```; conda create -n gatk -f ./scripts/gatkcondaenv.yml; ```; Hopefully, the tools will run without openmp and mkl, but I'm sure we are taking a performance hit so we should figure out what is the right channel to point to for these packages. @erniebrau have you ever had these isssues?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4822#issuecomment-393235457:488,perform,performance,488,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4822#issuecomment-393235457,1,['perform'],['performance']
Performance,"I've implemented the Gaussian-kernel binary-segmentation algorithm from this paper: https://hal.inria.fr/hal-01413230/document This method uses a low-rank approximation to the kernel to obtain an approximate segmentation in linear complexity in time and space. In practice, performance is actually quite impressive!. The implementation is relatively straightforward, clocking in at ~100 lines of python. Time complexity is O(log(maximum number of segments) * number of data points) and space complexity is O(number of data points * dimension of the kernel approximation), which makes use for WGS feasible. Segmentation of 10^6 simulated points with 100 segments takes about a minute and tends to recover segments accurately. Compare this with CBS, where segmentation of a WGS sample with ~700k points takes ~10 minutes---and note that these ~700k points are split up amongst ~20 chromosomes to start!. There are a small number of parameters that can affect the segmentation, but we can probably find good defaults in practice. What's also nice is that this method can find changepoints in moments of the distribution other than the mean, which means that it can straightforwardly be used for alternate-allele fraction segmentation. For example, all segments were recovered in the following simulated multimodal data, even though all of the segments have zero mean:. ![baf](https://user-images.githubusercontent.com/11076296/29100464-ad687946-7c79-11e7-99e4-962ab93709b4.png). Replacing the SNP segmentation in ACNV (which performs expensive maximum-likelihood estimation of the allele-fraction model) with this method would give a significant speedup there. Joint segmentation is straightforward and is simply given by addition of the kernels. However, complete data is still required. Given such a fast heuristic, I'm more amenable to augmenting this method with additional heuristics to clean up or improve the segmentation if necessary. We can also use it to initialize our more sophisticated HMM m",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-321121666:274,perform,performance,274,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-321121666,1,['perform'],['performance']
Performance,"I've never used these, but https://github.com/jvm-profiling-tools could potentially be a source for java profilers. From reading a bit about hprof, it seems to add a lot of overhead and has questionable accuracy. About workspaces with lots of contigs/smaller contigs -- the performance issue there is mostly during import. In your experiment above to subset the workspace, did the subsetted workspace return faster for SelectVariants? Or use less memory? I'm a bit surprised if so since your query is restricted to just that single array anyway. Regarding @jjfarrell's suggestion of ReblockGVCFs -- I can't speak to any loss of precision, etc there but I would be curious to see if you could run some of your input GVCFs through that, just to see how much smaller they seem to get.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1211399696:274,perform,performance,274,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1211399696,1,['perform'],['performance']
Performance,"I've opened PR #1004 for this. Would you like to take a look @droazen? I haven't tried this on a cluster, but I guess you can run it side-by-side with JP's optimization.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/994#issuecomment-148692643:156,optimiz,optimization,156,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/994#issuecomment-148692643,1,['optimiz'],['optimization']
Performance,I've performed a release of 4.0.9.0 using the poms generated with https://github.com/broadinstitute/gatk/pull/5224 and a bit of manual work.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5212#issuecomment-424502910:5,perform,performed,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5212#issuecomment-424502910,1,['perform'],['performed']
Performance,"I've tested this version in both GVCF and non-GVCF mode, and with and without -L intervals, and it seems to work reasonably. The output, as far as I can tell, is very close to the GATK3 output, but not an exact match. Looking in IGV I suspect that we might have a few boundary artifacts from the new traversal, but I'll have to do more work to confirm this. I've made an effort in the past few days to get the annotations, filtering, etc., closer to GATK3, and this has helped a lot. I have not tested all of the more exotic HC arguments, however, and it's very likely that a few are not completely functional. . Hooking up the native `PairHmm` resulted in a ~3x speedup and hopefully made us more competitive with GATK3 in performance -- I'll post some benchmarking numbers here later. The `isActive()` machinery was ported, but is not currently used (ie., we consider all sites as active for now). At one point I attempted a standalone tool to do the `isActive()` step, but the overhead of going through the reads twice and writing down a huge sites-level file with the results from the individual loci caused me to abandon that idea. Still, we may want to experiment more with a streamlined version of isActive() in the future if we can't get the performance we want from the current implementation. `ReadWindowWalker` has an entry point to plug in a future `isActive()` check if we choose to add one.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1567#issuecomment-194847622:724,perform,performance,724,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1567#issuecomment-194847622,2,['perform'],['performance']
Performance,"I've tried the latest GATK today. The error message changed. Please help. Thanks. Using GATK jar /omics/chatchawit/gatk2/gatk-package-4.0.4.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /omics/chatchawit/gatk2/gatk-package-4.0.4.0-local.jar Funcotator -R /omics/chatchawit/bundle/hsa38.fasta -V /omics/chatchawit/sm/out/sample21.vcf -O /omics/chatchawit/sm/anno/sample21.vcf --output-file-format VCF --data-sources-path /omics/chatchawit/bundle/dsrc/ --ref-version hg38; 10:24:13.971 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/omics/chatchawit/gatk2/gatk-package-4.0.4.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 10:24:14.182 INFO Funcotator - ------------------------------------------------------------; 10:24:14.183 INFO Funcotator - The Genome Analysis Toolkit (GATK) v4.0.4.0-0.0.2; 10:24:14.183 INFO Funcotator - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:24:14.183 INFO Funcotator - Executing as chatchawit@omics on Linux v3.13.0-133-generic amd64; 10:24:14.184 INFO Funcotator - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_161-b12; 10:24:14.184 INFO Funcotator - Start Date/Time: April 28, 2018 10:24:13 AM ICT; 10:24:14.184 INFO Funcotator - ------------------------------------------------------------; 10:24:14.184 INFO Funcotator - ------------------------------------------------------------; 10:24:14.185 INFO Funcotator - HTSJDK Version: 2.14.3; 10:24:14.185 INFO Funcotator - Picard Version: 2.18.2; 10:24:14.186 INFO Funcotator - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 10:24:14.186 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 10:24:14.186 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 10:24:14.186 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 10:24:14.186 INFO Fu",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385137363:653,Load,Loading,653,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385137363,1,['Load'],['Loading']
Performance,"IBBLE : false; >; > 16:17:05.844 INFO HaplotypeCaller - Deflater: JdkDeflater; >; > 16:17:05.844 INFO HaplotypeCaller - Inflater: JdkInflater; >; > 16:17:05.844 INFO HaplotypeCaller - GCS max retries/reopens: 20; >; > 16:17:05.844 INFO HaplotypeCaller - Requester pays: disabled; >; > 16:17:05.845 INFO HaplotypeCaller - Initializing engine; >; > 16:17:05.928 WARN IntelDeflaterFactory - IntelInflater is not supported, using Java.util.zip.Inflater; >; > 16:17:05.932 WARN IntelDeflaterFactory - IntelInflater is not supported, using Java.util.zip.Inflater; >; > 16:17:06.503 INFO FeatureManager - Using codec VCFCodec to read file file:///home/robert/test/snps.vcf; >; > 16:17:06.539 INFO IntervalArgumentCollection - Processing 61464 bp from intervals; >; > 16:17:06.551 INFO HaplotypeCaller - Done initializing engine; >; > 16:17:06.573 INFO HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output; >; > 16:17:06.588 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; >; > 16:17:06.589 **WARN** NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils347167544598047196.so: /tmp/libgkl_utils347167544598047196.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > 16:17:06.589 **WARN** IntelPairHmm - Intel GKL Utils not loaded; >; > 16:17:06.589 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; >; > 16:17:06.589 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; >; > 16:17:06.590 **WARN** NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils6186849302609329058.so: /tmp/libgkl_utils",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600:4985,Load,Loading,4985,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600,1,['Load'],['Loading']
Performance,"IIRC, the main commit in ADAM that would effect BQSR or HC was https://github.com/bigdatagenomics/adam/commit/1eed8e8e464f8f92a6e87afc1d334e751423e810#diff-abb5cd690409453a589fa8aadbfd7151 which _improved_ the performance of extracting regions from a 2 bit file. I run GATK4 HC pretty regularly with this change and haven't seen performance issues on datasets ranging in size from small targeted datasets up to WGS. Anywho, let me know if there's anything I can do to help debug the perf issue.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4376#issuecomment-366078617:210,perform,performance,210,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4376#issuecomment-366078617,2,['perform'],['performance']
Performance,"IMO I would only port LoglessPairHMM for a first iteration ... base on the premature optimization approach it should be Log10PairHMM as the former is an optimization of the later but I believe it makes a huge difference. . I second @droazen, I think JNI ones should be added later to get the performance back. About the GraphBased approach... I never got it to fly as it could as I was pulled to do other stuff. I think it should not be ported for now anyway.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/543#issuecomment-108655203:85,optimiz,optimization,85,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/543#issuecomment-108655203,3,"['optimiz', 'perform']","['optimization', 'performance']"
Performance,"ITE_FOR_SAMTOOLS : tru; 08:27:10.888 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : fals; 08:27:10.888 INFO Mutect2 - Deflater: IntelDeflate; 08:27:10.889 INFO Mutect2 - Inflater: IntelInflate; 08:27:10.889 INFO Mutect2 - GCS max retries/reopens: 2; 08:27:10.889 INFO Mutect2 - Requester pays: disable; 08:27:10.889 INFO Mutect2 - Initializing engin; 08:27:11.333 INFO Mutect2 - Done initializing engin; 08:27:11.381 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/gatk/gatk-package-4.1.1.0-local.jar!/com/intel/gkl/native/libgkl_utils.s; 08:27:11.383 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk/gatk-package-4.1.1.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.s; 08:27:11.426 INFO **IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHM**; 08:27:11.427 INFO IntelPairHmm - Available threads: 4; 08:27:11.428 INFO IntelPairHmm - Requested threads: 4; 08:27:11.428 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementatio; 08:27:11.432 INFO Mutect2 - Shutting down engin; [April 23, 2019 8:27:11 AM UTC] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.09 minutes.; Runtime.totalMemory()=190840832; java.lang.IllegalArgumentException: samples cannot be empt; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:724); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.ReferenceConfidenceModel.<init>(ReferenceConfidenceModel.java:116); 	at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticReferenceConfidenceModel.<init>(SomaticReferenceConfidenceModel.java:38); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2Engine.<init>(Mutect2Engine.java:149); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2.onTraversalStart(Mutect2.java:286); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:982); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4665#issuecomment-485729136:2937,multi-thread,multi-threaded,2937,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4665#issuecomment-485729136,1,['multi-thread'],['multi-threaded']
Performance,"If @akiezun is going to pick this up, he should start from where I left off. If it gets bumped to Beta, I'd be happy to keep it in my queue.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/591#issuecomment-157571184:134,queue,queue,134,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/591#issuecomment-157571184,1,['queue'],['queue']
Performance,"If I understand correctly, are you saying that the pseudocode for `loadIntervals` in case of `IntervalSetRule.UNION` is:. ```; initialize cumulative interval list to empty list; for (intervalString in intervals) {; parse the next interval; *recalculate* cumulative intervals = union(next interval, cumulative intervals); // as opposed to cumulative intervals.add(next interval); }; ```; which scales quadratically?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3788#issuecomment-341729635:67,load,loadIntervals,67,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3788#issuecomment-341729635,1,['load'],['loadIntervals']
Performance,"If the GKL can't be loaded on a particular platform/architecture, it should fall back to the Java implementations transparently and without error. If it's not doing so, that's a bug.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1985#issuecomment-231369127:20,load,loaded,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1985#issuecomment-231369127,1,['load'],['loaded']
Performance,"If these events were indeed not CNLOH, as we discussed, then I don't think we should merge this. Perhaps we should take a step back and answer definitively whether simply blacklisting common germline regions is enough to replicate/obviate most of the postprocessing. Should be straightforward to run an evaluation with and without blacklisting---and hopefully our truth data accurately reflects whether blacklisting is desirable. If tagging/filtering rare germline is still a concern, then I'd say the next step is to see whether simply changing segmentation parameters to artificially decrease resolution and/or simple length-based filtering suffices. Finally, simple filtering based on CR-AF as described above could be implemented. If the normal is available, we can make IS_NORMAL calls simply based on the overlap of the ModelSegments posteriors (with corresponding qualities). If not, then some heuristic determination of the normal state from the tumor alone as in Marton's caller could be performed. This would combine the IS_NORMAL calling and filtering steps into one simple tool. The output could be a tagged/filtered ModelSegments .seg file and the corresponding VCF.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5450#issuecomment-458551250:997,perform,performed,997,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5450#issuecomment-458551250,1,['perform'],['performed']
Performance,"In GATK3, this check is performed by the `BadCigarFilter`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/373#issuecomment-93023779:24,perform,performed,24,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/373#issuecomment-93023779,1,['perform'],['performed']
Performance,"In a nutshell, `GenotypeLikelihoodsAllelePair GenotypeLikelihoods.getAllelePair(final int PLindex)` returns the cached alleles corresponding to the PLindex. This cache is diploid because the method that initializes it, `GenotypeLikelihoodsAllelePair[] GenotypeLikelihoods.calculatePLcache(final int altAlleles)`, uses a diploid number of likelihoods ( `numLikelihoods(1 + altAlleles, 2)`) and adds an allele pair each PLindex, instead of the ploidy number of allele indices. If the methods are generalized, they should return `ArrayList<int>` instead of `GenotypeLikelihoodsAllelePair`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1382#issuecomment-167845181:112,cache,cached,112,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1382#issuecomment-167845181,2,['cache'],"['cache', 'cached']"
Performance,"In addition, I haven't been very through about how the random number generator is interacting with the multi-threading part, so just to be cautious: https://stackoverflow.com/questions/22313552/contention-in-concurrent-use-of-java-util-random",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5146#issuecomment-419549217:103,multi-thread,multi-threading,103,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5146#issuecomment-419549217,2,"['concurren', 'multi-thread']","['concurrent-use-of-java-util-random', 'multi-threading']"
Performance,"In general our germline tools are designed for short variants. I don't think any of them will handle a millions long indel well or at all. The SV or CNV tools sound like a better fit although I'm not sure exactly if they cover your use case exactly. Typically we process short variants and long variants like this separately. . We should be detecting this variant up front on when loading into genomicsDB if it's going to be problematic to retrieve it, and we should be giving a better error message. I don't think we'll be able to handle it through GenotypeGVCFs in any helpful way though. (The best I can imagine it doing is passing it through ungenotyped.)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7976#issuecomment-1376029770:381,load,loading,381,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7976#issuecomment-1376029770,1,['load'],['loading']
Performance,"In order to get it running though you will need to install the following things on each machine using apt-get: gawk, sysstat, and perf-tools-unstable. Additionally as root, you will have to set the /proc/sys/kernel/perf_event_paranoid variable from 1 to 0. For these tasks it might be possible to automate these steps by updating the system image that is used to setup dataproc clusters. In order to actually run and install PAT, you will need to download it from [here](https://github.com/intel-hadoop/PAT/tree/master/PAT) and add all the machines and ssh ports (including the master) in your cluster to the ""ALL_NODES"" setting in the config.template -> config file. You will also have to setup an SSH key to root on the cluster, which can be done with the command `gcloud compute ssh` and set the ""SSH_KEY"" variable in the config file to point to the google_compute_engine file in roots .ssh directory (public keys should have automatically been distributed to the other nodes). . At this point you need simply input the command line command you wish to run into the ""CMD_PATH"" variable and run ./pat run. I recommend running a spark-submit job using yarn-client as master. NOTE: the output will be a directory containing an excel spreadsheet and a bunch of data for each cluster. You will need to open the spreadsheet on a windows copy of excel and use ""control+q"" to run the macros that load the data.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1986#issuecomment-234947495:1391,load,load,1391,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1986#issuecomment-234947495,1,['load'],['load']
Performance,"In particular, can we improve the performance of `--strict` mode to the point where it could become the default?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5263#issuecomment-460399213:34,perform,performance,34,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5263#issuecomment-460399213,1,['perform'],['performance']
Performance,"In this case, it may make sense to insert an in-memory cache layer between Spark and Isilon for holding those intermediate datasets. For example, if Alluxio (Tachyon) is used, the benefits of locality could still be preserved and the I/O pressure to network as well as to Isilon storage will be largely released. GATK pipeline would achieve memory speed data sharing across different computational stages. The following is one of study cases:; [Making the Impossible Possible with Tachyon: Accelerate Spark Jobs from Hours to Seconds](https://dzone.com/articles/Accelerate-In-Memory-Processing-with-Spark-from-Hours-to-Seconds-With-Tachyon)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1508#issuecomment-188793060:55,cache,cache,55,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1508#issuecomment-188793060,1,['cache'],['cache']
Performance,"Increasing the vcf size would give us a better sense of how the broadcast scales with input size -- I don't think you have enough data points to draw any conclusions there yet. Also, you could increase the memory per executor to allow the larger broadcasts to work -- point is to figure out whether the network/bittorrent protocol is a bottleneck.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1675#issuecomment-210726420:336,bottleneck,bottleneck,336,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1675#issuecomment-210726420,1,['bottleneck'],['bottleneck']
Performance,Integration tests look okay (there's a Python one that says to skip if it's in docker):; - org.broadinstitute.hellbender.utils.python.StreamingPythonExecutorIntegrationTest#testRequirePythonEnvironment; Unit tests:; - org.broadinstitute.hellbender.utils.pairhmm.VectorPairHMMUnitTest#testLikelihoodsFromHaplotypes; - org.broadinstitute.hellbender.utils.io.IOUtilsUnitTest#testUnsuccessfulCanReadFileCheck (intended to be skipped); - fixed ; No variant calling tests ignored; No python tests ignored; No R tests ignored. The PairHMM one returns:; ```; 03:44:48.410 WARN NativeLibraryLoader - Unable to load libgkl_pairhmm_fpga.so from native/libgkl_pairhmm_fpga.so (/tmp/libgkl_pairhmm_fpga7585161099923450811.so: libgkl_pairhmm_shacc.so: cannot open shared object file: No such file or directory); ```; I don't know if that's expected or not -- maybe yes because it's looking for an FPGA?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5339#issuecomment-592086345:601,load,load,601,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5339#issuecomment-592086345,1,['load'],['load']
Performance,"IntelliJ seems to be having a bit of difficulting loading GATK these days:; > build.gradle error (413,0); > Received fatal alert: protocol_version",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2642#issuecomment-298437516:50,load,loading,50,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2642#issuecomment-298437516,1,['load'],['loading']
Performance,"Interesting, it's definitely possible it's coming from one of the other buckets. I don't think we have fine grained control over WHICH bucket we attempt to read requester pays status from, so it's possible if it's enabled it's necessary to have that permission on every bucket. It's annoying that the error message doesn't say which reader is performing the access. Is there a longer stack trace available?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7492#issuecomment-934908586:343,perform,performing,343,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7492#issuecomment-934908586,1,['perform'],['performing']
Performance,"Interesting, that's somewhat disturbing news, I wonder if we're paying for ssd's without actually being able to use them... It's also possible there's a different setting that's configuring the ssd's to be used for shuffle output. . We should investigate this further and 1) see if setting spark.local.dir makes a performance difference 2) ask the dataproc team about this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2426#issuecomment-283418564:314,perform,performance,314,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2426#issuecomment-283418564,1,['perform'],['performance']
Performance,"Interesting. At some point we do expect to see diminishing returns when adding more cores, because the parts that parellelize poorly will start to dominate, but I would hope that it's with more than 16 cores... . One thing we've seen is that performance can be harmed by using to many cores / executor. We've seen problems due to lock contention within spark on executors that have more than ~8 cores. I might try running 4 executors with 8 cores each and seeing if that's an improvement vs 2 executors with 16 cores each. . We're currently heavily re-writing MarkDuplicatesSpark which may be an improvement in the near future.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4516#issuecomment-371856207:242,perform,performance,242,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4516#issuecomment-371856207,1,['perform'],['performance']
Performance,"Interesting. Sorry this is causing so much trouble. From one of your above comments I wasn't clear if the solution using `--conf 'spark.submit.deployMode=cluster'` work correctly or not. . Is it possible that it's correct behavior for it to fail with the linkage error? According to the [mapr doc](https://maprdocs.mapr.com/52/DevelopmentGuide/c-loading-mapr-native-library.html) that command causes it to expect the application to load the library itself, but GATK by default doesn't have a copy of MAPR and won't load it on it's own. Have you included the mapr library somehow into the gatk jar? Or is it provided to spark some other way? I don't really know how maprfs works and how it interacts with hadoop paths.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3933#issuecomment-350315653:346,load,loading-mapr-native-library,346,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3933#issuecomment-350315653,3,['load'],"['load', 'loading-mapr-native-library']"
Performance,Is it a real performance gain?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/312#issuecomment-82519315:13,perform,performance,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/312#issuecomment-82519315,1,['perform'],['performance']
Performance,Is there a way to have java load a config file as system properties on startup?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2316#issuecomment-267124998:28,load,load,28,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2316#issuecomment-267124998,1,['load'],['load']
Performance,"Is your GenomicDB named as your contig ? If so it may produce an error if your working directory is the Genomic DB folder, see my comment here : . https://gatk.broadinstitute.org/hc/en-us/community/posts/26901826831899-GenotypeGVCFs-issue-when-using-include-non-variant-sites-parameter. In addition the option -L needs to be set in in both the GenomicDBimport step and the GenotypeGVCFs step, see below : . `singularity exec --bind /nvme/disk0/lecellier_data:/nvme/disk0/lecellier_data /home/hdenis/gatk_latest.sif gatk --java-options ""-Xmx15g -Xms4g"" GenomicsDBImport --genomicsdb-workspace-path ""${INDIR}GenomicDB/${CONTIG_NAME}"" --batch-size 50 -L $CONTIG --sample-name-map ""${INDIR}aspat_gvcf_clean.sample_map"" --tmp-dir /nvme/disk0/lecellier_data/WGS_GBR_data/tmp --reader-threads 7 --genomicsdb-shared-posixfs-optimizations true --bypass-feature-reader true `. `singularity exec --bind /nvme/disk0/lecellier_data:/nvme/disk0/lecellier_data /home/hdenis/gatk_latest.sif gatk --java-options ""-Xmx58g"" GenotypeGVCFs -R $REF_3 -V ""gendb://${INDIR}GenomicDB/${CONTIG_NAME}"" -O ""${OUTDIR}aspat_clean_${CONTIG_NAME}.vcf.gz"" --tmp-dir /nvme/disk0/lecellier_data/WGS_GBR_data/tmp --include-non-variant-sites true -L $CONTIG --only-output-calls-starting-in-intervals true`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8415#issuecomment-2212079735:816,optimiz,optimizations,816,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8415#issuecomment-2212079735,1,['optimiz'],['optimizations']
Performance,Issue solved after updating to the recent version and using the `--genomicsdb-shared-posixfs-optimizations true` flag.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7007#issuecomment-748076928:93,optimiz,optimizations,93,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7007#issuecomment-748076928,1,['optimiz'],['optimizations']
Performance,Issues that need to be addressed as part of this ticket:; - support for arguments defined in filter classes; - PluginManager -- to port or not to port?; - performance issues with searching the classpath; - separating out the readmetrics code from the code that applies filters,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6#issuecomment-69800773:155,perform,performance,155,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6#issuecomment-69800773,1,['perform'],['performance']
Performance,"It all depends on the network you are running from, as noted in the following discussion:. https://github.com/googlegenomics/utils-java/issues/9#issuecomment-60502722. So the closer you are to the data such as through GCE, and launching a GCE instance from a [relatively similar zone](https://cloud.google.com/compute/docs/regions-zones/regions-zones) such as (`us-east1-b`, `us-east1-c`, `us-east1-d`) the quicker the result. Sometimes the setup time to launch the instance might take some time as well. I don't have a setup as the Broad to run the same test and determine what might be happening, but I just re-ran the following test on an external (non-GCE) cluster and below are the results for a 1.46 GB file, which seem to come closer to @jean-philippe-martin's most recent results (and projected using my throughput, a 34.56 GB file would take about 13 min 38 sec, but not 55 min):. ``` Bash; $ gsutil ls -l gs://pgp-harvard-data-public/hu011C57/GS000018120-DID/GS000015172-ASM/GS01669-DNA_B05/ASM/REF/coverageRefScore-chr1-GS000015172-ASM.tsv.bz2. 1563675749 2014-04-24T20:26:25Z gs://pgp-harvard-data-public/hu011C57/GS000018120-DID/GS000015172-ASM/GS01669-DNA_B05/ASM/REF/coverageRefScore-chr1-GS000015172-ASM.tsv.bz2; TOTAL: 1 objects, 1563675749 bytes (1.46 GiB). $; $ time(gsutil cp -L transfer_statistics.txt gs://pgp-harvard-data-public/hu011C57/GS000018120-DID/GS000015172-ASM/GS01669-DNA_B05/ASM/REF/coverageRefScore-chr1-GS000015172-ASM.tsv.bz2 . ). Copying gs://pgp-harvard-data-public/hu011C57/GS000018120-DID/GS000015172-ASM/GS01669-DNA_B05/ASM/REF/coverageRefScore-chr1-GS000015172-ASM.tsv.bz2...; Downloading ..././coverageRefScore-chr1-GS000015172-ASM.tsv.bz2: 372.81 MiB/372.81 MiB; Downloading ..././coverageRefScore-chr1-GS000015172-ASM.tsv.bz2: 372.81 MiB/372.81 MiB; Downloading ..././coverageRefScore-chr1-GS000015172-ASM.tsv.bz2: 372.81 MiB/372.81 MiB; Downloading ..././coverageRefScore-chr1-GS000015172-ASM.tsv.bz2: 372.81 MiB/372.81 MiB; WARNING: Found no hashes to v",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1755#issuecomment-227913893:812,throughput,throughput,812,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1755#issuecomment-227913893,1,['throughput'],['throughput']
Performance,"It does seem that way. I don't think it's `serialVersionUID`, that usually manifests as failures to deserialize rather than class cast problems. I think it's likely an issue with 2 different class loaders loading that same class (based on discussion in #1386.) Possibly something analogous to https://issues.apache.org/jira/browse/SPARK-1403, but on yarn.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1315#issuecomment-188423698:197,load,loaders,197,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1315#issuecomment-188423698,2,['load'],"['loaders', 'loading']"
Performance,"It doesn't iterate over the overlapping intervals, it just checks whether any overlap at all. I've created a branch that uses `OverlapDetector` here: https://github.com/tomwhite/Hadoop-BAM/tree/intervals-optimization, which you can use to check if the changes address the performance issues you were seeing. . It would be good if `OverlapDetector` could take a `Locatable` so we can pass in `SAMRecord` directly and not create a new `Interval` object. Having an `overlaps()` method that returns a boolean would be nice too. I'm not sure how unmapped reads that have a coordinate set are handled. They may need a special case, as in https://github.com/HadoopGenomics/Hadoop-BAM/compare/master...tomwhite:intervals-optimization#diff-60ad8bad94dec0448bd32dcfebfdd6f4L234.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1532#issuecomment-209888970:204,optimiz,optimization,204,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1532#issuecomment-209888970,3,"['optimiz', 'perform']","['optimization', 'performance']"
Performance,"It got slow after the big rebase before the branch was code reviewed, I believe. Some dependent class may have been changed in a way that adversely affected the optimized BQSR during the window of time between rebases.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1006#issuecomment-148840243:161,optimiz,optimized,161,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1006#issuecomment-148840243,1,['optimiz'],['optimized']
Performance,"It is not designed to enable you to do that. It uses a scatter-gather approach for parallelism. You'll want to submit multiple tasks that each process a disjoint interval of the genome. Personally, I have wondered how Mutect2 performs for variants at the borders of interval boundaries.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7688#issuecomment-1046371449:226,perform,performs,226,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7688#issuecomment-1046371449,1,['perform'],['performs']
Performance,"It just seems a bit scary to be mucking about with start/end indexes over multiple pages when we don't have to be. I suspect we may be potentially losing out on some optimizations if we're creating new requests for each page instead of following up on the original one, although that's just speculation.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/869#issuecomment-135861754:166,optimiz,optimizations,166,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/869#issuecomment-135861754,1,['optimiz'],['optimizations']
Performance,"It looks like Hadoop-BAM uses Hadoop's TotalOrderPartitioner to do the sort in parallel (i.e. using more than one reducer) in such a way as to have a total ordering, so that reads in partition _i_ are all less than those in _i+1_. Then the output files are concatenated. This will need some alterations to get it working in Spark, since TotalOrderPartitioner relies on the distributed cache (which Spark doesn't have). I can take a look at this if you like, @akiezun.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1094#issuecomment-157406851:385,cache,cache,385,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1094#issuecomment-157406851,1,['cache'],['cache']
Performance,"It looks like at one point GATK 3.8 was available via the [homebrew science tap](https://github.com/ilovezfs/homebrew-science/blob/master/gatk.rb). I've tried adding their formula to my homebrew formula folder and installing via ``` brew install gatk.rb``` but there's a ton of errors. ```Updating Homebrew...; ==> Downloading https://github.com/broadgsa/gatk-protected/archive/3.8-1.tar.gz; Already downloaded: /Users/timothystiles/Library/Caches/Homebrew/gatk-3.8-1.tar.gz; ==> mvn package -Dmaven.repo.local=${PWD}/repo; Last 15 lines from /Users/timothystiles/Library/Logs/Homebrew/gatk/01.mvn:; [INFO] Scanning for projects...; [ERROR] [ERROR] Some problems were encountered while processing the POMs:; [FATAL] Non-parseable POM /private/tmp/gatk-20180118-71498-skz9cg/gatk-protected-3.8-1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /private/tmp/gatk-20180118-71498-skz9cg/gatk-protected-3.8-1/public/gatk-root/pom.xml, line 15, column 3; @; [ERROR] The build could not read 1 project -> [Help 1]; [ERROR]; [ERROR] The project org.broadinstitute.gatk:gatk-aggregator:[unknown-version] (/private/tmp/gatk-20180118-71498-skz9cg/gatk-protected-3.8-1/pom.xml) has 1 error; [ERROR] Non-parseable POM /private/tmp/gatk-20180118-71498-skz9cg/gatk-protected-3.8-1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /private/tmp/gatk-20180118-71498-skz9cg/gatk-protected-3.8-1/public/gatk-root/pom.xml, line 15, column 3 -> [Help 2]; [ERROR]; [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.; [ERROR] Re-run Maven using the -X switch to enable full debug logging.; [ERROR]; [ERROR] For more information about the errors and possible solutions, please read the following articles:; [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/ProjectBuildingException; [ERROR] [Help 2] http://cwiki.apache.org/confluence/display",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4164#issuecomment-358697586:441,Cache,Caches,441,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4164#issuecomment-358697586,1,['Cache'],['Caches']
Performance,It looks like it made the tests substantially slower.... I'm not totally clear on why. Maybe because it has to re-optimize code every time it restarts the jvm.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6093#issuecomment-521372663:114,optimiz,optimize,114,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6093#issuecomment-521372663,1,['optimiz'],['optimize']
Performance,"It looks like the errors are all of the flavor:. ```; Unable to load Maven meta-data from https://artifactory.broadinstitute.org/artifactory/libs-snapshot/com/github/samtools/htsjdk/2.9.1-34-gd7bae17-SNAPSHOT/maven-metadata.xml.; > Could not GET 'https://artifactory.broadinstitute.org/artifactory/libs-snapshot/com/github/samtools/htsjdk/2.9.1-34-gd7bae17-SNAPSHOT/maven-metadata.xml'. ; Received status code 401 from server: Unauthorized; ```; Perhaps Maven's temporarily in a bad mood, we'll have to try again later.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2879#issuecomment-306264180:64,load,load,64,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2879#issuecomment-306264180,1,['load'],['load']
Performance,"It looks like the tests are running very slowly because of a performance regression due to the changes in `IntegrationTestSpec.assertEqualTextFiles`. You can see this on tests that should be unaffected by the core changes in this PR, like the VQSR integration tests, which have no cloud/bucket dependency and usually take about [1 minute](https://storage.googleapis.com/hellbender-test-logs/build_reports/master_24775.2/tests/test/classes/org.broadinstitute.hellbender.tools.walkers.vqsr.VariantRecalibratorIntegrationTest.html), but took [25 minutes](https://storage.googleapis.com/hellbender-test-logs/build_reports/master_24563.2/tests/test/classes/org.broadinstitute.hellbender.tools.walkers.vqsr.VariantRecalibratorIntegrationTest.html) with this branch. The same thing happens when the VQSR tests are run locally with this branch; all the time is spent in `assertEqualTextFiles`. @jean-philippe-martin I don't want to push to this branch without your ok, but reverting the first few lines of `assertEqualTextFiles` seems to fix the problem locally. (Separately, I will do some profiling to figure out for posterity sake why that change had such a dramatic affect ).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5378#issuecomment-462772143:61,perform,performance,61,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5378#issuecomment-462772143,1,['perform'],['performance']
Performance,It looks like we need to update mockito. ; https://storage.googleapis.com/hellbender-test-logs/build_reports/master_27538.13/tests/test/index.html. ```. java.lang.IllegalArgumentException: Unknown Java version: 11; 	at net.bytebuddy.ClassFileVersion.ofJavaVersion(ClassFileVersion.java:135); 	at net.bytebuddy.ClassFileVersion$VersionLocator$ForJava9CapableVm.locate(ClassFileVersion.java:357); 	at net.bytebuddy.ClassFileVersion.ofThisVm(ClassFileVersion.java:147); 	at net.bytebuddy.dynamic.loading.ClassInjector$UsingReflection$Dispatcher$CreationAction.run(ClassInjector.java:301); 	at net.bytebuddy.dynamic.loading.ClassInjector$UsingReflection$Dispatcher$CreationAction.run(ClassInjector.java:290); 	at java.base/java.security.AccessController.doPrivileged(Native Method); 	at net.bytebuddy.dynamic.loading.ClassInjector$UsingReflection.<clinit>(ClassInjector.java:70); 	at net.bytebuddy.dynamic.loading.ClassLoadingStrategy$Default$InjectionDispatcher.load(ClassLoadingStrategy.java:184); 	at net.bytebuddy.dynamic.TypeResolutionStrategy$Passive.initialize(TypeResolutionStrategy.java:79); 	at net.bytebuddy.dynamic.DynamicType$Default$Unloaded.load(DynamicType.java:4456); 	at org.mockito.internal.creation.bytebuddy.SubclassBytecodeGenerator.mockClass(SubclassBytecodeGenerator.java:115); 	at org.mockito.internal.creation.bytebuddy.TypeCachingBytecodeGenerator$1.call(TypeCachingBytecodeGenerator.java:37); 	at org.mockito.internal.creation.bytebuddy.TypeCachingBytecodeGenerator$1.call(TypeCachingBytecodeGenerator.java:34); 	at net.bytebuddy.TypeCache.findOrInsert(TypeCache.java:138); 	at net.bytebuddy.TypeCache$WithInlineExpunction.findOrInsert(TypeCache.java:346); 	at net.bytebuddy.TypeCache.findOrInsert(TypeCache.java:161); 	at net.bytebuddy.TypeCache$WithInlineExpunction.findOrInsert(TypeCache.java:355); 	at org.mockito.internal.creation.bytebuddy.TypeCachingBytecodeGenerator.mockClass(TypeCachingBytecodeGenerator.java:32); 	at org.mockito.internal.creation.bytebuddy.SubclassB,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6119#issuecomment-532377836:493,load,loading,493,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6119#issuecomment-532377836,5,['load'],"['load', 'loading']"
Performance,"It seems like the problem was that the researcher was starting with a coordinate-sorted bam, whereas `MarkDuplicatesSpark` requires a name-sorted bam for good performance. @jamesemery feel free to close once you're satisfied that this is resolved, and once we've made whatever additional documentation clarifications are warranted.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5670#issuecomment-463756512:159,perform,performance,159,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5670#issuecomment-463756512,1,['perform'],['performance']
Performance,"It seems plausible to me, though, that the Google auth library may have been patched to perform checks that it wasn't performing previously. Maybe our project permissions have always been mis-configured :)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3591#issuecomment-330940762:88,perform,perform,88,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3591#issuecomment-330940762,2,['perform'],"['perform', 'performing']"
Performance,"It was a feature which we would have loved, but alas this isn't the case. We also relied on ```-ct``` to get percent of bases depending on their coverage (ex. 20x) which has now been dropped in GATK 4+ versions. We came across another tool called [mosdepth](https://github.com/brentp/mosdepth). When compared to DepthOfCoverage - ; - It uses multithreading (albeit only for deflation, so no performance gains when going beyond 4 threads).; - It gives coverage for exome within 5 minutes, and even faster when we don't need the per base coverage output.; - Per base coverage output can be skipped using ```-x``` the output of this matches closely to output from DepthOfCoverage. Do keep in mind, DepthOfCoverage also supports this skipping when using the parameter ```--omitDepthOutputAtEachBase``` which saves massively on I/O and cuts processing time from 50 minutes per sample to 40 minutes per sample. If you do decide to give it a try, we have some tips and suggestions - ; - The tool generates multiple output files. If looking for total coverage, check the last line of file ```output.mosdepth.summary.txt```; - If looking for percent of bases covered at target read depth, this information is present in file ```output.mosdepth.region.dist.txt```. If your target read depth is 20x, you can search this file with ```grep -P ""total\t20\t""``` and the third column should be the percentage (with only one decimal); - By using ```-d4``` switch, they claim the above percentage granularity increases to 4 decimal points.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7890#issuecomment-1153478071:391,perform,performance,391,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7890#issuecomment-1153478071,1,['perform'],['performance']
Performance,It was cursed with a bad cache.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5120#issuecomment-423018400:25,cache,cache,25,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5120#issuecomment-423018400,1,['cache'],['cache']
Performance,"It won't be able to run any faster than BWA mem does with a similar number of cores, since it is essentially just running bwamem. It's potentially faster as part of a spark pipeline so you can load and process data once instead of saving the data to disk and reloading it repeatedly. . The complete list of spark configuration parameters is available on the [spark docs](https://spark.apache.org/docs/3.5.0/configuration.html). Many of them are not relevant in local mode. From what I understand the local mode is going to execute as a single executor with the number of cores specified in the `local[#]` block ( or the total number of system threads if it's set to `*`) It will use the available memory that java is configured with. I'm pretty sure it's ignoring the memory and configuration parameters you've set. Those will be relevant if you configure a stand alone spark cluster (potentially one running exclusively on your local machine). . Our spark tools are not being actively developed for the most part. We've moved away from them to use single threaded tools widely sharded and managed by cromwell. The additional complexity of the spark environment made it hard to see much benefit when most of the tools are embarassingly parallel and easily shardable.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8897#issuecomment-2214866066:193,load,load,193,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8897#issuecomment-2214866066,1,['load'],['load']
Performance,It would help performance if we used constants instead of arrays in PairHMM. I'll investigate.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2036#issuecomment-288540961:14,perform,performance,14,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2036#issuecomment-288540961,1,['perform'],['performance']
Performance,"It's not a surprise that a local disk is faster than a remote one, but the; magnitude of the difference is a lot more than I would expect. I remember; at the time using direct GCS access to get the best possible performance in; the bit I was working on, but I don't remember exactly how much of a; difference it made. From my desktop it takes 2m25s to download the whole file, so the ~6min; difference seems really excessive, something is broken. One thing to look; into is whether the sharding is working correctly (are we getting the; correct number of parallel downloads?). Presumably this code is using the HDFS adapter. It'll be interesting to; compare vs the NIO version (and then the optimized NIO version once I write; it).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1755#issuecomment-213094600:212,perform,performance,212,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1755#issuecomment-213094600,2,"['optimiz', 'perform']","['optimized', 'performance']"
Performance,It's not likely that the reference loading issue will be fixed this month @huangk3. I'm answering in @lbergelson stead as he's out for a few weeks. This is on the team's radar so there is some chance that it could be but we are unable to say for sure.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2171#issuecomment-251757030:35,load,loading,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2171#issuecomment-251757030,1,['load'],['loading']
Performance,"It's on my list. Pretty near the bottom, but it's there. Runtime is probably getting up near an hour for big jobs. The memory requirements are horrific, because we load all the variants into memory and then we don't even use them all! For the biggest cohorts we use 104GB. I wish I was joking. If sklearn can minibatch GMMs then that would be amazing. We use a maximum of 2.5M variants for training and number of annotations/dimensions is O(10). The smallest exome cohort would train with about 80,000 variants with about 3GB of memory. That being said this definitely isn't the biggest cost contributor for joint calling, and hopefully all the sporadic failures have been hammered out.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6425#issuecomment-594061198:164,load,load,164,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6425#issuecomment-594061198,1,['load'],['load']
Performance,"It's possible that GenomicsDB was also optimized for diploids and thusly slower for other ploidies because a lot of the code was ported from GATK. I think `new-qual` is more likely to help, but you can certainly try CombineGVCFs. I don't have a lot of good benchmarks, honestly, but maybe @lbergelson does. GATK3 GenotypeGVCFs for 20,000 human samples took 113.54 hours for about 1.9Mbp, but that was on a CombinedGVCF already extracted from GenomicsDB. With that many samples there are a lot of multi-allelics so that part should be similar to your data. The java options were `java -XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10 -Xmx11500m`, so still less memory than your task seems to need. And without `new-qual`. It also seems to be much faster than your run, but it's hard to say how runtime should scale with the number of samples.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4467#issuecomment-370477452:39,optimiz,optimized,39,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4467#issuecomment-370477452,1,['optimiz'],['optimized']
Performance,"It's possible to override on the command line. . I tested it on our cluster and tools seemed to run fine. Removing the ""spark.driver.userClassPathFirst=true` causes problems on our cluster but the executor doesn't seem to make any difference. I'm worried we'll run into a problem where we need this to be both true and false to avoid 2 different simultaneous errors. My understanding is that the errors are happening because one class is being loaded by 2 different class loaders, but I don't understand how to fix that directly.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1518#issuecomment-190795541:444,load,loaded,444,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1518#issuecomment-190795541,2,['load'],"['loaded', 'loaders']"
Performance,It's some sort of race condition; ```; org.apache.spark.SparkException: Job aborted.; 	at org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:100); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1083); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1081); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1081); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:385); 	at org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopDataset(PairRDDFunctions.scala:1081); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply$mcV$sp(PairRDDFunctions.scala:1000); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply(PairRDDFunctions.scala:991); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply(PairRDDFunctions.scala:991); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:385); 	at org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:991); 	at org.apache.spark.api.java.JavaPairRDD.saveAsNewAPIHadoopFile(JavaPairRDD.scala:823); 	at org.disq_bio.disq.impl.formats.vcf.VcfSink.save(VcfSink.java:80); 	at org.disq_bio.disq.HtsjdkVariantsRddStorage.write(HtsjdkVariantsRddStorage.java:156); 	at org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSink.writeVariantsSingle(VariantsSparkSink.java:134); 	at org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSink.writeVariants(VariantsSparkSink.java:110); 	a,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429:18,race condition,race condition,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429,1,['race condition'],['race condition']
Performance,"It's useful to have a baseline during performance benchmarking to answer the question ""are we better than a naive shuffle?"".",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1203#issuecomment-288522230:38,perform,performance,38,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1203#issuecomment-288522230,1,['perform'],['performance']
Performance,"Its the same optimizations for level as we discussed before, no new optimizations. The previous merge did not include all the optimizations (error on my part)... hence I have to do a new PR.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4379#issuecomment-364265035:13,optimiz,optimizations,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4379#issuecomment-364265035,3,['optimiz'],['optimizations']
Performance,Ivc2NhbGFibGUvbW9kZWxpbmcvVmFyaWFudEFubm90YXRpb25zTW9kZWxCYWNrZW5kLmphdmE=) | `100.000% <ø> (ø)` | |; | [...sr/scalable/modeling/VariantAnnotationsScorer.java](https://codecov.io/gh/broadinstitute/gatk/pull/8132?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvbW9kZWxpbmcvVmFyaWFudEFubm90YXRpb25zU2NvcmVyLmphdmE=) | `64.706% <ø> (-13.072%)` | :arrow_down: |; | [...able/ExtractVariantAnnotationsIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/8132?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvRXh0cmFjdFZhcmlhbnRBbm5vdGF0aW9uc0ludGVncmF0aW9uVGVzdC5qYXZh) | `98.214% <ø> (+1.548%)` | :arrow_up: |; | [...rs/vqsr/scalable/TrainVariantAnnotationsModel.java](https://codecov.io/gh/broadinstitute/gatk/pull/8132?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvVHJhaW5WYXJpYW50QW5ub3RhdGlvbnNNb2RlbC5qYXZh) | `73.529% <45.161%> (-4.248%)` | :arrow_down: |; | [...stering/BayesianGaussianMixtureModelPosterior.java](https://codecov.io/gh/broadinstitute/gatk/pull/8132?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9jbHVzdGVyaW5nL0JheWVzaWFuR2F1c3NpYW5NaXh0dXJlTW9kZWxQb3N0ZXJpb3IuamF2YQ==) | `58.333% <58.333%> (ø)` | |; | [...walkers/vqsr/scalable/ScoreVariantAnnotations.java](https://codecov.io/gh/broadinstitute/gatk/pull/8132?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_cam,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8132#issuecomment-1370265333:3585,scalab,scalable,3585,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8132#issuecomment-1370265333,1,['scalab'],['scalable']
Performance,Jar on Maven central updated - please clear any cached jars,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-295584988:48,cache,cached,48,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-295584988,1,['cache'],['cached']
Performance,"Java implementation of segmentation is now in the sl_wgs_segmentation dev branch, with a few simple unit tests. I'll expand on these and add tests for denoising in the future, but for now we have a working revised pipeline up through segmentation. The CLI is simply named ModelSegments (since my thinking is that it could eventually replace ACNV). I ran it on some old denoised exomes. Runtime is <10s, comparable to CBS. Here's a particularly noisy exome:. CBS found 1398 segments:; ![cbs](https://user-images.githubusercontent.com/11076296/30165095-cdf6251a-93ac-11e7-91fb-dcc8f48fe07f.png). Kernel segmentation with a penalty given by a = 1, b = 0 found 1018 segments:; ![kern](https://user-images.githubusercontent.com/11076296/30165106-dbbe0b40-93ac-11e7-99ec-5d58d8417d8b.png). Kernel segmentation with a penalty given by a = b = 1 (which is probably a reasonable default penalty, at least based on asymptotic theoretical arguments) reduced this to 270 segments :; ![kern-smooth](https://user-images.githubusercontent.com/11076296/30165113-e2b545a8-93ac-11e7-97a9-a692e43ebbdf.png). The number of segments can similarly be controlled in WGS. WGS runtime is ~7min for 250bp bins, ~30s of which is TSV reading, and there is one more spot in my implementation that could stand a bit of optimization, which might bring the runtime down. In contrast, I kicked off CBS 45 minutes ago, and it's still running... @LeeTL1220 this is probably ready to hand off to you for some WDL writing and preliminary evaluation. ; Although I can't guarantee that there aren't bugs, I ran about ~80 exomes with no problem. We can talk later today.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-327797936:1289,optimiz,optimization,1289,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-327797936,1,['optimiz'],['optimization']
Performance,"Just a quick test of `--splitMultiallelics` looks good:; ```; WMCF9-CB5:shlee$ ./gatk LeftAlignAndTrimVariants -R ~/Documents/ref/hg38/Homo_sapiens_assembly38.fasta -V ~/Downloads/zeta_snippet_shlee/zeta_snippet.vcf.gz --maxIndelSize 250 -O zeta_snippet_leftalign_maxindelsize250_splitmultiallelics.vcf.gz --splitMultiallelics; Using GATK wrapper script /Users/shlee/Documents/branches/hellbender/build/install/gatk/bin/gatk; Running:; /Users/shlee/Documents/branches/hellbender/build/install/gatk/bin/gatk LeftAlignAndTrimVariants -R /Users/shlee/Documents/ref/hg38/Homo_sapiens_assembly38.fasta -V /Users/shlee/Downloads/zeta_snippet_shlee/zeta_snippet.vcf.gz --maxIndelSize 250 -O zeta_snippet_leftalign_maxindelsize250_splitmultiallelics.vcf.gz --splitMultiallelics; 17:52:19.004 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/shlee/Documents/branches/hellbender/build/install/gatk/lib/gkl-0.8.5.jar!/com/intel/gkl/native/libgkl_compression.dylib; Sep 05, 2018 5:52:19 PM shaded.cloud_nio.com.google.auth.oauth2.DefaultCredentialsProvider warnAboutProblematicCredentials; WARNING: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a ""quota exceeded"" or ""API not enabled"" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/.; 17:52:19.130 INFO LeftAlignAndTrimVariants - ------------------------------------------------------------; 17:52:19.131 INFO LeftAlignAndTrimVariants - The Genome Analysis Toolkit (GATK) v4.0.8.1-24-gb43bc27-SNAPSHOT; 17:52:19.131 INFO LeftAlignAndTrimVariants - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:52:19.131 INFO LeftAlignAndTrimVariants - Executing as shlee@WMCF9-CB5 on Mac OS X v10.13.6 x86_64; 17:52:19.131 INFO LeftAlignAndTrimVariants ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3487#issuecomment-418893971:811,Load,Loading,811,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3487#issuecomment-418893971,1,['Load'],['Loading']
Performance,Just adding a note here that `FeatureCache` should eventually be refactored to use the simplified Interval class (when it exists) to track cache boundaries and compute overlap.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/100#issuecomment-76229630:139,cache,cache,139,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/100#issuecomment-76229630,1,['cache'],['cache']
Performance,"Just for future reference, note that comments in `testVariantRecalibratorSNPMaxAttempts` are also incorrect or out of date. The test passes even if you limit it to one attempt. ```; // For this test, we deliberately *DON'T* sample a single random int as above; this causes; // the tool to require 4 attempts to acquire enough negative training data to succeed; ```. So again, the tests were already ""broken."" But still, rather than attempt to fix them, I think it's best to follow the principle of not changing both production and test code to the extent that it is possible in this scenario. We've already updated enough exact-match expected results to make me a bit uncomfortable!. Someone else may want to tackle fixing the tests in a separate push, but I think it makes sense for me to focus on avoiding these sorts of issues when writing tests for the new tools. EDIT: For the record, I confirmed that the undesired behavior in this test that the RNG hack was trying to avoid was fixed (and hence, the test was ""broken"") in #6425. Probably wasn't noticed because this is the only non-exact-match test and the test isn't strict enough to check that attempts 1-3 fail, it only checks that we succeed by attempt 4. Again, someone else may feel free to examine the actual coverage of this test and whether it's safe to remove it and/or clean up all the duct tape---but at some point, it becomes difficult to tell which pieces of duct tape are load bearing!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7709#issuecomment-1064236628:1444,load,load,1444,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7709#issuecomment-1064236628,1,['load'],['load']
Performance,"Just tested this locally outside of the Docker. I do not see the WARN:; ```; WMCF9-CB5:shlee$ gatk40110 LearnReadOrientationModel -alt-table 13_tumor-alt.tsv -ref-hist 13_tumor-ref.metrics -alt-hist 13_tumor-alt-depth1.metrics -O 13_tumor-artifact-prior-table.tsv ; Using GATK jar /Applications/genomicstools/gatk/gatk-4.0.11.0/gatk-package-4.0.11.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /Applications/genomicstools/gatk/gatk-4.0.11.0/gatk-package-4.0.11.0-local.jar LearnReadOrientationModel -alt-table 13_tumor-alt.tsv -ref-hist 13_tumor-ref.metrics -alt-hist 13_tumor-alt-depth1.metrics -O 13_tumor-artifact-prior-table.tsv; 12:16:19.960 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Applications/genomicstools/gatk/gatk-4.0.11.0/gatk-package-4.0.11.0-local.jar!/com/intel/gkl/native/libgkl_compression.dylib; Nov 26, 2018 12:16:20 PM shaded.cloud_nio.com.google.auth.oauth2.DefaultCredentialsProvider warnAboutProblematicCredentials; WARNING: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a ""quota exceeded"" or ""API not enabled"" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/.; 12:16:20.176 INFO LearnReadOrientationModel - ------------------------------------------------------------; 12:16:20.177 INFO LearnReadOrientationModel - The Genome Analysis Toolkit (GATK) v4.0.11.0; 12:16:20.177 INFO LearnReadOrientationModel - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:16:20.177 INFO LearnReadOrientationModel - Executing as shlee@WMCF9-CB5 on Mac OS X v10.13.6 x86_64; 12:16:20.177 INFO LearnReadOrientationModel - Java run",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-441721615:816,Load,Loading,816,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-441721615,1,['Load'],['Loading']
Performance,"Just to add - GenomicsDB relies on the filesystem for integrity checks just like any other file on the filesystem. We could add integrity checks at a micro chunk-level, but that would be at the expense of performance.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6688#issuecomment-713055866:205,perform,performance,205,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6688#issuecomment-713055866,1,['perform'],['performance']
Performance,"Just to chime in with some possibly-not-relevant experience, I had pretty good luck in my past life distributing references (the BWA index, in my case) with Hadoop's distributed cache mechanism (which I guess is similar to broadcasting in Spark?). It would sort of saturate the network, though, when a big job was starting up, and I guess the characteristics of the network you're running on could make a difference.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/567#issuecomment-112857196:178,cache,cache,178,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/567#issuecomment-112857196,1,['cache'],['cache']
Performance,"Just wanted to add - I just tried installing the GATK docker as described here: https://gatk.broadinstitute.org/hc/en-us/articles/360035889991--How-to-Run-GATK-in-a-Docker-container. As I'd think that all software dependencies and whatnot should be fine. However, I still get the same error message:. /gatk/./gatk --java-options ""-Xmx25g"" SplitNCigarReads \; > -R Homo_sapiens.GRCh38.dna.primary_assembly.fa -I subset_TINY_rehead.bam \; > --tmp-dir /gatk/my_data/temp -O thing.bam; Using GATK jar /gatk/gatk-package-4.1.3.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx25g -jar /gatk/gatk-package-4.1.3.0-local.jar SplitNCigarReads -R Homo_sapiens.GRCh38.dna.primary_assembly.fa -I subset_TINY_rehead.bam --tmp-dir /gatk/my_data/temp -O thing.bam. 21:12:14.158 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.3.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Mar 02, 2023 9:12:16 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 21:12:16.383 INFO SplitNCigarReads - ------------------------------------------------------------; 21:12:16.384 INFO SplitNCigarReads - The Genome Analysis Toolkit (GATK) v4.1.3.0; 21:12:16.384 INFO SplitNCigarReads - For support and documentation go to https://software.broadinstitute.org/gatk/; 21:12:16.384 INFO SplitNCigarReads - Executing as root@9d399eec0e24 on Linux v5.19.0-32-generic amd64; 21:12:16.384 INFO SplitNCigarReads - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_191-8u191-b12-0ubuntu0.16.04.1-b12; 21:12:16.384 INFO SplitNCigarReads - Start Date/Time: March 2, 2023 9:12:14 PM UTC; 21:12:16.385 INFO SplitNCigarReads - ------------------------------------------------------------; 21:12:16.385 INFO SplitNCigarReads - ----------------------",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452564826:928,Load,Loading,928,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452564826,1,['Load'],['Loading']
Performance,KeyReadsSpark.java:43); at org.broadinstitute.hellbender.tools.spark.pipelines.KeyReadsSpark.lambda$runTool$72eaf22$1(KeyReadsSpark.java:28); at org.broadinstitute.hellbender.tools.spark.pipelines.KeyReadsSpark$$Lambda$11/1228804001.call(Unknown Source); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1002); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1002); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:219); at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41); at org.apache.spark.scheduler.Task.run(Task.scala:64); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```. This start offset is 2176858951 (142662628213169L>>>16) - i.e. around 2GB in. I've managed to reproduce with a local program now. This reveals the following problem:. ```; Caused by: java.lang.IllegalArgumentException: Unrecognized CigarOperator: 11; at htsjdk.samtools.CigarOperator.binaryToEnum(CigarOperator.java:143); at htsjdk.samtools.BinaryCigarCodec.binaryCigarToCigarElement(BinaryCigarCodec.java:87); at htsjdk.samtools.BinaryCigarCodec.decode(BinaryCigarCodec.java:63); at htsjdk.samtools.BAMRecord.getCigar(BAMRecord.java:243); at htsjdk.samtools.SAMRecord.getUnclippedStart(SAMRecord.java:482); at org.seqdoop.hadoop_bam.TestBAMInputFormat.getUnclippedStart(TestBAMInputFormat.java:130); at org.seqdoop.hadoop_bam.TestBAMInputFormat.getStrandedUnclippedStart(TestBAMInputFormat.java:122); at,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1098#issuecomment-156150350:1580,concurren,concurrent,1580,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1098#issuecomment-156150350,1,['concurren'],['concurrent']
Performance,LGTM!. Have any numbers on the performance improvement? It wasn't easy to see an old run before Jasix that would be a good 1-to-1 comparison.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8133#issuecomment-1355651843:31,perform,performance,31,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8133#issuecomment-1355651843,1,['perform'],['performance']
Performance,Latest results show that GCS buckets perform even better than fine. Closing this issue.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1755#issuecomment-282843205:37,perform,perform,37,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1755#issuecomment-282843205,1,['perform'],['perform']
Performance,"Let us take an example. Suppose, we configure GenomicsDB with 3 column partitions - 0-10, 10-100, 100-300 and want to run GenomicsDBImport tool with an interval [0,100]. In this case, the import tool will only write contigs between 0 and 100 into first two partitions (according to the loader JSON file). Is this what you had in mind? The command line will look like:; $ gatk-launch GenomicsDBImport -L 0-100 --loaderJSONfile loader.json --streamIdJSONFile stream.json. This can definitely be done. However, the client still needs to know the column partitions.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-277372931:286,load,loader,286,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-277372931,3,['load'],"['loader', 'loaderJSONfile']"
Performance,"Let's discuss further before you get too far along. The design of the Collections code was intended to ensure that very strict file formats are adhered to within the CNV pipeline. Making it more flexible to accommodate TSVs with arbitrary column headers, relax requirements for sequence dictionaries, etc. undermines that goal. There are also two other issues to consider:. 1) It looks like @jonn-smith has also been putting considerable effort into building a TSV framework for Funcotator. Perhaps CombineSegmentBreakpoints should consider using that framework instead, if it is more appropriate. We can also discuss bringing the CNV pipeline over into that framework, but this should definitely wait until after release. The end goal is for CNV team to spend as little time as possible writing or maintaining any code related to TSV parsing. 2) @mbabadi has put together some python evaluation code for the new gCNV, which makes use of the IntervalTree python package and PyVCF to accomplish some things that are very similar to what CombineSegmentBreakpoints is doing. Perhaps we could implement a similar approach purely in Java by making use of the IntervalTree implementation in htsjdk. I think for now we should treat CombineSegmentBreakpoints as a one-off tool to be used for internal validations. After release, we should design a more generic evaluation tool. This tool could take as input multiple collections of annotated locatables, with a few rigidly defined formats allowed (e.g., VCF, CNV Collection TSVs, perhaps some TSVs from other tools, etc.), with one designated as ground truth. The regions for evaluation could also be specified via -L (since it is possible this might not completely specified by the ground-truth collection). The appropriate intersections and lookups could then be performed.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3995#issuecomment-352860616:1807,perform,performed,1807,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3995#issuecomment-352860616,1,['perform'],['performed']
Performance,"Let's talk about groupReadPairs, pairedReads, and unpairedReads. The groupBy method called by groupReadPairs is very expensive both in time and in memory. It's a full hash shuffle of GATKReads (time expensive), that results in a gazillion 1- and 2-element Lists (memory expensive). So you certainly don't want to do it twice. But the way pairedReads and unpairedReads is set up, you *will* do it twice if you want to process both paired and unpaired reads. (And even if you aren't, someone else might try to use this code to do so.). So my first suggestion is that you remove the call to groupReadPairs from pairedReads and unpairedReads, and let a user groupReadPairs once, and reuse the resulting JavaPairRDD to process paired and unpaired reads. My second suggestion is quite a bit more complicated, but I think it would result in far better performance. I'll sketch it out here, and then I can explain it further in person, if it's a direction you'd like to pursue. The first step is to create a JavaRDD<GATKRead> in which all pairs sharing a template name are in the same partition (but without grouping them). To do that, you temporarily boost the input JavaRDD into a JavaPairRDD<String,GATKRead> by extracting the read name as a key. Then you repartition (to do the shuffle). Then you map back to an ordinary JavaRDD<GATKRead> by keeping just the value. (Note: if the BAM has queryname sort order, you can just skip this step entirely.); Now you can do a mapPartition operation to filter for paired or unpaired reads: Iterate over the reads in the partition, and keep a hash map of [name -> read] of reads that have not yet found mates. To filter for paired reads, whenever you find the name of the current read already in the table, just emit the current read and the read in the map as a pair, and delete the read from the map (you're done with that name -- this keeps the table smaller). To filter for unpaired reads, just delete any map entry that you successfully look up, and insert any ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2664#issuecomment-299955039:845,perform,performance,845,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2664#issuecomment-299955039,1,['perform'],['performance']
Performance,Look how much cleaner these tests are with the loader api. It would have saved me so much pain if we'd had it a month ago :/,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2106#issuecomment-241125956:47,load,loader,47,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2106#issuecomment-241125956,1,['load'],['loader']
Performance,"Looking at the LL score cutoff from the plot above (around -.35) shows that the recall is expected to be quite low at this cutoff. This is also the case when looking at NA12878 with GiaB truth as is seen in the ROC plot below. The group of red points to the right is the region where the F1 score is maximized, while the purple points to the left represent the cutoff using the LL score. It looks like the LL score is much too conservative and chooses a sensitivity around .4 even when the overall ROC curve looks pretty good and a more optimal tradeoff exists. . ![image](https://user-images.githubusercontent.com/13020550/176241604-381d3a64-ee7f-4cf7-8ebe-bf18ae967301.png). For now we can afford to use a held out truth sample to tune this threshold so I don't think we need to use the LL score at this point in time. It would still be good to look into using the positive and negative model (this version only used the positive model) and plot the F1 and LL scores on the same plot to dig into what's going on.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1169000346:733,tune,tune,733,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1169000346,1,['tune'],['tune']
Performance,"Looking at the htsjdk code responsible for the original throw (as far as I can see in the stack enclosed in the description) there is a few ""smells"" in the way synchronized is used or not use ReferenceSource.java. It is likely to be the reason behind the error considering that is failing in multi-thread. . Probably adding synchronized to getReferenceBasesByRegion would fix that. Is a htsjdk issue and not a GATK one. Do you want to add a workaround in GATK or press for a fix and update of the htsjdk dependency. @droazen?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8139#issuecomment-1376298392:292,multi-thread,multi-thread,292,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8139#issuecomment-1376298392,1,['multi-thread'],['multi-thread']
Performance,Looks good. Hopefully there aren't race conditions lurking in other tools.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/379#issuecomment-93735639:35,race condition,race conditions,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/379#issuecomment-93735639,1,['race condition'],['race conditions']
Performance,"M 1.8.0_25-b17; Version: Version:4.pre-alpha-210-gd7179f7-SNAPSHOT JdkDeflater; 07:07:56.668 INFO PrintReads - Initializing engine; 07:07:56.815 INFO PrintReads - Shutting down engine; [December 5, 2015 7:07:56 AM EST] org.broadinstitute.hellbender.tools.PrintReads done. Elapsed time: 0.00 minutes.; Runtime.totalMemory()=257425408; ***********************************************************************. A USER ERROR has occurred: Input files reference and reads have incompatible contigs: Found contigs with the same name but different lengths: ; contig reference = 1 / 1000000; contig reads = 1 / 249250621.; reference contigs = [1]; reads contigs = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, X, Y, MT, GL000207.1, GL000226.1, GL000229.1, GL000231.1, GL000210.1, GL000239.1, GL000235.1, GL000201.1, GL000247.1, GL000245.1, GL000197.1, GL000203.1, GL000246.1, GL000249.1, GL000196.1, GL000248.1, GL000244.1, GL000238.1, GL000202.1, GL000234.1, GL000232.1, GL000206.1, GL000240.1, GL000236.1, GL000241.1, GL000243.1, GL000242.1, GL000230.1, GL000237.1, GL000233.1, GL000204.1, GL000198.1, GL000208.1, GL000191.1, GL000227.1, GL000228.1, GL000214.1, GL000221.1, GL000209.1, GL000218.1, GL000220.1, GL000213.1, GL000211.1, GL000199.1, GL000217.1, GL000216.1, GL000215.1, GL000205.1, GL000219.1, GL000224.1, GL000223.1, GL000195.1, GL000212.1, GL000222.1, GL000200.1, GL000193.1, GL000194.1, GL000225.1, GL000192.1, NC_007605]. ***********************************************************************; ```. I think you meant ""the Picard version of this metric does not complain"", which is expected given that Picard tools do not do the same automatic sequence dictionary validation as walkers and spark tools. If you think they should, you should create a different ticket ""Multi-input Picard tools and metrics should perform sequence dictionary validation on their inputs"" or file a specific bug against the Picard version of `CollectQualityYieldMetrics`. Closing",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1262#issuecomment-162176558:2920,perform,perform,2920,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1262#issuecomment-162176558,1,['perform'],['perform']
Performance,MYWJlbGVkVmFyaWFudEFubm90YXRpb25zRGF0dW0uamF2YQ==) | `68.421% <45.455%> (-3.801%)` | :arrow_down: |; | [...vqsr/scalable/LabeledVariantAnnotationsWalker.java](https://codecov.io/gh/broadinstitute/gatk/pull/8074/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvTGFiZWxlZFZhcmlhbnRBbm5vdGF0aW9uc1dhbGtlci5qYXZh) | `86.822% <46.154%> (+0.208%)` | :arrow_up: |; | [...rs/vqsr/scalable/TrainVariantAnnotationsModel.java](https://codecov.io/gh/broadinstitute/gatk/pull/8074/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvVHJhaW5WYXJpYW50QW5ub3RhdGlvbnNNb2RlbC5qYXZh) | `77.778% <66.667%> (-2.991%)` | :arrow_down: |; | [...lkers/vqsr/scalable/ExtractVariantAnnotations.java](https://codecov.io/gh/broadinstitute/gatk/pull/8074/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvRXh0cmFjdFZhcmlhbnRBbm5vdGF0aW9ucy5qYXZh) | `92.188% <100.000%> (+1.116%)` | :arrow_up: |; | [...walkers/vqsr/scalable/ScoreVariantAnnotations.java](https://codecov.io/gh/broadinstitute/gatk/pull/8074/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvU2NvcmVWYXJpYW50QW5ub3RhdGlvbnMuamF2YQ==) | `76.250% <100.000%> (+0.149%)` | :arrow_up: |; | [...r/scalable/data/LabeledVariantAnnotationsData.java](https://codecov.io/gh/broadinstitute/gatk/pull/8074/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8074#issuecomment-1294055323:2754,scalab,scalable,2754,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8074#issuecomment-1294055323,1,['scalab'],['scalable']
Performance,"Marissa Powers here -- I'm an Intel engineer on the same team as Ed. It sounds like we all agree on having Intel-optimized TF as the default and figuring out the best intervention for older machines from there. We can add the AVX flag within CNNScoreVariant (and any other AI tool). From there, we can (1) provide a detailed error output describing the issue, (2) provide a non-AVX TF build, and (3) automatically roll back TF to the provided version. @EdwardDixon, it sounds like @cmnbroad is suggesting is options (1), (2), and (3), while you're suggesting just (1). Sound right?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-429409667:113,optimiz,optimized,113,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-429409667,1,['optimiz'],['optimized']
Performance,"Meh, I think it may too much hassle for a 5-7% gain. Caching comes with the; risk of getting out of synch with the state. Maybe we should table this for; now. On Monday, July 25, 2016, droazen notifications@github.com wrote:. > @lbergelson https://github.com/lbergelson I'm not convinced we want to; > pre-calculate it at construction time, though. The null check is no big; > deal, and only needed in one place for each field (the method that; > retrieves or recalulates the cached value).; > ; > —; > You are receiving this because you were assigned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235127503,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/AB5rLzA4C5ZBD8ul-RspK7TpQbACMj-Bks5qZVS7gaJpZM4JR8AP; > . ## . Sent from Gmail Mobile",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235129333:476,cache,cached,476,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235129333,1,['cache'],['cached']
Performance,"Modifying what I wrote earlier, got confused with another issue. I am not familiar with Lustre and Lustre configuration. Did the excessive file locking from Lustre(FUTEX_WAIT_PRIVATE?) go away with `--genomicsdb-shared-posixfs-optimizations`? . Is there anyway to configure Lustre buffer sizes for writing? If not, can you try setting environment variable TILEDB_UPLOAD_BUFFER_SIZE to something like 5242880(5M) and try `GenomicsDBImport`? Does it help with performance? Is the amount of file locking lower than before?. If the performance is still not acceptable... What version of gatk are you using? Can you use the latest gatk and try using the `--bypass-feature-reader` option with `GenomicsDBImport`? Does this help with performance?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7646#issuecomment-1039746554:227,optimiz,optimizations,227,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7646#issuecomment-1039746554,4,"['optimiz', 'perform']","['optimizations', 'performance']"
Performance,"Most of the changes in GenomicsDB 1.3.0 was import and performance related. However, #6500 did add support for MNPs, not sure if that played a part here and the default streaming of data from GenomicsDB is now VCFCodec whereas it was BCFCodec before. @mlathara, can you think of anything else here?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6744#issuecomment-670816626:55,perform,performance,55,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6744#issuecomment-670816626,1,['perform'],['performance']
Performance,"Most of the scaling issues in Cromwell/Terra have been resolved. Terra still has limitations on workflow metadata size, and passing long file arrays to ever task in large scatters (i.e. the full list of counts files is passed into every gCNV shard) can limit our batch sizes for workflows that embed gCNV (e.g. GatherBatchEvidence in gatk-sv). gCNV workflows also don't call cache reliably (presumably due to timeouts) probably again due to the large file arrays, including 2D arrays.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4397#issuecomment-928168236:375,cache,cache,375,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4397#issuecomment-928168236,1,['cache'],['cache']
Performance,"Multiple causes can cause closed connections when reading from GCS, almost all of which are outside of our control. This will never be ""completely fixed"" in the sense that even if the code is perfect it's completely possible to send too many requests to GCS, and it'll respond by closing connections. The main factors that I know of are:. - number of concurrent accesses to the GCS bucket in question; - number of concurrent accesses to the GCP project in question; - storage class of the GCS bucket in question (the more expensive ones have more replicas, thus can handle a higher load). If you're running into those difficulties I would suggest trying to reduce the load (reduce the number of concurrent workers or threads) and making sure it's not a single-region storage bucket. If that fails, perhaps try using a different bucket that no one else is also using (to reduce other sources of load). If I understand correctly that you didn't change the version you're using but are suddenly seeing more issues than before, then perhaps the cause is a server-side change from GCS (outside of our control), a change in configuration (are you reading from a bucket of a different class from before), or perhaps just an increase of other activity on the same bucket/project. The current code is very persistent in its retries: as you can see from the messages it spent a whole half hour waiting. If it's an overload situation then you may get better performance by reducing the worker count (as they will have to retry less).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5631#issuecomment-526270716:351,concurren,concurrent,351,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5631#issuecomment-526270716,7,"['concurren', 'load', 'perform']","['concurrent', 'load', 'performance']"
Performance,"My builds are queued, but not started for more than 4 hours",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2733#issuecomment-305019422:14,queue,queued,14,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2733#issuecomment-305019422,1,['queue'],['queued']
Performance,My calculations give a runtime estimate of $0.18 for the 1D and... more for the 2D. We'll optimize later. It'll be fine.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4225#issuecomment-363560735:90,optimiz,optimize,90,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4225#issuecomment-363560735,1,['optimiz'],['optimize']
Performance,"My knowledge about .so files, linkers and architectures is very limited but to me it sounds like the GKL library is part of the jar and GATK tries to load it from there. And reading [this](https://github.com/broadinstitute/gatk/issues/2302) thread it sounds like the GKL library of GATK should work on ppc. Because of the ""no such file or directory"" messages (in the first warnings) I have also tried to point the `--tmp-dir` parameter of HaplotypeCaller to my home directory to make sure that it's not just a permission problem.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687564287:150,load,load,150,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687564287,1,['load'],['load']
Performance,Nevermind Chris. I've found the documentation for the feature at https://cromwell.readthedocs.io/en/stable/optimizations/FileLocalization/.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4806#issuecomment-437178155:107,optimiz,optimizations,107,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4806#issuecomment-437178155,1,['optimiz'],['optimizations']
Performance,NhbGFibGUvTGFiZWxlZFZhcmlhbnRBbm5vdGF0aW9uc1dhbGtlci5qYXZh) | `86.822% <46.154%> (+0.208%)` | :arrow_up: |; | [...rs/vqsr/scalable/TrainVariantAnnotationsModel.java](https://codecov.io/gh/broadinstitute/gatk/pull/8074/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvVHJhaW5WYXJpYW50QW5ub3RhdGlvbnNNb2RlbC5qYXZh) | `77.778% <66.667%> (-2.991%)` | :arrow_down: |; | [...lkers/vqsr/scalable/ExtractVariantAnnotations.java](https://codecov.io/gh/broadinstitute/gatk/pull/8074/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvRXh0cmFjdFZhcmlhbnRBbm5vdGF0aW9ucy5qYXZh) | `92.188% <100.000%> (+1.116%)` | :arrow_up: |; | [...walkers/vqsr/scalable/ScoreVariantAnnotations.java](https://codecov.io/gh/broadinstitute/gatk/pull/8074/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvU2NvcmVWYXJpYW50QW5ub3RhdGlvbnMuamF2YQ==) | `76.250% <100.000%> (+0.149%)` | :arrow_up: |; | [...r/scalable/data/LabeledVariantAnnotationsData.java](https://codecov.io/gh/broadinstitute/gatk/pull/8074/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvZGF0YS9MYWJlbGVkVmFyaWFudEFubm90YXRpb25zRGF0YS5qYXZh) | `75.510% <100.000%> (+1.283%)` | :arrow_up: |; | [.../hellbender/utils/genotyper/AlleleLikelihoods.java](https://codecov.io/gh/broadinstitute/gatk/pull/8074/diff?src=pr&el=tree&utm_medium=referral&utm_sourc,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8074#issuecomment-1294055323:3185,scalab,scalable,3185,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8074#issuecomment-1294055323,1,['scalab'],['scalable']
Performance,"No. I fixed that as far as I can tell. I have it in my path and I can invoke it from anywhere. The problem here is that inteldeflator is being specified through a hardcoded system property baked into the gradle launch scripts. We can fix it in gatk-launch by overriding it, or we can fix it properly by having the loader look in the classpath in the right place for it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1692#issuecomment-207471559:314,load,loader,314,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1692#issuecomment-207471559,1,['load'],['loader']
Performance,"Not a bad idea, will look into that tomorrow. Note that you are using Tensorflow 1.4 or 1.5 and that from v1.6 even the; non-Intel optimized build supports only AVX capable machines. On Thu 11 Oct 2018, 21:07 droazen, <notifications@github.com> wrote:. > *@droazen* commented on this pull request.; > ------------------------------; >; > In; > src/main/java/org/broadinstitute/hellbender/tools/walkers/vqsr/CNNScoreVariants.java; > <https://github.com/broadinstitute/gatk/pull/5291#discussion_r224587026>:; >; > > @@ -198,6 +200,13 @@; > return new String[]{""No default architecture for tensor type:"" + tensorType.name()};; > }; > }; > +; > + IntelGKLUtils utils = new IntelGKLUtils();; > + if (utils.isAvxSupported() == false); > + {; > + return new String[]{CNNScoreVariants.AVXREQUIRED_ERROR};; >; > Maybe the answer is for the conda environments to set an extra environment; > variable that would allow GATK to detect which conda environment it's in.; > Then you could have a check in CNNScoreVariants that aborts the tool only; > if AVX is not present AND you're running in the Intel conda environment,; > and point the user to the non-Intel conda environment in the error message.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/pull/5291#discussion_r224587026>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AG6lr8HM6ItLWqfSaTKeVY4yCp07il29ks5uj6TugaJpZM4XNHdi>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-429109651:131,optimiz,optimized,131,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-429109651,1,['optimiz'],['optimized']
Performance,"Not allowing the data for each sample to be broken down and sent to multiple parallel workers would have serious performance implications, you realize...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/320#issuecomment-84095044:113,perform,performance,113,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/320#issuecomment-84095044,1,['perform'],['performance']
Performance,"Not bad, but why can't the BAM be split as multiple objects in the same bucket where the directory is the name of the BAM. I was having this discussion with Dion at the following thread:. https://github.com/googlegenomics/utils-java/issues/62#issuecomment-220444203. You can have a folder in the GS location be the name of the BAM, and even sort them like a distributed B-tree. This way you can even simultaneously process reads as new data is streaming in from the GS location. Since the Google disk IOPS are as follows, based on the following link:. https://cloud.google.com/compute/docs/disks/performance#type_comparison. | Read | Write |; | --- | --- |; | 3000 IOPS | 0 IOPS |; | 2250 IOPS | 3750 IOPS |; | 1500 IOPS | 7500 IOPS |; | 750 IOPS | 11250 IOPS |; | 0 IOPS | 15000 IOPS |. So it all depends on perspective of what folks prefer, which in this case means that we can minimize the 6 min component. Then comes the 1.5 min portion of HDFS, which can occur in parallel and could also be memory-mapped and/or SSD accessed. So there are still ways to improve the access and processing time, but it depends on how fast - or instantaneous - folks want to have the results processed and returned back.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1755#issuecomment-235130868:596,perform,performance,596,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1755#issuecomment-235130868,1,['perform'],['performance']
Performance,"Not sure what your `GenotypeGVCFs` command was, but did you use the `--genomicsdb-shared-posixfs-optimizations` option? This option is available for the import too and may improve your performance.; ```; --genomicsdb-shared-posixfs-optimizations <Boolean>; Allow for optimizations to improve the usability and performance for shared Posix; Filesystems(e.g. NFS, Lustre). If set, file level locking is disabled and file system; writes are minimized. Default value: false. Possible values: {true, false} ; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8637#issuecomment-1879779166:97,optimiz,optimizations,97,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8637#issuecomment-1879779166,5,"['optimiz', 'perform']","['optimizations', 'performance']"
Performance,"Note that GATK3 uniquified the `GATKCommandLine` lines using a scheme like this:. ```; ##GATKCommandLine.SelectVariants.2=<ID=SelectVariants,Version=3.4-228-g2497091,Date=""Tue Jan 05 13:48:45 EST 2016"",Epoch=1452019725506,CommandLineOptions=""analysis_type=SelectVariants input_file=[] showFullBamList=false read_buffer_size=null phone_home=AWS gatk_key=null tag=NA read_filter=[] disable_read_filter=[] intervals=[3:113005755-195507036] excludeIntervals=null interval_set_rule=UNION interval_merging=ALL interval_padding=0 reference_sequence=/humgen/1kg/reference/human_g1k_v37.fasta nonDeterministicRandomSeed=false disableDithering=false maxRuntime=-1 maxRuntimeUnits=MINUTES downsampling_type=BY_SAMPLE downsample_to_fraction=null downsample_to_coverage=1000 baq=OFF baqGapOpenPenalty=40.0 refactor_NDN_cigar_string=false fix_misencoded_quality_scores=false allow_potentially_misencoded_quality_scores=false useOriginalQualities=false defaultBaseQualities=-1 performanceLog=null BQSR=null quantize_quals=0 static_quantized_quals=null round_down_quantized=false disable_indel_quals=false emit_original_quals=false preserve_qscores_less_than=6 globalQScorePrior=-1.0 validation_strictness=SILENT remove_program_records=false keep_program_records=false sample_rename_mapping_file=null unsafe=null disable_auto_index_creation_and_locking_when_reading_rods=false no_cmdline_in_header=false sites_only=true never_trim_vcf_format_field=false bcf=false bam_compression=null simplifyBAM=false disable_bam_indexing=false generate_md5=false num_threads=1 num_cpu_threads_per_data_thread=1 num_io_threads=0 monitorThreadEfficiency=false num_bam_file_handles=null read_group_black_list=null pedigree=[] pedigreeString=[] pedigreeValidationType=STRICT allow_intervals_with_unindexed_bam=false generateShadowBCF=false variant_index_type=DYNAMIC_SEEK variant_index_parameter=-1 reference_window_stop=0 logging_level=INFO log_to_file=null help=false version=false variant=(RodBinding name=variant source=/humgen/gsa",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4252#issuecomment-364237834:962,perform,performanceLog,962,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4252#issuecomment-364237834,1,['perform'],['performanceLog']
Performance,"Note that although all walkers now have comprehensive sequence dictionary validation performed on their inputs (via the GATKTool base class, which is aware of all primary tool inputs and so is able to perform this check automatically), at present we need to do this validation manually in dataflow tools (as I did with BQSR dataflow here) -- but it would be nice if we could get it to happen automatically in a base class as it does on the walker side of things. Created as ticket https://github.com/broadinstitute/hellbender/issues/669",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/668#issuecomment-122948454:85,perform,performed,85,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/668#issuecomment-122948454,2,['perform'],"['perform', 'performed']"
Performance,"Note that no segmentation or resolution parameters have been tuned yet for either performance or runtime. Some of these are very easy wins. For example, `kernel-approximation-dimension` is set to a default of 100, and the time for segmentation scales roughly linearly with this (documentation erroneously states that the scaling is quadratic, this should be fixed---my bad). In practice, setting this to as little as 2 seems to work OK for some cases, so we should evaluate this more rigorously. This can cut WGS segmentation down from ~10 minutes (out of the total ~60 minutes for 250bp bins, typically) to ~1 minute.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4122#issuecomment-461607380:61,tune,tuned,61,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4122#issuecomment-461607380,2,"['perform', 'tune']","['performance', 'tuned']"
Performance,Note to self: move the purging of the cache to the end of execution and test that leftover cache values are never present after invoking referenceConfidenceModel,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5911#issuecomment-489242346:38,cache,cache,38,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5911#issuecomment-489242346,2,['cache'],['cache']
Performance,Note: it looks like this doesn't fix the ConcurrentModificationException during serialization while running the unit tests on my Picard branch.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3581#issuecomment-329793309:41,Concurren,ConcurrentModificationException,41,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3581#issuecomment-329793309,1,['Concurren'],['ConcurrentModificationException']
Performance,"Now run with `--maxIndelSize 250`.; ```; WMCF9-CB5:shlee$ ./gatk LeftAlignAndTrimVariants -R ~/Documents/ref/hg38/Homo_sapiens_assembly38.fasta -V ~/Downloads/zeta_snippet_shlee/zeta_snippet.vcf.gz --maxIndelSize 250 -O zeta_snippet_leftalign_maxindelsize250.vcf.gz; Using GATK wrapper script /Users/shlee/Documents/branches/hellbender/build/install/gatk/bin/gatk; Running:; /Users/shlee/Documents/branches/hellbender/build/install/gatk/bin/gatk LeftAlignAndTrimVariants -R /Users/shlee/Documents/ref/hg38/Homo_sapiens_assembly38.fasta -V /Users/shlee/Downloads/zeta_snippet_shlee/zeta_snippet.vcf.gz --maxIndelSize 250 -O zeta_snippet_leftalign_maxindelsize250.vcf.gz; 17:24:16.345 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/shlee/Documents/branches/hellbender/build/install/gatk/lib/gkl-0.8.5.jar!/com/intel/gkl/native/libgkl_compression.dylib; Sep 05, 2018 5:24:16 PM shaded.cloud_nio.com.google.auth.oauth2.DefaultCredentialsProvider warnAboutProblematicCredentials; WARNING: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a ""quota exceeded"" or ""API not enabled"" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/.; 17:24:16.502 INFO LeftAlignAndTrimVariants - ------------------------------------------------------------; 17:24:16.502 INFO LeftAlignAndTrimVariants - The Genome Analysis Toolkit (GATK) v4.0.8.1-24-gb43bc27-SNAPSHOT; 17:24:16.502 INFO LeftAlignAndTrimVariants - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:24:16.502 INFO LeftAlignAndTrimVariants - Executing as shlee@WMCF9-CB5 on Mac OS X v10.13.6 x86_64; 17:24:16.502 INFO LeftAlignAndTrimVariants - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_111-b14; 17:24:16.503 INFO LeftAlignAndTrimV",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3487#issuecomment-418887543:710,Load,Loading,710,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3487#issuecomment-418887543,1,['Load'],['Loading']
Performance,"Now that I look at https://github.com/broadinstitute/gatk-evaluation/tree/master/pipeline-optimizer it will take a bit of work to get up to date (it was tied to a particular version of cromwell-tools and Advisor, which is python2-based, has not been actively maintained---exactly why I wanted to move to a more well supported solution...). . If it's not too much trouble, I'll try to get everything running locally on this older stuff for the first round of optimization, but this problem is probably a perfect candidate for the Neptune-based solution that @dalessioluca recently finished at https://github.com/dalessioluca/cromwell_for_ML.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-710153874:90,optimiz,optimizer,90,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-710153874,2,['optimiz'],"['optimization', 'optimizer']"
Performance,"Now, it seems like calling `contaminationDownsampling` right after `retainEvidence` could cause problems if both methods remove reads. However, one might correctly point out that although the cache invalidation I mentioned is not handled systematically, the method `removeEvidenceByIndex` _does_ have some code to update the evidence by sample and the evidence index map. It's possible that this code is totally fine and that this lead is a dead end. However, the code looks like it could be simpler and it's tough to parse. For example, try to track the `to` variable, which determines the determination of the outer `for` loop:. ```; for (int etrIndex = 1, to = nextIndexToRemove, from = to + 1; to < newEvidenceCount; etrIndex++, from++) {; if (etrIndex < evidencesToRemove.length) {; nextIndexToRemove = evidencesToRemove[etrIndex];; evidenceIndex.remove(evidences.get(nextIndexToRemove));; } else {; nextIndexToRemove = oldEvidenceCount;; }; for (; from < nextIndexToRemove; from++) {; final EVIDENCE evidence = evidences.get(from);; evidences.set(to, evidence);; evidenceIndex.put(evidence, to++);; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6586#issuecomment-625030697:192,cache,cache,192,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6586#issuecomment-625030697,1,['cache'],['cache']
Performance,"O FilterAlignmentArtifacts - Requester pays: disabled; 19:11:57.326 WARN FilterAlignmentArtifacts - . !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: FilterAlignmentArtifacts is an EXPERIMENTAL tool and should not be used for production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 19:11:57.326 INFO FilterAlignmentArtifacts - Initializing engine; 19:11:57.666 INFO FeatureManager - Using codec VCFCodec to read file file:///output/sample.FilterMutectCalls.vcf.gz; 19:11:57.757 INFO FilterAlignmentArtifacts - Done initializing engine; 19:11:57.827 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/app/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 19:11:57.861 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 19:11:57.862 INFO IntelPairHmm - Available threads: 4; 19:11:57.862 INFO IntelPairHmm - Requested threads: 4; 19:11:57.862 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 19:11:57.862 INFO ProgressMeter - Starting traversal; 19:11:57.862 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; *** glibc detected *** /for/bar/bin/java: double free or corruption (out): 0x00007f450af58700 ***; ======= Backtrace: =========; /lib64/libc.so.6(+0x3d01675dee)[0x7f45058afdee]; /lib64/libc.so.6(+0x3d01678c80)[0x7f45058b2c80]; /tmp/libgkl_smithwaterman410767516409374085.so(_Z19runSWOnePairBT_avx2iiiiPhS_iiaPcPs+0x338)[0x7f4499f4cfa8]; /tmp/libgkl_smithwaterman410767516409374085.so(Java_com_intel_gkl_smithwaterman_IntelSmithWaterman_alignNative+0xd8)[0x7f4499f4cbf8]; [0x7f44f58be6a2]; ======= Memory map: ========; ```. Then we **disabled** AVX2 in the newer cluster using Intels [sde64](https://software.intel.com/en-us/articles/intel-software-development-emulator) with `-ivb`, which directed GATK to use the Java implementation, and the filter worked without core dump. ```; sde64 -ivb -- faa.sh",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-660645356:4246,multi-thread,multi-threaded,4246,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-660645356,1,['multi-thread'],['multi-threaded']
Performance,"OK -- got it. I wasn't sure if the below comment was implying any better performance. . > Using this subset workspace seems to execute just fine as an input for SelectVariants. Sounds like you were just saying it worked, which is expected. As I said, I wouldn't expect the query to work any faster with such a single contig. I think some sort of profiling run, and trying ReBlockGVCFs on a few examples inputs are probably the best next steps.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1211417392:73,perform,performance,73,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1211417392,1,['perform'],['performance']
Performance,"OK @droazen @davidbenjamin I think this is ready for review. Note that:. 1) I did not restore the optimization introduced by @davidbenjamin in #5466. Happy to file an issue to restore it later by adding the appropriate parameter check, if we think it's important.; 2) I only added integration tests for HC and M2, since there is only a minimal test for FilterAlignmentArtifacts at this time. But I would think that these tests are enough to show that the exposure preserves behavior (unless I somehow got extremely unlucky with the test data and parameter values...); 3) Apologies to the reviewer for the somewhat complicated commit history, which resulted from introducing/removing the TSV stuff and was more trouble than it might be worth to reorder/resolve in the final rebase. I think it would be easiest for the reviewer to look at the 4 ""bubbled up..."" commits separately when reviewing the exposure of each parameter set, but then look at the overall commit when reviewing more superficial things or the tests.; 4) I'd appreciate it if the reviewer double checked that I did not switch anything up in parameter names, doc strings, default values, etc. when introducing the 12 explicit args in AssemblyBasedCallerArgumentCollection. Lots of copying and pasting there and would be easy to screw things up, as you might imagine! Pretty sure the tests bear out that this was done correctly, but you never know. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6885#issuecomment-897081017:98,optimiz,optimization,98,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6885#issuecomment-897081017,1,['optimiz'],['optimization']
Performance,"OK, I think I accidentally removed the `getopt` dependency in #3935. Not sure why tests didn't fail as expected. EDIT: I think this is because R dependencies are cached on Travis.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4209#issuecomment-359983165:162,cache,cached,162,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4209#issuecomment-359983165,1,['cache'],['cached']
Performance,"OK, added an integration test to check that MKL is enabled. If we move the conda install of these packages into the base image, then we might need to perform these checks elsewhere, e.g., in the bash script for building the image. Gotta push the base image and update the main Dockerfile. I'll merge after the weekend unless there are any more comments. Again, @droazen please be sure to highlight that R plotting will now require the conda environment in the release notes!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-622500992:150,perform,perform,150,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-622500992,1,['perform'],['perform']
Performance,"OK, relaxed the exact match to a delta of 1E-6 (chosen because doubles are formatted in somatic CNV outputs as `""%.6f""`) and tests pass on Travis (modulo an unrelated intermittent timeout failure). Note also that I was also able to reproduce locally by switching between Java 8 and 11. Had to add some quick test code for doing the comparisons; not actually sure if we have other utility methods to do so somewhere in the codebase. Another interesting note: I tried to clean up the offending use of log10factorial in AlleleFractionLikelihoods, but this introduced numerical differences at the ~1E-3 level. I think all of the round tripping between log and log10 actually adds up. Some digging revealed that this was introduced way back in gatk-protected in https://github.com/broadinstitute/gatk-protected/commit/aeec297e104db9f5196cb8f8e6691133302474bc#diff-34bd76cb2a416a212e25cbfb11298207265fb9cced775918aefcdb6b91ebc247. Despite the fact that we could easily replace the use of log10factorial with a private logGamma cache, at this point I think it makes more sense to freeze the current behavior. But if similar numerical changes are introduced to ModelSegments in the future, then it might make sense to clean this up at that point as well. Anyway, changed the title of the PR to reflect this update. Should be ready to go!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7652#issuecomment-1023793014:1021,cache,cache,1021,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7652#issuecomment-1023793014,1,['cache'],['cache']
Performance,"OK, thanks @droazen, that sounds sensible. Just pushed the rebase and will try to get to the rest of the changes this week. Just to clarify about running future evaluations/optimizations---I might need some pointers from whoever has been running the ""canonical"" evaluations (@michaelgatzen, maybe)? The ones I've run so far are on chr22 on the CHM mix, stratified in a few different ways (high/low complexity/confidence). We'll probably want to do something more thorough (e.g., akin to whatever is done for functional equivalence) to justify changing the defaults, I'm guessing. But we can cross those bridges once we get there!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6885#issuecomment-892034856:173,optimiz,optimizations---I,173,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6885#issuecomment-892034856,1,['optimiz'],['optimizations---I']
Performance,"OK, thanks @ldgauthier, I think I've addressed all the comments but one. A little TODO list for my benefit:. - [x] Updated the GATK version in the ExcessHet documentation to 4.2.2.0, but we'll see if I need to revisit that.; - [x] Not quite sure about the `ReducibleAnnotation` business. Let me know how to make these changes, or else happy to punt and file an issue.; - [ ] Also not sure I've parsed the results of the Jenkins tests, at least in terms of comparing how many sites get hard filtered with/out the change. Where should I be looking at to see the baseline result for that step? Also looks like a lot of results for https://gotc-jenkins.dsp-techops.broadinstitute.org/job/warp-workflow-tests/11755/ were call-cached, is that to be expected? Haven't looked at these tests before, so maybe you can walk me through them at some point. But I guess we can be sure that the overall results don't change too much (at least for 50 samples), which is a good start.; - [x] Didn't quite get to making those plots of the change in decision boundary, will do that tomorrow or later this week. EDIT: Nevermind, took like 5 minutes to throw them together (albeit using the slow python implementation and some for loops...), see below.; - [x] Hmm, looks like my own PR #6885 might've introduced a few more exact match test failures...grr. Here are some plots for N = 50, 100, and 500 samples showing (in black) those counts that previously fell under the 3E-6 threshold with the mid-p correction but now pass without it. As you can see, not much to sweat from these ""theoretical"" plots, but good to convolve with the actual allele frequency spectrum and get an idea of how many sites occupy these black squares in practice (as well as start us down the road of reexamining the threshold itself):. ![image](https://user-images.githubusercontent.com/11076296/132413689-37f3dfeb-e3f5-4869-a803-fe27f3cd79bd.png); ![image](https://user-images.githubusercontent.com/11076296/132413649-d716ee7d-6763-4275-82de-e",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7394#issuecomment-914612907:721,cache,cached,721,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7394#issuecomment-914612907,1,['cache'],['cached']
Performance,"OK. As a reference, how does GATK deal with max-alternate-alleles for normal human variant calling? Presumably really high alternate alleles would primarily happen in repetitive/index prone-regions? FWIW, When we execute GenotypeGVCFs, we run as ~1000 jobs where each takes an even chunk of the genome, by base pairs. . Yes, I did see the bypass-feature-reader option, but we have jobs in-flight and I'm reluctant to change too many things as once. We will try this when possible though. As far as number of batches imported: I would need to check, but I believe it's only ~5 batches with perhaps 50-100 samples/ea. So I guess it's not that many new batches in the scheme of things, but anecdotally we have noticed that with the last couple rounds of import we needed to reduce batch size to make it work (i.e. not get hung). It is conceivable there is some other factor that is causing that variable performance.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7542#issuecomment-964442581:901,perform,performance,901,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7542#issuecomment-964442581,1,['perform'],['performance']
Performance,"OK. In GATK3, the sharding size is calculated in `GenomeAnalysisEngine.getShardStrategy`. Since `GenotypeGVCFs` is a RodWalker and does not use input reads (BAM file), the shard size is `1,000,000`. ; This might be more than a thread safety bug (which is easy to fix, by making `GenotypingEngine.calculateOutputAlleleSubset() ` `synchronized`). What worries me is If the cache of upstream deletions spans intervals, this code will not work since the processing is asynchronous. For example, if there are 2 threads and the removed deletion crosses the shard barrier and the downstream interval thread is first to process, it will not see the upstream removed deletion.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2326#issuecomment-270467197:371,cache,cache,371,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2326#issuecomment-270467197,1,['cache'],['cache']
Performance,"OK. StratManager is about 80% wired to do the merging this would require. It would require me to implement some existing methods, like VariantEvaluator.combine(), but the basic idea exists today in StratificationManager.combineStrats(). I'd probably propose to expose this via VariantEvalEngine by adding saveToDisk(File targetFile), and restore(List<File> serializedStratManagers) methods. I'd propose to keep interaction with this process protected and only expose the save/restore methods. Alternately, I could expose save, restore (single-object) and combine methods. Regarding actually saving the state of StratificationManager to disk: some form of Jackson-based serialization would probably be easiest. My initial thought would be to make a new class specifically for serializing: SerializedStratificationManager. This would cache the relevant information and be the object that is serialized/deserialized. I would add a new constructor to StratificationManager that accepts this object and restores the state within StratificationManager. I think this could be all be kept internal to GATK. Does that broad outline seem like a concept you might accept in GATK?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-755351932:832,cache,cache,832,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-755351932,1,['cache'],['cache']
Performance,"Oh boy this smacks of being a race condition. More interesting is the fact that its failing specifically when threads = 1 and cram is off? I'm not sure if its also failing elsewhere on other runs though. It looks like its failing when it tries to add a new `final ReadsPathDataSource result = new ReadsPathDataSource(readArguments.getReadPaths(), factory);` to the datasource pool which sounds particularly nasty since its on an `ArrayList.add()` call. . @vruano",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7403#issuecomment-897021763:30,race condition,race condition,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7403#issuecomment-897021763,1,['race condition'],['race condition']
Performance,"Oh nice, that seems to be the ticket. All reads are used when setting that parameter. May I ask what the logic behind performing the downsampling is? Isn't there a risk of removing valid alignments that contribute to low abundance variation events? This would maybe only really be a problem when you are analysing sequences from a population of cells/microbes, but maybe the reward is greater than the risk?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7873#issuecomment-1139111543:118,perform,performing,118,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7873#issuecomment-1139111543,1,['perform'],['performing']
Performance,"Ok @jean-philippe-martin, I have an updated patch that seems to resolve the 503 errors! It's here: https://github.com/droazen/google-cloud-java/tree/dr_retry_CloudStorageReadChannel_fetchSize. Will you have time before you leave on vacation to open a PR against google-cloud-java? If not, let me know and we'll try to sort out our CLA issues and PR it ourselves. I didn't have time to write unit tests, unfortunately, though we're running it now with 1000 concurrent jobs each accessing 11,000 files and not seeing any errors.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3253#issuecomment-315447319:456,concurren,concurrent,456,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3253#issuecomment-315447319,1,['concurren'],['concurrent']
Performance,"Okay apparently there is not a version number in the picard.jar file downloaded from https://github.com/broadinstitute/picard/releases and thus if easybuild detects a cache copy, it will use that instead of downloading it which was an older version. They forced it to download and now version is correct. I will re-try and see if the error persists. Thanks for catching that.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3154#issuecomment-1419783633:167,cache,cache,167,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3154#issuecomment-1419783633,1,['cache'],['cache']
Performance,"Old code and classes are used for segment union. We should port or possibly replace this with a simple method that uses kernel segmentation. EDIT: Actually, just tried running a WGS sample and this is still a major bottleneck. EDIT 2: Hmm...actually doesn't seem to be an issue on my desktop (compared to my laptop, on which the run hangs here). Will try to track down the source of the discrepancy. EDIT 3: Added segment union based on single-changepoint detection using kernel segmentation.; - [x] Segment union should be replaced by a proper joint kernel segmentation. EDIT: I've added this, but there could be some minor improvements. Right now, only use one het per copy-ratio interval and throw away those off-target/bin. There are a few percent of targets/bins that have more than one het, and we could potentially rescue the off-target/bin hets as well with some care.; - Old code and models are used for modeling. Since the old allele-fraction model only models hets, we perform a `GetHetCoverage`-like binomial genotyping step (and output the results) before modeling. However, instead of assuming the null hypothesis of het (f = 0.5) and accepting a site when we cannot reject the null, we assume the null hypothesis of hom (f = error rate or 1 - error rate) and accept a site when we can reject the null. This entire process is very similar to what @davidbenjamin does in https://github.com/broadinstitute/gatk/pull/3638. We should consider combining this code (along with `AllelicCount`/`PileupSummary`) at some point.; - [x] Added option to use matched normal.; - [ ] Rather than port over the old modeling code, I would rather expand the allele-fraction model to allow for the modeling of hom sites. I wrote up such a model in some notes I sent around a few months back. This model allows for an allelic PoN that uses all sites to learn reference bias, not just hets. Depending on how our python development proceeds, I may try to implement this model using the old `GibbsSampler` code",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:4560,perform,perform,4560,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828,1,['perform'],['perform']
Performance,"On another note, if we really have hundreds of readers in parallel it's possible they're being throttled by GCS and that may be why we're seeing opens fail. GCS is counting on us backing off to reduce its load.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300320552:95,throttle,throttled,95,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300320552,2,"['load', 'throttle']","['load', 'throttled']"
Performance,"One consideration is that (unlike the GATK) we are supporting reference-less traversals, which makes the requirement to always provide a sequence dictionary for every interval a bit onerous... But if the consensus is that we should still always require validation against a sequence dictionary, but encapsulated within GenomeLoc (eliminating the need for GenomeLocParser), I would support that. I agree that the performance concerns are not worth addressing unless a profiler shows that they are an actual problem.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/100#issuecomment-69813473:412,perform,performance,412,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/100#issuecomment-69813473,1,['perform'],['performance']
Performance,"One final thing: i'm happy to try to debug this, and was going to write a test case based on the existing GenomicsDB integration tests. However, when I try to run any integration test involving genomicsdb, I get an exception like the following. I am on windows, so perhaps this is the issue?. 09:03:37.460 FATAL GenomicsDBLibLoader - ; java.io.FileNotFoundException: File /tiledbgenomicsdb.dll was not found inside JAR.; 	at org.genomicsdb.GenomicsDBLibLoader.loadLibraryFromJar(GenomicsDBLibLoader.java:118) ~[genomicsdb-1.3.2.jar:?]; 	at org.genomicsdb.GenomicsDBLibLoader.loadLibrary(GenomicsDBLibLoader.java:55) [genomicsdb-1.3.2.jar:?]; 	at org.genomicsdb.GenomicsDBUtilsJni.<clinit>(GenomicsDBUtilsJni.java:30) [genomicsdb-1.3.2.jar:?]; 	at org.genomicsdb.GenomicsDBUtils.createTileDBWorkspace(GenomicsDBUtils.java:46) [genomicsdb-1.3.2.jar:?]; 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.overwriteCreateOrCheckWorkspace(GenomicsDBImport.java:1005) [classes/:?]; 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.onTraversalStart(GenomicsDBImport.java:661) [classes/:?]; 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1056) [classes/:?]",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7005#issuecomment-749138102:460,load,loadLibraryFromJar,460,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7005#issuecomment-749138102,2,['load'],"['loadLibrary', 'loadLibraryFromJar']"
Performance,"One note that might be useful (or known already to the team): simply calling `cache()` doesn't cause any action. It seems that one might need to force the computation to be done on the RDD (e.g. `count()`), for caching to work, if the predicate depends on the results of computation. (ref last comment in #1877)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1811#issuecomment-225204823:78,cache,cache,78,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1811#issuecomment-225204823,1,['cache'],['cache']
Performance,"One observation that illustrates the need for care when optimizing metrics: for a few of the F1 optimizations, the haplotype-to-reference match-value parameter gets driven to its minimal value (1). Not 100% sure, but I'm guessing this might effectively boost precision by somehow cutting down on the complexity of proposed haplotypes---it depends on what the exact behavior of our SW algorithm is for negative scores. @davidbenjamin any thoughts on this behavior?. Something I don't quite understand yet is if we can impose some effective constraints on the parameters or otherwise reduce the number of independent dimensions. For example, it seems reasonable to me to fix the gap-extend penalties to -1 and let all other parameters be defined w.r.t. them. But perhaps we can also fix the match values similarly?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-712268193:56,optimiz,optimizing,56,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-712268193,2,['optimiz'],"['optimizations', 'optimizing']"
Performance,"One proposal for moving forward would be to have a default properties file with a known name/location that is included in the gatk jar (say, ""gatk.default.properties""), which is always loaded and populates the initial configuration, and then use the classloader getResources method to also load all resources with some other known name (say, ""gatk.properties""). That way any properties files on the classpath with the known name would be automatically discovered and loaded. The apache commons API allows looks like it has good support for handling this using a [composite](http://commons.apache.org/proper/commons-configuration/userguide/howto_compositeconfiguration.html#Composite_Configuration_Details) configuration. We would have to define some rules around override semantics, but it looks like the api provides a lot of control over that as well.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2322#issuecomment-274654954:185,load,loaded,185,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2322#issuecomment-274654954,3,['load'],"['load', 'loaded']"
Performance,"One suspects that there's a lot of inefficiency in this approach. If we dissected the ReadDataSource, we could probably figure out a way to separate the parts that need to be reinitialized for a new pass (iterators) from the parts that don't (cached index, e.g.).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5985#issuecomment-499609179:243,cache,cached,243,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5985#issuecomment-499609179,1,['cache'],['cached']
Performance,One way to mitigate this problem is to set the GRADLE_USER_HOME environment variable to move the gradle cache onto a shorter path. This solved the problem for me for BaseRecalibratorDataflow. ; In my case I saw about over 100 jars listed.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/580#issuecomment-114291341:104,cache,cache,104,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/580#issuecomment-114291341,1,['cache'],['cache']
Performance,"Our current mode of communication in gCNV is disk I/O. In my estimate, I/O is _not_ going to take a significant amount of time relatively. The overall structure is as follows: (java) pre-processing and filtering counts and writing processed counts to disk, (python) reading the processed counts, making CNV calls, writing model parameters and CNV posteriors to disk, (back to java) loading python results from disk and post-processing, i.e. creating .vcf files, .seg files, etc.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3698#issuecomment-337305574:382,load,loading,382,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3698#issuecomment-337305574,1,['load'],['loading']
Performance,"Our initial tests show the optimized version to be ~2x faster than the unoptimized version, which is much less of a gain than JP was initially seeing. One issue we've identified so far is that the custom sharding code is not written in a way that is aware of the way spark is actually partitioning the RDDs, so it's very easy to end up with all the work in a small number of partitions and/or a disproportionate amount of data in a single partition (the protection JP built into his code against large shards is not partition-aware and so not effective).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1006#issuecomment-149329134:27,optimiz,optimized,27,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1006#issuecomment-149329134,1,['optimiz'],['optimized']
Performance,"Overriding the defaults would get us most of the way there. Right now, we perform the following check on the IAC and fail if the defaults aren't changed to values that the CNV tools require, which is awkward:. ```; /**; * Validate that the interval-argument collection parameters minimally modify the input intervals.; */; public static void validateIntervalArgumentCollection(final IntervalArgumentCollection intervalArgumentCollection) {; Utils.validateArg(intervalArgumentCollection.getIntervalSetRule() == IntervalSetRule.UNION,; ""Interval set rule must be set to UNION."");; Utils.validateArg(intervalArgumentCollection.getIntervalExclusionPadding() == 0,; ""Interval exclusion padding must be set to 0."");; Utils.validateArg(intervalArgumentCollection.getIntervalPadding() == 0,; ""Interval padding must be set to 0."");; Utils.validateArg(intervalArgumentCollection.getIntervalMergingRule() == IntervalMergingRule.OVERLAPPING_ONLY,; ""Interval merging rule must be set to OVERLAPPING_ONLY."");; }; ```. If we override defaults, we'd still perform the check to make sure the user didn't muck with them, but it'd still be nicer than forcing the user to change the original defaults on their own. However, there are still two more awkward points: 1) there is no value for `-interval-set-rule` that does *nothing* to the incoming intervals (you must either union or intersect), and 2) we have to specify our own `padding` argument in `PreprocessIntervals` that is distinct from `-interval-padding`, since we want to implement our own padding. The first can be easily addressed by adding an option to do nothing to the intervals; I'm not so sure what the best way to handle the second would be, so we can punt on it if it'd be more work than it's worth---these are relatively minor pain points, in the end.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4341#issuecomment-363219857:74,perform,perform,74,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4341#issuecomment-363219857,2,['perform'],['perform']
Performance,"PR resolving https://github.com/broadinstitute/gatk/issues/1838 has been opened: https://github.com/broadinstitute/gatk/pull/1847. I included a method to extract and load the library from the classpath, as well as a unit test showing how to select between the Linux and Mac versions of the library.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1750#issuecomment-220704557:166,load,load,166,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1750#issuecomment-220704557,1,['load'],['load']
Performance,"Pair to find coverage for each interval, then reduceByKey; * Job 3 [getQNames] mapPartitions; * Job 4 [addAssemblyQNames -> getKmerIntervals] mapPartitionsToPair, then reduceByKey, then mapPartitions; * Job 5 [getAssemblyQNames] mapPartitions twice and a collect; * Job 6 [generateFastqs] mapPartitions and combineByKey, then write FASTQ to files. A few observations:; * Jobs 0,1,3 are simple map jobs - very quick 1-2 mins each.; * Job 2 is a simple MR, with a tiny shuffle to sum by key (<1MB of shuffle data); * Job 4 takes a bit longer longer (3min), and shuffles ~3GB. This is a lot faster than when I ran it before with less memory, when it took 9 min. Is it creating a lot of garbage? If you wanted to speed things up you might look at what this is doing on a local machine and see if there are any opportunities to improve CPU efficiency.; * Job 5 takes ~8 mins, and has no shuffle. CPU intensive processing again?; * Job 6 take a little over 3 mins, shuffling ~3GB. Overall, it looks like it’s performing pretty well. There is very little data being shuffled relative to the size of the input (~6GB to 133GB input), so it’s not worth looking into optimizing the data structures there. The input data is being read multiple times, so it _might_ be worth seeing if it can be cached by Spark to avoid reading from disk over and over again. This is only worth it if you have sufficient memory available across the cluster to hold the input (which will be bigger than the on-disk size) _plus_ enough memory for the processing, which as we saw is quite memory hungry anyway. There might be some CPU efficiencies to pursue, especially if some code paths are creating a lot of objects that need garbage collecting (as Jobs 4 and 5 seem to be). Jobs 4 and 5 seem to have some skew (judging from the task time distribution in the UI). You might investigate this by logging the amount of data that each task processes (or rather than logging, generating another output that is some description of the t",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2458#issuecomment-292171884:2301,perform,performing,2301,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2458#issuecomment-292171884,1,['perform'],['performing']
Performance,"Per comments on the Slack channel, this can also be used as a component in germline tagging for matched tumor-normal pairs. For the same series above:. Here is the 100% normal run through ModelSegments, which yields 36 segments:; ![N modeled](https://user-images.githubusercontent.com/11076296/76631751-931d1a80-6518-11ea-8c18-2a8f41057ce3.png). Joint segmentation of the 100% normal and the 100% tumor yields 241 segments (up from 130 for the 100% tumor alone, as above). Using this joint segmentation for subsequent ModelSegments runs:. For the 100% normal, this yields 88 segments (up from 36):; ![N-SJS modeled](https://user-images.githubusercontent.com/11076296/76632024-ebecb300-6518-11ea-89ff-109c97970ef0.png). For the 100% tumor, this yields 166 segments (up from 130):; ![T-SJS modeled](https://user-images.githubusercontent.com/11076296/76632125-13dc1680-6519-11ea-9901-0c78809d08ba.png). I haven't performed detailed validations, but some spot checking suggests that this actually mitigate oversegmentation while still increasing sensitivity to shared events. For example, there is a small 13-bin deletion in chr19 that is found when running the 100% normal alone, but gets broken up into two adjacent deletions when running the 100% tumor alone (probably just due to statistical noise in the copy ratios). When running jointly, the deletion does not get broken up. However, as discussed over Slack, we should probably run some scenarios with simulated data to check behavior---for example, how robust is the joint segmentation to some of the samples being noisy/oversegmented?. There are lots of options for restructuring the workflow. We could potentially modify ModelSegments to take in the denoised copy ratios from the normal, when available, and add modeling of the normal and germline tagging to that tool. Or we could break things up into separate tools. @fleharty any opinions?. Note that another benefit of using this joint segmentation for germline tagging is that common breakp",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6499#issuecomment-598764477:910,perform,performed,910,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6499#issuecomment-598764477,1,['perform'],['performed']
Performance,"Please find the stack trace below. Hopefully helpful. Using GATK jar /omics/chatchawit/gatk3/gatk-package-4.0.4.0-34-g2cc7abd-SNAPSHOT-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /omics/chatchawit/gatk3/gatk-package-4.0.4.0-34-g2cc7abd-SNAPSHOT-local.jar Funcotator -R /omics/chatchawit/bundle/hsa38.fasta -V /omics/chatchawit/sm/out/test.vcf -O /omics/chatchawit/sm/anno/test.vcf --output-file-format VCF --data-sources-path /omics/chatchawit/bundle/dsrc/ --ref-version hg38; 23:24:49.725 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/omics/chatchawit/gatk3/gatk-package-4.0.4.0-34-g2cc7abd-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_compression.so; 23:24:49.934 INFO Funcotator - ------------------------------------------------------------; 23:24:49.934 INFO Funcotator - The Genome Analysis Toolkit (GATK) v4.0.4.0-34-g2cc7abd-SNAPSHOT-0.0.3; 23:24:49.934 INFO Funcotator - For support and documentation go to https://software.broadinstitute.org/gatk/; 23:24:49.935 INFO Funcotator - Executing as chatchawit@omics on Linux v3.13.0-133-generic amd64; 23:24:49.935 INFO Funcotator - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_161-b12; 23:24:49.935 INFO Funcotator - Start Date/Time: May 23, 2018 11:24:49 PM ICT; 23:24:49.935 INFO Funcotator - ------------------------------------------------------------; 23:24:49.935 INFO Funcotator - ------------------------------------------------------------; 23:24:49.936 INFO Funcotator - HTSJDK Version: 2.15.0; 23:24:49.936 INFO Funcotator - Picard Version: 2.18.2; 23:24:49.936 INFO Funcotator - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 23:24:49.936 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 23:24:49.936 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 23:24:49.936 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_W",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-391421032:659,Load,Loading,659,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-391421032,1,['Load'],['Loading']
Performance,Please use the template in the WDL GATK repo doc that was shared. Or we can modify that template. I'd like the document to match what is generated automatically. The template in that document includes optimizations and is quite portable.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2480#issuecomment-358440295:201,optimiz,optimizations,201,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2480#issuecomment-358440295,1,['optimiz'],['optimizations']
Performance,Possibly you are running into the Spark performance regression described in https://github.com/broadinstitute/gatk/issues/4376. This was patched in the latest release (4.0.2.0) -- could you try running with that release and see if the issue is resolved?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4479#issuecomment-369947491:40,perform,performance,40,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4479#issuecomment-369947491,1,['perform'],['performance']
Performance,"Post hack-a-thon, we're almost ~2x faster than GATK3 in GVCF mode (2.7 min in GATK4 vs. 4.64 min in GATK3) in the `dr_runnable_haplotypecaller_ar` branch, but concordance has dropped from 99% to ~90%. Will have to do some testing to find out why, but the initial results are extremely promising performance-wise, at least. @lbergelson thinks he's found a bug in `SelectVariants` that might be distorting our concordance numbers for GVCFs, as well. @akiezun and @gspowley also found a bunch of low-hanging-fruit type optimizations that should buy us another 5% or so. It will take some time to merge everything back into this branch and see where we stand overall.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1567#issuecomment-200515306:295,perform,performance-wise,295,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1567#issuecomment-200515306,2,"['optimiz', 'perform']","['optimizations', 'performance-wise']"
Performance,"Prior to this, gCNV shard size and VM type have never been optimized. For WES cohort mode, we arbitrarily ran with:. 37 shards of 200 samples by 5000 intervals on n1-standard-8s (8 CPU, 30GB memory, $0.08 / hr) each taking ~2 hours = ~3 cents / sample. This PR gets rid of a memory spike in the sampling of denoised copy ratios, fixes a memory leak by updating theano, and also adds some theano flags that typically yield a factor of ~2 speedup (notably, the OpenMP elemwise flag, although we also get a slight boost from using numpy MKL). This allows us to run, e.g.: . 2 shards of 50 samples by 100000 intervals on n1-standard-8s (8 CPU, 30GB memory, $0.08 / hr) each taking ~5 hours = ~1.6 cents / sample; 4 shards of 50 samples by 50000 intervals on n1-highmem-4s (4 CPU, 26GB memory, $0.05 / hr) each taking ~3.25 hours = ~1.3 cents / sample; 45 shards of 50 samples by 5000 intervals on *n1-standard-1s* (1CPU, 3.75GB memory, $0.01 / hr) each taking ~0.5 hours = ~0.5 cents / sample. For these runs, we used a slightly larger interval list and 1/4 the number of samples than in the first example, but because everything scales linearly, it's probably fair to compare the per-sample-and-interval costs. So we get a factor of ~8 savings if we keep the shard size the same. The cost was already satisfactory, but fixing the leak allows us to more easily run scatters that are not so wide, which may be crucial for running the megaWDL. Adding the OpenMP flag also lets CPU scalability work as intended. We can do a more systematic optimization for cost if desired, and we should also revalidate to make sure performance doesn't vary too much with shard size (from spot checking, it looks like marginal and/or single-bin calls may flicker on and off). Note that we have still not optimized inference for WES, although I believe @vruano has done some optimizations for WGS. @mwalker174 @vruano for WGS with 2kb bins, I would expect the cost of the gCNV step to be ~10 cents in cohort mode before infer",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5781#issuecomment-471570697:59,optimiz,optimized,59,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5781#issuecomment-471570697,1,['optimiz'],['optimized']
Performance,R_SAMTOOLS : false; 13:48:31.695 INFO CountReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 13:48:31.695 INFO CountReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 13:48:31.695 INFO CountReadsSpark - Deflater: IntelDeflater; 13:48:31.695 INFO CountReadsSpark - Inflater: IntelInflater; 13:48:31.696 INFO CountReadsSpark - GCS max retries/reopens: 20; 13:48:31.696 INFO CountReadsSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 13:48:31.696 WARN CountReadsSpark -. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: CountReadsSpark is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 13:48:31.696 INFO CountReadsSpark - Initializing engine; 13:48:31.696 INFO CountReadsSpark - Done initializing engine; 18/12/21 13:48:32 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/12/21 13:48:33 WARN component.AbstractLifeCycle: FAILED ServerConnector@1cba0321{HTTP/1.1}{0.0.0.0:4040}: java.net.BindException: Address already in use; java.net.BindException: Address already in use; at sun.nio.ch.Net.bind0(Native Method); at sun.nio.ch.Net.bind(Net.java:433); at sun.nio.ch.Net.bind(Net.java:425); at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223); at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74); at org.eclipse.jetty.server.ServerConnector.open(ServerConnector.java:321); at org.eclipse.jetty.server.AbstractNetworkConnector.doStart(AbstractNetworkConnector.java:80); at org.eclipse.jetty.server.ServerConnector.doStart(ServerConnector.java:236); at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68); at org.eclipse.jetty.server.Server.doStart(Server.java:366); at org.eclipse.jetty.util.component.AbstractLife,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-449510725:4088,load,load,4088,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-449510725,1,['load'],['load']
Performance,"Ran some comparisons between funcotator and oncotator using the following data sources against a set of ~70 variants (see attached):; - achilles; - cancer_gene_census; - clinvar; - cosmic; - cosmic_fusion; - cosmic_tissue; - dna_repair_genes; - familial; - gencode; - gencode_xhgnc (Only Funcotator had this data source); - gencode_xrefseq; - hgnc; - oreganno; - simple_uniprot. While the details of the results were not compared between the two tools (unit tests are designed to do this comparison), the tools runtimes were captured (using the BSD `time` utility) over 10 iterations of annotating this data set (VCF->VCF). Full results set is attached, but the long and short of it is that depending on which timing you count by (real/user/system/user+system) **Funcotator is faster than Oncotator by anywhere from 8% to 57% and has not had any performance tuning.**. I have attached the timing measurements, as well as a gzip containing the inputs and the script I used to collect the timing information:. [benchmarking_funcotator_oncotator.tar.gz](https://github.com/broadinstitute/gatk/files/1601085/benchmarking_funcotator_oncotator.tar.gz); [BENCHMARK_funcotator_oncotator.xlsx](https://github.com/broadinstitute/gatk/files/1601079/BENCHMARK_funcotator_oncotator.xlsx)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3857#issuecomment-355068642:846,perform,performance,846,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3857#issuecomment-355068642,1,['perform'],['performance']
Performance,"Ran tests with JOIN query vs. IN query and performance/bytes scanned appeared identical. Going with IN query because it's more readable (to me, at least).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7762#issuecomment-1092333101:43,perform,performance,43,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7762#issuecomment-1092333101,1,['perform'],['performance']
Performance,"Rationale for engine changes:; This tool opens a large number of feature files (TSVs, not VariantContexts) and iterates over them simultaneously. No querying, just a single pass through each.; Issue 1: When a feature file lives in the cloud, it takes unacceptably long (several seconds, typically) to initialize it. A few seconds doesn't seem like a long time, but when there are large numbers of feature files to open, it adds up. This is caused by a large number of codecs (mostly the vcf-processing codecs) opening and reading the first few bytes of the file in the canDecode method. To avoid this I've reversed the order in which we test each codec, checking first if it produces the correct subtype of Feature, and only then calling canDecode. If you don't know what specific subtype you need, you can just ask for any Feature by passing Feature.class. It's much faster that way.; Issue 2: Each open feature source soaks up a huge amount of memory. That's because text-based feature reading is optimized for VCFs, which can have enormously long lines. So huge buffers are allocated. The problem is compounded for cloud-based feature files for which we allocate a large cloud prefetch buffer. (Though that feature can be turned off, which helps a little.) But the biggest memory hog is the TabixReader, which always reads in the index, regardless of whether it's used or not. Tabix indices are very large. To avoid this, I've created a smaller, simpler FeatureReader subclass called a TextFeatureReader that loads the index only when necessary. The revisions allow the new tool to run using an order of magnitude less memory. Faster, too.; Issue 3: The code in FeatureDataSource that creates a FeatureReader is brittle, and tests for various subclasses. To allow use of the new TextFeatureReader, I added a FeatureReaderFactory interface that allows one to ask the codec for an appropriate FeatureReader.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8031#issuecomment-1284340770:999,optimiz,optimized,999,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8031#issuecomment-1284340770,2,"['load', 'optimiz']","['loads', 'optimized']"
Performance,"Re-assigning to @akiezun, since he requested a few non-performance tickets to keep him from losing his sanity. This ticket should also encapsulate porting any other changes to the GATK3 annotation subsystem that have happened since the allele-specific support was added.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1641#issuecomment-215461621:55,perform,performance,55,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1641#issuecomment-215461621,1,['perform'],['performance']
Performance,Re-assigning to @jonn-smith and @jamesemery to assess whether this is worth resurrecting as part of the current HaplotypeCaller performance work.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2321#issuecomment-453558455:128,perform,performance,128,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2321#issuecomment-453558455,1,['perform'],['performance']
Performance,"Re-opening for alpha-3, since we will attempt to make additional performance improvements this quarter.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1800#issuecomment-236578897:65,perform,performance,65,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1800#issuecomment-236578897,1,['perform'],['performance']
Performance,Re-opening! I don't think we've answered all of the important questions here -- in particular:. -what % of total wall-clock runtime does the broadcast step take up?; -how does this change (if at all) as you add/remove executors?; -how does this change if you increase the vcf file size?. These are critical questions that will have a huge influence on the decisions we make in Q2 with respect to Spark performance work.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1675#issuecomment-208918434:402,perform,performance,402,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1675#issuecomment-208918434,1,['perform'],['performance']
Performance,Reader(AbstractFeatureReader.java:106); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getReaderFromVCFUri(GenomicsDBImport.java:437); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getFeatureReaders(GenomicsDBImport.java:419); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.traverse(GenomicsDBImport.java:344); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:740); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:220); Caused by: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: Read timed out; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.getBuf(SeekableByteChannelPrefetcher.java:136); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.fetch(SeekableByteChannelPrefetcher.java:255); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:300); 	... 28 more; Caused by: com.google.cloud.storage.StorageException: Read timed out; 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:186); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:512); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:128); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:125); 	at sh,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300298180:2768,concurren,concurrent,2768,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300298180,1,['concurren'],['concurrent']
Performance,Reading performance is particularly important,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5205#issuecomment-423304679:8,perform,performance,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5205#issuecomment-423304679,1,['perform'],['performance']
Performance,"Rebased and tweaked a few package versions, some of which are slightly different than those currently in the base (as is the version of R). Looks like `build-essential` is all you need in the base image for tests to not fallback on slow implementations, but we might want to double check that native dependencies are correct. Not really sure how urgent this is, and I have to admit I've lost track of the remaining ways R can break our builds. The possibility of a bad Travis cache (which I think was a common issue in the past) is now prevented by #6454, right?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-598996311:476,cache,cache,476,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-598996311,1,['cache'],['cache']
Performance,"Regarding intervals from the command line:. The column partition(s) is/are specified in the loader JSON file - the current functionality allows the user to not have to determine the list of contigs that overlap with a given column partition. All he/she needs to do is specify which column partition is being written to. We are ok with accepting contig intervals from the command line - in that case, it's the user's responsibility to ensure that the contig intervals are correct for the specific column partition being written to and are in the correct order. Regarding the other JSON input file, [this wiki section](https://github.com/Intel-HLS/GenomicsDB/wiki/Java-interface-for-importing-VCF-CSV-files-into-TileDB-GenomicsDB#mode-2-java-api-for-importing-variantcontext-objects) explains the why and what. This goes back to our discussions of meta-data and names. As Kushal mentioned, we can explain in more detail next week.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-277348689:92,load,loader,92,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-277348689,1,['load'],['loader']
Performance,"Regarding the non-Docker integration tests failing earlier today, I think this was because the R packages were added to the Travis cache in #3101. @cmnbroad cleared the cache to see if we could reproduce a compiler error introduced in #3934 on Travis (for the record, we could reproduce it on my local Ubuntu machine and gsa5, but not on Travis). This removed the cached getopt dependency, which then caused tests to fail. See #4246.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4209#issuecomment-359999441:131,cache,cache,131,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4209#issuecomment-359999441,3,['cache'],"['cache', 'cached']"
Performance,"Reopening -- @ldgauthier reports that this error still occurs even after the patch in https://github.com/broadinstitute/gatk/pull/5099. With that patch, we are now retrying on `UnknownHostException`, but the retries are all failing: . ```; [August 14, 2018 7:09:18 AM UTC] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 896.64 minutes.; Runtime.totalMemory()=3966238720; java.util.concurrent.CompletionException: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: Failure while waiting for FeatureReader to initialize with exception: com.google.cloud.storage.StorageException: All 20 reopens failed. Waited a total of 1918000 ms between attempts; at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273); at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1592); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: Failure while waiting for FeatureReader to initialize with exception: com.google.cloud.storage.StorageException: All 20 reopens failed. Waited a total of 1918000 ms between attempts; at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lambda$getFeatureReadersInParallel$614(GenomicsDBImport.java:605); at java.util.LinkedHashMap.forEach(LinkedHashMap.java:684); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getFeatureReadersInParallel(GenomicsDBImport.java:600); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.createSampleToReaderMap(GenomicsDBImport.java:491); at com.intel.genomicsdb.importer.GenomicsDBImp",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-412904420:418,concurren,concurrent,418,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-412904420,4,['concurren'],['concurrent']
Performance,"Requires https://github.com/samtools/htsjdk/pull/724. I tested on a cluster, loading features (VCF) from HDFS, by running:. ```bash; ./gatk-launch PileupSpark \; --input hdfs:///user/$USER/bam/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam \; --output hdfs:///user/$USER/out/pileup \; --reference hdfs:///user/$USER/fasta/human_g1k_v37.20.21.fasta \; -L 20:10000092-10000112 \; -metadata hdfs:///user/$USER/vcf/dbsnp_138.b37.20.21.vcf \; -- \; --sparkRunner SPARK \; --sparkSubmitCommand spark2-submit \; --sparkMaster yarn-client \; --driver-memory 3G \; --num-executors 1 \; --executor-cores 1 \; --executor-memory 3G; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2344#issuecomment-273178186:77,load,loading,77,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2344#issuecomment-273178186,1,['load'],['loading']
Performance,"Review complete -- back to @jean-philippe-martin. Please feel free to merge after addressing comments. . If you have time to do a quick test, it would be interesting to hear whether either the shard balancing or hierarchical reduce changes ended up fixing your performance issues (since if they didn't, we'll have to assign someone to work on that in your absence).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/987#issuecomment-148544513:261,perform,performance,261,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/987#issuecomment-148544513,1,['perform'],['performance']
Performance,"Reviving this. This will essentially be a major refactor/rewrite of CreatePanelOfNormals to make it scalable enough to handle WGS. - [x] CombineReadCounts is too cumbersome for large matrices. Change CreatePanelOfNormals to take in multiple -I instead.; - [x] Rename NormalizeSomaticReadCounts to DenoiseReadCounts and require integer read counts as input. These will still be backed by a ReadCountCollection until @asmirnov239's changes are in.; - [x] Remove optional outputs (factor-normalized and beta-hats) from DenoiseReadCounts. For now, TN and PTN output will remain in the same format (log2) to maintain compatibility with downstream tools.; - [x] Maximum number of eigensamples K to retain in the PoN is specified; the smaller of this or the number of samples remaining after filtering is used. The number actually used to denoise can be specified in DenoiseReadCounts. If we are going to spend energy computing K eigensamples, there is no reason we shouldn't expose all of them in the PoN, even if we don't want to use all of them for denoising. (Also, the current SVD utility methods do not allow for specification of K < N when performing SVD on an MxN matrix, even though the backend implementations that are called do allow for this; this is terrible. In any case, randomized SVD should be much faster than the currently available implementations, even when K = N).; - [x] Rename CreatePanelOfNormals to CreateReadCountPanelOfNormals; - [x] Refer to ""targets"" as intervals. See #3246.; - [x] Remove QC.; - [x] Refer to proportional coverage as fractional coverage.; - [x] Perform optional GC-bias correction internally if annotated intervals are passed as input.; - [x] Make standardization process for panel and case samples identical. Currently, a sample mean is taken at one point in the PoN standardization process, while a sample median is taken in the case standardization process.; - [x] HDF5 PoN will store version number, all integer read counts, all/panel intervals, all/panel ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-313921687:100,scalab,scalable,100,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-313921687,1,['scalab'],['scalable']
Performance,Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvRXh0cmFjdFZhcmlhbnRBbm5vdGF0aW9uc0ludGVncmF0aW9uVGVzdC5qYXZh) | `ø` |; | [...rs/vqsr/scalable/TrainVariantAnnotationsModel.java](https://app.codecov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvVHJhaW5WYXJpYW50QW5ub3RhdGlvbnNNb2RlbC5qYXZh) | `37.037%` |; | [...alable/modeling/PythonVariantAnnotationsModel.java](https://app.codecov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvbW9kZWxpbmcvUHl0aG9uVmFyaWFudEFubm90YXRpb25zTW9kZWwuamF2YQ==) | `66.667%` |; | [...e/TrainVariantAnnotationsModelIntegrationTest.java](https://app.codecov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvVHJhaW5WYXJpYW50QW5ub3RhdGlvbnNNb2RlbEludGVncmF0aW9uVGVzdC5qYXZh) | `77.778%` |; | [...vqsr/scalable/LabeledVariantAnnotationsWalker.java](https://app.codecov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvTGFiZWxlZFZhcmlhbnRBbm5vdGF0aW9uc1dhbGtlci5qYXZh) | `100.000%` |; | ... and [2 more](https://app.codecov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8131#issuecomment-1352314153:5181,scalab,scalable,5181,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8131#issuecomment-1352314153,1,['scalab'],['scalable']
Performance,Rpc.read(HttpStorageRpc.java:512); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:128); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:125); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:92); 	at shaded.cloud_nio.com.google.api.gax.retrying.RetryingFutureImpl.executeAttempt(RetryingFutureImpl.java:141); 	at shaded.cloud_nio.com.google.api.gax.retrying.RetryingFutureImpl.access$500(RetryingFutureImpl.java:59); 	at shaded.cloud_nio.com.google.api.gax.retrying.RetryingFutureImpl$AttemptFutureCallback.onFailure(RetryingFutureImpl.java:177); 	at shaded.cloud_nio.com.google.api.gax.core.ApiFutures$1.onFailure(ApiFutures.java:52); 	at shaded.cloud_nio.com.google.common.util.concurrent.Futures$6.run(Futures.java:1764); 	at shaded.cloud_nio.com.google.common.util.concurrent.MoreExecutors$DirectExecutor.execute(MoreExecutors.java:456); 	at shaded.cloud_nio.com.google.common.util.concurrent.Futures$ImmediateFuture.addListener(Futures.java:153); 	at shaded.cloud_nio.com.google.common.util.concurrent.ForwardingListenableFuture.addListener(ForwardingListenableFuture.java:47); 	at shaded.cloud_nio.com.google.api.gax.core.internal.ApiFutureToListenableFuture.addListener(ApiFutureToListenableFuture.java:53); 	at shaded.cloud_nio.com.google.common.util.concurrent.Futures.addCallback(Futures.java:1776); 	at shaded.cloud_nio.com.google.common.util.concurrent.Futures.addCallback(Futures.java:1713); 	at shaded.cloud_nio.com.google.api.gax.core.ApiFutures.addCallback(ApiFutures.java:47); 	at shaded.cloud_nio.com.google.api.gax.retrying.RetryingFutureImpl.setAttemptFuture(RetryingFutureImpl.java:107); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:100); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:47); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:125); 	at com.google.cloud.st,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300298180:4570,concurren,concurrent,4570,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300298180,1,['concurren'],['concurrent']
Performance,"Running with the log10 cache on a non-bp file, 10k samples, 1k lines: 186M. GATK3.5; real 0m26.499s; user 0m42.367s. GATK4; real 0m33.999s; user 0m41.278s",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1608#issuecomment-227769422:23,cache,cache,23,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1608#issuecomment-227769422,1,['cache'],['cache']
Performance,"ST controlMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/0e5c32ab-65e6-451f-a04e-6a3f5e7fe5c8/call-NISTSampleHeadToHead/BenchmarkComparison/ed0dc9e1-2d64-47e4-82e0-811971957020/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""NIST controlindelF1Score"": ""0.9902"",; ""NIST controlindelPrecision"": ""0.9903"",; ""NIST controlsnpF1Score"": ""0.9899"",; ""NIST controlsnpPrecision"": ""0.9887"",; ""NIST controlsnpRecall"": ""0.9911"",; ""NIST controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/0e5c32ab-65e6-451f-a04e-6a3f5e7fe5c8/call-NISTSampleHeadToHead/BenchmarkComparison/ed0dc9e1-2d64-47e4-82e0-811971957020/call-BenchmarkVCFControlSample/Benchmark/8c516721-e955-41d1-907e-fcee92f592d3/call-CombineSummaries/summary.csv"",; ""NIST evalHCprocesshours"": ""100.56416111111112"",; ""NIST evalHCsystemhours"": ""0.19999166666666665"",; ""NIST evalHCwallclockhours"": ""74.00048055555555"",; ""NIST evalHCwallclockmax"": ""4.007605555555555"",; ""NIST evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/0e5c32ab-65e6-451f-a04e-6a3f5e7fe5c8/call-NISTSampleHeadToHead/BenchmarkComparison/ed0dc9e1-2d64-47e4-82e0-811971957020/call-EVALRuntimeTask/cacheCopy/monitoring.pdf"",; ""NIST evalindelF1Score"": ""0.9902"",; ""NIST evalindelPrecision"": ""0.9903"",; ""NIST evalsnpF1Score"": ""0.9899"",; ""NIST evalsnpPrecision"": ""0.9887"",; ""NIST evalsnpRecall"": ""0.9911"",; ""NIST evalsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/0e5c32ab-65e6-451f-a04e-6a3f5e7fe5c8/call-NISTSampleHeadToHead/BenchmarkComparison/ed0dc9e1-2d64-47e4-82e0-811971957020/call-BenchmarkVCFTestSample/Benchmark/427c5010-a177-42d8-81be-5a387beed653/call-CombineSummaries/summary.csv"",; ""ROC_Plots_Reported"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/0e5c32ab-65e6-451f-a04e-6a3f5e7fe5c8/call-CreateHTMLReport/cacheCopy/report.html""; },; ""errors"": null; } ; </pre> </details>",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1535104202:21386,cache,cacheCopy,21386,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1535104202,2,['cache'],['cacheCopy']
Performance,STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 /share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-spark.jar CountReadsSpark --input /project/casa/gcad/test/HG04302.alt_bwamem_GRCh38DH.20150718.GBR.low_coverage.cram --reference file:///restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --spark-master yarn; 2019-01-07 11:33:18 WARN SparkConf:66 - The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 2019-01-07 11:33:19 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 11:33:24.377 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 11:33:24.549 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 11:33:26.271 INFO CountReadsSpark - ------------------------------------------------------------; 11:33:26.272 INFO CountReadsSpark - The Genome Analysis Toolkit (GATK) v4.0.12.0; 11:33:26.272 INFO CountReadsSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:33:26.272 INFO CountReadsSpark - Executing as farrell@scc-hadoop.bu.edu on Linux v2.6.32-754.6.3.el6.x86_64 amd64; 11:33:26.273 INFO CountReadsSpark - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:2210,load,load,2210,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,1,['load'],['load']
Performance,"SVIntervals are comparable, and provide the same total order as coordinate-sorted BAMs. They can encode any contiguous stretch of bases on the reference, or strand-sensitively on its reverse complement. They can encode 0-length intervals (i.e., locations) to indicate things like the precise, unambiguous location of an insert between two reference bases. They're super light-weight, and cache friendly (no references), and they can be compared and tested for equality quickly and locally. They know how to serialize themselves with Kryo. They have a much more complete set of operations to calculate overlaps and underlaps, total order, etc. How will we implement isUpstreamOf using SimpleIntervals, when SimpleIntervals has no concept of contig order?; I think we'd have a heck of a job retrofitting our code to use SimpleIntervals, and a lot of testing to do to prove that the performance loss isn't significant. I'm sorry we struck out in an incompatible direction, but SimpleInterval just didn't seem up to the job at the time we started the SV project.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5154#issuecomment-418520720:388,cache,cache,388,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5154#issuecomment-418520720,2,"['cache', 'perform']","['cache', 'performance']"
Performance,Sampling method optimized in #5781.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5754#issuecomment-471632458:16,optimiz,optimized,16,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5754#issuecomment-471632458,1,['optimiz'],['optimized']
Performance,Saw it again [here](https://travis-ci.com/broadinstitute/gatk/jobs/180435353) (now restarted). If I'm reading the serialization stack in the right order:. ```; Serialization trace:; classes (sun.misc.Launcher$AppClassLoader); classLoader (org.apache.hadoop.conf.Configuration); conf (org.apache.hadoop.hdfs.DistributedFileSystem); fs (hdfs.jsr203.HadoopFileSystem); hdfs (hdfs.jsr203.HadoopPath); path (htsjdk.samtools.seekablestream.SeekablePathStream); seekableStream (htsjdk.tribble.TribbleIndexedFeatureReader); featureReader (org.broadinstitute.hellbender.engine.FeatureDataSource); featureSources (org.broadinstitute.hellbender.engine.FeatureManager); ```. it looks like we're trying to serialize a ClassLoader. The FieldSerializer does appear to use a ClassLoader to load classes during serialization.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5680#issuecomment-467462740:774,load,load,774,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680#issuecomment-467462740,1,['load'],['load']
Performance,Saw the recent https://github.com/samtools/htsjdk/pull/744 fix in htsjdk; I'm going to assume that it fixed whatever performance problems people have had previously linked to large genomes. If someone runs into new issues we'll create new ticket as appropriate.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1688#issuecomment-264069947:117,perform,performance,117,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1688#issuecomment-264069947,1,['perform'],['performance']
Performance,"See the ScatterIntervals and related tasks in the gCNV WDLs at the link above. The idea is to performs multiple runs of gCNV in shards of >10,000 intervals each across the exome. These shards can be specified by passing the set of intervals for each shard via the -L argument to GermlineCNVCaller. The gCNV model is independently fit in each shard, using the global depth and contig-level ploidy parameters determined across the whole exome in the DetermineGermlineContigPloidy step. The shards are then stitched together in the PostprocessGermlineCNVCalls step. I believe we typically run shards of 10,000 intervals for cohorts of 100-200 samples using n1-standard-8 machines, which have 30GB of memory (although looking at some recent runs, I think we've used as few as 5,000 intervals per shard). I think as few as several tens of samples could be sufficient to train a batch, but 100-200 is preferable---of course, this will depend on the specifics of your particular data set.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5053#issuecomment-407750916:94,perform,performs,94,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5053#issuecomment-407750916,1,['perform'],['performs']
Performance,"Separate repos for each native implementation will allow the native-code providers to be much more nimble and will free the Broad from maintenance of native code - this is much better placed with experts on AVX and Power etc. We are experts on genomics but others are experts on native-code optimizations. With separate repos, IBM and Intel (and others else in the future) can keep on innovating and improving native code without having to test on multiple platforms and GATK takes advantage of those platform-specific libraries and integrates them into the genome toolkit. I think the small amount of initial code duplication is well worth it. Happy to discuss more - feel free to reach out directly.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1788#issuecomment-217197098:291,optimiz,optimizations,291,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1788#issuecomment-217197098,1,['optimiz'],['optimizations']
Performance,Several classes have static caches in them. ; - [x] ReadCovariates #915 ; - [ ] MathUtils,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/914#issuecomment-142407113:28,cache,caches,28,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/914#issuecomment-142407113,1,['cache'],['caches']
Performance,So the null pointer is caused by bwa mem complaining that it cannot load the index from the HDFS.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-311699030:68,load,load,68,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-311699030,1,['load'],['load']
Performance,"So there may be some unfortunate performance implications with some of these changes. Utils.nonNull(value, message) and it's compatriots will always compute the message even if the error condition is not met. Using any message which isn't a constant will generate garbage in the form of strings. In most cases this isn't a problem, but it is not ideal if it's placed in a tight loop. . We could offset the problem by adding a family of Utils functions that take a lambda String producer instead of a string itself, this would allow the message to be computed only when the error condition is triggered avoiding garbage creation.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1979#issuecomment-231126857:33,perform,performance,33,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1979#issuecomment-231126857,1,['perform'],['performance']
Performance,"Solution is to add the optional ability to `FeatureDataSource` to cache Features around the current locus, not merely past the current locus.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1151#issuecomment-169136166:66,cache,cache,66,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1151#issuecomment-169136166,1,['cache'],['cache']
Performance,"Some of the CNV tools are miscategorized: GetHetCoverage + tools in the ""Intervals Manipulation"" category. The latter should probably be considered CNV-specific because they either use the target-file format (which is only used in the legacy CNV + ACNV pipeline) or perform a task that is specific to the CNV pipeline and probably not of general interest (PreprocessIntervals).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-347530234:266,perform,perform,266,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-347530234,1,['perform'],['perform']
Performance,Sorry but it doesn't seem to be resolved.; ```; $ gatk ViewSam -I /data/MCF7_PDS/MCF7_PDS.bam --ALIGNMENT_STATUS=Aligned --PF_STATUS PF --HEADER_ONLY true ; Using GATK jar /resources/tools/gatk-4.0.4.0/gatk-package-4.0.4.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /resources/; tools/gatk-4.0.4.0/gatk-package-4.0.4.0-local.jar ViewSam -I /data/MCF7_PDS.bam --ALIGNMENT_STATUS=Aligned --PF_STATUS PF --HEADER_ONLY; true; 08:51:42.543 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/resources/tools/gatk-4.0.4.0/gatk-package-4.0.4.0-local.jar!/com/intel/gkl/native/libgkl_compr; ession.so; [Mon May 07 08:51:42 CEST 2018] ViewSam --INPUT /data/MCF7_PDS.bam --ALIGNMENT_STATUS Aligned --PF_STATUS PF --HEADER_ONLY true --REC; ORDS_ONLY false --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX false --CREATE_MD5_FILE false --GA4GH; _CLIENT_SECRETS client_secrets.json --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false; [Mon May 07 08:51:42 CEST 2018] Executing as [...] on Linux 4.4.0-87-generic amd64; OpenJDK 64-Bit Server VM 1.8.0_151-8u151-b12-0ubuntu0.16.04.2-b12; Deflater; : Intel; Inflater: Intel; Provider GCS is available; **Picard version: Version:4.0.4.0**; ```; And if I use `--version true`:; ```; $ gatk ViewSam -I /data/MCF7_PDS.bam --ALIGNMENT_STATUS=Aligned --PF_STATUS PF --HEADER_ONLY true --version true; Using GATK jar /resources/tools/gatk-4.0.4.0/gatk-package-4.0.4.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /resources/tools/gatk-4.0.4.0/gatk-package-4.0.4.0-local.jar ViewSam -I /data/MCF7_PDS.bam --ALIGNMENT_STATUS=Aligned --PF_STATUS PF ,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4733#issuecomment-386976106:605,Load,Loading,605,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4733#issuecomment-386976106,1,['Load'],['Loading']
Performance,"Sorry to be a pain... I'm still getting this problem with 4.0.2.1, which according to the tag [description](https://github.com/broadinstitute/gatk/releases/tag/4.0.2.1) should have this issue fixed. ```; Using GATK jar /home/db291g/applications/gatk/gatk-4.0.2.1/gatk-package-4.0.2.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -jar /home/db291g/applications/gatk/gatk-4.0.2.1/gatk-package-4.0.2.1-local.jar FilterMutectCalls --variant gatk4/WW00274.vep.vcf.gz --output test.vcf.gz; 10:00:20.977 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/db291g/applications/gatk/gatk-4.0.2.1/gatk-package-4.0.2.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 10:00:21.082 INFO FilterMutectCalls - ------------------------------------------------------------; 10:00:21.083 INFO FilterMutectCalls - The Genome Analysis Toolkit (GATK) v4.0.2.1; 10:00:21.083 INFO FilterMutectCalls - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:00:21.083 INFO FilterMutectCalls - Executing as db291g@login01 on Linux v2.6.32-431.23.3.el6.x86_64 amd64; 10:00:21.083 INFO FilterMutectCalls - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 10:00:21.083 INFO FilterMutectCalls - Start Date/Time: March 7, 2018 10:00:20 AM GMT; 10:00:21.083 INFO FilterMutectCalls - ------------------------------------------------------------; 10:00:21.083 INFO FilterMutectCalls - ------------------------------------------------------------; 10:00:21.084 INFO FilterMutectCalls - HTSJDK Version: 2.14.3; 10:00:21.084 INFO FilterMutectCalls - Picard Version: 2.17.2; 10:00:21.084 INFO FilterMutectCalls - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 10:00:21.084 INFO FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 10:00:21.084 INFO FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOL",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4363#issuecomment-371088787:660,Load,Loading,660,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4363#issuecomment-371088787,1,['Load'],['Loading']
Performance,"Sorry, if I understand, David's tool is to check for correctness of of MarkDuplicates. I'm just looking at the performance.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1100#issuecomment-159143808:111,perform,performance,111,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1100#issuecomment-159143808,1,['perform'],['performance']
Performance,"Sorry, just saw this now. We still don't have a simple solution for training models without pysam. We can probably do something similar to what we do with inference, but I think the current priority is to improve inference throughput so it will probably be a little while before we get to re-writing the training code. If people feel we should re-prioritize please let me know.; I have installed the conda environment on the same OSX version, without seeing this issue.; Which gcc version are you using @mwalker174 ? ; My `gcc -v` output is:; ```; Configured with: --prefix=/Applications/Xcode.app/Contents/Developer/usr --with-gxx-include-dir=/usr/include/c++/4.2.1; Apple LLVM version 8.0.0 (clang-800.0.42.1); Target: x86_64-apple-darwin15.6.0; Thread model: posix; InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4742#issuecomment-391014193:223,throughput,throughput,223,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4742#issuecomment-391014193,1,['throughput'],['throughput']
Performance,"SortSam - Deflater IntelDeflater; 18:01:51.713 INFO SortSam - Initializing engine; 18:01:51.713 INFO SortSam - Done initializing engine; 18:02:01.512 INFO SortSam - Shutting down engine; [December 7, 2016 6:02:01 PM AST] org.broadinstitute.hellbender.tools.picard.sam.SortSam done. Elapsed time: 0.16 minutes.; Runtime.totalMemory()=1911029760; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/xerial/snappy/LoadSnappy; 	at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:86); 	at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:52); 	at htsjdk.samtools.util.TempStreamFactory.getSnappyLoader(TempStreamFactory.java:42); 	at htsjdk.samtools.util.TempStreamFactory.wrapTempOutputStream(TempStreamFactory.java:74); 	at htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:223); 	at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:166); 	at htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:192); 	at org.broadinstitute.hellbender.tools.picard.sam.SortSam.doWork(SortSam.java:52); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:112); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgram.instanceMain(PicardCommandLineProgram.java:62); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116); 	at org.broadinstitute.hellbender.Main.main(Main.java:158); Caused by: java.lang.ClassNotFoundException: org.xerial.snappy.LoadSnappy; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:381); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:424); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357); 	... 15 more",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2299#issuecomment-265469924:3117,Load,LoadSnappy,3117,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2299#issuecomment-265469924,4,"['Load', 'load']","['LoadSnappy', 'loadClass']"
Performance,"Sounds good. I have not characterized the performance changes (if any). There are some minor differences in output, but the output is now more correct for symbolic alleles.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5834#issuecomment-476358270:42,perform,performance,42,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5834#issuecomment-476358270,1,['perform'],['performance']
Performance,"Sounds like a reasonable strategy to me. WRT to the performance costs, it sounds like @tomwhite's serialization changes would be a much bigger benefit than the cost of having to serialize the reference names.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141310650:52,perform,performance,52,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141310650,1,['perform'],['performance']
Performance,Sounds like the fix here might kill some/all of our performance gains from implementing this feature... :(,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3688#issuecomment-377291391:52,perform,performance,52,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3688#issuecomment-377291391,1,['perform'],['performance']
Performance,"Spot(TM) 64-Bit Server VM 1.8.0_112-b15; Version: 4.alpha.2-1125-g27b5190-SNAPSHOT; 16:55:20.229 INFO BwaAndMarkDuplicatesPipelineSpark - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 16:55:20.229 INFO BwaAndMarkDuplicatesPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:55:20.229 INFO BwaAndMarkDuplicatesPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - Deflater: IntelDeflater; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - Inflater: IntelInflater; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - Initializing engine; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.varia.NullAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312229998:4569,load,loaded,4569,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312229998,1,['load'],['loaded']
Performance,"Stack trace, not so useful... ```; 15/10/16 14:54:47 ERROR org.apache.spark.scheduler.TaskResultGetter: Could not deserialize TaskEndReason: ClassNotFound with classloader org.apache.spark.util.MutableURLClassLoader@587c290d; 15/10/16 14:54:47 ERROR org.apache.spark.scheduler.TaskResultGetter: Could not deserialize TaskEndReason: ClassNotFound with classloader org.apache.spark.util.MutableURLClassLoader@587c290d; 15/10/16 14:54:47 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 1374.2 in stage 0.0 (TID 2218, high-mem-32-4-w-14.c.genomics-pipelines.internal): UnknownReason; 15/10/16 14:54:47 WARN org.apache.spark.ThrowableSerializationWrapper: Task exception could not be deserialized; java.lang.ClassNotFoundException: htsjdk.samtools.util.RuntimeEOFException; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at java.lang.Class.forName0(Native Method); at java.lang.Class.forName(Class.java:348); at org.apache.spark.serializer.JavaDeserializationStream$$anon$1.resolveClass(JavaSerializer.scala:67); at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1613); at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1518); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1774); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:371); at org.apache.spark.ThrowableSerializationWrapper.readObject(TaskEndReason.scala:163); at sun.reflect.GeneratedMethodAccessor25.invoke(Unknown Source); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017); at java.io.ObjectInputStream.readSerialData(Obj",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1005#issuecomment-148739184:867,load,loadClass,867,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1005#issuecomment-148739184,2,['load'],['loadClass']
Performance,"Standard disclaimer up front that I'm still a bit new and can't speak to; the needs of the GATK outside the small bits I've looked at. The data I've wanted so far had all come from the UCSC genome browser,; which has a reasonable format and good documentation. My personal feeling; is there's little reason to reformat the data or choose a different format; for data we may create. For me the real problem is establishing fast and; reliable look up of that data. I've so far just thrown a couple light; weight tracts into the resources on my git branch and loaded them into an; interval tree. That storage strategy will not scale for all of the UCSC; data, and in any case might antagonize them. On Mon, Apr 30, 2018, 11:59 PM samuelklee <notifications@github.com> wrote:. > I tend to agree. VCF is perhaps a special case of the format we actually; > need, so no point in shoehorning output that doesn’t fit just for the sake; > of standardization. @sooheelee <https://github.com/sooheelee> may feel; > more strongly otherwise.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/4717#issuecomment-385593864>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AGKhCLeOf8aOnroZA0wirz8zeBoeodNsks5tt92xgaJpZM4TtIPq>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-385654418:557,load,loaded,557,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-385654418,1,['load'],['loaded']
Performance,"Starting traversal; 16:28:04.158 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 16:28:05.198 INFO GenomicsDBImport - Starting batch input file preload; 16:29:23.571 INFO GenomicsDBImport - Finished batch preload; 16:48:46.140 INFO GenomicsDBImport - Shutting down engine; [May 4, 2018 4:48:46 PM EDT] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 20.96 minutes.; Runtime.totalMemory()=22281715712; java.util.concurrent.CompletionException: java.lang.OutOfMemoryError: Java heap space; at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273); at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1592); at java.util.concurrent.CompletableFuture$AsyncSupply.exec(CompletableFuture.java:1582); at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056); at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692); at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157); Caused by: java.lang.OutOfMemoryError: Java heap space; at com.intel.genomicsdb.importer.SilentByteBufferStream.<init>(SilentByteBufferStream.java:55); at com.intel.genomicsdb.importer.GenomicsDBImporterStreamWrapper.<init>(GenomicsDBImporterStreamWrapper.java:70); at com.intel.genomicsdb.importer.GenomicsDBImporter.addBufferStream(GenomicsDBImporter.java:397); at com.intel.genomicsdb.importer.GenomicsDBImporter.addSortedVariantContextIterator(GenomicsDBImporter.java:358); at com.intel.genomicsdb.importer.GenomicsDBImporter.<init>(GenomicsDBImporter.java:167); at com.intel.genomicsdb.importer.GenomicsDBImporter.lambda$null$2(GenomicsDBImporter.java:598); at com.intel.genomicsdb.importer.GenomicsDBImporter$$Lambda$58/15335646.get(Unknown Source); at java.util.concurrent.Comple",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388079572:3937,concurren,concurrent,3937,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388079572,1,['concurren'],['concurrent']
Performance,"Status of branch before today's HaplotypeCaller hack-a-thon:. Our output is currently 97%-99% concordant with GATK3 on our test bam `CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam`:. ```; With -ERC GVCF; GATK3 total: 19910; GATK4 total: 19916; Concordant: 19882; Called in 3 but not 4: 28; Called in 4 but not 3: 34. With -ERC NONE:; GATK3 total: 1014; GATK4 total: 997; Concordant: 986; Called in 3 but not 4: 28; Called in 4 but not 3: 11; ```. We're about ~15% slower than GATK3 in GVCF mode, and more than twice as slow with -ERC NONE:. ```; GVCF MODE:; GATK3: 4.64 min; GATK4: 5.45 min. VCF MODE:; GATK3: 1.73 min; GATK4: 3.74 min; ```. The profile shows plenty of opportunities for optimization, so we should be able to achieve performance parity (in GVCF mode, at least) without too much difficulty.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1567#issuecomment-200372304:681,optimiz,optimization,681,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1567#issuecomment-200372304,2,"['optimiz', 'perform']","['optimization', 'performance']"
Performance,Step.java:48); at org.gradle.internal.execution.steps.ResolveInputChangesStep.execute(ResolveInputChangesStep.java:33); at org.gradle.internal.execution.steps.CancelExecutionStep.execute(CancelExecutionStep.java:39); at org.gradle.internal.execution.steps.TimeoutStep.executeWithoutTimeout(TimeoutStep.java:73); at org.gradle.internal.execution.steps.TimeoutStep.execute(TimeoutStep.java:54); at org.gradle.internal.execution.steps.CatchExceptionStep.execute(CatchExceptionStep.java:35); at org.gradle.internal.execution.steps.CreateOutputsStep.execute(CreateOutputsStep.java:51); at org.gradle.internal.execution.steps.SnapshotOutputsStep.execute(SnapshotOutputsStep.java:45); at org.gradle.internal.execution.steps.SnapshotOutputsStep.execute(SnapshotOutputsStep.java:31); at org.gradle.internal.execution.steps.CacheStep.executeWithoutCache(CacheStep.java:208); at org.gradle.internal.execution.steps.CacheStep.execute(CacheStep.java:70); at org.gradle.internal.execution.steps.CacheStep.execute(CacheStep.java:45); at org.gradle.internal.execution.steps.BroadcastChangingOutputsStep.execute(BroadcastChangingOutputsStep.java:49); at org.gradle.internal.execution.steps.StoreSnapshotsStep.execute(StoreSnapshotsStep.java:43); at org.gradle.internal.execution.steps.StoreSnapshotsStep.execute(StoreSnapshotsStep.java:32); at org.gradle.internal.execution.steps.RecordOutputsStep.execute(RecordOutputsStep.java:38); at org.gradle.internal.execution.steps.RecordOutputsStep.execute(RecordOutputsStep.java:24); at org.gradle.internal.execution.steps.SkipUpToDateStep.executeBecause(SkipUpToDateStep.java:96); at org.gradle.internal.execution.steps.SkipUpToDateStep.lambda$execute$0(SkipUpToDateStep.java:89); at org.gradle.internal.execution.steps.SkipUpToDateStep.execute(SkipUpToDateStep.java:54); at org.gradle.internal.execution.steps.SkipUpToDateStep.execute(SkipUpToDateStep.java:38); at org.gradle.internal.execution.steps.ResolveChangesStep.execute(ResolveChangesStep.java:76); at org.gradle.i,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6466#issuecomment-590387973:9260,Cache,CacheStep,9260,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6466#issuecomment-590387973,2,['Cache'],['CacheStep']
Performance,"Subsampling seems to be the way to go, see #2858. For the record, I did try to implement caching, but this results in excessive cache checking. In general, I think a better solution is to structure code so that expensive global quantities are not unnecessarily recomputed locally. At some point, this sort of undesirable recomputation snuck in during a refactoring of the allele-fraction likelihood code, probably when we tried to make the method for computing site likelihoods pull double duty based on the presence or absence of an allelic PoN. With an allelic PoN, we need to compute a log gamma at each site based on the site-specific bias hyperparameters; without a PoN, we only need to do this once for all sites, since the bias hyperparameters are now global, but the code naively recomputes it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2860#issuecomment-335621709:128,cache,cache,128,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2860#issuecomment-335621709,1,['cache'],['cache']
Performance,"Sure, it's a fair point that we could write this genotype-compare operation as a standalone tool. It's convenient to make it work via VariantAnnotator so that we can do it concurrently with other annotation tasks, instead of needed two passes through the VCF. I primarily ask b/c this is a downgrade from GATK3, where the equivalent of FeatureContext was passed. . Would it be reasonable for the FeatureManager to somehow get exposed? In my example I am calling FeatureContext.getValues() without supplying an interval, but I certainly could (and maybe should). If one supplies a specific query interval, the Feautrecontext isnt doing much other than providing an abstraction between FeatureManager, and the boundaries set on Featurecontext (which i agree are tricky to figure out in some cases) dont really matter.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6930#issuecomment-754256795:172,concurren,concurrently,172,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6930#issuecomment-754256795,1,['concurren'],['concurrently']
Performance,"TK BaseRecalibrator was run for each sample. The 32 recalibration tables per sample were merged into one table per sample with GATK GatherReports.; > ; > Base-recalibrated BAM files were produced with GATK ApplyBQSR in parallel over each of the 3,366 contigs in the hg 38 + alt reference genome, plus the unmapped reads. These were merged into a final BAM per sample with GATK GatherBamFiles.; > ; > SplitIntervals was used to define 3,200 evenly sized genomic intervals across hg38 + alt contigs, excluding telomeres, centromeres, unplaced contigs, unlocalized contigs, decoy and the Epstein-Barr viral sequence (Supplementary data 1). These intervals were used for scattering tasks in the Germline short variant calling and Somatic short variant calling workflows outlined below.; > ; > Germline short variant calling; > ; > HaplotypeCaller was run at the 3,200 pre-defined intervals for each sample and interval VCFs were gathered into GVCFs per sample. The following steps were then performed for the OSCC, TCGA and CSCC cohorts separately. Per sample GVCFs were consolidated with GenomicsDBImport and joint-called with GenotypeGVCFs at the 3,200 pre-defined intervals. Multi-sample interval calls were gathered using GatherVcfs. VariantFiltration and MakeSitesOnlyVcf was used to filter records with excess heterozygosity and retain only the site-level annotations. VariantRecalibrator and ApplyRecal were then applied for SNP and indels separately following GATK’s recommendations to obtain a final, filtered cohort VCF.; > ; > Somatic short variant calling; > ; > Somatic short variant calling was performed for tumour-normal pairs, first by preparing a panel of normals (PoN) for the OSCC, TCGA and CSCC cohorts separately. To create a PoN, Mutect2 was run in tumour only mode for the normal samples in the cohort at the 3,200 pre-defined intervals. GatherVcfs was used to gather interval VCFs into per sample VCFs. GenomicsDBImport and CreateSomaticPON was used to consolidate sample VCFs an",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6674#issuecomment-656499472:2694,perform,performed,2694,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6674#issuecomment-656499472,1,['perform'],['performed']
Performance,"TRACE_ON_USER_EXCEPTION=true --deploy-mode client --num-executors 59 --executor-cores 4 --executor-memory 24180M --driver-memory 10G /mnt/raid5/frankliu/code/GATK/gatk/build/libs/gatk-package-4.alpha.2-120-g00a40ea-SNAPSHOT-spark.jar CountReadsSpark -I hdfs://arlab174:54310/GATK4TEST/BroadData/CEUTrio.HiSeq.WEx.b37.NA12892.bam -O hdfs://arlab174:54310/GATK4TEST/Output/Test_CEU_ReadsCount --sparkMaster yarn; 14:56:20.999 INFO IntelGKLUtils - Trying to load Intel GKL library from:; 	jar:file:/mnt/raid5/frankliu/code/GATK/gatk/build/libs/gatk-package-4.alpha.2-120-g00a40ea-SNAPSHOT-spark.jar!/com/intel/gkl/native/libgkl_compression.so; Exception in thread ""main"" java.lang.UnsatisfiedLinkError: /tmp/frankliu/libgkl_compression8271576113600851848.so: /tmp/frankliu/libgkl_compression8271576113600851848.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64-bit platform); 	at java.lang.ClassLoader$NativeLibrary.load(Native Method); 	at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941); 	at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824); 	at java.lang.Runtime.load0(Runtime.java:809); 	at java.lang.System.load(System.java:1086); 	at com.intel.gkl.IntelGKLUtils.load(IntelGKLUtils.java:133); 	at com.intel.gkl.compression.IntelDeflater.load(IntelDeflater.java:58); 	at com.intel.gkl.compression.IntelDeflater.load(IntelDeflater.java:53); 	at com.intel.gkl.compression.IntelDeflaterFactory.<init>(IntelDeflaterFactory.java:16); 	at com.intel.gkl.compression.IntelDeflaterFactory.<init>(IntelDeflaterFactory.java:20); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:143); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.M",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2302#issuecomment-265854885:1651,load,load,1651,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2302#issuecomment-265854885,1,['load'],['load']
Performance,"Testing branch `ck_3487_port_LeftAlignAndTrimVariants`, which ports LeftAlignAndTrimVariants from GATK3 to GATK4. ### stdout; ```; WMCF9-CB5:shlee$ ./gatk LeftAlignAndTrimVariants -R ~/Documents/ref/hg38/Homo_sapiens_assembly38.fasta -V ~/Downloads/zeta_snippet_shlee/zeta_snippet.vcf.gz -O zeta_snippet_leftalign.vcf.gz; Using GATK wrapper script /Users/shlee/Documents/branches/hellbender/build/install/gatk/bin/gatk; Running:; /Users/shlee/Documents/branches/hellbender/build/install/gatk/bin/gatk LeftAlignAndTrimVariants -R /Users/shlee/Documents/ref/hg38/Homo_sapiens_assembly38.fasta -V /Users/shlee/Downloads/zeta_snippet_shlee/zeta_snippet.vcf.gz -O zeta_snippet_leftalign.vcf.gz; 16:34:35.251 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/shlee/Documents/branches/hellbender/build/install/gatk/lib/gkl-0.8.5.jar!/com/intel/gkl/native/libgkl_compression.dylib; Sep 05, 2018 4:34:35 PM shaded.cloud_nio.com.google.auth.oauth2.DefaultCredentialsProvider warnAboutProblematicCredentials; WARNING: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a ""quota exceeded"" or ""API not enabled"" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/.; 16:34:35.413 INFO LeftAlignAndTrimVariants - ------------------------------------------------------------; 16:34:35.414 INFO LeftAlignAndTrimVariants - The Genome Analysis Toolkit (GATK) v4.0.8.1-24-gb43bc27-SNAPSHOT; 16:34:35.414 INFO LeftAlignAndTrimVariants - For support and documentation go to https://software.broadinstitute.org/gatk/; 16:34:35.414 INFO LeftAlignAndTrimVariants - Executing as shlee@WMCF9-CB5 on Mac OS X v10.13.6 x86_64; 16:34:35.414 INFO LeftAlignAndTrimVariants - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_111-b14; 16:34:35.414 IN",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3487#issuecomment-418875494:730,Load,Loading,730,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3487#issuecomment-418875494,1,['Load'],['Loading']
Performance,"Testing updated branch with improved messaging.; ```; WMCF9-CB5:shlee$ ./gatk LeftAlignAndTrimVariants -R ~/Documents/ref/hg38/Homo_sapiens_assembly38.fasta -V ~/Downloads/zeta_snippet_shlee/zeta_snippet.vcf.gz -O zeta_snippet_leftalign_96branch.vcf.gz; Using GATK wrapper script /Users/shlee/Documents/branches/hellbender/build/install/gatk/bin/gatk; Running:; /Users/shlee/Documents/branches/hellbender/build/install/gatk/bin/gatk LeftAlignAndTrimVariants -R /Users/shlee/Documents/ref/hg38/Homo_sapiens_assembly38.fasta -V /Users/shlee/Downloads/zeta_snippet_shlee/zeta_snippet.vcf.gz -O zeta_snippet_leftalign_96branch.vcf.gz; 12:55:31.964 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/shlee/Documents/branches/hellbender/build/install/gatk/lib/gkl-0.8.5.jar!/com/intel/gkl/native/libgkl_compression.dylib; Sep 06, 2018 12:55:32 PM shaded.cloud_nio.com.google.auth.oauth2.DefaultCredentialsProvider warnAboutProblematicCredentials; WARNING: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a ""quota exceeded"" or ""API not enabled"" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/.; 12:55:32.083 INFO LeftAlignAndTrimVariants - ------------------------------------------------------------; 12:55:32.083 INFO LeftAlignAndTrimVariants - The Genome Analysis Toolkit (GATK) v4.0.8.1-25-g0c6f06f-SNAPSHOT; 12:55:32.083 INFO LeftAlignAndTrimVariants - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:55:32.083 INFO LeftAlignAndTrimVariants - Executing as shlee@WMCF9-CB5 on Mac OS X v10.13.6 x86_64; 12:55:32.083 INFO LeftAlignAndTrimVariants - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_111-b14; 12:55:32.083 INFO LeftAlignAndTrimVariants - Start Date/Time: September 6",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3487#issuecomment-419190326:671,Load,Loading,671,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3487#issuecomment-419190326,1,['Load'],['Loading']
Performance,"Thank you @kshakir. What I see there is that the code sets the default NIO option, and as part of this is creates a google cloud `StorageOptions` object. Sadly for us, when this object is created it determines which Google credentials to use, and if nothing was specified by the user it will send some network messages to try to figure out whether it's running on a Google Compute Engine machine. When we wrote the default-setting code we didn't realize that setting the number of retries was going to cause a network message to be sent, with the associated potential retries and delays. We can't change the way Google Compute Engine works, or how the Google authentication works either. Ideally we'd want some way to only search for credentials when we know NIO is going to be used. The point of these defaults is that they're used for anything that uses NIO, including third-party library code. We can't fully replicate this behavior in a different way from the outside. So I think the ""correct"" fix would be to go deep inside the Google NIO library and change it so that instead of providing a default configuration (that the user would have to put together, causing the problem you've seen), we can provide a *callback* that sets the configuration when the Google Cloud NIO provider is loaded. This is harder for future developers to wrap their heads around, but at least it would prevent this delay if NIO is not used. I'd like to think about this some more before doing something quite this drastic, though.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3491#issuecomment-443837504:1290,load,loaded,1290,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3491#issuecomment-443837504,1,['load'],['loaded']
Performance,"Thank you @mehrzads for your contribution. It would appear that this optimization is aimed to incorporate the knowledge that we could never possibly visit a given vertex more than K times in the first K best paths. Since the graphs are prone to exponential expansion of paths this seems like an important safeguard against this exponential expansion of the graph. . Looking at the code and the algorithm behavior it is intending to copy I see that there is a degenerate case in the current code that can cause the results to be order dependent. My belief is that this code can fall over by virtue of the fact that we refuse to make new incoming edges to a given vertex if there are already too many incoming edges for that vertex. Unfortunately this heuristic doesn't strike me as being valid, because those incoming edges can have any weight, including very high weights because they are bad paths through the graph that we created at a previous step. . I think a more correct optimization would be to limit the number of edges we create LEAVING a given vertex. The logic for this is that while we may not necessarily see all of the incoming edges in the correct weight order we will necessarily see all of the leaving edges in the correct order because those paths are pulled off of the priority queue in the correct order. Thus we can safely ignore any additional paths we see leaving a given edge because by construction as they would necessarily have at least one path that is cheaper than all of the paths leaving the current node.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5907#issuecomment-494105417:69,optimiz,optimization,69,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5907#issuecomment-494105417,3,"['optimiz', 'queue']","['optimization', 'queue']"
Performance,"Thank you for the kind explanation, *@davidbenjamin*. I understand your; rationale. BTW, in my test with a PoN of ~100 sample, this set of changes; makes significant performance improvement regardless of the value of; --genotypePonSites. Could you advise how many samples you used to create; the PoN you test? Thanks!. On Tue, Aug 8, 2017 at 9:28 PM, David Benjamin <notifications@github.com>; wrote:. > *@davidbenjamin* commented on this pull request.; > ------------------------------; >; > In src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/; > Mutect2Engine.java; > <https://github.com/broadinstitute/gatk/pull/3304#discussion_r132078537>:; >; > > }; >; > - if (hasNormal() && normalContext != null && countNonRef(refBase, normalContext) > normalContext.getBasePileup().size() * MTAC.minNormalVariantFraction) {; > + if (!MTAC.genotypePonSites && !featureContext.getValues(MTAC.pon, new SimpleInterval(context.getContig(), (int) context.getPosition(), (int) context.getPosition())).isEmpty()) {; >; > I deliberately made --genotypePonSites false by default because running; > local assembly and realignment of PoN sites is very expensive, especially; > so because PoN sites are frequently in regions that yield very messy; > assembly graphs, hence many haplotypes. It's true that explicit results can; > be useful, and we frequently want them in the course of development, but a; > tenet of the GATK is to make the tools work as well as possible with; > default settings.; >; > —; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/pull/3304#discussion_r132078537>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABD6FgXbu1QybkQOkGpGBtjNQINUx13rks5sWRk3gaJpZM4OcvPO>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3304#issuecomment-321154425:166,perform,performance,166,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3304#issuecomment-321154425,1,['perform'],['performance']
Performance,"Thank you for the speedy review @davidbenjamin. I agree with you that the obvious place to trim the alleles is in `removeAltAllelesIfTooManyGenotypes(ploidy, alleleMapper, mergedVC)` as it is the place where we actually edit the output. Indeed my first attempt at this fix was to make that change. Unfortunately, because the `readAlleleLikelihoods` object is constructed with the un-trimmed alleles in the `alleleMapper` was causing failures because the Liklihoods object would have mismatching alleles. To fix `removeAltAllelesIfTooManyGenotypes(ploidy, alleleMapper, mergedVC)` we would have to edit the alleleMapper object, which would be difficult given that I would prefer to just use the allele trimming library object. . Another proposal would have been to just hold onto the `mergedVC` object before we cull the extra alleles and then just compare the alleles at the end. Unfortunately due to engine code optimizations we have enabled an unsafe allele list copy for these alleles in the HaplotypeCaller (to save ourselves the cost of allocating dozens of identical ArrayLists to store Haplotypes every time we use the VariantContextBuilder). . To clarify, it is possible to move the check to the right place its likely to force me to write a non-library implementation of the trimming code that tracks what edits it made and I was trying to avoid doing that.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6044#issuecomment-512355693:913,optimiz,optimizations,913,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6044#issuecomment-512355693,1,['optimiz'],['optimizations']
Performance,"Thank you very much for the detailed answer, @lbergelson. I understand the point 1 and 3, but regarding 2: there is also a cost of running a Spark tool with 1 thread? I would love to use the framework for ""sparkify"" my tools, but I would like to be sure that there is no cost for running it locally without multi-thread...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2345#issuecomment-273421641:307,multi-thread,multi-thread,307,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2345#issuecomment-273421641,1,['multi-thread'],['multi-thread']
Performance,"Thank you!. On Mon, Oct 22, 2018 at 10:33 AM meganshand <notifications@github.com>; wrote:. > @SusieX <https://github.com/SusieX>; >; > Marking duplicates: I do recommend removing duplicates (we run; > MarkDuplicates from Picard).; >; > BQSR: The pipeline we're developing is for Whole Genome data, so our bams; > have gone through BQSR in the whole genome pipeline. We're using those; > recalibrated base qualities. I haven't tested running BQSR only on the; > mitochondria so I don't know how well that would work.; >; > If you do need to run BQSR only on the mitochondria I'd start by using the; > phylotree sites as --known-sites, but you'd need to have those sites in; > vcf format. Again, I haven't tested this so I don't know how well it will; > perform.; >; > If you end up using BQSR I think you're pipeline (BAM -> remove dup -> BQ; > recalibrate -> Mutect2 call -> FilterMutectCalls) is correct. Good luck!; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/pull/5193#issuecomment-431852996>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AQ4DHh9X94wZ-4488FohFKnkv1SCe6c2ks5undccgaJpZM4WqV76>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5193#issuecomment-431899805:753,perform,perform,753,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5193#issuecomment-431899805,1,['perform'],['perform']
Performance,"Thanks @bbimber. Can you run the command without the `--consolidate` option? We will be working on a `--consolidate-only` option for `gatk GenomicsDBImport` soon, but can provide you a standalone tool to perform only the consolidation later today. Is that OK?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6910#issuecomment-724313006:204,perform,perform,204,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6910#issuecomment-724313006,1,['perform'],['perform']
Performance,"Thanks @bbimber. Just tested with the files you shared and the book keeping file uncompresses and loads fine. So, no problem on that count. As you have noticed, consolidation is very resource-intensive in GenomicsDBImport. When fragments are not consolidated on-disk during import, they get consolidated in-memory during queries. In this case, it is possible that with 79 fragments and with 10MB(hardcoded currently) being used per fragment per attribute just for consolidation, with memory fragmentation and other internal buffers, we may have run out of memory. Possible solutions -; 1. Can you run with something less that 178 with java heap options `-Xmx178g -Xms178g`, so the native process gets a little more?; 2. There is no way to perform consolidate only from gatk currently. You could try importing another set with the `--consolidate` and possibly the `--bypass-feature-reader` option.; 3. I am working on a fix for consolidation for Cloud Storage mainly, it may perform a little better generally and can point to a gatk branch when created if you are interested. @mlathara, any other ideas?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1042407322:98,load,loads,98,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1042407322,3,"['load', 'perform']","['loads', 'perform']"
Performance,"Thanks @cmnbroad! I optimized the imports, now waiting for Travis.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4841#issuecomment-409585231:20,optimiz,optimized,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4841#issuecomment-409585231,1,['optimiz'],['optimized']
Performance,"Thanks @danagibbon, I may know what the issue is. `hdfs` support in GenomicsDB still relies on JVM/Java 11 and we had some workarounds with thread local caches from a while ago. I will create a branch sometime next week without `hdfs` which will hopefully get us past this issue.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8683#issuecomment-1936476061:153,cache,caches,153,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8683#issuecomment-1936476061,1,['cache'],['caches']
Performance,"Thanks @davidbenjamin for the feedback and sorry for the slow response. We have been working on improving PairHMM by adding AVX-512 (#3615) and FPGA (#2725) implementations. . We are also adding AVX2 (#3701) and AVX-512 (future PR) Smith-Waterman, which will improve the performance of Mutect2. We have the data above and will provide benchmarking results of your Mutect2 command with these improvements.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2562#issuecomment-338732871:271,perform,performance,271,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2562#issuecomment-338732871,1,['perform'],['performance']
Performance,"Thanks @davidbenjamin, I can try that out. Any other parameters or modes that you feel might be gating any of these metrics/optimizations, which should be explored jointly with the SW parameters?. I guess the same question applies for `linked-de-bruijn-graph`, which is currently marked as experimental: what would be the procedure/criteria for changing the default behavior? Hopefully, we can answer this question for the case of a binary parameter before tackling 12 parameters! In general, I'm interested in establishing clear processes so it's easier for anybody to propose improvements. If there's no clear answer just yet, I'm happy to stop at exposing these parameters, perhaps consolidating defaults to one of the current sets if that is not too disagreeable (which is just slightly more complicated than a binary decision). Don't want to rabbit hole if there's no need. Hopefully, at the least, the blog-like documentation above will provide useful pointers to anyone that might want to tackle similar efforts in the future.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-715407619:124,optimiz,optimizations,124,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-715407619,1,['optimiz'],['optimizations']
Performance,"Thanks @drifty914, it sounds like you may need to limit the number of concurrent jobs that Cromwell is allowed to scatter. We typically run gCNV in the cloud and scatter across multiple VMs, so we haven't encountered this issue before. At the same time, you could also try to reduce the total number of shards (by increasing num_intervals_per_scatter), which should be fine if each shard has enough memory. We typically scatter 200 samples x 5000 intervals, which fits comfortably in VMs with 30GB of memory. We haven't gotten a chance to profile how much of this memory is being used in detail, so you might be able to get by with much less. I don't think this is a matter of a memory leak or files being left open by the tool, as it looks like your job fails during the theano compilation step. I'll try to get an idea of how many files theano opens for each compilation, but I don't think this is something we have much control over. We have thought about whether it might be possible to reuse the same compiled theano model for identically sized shards, but haven't gotten a chance to investigate this yet either.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5714#issuecomment-468502707:70,concurren,concurrent,70,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714#issuecomment-468502707,1,['concurren'],['concurrent']
Performance,"Thanks @droazen & @ldgauthier. I can certainly run a bunch more iterations of the same HC run on the same data. I'm not super hopeful it will turn anything up though. I can also try selecting a bunch of the different PairHMM implementations. I can't share too much, but this issue turned up in a very high throughput (1000s of samples a day) clinical pipeline. We're going back and looking for other instances where we see an excess of that `Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null` message, and re-running those samples to see if, on re-run, they generate different outputs. I realize the AVX-specific hardware issue is perhaps a little far-fetched, though given the volume of the pipeline and the fact that it runs in a cloud environment, I think it's entirely reasonable to suspect we'll run into hardware/instance issues occasionally. And there are AVX or at least SIMD specific registers, so if one of those were to see problems that could cause the PairHMM issues, without causing issues in other software that doesn't leverage the SIMD/AVX instructions. My main question really is this: is anyone familiar enough with the Intel PairHMM implementation and interface that they could weigh in on whether or not unexpected hardware errors could result in the return of empty likelihoods from the PairHMM instead of some kind of error, exception or segfault?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6889#issuecomment-709555915:306,throughput,throughput,306,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6889#issuecomment-709555915,1,['throughput'],['throughput']
Performance,Thanks @gbggrant! @droazen @ldgauthier I just pushed a branch that resolves the error by simplifying the evidence-to-index cache.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6586#issuecomment-626448066:123,cache,cache,123,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6586#issuecomment-626448066,1,['cache'],['cache']
Performance,"Thanks @kdatta. The branch builds now, but there are a couple of problems that cause several tests to fail, including some existing tests that used to pass. You can see the results [here](https://travis-ci.org/broadinstitute/gatk/jobs/221534229). - The main issue is that GenomicsDB fails to load. This causes the importer tests to fail, as well as the existing GenomicsDB integration tests. (Note that the importer tests fail with a null pointer exception, but that problem is secondary and only happens when the db fails to load, which is the root problem.) We can fix the NPE in code review, for now the main issue is fix the core problem of why genomics db fails to load. - The changes in OptionalVariantInputArgumentCollection and RequiredVariantInputArgumentCollection are causing argument name collisions in other tools, which is why ExampleIntervalWalkerIntegrationTest tests are failing in this branch. The simplest fix in the short term is to just revert the changes you made to those two classes, and remove the new VariantInputArgumentCollection class. These aren't being used by the importer tool anyway. It should be pretty easy to reproduce load issue, it happens on my laptop and and travis, but let me know if you need help or have questions about any of this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-294148791:292,load,load,292,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-294148791,4,['load'],['load']
Performance,Thanks @lbergelson. I merged your changes from `lb_connect_pairhmmargs` into this PR. This works with the `gatk-protected` changes in https://github.com/broadinstitute/gatk-protected/tree/lb_connect_pairhmmargs. Here's an idea of the performance improvement when running HaplotypeCaller (HC) on 4 CPUs. | PairHMM Threads | PairHMM Time (sec) | HC Time (sec) | HC Speedup |; | --- | --- | --- | --- |; | 1 | 50.7 | 96.6 | 1x |; | 2 | 28.0 | 73.8 | 1.31x |; | 3 | 21.6 | 67.2 | 1.44x |; | 4 | 18.8 | 65.4 | 1.48x |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2259#issuecomment-261353229:234,perform,performance,234,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2259#issuecomment-261353229,1,['perform'],['performance']
Performance,"Thanks @ldgauthier , we will try that option out in few weeks. For now as a work-around for this, we are writing it to local EBS volumes which gives best performance for this job and then we move the files to the shared storage.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7646#issuecomment-1088707338:154,perform,performance,154,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7646#issuecomment-1088707338,1,['perform'],['performance']
Performance,"Thanks @magicDGS. I see the segfault is happening during PairHMM initialization when GKL is loaded. We have a Mac with the exact same OS version, but haven't been able to reproduce the error yet. Let's rule out any HW differences. Can you please post the results of this command?. ```; > sysctl -a | grep machdep.cpu.*features; machdep.cpu.features: FPU VME DE PSE TSC MSR PAE MCE CX8 APIC SEP MTRR PGE MCA CMOV PAT PSE36 CLFSH DS ACPI MMX FXSR SSE SSE2 SS HTT TM PBE SSE3 PCLMULQDQ DTES64 MON DSCPL VMX EST TM2 SSSE3 FMA CX16 TPR PDCM SSE4.1 SSE4.2 x2APIC MOVBE POPCNT AES PCID XSAVE OSXSAVE SEGLIM64 TSCTMR AVX1.0 RDRAND F16C; machdep.cpu.leaf7_features: SMEP ERMS RDWRFSGS TSC_THREAD_OFFSET BMI1 AVX2 BMI2 INVPCID FPU_CSDS; machdep.cpu.extfeatures: SYSCALL XD 1GBPAGE EM64T LAHF LZCNT RDTSCP TSCI; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1985#issuecomment-231397487:92,load,loaded,92,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1985#issuecomment-231397487,1,['load'],['loaded']
Performance,"Thanks @samuelklee , I will incorporate your conda update into this branch, now that we've dealt with the test failures!. I patched the VETS test code to include the h5diff (and diff) output in the exception messages when one of these commands fails, and switched to the existing `BaseTest` methods for running the process and capturing the output. You can see what the output looks like (when we remove the epsilon tolerance) here:. https://storage.googleapis.com/hellbender-test-logs/build_reports/8610/merge_7165443572.3/tests/testOnPackagedReleaseJar/classes/org.broadinstitute.hellbender.tools.walkers.vqsr.scalable.ScoreVariantAnnotationsIntegrationTest.html. https://storage.googleapis.com/hellbender-test-logs/build_reports/8610/merge_7165443572.3/tests/testOnPackagedReleaseJar/classes/org.broadinstitute.hellbender.tools.walkers.vqsr.scalable.TrainVariantAnnotationsModelIntegrationTest.html. As you suspected/hoped, all the differences were tiny. When you have a chance, could you please review these changes to the VETS tests and let me know if you spot any issues?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8610#issuecomment-1850563977:612,scalab,scalable,612,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8610#issuecomment-1850563977,2,['scalab'],['scalable']
Performance,"Thanks @samuelklee. I have taken a read through most of the forum posts that I could find that reference the `GermlineCNVCaller` or just `germline cnv`. I've also read through the tutorial dock, and poked around in the published WDL. I've heard good things about the results of the GATK germline CNV caller, and thus would really like to get it performing well in my own hands. I _think_ at this point I have it so that I should be able to generate calls for my 20 samples in 15-20 hours on a 96-core machine. If the results look better than the alternatives then I'll probably end up spending more time trying to figure out how best to run things to minimize runtime/cost while maintaining sensitivity. I'll keep you posted.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6166#issuecomment-532371274:345,perform,performing,345,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6166#issuecomment-532371274,1,['perform'],['performing']
Performance,"Thanks @spatel-gfb. The consolidate argument is what will give you the query performance as the consolidation happens in memory when running `GenotypeGVCFs` or `SelectVariants` otherwise. Working on the fix for the consolidation issue, hopefully will have it in a branch in a day or so. Yes, running queries from a local disk will be the fastest, but running from NFS/Lustre/Cloud Storage should be comparable.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7646#issuecomment-1039672759:77,perform,performance,77,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7646#issuecomment-1039672759,1,['perform'],['performance']
Performance,"Thanks Adam the file seems to be reading the zipped vcf file.; Thanks. Tushar B Pathare; High Performance Computing (HPC) Administrator; General Parallel File System; Scientific Computing; Bioinformatics Division; Research. Sidra Medical and Research Centre; Sidra OPC Building; PO Box 26999 | Doha, Qatar; Near QNCC,5th Floor; Office 4003 3333 ext 37443 | M +974 74793547; tpathare@sidra.orgmailto:tpathare@sidra.org | www.sidra.orghttp://www.sidra.org/. [cid:B44BCED3-0B3F-40A9-B1D5-31EB5FA1A726]. Follow us on:; [cid:EA30C7A2-88E3-4601-B54A-0AB32237D76F]https://www.facebook.com/SidraMedicalAndResearchCenter. [cid:A3C013FB-FC00-44E5-BB4A-F6BBE83393FD]https://twitter.com/sidra. [cid:F96B6BE9-8375-4FBC-98A0-F37E2C154AB5]https://www.linkedin.com/company/sidra-medical-and-research-center. [cid:C2E7A17B-1BC3-4FE4-AC2E-68CFC411450D]https://www.instagram.com/sidramedicalandresearchcenter/. [cid:6C6F39A4-C399-4742-8258-02A135D6E920]https://www.youtube.com/user/SidraMedical. [cid:6218F7A6-934F-492F-97F4-9CBACB33003F]. From: Adam Kiezun <notifications@github.com<mailto:notifications@github.com>>; Reply-To: broadinstitute/gatk <reply@reply.github.com<mailto:reply@reply.github.com>>; Date: Thursday, May 19, 2016 at 4:32 PM; To: broadinstitute/gatk <gatk@noreply.github.com<mailto:gatk@noreply.github.com>>; Cc: Tushar Pathare <tpathare@sidra.org<mailto:tpathare@sidra.org>>, Mention <mention@noreply.github.com<mailto:mention@noreply.github.com>>; Subject: Re: [broadinstitute/gatk] ./gatk-launch CountVariantsSpark --variant not reading gz vcf (#1815). @tushu1232https://github.com/tushu1232 can you retry on the latest master? It should work now. —; You are receiving this because you were mentioned.; Reply to this email directly or view it on GitHubhttps://github.com/broadinstitute/gatk/issues/1815#issuecomment-220325031. Disclaimer: This email and its attachments may be confidential and are intended solely for the use of the individual to whom it is addressed. If you are not the intended",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1815#issuecomment-221013496:94,Perform,Performance,94,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1815#issuecomment-221013496,1,['Perform'],['Performance']
Performance,"Thanks a lot for this explanation. For the moment I'm more interested in the toolkit that I'm implementing, which does not have a master script for running. It is good to know that if I implement a Spark tool it could run locally with a shadow jar. I will study a bit more about spark in the near future. So just the last question, and I will close this: why there are some tools that have a Spark version and a normal version in GATK4? If the Spark version could run locally, is there any performance issue related to run it without Spark?. Thanks a lot for all your answers, it is very informative :)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2345#issuecomment-273309729:490,perform,performance,490,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2345#issuecomment-273309729,1,['perform'],['performance']
Performance,Thanks everyone for working this out. Is there any chance we can detect this problem on load and fail rather than silently giving bad output?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3429#issuecomment-324390105:88,load,load,88,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3429#issuecomment-324390105,1,['load'],['load']
Performance,"Thanks for adding this! Incidentally, I noticed a few messages are still emitted by com.github.fommil.jni.JniLoader (which uses a different logger) in CreateReadCountPanelOfNormals when native libraries are loaded by MLlib, but probably more trouble than it's worth to clean those up. Couple of minor comments, looks fine to me but maybe engine team should chime in.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5825#issuecomment-475703103:207,load,loaded,207,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5825#issuecomment-475703103,1,['load'],['loaded']
Performance,"Thanks for all the comments! I've made fixes for the minor ones. I'd like to address the secondary and supplemental reads improvement separately (https://github.com/broadinstitute/gatk/issues/2418), so that it doesn't block progress on testing the intervals optimization. I hope this PR can go in once we have a Hadoop-BAM release (which I'll do soon). (Regarding the ""keep paired reads together"" code - as I've explained in a comment, it's pragmatic to have this code in GATK.)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2350#issuecomment-281688390:258,optimiz,optimization,258,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2350#issuecomment-281688390,1,['optimiz'],['optimization']
Performance,"Thanks for bringing this to our attention, @Tintest. I think that we may be able to address this by setting `base_compiledir` via `os.environ[""THEANO_FLAGS""]` appropriately (see http://deeplearning.net/software/theano/library/config.html). @mbabadi @cmnbroad any thoughts? . In any case, thanks for trying out the GermlineCNVCaller pipeline. You may have to tune some parameters, depending on your data type. You may find the following discussions helpful:. https://gatkforums.broadinstitute.org/gatk/discussion/11711/germlinecnvcaller-interval-merging-rule-error. https://github.com/broadinstitute/gatk/issues/4719. Note that we're still in beta, but our preliminary evaluations have demonstrated improved performance over other callers in both WES and WGS.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-390303432:358,tune,tune,358,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-390303432,2,"['perform', 'tune']","['performance', 'tune']"
Performance,"Thanks for chiming in!. @davidbenjamin can you give a little more detail on the kind of merging operations you'd need?. @tedsharpe I believe bedGraph only allows a single annotation/track. I'm not sure if the track definition line is intended to hold any metadata other than display parameters, either? https://genome.ucsc.edu/goldenPath/help/bedgraph.html. As for the unmarked column header line, the reason I decided this would be useful in the CNV TSV formats is that it's very easy to throw the table into a pandas or R dataframe for quick analysis, where you can then use the column names to manipulate the table. Typically, pandas/R TSV loading methods let you specify the `@` comment character to strip the SAM header (although we recently ran into some trouble with this in https://github.com/broadinstitute/gatk/pull/581). Note that we *require* a single unmarked column header, which is easy enough to skip (in the case you don't want to use it) if you know it's there. On the other hand, one could argue that if we store the type of each column in the metadata, then any analysis code should technically use that to parse the table (rather than letting pandas/R automatically infer the type of each column). So a marked column header line would make quick analyses a bit more difficult (as users would need to write parsing code), but could encourage more careful downstream code practices. @SHuang-Broad Just to be clear, the way I originally used ""annotation"" refers to any quantity that could be represented by a single type in a column (not in the sense of variant annotation). If string types are allowed, this is indeed pretty flexible! All I care about extracting is the common functionality related to the fact that we have locatable columns. I think the concerns you raise about e.g. SV representation in VCF are a separate matter, but happy to discuss further. I think once we decide what the header needs to be able to represent and what it should look like, this problem is most",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-480917329:643,load,loading,643,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-480917329,1,['load'],['loading']
Performance,"Thanks for comments about the documentation @LeeTL1220 - I fixed them (I hope) trying to explain the logic behind the element tracker. In principle a developer shouldn't care about when to use them, because they should come from a `LocusIteratorByState` or from inside a previous tracker. In addition, the implementation should be (most of the cases) hidden from the API user, which should use `ReadPileup`. The idea of the trackers come from GATK3, so this is a custom port with some design differences. The basic idea is to cache some operations that may be time consuming for large pileups (sorting, split by sample, extract a single sample). I actually haven't test the performance in a proper way, just running some tools in development with the branch and it feels like is faster - in my case I use all the features that are cache: split by sample, retrieving several times single-samples and also calling `fixOverlaps()` (which uses using sorted pileups). I think that because the `LocusIteratorByState` is already splitting by sample, that can improve even more performance, because it will come directly in the state where it can be used by-sample in an efficient way. And maybe, if the tool does not require to split by sample at all, we can add an option to disable that behavior while creating the tracker. Looking forward for your comments and ideas about this...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2321#issuecomment-332144484:526,cache,cache,526,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2321#issuecomment-332144484,4,"['cache', 'perform']","['cache', 'performance']"
Performance,"Thanks for looking into this, @vruano. We've not optimized CollectReadCounts for runtime or experimented with how the filters might affect performance (the latter would be easier with the FC-based evaluation in place). Can you first try to use `disable-read-filter` on the offending filters to see if that affects runtime or the resulting counts?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5233#issuecomment-425212036:49,optimiz,optimized,49,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5233#issuecomment-425212036,2,"['optimiz', 'perform']","['optimized', 'performance']"
Performance,"Thanks for opening this issue. I hope that this performance issue can be fixed in HTSJDK soon, but I agree a warning would be useful in the intervening time. . To clarify, I believe this applies to any tool that loads a VCF but does not need to parse genotypes - not just SelectVariants. For instance, I saw a 5-10x slowdown in SVAnnotate with unsorted sample IDs for a VCF with ~2500 samples. . Another note: GATK did not reorder the sample IDs in the output VCF during my tests of SVAnnotate, but did reorder IDs during SelectVariants.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7732#issuecomment-1074303682:48,perform,performance,48,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7732#issuecomment-1074303682,2,"['load', 'perform']","['loads', 'performance']"
Performance,"Thanks for pointing this out, @cxfustc. We should probably update to more recent versions of Apache Commons Math, where this appears to be fixed. We may want to check for any other performance/runtime improvements. @lbergelson what do you think?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6133#issuecomment-527147418:181,perform,performance,181,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6133#issuecomment-527147418,1,['perform'],['performance']
Performance,"Thanks for the quick answer, @davidbenjamin! By the moment is just that, but in the future I would plan to perform more complicating modeling. This is just a proof-of-concept that I'm working on as a starting point. Could it be possible to implement addition of new alleles, reads for a sample and likelihoods? Something like `add(String sample, GATKRead read, Allele allele, double likelihood)` method, which takes reads/alleles included or not in the initialization?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2185#issuecomment-250101548:107,perform,perform,107,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2185#issuecomment-250101548,1,['perform'],['perform']
Performance,"Thanks for the review @jamesemery. I have tried these changes on a cluster with an exome-sized input, with no performance issues, so I'd be comfortable merging. As you say it will be a useful basis for the HaplotypeCallerSpark work we are doing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5386#issuecomment-436634110:110,perform,performance,110,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5386#issuecomment-436634110,1,['perform'],['performance']
Performance,"Thanks for the review James. Regarding keeping Broadcast for BQSR - I'm not sure we need to. The Spark files approach is superior for several reasons: it's a lot faster (~2x), it uses a lot less memory (MB vs GB), it works progressively (conversely, if the reference is read into memory it takes a few minutes and can't be parallelized, so can't be sped up as much due to Amdahl's law), it doesn't require conversion of `fasta` reference files to `2bit`. We don't know about how this performs on single multi-core machines (in Spark local mode), but I think we can examine and optimize that later.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5127#issuecomment-416182781:484,perform,performs,484,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5127#issuecomment-416182781,2,"['optimiz', 'perform']","['optimize', 'performs']"
Performance,"Thanks for the review.; Regarding the code complexity, I definitely agree. I was trying to not do too much code surgery. For your suggestion regarding just outputting the total, perhaps if I name it something like total optical duplicates? The issue is that you could have two optical duplicate clusters, but I guess we don't really care, right? We just need the total?; Also, I can rewrite the test to load the information in as a map and then just make sure the sizes are the same and query the map. Would that be better?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/749#issuecomment-127370106:403,load,load,403,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/749#issuecomment-127370106,1,['load'],['load']
Performance,"Thanks for these questions, @tfenne, and glad you are experimenting with the workflow. Several Broad-internal groups are running various WGS and WES analyses and are seeing encouraging performance, so I’m looking forward to hearing feedback from you as well. However, I am currently indisposed and may be out for the next month or so, but @mwalker174 (who has now taken over from me as CNV tech lead, with a focus on the germline workflows) and @asmirnov239 should be able to point you in the right direction and respond to you in more detail. In the meantime, you may find some pointers in various forum posts or issues here on GitHub. We’re looking into improving documentation processes for runtime/requirements GATK-wide, which should help with this sort of thing in the future.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6166#issuecomment-532343246:185,perform,performance,185,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6166#issuecomment-532343246,1,['perform'],['performance']
Performance,"Thanks for these suggestions, @sooheelee. Note that there's already an issue filed to add `@PG` tags at https://github.com/broadinstitute/gatk/issues/4117. As for the `@RG` tag, I was following the example of the SV team, which I saw introduced a custom RG ID (although this may only be used for tagging intermediate files---not sure?) Although not ideal, I think passing the sequence dictionary and sample name in this way allows us to reuse the relevant SAM header code and also prevents the possibility of users mixing up samples. (Recall that the old pipeline required the sample name to be passed in as a separate input to each tool and that no validation that each input came from the same sample was performed.). As for VCF output, we are also planning to add this to the ModelSegments pipeline (it is already in the gCNV pipeline). See https://github.com/broadinstitute/gatk/issues/4114. However, I think it makes sense for this to wait until the improved caller is in. I'll close this as a dupe for now, but we'll be able to reference your comments here from those issues in the future.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4481#issuecomment-369986160:707,perform,performed,707,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4481#issuecomment-369986160,1,['perform'],['performed']
Performance,Thanks for your suggestion @wir963. The `--min-score-identity` and `--host-min-identity` parameters can be used to tune your desired specificity/sensitivity for microbe read detection. The default settings should guarantee that the identified microbial reads have better alignments to the microbe reference than host.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6819#issuecomment-705027510:115,tune,tune,115,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6819#issuecomment-705027510,1,['tune'],['tune']
Performance,"Thanks to @lbergelson and the magic that is git-bisect we have isolated the reproducibility bug on my toy example this commit in master: ; #5554 @davidbenjamin ; It looks like when we added that optimization to restrict the number of outgoing edges for vertexes we may have introduced non-determinism (which looks like it might stem from tie-breaking around the maxHaplotypes number output for the results. Since the priority queue has undefined behavior for tied haplotypes its not surprising that there was some non-determinism in the output, what is surprising is that this never manifested itself (apparently) until this branch. This doesn't quite explain why Google inputs should make a difference at all, that is worth investigating further. . Louis and I propose after some discussion to create an arbitrary tiebreaker for the paths, since the graph code doesn't give paths unique deterministic identifiers we could arbitrarily tiebreak based on the bases. This should only make a difference at the very fringes of the output in sites where we already discover a pathologically large number of haplotypes. Any objections @davidbenjamin @ldgauthier",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6105#issuecomment-523173394:195,optimiz,optimization,195,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6105#issuecomment-523173394,2,"['optimiz', 'queue']","['optimization', 'queue']"
Performance,"Thanks to a suggestion of mwalker174, I solved the issue. I had to specify, for each input and output file, the full path from the root. All the HPC nodes share the same hdfs, so it worked. What leaves me a bit perplexed, is that the task took 5 minutes to run on a 16 cores nodes, and exactly the same to run on a master-worker setup with 5 workers and 16 cores each (the log cofirms that the tasks were distributed to the workers IP addreses). Is this something expected? Shall I maybe try with alarger input to see the difference in performances? Thank you!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4699#issuecomment-384352326:536,perform,performances,536,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699#issuecomment-384352326,1,['perform'],['performances']
Performance,"Thanks very much for your analysis. Job 4 does create a lot of garbage, but that appears to be inevitable whenever you are dealing with a PairRDD: You have to use a Tuple2 to represent key and value rather than using a more memory-conservative custom data object. You end up with a gazillion tiny objects that survive only during the shuffle. Too bad they didn't base PairRDD on an interface like Map.Entry. Also too bad that you cannot force a shuffle on a (plain old, non-Pair) RDD. Why not just treat it as a key-only structure and allow repartitioning? I mention this not merely to whine, but also in the faint hope that you've developed some helpful workarounds. I don't think we have enough memory to persist the reads, but we can revisit that later. Job 5 *is* doing a lot of computation. It's turning each read into kmers and testing each of those kmers to see if they exist in a large hash table. I don't think there's much opportunity for further optimization -- I knew this would be a bottleneck and tried my best to make the code efficient. The skew in task size is definitely a problem, and I'll be looking for opportunities to address that issue. Thanks again.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2458#issuecomment-292230002:957,optimiz,optimization,957,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2458#issuecomment-292230002,2,"['bottleneck', 'optimiz']","['bottleneck', 'optimization']"
Performance,"Thanks! This is a good question. I had seen it too and looked into it much greater detail now. For Extract (both for VCF and PGEN) the only BigQuery activity that is being recorded is on the `sample_info` table. Each shard in a multi-sharded extract is querying the entire table to get a map of sample id to sample name (excluding withdrawn). Since every shard is running the same query (again, not sample-specific, it's the whole table!), BigQuery returns the cached result in almost all cases and so, the scanned value is 0. For the run in question, there were two shards which returned values of 54. I think they ran close enough in time that they weren't cached.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8958#issuecomment-2302656350:461,cache,cached,461,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8958#issuecomment-2302656350,2,['cache'],['cached']
Performance,"Thanks, @cmnbroad!. - You're right about gatkbase-2.1.0, that image is coming from #5026, which needs some more work. We can delete it for the time being if you think it'll cause confusion.; - Correct, I think the import statement for `reshape` in BQSR.R was always incorrect/extraneous. `reshape2` is the correct dependency for `ggplot2` (which is itself imported), and `reshape` is not explicitly used in BQSR.R. So to recap: I removed the installation of this unnecessary package, but failed to remove an unnecessary import statement since it was in an untested code path, which was then caught when users tried to run the tool. Investigation of this issue then revealed that `ggplot2` was not installed correctly in the current base image, due to a completely unrelated dependency issue.; - Good call on clearing the Travis cache. Not actually sure how to do that, do I just delete the cache at https://travis-ci.org/broadinstitute/gatk/caches for this particular branch?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5040#issuecomment-408447924:828,cache,cache,828,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5040#issuecomment-408447924,3,['cache'],"['cache', 'caches']"
Performance,"Thanks, @droazen. While I understand the effects of the funding landscape on academic resources, it seems to me this is a full capitulation of the GATK developer team given a serious bug, especially in light of the fact that the team seems to have enough resources to continue working on Mutect3. Mutect2 has been one of the best performing variant callers of the last years and is a major reason for the Broad's good reputation in the oncology bioinformatics field. GATK and Mutect2 are used by hundreds of institutions in clinical practice, affecting thousands of real patients' lives. Almost all of these institutions are likely to use clinical WES assays due to cost reasons and will thus have been directly affected by this issue _for the last three years_. Also, almost all of these institutions will never learn of this bug since they likely trusted in the developers to have proper functional regression tests in place. If this is indeed the best the Broad can do as an institution, then I will take your offer of providing a build of Mutect2 4.1.8.1 with the log4j vulnerability patched out - thank you. The one thing that I am asking for in addition (for the sake of the overall oncology bioinformatics community), however, is that you conduct a best effort to notify organizations (universities, hospitals, and biotechs/pharmaceuticals that you know are using Mutect2) and best-practise workflow owners (Nextflow, Snakemake, WDL, CWL etc. that include Mutect2) of the forced downgrade. Also, I think it makes sense to include a very prominent warning into the Mutect2 READMEs and GATK best practice documentations and guides. I know that this is work, too, but with success comes responsibility, and I can just hope that providing proper warnings uses less developer bandwidth than applying binary search to find out which of these [10 commits between 4.1.8.1 and 4.1.9.0 that are touching variant filtering (see below)](https://github.com/broadinstitute/gatk/compare/4.1.8.1...4.1.9.0) bro",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1535909226:330,perform,performing,330,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1535909226,1,['perform'],['performing']
Performance,"Thanks, I found the weights and loaded the 1D model. Would it also be possible to get the original training script that generated this model?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4511#issuecomment-375091383:32,load,loaded,32,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4511#issuecomment-375091383,1,['load'],['loaded']
Performance,"Thanks. This shows that bcast can go faster when given more RAM: going from; the earlier experimental envelope of 192GB of RAM to 256GB instead, the; bcast version can go from 19 min to 10min (sharded takes 7min with the; small amount of memory). This is consistent with my earlier experiments; that also showed that increasing the RAM-per-core sped up the bcast; computation. I wouldn't worry about having found a configuration where sharded performs; poorly. My experiments so far have shown that there are lots of; configuration points where one or the other performs poorly; the onus is on; finding the good configuration values that result in better performance, or; identifying which algorithms may be performant for a larger range of; parameter choices. To maximize understanding I would suggest that we not change too many; parameters at a time. For example it seems unnecessary to have multiple; variants of the 128Mbp input. I found there can be significant (>50%) variation between identical runs.; One way to reduce this is to set spark.task.maxFailures=1 which I strongly; recommend for all experiments going forward. One concern I have is that our cluster memory is already far larger than; the input size. In fact here each machine can fit the whole input; comfortably in RAM. This is not going to be the case for the full input.; Since it would take too long to iterate using the full input, it seems wise; instead to reduce both the input size and to keep a close eye on the amount; of memory we're using to make sure we're not going down a path that would; not be able to cope with the full input. On Wed, Nov 18, 2015 at 11:08 AM, droazen notifications@github.com wrote:. > I did some additional runs on the Broad cluster on a 14 GB bam, twice as; > large as the bam used in the plot above. This was with 60 cores, 4 cores; > per executor, and 16 GB of memory per executor. Results:; > ; > Broadcast (3 runs): 10m52.020s, 11m46.975s, 10m17.274s; > Sharded (3 runs): 19m33.310s, 13m3",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/995#issuecomment-157838734:443,perform,performs,443,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/995#issuecomment-157838734,4,['perform'],"['performance', 'performant', 'performs']"
Performance,That is quite a check to be performing for every single read just to handle this edge case....perhaps a filter would be better?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/373#issuecomment-96001344:28,perform,performing,28,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/373#issuecomment-96001344,1,['perform'],['performing']
Performance,"That sounds like a bad implementation from our side - apologies. We are working on fully fixing the Protobuf implementation. As part of that task, the temp loader/query JSON files will no longer be created (the vid and callset JSONs will still be needed).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4106#issuecomment-356986554:156,load,loader,156,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4106#issuecomment-356986554,1,['load'],['loader']
Performance,"That's a pain... Did they specify how long a path is acceptable?. On Fri, Jun 19, 2015 at 3:42 PM, JP Martin notifications@github.com wrote:. > One of our tests (BaseRecalibratorDataflow, on cloud) started failing. It; > turns out that the culprit is a Dataflow limitation. This is what I got; > back from the DF team:; > ; > _I examined logs of this pipeline on the service and in this case,; > metadata.items[3] is the pipelineOptions item, whose biggest part is; > --filesToStage, built from the classpath: it seems you have too many .jar's; > in classpath, or the jars have too long (absolute) filenames. It seems that; > you are using Gradle and all the absolute filenames point deep inside; > gradle cache directories. So, as a work-around, you could consider asking; > Gradle to build a self-contained distribution of your application, put it; > in a less deep directory, and run that._; > ; > We may run into this problem for other tests as well, so it's good to know; > about the issue.; > ; > —; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/hellbender/issues/580.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/580#issuecomment-113630079:706,cache,cache,706,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/580#issuecomment-113630079,1,['cache'],['cache']
Performance,"That's correct, @akiezun.; However, it's not just stripping out that code. There are a ton of optimizations that can then be made to the code to simplify it afterwards. These classes were made very bulky to accommodate the indel calibration, and we should really remove that bulk. I can help with that.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1056#issuecomment-152047427:94,optimiz,optimizations,94,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1056#issuecomment-152047427,1,['optimiz'],['optimizations']
Performance,"The Feature caching does not appear to be the culprit, as with the Feature cache size increased from 1000 bases to 10000 bases, hellbender still took `12m29.527s` on the `1:1-20000000` interval (though we know that turning off this caching completely has a catastrophic effect on hellbender BQSR performance, so the caching does make a big difference -- it just appears that the current default of 1000 bases worth of cached records is adequate)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1032#issuecomment-150651686:75,cache,cache,75,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1032#issuecomment-150651686,3,"['cache', 'perform']","['cache', 'cached', 'performance']"
Performance,The Hail team noticed this as well. I think Cotton told me that GDB appeared to be writing 3X more data than he expected. @nalinigans how hard would this be to optimize?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6487#issuecomment-595324294:160,optimiz,optimize,160,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6487#issuecomment-595324294,1,['optimiz'],['optimize']
Performance,The OpenMP stuff is causing problems for us already when the correct versions of the libraries are not present. We are going to back it out in master until this ticket (adding a fallback when OpenMP can't be loaded on Linux) is done.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1819#issuecomment-219843582:208,load,loaded,208,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1819#issuecomment-219843582,1,['load'],['loaded']
Performance,"The `ExtractCohortLiteFilterRecordUnitTest` appears to be blowing up b/c it's looking for a ""calibration_sensitivity"" field in the `src/test/resources/org/broadinstitute/hellbender/tools/gvs/extract/ExtractCohortLiteFilterRecord/test_input.avro` file that's not there. Unexpectedly (for VQSR Lite) there is a ""vqslod"" field. ```; $ avro-tools getschema ~/gitrepos/gatk/src/test/resources/org/broadinstitute/hellbender/tools/gvs/extract/ExtractCohortLiteFilterRecord/test_input.avro ; 23/04/11 16:22:10 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; {; ""type"" : ""record"",; ""name"" : ""Root"",; ""fields"" : [ {; ""name"" : ""location"",; ""type"" : [ ""null"", ""long"" ]; }, {; ""name"" : ""alt"",; ""type"" : [ ""null"", ""string"" ]; }, {; ""name"" : ""ref"",; ""type"" : [ ""null"", ""string"" ]; }, {; ""name"" : ""vqslod"",; ""type"" : [ ""null"", ""double"" ]; }, {; ""name"" : ""yng_status"",; ""type"" : [ ""null"", ""string"" ]; } ]; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8284#issuecomment-1504046383:540,load,load,540,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8284#issuecomment-1504046383,1,['load'],['load']
Performance,"The access pattern I saw is that the queries start small and creep up to; tens or hundreds of thousands (Order of the queried numbers). With the; exponential cache expansion, the cache quickly catches up. So it's fine.; We'll revise when needed. On Wednesday, July 6, 2016, Louis Bergelson notifications@github.com; wrote:. > @akiezun https://github.com/akiezun My only concern now is that someone; > takes log10 of a very large number triggering a massive and slow cache; > expansion. This caching scheme is good for clustered queries of small; > values, but terrible for sparse large queries. Is that a case we need to; > consider?; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/gatk/pull/1957#issuecomment-230863916,; > or mute the thread; > https://github.com/notifications/unsubscribe/AB5rL32j0zFsPo1VQ4T6gxnEHvjpYZ02ks5qS_VhgaJpZM4JCXE9; > . ## . Sent from Gmail Mobile",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1957#issuecomment-230865040:158,cache,cache,158,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1957#issuecomment-230865040,3,['cache'],['cache']
Performance,"The advantage of using SLF4J is that it is a general facade, so it makes simpler to change for one logging system to other if the bound is implemented. For the most common logging systems (log4j, jul, JLC, etc.), there are this implementation and even no-op logging. One of the nice things from slf4j is that it allows to use the logging format set by the software to every library dependency, controlling the verbosity of other libraries too. . After having a look to the gradle dependencies, it seems that ADAM and Spark use slf4j. This will allow better integration with the two libraries: now the `slf4-jdk` is completely removed, and I don't know if this will blow up at some point if some of the ADAM/Spark classes try to load them. In addition, it will make GATK4 more general. Regarding features, I'm not using more that what log4j is providing, but I'm quite familiar with logback and I have a bias to use it if possible, but the GATK framework as it is implemented now ""force"" to use log4j. But anyway, I'm happy also with using log4j and I was only suggesting this for make GATK4 more general (and to come back in my work to logback, but that is just personal taste). @lbergelson, feel free to close the issue.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2176#issuecomment-259211054:728,load,load,728,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2176#issuecomment-259211054,1,['load'],['load']
Performance,"The approach I've taken in https://github.com/broadinstitute/hellbender/compare/master...tomwhite:hadoop-references is similar to sharding, in that each worker streams the portion that it needs from HDFS. @droazen I agree that comparing with a ""broadcast everything"" approach would be valuable - I'll write some code to do that and create a PR for it. @cwhelan - it's very similar to the Hadoop distributed cache, the main difference is that Spark uses a BitTorrent-like mechanism for distributing the data, which is more efficient for the network. You still need the memory to store the data in on the nodes, which for a genome is ~3GB. But that shouldn't be too onerous with modern hardware.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/567#issuecomment-112859930:407,cache,cache,407,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/567#issuecomment-112859930,1,['cache'],['cache']
Performance,"The bottleneck is in writing a single file, since building the individual parts can be done in parallel, and the level of parallelism can be increased (on larger clusters). Writing BAMs into a single file can only be done on a local filesystem (since HDFS does not support writing to arbitrary file offsets), but I think that's a good point: single BAMs are only really needed when you want to export a BAM file. There should be no need to create a single BAM if you are going to go on to do further processing using Spark.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1174#issuecomment-158941959:4,bottleneck,bottleneck,4,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1174#issuecomment-158941959,1,['bottleneck'],['bottleneck']
Performance,The bulk of this issue was addressed in #6266. I believe @samuelklee also did some detailed profiling and determined that the memory bottlenecks were inference-related.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5716#issuecomment-926158295:133,bottleneck,bottlenecks,133,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5716#issuecomment-926158295,1,['bottleneck'],['bottlenecks']
Performance,"The change is because we started using VariantsSparkSink to write VCFs on the cluster, rather than writing them in a single thread from the driver (which doesn't scale). VariantsSparkSink only supports .bgz extensions currently, not .gz. So if you change the output extension to .bgz it will work. Gzip is not splittable so if possible we'd avoid outputting it at all. Hail for example will not load .gz unless the force option is used. (There are actually two force options, one to read it as regular gzip, the other to read it as bgzip, so you have to know which flavour of gzip it is...). We could do one of the following:. 1. Throw an error if the extension is .gz.; 2. Write bgzipped output to the .gz file.; 3. Write regular gzipped output to the .gz file. Thoughts?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3725#issuecomment-341689307:395,load,load,395,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3725#issuecomment-341689307,1,['load'],['load']
Performance,"The change is good :thumbsup:. You may also want to investigate having fewer parallel reads, so we get throttled less.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3072#issuecomment-307443641:103,throttle,throttled,103,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3072#issuecomment-307443641,1,['throttle'],['throttled']
Performance,"The code looks good, one trivial comment on it. I'm worried that it defeats the purpose of the cache though, see my comment above.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1395#issuecomment-166956674:95,cache,cache,95,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1395#issuecomment-166956674,1,['cache'],['cache']
Performance,"The command line I use is as /opt/reseq_softwares/gatk-4.1.8.0/gatk GenomicsDBImport --java-options ""-Xmx100g -Xms100g -DGATK_STACKTRACE_ON_USER_EXCEPTION=true"" -R /mnt/nvme1/reference/Gallus_gallus/Ensembl_g6a/new_20220624/Gallus_gallus.GRCg6a.dna.toplevel.fa --sample-name-map samplelist -L chr33.bed --genomicsdb-workspace-path chr33.db.The output log file is as follows，Using GATK jar /mnt/nvme1/opt/reseq_softwares/gatk-4.1.8.0/gatk-package-4.1.8.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx100g -Xms100g -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -jar /mnt/nvme1/opt/reseq_softwares/gatk-4.1.8.0/gatk-package-4.1.8.0-local.jar GenomicsDBImport -R /mnt/nvme1/reference/Gallus_gallus/Ensembl_g6a/Gallus_gallus.GRCg6a.dna.toplevel.fa --sample-name-map samplelist -L chr33.bed --genomicsdb-workspace-path chr33.db; 11:19:39.692 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/mnt/nvme1/opt/reseq_softwares/gatk-4.1.8.0/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jul 04, 2022 11:19:40 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 11:19:40.099 INFO GenomicsDBImport - ------------------------------------------------------------; 11:19:40.099 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.1.8.0; 11:19:40.100 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:19:40.100 INFO GenomicsDBImport - Executing as maosong@nygpu on Linux v3.10.0-1160.45.1.el7.x86_64 amd64; 11:19:40.100 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_312-b07; 11:19:40.100 INFO GenomicsDBImport - Start Date/Time: 2022年7月4日 上午11时19分39秒; 11:19:40.100 INFO GenomicsDBImport - ----------------------------------------",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7952#issuecomment-1196217460:998,Load,Loading,998,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7952#issuecomment-1196217460,1,['Load'],['Loading']
Performance,"The fix here is to stop Barclay from auto-expanding files with a "".list"" extension (the extension will be changed to "".args""). With this change, for the common case of a .list file, the interval merging code will see a single (file) value, which it will load directly, rather than the already expanded list of interval strings. Loading the file directly results in a single call to mergeListsBySetOperator (which re-copies the entire aggregate interval list on each call), rather one call per interval. Note that this doesn't actually change the underlying order of growth of mergeListsBySetOperator; it just minimizes the magnitude of the input size for the common .list case. The same problem would still exist if a "".args"" file was supplied as the intervals file. we may want to address that in a separate PR.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3788#issuecomment-342526178:254,load,load,254,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3788#issuecomment-342526178,2,"['Load', 'load']","['Loading', 'load']"
Performance,"The following four methods would be a great start:; double VariantContext::getAF(Allele a); int VariantContext::getAC(Allele a). double VariantContext::getMaxAF(void) //returns AF in biallelic case, probably calls getAF(allele) over all alt alleles (from existing getAlternateAlleles() fcn) in multiallelic case; potentially cache the value after we find it; int VariantContext::getMaxAC(void) //returns AC in biallelic case; Those would be the ones that go into the JEXLMap for SelectVariants. Maybe similar for minimum AF/AC. This will probably involve parsing and cacheing the AF and AC, which we don't do now. Would we want to make an array similar to List<Allele> alleles where the zeroth one is for reference? Then allele indices and AF/AC indices would line up. Maybe also something like these...; ArrayList<double> VariantContext::getAllAFs(void) //returns the cached AF list; ArrayList<int> VariantContext::getAllACs(void) //returns the cached AC list. Maybe @vdauwera has additional use cases?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/241#issuecomment-76944815:325,cache,cache,325,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/241#issuecomment-76944815,4,['cache'],"['cache', 'cached', 'cacheing']"
Performance,The idea behind this branch: make the output to readsSparkSort consistent and configurable. So that if a tool alters reads without changing their sort order then no sort will be performed by default. It also means that if you request sharded output there is the ability to ask reasSparkSource to sort the file for you.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4874#issuecomment-416339183:178,perform,performed,178,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4874#issuecomment-416339183,1,['perform'],['performed']
Performance,"The issue is definitely in htsjdk -- GATK is just calling `queryOverlapping()` a single time per file with all of the intervals, having first called `QueryInterval.optimizeIntervals()` on the interval list as required by the API.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2356#issuecomment-275424956:164,optimiz,optimizeIntervals,164,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2356#issuecomment-275424956,1,['optimiz'],['optimizeIntervals']
Performance,"The local assembly step currently in SV pipeline also uses RDD cacheing (`validateAndSaveResults` of `RunSGAViaProcessBuilderOnSpark.java`), so this would help.; Thanks, @akiezun and Lucy",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1811#issuecomment-223135473:63,cache,cacheing,63,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1811#issuecomment-223135473,1,['cache'],['cacheing']
Performance,The main issue with this task was that the query results were being limited to 100 by default. So we use the -n param now in the query. Another issue was that we were running bq show on a table variable $TABLE which is never defined.; I also changed this because the approach (returning all the samples names of the samples that have been loaded) didn't seem scalable. I wanted to only return at most the number of samples we are trying to ingest.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7470#issuecomment-921024230:339,load,loaded,339,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7470#issuecomment-921024230,2,"['load', 'scalab']","['loaded', 'scalable']"
Performance,"The memory footprint used by ReadCountCollectionUtils.parse is extremely large compared to the size of the file. I suspect there may be memory leaks in TableReader. Speed is also an issue; pandas is 4x faster (taking ~10s on my desktop) on a 300bp-bin WGS read-count file with ~11.5 million rows if all columns are read, and 10x faster if the Target name field (which is pretty much useless) is ignored. (Also, for some reason, the speed difference is much greater when running within my Ubuntu VM on my Broad laptop; what takes pandas ~15s to load takes ReadCountCollectionUtils.parse so long that I just end up killing it...not sure why this is?)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316417271:544,load,load,544,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316417271,1,['load'],['load']
Performance,The multiple output file problem is fixed in #2350. The ConcurrentModificationException issue needs more investigation.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2349#issuecomment-277660539:56,Concurren,ConcurrentModificationException,56,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349#issuecomment-277660539,1,['Concurren'],['ConcurrentModificationException']
Performance,"The original reason for this issue was to investigate why BAM files compressed with GKL level 1 DEFLATE compression are larger than BAM files compressed with JDK level 5 DEFLATE compression. Stating the obvious, level 1 compressed files are expected to be larger file than level 5 compressed files. However, GKL level 1 and 2 trade compression ratio for compression speed, so we ran some experiments on data from #4249 to investigate. The chart below shows compression speed vs. compression ratio for 11 files compressed using GKL (use-jdk-deflater = False) and JDK (use-jdk-deflater = True) for compression levels 1 through 9. The chart shows BAM files with binned quality scores can be compressed more given more CPU effort (i.e. higher compression level). The effect is the same for GKL compression and JDK compression, but the compression ratio difference is higher for GKL due to the ratio vs. speed trade off. ![compression study](https://user-images.githubusercontent.com/10476709/38428795-a5f11e4c-398a-11e8-8cfb-800b0423a95a.png). My conclusion is GATK default compression settings work well for some use models (i.e. minimizing cloud compute cost). The default settings may not be optimal for all use models, so users are free to tune the compression options to minimize their own cost function.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3413#issuecomment-379290482:1240,tune,tune,1240,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3413#issuecomment-379290482,1,['tune'],['tune']
Performance,"The plot above shows the results of running our four implementations of BQSR (the walker version, the shuffle version, the broadcast version, and the ""manual sharding"" version) on a ~7 GB bam with ~250 MB vcf and full size (~3 GB) reference (though I used the compressed `.2bit` version of the reference for the broadcast tests, which came in at ~750 MB). -The single-threaded walker version running on a Broad server with inputs over NFS took ~45 minutes to process the file (this is the hard-to-see point in the upper left of the plot). -The shuffle version on 96 cores (3 executors with 32 cores each) took ~38 minutes (this is the hard-to-see point towards the upper right of the plot). -The broadcast version with 4 cores per executor (the BROADCAST_4 line) scaled reasonably well up to 32 total cores, then got slower after that. I tried increasing the number of cores per executor to 16 and 32 (these are the BROADCAST_16 and BROADCAST_32 lines respectively), but this didn't help. I suspect that the broadcast approach comes with a fixed amount of overhead that dominates the runtime on a small input, and by increasing the size of the bam input we may see it scale past 32 cores. -The ""manual sharding"" version (aka ""BaseRecalibratorSparkOptimized"") performed the best, despite having to read the bam from a GCS bucket rather than from the local HDFS like the other implementations, but also didn't scale very well past 32 cores. Tried increasing the number of cores per executor here as well from 4 to 32 (the SHARDING_4 and SHARDING_32 lines), but it made no significant difference.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/995#issuecomment-152317495:1259,perform,performed,1259,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/995#issuecomment-152317495,1,['perform'],['performed']
Performance,The solution is definitely not to run Mutect2 in GVCF mode. It's too different from VCF mode and has a big performance cost.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6744#issuecomment-677022081:107,perform,performance,107,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6744#issuecomment-677022081,1,['perform'],['performance']
Performance,"The thing about logarithms is that consistency between processors sacrifices performance because the `Math` library uses on-hardware implementations when possible. . I think the problem here is really our bad criterion for choosing the best haplotypes. I bet the calls would be the same, perhaps with tiny differences in QUALs, using the `--linked-de-bruijn-graph` argument to circumvent the `KBestHaplotypeFinder`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8338#issuecomment-1587997729:77,perform,performance,77,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8338#issuecomment-1587997729,1,['perform'],['performance']
Performance,"The two main resources limiting how many gVCFs you can import at once will be memory and file handles. . I'm not sure if you mean incremental import or batch size when you mention the iterative approach. I assume the latter, but just want to clarify that there isn't any reason to break up the import using incremental import. The batch size parameter effectively does that, so you might as well import all gVCFs you have available (optionally using batch size if you're running out of memory). I'll throw out batch sizes of 50 or 100 as being reasonable, but the size of the intervals being imported will make a difference. It would be best to try to monitor how much memory is being used with those settings. There won't be a huge difference in import performance depending on the number of batches (ignoring memory issues) but if you have more than a 100 or so batches and you don't enable the `--consolidate` option you may see some query performance degradation.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6688#issuecomment-656376952:754,perform,performance,754,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6688#issuecomment-656376952,2,['perform'],['performance']
Performance,"The underlying issue here is is that the GATK conda env environment isn't established since bioconda doesn't appear to configure it. The NPE needs is fixed by #7816. In this particular case it appears that some of the requirements are satisfied, since the code gets past the initial check to see if the GATK python code is available. But then the actual CNN code can't be loaded.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7811#issuecomment-1110010269:372,load,loaded,372,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7811#issuecomment-1110010269,1,['load'],['loaded']
Performance,"Theory from @cmnbroad is below:. ```; I think this is happening because were trying to serialize the class loader sun.misc.Launcher$AppClassLoader), which appears to be reached through the graph by way of via https://github.com/damiencarol/jsr203-hadoop/blob/master/src/main/java/hdfs/jsr203/HadoopFileSystem.java#L82. We probably need to short circuit that with a custom serializer for one of these:. Serialization trace:; classes (sun.misc.Launcher$AppClassLoader); classLoader (org.apache.hadoop.conf.Configuration); conf (org.apache.hadoop.hdfs.DistributedFileSystem); fs (hdfs.jsr203.HadoopFileSystem); hdfs (hdfs.jsr203.HadoopPath); path (htsjdk.samtools.seekablestream.SeekablePathStream); seekableStream (htsjdk.tribble.TribbleIndexedFeatureReader); featureReader (org.broadinstitute.hellbender.engine.FeatureDataSource); featureSources (org.broadinstitute.hellbender.engine.FeatureManager). See, for instance, dbpedia/distributed-extraction-framework#9.; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6730#issuecomment-671508579:107,load,loader,107,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6730#issuecomment-671508579,1,['load'],['loader']
Performance,There are a few other performance fixes for SelectVariants coming in the next release of GATK as well. We discovered a number of really pathologically cases that we are fixing bu haven't released yet.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6446#issuecomment-680118909:22,perform,performance,22,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6446#issuecomment-680118909,1,['perform'],['performance']
Performance,"There are currently two failing tests, both of which need fixes in htsjdk.; * CountVariantsSparkIntegrationTest. This is a concurrency issue where VCFCodec (which isn't threadsafe) is being shared by multiple tasks in each Spark executor. The fix is for each task (partition) to have its own VCFCodec. This needs a couple of small changes in htsjdk to make it possible to access the codec's version and header fields so the codec can be recreated. See https://github.com/samtools/htsjdk/pull/1176.; * CpxVariantReInterpreterSparkIntegrationTest. I tracked this down to a problem with the buffer in htsjdk's SeekableBufferedStream. See https://github.com/samtools/htsjdk/pull/1175 for the fix.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5138#issuecomment-417241035:123,concurren,concurrency,123,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5138#issuecomment-417241035,1,['concurren'],['concurrency']
Performance,There are no plans to implement Queue-style scatter-gather annotations in hellbender currently -- closing.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/320#issuecomment-169114137:32,Queue,Queue-style,32,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/320#issuecomment-169114137,1,['Queue'],['Queue-style']
Performance,There aren't any plans to add the module functionality from `VariantEval` to the Picard tool. It would almost certainly be easier to do a straightforward port of `VariantEval` to GATK4 than to try to perform major surgery on `CollectVariantCallingMetrics` in Picard.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/616#issuecomment-320765151:200,perform,perform,200,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/616#issuecomment-320765151,1,['perform'],['perform']
Performance,"There have been recent problems in GATK/Queue related to unmapped reads and intervals. Not sure if that's relevant here, but it may be worth monitoring how that's resolved.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/166#issuecomment-73307229:40,Queue,Queue,40,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/166#issuecomment-73307229,1,['Queue'],['Queue']
Performance,"There is still more that could be done to improve performance, we take 18 minutes to do what I'm told took 12 in gatk3, but I'm not sure it's worth it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6672#issuecomment-648227914:50,perform,performance,50,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6672#issuecomment-648227914,1,['perform'],['performance']
Performance,"There was a confirmed performance regression in `BaseRecalibratorSpark` just before the 4.0 release: https://github.com/broadinstitute/gatk/issues/4376. I suspect that's what's responsible for the results reported above. Until this is patched (most likely in the next GATK release), I'd recommend running the regular `BaseRecalibrator` instead of the spark version.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4300#issuecomment-365991967:22,perform,performance,22,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4300#issuecomment-365991967,1,['perform'],['performance']
Performance,"There was an issue with the compressed write of book-keeping files. The import would seem to have succeeded, but the issue would show up during queries sometimes. This was fixed in `4.2.3.0` - see https://github.com/broadinstitute/gatk/pull/7520. If you did use `4.2.3.0`, it should not have been a problem. . Yes please, can you arrange to share the `_book_keeping.tdb.gz` file? Can you also share the `__array_schema.tdb` file if possible. Also, how many fragments(folders starting with __) do you see in /home/exacloud/gscratch/prime-seq/cachedData/16b9ede7-6db8-103a-9262-f8f3fc86a851/WGS_Feb22_1852.gdb/1$1$223616942, there may be multiple if you did not use the `--consolidate` option during import?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1042059426:541,cache,cachedData,541,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1042059426,1,['cache'],['cachedData']
Performance,"These all sound like positive improvements. Provided they don't affect performance by dramatically increasing the number of discovered haplotypes, I'm on board. Hopefully this will go a long way towards removing the dependence of calling on the active region boundaries.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3697#issuecomment-447012273:71,perform,performance,71,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3697#issuecomment-447012273,1,['perform'],['performance']
Performance,"These changes look good to me in terms of reverting to GATK3-matching results. On a separate note though, the optimizations on my branch are largely aimed at reducing unnecessary allocations and boxing, since this (GMM expectation step) is the main hotspot in VQSR, and we iterate through it millions of times. That may conflict with the use of streams as introduced in these PRs (I don't think they introduce any boxing since they use DoubleStreams, but may still result in the same number of array allocation(s) that were in the original code). When I merge in my changes I'll do some more profiling. The stream code certainly looks nicer so it would be nice if we can keep it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2241#issuecomment-257578991:110,optimiz,optimizations,110,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2241#issuecomment-257578991,1,['optimiz'],['optimizations']
Performance,These now seem to be working on both firecloud and cromwell with intel-optimized tensorflow and t tranche-filtering. Back to you @ldgauthier.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4774#issuecomment-404210394:71,optimiz,optimized,71,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4774#issuecomment-404210394,1,['optimiz'],['optimized']
Performance,"They are retried. You're seeing this message because it failed more than `maxChannelReopens` times. The new version which you really want to use for any test at this point is described there:; https://github.com/broadinstitute/gatk/issues/2685#issuecomment-302798685. Among other things, this new version puts in a message about 'retry failed' when it runs out of retries to eliminate the very confusion that you ran into. GenomicsDBImport opens a large number of parallel connections and as a result is getting throttled fairly heavily (by the host GCE machine if nothing else). This results in timeouts and dropped connections. One way forward is to increase the retry delays, another is to find a way to do the same work with fewer parallel open connections.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2749#issuecomment-304123753:512,throttle,throttled,512,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2749#issuecomment-304123753,1,['throttle'],['throttled']
Performance,"They discuss that add more ram.; I try with 8 cpu and 500 Go of RAM, but still not working. Error for one bam file:. ```. 15:47:36.554 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/genouest/uni_limoges_fr/jpollet/.conda/envs/myd88/share/gatk4-4.1.4.0-1/gatk-package-4.1.4.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Nov 28, 2019 3:47:37 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 15:47:37.239 INFO Mutect2 - ------------------------------------------------------------; 15:47:37.240 INFO Mutect2 - The Genome Analysis Toolkit (GATK) v4.1.4.0; 15:47:37.240 INFO Mutect2 - For support and documentation go to https://software.broadinstitute.org/gatk/; 15:47:37.240 INFO Mutect2 - Executing as jpollet@cl1n031.genouest.org on Linux v3.10.0-693.21.1.el7.x86_64 amd64; 15:47:37.240 INFO Mutect2 - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_192-b01; 15:47:37.246 INFO Mutect2 - Start Date/Time: 28 novembre 2019 15:47:36 CET; 15:47:37.246 INFO Mutect2 - ------------------------------------------------------------; 15:47:37.246 INFO Mutect2 - ------------------------------------------------------------; 15:47:37.246 INFO Mutect2 - HTSJDK Version: 2.20.3; 15:47:37.246 INFO Mutect2 - Picard Version: 2.21.1; 15:47:37.247 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 15:47:37.247 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 15:47:37.247 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 15:47:37.247 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 15:47:37.247 INFO Mutect2 - Deflater: IntelDeflater; 15:47:37.247 INFO Mutect2 - Inflater: IntelInflater; 15:47:37.247 INFO Mutect2 - GCS max retries/reopens: 20; 15:47:37.247 INFO Mutect2 - Requester pays: disabled; 15:47:37.247 INFO Mutect2 - Initializing engine; 15:47:41.204 INFO Mutect2 - Done initializi",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6271#issuecomment-559553558:162,Load,Loading,162,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6271#issuecomment-559553558,1,['Load'],['Loading']
Performance,"Things left for later:; * `GenotypeIndexCalculator` sometimes interacts with primitive arrays, sometimes with `GenotypeAlleleCounts`; * `GenotypeLikelihoodCalculator` has extraneous responsibilities and doesn't interact with `GenotypeAlleleCounts` as well as it should.; * `alleleCountsToIndex(final GenotypeAlleleCounts newGAC, final int[] newToOldAlleleMap)` in `GenotypeIndexCalculator` needs refactoring.; * `GenotypeLikelihoodCalculators` is really just a cache of `GenotypeAlleleCounts`.; * `GenotypeAlleleCounts` has some unused and barely-used methods, and it precomputes a lot of quantities that are not often needed and could be computed on-the-fly without difficulty or expense.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1066400217:461,cache,cache,461,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1066400217,1,['cache'],['cache']
Performance,"This change degrades performance on Spark, so should not be merged.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3533#issuecomment-326366040:21,perform,performance,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3533#issuecomment-326366040,1,['perform'],['performance']
Performance,"This error message is related to GATK's ability to load files on Google buckets (""gcs://bucket/file.bam""). This ability is enabled even when running locally (this aspect is intentional, because it's useful to be able to run a local GATK instance to process remote data without having to fire up a VM). As the bucket-reading code (""NIO"") initializes, it looks for credentials to use. Those can be set via an environment variable or via `gcloud auth`, as described in GATK's README. If neither of these are set, it checks whether it's currently running in a Google virtual machine (so it can figure out who owns the virtual machine that it's running on, and use those credentials). Apparently this code throws an exception if it runs out of ways to find credentials, and our code prints it out and moves on. The message is useful, for if we *were* running in a google VM and the credential-finding failed, we'd certainly like to know. Whether we need the full stack trace, now, that's a choice we have to make.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4369#issuecomment-424038095:51,load,load,51,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4369#issuecomment-424038095,1,['load'],['load']
Performance,"This is a serious issue. Here's another error I've seen when running with `--sparkRunner SPARK` and `--sparkMaster local[*]`:. ```; ./gatk-launch PrintReadsSpark -I src/test/resources/org/broadinstitute/hellbender/tools/flag_stat.bam -O foo.bam -- --sparkRunner SPARK --sparkMaster local[*]. org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost): ; java.lang.ClassCastException: htsjdk.samtools.SAMFileHeader cannot be cast to htsjdk.samtools.SAMFileHeader; at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.lambda$writeReadsSingle$4e0a7f18$1(ReadsSparkSink.java:186); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink$$Lambda$86/1825278638.call(Unknown Source); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:153); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:153); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:686); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:686); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277); at org.apache.spark.rdd.RDD.iterator(RDD.scala:244); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277); at org.apache.spark.rdd.RDD.iterator(RDD.scala:244); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63); at org.apache.spark.scheduler.Task.run(Task.scala:70); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1315#issuecomment-164085312:1794,concurren,concurrent,1794,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1315#issuecomment-164085312,2,['concurren'],['concurrent']
Performance,"This is because GATK3 creates sequence dictionaries from VCF indices, and when an index is not present it creates one on-the-fly (see `RMDTrackBuilder.getFeatureSource()`). We could do the same in GATK4 (though we may not want to index vcfs on-the-fly if they're unindexed, as that is a source of race conditions -- safer to throw and make the user index the files).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1029#issuecomment-156497237:297,race condition,race conditions,297,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1029#issuecomment-156497237,1,['race condition'],['race conditions']
Performance,This is easily resolvable without performance profiling since the only remaining use of the `IntervalSkipList` was in `ContextShard` which is now also unused / untested. Removing context shard and rebasing.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4154#issuecomment-461935085:34,perform,performance,34,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4154#issuecomment-461935085,1,['perform'],['performance']
Performance,"This is essentially the same issue as https://github.com/broadinstitute/gatk/issues/1116, in which we aspired to find a more generalized solution. . Is the performance hit for doing automatically this really too much, if we only do it when writing to a newer version than the one we read ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1483#issuecomment-190431761:156,perform,performance,156,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1483#issuecomment-190431761,1,['perform'],['performance']
Performance,"This is great! I'm glad you're making the tools we need to dig into performance. A few things:; (1) Have you tested this? If so, how?; (2) Is there any issues with BunnyLog not being Serializable? It is used within processElement, so I'd expect it'd need to be.; (3) Please add some unit tests. I was thinking something like a fake program that just had delays (sleep) and verifying that the times produced were at least those you'd expect with the delays.; (4) I expect that this will eventually be paired with a logs analysis script. Do you have specific plans yet? I'd love to see a doc explaining what logging you'd like to see and how you think we should analyze it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/628#issuecomment-119945250:68,perform,performance,68,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/628#issuecomment-119945250,1,['perform'],['performance']
Performance,"This is ready for review again, @droazen. I split the walkers in two: `ReadSliderWalker` and `VariantSliderWalker`, both based on `ReadWalker` and `VariantWalker` to traverse in a sliding-window approach. I also implemented some `ArgumentCollectionDefinition` for the three parameters (window-size, window-step and window-padding). There are some changes that I would like to discuss with you:; - `ShardSource<T>`: a generic class for lazily load sources of certain type. Now `ReadShard` extends this class, but there is actually no change in the definitions. I wonder if it is worthy to maintain `ReadShard` as a separate class, because it seems that the only usage is in `AssemblyRegionWalker` and it could be easily changed by a `ShardSource<GATKRead>`.; - `FilteringIterator<T>`: another generic class for filter on iteration records. Now `ReadFilteringIterator` extends this class, but again I don't find any usage that requires an specific case instead of a generic.; - There is some repetition in the test code: `ShardSourceUnitTest` and `ReadShardUnitTest` are exactly the same. I didn't want to remove the later, but I think that it is redundant; the same for the `FilteringIteratorUnitTest` adn `ReadFilteringIterator.; - Because of the inclusion of `ArgumentCollectionDefinition`s for window-related parameters, I just want to know if I should include them in `AssemblyRegionWalker` to maintain consistency in the user options. Back to you @droazen, and thanks again for all the help.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-224348995:442,load,load,442,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-224348995,1,['load'],['load']
Performance,This is redundant with https://github.com/broadinstitute/gatk/issues/1609 because https://github.com/broadinstitute/gatk/issues/1609 will run spark tools on hdfs for performance. Closing this one.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1402#issuecomment-226808573:166,perform,performance,166,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1402#issuecomment-226808573,1,['perform'],['performance']
Performance,"This is still throwing errors. Importing `chr2	6861126	.	ACCCACACAC	*,<NON_REF>	2.22	.	AS_RAW_BaseQRankSum=|||;AS_RAW_MQ=0.00|0.00|10800.00|0.00;AS_RAW_MQRankSum=|||;AS_RAW_ReadPosRankSum=|||;AS_SB_TABLE=0,0|0,0|3,0|0,0;DP=6;QUALapprox=126;RAW_MQandDP=21600,6;VarDP=3	GT:AD:DP:GQ:PL:SB	1/1:0,3,0:3:10:126,10,0,129,18,168:0,0,3,0`; for interval -L chr2:6861134-7268940 produces the log; ```; 11:11:14.566 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 11:11:15.838 INFO GenomicsDBImport - Importing batch 1 with 1 samples; libc++abi.dylib: terminating with uncaught exception of type LoadOperatorException: LoadOperatorException : Found cell [ 0, [ 255817547, 255817547 ] ] that does not belong to TileDB/GenomicsDB partition with row_bounds [ 0, 0 ] column_bounds [ 255817555, 256225361 ]; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5449#issuecomment-484558863:621,Load,LoadOperatorException,621,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5449#issuecomment-484558863,2,['Load'],['LoadOperatorException']
Performance,"This looks OK to me. If I understand correctly, the decaying probability to stay in a class/state now only applies to the silent class and the baseline copy-number state?. @asmirnov239 Let's try to do a quick run on GPC2 to see how performance changes. You might want to sweep `p-alt`, `p-active`, and `cnv-coherence-length` again, since I don't think we should expect the optimal parameters to be the same across both HMMs; perhaps just sample the corners of the hypercube. Then let's rerun a SFARI cohort with 1) the filtered target list, 2) this HMM change, and 3) both, using optimal parameters.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4988#issuecomment-403460454:232,perform,performance,232,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4988#issuecomment-403460454,1,['perform'],['performance']
Performance,"This looks promising, at a minimum we should try setting up the docker with hadoop native libraries so the performance gains can be extended to most use cases. This might also include adding some magic to the gatk launch script inside the docker to detect and run with the correct version of the hadoop libraries.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4746#issuecomment-387519906:107,perform,performance,107,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4746#issuecomment-387519906,1,['perform'],['performance']
Performance,This needs a fix in the spark loading code. We need to explicitly account for empty shards when doing the shuffle of the first readname to the shard to the left it.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6709#issuecomment-662564530:30,load,loading,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6709#issuecomment-662564530,1,['load'],['loading']
Performance,"This same issue is coming up in a number of different ways. What's happening is that we ask for the `FileSystem` and then try to open a path using whatever it gives back. The `FileSystem` it returns only knows how to open 1 type of file, either file:/// or hdfs:///. We could specify a specific file system, but I'm afraid that there will be nasty consequences from that that I don't understand... like getting the wrong hdfs block size or something like that. ```; FileSystem fs = FileSystem.get(ctx.hadoopConfiguration());; Path path = fs.makeQualified(new Path(filePath));; if (fs.isDirectory(path)) {; FileStatus[] bamFiles = fs.listStatus(path, new PathFilter() {; private static final long serialVersionUID = 1L;; @Override; public boolean accept(Path path) {; return path.getName().startsWith(HADOOP_PART_PREFIX);; }; });; if (bamFiles.length == 0) {; throw new UserException(""No BAM files to load header from in: "" + path);; }; path = bamFiles[0].getPath(); // Hadoop-BAM writes the same header to each shard, so use the first one; }; return SAMHeaderReader.readSAMHeaderFrom(path, ctx.hadoopConfiguration());; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1417#issuecomment-169459948:900,load,load,900,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1417#issuecomment-169459948,1,['load'],['load']
Performance,"This should add support for reading bam files with CSI indexes, as well as porting the FastaReferenceWriter to htsjdk and a lot of other changes to htsjdk. @samuelklee This includes the overlap detector optimizations you wanted as well as the changes to let you write interval file to paths.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5812#issuecomment-474098651:203,optimiz,optimizations,203,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5812#issuecomment-474098651,1,['optimiz'],['optimizations']
Performance,This should be resolved in GATK 4.1.8.0 with the new `--genomicsdb-shared-posixfs-optimizations` argument -- closing.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6321#issuecomment-651285797:82,optimiz,optimizations,82,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6321#issuecomment-651285797,1,['optimiz'],['optimizations']
Performance,"This sounds like a good idea, but it might be tricky because much of the file reading is likely done by native code that we're just wrapping. We could do something like automatically downloading and caching the file locally and handing the cached version to the native library. Would that suit your needs?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3178#issuecomment-314147710:240,cache,cached,240,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3178#issuecomment-314147710,1,['cache'],['cached']
Performance,This ticket is just a meta-ticket that was used to spawn additional performance-related tickets for spark -- closing.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/986#issuecomment-157449284:68,perform,performance-related,68,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/986#issuecomment-157449284,1,['perform'],['performance-related']
Performance,"This will definitely not be part of this PR, but I wonder if you'd be willing to comment on this: it would be quite helpful if I could get VariantEval to run in a scatter/gather mode, in which I submit jobs to the cluster, each saves results for an internal and then I combine them. One way to make this work would be for VariantEvalEngine to save the StratificationManager (or some version of this) to disk, perhaps serialized to JSON with Jackson. There would need to be a separate tool that takes N serialized StratificationManager objects, loads then, merges, and then writes the actual report. It's not completely implemented, but there is already a concept of combineStrats() in StratificationManager. It doesnt work complete, but this doesnt seem all that far from being able to take two StratificationManager objects and returning the merged form. Is this a reasonable idea?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-750694321:544,load,loads,544,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-750694321,1,['load'],['loads']
Performance,"To add some context, the test file was a file with a single no-call with basically no information at each position in the genome. A very weird (but valid) input. The problem was that we were regenerating the sequence dictionary on every site which is an expensive operation. Caching the sequence dictionary at the beginning improved the speed by 2 orders of magnitude on this file. . There are probably additional performance optimizations available for data like this. We spend a lot of time in overhead of creating empty AlleleLikelihoods objects and things like that. I don't think it's worth pursuing at the moment though.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6663#issuecomment-648232849:414,perform,performance,414,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6663#issuecomment-648232849,2,"['optimiz', 'perform']","['optimizations', 'performance']"
Performance,"To clarify our experimental setup: for the benchmark we are using the latest release (v.1.2) somatic ""ground truth"" of the HCC1395 cell line from . [https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/seqc/Somatic_Mutation_WG/release/latest/](https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/seqc/Somatic_Mutation_WG/release/latest/). It contains ~40k SNVs and ~2k INDELs in ~2.4Gb high-confidence regions.; In high-confidence regions intersected with WES bed, we still have ~1.1k SNVs and ~100 INDELs. Therefore, even in the WES analysis scenario, the SNV counts should be high enough to draw reliable conclusions when comparing performance between different callers and releases.; For WES INDELs, the counts are indeed rather low and results should be interpreted with caution.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1172101032:633,perform,performance,633,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1172101032,1,['perform'],['performance']
Performance,To ensure that our native libraries are all loaded in a consistent way. For @lbergelson,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2408#issuecomment-279831623:44,load,loaded,44,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2408#issuecomment-279831623,1,['load'],['loaded']
Performance,To help with shuffle performance / memory usage.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1687#issuecomment-205976930:21,perform,performance,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1687#issuecomment-205976930,1,['perform'],['performance']
Performance,UkhpZGRlblN0YXRlLmphdmE=) | `100% <100%> (ø)` | `4 <1> (+1)` | :arrow_up: |; | [...tools/exome/segmentation/ClusteringGenomicHMM.java](https://codecov.io/gh/broadinstitute/gatk/pull/3036?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9zZWdtZW50YXRpb24vQ2x1c3RlcmluZ0dlbm9taWNITU0uamF2YQ==) | `100% <100%> (ø)` | `14 <6> (+1)` | :arrow_up: |; | [...a/org/broadinstitute/hellbender/utils/hmm/HMM.java](https://codecov.io/gh/broadinstitute/gatk/pull/3036?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9obW0vSE1NLmphdmE=) | `38.462% <100%> (+11.189%)` | `7 <3> (+3)` | :arrow_up: |; | [...hellbender/utils/hmm/ForwardBackwardAlgorithm.java](https://codecov.io/gh/broadinstitute/gatk/pull/3036?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9obW0vRm9yd2FyZEJhY2t3YXJkQWxnb3JpdGhtLmphdmE=) | `87.766% <100%> (-0.469%)` | `13 <0> (ø)` | |; | [...lbender/tools/exome/segmentation/CopyRatioHMM.java](https://codecov.io/gh/broadinstitute/gatk/pull/3036?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9zZWdtZW50YXRpb24vQ29weVJhdGlvSE1NLmphdmE=) | `100% <100%> (ø)` | `5 <1> (ø)` | :arrow_down: |; | [...egmentation/PerformAlleleFractionSegmentation.java](https://codecov.io/gh/broadinstitute/gatk/pull/3036?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9zZWdtZW50YXRpb24vUGVyZm9ybUFsbGVsZUZyYWN0aW9uU2VnbWVudGF0aW9uLmphdmE=) | `88.889% <100%> (ø)` | `4 <0> (+2)` | :arrow_up: |; | [...r/tools/exome/segmentation/JointAFCRSegmenter.java](https://codecov.io/gh/broadinstitute/gatk/pull/3036?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9zZWdtZW50YXRpb24vSm9pbnRBRkNSU2VnbWVudGVyLmphdmE=) | `100% <100%> (+0.99%)` | `32 <5> (ø)` | :arrow_down: |; | ... and [103 more](https://codecov.io/gh/broadinstitute/gatk/pull/3036?src=pr&el=tree-more) | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3036#issuecomment-306513201:3334,Perform,PerformAlleleFractionSegmentation,3334,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3036#issuecomment-306513201,1,['Perform'],['PerformAlleleFractionSegmentation']
Performance,"Update:. 1000 samples on one interval, 11GB Xmx to the JVM:; 20 runs for each parameter set. - batchSize at **200** and buffering **disabled** yields: 25% success and 75% OOM; Runtime (when it succeed) is ~ 1.75x longer than batchSize 100 (with or without buffer); ; - batchSize at 100: ; Tried on both OpenJDK8 and Oracle JDK, with buffer disabled and default.; Overall similar performances. Got 2 failures over the 80 total runs:; - One SSL handshake on the ""default buffer OpenJDK""; - One `com.google.cloud.storage.StorageException: 503 Service Unavailable` on the ""buffer disabled OracleJDK""",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-301153727:379,perform,performances,379,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-301153727,1,['perform'],['performances']
Performance,"Update:; - [ ] A cool name! (Marduk, Clarendon, NeuroVar) ? maybe I'll do a slack poll of dsde methods?; - [x] Model training script (in Python, eventually in Java); - [x] Pretrained model for WGS; - [x] Pretrained model for WEx (still being validated and was only trained on NA12878); - [x] Model inference and VCF annotation (in Java); - [x] Solution for applying filters based on CNN score cutoff (tranches.py script); - [ ] Alternate joint calling WDL? Or for re-filtering? (ideally with a $$$ estimate); - [ ] Performance optimizations?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4225#issuecomment-363436039:515,Perform,Performance,515,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4225#issuecomment-363436039,2,"['Perform', 'optimiz']","['Performance', 'optimizations']"
Performance,"Update:; The problem seemingly was caused by the logging part in Spark (thanks to Carlos H. Andrade Costa from IBM for pointing this out). So a temporary solution that works for now is to add the following lines in the initialization script for spinning up the cluster to turn off logging, . ```bash; sudo sed -i -- 's/log4j.rootCategory=INFO, console/log4j.rootCategory=OFF, console/g'\; /etc/spark/conf/log4j.properties; sudo sed -i -- 's/log4j.logger.org.apache.spark=WARN/log4j.logger.org.apache.spark=OFF/g'\; /etc/spark/conf/log4j.properties; sudo sed -i -- 's/hadoop.root.logger=INFO,console/hadoop.root.logger=OFF,console/g'\; /etc/hadoop/conf/log4j.properties; ```. assuming the cluster was created with . --image-version preview. I test run the tool where I observed the bug (`FindBreakpointEvidenceSpark`) twice on a cluster created & initialized this way and didn't observe any `ConcurrentModificationException`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2277#issuecomment-262640914:891,Concurren,ConcurrentModificationException,891,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2277#issuecomment-262640914,1,['Concurren'],['ConcurrentModificationException']
Performance,"User is reporting a nearly exact 2 minute pause at tool startup. Seems very suspicious, possibly some sort of gcs operation trying and timing out?. ```; 14:33:39.416 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/opt/gatk-package-4.beta.3-local.jar!/com/intel/gkl/native/libgkl_compression.so; 14:35:46.843 INFO BaseRecalibrator - HTSJDK Defaults.COMPRESSION_LEVEL : 5; 14:35:46.843 INFO BaseRecalibrator - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 14:35:46.843 INFO BaseRecalibrator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 14:35:46.844 INFO BaseRecalibrator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 14:35:46.844 INFO BaseRecalibrator - Deflater: IntelDeflater; 14:35:46.844 INFO BaseRecalibrator - Inflater: IntelInflater; 14:35:46.844 INFO BaseRecalibrator - GCS max retries/reopens: 20; 14:35:46.844 INFO BaseRecalibrator - Using google-cloud-java patch 317951be3c2e898e3916a4b1abf5a9c220d84df8; 14:35:46.844 INFO BaseRecalibrator - Initializing engine; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3491#issuecomment-325211756:193,Load,Loading,193,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3491#issuecomment-325211756,1,['Load'],['Loading']
Performance,"Using git bisect I found the commit that caused the performance regression: 8a366c7ba570c61338f7109b86c3284b80d5cf47. If I revert this, then both BQSR and HC both take the expected amount of time running on an exome (~15 min, ~10 min, respectively).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4376#issuecomment-365969330:52,perform,performance,52,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4376#issuecomment-365969330,1,['perform'],['performance']
Performance,"VD are identical, despite the code being rewritten from scratch to be more memory efficient (e.g., filtering is performed in place)---phew!. If we stored read counts as HDF5 instead of as plain text, this would make things much faster. Perhaps it would be best to make TSV an optional output of the new coverage collection tool. As a bonus, it would then only take a little bit more code to allow previous PoNs to be provided via -I as an additional source of read counts. Remaining TODO's:. - [x] Allow DenoiseReadCounts to be run without a PoN. This will just perform standardization and optional GC correction. This gives us the ability to run the case sample pipeline without a PoN, which will give users more options and might be good enough if the data is not too noisy.; - [x] Actually, I'm not sure why we take perform SVD on the intervals x samples matrix and take the left-singular vectors. <s>Typically, SVD is performed on the samples x intervals matrix and the right-singular vectors are taken, which saves some extra computation that is necessary to calculate the left-singular vectors.</s> (EDIT: Actually, looks like Spark's SVD is faster on tall and skinny matrices, which might be due to the fact that the underlying implementation calls Fortran code. I still think that representing samples as row vectors has some benefits, so I've changed things to reflect this; I now just take a transpose before performing the SVD, so that we still operate on the same intervals x samples matrix.) This will also save us some transposing, which we do anyway to make HDF5 writes faster.; - [x] Change HDF5 matrix writing to allow matrices with NxM > MAX_INT, which can be done naively by chunking and writing to multiple HDF5 subdirectories. This will allow for smaller bin sizes. (EDIT: I implemented this in a way that allows one to set the maximum number of values allowed per chunk, so that heap usage can be controlled, but the downside is that this translates into a corresponding limit o",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351:1915,perform,performed,1915,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351,1,['perform'],['performed']
Performance,"Valentin & I discussed this in person just now, with the following results:. -The various *Context objects should probably be refactored to return empty lists upon lack of input, as Valentin suggested, instead of being `Optional`. -There may be a need to allow tools to request additional context around the current locus/interval, but tools should probably not be performing arbitrary queries as a general rule, since it would be difficult or impossible to optimize a traversal in which the access pattern is random. If a tool needs to group disparate data items together (eg., mates on different contigs), there should be an initial grouping step to prepare the required data for the main analysis, instead of random queries within the main analysis. -apply()/map() should take its inputs as parameters instead of directly accessing member variables into which input data has been injected.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/242#issuecomment-76805735:365,perform,performing,365,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/242#issuecomment-76805735,2,"['optimiz', 'perform']","['optimize', 'performing']"
Performance,Very excited that this is almost ready! I'm presently going to run some tests on matched BGE/exome samples to assess runtime performance and results with gatk-sv.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8561#issuecomment-2125097844:125,perform,performance,125,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8561#issuecomment-2125097844,1,['perform'],['performance']
Performance,"Very interesting, @akiezun, but aren't you forgetting something? We haven't yet shown that the cost of broadcasting the variants is anything more than a tiny fraction of the total runtime, and we haven't shown how that cost scales with the number of nodes and the size of the vcfs. It's pointless to spend tons of time optimizing something and adding extra complexity if it's not a substantial fraction of runtime. I've opened https://github.com/broadinstitute/gatk/issues/1675, and proposed it as a pre-requisite to this ticket.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1407#issuecomment-203953120:319,optimiz,optimizing,319,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1407#issuecomment-203953120,1,['optimiz'],['optimizing']
Performance,W50RXZpZGVuY2UuamF2YQ==) | `0% <0%> (-83.036%)` | `0 <0> (-13)` | |; | [...er/tools/spark/sv/FindBreakpointEvidenceSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/3133?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9GaW5kQnJlYWtwb2ludEV2aWRlbmNlU3BhcmsuamF2YQ==) | `8.523% <0%> (-63.352%)` | `0 <0> (-43)` | |; | [...ellbender/tools/spark/sv/ReadsForQNamesFinder.java](https://codecov.io/gh/broadinstitute/gatk/pull/3133?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9SZWFkc0ZvclFOYW1lc0ZpbmRlci5qYXZh) | `0% <0%> (-93.548%)` | `0 <0> (-7)` | |; | [...exome/germlinehmm/xhmm/XHMMArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/3133?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9nZXJtbGluZWhtbS94aG1tL1hITU1Bcmd1bWVudENvbGxlY3Rpb24uamF2YQ==) | `0% <0%> (-100%)` | `0% <0%> (-2%)` | |; | [...te/hellbender/tools/exome/PerformSegmentation.java](https://codecov.io/gh/broadinstitute/gatk/pull/3133?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9QZXJmb3JtU2VnbWVudGF0aW9uLmphdmE=) | `0% <0%> (-100%)` | `0% <0%> (-3%)` | |; | [...dinstitute/hellbender/utils/svd/OjAlgoAdapter.java](https://codecov.io/gh/broadinstitute/gatk/pull/3133?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9zdmQvT2pBbGdvQWRhcHRlci5qYXZh) | `0% <0%> (-100%)` | `0% <0%> (-10%)` | |; | [...llbender/tools/walkers/bqsr/GatherBQSRReports.java](https://codecov.io/gh/broadinstitute/gatk/pull/3133?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Jxc3IvR2F0aGVyQlFTUlJlcG9ydHMuamF2YQ==) | `0% <0%> (-100%)` | `0% <0%> (-2%)` | |; | [...itute/hellbender/tools/exome/SampleCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/3133?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlb,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3133#issuecomment-310490622:2701,Perform,PerformSegmentation,2701,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3133#issuecomment-310490622,1,['Perform'],['PerformSegmentation']
Performance,"WIth the 4.2.2.0 ReblockGVCF it is running fine. This was without rerunning the HaplotypeCaller to create the gvcf just the reblock. . ```; Using GATK jar /share/pkg.7/gatk/4.2.2.0/install/gatk-4.2.2.0/gatk-package-4.2.2.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /share/pkg.7/gatk/4.2.2.0/install/gatk-4.2.2.0/gatk-package-4.2.2.0-local.jar ReblockGVCF -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -V gvcf.gather/GARDWGSN00001.autosome.g.vcf.gz -drop-low-quals -rgq-threshold 20 -do-qual-approx -O g1.test.reblock.g.vcf.gz; 00:54:40.318 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.2.0/install/gatk-4.2.2.0/gatk-package-4.2.2.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Aug 25, 2021 12:54:40 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 00:54:40.501 INFO ReblockGVCF - ------------------------------------------------------------; 00:54:40.501 INFO ReblockGVCF - The Genome Analysis Toolkit (GATK) v4.2.2.0; 00:54:40.501 INFO ReblockGVCF - For support and documentation go to https://software.broadinstitute.org/gatk/; 00:54:40.501 INFO ReblockGVCF - Executing as farrell@scc-hadoop.bu.edu on Linux v3.10.0-1160.36.2.el7.x86_64 amd64; 00:54:40.502 INFO ReblockGVCF - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 00:54:40.502 INFO ReblockGVCF - Start Date/Time: August 25, 2021 12:54:40 AM EDT; 00:54:40.502 INFO ReblockGVCF - ------------------------------------------------------------; 00:54:40.502 INFO ReblockGVCF - ------------------------------------------------------------; 00:54:40.503 INFO ReblockGVCF - HTSJDK Version: 2.24.1; 00:54:40.503 INFO ReblockGVCF - Picard",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7334#issuecomment-905183643:789,Load,Loading,789,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7334#issuecomment-905183643,1,['Load'],['Loading']
Performance,We already cache these values,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2058#issuecomment-494963009:11,cache,cache,11,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2058#issuecomment-494963009,1,['cache'],['cache']
Performance,"We aren't using any multi-threading inside the GenomicsDB jar. If there is a way to reproduce this, we can take a look.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4518#issuecomment-374035037:20,multi-thread,multi-threading,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4518#issuecomment-374035037,1,['multi-thread'],['multi-threading']
Performance,"We can split the tests up into a bunch of build units, but we get rapidly diminishing returns since we spend minutes installing things and building the artifacts for every vm. Travis doesn't have any notion of a build pipeline, so we can't say ""build the artifact on 1 vm and then share it and run the tests on these 10 vms"". Since we have a hard limit of the number of VMs we can use at a time for DSDE ( currently 15) being less efficient is very bad news. ( We could of course pay for more... I recently asked travis support if we could switch to an unlimited pay for usage model and they said no. ) We could also try optimizing wasted vm time by prebuilding dockers to run in, but that's additional complication. (maybe not to much, might be worth it). We also would need a scheme for dividing tests evenly between VM's. I tried writing a testng test listener to distribute them between N nodes based on the hashcode of the test class, for use on circleci, but it fell over and exploded for some reason that I never debugged. We could either debug it, or possibly split the tests by splitting the list of test files and then passing those in specifically. (May run into character limits for command lines... ). Balancing it is going to be tricky though in any case since the tests are very unevenly expensive. (We could probably manually balance it since there are a small number of slow tests and the rest are so fast they don't really matter) . The easiest thing would be to set the test to run in parallel on their existing vm. I can try turning it on again. I remember it caused problems before though, which I assume we haven't addressed. Each vm has 2 cores, so we might see some speedup. I suspect we may be using both cores to some degree already, since performance nearly doubled when we switched from the 1 core to 2 core build machines. We could probably build a more efficient pipeline on jenkins if we wanted since it does have a notion of pipelines.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1769#issuecomment-214492346:621,optimiz,optimizing,621,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1769#issuecomment-214492346,2,"['optimiz', 'perform']","['optimizing', 'performance']"
Performance,"We can work around the TestNG issue, but the `.so` not loading on travis is potentially serious, and prevents us from handing the tool off for profiling. Do we need to regenerate the `.so` @kgururaj @kdatta ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-296268544:55,load,loading,55,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-296268544,1,['load'],['loading']
Performance,"We decided to remove the ""conversion"" to AllelicCapseg output from ModelSegments, since this was an ill defined procedure. The models used by AllelicCapseg and AllelicCNV/ModelSegments are simply different, so it's not possible to define a unique conversion between their model parameters. Compounding this, we also had difficulty finding up-to-date documentation about the models used by various versions of both AllelicCapseg and ABSOLUTE. That said, some of this removed functionality can be found in unsupported WDLs at https://github.com/broadinstitute/gatk/tree/master/scripts/unsupported/combine_tracks_postprocessing_cnv (specifically, see the PrototypeACSConversion task in combine_tracks.wdl). These scripts also attempt to perform rudimentary filtering of germline events found in the matched normal; see first link below for some additional caveats. Note that we cannot really answer further questions or otherwise support these scripts (and it's possible that the experimental/beta GATK tools used in the WDLs may be removed in the future), and the developer responsible for them has moved on from the Broad---use them at your own risk. See also https://gatkforums.broadinstitute.org/gatk/discussion/comment/59467 https://github.com/broadinstitute/gatk/pull/5450 https://github.com/broadinstitute/gatk/issues/5804 for additional context.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6685#issuecomment-652407603:734,perform,perform,734,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6685#issuecomment-652407603,1,['perform'],['perform']
Performance,"We do, but not on performance. The performance reading from GCS buckets is just fine, when we take into account the time it would otherwise take to copy the data over from the bucket.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1755#issuecomment-282856936:18,perform,performance,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1755#issuecomment-282856936,2,['perform'],['performance']
Performance,"We don't yet have good regression tests for Spark that run on a cluster and are separate from the jenkins performance tests. https://github.com/broadinstitute/gatk/issues/2298 will satisfy part of the requirements for this ticket once it's done (by catching the most basic regressions before merge), but there's also a need for larger-scale correctness tests whose status is clearly visible on github.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2288#issuecomment-287473713:106,perform,performance,106,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2288#issuecomment-287473713,1,['perform'],['performance']
